{"text": "\n![HERO image](./img/hero.png)\n\n**Overview of Key Sections:**\n- [**Vector Distance Calculations**](#vector-distance-calculations) Different vector distance metrics popularly used in Weaviate.\n- [**Implementations of Distance Calculations in Weaviate**](#vector-distance-implementations) Improvements under the hood for implementation of Dot product and L2 distance metrics.\n- [**Intel\u2019s 5th Gen Intel Xeon Processor, Emerald Rapids**](#enter-intel-emerald-rapids)  More on Intel's new 5th Gen Xeon processor.\n- [**Benchmarking Performance**](#lets-talk-numbers) Performance numbers on microbenchmarks along with simulated real-world usage scenarios. \n\n\nWhat\u2019s the most important calculation a vector database needs to do over and over again? What simple operation does it spend the majority of its time performing?\n\nIf you guessed **vector distance calculations** \u2026 BINGO! \ud83c\udf89\n\nWhile vector databases use many techniques and algorithms to improve performance (including locality graphs, quantization, hash based approaches), at the end of the day, efficient distance calculations between high-dimensional vectors is a requirement for a good vector database. In fact, when profiling Weaviate indexed using HNSW, we find that 40%-60% of the CPU time is spent doing vector distance calculations.\n\nSo when someone tells us that they can make this quintessential process *much faster* they have our full attention! If you want to learn how to leverage algorithmic and hardware optimizations to make vector search 40% faster keep reading!\n\nIn this post we\u2019ll do a technical deep dive into different implementations for vector distance calculations and optimizations enabled by Intel\u2019s new 5th Gen Xeon Processor - Emerald Rapids, parallelization techniques using SIMD with the Intel AVX-256 and Intel AVX-512 instruction sets, loop unrolling and compiler optimizations by transpiling C to Go assembly. We explain how we attained a **~40% QPS speed up at 90% Recall** in Weaviate running on Intel\u2019s new Xeon Processor, Emerald Rapids.\n\n## Vector Distance Calculations\n\nVector databases use vector representations generated from machine learning models to capture the meaning of data. The fact that these vectors are numerical representations of data objects allows us to apply mathematical operations to them, such as calculating the distance between two vector representations to determine their similarity.\n\nTo calculate the distance between two vectors, you can use several similarity measures, detailed below:\n\n\n![Vector Distance Calculations](./img/vector_distance.png)\n\n\nAs you can see, there are many different similarity measures - cosine, dot product or euclidean (also know as L2) are the most commonly used in practice. To learn more about the different distance metrics, you can continue reading our blog post on [What are Distance Metrics in Vector Search?](/blog/distance-metrics-in-vector-search/)\n\nWhen a vector database needs to retrieve semantically similar objects it has to calculate one of these distance metrics to assess how close a query point is to candidate points in the database. In fact, this vector distance calculation needs to be conducted when you build the index as well as anytime you perform any CRUD operation.\n\nThe runtime complexity for these vector distance operations scales linearly with the dimensionality of the vector. Typically vector dimensionality tends to be anywhere from 256 to 4096 in practice - this number is trending upwards over time - which means that distance calculations are also taking longer to perform.\n\nThis being the case, we need to ensure that this operation is done as fast and efficiently as possible, whether through leveraging hardware or software efficiencies.\n\nLet\u2019s take a look at how these calculations can be implemented, for simplicity's sake we will consider the dot product only.\n\n## Vector Distance Implementations\n\n### Naive Go Implementation\n\nThe simplest way to implement the dot product in Go is to loop over the vector dimensions one at a time and sum the product of the vectors. This is quite slow for higher dimensionality but it works well for low dimensional vectors.\n\n```Go\nfunc dotProduct(a, b []float32) float32 {\n\n    var result float32\n\n    for i := 0; i = 128) {\n\n    __m512 acc = _mm512_setzero_ps();\n\n    while (n >= 128) {\n      \n        __m512 a_vec, b_vec;\n        for (int i = 0; i \n\n## Let\u2019s Talk Numbers!\n\nTo benchmark the improvements we opted to conduct a performance evaluation of Weaviate using Intel\u2019s Emerald Rapids 5th Gen Xeon Processor, simulating real-world usage scenarios. Initially, we devised a Go micro benchmark to compare the performance of the two instruction sets across L2 and dot product distance metrics. The results, depicted in the graphs below, provide valuable insights into the relative efficiencies of AVX-256 and AVX-512 instructions.\n\n\n\n  \n  Figure 1: Improvements in vector distance operations performed using 1536 dimensional vectors.\n\n\n\nAs you can see above we get some pretty remarkable speedups for single distance calculations. For dot product operations at 1536 dimensions, AVX-512 is 28.6 times faster than pure Go and 6 times faster than AVX-256. Whereas for L2 distance measurements at 1536 dimensions, AVX-512 is 19 times faster than pure Go and 5.6 times faster than AVX-256.\n\nSubsequently, we conducted ANN benchmarks utilizing two different [datasets DBPedia](http://dev.dbpedia.org/Download_DBpedia) embedded using OpenAI `text-embedding-ada-002` model (1536 dimensions), and Meta\u2019s 768 dimension [Sphere dataset](/blog/sphere-dataset-in-weaviate), aiming to gauge the practical implications of these instructions on real-world tasks. As seen in the graphs the benchmark results showed notable performance improvements, further validating the advantages of AVX-512 over AVX-256 in Emerald Rapids.\n\n\n  \n  Figure 2: Intel Emerald Rapids 5th Gen Xeon ANN performance on DBPedia ada-002 embeddings.\n\n\n\n  \n  Figure 3: Intel Emerald Rapids 5th Gen Xeon ANN performance on Sphere embeddings.\n\n\nHere on the sphere-1 million dataset when searched using dot products we see a remarkable 42% improvement in Weaviate queries per second(QPS) @ 90% Recall, while on the dbpedia-500k dataset when searched using L2 distance we see a 40% improvement in QPS @ 90% Recall!\n\nFurthermore, our assessment revealed a 16% enhanced AVX-512 performance in Intel\u2019s 4th Gen Xeon Processor, Sapphire Rapids compared to AVX-256. As a result, we made the strategic decision to enable the latest Intel Xeon Gen Processors, both Emerald Rapids and Sapphire Rapids, to leverage AVX-512 instructions starting from Weaviate version `1.24.2`. This proactive measure ensures that users can harness the full potential of their hardware, now and in the future.\n\n\n", "type": "Blog", "name": "Blog Intel", "path": "blog/2024-03-26-intel/index.mdx", "link": "https://weaviate.io/blog/intel", "timestamp": "2024-05-08 10:52:02", "reader": "JSON", "meta": {}, "chunks": []}