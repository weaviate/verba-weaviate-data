{"text": "\n## What is Sphere?\nSphere is an open-source dataset recently released by Meta. It is a collection of 134 million documents (broken up into 906 million 100-word snippets). It is one of the largest knowledge bases that can help solve knowledge-intensive natural language tasks such as question-answering, fact-checking, and much more.\n\nSimply stated, Sphere aims to act as a \"universal, uncurated and unstructured source of knowledge.\" This means that the next time you have a question like: \"Was McDonald's, the food chain, founded by the same Ol' McDonald who had a farm?\" Sphere will have the relevant knowledge to answer your question and point you toward a relevant article. The potential for this large of a dataset is awe-inspiring and the applications one can dream up are limitless - from combating fake news on social media platforms to helping locate your next dream vacation spot.\n\nAdditionally, Sphere is ideal for hybrid vector search at scale since it is one of the few large scale datasets where vectors are provided in addition to their corresponding text fields. You can learn more about the model that is used to generate these vectors here. For all of these reasons we wanted to make this resource as accessible to the community as possible.\n\n## The Challenges of using Sphere\n\nThe only limitation of this dataset is how difficult it is to access and use for the average developer. In this regard the enormity of the dataset ends up as a double-edged sword. It is challenging to use Sphere in its current open-source format for anyone other than large industry and academic labs - even for them the UX leaves a lot to be desired.\n\n*Don't believe me?*\n\nTry following the readme to get Sphere set up and usable on your machine. The first limitation you'll run into is that the smallest open-sourced sparse Sphere indices file is a whopping 833 Gigabytes in its compressed format. Once you get past that hurdle,  to start using the Sphere dataset for its intended purpose of hybrid search testing and benchmarking, it requires another herculean effort.\n\n## The Sphere Dataset in Weaviate\nIn an effort to make this powerful resource accessible to everyone, we are happy to announce that Sphere is now **available** not only in **Weaviate** but also as **JSON** or **Parquet files**. The dataset can be easily imported with Python and Spark! You can import **large vectorized chunks** of Sphere (or the whole thing if you want!) and start searching through it  in just a few lines of code!\n\nThe power of the sun in the palm of your hand! *ensues evil maniacal laughter*\n\n\n\n  \nYour browser does not support the video tag.\n\n\n\nGet it? ... It's a Sphere \ud83e\udd41ba dum tsss\ud83e\udd41 I'll show myself out\u2026\n\n\nThere are two ways to import the Sphere dataset into Weaviate. You can use the Python client (less than 75 lines of code) or the Weaviate Spark connector.\n\n### Importing Sphere with Python\nThe setup is quite straightforward, all you need is the Weaviate Client. We provide an example that uses the Python Client and the dpr-question_encoder-single-nq-base model (i.e., the module that is used to vectorize objects in Sphere).\n\nWe have prepared files ranging from 100K data points all the way up to the entire Sphere dataset, which consists of 899 million lines. You can download them from here:\n* 100k lines\n* 1M lines\n* 10M lines\n* 100M lines\n* 899M lines\n\nOnce you have the dataset file downloaded and unpacked, the next step is to import the dataset into Weaviate with Python:\n\n```python\nimport sys\nimport os\nimport time\nimport json\nimport weaviate\n\n# Variables\nWEAVIATE_URL    = 'https://loadtest.weaviate.network/'\nBATCH_SIZE      = 100\nSPHERE_DATASET  = 'sphere.100k.jsonl' # update to match your filename\n\nclient = weaviate.Client(\n    url=WEAVIATE_URL,\n    timeout_config=600\n)\n\nclient.batch.configure(\n    batch_size=BATCH_SIZE,\n    dynamic=True,\n    num_workers=os.cpu_count()\n)\n\n# Set DPR model used for the Page class\nclient.schema.create_class({\n    \"class\": \"Page\",\n    \"vectorizer\": \"text2vec-huggingface\",\n    \"moduleConfig\": {\n        \"passageModel\": \"sentence-transformers/facebook-dpr-ctx_encoder-single-nq-base\",\n        \"queryModel\": \"sentence-transformers/facebook-dpr-question_encoder-single-nq-base\",\n        \"options\": {\n            \"waitForModel\": True,\n            \"useGPU\": True,\n            \"useCache\": True\n        }\n    },\n    \"properties\": []\n})\n\n# Import the data, Weaviate will use the auto-schema function to\n# create the other properties and other default settings.\nstart = time.time()\nc = 0\nwith open(SPHERE_DATASET) as jsonl_file:\n    with client.batch as batch:\n        for jsonl in jsonl_file:\n            json_parsed = json.loads(jsonl)\n            batch.add_data_object({\n                    'url':  json_parsed['url'],\n                    'title': json_parsed['title'],\n                    'raw': json_parsed['raw'],\n                    'sha': json_parsed['sha']\n                },\n                'Page',\n                json_parsed['id'],\n                vector=json_parsed['vector']\n            )\n            c += 1\n            if (c % (BATCH_SIZE * 10)) == 0:\n                print('Imported', c)\n\nend = time.time()\nprint('Done in', end - start)\n```\n*Make sure to update the SPHERE_DATASET property to correctly match your `.jsonl` filename.*\n\n### Importing Sphere with Spark\nIf you want to start training large language models for knowledge-intensive tasks on Sphere, then you might want to leverage big data frameworks. This is where Apache Spark enters the picture!\n\nTo process Sphere with Spark, you can use PySpark and Weaviate's Python Client. The setup is slightly more difficult than simply importing the dataset with python; however, once you do have it setup, it is lightning fast! \u26a1\n\nYou can see the step-by-step instructions detailed in this tutorial. The tutorial demonstrates how to get Sphere into a Spark dataframe, import it into Weaviate, and conduct queries. Once you have Sphere imported into your Spark instance you can leverage Spark functionality to start training powerful models. In this particular example we are using the Weaviate Spark connector making it easy to load data from Spark to Weaviate.\n\nWe have also prepared two Parquet files one with 1M data points and another with the entire Sphere dataset, which consists of 899 million lines. You can download them into a dataframe as follows:\n\n```\ndf = spark.read.parquet(\"gs://sphere-demo/parquet/sphere.1M.parquet\")\ndf = spark.read.parquet(\"gs://sphere-demo/parquet/sphere.899M.parquet\")\n```\n## Searching through Sphere with Weaviate\nNow that the instructions are out of the way lets have some fun and show you the combined power of Sphere on Weaviate! We imported the entire Sphere dataset into Weaviate - yes, all ~899 million objects, see below for proof!\n\nimport proof\n\nOnce the Sphere dataset is in Weaviate we can use it in conjunction with all of the functionality that comes with Weaviate.\n\nSince the primary usage of Sphere is around conducting large scale hybrid search below you can see an example of where we leverage vector search to find out what Sphere says is the best food to eat in Italy while simultaneously using conventional word matching to ensure the returned objects are from a credible source, the New York Times in this case.\n\nquery\n\nAnd that's all folks! Now with the Sphere dataset readily available and easy to import into Weaviate anyone can start to build with this powerful tool in conjunction with the loads of awesome features that we already offer in Weaviate. Happy coding!\n\n\nimport StayConnected from '/_includes/stay-connected.mdx'\n\n\n", "type": "Blog", "name": "blog-sphere-dataset-in-weaviate", "path": "blog/2022-12-06-sphere-dataset-in-weaviate/index.mdx", "link": "https://weaviate.io/blog/sphere-dataset-in-weaviate", "timestamp": "2023-11-13 10:42:04", "reader": "JSON", "meta": {}, "chunks": []}