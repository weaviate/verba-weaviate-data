{"text": "\n## Introduction\n\nA couple weeks back in our blog on [ChatGPT plugins](/blog/weaviate-retrieval-plugin) we talked about the potential for plugins to help expand ChatGPT\u2019s functionality by allowing it to leverage third-party resources to act upon the conversations that you have with it. The value for these plugins is greatest when they help make up for a current short-coming that ChatGPT has. For example ChatGPT is built on top of GPT 4.0 which is a large language model which doesn\u2019t understand mathematical and algebraic reasoning as well as it does written language and thus using the WolframAlpha plugin as a \u201cmath-mode\u201d when needing to solve mathematical problems makes perfect sense!\n\nAnother short-coming of ChatGPT we spoke about was that it lacks the use of context in answering your questions unless this context is specifically conveyed in the body of the prompt. The solution to this shortcoming was the [ChatGPT retrieval plugin](https://github.com/openai/chatgpt-retrieval-plugin) which connects ChatGPT to a [vector database](https://weaviate.io/blog/what-is-a-vector-database) which provides a robust solution to the above problem. The vector database connected to ChatGPT can be used to store and refer to relevant information when answering prompts and acts as longterm memory of the LLM.\n\nPlugins are a very powerful way that you and I can contribute to improving LLM use-cases without having to retrain the underlying GPT model. Let\u2019s say you\u2019re using ChatGPT and realize that it doesn't carry a conversation well enough when you ask it a question about the weather or it doesn\u2019t have a specialized enough understanding of your health to suggest tasty and healthy recipes based on your previous blood sugar, pressure levels and health conditions. You can create a plugin to tackle these problems and in doing so improve usability for everyone since they can simply install your plugin and use it!\n\nThe only questions then are **how do you get access to the exclusive plugin alpha** that OpenAI is running and **how do you go about creating a plugin for ChatGPT!?** Worry not, we come bearing good news on both notes \ud83d\ude00.\n\nWeaviate is partnering with OpenAI and Cortical Ventures to host a full-day [Generative AI Hackathon at ODSC East](https://www.eventbrite.com/e/generative-ai-hackathon-odsc-east-2023-tickets-616336738777) on May 11th in Boston at the Hynes Convention Center. There you will get access to the **OpenAI API** and **ChatGPT plugin tokens** that OpenAI is providing and you will be able to create your own plugins as well as AutoGPT-like apps to solve problems near and dear to your heart using tools like ChatGPT and Weaviate!  You can register using the link provided above, slots are limited so don\u2019t delay!\n\n![hackathon](./img/hackathon.png)\n\nNow getting to how you can create your own plugin for ChatGPT, here we will go through the step-by-step process of how we created the Weaviate Retrieval Plugin. The Weaviate retrieval plugin connects ChatGPT to an instance of Weaviate and allows it to query relevant documents from the vector database, upsert documents to \u201cremember\u201d information for later and also delete documents to \u201cforget\u201d them! The process that we took to create this plugin is quite similar to what one might take in creating a general plugin and thus we believe it\u2019s quite instructive and we hope that it helps!\n\n## How to Create a ChatGPT Plugin\n\n![plugin](./img/plugin-light.png#gh-light-mode-only)\n![plugin](./img/plugin-dark.png#gh-dark-mode-only)\n\nThe entire code repository for the complete Weaviate Retrieval Plugin is located [here](https://github.com/weaviate/howto-weaviate-retrieval-plugin). Let\u2019s go through the steps one-by-one including code snippets and some challenges we encountered and how we eventually solved them.\n\nThe tech stack we used to develop this plugin is as follows:\n1. Python: write everything in python\n2. FastAPI: the server used to run the plugin\n3. Pytest: to write and run our tests\n4. Docker: we create containers to build, test and deploy the plugin\n\nBelow are the steps we took to develop the plugin, Part 1 focuses on building a web application with our desired endpoint, Part 2 is specific to the development of a ChatGPT plugin while Part 3 is all about remote deployment using Fly.io. We cover the steps in order but feel free to skip steps depending on you level of comfort with the material.\n\n## Part 1: Building a Web App\n\n**Step 1: Setup the Development Environment**\n\nTo setup our development environment we used [Dev Containers](https://containers.dev/). The `devcontainer.json` [file](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/.devcontainer/devcontainer.json)  was updated by adding [Fly.io](https://fly.io/), [Docker](https://www.docker.com/) and [Poetry](https://python-poetry.org/). You can find other dev container templates [here](https://containers.dev/templates).\n\n**Step 2. Test the Setup**\n\n1. After setting up the environment we tested that everything worked by:\nCreate a [dummy endpoint](https://github.com/weaviate/howto-weaviate-retrieval-plugin/commit/57c00bc85aea3a155d330f0de72525ee26a665d1#diff-c16fbf0c6f7b90a46b94b36f88893c2d174476088608841f7254afba0e81373d) which will simply respond with a `{\u201cHello\u201d: \u201cWorld\u201d}` object when called.\n\n```python\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    \"\"\"\n    Say hello to the world\n    \"\"\"\n    return {\"Hello\": \"World\"}\n```\n\n2. Set up tests using PyTest which accomplish two goals - firstly we want to check that our Weaviate instance is up and running which is setup [here](https://github.com/weaviate/howto-weaviate-retrieval-plugin/commit/57c00bc85aea3a155d330f0de72525ee26a665d1#diff-e52e4ddd58b7ef887ab03c04116e676f6280b824ab7469d5d3080e5cba4f2128) and secondly that the Fast API endpoint is responding. Both of these tests are defined [here](https://github.com/weaviate/howto-weaviate-retrieval-plugin/commit/57c00bc85aea3a155d330f0de72525ee26a665d1#diff-5f4f40de92f9f92f3638fdd1c3eace62db4d153ff64b915f82e43e6781960ed6)\n\n3. We also created a [makefile](https://github.com/weaviate/howto-weaviate-retrieval-plugin/commit/57c00bc85aea3a155d330f0de72525ee26a665d1#diff-beda42571c095172ab63437d050612a571d0d9ddd3ad4f2aecbce907a9b7e3d0) to automate running and tests and firing up the endpoint. In our makefile we also specify a `run` command which will spin up the server locally to ensure the network connectivity settings are all setup properly. You can also connect to port 8000 which is the default port that FastAPI listens to to check connectivity.\n\n4. The last step to verify everything is running correctly is to go to `localhost:8000/docs` which should give you the [Swagger UI](https://swagger.io/tools/swagger-ui/) for your endpoint. The Swagger UI gives you the ability to play around with your server, interact with any endpoints you may have defined, and it all gets updated in real-time - this is particularly convenient when we later want to call endpoints manually to interact with our Weaviate instance to query, upsert and delete objects.\n\n![swaggerui](./img/swagger.png)\nOnce you've done all of the above and everything looks to be in good order you can start implementing plugin specific functions.\n\n**Step 3: Implement a function to get vector embeddings**\n\nSince we are implementing a plugin that connects a vector database to ChatGPT we will need to define a way to generate vector embeddings which can be used when we upsert documents to our database to generate and store vector embeddings for our documents, this function will also be used to vectorize queries when querying and performing vector search over the vector database. This function is [implemented here](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/server/embedding.py).\n```python\nimport openai\n\ndef get_embedding(text):\n    \"\"\"\n    Get the embedding for a given text\n    \"\"\"\n    results = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")\n\n    return results[\"data\"][0][\"embedding\"]\n```\n\nHere we simply chose to use the [`ada-002` model](https://openai.com/blog/new-and-improved-embedding-model) as OpenAI specifies that this particular model is used for their template retrieval plugin, however since the querying is done in the vector database we could have chosen to use any vectorizer.\n\n**Step 4: Implement function to initialize the Weaviate Client and vector database**\n\nNext we implement a [couple of functions](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/server/database.py) to initialize the Weaviate python client and through the client initialize the Weaviate instance by checking if a schema exists and if it doesn\u2019t we add one.\n\n```python\nimport weaviate\nimport os\nimport logging\n\nINDEX_NAME = \"Document\"\n\nSCHEMA = {\n    \"class\": INDEX_NAME,\n    \"properties\": [\n        {\"name\": \"text\", \"dataType\": [\"text\"]},\n        {\"name\": \"document_id\", \"dataType\": [\"string\"]},\n    ],\n}\n\n\ndef get_client():\n    \"\"\"\n    Get a client to the Weaviate server\n    \"\"\"\n    host = os.environ.get(\"WEAVIATE_HOST\", \"http://localhost:8080\")\n    return weaviate.Client(host)\n\n\ndef init_db():\n    \"\"\"\n    Create the schema for the database if it doesn't exist yet\n    \"\"\"\n    client = get_client()\n\n    if not client.schema.contains(SCHEMA):\n        logging.debug(\"Creating schema\")\n        client.schema.create_class(SCHEMA)\n    else:\n        class_name = SCHEMA[\"class\"]\n        logging.debug(f\"Schema for {class_name} already exists\")\n        logging.debug(\"Skipping schema creation\")\n```\n\n**Step 5: Initialize the database when the server starts and add a dependency for the Weaviate client**\n\nWe now need to integrate the usage of these functions so that on starting the ChatGPT plugin server up we automatically initialize the Weaviate instance and connection of the client. We do this by using FastAPI\u2019s [lifespan](https://fastapi.tiangolo.com/advanced/events/#lifespan-events) feature in the  [main server python script](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/server/main.py) which gets run every time the server starts. This simple function calls our database initialization function defined above which yields the Weaviate client object. Any logic that needs to be run on server shutdown can be included after the `yield` statement below. Since we don\u2019t need to do anything specific for our plugin we leave it empty.\n\n```python\nfrom fastapi import FastAPI\nfrom contextlib import asyncontextmanager\n\nfrom .database import get_client, init_db\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    init_db()\n    yield\n\n\napp = FastAPI(lifespan=lifespan)\n\ndef get_weaviate_client():\n    \"\"\"\n    Get a client to the Weaviate server\n    \"\"\"\n    yield get_client()\n```\n\nAfter this point the initial server setup and testing is complete. Now we get to the fun part of implementing our endpoints that will give ChatGPT different ways to interact with our plugin!\n\n## Part 2: Implementing OpenAI Specific Functionality\n\n![fun](./img/areyahavingfunmrkrabs.png)\n\n**Step 1: Development of the Weaviate Retrieval Plugin specific endpoints**\n\nOur plugin has three specific endpoints: `/upsert`, `/query` and `/delete`. These functions give ChatGPT the ability to add objects to the Weaviate instance, query and search through objects in the Weaviate instance and lastly delete objects if needed. Upon interacting with ChatGPT while the plugin is enabled it can be instructed to use a particular endpoint via prompt but will also independently decide when to use the appropriate endpoint to complete the response to a query! These endpoints are what extend the functionality of ChatGPT and enable it to interact with the vector database.\n\nWe developed these three endpoints through test driven development, as such we will display the tests that each endpoint must first pass and then the implementation that satisfies these tests. In preparation to setup the Weaviate instance for these tests we added the following test documents through a fixture:\n```python\n@pytest.fixture\ndef documents(weaviate_client):\n    docs = [\n        {\"text\": \"The lion is the king of the jungle\", \"document_id\": \"1\"},\n        {\"text\": \"The lion is a carnivore\", \"document_id\": \"2\"},\n        {\"text\": \"The lion is a large animal\", \"document_id\": \"3\"},\n        {\"text\": \"The capital of France is Paris\", \"document_id\": \"4\"},\n        {\"text\": \"The capital of Germany is Berlin\", \"document_id\": \"5\"},\n    ]\n\n    for doc in docs:\n        client.post(\"/upsert\", json=doc)\n```\n\n**Implementing the `/upsert` endpoint:**\n\nAfter using the `/upsert` endpoint we mainly want to test that that we got the appropriate status code in addition to checking that the content, id and vector\u2019s were all upserted correctly.\n\nHere's the test that carries this out:\n\n```python\ndef test_upsert(weaviate_client):\n    response = client.post(\"/upsert\", json={\"text\": \"Hello World\", \"document_id\": \"1\"})\n    assert response.status_code == 200\n\n    docs = weaviate_client.data_object.get(with_vector=True)[\"objects\"]\n    assert len(docs) == 1\n    assert docs[0][\"properties\"][\"text\"] == \"Hello World\"\n    assert docs[0][\"properties\"][\"document_id\"] == \"1\"\n    assert docs[0][\"vector\"] is not None\n```\n\nThe implementation below satisfies all of these requirements and tests above:\n```python\n@app.post(\"/upsert\")\ndef upsert(doc: Document, client=Depends(get_weaviate_client)):\n    \"\"\"\n    Insert a document into weaviate\n    \"\"\"\n    client.batch.configure(batch_size=100)  # Configure batch\n    with client.batch as batch:\n        batch.add_data_object(\n            data_object=doc.dict(),\n            class_name=INDEX_NAME,\n            vector=get_embedding(doc.text),\n        )\n\n    return {\"status\": \"ok\"}\n```\n\nThe `/query` and `/delete` endpoints were developed similarly, if you're interested you can read below!\n\n\n  See details for /query endpoint implementation.\n\n**Implement the `/query` endpoint:**\n\nFor this endpoint we mainly want to check that it returns the right number of objects and that the required document that we were expecting is part of the returned objects.\n\n```python\ndef test_query(documents):\n    LIMIT = 3\n    response = client.post(\"/query\", json={\"text\": \"lion\", \"limit\": LIMIT})\n\n    results = response.json()\n\n    assert len(results) == LIMIT\n    for result in results:\n        assert \"lion\" in result[\"document\"][\"text\"]\n```\n\nThe implementation below will take in a query and return a list of retrieved documents and metadata.\n\n```python\n@app.post(\"/query\", response_model=List[QueryResult])\ndef query(query: Query, client=Depends(get_weaviate_client)) -> List[Document]:\n    \"\"\"\n    Query weaviate for documents\n    \"\"\"\n    query_vector = get_embedding(query.text)\n\n    results = (\n        client.query.get(INDEX_NAME, [\"document_id\", \"text\"])\n        .with_near_vector({\"vector\": query_vector})\n        .with_limit(query.limit)\n        .with_additional(\"certainty\")\n        .do()\n    )\n\n    docs = results[\"data\"][\"Get\"][INDEX_NAME]\n\n    return [\n        QueryResult(\n            document={\"text\": doc[\"text\"], \"document_id\": doc[\"document_id\"]},\n            score=doc[\"_additional\"][\"certainty\"],\n        )\n        for doc in docs\n    ]\n```\n\n\n\n  See details for /delete endpoint implementation.\n\n**Implement the `/delete` endpoint:**\n\nHere we simply want to check that the response returned correctly and that after removing one object we the number of total objects in the Weaviate instance goes down by one.\n\n```python\ndef test_delete(documents, weaviate_client):\n    num_docs_before_delete = weaviate_client.data_object.get()[\"totalResults\"]\n\n    response = client.post(\"/delete\", json={\"document_id\": \"3\"})\n    assert response.status_code == 200\n\n    num_docs_after_delete = weaviate_client.data_object.get()[\"totalResults\"]\n\n    assert num_docs_after_delete == num_docs_before_delete - 1\n```\n\nAnd the implementation of the endpoint is as follows:\n\n```python\n@app.post(\"/delete\")\ndef delete(delete_request: DeleteRequest, client=Depends(get_weaviate_client)):\n    \"\"\"\n    Delete a document from weaviate\n    \"\"\"\n    result = client.batch.delete_objects(\n        class_name=INDEX_NAME,\n        where={\n            \"operator\": \"Equal\",\n            \"path\": [\"document_id\"],\n            \"valueText\": delete_request.document_id,\n        },\n    )\n\n    if result[\"results\"][\"successful\"] == 1:\n        return {\"status\": \"ok\"}\n    else:\n        return {\"status\": \"not found\"}\n```\n\n\nHere we showed you how our endpoints work, this will be where your plugin will be most unique, depending on what functionality you want implemented you can create corresponding endpoints and test them.\n\nNotice the docstrings we\u2019ve included with all of our endpoints, these will be very important in the next step!\n\n**Step 2: Prepare plugin manifest files**\n\nThis is where you specify to OpenAI and specifically ChatGPT which endpoints your plugin is exposing, how it can use those endpoints to accomplish specific tasks, what errors to expect if the endpoints are not used correctly and much more! [OpenAI instruction specify](https://platform.openai.com/docs/plugins/getting-started/plugin-manifest) that you need to create two files: the [`openapi.yaml` file](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/.well-known/openapi.yaml) and the [`ai-plugin.json` file](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/.well-known/ai-plugin.json).\n\nAs you can see both of these files need to be in the `.well-known` directory which must be mounted into the app as follows in order for ChatGPT to utilize them correctly.\n\n`app.mount(\"/.well-known\", StaticFiles(directory=\".well-known\"), name=\"static\")`\n\nLets have a closer look at at the two files:\n\n`ai-plugin.json`\n\n```json\n{\n    \"schema_version\": \"v1\",\n    \"name_for_human\": \"Weaviate Retrieval Plugin V2\",\n    \"name_for_model\": \"Weaviate_Retrieval_Plugin\",\n    \"description_for_human\": \"Plugin to interact with documents using natural language. You can query, add and remove documents.\",\n    \"description_for_model\": \"Plugin to interact with documents using natural language. You can query, add and remove documents.\",\n    \"auth\": {\n        \"type\": \"user_http\",\n        \"authorization_type\": \"bearer\"\n      },\n    \"api\": {\n        \"type\": \"openapi\",\n        \"url\": \"https://demo-retrieval-app.fly.dev/.well-known/openapi.yaml\",\n        \"is_user_authenticated\": false\n    },\n    \"logo_url\": \"https://demo-retrieval-app.fly.dev/.well-known/logo.png\",\n    \"contact_email\": \"support@example.com\",\n    \"legal_info_url\": \"http://www.example.com/legal\"\n}\n```\nThis specifies data such as the name of the app, logo assets, and more interestingly, under the `name_for_model` field, how the model(in this case ChatGPT/GPT4.0) will refer to the plugin and the description of the plugin,`description_for_model`, that can be read and understood by the model.\n\n`openapi.yaml`\n\n![YAML](./img/openaiapi.png)\n\nThis file most importantly specifies the endpoints and describes each one to ChatGPT.\n\nGenerating this `.yaml` file was quite challenging until we realized that you could simply generate the spec in json format by going to the SwaggerUI using `/docs` and clicking on the `/openapi.json` link. Then you can use this [website](https://www.json2yaml.com/) to convert from `.json` to `.yaml`.\n\n\nThese two files are critical in order for ChatGPT to understand and utilize the exposed endpoints for your plugin correctly.\n\nOne very interesting finding in our experiments was that ChatGPT reads these files to understand not just when to use the endpoints but also how to correctly use them! So if ChatGPT is not correctly using your endpoints you should try and improve the descriptions for the plugin and the endpoints. OpenAI includes some [best practices](https://platform.openai.com/docs/plugins/getting-started/writing-descriptions) for creating these descriptions. From our experiments we found that if the description was not sufficient in describing how the endpoint should be used ChatGPT would call the endpoints with the incorrect syntax and would try again if it did fail. See the example below:\n\n![error](./img/error.png)\n\n**Take-aways:** ChatGPT doesn\u2019t have hard-coded instructions on when and how to use the plugin endpoints. You need to be very careful about how you describe your plugin and endpoints to ChatGPT so that they can be used as intended! The `openapi.json` spec that FastAPI generates for you is based on how you documented the endpoints in your code i.e. the function\u2019s name, docstring, query description and the field\u2019s description in your pydantic models. The steps used to do this are outside the scope of this blog post, for more details please refer to the FastAPI documentation. In general you want to have complete and comprehensive documentation for your plugin because the documentation is what will allow it to be used correctly!\n\nAdditionally you have to be careful when specifying the descriptions, doc strings etc. not to exceed the context length since the plugin description, API requests, and API responses are all inserted into the conversation with ChatGPT. This counts against the context limit of the model.\n\n**Step 3: Local deployment of your plugin and testing using the ChatGPT user interface**\n\nAllow http://localhost:8000 and https://chat.openai.com to make cross requests to the plugin\u2019s server. We can do this easily with FastAPI\u2019s CORSMiddleware middleware.\n\n```python\n# for localhost deployment\nif os.getenv(\"ENV\", \"dev\") == \"dev\":\n    origins = [\n        f\"http://localhost:8000\",\n        \"https://chat.openai.com\",\n    ]\n\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=origins,\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n```\n\nThe above code will only be used during local testing and will enable your locally deployed app to talk with ChatGPT. Note that our plugin actually has two apps which differ based on the type of authentication used, for the local deployment and testing we don\u2019t use authentication and for remote deployment we use Bearer tokens and https as we will explain later. Following this you can run the plugin locally by following the instructions detailed [here](https://platform.openai.com/docs/plugins/getting-started/running-a-plugin). This will allow you to test all the endpoints through the ChatGPT UI and see they are functioning correctly.\n\nHave a look at some of the endpoint testing we did locally before moving onto the next point:\n\n![test1](./img/test1.png)\n![test2](./img/test2.png)\n*Making sure the upsert and query endpoints are working correctly. Note that here depending on the language used in our prompt ChatGPT will choose to call the appropriate endpoints.*\n\n## Part 3: Remote Deployment to Fly.io\n\n**Step 1: Prepare to deploy plugin remotely to Fly.io**\n\nOnce we have tested the plugin locally to our satisfaction we can deploy remotely and install it into ChatGPT. Here are the steps we follow to share our plugin with those who have access to the alpha:\n\n1. Create a remote Weaviate Instance: This was done using the [Weaviate Cloud Services](https://console.weaviate.cloud/)\n\n2. Add a [dockerfile](https://github.com/weaviate/howto-weaviate-retrieval-plugin/blob/main/Dockerfile). This dockerfile is just a modified version the file [template provided by OpenAI](https://github.com/openai/chatgpt-retrieval-plugin/blob/main/Dockerfile) and will be used to setup your environment remotely and starts up the server.\n\n3. Update the plugin manifest config files `ai-plugin.json` and `openapi.yaml` to now [use authentication](https://platform.openai.com/docs/plugins/getting-started/plugin-manifest) in the form of a bearer token and your newly created WCS instance instead of localhost.\n\n4. Update the app to make sure all communication is authenticated.\n\nYou can see the full diff for the project as setup for local deployment thus far and how it was changed to ready it for remote deployment [here](https://github.com/weaviate/howto-weaviate-retrieval-plugin/commit/9ba22539a321e6cb8cb676c2e3a6f3a945a3f967).\n\n**Step 2: Deploy to Fly.io and Install in ChatGPT**\n\nThis is the last step and allows you to deploy your plugin to Fly.io, the detailed instructions provided [here](https://github.com/openai/chatgpt-retrieval-plugin/blob/main/docs/deployment/flyio.md) can be followed as is. Following this you can open up ChatGPT in your browser and if you have access to the plugin alpha you can install your plugin by specifying the URL where it is hosted and provide the bearer token to authenticate.\n\n## Conclusions\n\nAnd that, ladies and gentlemen, is how we created our Weavaite retrieval plugin that augments ChatGPT with longterm memory. The process to create different plugins is quite similar and we believe most of these steps can be executed in a similar manner to create a wide variety of plugins with the most variation being found in Part 2 where you define endpoint specific to you plugin.\n\nLet's end off by visualizing the [flow](https://platform.openai.com/docs/plugins/introduction/plugin-flow) of how ChatGPT can now use the plugin we just created. The figure below details how the `/query` endpoint can be used. Depending on the prompt it can also call the `/delete` and `/upsert` endpoints as well.\n\n![diagram](./img/diagram-light.png#gh-light-mode-only)\n![diagram](./img/diagram-dark.png#gh-dark-mode-only)\n*More specifically when the user prompts ChatGPT it will look at the `openapi.yaml` file to read the endpoint descriptions and will decide the appropriate endpoint to use in the execution of the prompt. Above it chooses to use the `/query` endpoint. Then it will try to build the correct request and retry upon failure. The query will return to ChatGPT relevant documents from Weaviate which it will use to answer the original prompt!*\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n\n", "type": "Blog", "name": "Blog How-to-chatgpt-plugin", "path": "blog/2023-04-27-how-to-chatgpt-plugin/index.mdx", "link": "https://weaviate.io/blog/how-to-chatgpt-plugin", "timestamp": "2024-05-08 10:51:36", "reader": "JSON", "meta": {}, "chunks": []}