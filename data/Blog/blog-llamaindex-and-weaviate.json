{"text": "\nWhile large language models (LLMs) like GPT-4 have impressive capabilities in generation and reasoning, they have limitations in terms of their ability to access and retrieve specific facts, figures, or contextually relevant information. A popular solution to this problem is setting up a retrieval-augmented generation (RAG) system: combine the language model with an external storage provider, and create an overall software system that can orchestrate the interactions with and between these components in order to create a \u201cchat with your data\u201d experience.\n\nThe combination of Weaviate and LlamaIndex provide the critical components needed to easily setup a powerful and reliable RAG stack, so that you can easily deliver powerful LLM-enabled experiences over your data, such as search engines, chatbots, and more. First, we can use Weaviate as the vector database that acts as the external storage provider. Next, we can use a powerful data framework such as LlamaIndex to help with data management and orchestration around Weaviate when building the LLM app.\n\nIn this blog post, we walk through an overview of LlamaIndex and some of the core data management and query modules. We then go through an initial demo notebook.\n\nWe\u2019re kicking off a new series to guide you on how to use LlamaIndex and Weaviate for your LLM applications.\n\n## An Introduction to LlamaIndex\nLlamaIndex is a data framework for building LLM applications. It provides a comprehensive toolkit for ingestion, management, and querying of your external data so that you can use it with your LLM app.\n\n### Data Ingestion\nOn data ingestion, LlamaIndex offers connectors to 100+ data sources, ranging from different file formats (.pdf, .docx, .pptx) to APIs (Notion, Slack, Discord, etc.) to web scrapers (Beautiful Soup, Readability, etc.). These data connectors are primarily hosted on LlamaHub. This makes it easy for users to integrate data from their existing files and applications.\n\n### Data Indexing\nOnce the data is loaded, LlamaIndex offers the ability to index this data with a wide variety of data structures and storage integration options (including Weaviate). LlamaIndex supports indexing unstructured, semi-structured, and structured data. A standard way to index unstructured data is to split the source documents into text \u201cchunks\u201d, embed each chunk, and store each chunk/embedding in a vector database.\n\n### Data Querying\nOnce your data is ingested/stored, LlamaIndex provides the tools to define an advanced retrieval / query \u201cengine\u201d over your data. Our retriever constructs allow you to retrieve data from your knowledge base given an input prompt. A query engine construct allows you to define an interface that can take in an input prompt, and output a knowledge-augmented response - it can use retrieval and synthesis (LLM) modules under the hood.\n\nSome examples of query engine \u201ctasks\u201d are given below, in rough order from easy to advanced:\n* Semantic Search: Retrieve the top-k most similar items from the knowledge corpus by embedding similarity to the query, and synthesize a response over these contexts.\n\n* Structured Analytics: Convert natural language to a SQL query that can be executed\n\n* Query Decomposition over Documents: Break down a query into sub-questions, each over a subset of underlying documents. Each sub-question can be executed against its own query engine.\n\n## Demo Notebook Walkthrough\n\nLet\u2019s walk through a simple example of how LlamaIndex can be used with Weaviate to build a simple Question-Answering (QA) system over the Weaviate blogs!\n\nThe full code can be found in the Weaviate recipes repo.\n\nThe first step is to setup your Weaviate client. In this example, we connect to an Embedded Weaviate instance.\n\n```python\nimport os\nimport weaviate\n\n# connect to your weaviate instance\nclient = weaviate.Client(embedded_options=weaviate.embedded.EmbeddedOptions(), additional_headers={ 'X-OpenAI-Api-Key': os.environ[\"OPENAI_API_KEY\"]})\n```\n\nEnsure you have an Open AI API Key set up in your environment variables for use by the Weaviate client.\n\nThe next step is to ingest the Weaviate documentation and parse the documents into chunks. You can choose to use one of our many web page readers to scrape any website yourself - but luckily, the downloaded files are already readily available in the recipes repo.\n\n```python\nfrom llama_index import SimpleDirectoryReader\nfrom llama_index.node_parser import SimpleNodeParser\n\n# load the blogs in using the reader\nblogs = SimpleDirectoryReader('./data').load_data()\n\n# chunk up the blog posts into nodes\nparser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=20)\nnodes = parser.get_nodes_from_documents(blogs)\n```\n\nHere, we use the SimpleDirectoryReader to load in all documents from a given directory. We then use our `SimpleNodeParser` to chunk up the source documents into Node objects (text chunks).\n\nThe next step is to 1) define a `WeaviateVectorStore`, and 2) build a vector index over this vector store using LlamaIndex.\n\n```python\nfrom llama_index.vector_stores import WeaviateVectorStore\nfrom llama_index import VectorStoreIndex, StorageContext\n\n# construct vector store\nvector_store = WeaviateVectorStore(weaviate_client = client, index_name=\"BlogPost\", text_key=\"content\")\n\n# setting up the storage for the embeddings\nstorage_context = StorageContext.from_defaults(vector_store = vector_store)\n\n# set up the index\nindex = VectorStoreIndex(nodes, storage_context = storage_context)\n```\n\nOur WeaviateVectorStore abstraction creates a central interface between our data abstractions and the Weaviate service. Note that the `VectorStoreIndex` is initialized from both the nodes and the storage context object containing the Weaviate vector store. During the initialization phase, the nodes are loaded into the vector store.\n\nFinally, we can define a query engine on top of our index. This query engine will perform semantic search and response synthesis, and output an answer.\n\n```python\n\n\u200b\u200bquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What is the intersection between LLMs and search?\")\nprint(response)\n```\n\nYou should get an answer like the following:\n\n```\nThe intersection between LLMs and search is the ability to use LLMs to improve search capabilities, such as retrieval-augmented generation, query understanding, index construction, LLMs in re-ranking, and search result compression. LLMs can also be used to manage document updates, rank search results, and compress search results. LLMs can be used to prompt the language model to extract or formulate a question based on the prompt and then send that question to the search engine, or to prompt the model with a description of the search engine tool and how to use it with a special `[SEARCH]` token. LLMs can also be used to prompt the language model to rank search results according to their relevance with the query, and to classify the most likely answer span given a question and text passage as input.\n```\n\n## Next Up in this Series\n\nThis blog post shared an initial overview of the LlamaIndex and Weaviate integration. We covered an introduction to the toolkits offered in LlamaIndex and a notebook on how to build a simple QA engine over Weaviate\u2019s blog posts. Now that we have a baseline understanding, we will build on this by sharing more advanced guides soon. Stay tuned!\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n\n", "type": "Blog", "name": "blog-llamaindex-and-weaviate", "path": "blog/2023-06-22-llamaindex-and-weaviate/index.mdx", "link": "https://weaviate.io/blog/llamaindex-and-weaviate", "timestamp": "2023-11-13 10:42:24", "reader": "JSON", "meta": {}, "chunks": []}