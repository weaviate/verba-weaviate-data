{"text": "\nWeaviate is designed to be easy to monitor and observe by following a cloud native approach. To do this Weaviate supports the following features\n\n1. Publishing of Prometheus metrics to the standard `/metrics` endpoint\n\n2. Use of built-in Kubernetes liveness and readiness checks\n\n3. Configuration of settings via environment variables\n\n4. Simplified deployment via helm charts\n\nThere is existing documentation on the exported metrics which also has an example for how to use a Prometheus instance for metrics.\n\nOne common question though is: How can I integrate Weaviate with my existing observability stack?\n\nThis article describes two approaches using either Grafana agent or Datadog agent to scrape these metrics. It also provides a list of important metrics to monitor.\n\n## Prerequisites\n\nIt is assumed that you have already deployed Weaviate. By default Prometheus monitoring is disabled, so you can enable it with this environment setting:\n\n```sh\nPROMETHEUS_MONITORING_ENABLED=true\n```\n\nWeaviate will then publish Prometheus metrics on port `2112`.\n\nIf you are using Weaviate `1.17` or lower, you may want to upgrade to `1.18` before enabling Prometheus metrics. The reason being Weaviate previously published many histograms which has since been replaced by summaries for performance reasons. Additionally, be careful enabling Prometheus metrics if you have many thousands of classes as you may end up with high cardinality labels due to some metrics being produced per class.\n\n## Grafana Agent\nmonitoring weaviate in production\nmonitoring weaviate in production\n\nFor the first approach we will use the open-source Grafana agent. In this case, we will show writing to Grafana Cloud for hosted metrics. This is configurable via the remote write section if you alternatively want to write to a self-hosted Mimir or Prometheus instance.\n\n\n### Steps to Install\n\n1\\. Install Grafana agent in your target environment following the set-up guide.\n\n2\\. Configure the Grafana `agent.yaml` to include a scrape job called `weaviate`. This will autodiscover Weaviate pods\n in Kubernetes. The `app=weaviate` label is automatically added by the Weaviate helm chart which makes autodiscovery easy.\n\n```yaml\nmetrics:\n    configs:\n    - name: weaviate\n    # reference https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config\n    scrape_configs:\n    - job_name: weaviate\n        scrape_interval: 30s\n        scheme: http\n        metrics_path: /metrics\n        kubernetes_sd_configs:\n        - role: pod\n        selectors:\n        - role: \"pod\"\n            label: \"app=weaviate\"\n    remote_write:\n    - url: \n        basic_auth:\n        username: \n        password: \n```\n\n3\\. Validate that you are receiving data by going to explore and running the following PromQL query in Grafana.\n\n```\ngo_memstats_heap_inuse_bytes{job=\"weaviate\"}\n```\n### Dashboards\n\nOne benefit of this approach is that you can now reuse the existing Weaviate Grafana dashboards.\n\nSteps to import these dashboards:\n\n1\\. Download and import the preexisting dashboards.\n\n2\\. If you're using Grafana Cloud hosted Prometheus you will need to patch the dashboards to change the datasource uid to be `grafanacloud-prom` as below.\n\n```sh\nsed 's/\"uid\": \"Prometheus\"/\"uid\": \"grafanacloud-prom\"/g' querying.json > querying-patched.json\n```\n\nThe dashboards should now be visible!\n\nquery latency\n\n## Datadog\n\nDatadog is another popular solution for observability, and the Datadog agent has support for scraping Prometheus metrics.\n\n### Steps to Install\n\n1\\. Install the datadog agent. For this example, installation was done using their Helm charts.\n\n2\\. Provide a `datadog-values.yml` config including the below. You can also capture Weaviate logs using the method.\n\n```yaml\ndatadog:\n# Note DD_KUBELET_TLS_VERIFY only needs to be set if running a local docker kubernetes cluster\n#  env:\n#  - name: DD_KUBELET_TLS_VERIFY\n#    value: \"false\"\n  clusterName: weaviate-deployment\n  prometheusScrape:\n    enabled: true\n    serviceEndpoints: true\n    additionalConfigs:\n      - configurations:\n        - max_returned_metrics: 20000\n          min_collection_interval: 30\n```\n\n3\\. Customize the Weaviate helm chart to have annotations `prometheus.io/scrape` and `prometheus.io/port`\n\n```yaml\n# Pass any annotations to Weaviate pods\nannotations:\n  prometheus.io/scrape: \"true\"\n  prometheus.io/port: \"2112\"\n```\n\n4\\. Validate metrics are available. `go_memstats_heap_inuse_bytes` should always be present even with an empty schema.\n\ndatadog summary\n\n\n## Key Metrics\n\nBelow are some key Weaviate metrics to monitor. Standard CPU, Disk, Network metrics are also useful as are [Kubernetes\nevents](https://grafana.com/blog/2023/01/23/how-to-use-kubernetes-events-for-effective-alerting-and-monitoring/).\nNote that some Weaviate metrics will not appear until an operation has occurred (for instance batch operations).\n\n### Heap Usage\n\nFor heap usage, the expectation is the memory will have a standard jagged pattern underload but that memory will drop periodically\ndue to the Go garbage collection. If memory is not dropping and is staying very close to the GOMEMLIMIT, you may need to increase resources.\n\n```\ngo_memstats_heap_inuse_bytes\n```\n\n### Batch Latency\n\nBatch latency is important as batch operations are the most efficient way to write data to\nWeaviate. Monitoring this can give an indication if there is a problem with indexing data. This metric has a label `operation` which\nallows you to see how long objects, vectors, and inverted index sub operations take. If you are using a vectorizer module you will see additional latency due to the overhead of sending data to the module.\n\n```\nrate(batch_durations_ms_sum[30s])/rate(batch_durations_ms_count[30s])\n```\n\nFor batch deletes the corresponding `batch_delete_durations_ms` metric will also be useful.\n\n### Object Latency\n\nGenerally, batch indexing is recommended but there are situations where you would do single `PUT` or `DELETE` operations\nsuch as handling live changes from a user in an application. In this case you will want to monitor the object latency\ninstead.\n\n```\nrate(objects_durations_ms_sum{operation=\"put\"}[30s])/rate(objects_durations_ms_latency{operation=\"put\"}[30s])\n```\n\n### Query Latency and Rate\n\nThe latency and number of queries per second are also important, particularly for monitoring usage patterns.\n\n```\nrate(queries_durations_ms_sum[30s])/rate(queries_durations_ms_count[30s])\nrate(queries_durations_ms_count)[30s]\n```\n\n## Other Integrations\n\nMany other solutions that have integrations for Prometheus that can also be used:\n\n* Elastic\n* Splunk\n* New Relic\n\n\nimport StayConnected from '/_includes/stay-connected.mdx'\n\n\n", "type": "Blog", "name": "blog-monitoring-weaviate-in-production", "path": "blog/2023-03-28-monitoring-weaviate-in-production/index.mdx", "link": "https://weaviate.io/blog/monitoring-weaviate-in-production", "timestamp": "2024-02-08 20:24:19", "reader": "JSON", "meta": {}, "chunks": []}