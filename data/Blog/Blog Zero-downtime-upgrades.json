{"text": "\n![Image of Weaviate robots pointing at each other](./img/hero.png)\n\n## The tyranny of database downtime\n\nLike the old saying goes, a chain is only as strong as its weakest link. For tech infrastructure products, the weak link can often be its uptime. Think about how big a deal it is when social networks, web apps or databases are not available.\n\nThis is why we at Weaviate really pride ourselves on having a robust, production-ready database that can scale as our users do. For example, many of our users already run Weaviate with multi-tenancy (introduced in version `1.20`) to host thousands of active tenants or even more.\n\nOne side effect of scaling is that as load increases on each node, it will take longer to start up. While a fresh Weaviate instance typically starts up essentially instantaneously, a node with 1000s of tenants can take up over 1 minute.\n\nNode-level downtime is an unavoidable fact of life, since either hardware and software may necessitate restarts for maintenance and/or updates. But node-level downtime doesn\u2019t have to lead to user-level downtime and failed requests.\n\nHow, you ask? Having redundancy is key to achieving this. And such redundancy, or replication, has been available for a while in Weaviate for production use.\n\nIn fact, configuring replication is actually easy and simple, and using it can lead to huge benefits. In this post, we will explain the benefits of replication and show you how to configure it.\n\n- **Node**: A single machine in a cluster. It often refers to a physical or virtual machine that runs part of an application or service.\n- **Pod**: A Kubernetes term for a group of one or more containers, with shared storage/network, and a specification for how to run the containers. Pods are the smallest deployable units in Kubernetes.\n- **Tenant**: In the context of Weaviate, an isolated environment or subsection within the system, designed to separate data and access between different end users or groups.\n\n## Enabling replication in Weaviate\n\nJust the simple act of enabling replication on a Weaviate cluster will provide huge benefits. Doing so might actually be simpler than you might imagine.\n\n### How to enable replication on Weaviate Cloud Services (WCS)\n\nEnabling replication on a Weaviate Cloud Services cluster is as simple as selecting the `Enable High Availability` button at cluster creation time. (Not available on sandboxes.)\n\n\nThis will enable a multi-node configuration in Weaviate and ensures that each class is configured with the appropriate replication factor.\n\n### How to enable replication on self-deployed Weaviate\n\nEnabling replication in a self-deployment setup such as a Docker or Kubernetes setup involves the following two steps. First, modify the configuration file to enable a multi-node setup (e.g. 3), and add the `replicationConfig` setting to the collection definition like so:\n\n```json\n{\n  class: YOUR_CLASS_NAME,\n  ...\n  replicationConfig: {\n    factor: 3\n  }\n}\n```\n\nThe replication factor should be less than or equal to the number of nodes.\n\nOnce you\u2019ve modified the configuration file and set the replication factor, you should have a multi-node setup. If you are keen to try running a multi-node setup yourself, follow the optional guide here. Or you can read ahead ;).\n\n\n  Optional: Try it yourself (with minikube)\n\nYou can try running a local, multi-node Weaviate cluster with `minikube`, which can conveniently run a local Kubernetes cluster. We note that deploying Weaviate on a cloud provider\u2019s kubernetes service follows a similar process.\n\n\n\nFirst, install `minikube` and `helm` for your system by following these guides ([minikube](https://minikube.sigs.k8s.io/docs/start), [helm](https://helm.sh/docs/intro/install)). We also recommend installing `kubectl` ([by following this guide](https://kubernetes.io/docs/tasks/tools/#kubectl)).\n\n\n\nOnce minikube is installed, start a three-node minikube cluster by running the following from the shell:\n\n```shell\nminikube start --nodes 3\n```\n\nOnce the nodes have been created, you should be able to interact with them through the `kubectl` command-line tool. To see a list of the newly spun up nodes, run:\n\n```shell\nkubectl get nodes -o wide\n```\n\nYou should see an output similar to the following, indicating that three nodes are up and onto which you can deploy Weaviate:\n\n```shell\nNAME           STATUS   ROLES           AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\nminikube       Ready    control-plane   134m   v1.27.3   192.168.49.2           Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\nminikube-m02   Ready              134m   v1.27.3   192.168.49.3           Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\nminikube-m03   Ready              133m   v1.27.3   192.168.49.4           Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\n```\n\nNow, add the Weaviate helm repository to your local helm configuration by running:\n\n```shell\nhelm repo add weaviate https://weaviate.github.io/weaviate-helm\n```\n\nAnd save the default configuration with:\n\n```shell\nhelm show values weaviate/weaviate > values.yaml\n```\n\nEdit `values.yaml` by changing the root-level configuration `replicas: 1` for the root image to `replicas: 3`, and save it.\n\n```yaml\n...\n# Scale replicas of Weaviate. Note that as of v1.8.0 dynamic scaling is limited\n# to cases where no data is imported yet. Scaling down after importing data may\n# break usability. Full dynamic scalability will be added in a future release.\n# highlight-start\nreplicas: 3\n# highlight-end\n...\n```\n\nYou can now deploy Weaviate in this configuration by running:\n\n```shell\nkubectl create namespace weaviate\n\nhelm upgrade --install \\\n  \"weaviate\" \\\n  weaviate/weaviate \\\n  --namespace \"weaviate\" \\\n  --values ./values.yaml\n```\n\nThis will deploy the Weaviate clusters. You can check the status of the deployment by running:\n\n```shell\nkubectl get pods -n weaviate\n```\n\nThis should produce an output similar to the following:\n\n```shell\nNAME         READY   STATUS    RESTARTS   AGE\nweaviate-0   1/1     Running   0          3m00s\nweaviate-1   1/1     Running   0          2m50s\nweaviate-2   1/1     Running   0          2m40s\n```\n\nNow, you need to expose the Weaviate service to the outside world - i.e. to your local machine. You can do this by running:\n\n```shell\nminikube service weaviate --namespace weaviate\n```\n\nThis should show an output similar to the following that shows the URL to access the Weaviate cluster:\n\n```shell\n|-----------|----------|-------------|------------------------|\n| NAMESPACE |   NAME   | TARGET PORT |          URL           |\n|-----------|----------|-------------|------------------------|\n| weaviate  | weaviate |             | http://127.0.0.1:54847 |\n|-----------|----------|-------------|------------------------|\n```\n\nAnd it should also open a browser window showing the list of Weaviate endpoints.\n\n\n\nJust like that, you\u2019ve got a multi-node Weaviate cluster. Remember that when you create a class, you must have replication enabled. You can do this by adding the `replicationConfig` parameter to the collection definition, like so:\n\n```json\n{\n  \"class\": \"ClassWithReplication\",\n  \"properties\": [\n    {\n      \"name\": \"exampleProperty\",\n      \"dataType\": [\"text\"]\n    }\n  ],\n  // highlight-start\n  \"replicationConfig\": {\n    \"factor\": 3\n  }\n  // highlight-end\n}\n```\n\nAnd when you insert objects into `ClassWithReplication`, they will be replicated across the three nodes. You can verify this by visiting the `/nodes` endpoint, which will verify that each node contains the same number of objects.\n\n\n\n## Benefits of replication\n\nSo, let\u2019s cover the implications of doing this. What does replication get us? A big one is *availability*. With no replication, any node being down will make its data unavailable.\n\nBut in a Kubernetes setup composed of say, three Weaviate nodes (three Kubernetes \u201cpods\u201d) and a replication factor of three, you can have any one of the three nodes down and still reach consensus.\n\nThis reflects Weaviate\u2019s leaderless replication architecture, meaning any node can be down without affecting availability at a cluster level as long as the right data is available somewhere.\n\nYou can see how replication significantly improves availability.\n\nWeaviate provides further configurability and nuance for you in this area by way of a consistency guarantee setting. For example, a request made with a consistency level of QUORUM would require over half of the nodes which contain the data to be up, while a request with ONE consistency would only require one node to be up.\n\n- The replication algorithm makes sure that no node holds a tenant twice. Replication is always spread out across nodes.\n\n## Implications for database maintenance\n\nIn production, this can dramatically reduce the critical downtime. Let\u2019s take an example three-pod Kubernetes setup with 10,000 tenants, and see how replication affects availability during a rolling update of Weaviate versions.\n\nEach Weaviate pod will restart one by one, as demonstrated in the example below (from `kubectl get pods`), which shows `weaviate-2` as having been recently restarted and ready, while `weaviate-1` is just restarting.\n\n![Node statuses showing restarts](./img/node_statuses.png)\n\nWhat will the cluster availability be like during this period? We performed an experiment simulating non-trivial load with ~3,800 queries per second to approximate a real-life scenario. Here are the results:\n\n### Queries during maintenance - without replication\n\nThe below figure shows results from our setup with no replication. The area chart at the top shows how many requests failed, and the line graph shows pod readiness.\n\nWe see immediately that over the course of the update time, just about one out of nine (11.5%) requests failed.\n\n![Monitoring stats showing failures during restart](./img/queries_without_replication.png)\n\nAnd at times, it is even worse, with the failure rate being as high as one in three when the node is down and before it starts to load tenants.\n\nThe failure rate here is less than one in three, as Weaviate is capable of loading tech tenant\u2019s data (i.e. shard) and making them available. This makes the restarting node \u2018partly\u2019 available.\n\n### Queries during maintenance - with replication\n\nOn the other hand, this figure shows results from a scenario with replication configured with a factor of 3.\n\n![Monitoring stats showing no failures during restart](./img/queries_with_replication.png)\n\nWe see that a grand total of zero queries failed here over the course of 8-9 minutes, even though individual pods did go down as they did before. In other words, the end users wouldn\u2019t even have noticed that a new version was rolled out, as node-level downtime did not lead to system-level downtime.\n\nDid we mention that the only change between the two was setting the replication factor? It\u2019s just that easy.\n\nBefore you rush off to switch on replication, though, stick with us to read about the trade-offs and our recommendations. \ud83d\ude09\n\n## Trade-offs & discussions\n\nWhile replication and high availability are wonderful, we won\u2019t quite pretend that it comes for free. Having additional replicas of course means that there are more tenants and objects overall. Although they are duplicated, they are just as *real* as objects as any others.\n\nThis means that a replication factor of 3 leads to the cluster essentially handling three times the load as no replication. The benefit here is of course redundancy and thus availability, but the cost is additional cost, such as increased hardware (e.g. memory) requirements. Note that the rolling update time was longer on our replication example than the non-replication example, as each pod now holds ~10,000 tenants rather than ~3,333 tenants.\n\nAnother challenge is that any write request that comes in while any nodes are down will be temporarily missing on that node for a while. This will be repaired in time through a read-repair that [happens automatically in the background](/developers/weaviate/concepts/replication-architecture/consistency#repairs).\n\nTo reduce the length of this time, there is a proposed feature to proactively start repairing those inconsistencies (i.e. perform asynchronous replication). If this is important, please [upvote the feature here](https://github.com/weaviate/weaviate/issues/2405).\n\nBut we think that the cost of high availability is worth these prices. We take system availability seriously, and architect Weaviate according to this philosophy. This is one of the reasons that we use [leaderless replication](/developers/weaviate/concepts/replication-architecture/cluster-architecture#leaderless-design), and why replication in the first place is so important to us - because it enables our users to have robust systems on which they can rely.\n\nTake the longer start-up time for example. Adding replication caused node-level start-up time to increase in our experiment. But the end result was that a hundred percent of requests succeeded. In other words, the end user would not have noticed anything was going on. And again, what are peace of mind and avoiding the wrath of angry users during downtime worth to you??\n\n## Recommendations & Wrap-up\n\nAs we mentioned before, all you need to configure to enable replication is this in the collection definition:\n\n```json\n{\n  class: 'YOUR_CLASS_NAME',\n  ...\n  replicationConfig: {\n    factor: 3,\n  }\n}\n```\n\nBut what replication factor would we recommend? That\u2019s something of a subjective question, but our starting recommendation is 3.\n\nThe reason is that odd numbers are preferred for consistency so that consensus can always be reached. Higher factors are also possible, but this is more of a measure to scale query throughput, rather than lead to more availability. And for more cost-sensitive applications, even 2 would introduce high availability and robustness to the system.\n\nAs we've mentioned before - using Weaviate Cloud Services is a convenient way to set up a cluster with replication enabled. Set the `Enable High Availability` button to `Yes` at cluster creation time, and it will spin up a multi-node cluster with replication enabled, including the appropriate replication factor for each class.\n\nSo there it is. We hope we\u2019ve convinced you of the benefits of using replication, and how easy it is to use. For sure it might not be for everybody and every use case. But if you are using Weaviate at scale, in production, we believe enabling it will add significant value and encourage you to consider its use.\n\n\n", "type": "Blog", "name": "Blog Zero-downtime-upgrades", "path": "blog/2023-11-30-zero-downtime-upgrades/index.mdx", "link": "https://weaviate.io/blog/zero-downtime-upgrades", "timestamp": "2024-05-08 10:51:52", "reader": "JSON", "meta": {}, "chunks": []}