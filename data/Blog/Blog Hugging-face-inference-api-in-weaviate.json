{"text": "\nVector databases use Machine Learning models to offer incredible functionality to operate on your data. We are looking at anything from **summarizers** (that can summarize any text into a short) sentence), through **auto-labelers** (that can classify your data tokens), to **transformers** and **vectorizers** (that can convert any data \u2013 text, image, audio, etc. \u2013 into vectors and use that for context-based queries) and many more use cases.\n\nAll of these use cases require `Machine Learning model inference` \u2013 a process of running data through an ML model and calculating an output (e.g. take a paragraph, and summarize into to a short sentence) \u2013 which is a compute-heavy process.\n\n### The elephant in the room\nRunning model inference in production is hard.\n* It requires expensive specialized hardware.\n* You need a lot more computing power during the initial data import.\n* Hardware tends to be underutilized once the bulk of the heavy work is done.\n* Sharing and prioritizing resources with other teams is hard.\n\nThe good news is, there are companies \u2013 like Hugging Face, OpenAI, and Cohere \u2013 that offer running model inference as a service.\n\n> \"Running model inference in production is hard,\nlet them do it for you.\"\n\n## Support for Hugging Face Inference API in Weaviate\nStarting from Weaviate `v1.15`, Weaviate includes a Hugging Face module, which provides support for Hugging Face Inference straight from the vector database.\n\nThe Hugging Face module, allows you to use the [Hugging Face Inference service](https://huggingface.co/inference-api#pricing) with sentence similarity models, to vectorize and query your data, straight from Weaviate. No need to run the Inference API yourself.\n\n> You can choose between `text2vec-huggingface` (Hugging Face) and `text2vec-openai` (OpenAI) modules to delegate your model inference tasks.\n> Both modules are enabled by default in the [Weaviate Cloud Services](/pricing).\n\n## Overview\n![Overview](./img/hugging-face-module-overview.png)\n\nThe Hugging Face module is quite incredible, for many reasons.\n\n### Public models\nYou get access to over 1600 pre-trained [sentence similarity models](https://huggingface.co/models?pipeline_tag=sentence-similarity). No need to train your own models, if there is already one that works well for your use case.\n\nIn case you struggle with picking the right model, see our blog post on [choosing a sentence transformer from Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face).\n\n### Private models\nIf you have your own models, trained specially for your data, then you can upload them to Hugging Face (as private modules), and use them in Weaviate.\n\n\n*We are working on an article that will guide you on how to create your own model and upload it to Hugging Face.*\n\n### Fully automated and optimized\nWeaviate manages the whole process for you. From the perspective of writing your code \u2013 once you have your schema configuration \u2013 you can almost forget that Hugging Face is involved at all.\n\nFor example, when you import data into Weaviate, Weaviate will automatically extract the relevant text fields, send them Hugging Face to vectorize, and store the data with the new vectors in the database.\n\n### Ready to use with a minimum of fuss\nEvery new Weaviate instance created with the [Weaviate Cloud Services](/pricing) has the Hugging Face module enabled out of the box. You don't need to update any configs or anything, it is there ready and waiting.\n\nOn the other hand, to use the Hugging Face module in Weaviate open source (`v1.15` or newer), you only need to set `text2vec-huggingface` as the default vectorizer. Like this:\n\n```yaml\nDEFAULT_VECTORIZER_MODULE: text2vec-huggingface\nENABLE_MODULES: text2vec-huggingface\n```\n\n## How to get started\n\nThis article is not meant as a hands-on tutorial.\nFor more detailed instructions please check the [documentation](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface).\n\nThe overall process to use a Hugging Face module with Weaviate is fairly straightforward.\n\n![Recipe for using the Hugging Face module](./img/how-to-get-started-recipe.png)\nIf this was a cooking class and you were following a recipe.\n\nYou would need the following ingredients:\n* Raw Data\n* Hugging Face API token \u2013 which you can request from [their website](https://huggingface.co/settings/tokens)\n* A working Weaviate instance with the `text2vec-huggingface` enabled\n\nThen you would follow these steps.\n\n### Step 1 \u2013 initial preparation \u2013 create schema and select the hf models\nOnce you have a Weaviate instance up and running.\nDefine your schema (standard stuff \u2013 pick a class name, select properties, and data types). As a part of the schema definition, you also need to provide, which Hugging Face model you want to use for each schema class.\n\n\nThis is done by adding a `moduleConfig` property with the `model` name, to the schema definition, like this:\n```javascript\n{\n    \"class\": \"Notes\",\n    \"moduleConfig\": {\n        \"text2vec-huggingface\": {\n            \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # model name\n            ...\n        }\n    },\n    \"vectorizer\": \"text2vec-huggingface\",  # vectorizer for hugging face\n   ...\n}\n```\n\n*If you are wondering, yes, you can use a different model for each class.*\n\n### Step 2 \u2013 cook for some time \u2013 import data\nStart importing data into Weaviate.\n\nFor this, you need your Hugging Face API token, which is used to authorize all calls with \ud83e\udd17.\n\nAdd your token, to a Weaviate client configuration. For example in Python, you do it like this:\n\n```javascript\nclient = weaviate.Client(\n    url='http://localhost:8080',\n    additional_headers={\n        'X-HuggingFace-Api-Key': 'YOUR-HUGGINGFACE-API-KEY'\n    }\n)\n```\nThen import the data the same way as always. And Weaviate will handle all the communication with Hugging Face.\n\n### Step 3 \u2013 serving portions \u2013 querying data\nOnce, you imported some or all of the data, you can start running queries.\n(yes, you can start querying your database even during the import).\n\nRunning queries also requires the same token.\nBut you can reuse the same client, so you are good to go.\n\nThen, you just run the queries, as per usual:\n```javascript\nnearText = {\n    \"concepts\": [\"How to use Hugging Face modules with Weaviate?\"],\n    \"distance\": 0.6,\n}\n\nresult = (\n    client.query\n    .get(\"Notes\", [\n        \"name\",\n        \"comment\",\n        \"_additional {certainty distance} \"])\n    .with_near_text(nearText)\n    .do()\n)\n```\n\n## Summary\n> Now you can use [Hugging Face](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) or [OpenAI](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) modules in Weaviate to delegate model inference out.\n\nJust pick the model, provide your API key and start working with your data.\n\nWeaviate optimizes the communication process with the Inference API for you, so that you can focus on the challenges and requirements of your applications. No need to run the Inference API yourself.\n\n## What next\nCheck out the [text2vec-huggingface](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) documentation to learn more about the new module.\n\n\n", "type": "Blog", "name": "Blog Hugging-face-inference-api-in-weaviate", "path": "blog/2022-09-27-hugging-face-inference-api-in-weaviate/index.mdx", "link": "https://weaviate.io/blog/hugging-face-inference-api-in-weaviate", "timestamp": "2024-05-08 10:51:18", "reader": "JSON", "meta": {}, "chunks": []}