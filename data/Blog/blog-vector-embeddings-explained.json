{"text": "\nSemantic searches (as well as question answering) are essentially searches by similarity, such as by the meaning of text, or by what objects are contained in images. For example, consider a library of wine names and descriptions, one of which mentioning that the wine is \u201cgood with **fish**\u201d. A \u201cwine for **seafood**\u201d keyword search, or even a synonym search, won\u2019t find that wine. A meaning-based search should understand that \u201cfish\u201d is similar to \u201cseafood\u201d, and \u201cgood with X\u201d means the wine is \u201cfor X\u201d\u2014and should find the wine.\n\nvector embeddings example\n\nHow can computers mimic our understanding of language, and similarities of words or paragraphs? To tackle this problem, semantic search uses at its core a data structure called **vector embedding** (or simply, **vector** or **embedding**), which is an array of numbers. Here's how the semantic search above works, step by step:\n\n1. The vector database computes a vector embedding for each data object as it is inserted or updated into the database, using a given model.\n2. The embeddings are placed into an index, so that the database can quickly perform searches.\n3. For each query,\n    1. a vector embedding is computed using the same model that was used for the data objects.\n    2. using a special algorithm, the database find the closest vectors to the given vector computed for the query.\n\nThe quality of the search depends crucially on the quality of the model - this is the \"secret sauce\", as many models are still closed source. The speed of the search depends crucially on Weaviate, which is open-source and continuously improving its performance.\n\n\n## What exactly are vector embeddings?\nVectors or vector embeddings are numeric representations of data that capture certain features of the data. For example, in the case of text data, \u201ccat\u201d and \u201ckitty\u201d have similar meaning, even though the _words_ \u201ccat\u201d and \u201ckitty\u201d are very different if compared letter by letter. For semantic search to work effectively, representations of \u201ccat\u201d and \u201ckitty\u201d must sufficiently capture their semantic similarity. This is where vector representations are used, and why their derivation is so important.\n\nIn practice, vector embeddings are arrays of real numbers, of a fixed length (typically from hundreds to thousands of elements), generated by machine learning models. The process of generating a vector for a data object is called vectorization. Weaviate generates vector embeddings using modules (OpenAI, Cohere, Google PaLM etc.), and conveniently stores both objects and vector embeddings in the same database. For example, vectorizing the two words above might result in the following word embeddings:\n\n```text\ncat = [1.5, -0.4, 7.2, 19.6, 3.1, ..., 20.2]\nkitty = [1.5, -0.4, 7.2, 19.5, 3.2, ..., 20.8]\n```\n\nThese two vectors have a very high similarity. In contrast, vectors for \u201cbanjo\u201d or \u201ccomedy\u201d would not be very similar to either of these vectors. To this extent, vectors capture the semantic similarity of words.\n\nNow that you\u2019ve seen what vectors are, and that they can represent meaning to some extent, you might have further questions. For one, what does each number represent? That depends on the machine learning model that generated the vectors, and isn\u2019t necessarily clear, at least in terms of our human conception of language and meaning. But we can sometimes gain a rough idea by correlating vectors to words with which we are familiar.\n\nVector-based representation of meaning caused quite a stir a few years back, with the revelation of mathematical operations between words. Perhaps *the* most famous result was that of\n\n    \u201cking \u2212 man + woman \u2248 queen\u201d\n\nIt indicated that the difference between \u201cking\u201d and \u201cman\u201d was some sort of \u201croyalty\u201d, which was analogously and mathematically applicable to \u201cqueen\u201d minus \u201cwoman\u201d. Jay Alamar provided a helpful visualization around this equation. Several concepts (\u201cwoman\u201d, \u201cgirl\u201d, \u201cboy\u201d etc.) are vectorized into (represented by) an array of 50 numbers generated using the GloVe model. In vector terminology), the 50 numbers are called dimensions. The vectors are visualized using colors and arranged next to each word:\n\nvector embeddings visualization\n*Credit: Jay Alamar*\n\nWe can see that all words share a dark blue column in one of the dimensions (though we can\u2019t quite tell what that represents), and the word \u201cwater\u201d _looks_ quite different from the rest, which makes sense given that the rest are people. Also, \u201cgirl\u201d and \u201cboy\u201d look more similar to each other than to \u201cking\u201d and \u201cqueen\u201d respectively, while \u201cking\u201d and \u201cqueen\u201d look similar to each other as well.\n\nSo we can *see* that these vector embeddings of words align with our intuitive understanding of meaning. And even more amazingly, vector embeddings are not limited to representing meanings of words.\n\nIn fact, effective vector embeddings can be generated from any kind of data object. Text data is the most common, followed by images, then audio data (this is how Shazam recognizes songs based on a short and even noisy audio clip), but also time series data, 3D models, video, molecules etc. Embeddings are generated such that two objects with similar semantics will have vectors that are \"close\" to each other, i.e. that have a \"small\" distance between them in vector space. That distance can be calculated in multiple ways, one of the simplest being \"The sum of the absolute differences between elements at position `i` in each vector\" (recall that all vectors have the same fixed length).\n\nLet's look at some numbers (no math, promise!) and illustrate with another text example:\n\nObjects (data): words including `cat`, `dog`, `apple`, `strawberry`, `building`, `car`\n\nSearch query: `fruit`\n\nA set of simplistic vector embeddings (with only 5 dimensions) for the objects and the query could look something like this:\n\n| Word               | Vector embedding                |\n|--------------------|---------------------------------|\n| `cat`              | `[1.5, -0.4, 7.2, 19.6, 20.2]`  |\n| `dog`              | `[1.7, -0.3, 6.9, 19.1, 21.1]`  |\n| `apple`            | `[-5.2, 3.1, 0.2, 8.1, 3.5]`    |\n| `strawberry`       | `[-4.9, 3.6, 0.9, 7.8, 3.6]`    |\n| `building`         | `[60.1, -60.3, 10, -12.3, 9.2]` |\n| `car`              | `[81.6, -72.1, 16, -20.2, 102]` |\n| **Query: `fruit`** | `[-5.1, 2.9, 0.8, 7.9, 3.1]`    |\n\nIf we look at each of the 5 elements of the vectors, we can see quickly that `cat` and `dog` are much closer than `dog` and `apple` (we don\u2019t even need to calculate the distances). In the same way, `fruit` is much closer to `apple` and `strawberry` than to the other words, so those will be the top results of the \u201cfruit\u201d query.\n\nBut where do these numbers come from? That\u2019s where the real magic is, and where advances in modern deep learning have made a huge impact.\n\n## How are vector embeddings generated?\nThe magic of vector search resides primarily in how the embeddings are generated for each entity and the query, and secondarily in how to efficiently search within very large datasets (see our \u201cWhy is Vector Search so Fast\u201d article for the latter).\n\nAs we mentioned, vector embeddings can be generated for various media types such as text, images, audio and others. For text, vectorization techniques have evolved tremendously over the last decade, from the venerable word2vec (2013), to the state-of-the-art transformer models era, spurred by the release of BERT) in 2018.\n\n### Word-level dense vector models (word2vec, GloVe, etc.)\nword2vec is a family of model architectures that introduced the idea of \u201cdense\u201d vectors in language processing, in which all values are non-zero.\n\nWord2vec in particular uses a neural network model to learn word associations from a large corpus of text (it was initially trained by Google with 100 billion words). It first creates a vocabulary from the corpus, then learns vector representations for the words, typically with 300 dimensions. Words found in similar contexts have vector representations that are close in vector space, but each word from the vocabulary has only one resulting word vector. Thus, the meaning of words can be quantified - \u201crun\u201d and \u201cran\u201d are recognized as being far more similar than \u201crun\u201d and \u201ccoffee\u201d, but words like \u201crun\u201d with multiple meanings have only one vector representation.\n\nAs the name suggests, word2vec is a word-level model and cannot by itself produce a vector to represent longer text such as sentences, paragraphs or documents. However, this can be done by aggregating vectors of constituent words, which is often done by incorporating weightings such that certain words are weighted more heavily than others.\n\nHowever, word2vec still suffers from important limitations:\n* it doesn\u2019t address words with multiple meanings (polysemantic): \u201crun\u201d, \u201cset\u201d, \u201cgo\u201d, or \u201ctake\u201d each have over 300 meanings (!)\n* it doesn\u2019t address words with ambiguous meanings: \u201cto consult\u201d can be its own antonym, like many other words\n\nWhich takes us to the next, state-of-the-art, models.\n\n### Transformer models (BERT, ELMo, and others)\nThe current state-of-the-art models are based on what\u2019s called a \u201ctransformer\u201d architecture as introduced in this paper.\n\nTransformer models) such as BERT and its successors improve search accuracy, precision and recall by looking at every word\u2019s context to create full contextual embeddings (though the exact mechanism of BERT\u2019s success is not fully understood). Unlike word2vec embeddings which are context-agnostic, transformer-generated embeddings take the entire input text into account\u2014each occurrence of a word has its own embedding that is modified by the surrounding text. These embeddings better reflect the polysemantic nature of words, which can only be disambiguated when they are considered in context.\n\nSome of the potential downsides include:\n* increased compute requirements: fine-tuning transformer models is much slower (on the order of hours vs. minutes)\n* increased memory requirements: context-sensitivity greatly increases memory requirements, which often leads to limited possible input lengths\n\nDespite these downsides, transformer models have been wildly successful. Countless text vectorizer models have proliferated over the recent past. Plus, many more vectorizer models exist for other data types such as audio, video and images, to name a few. Some models, such as CLIP, are capable of vectorizing multiple data types (images and text in this case) into one vector space, so that an image can be searched by its content using only text.\n\n## Storing and using vector embeddings with a Weaviate vector database\n\nFor this reason, Weaviate is configured to support many different vectorizer models and vectorizer service providers. You can even bring your own vectors, for example if you already have a vectorization pipeline available, or if none of the publicly available models are suitable for you.\n\nFor one, Weaviate supports using any Hugging Face models through our `text2vec-hugginface` module, so that you can choose one of the many sentence transformers published on Hugging Face. Or, you can use other very popular vectorization APIs such as OpenAI or Cohere through the `text2vec-openai` or `text2vec-cohere` modules. You can even run transformer models locally with `text2vec-transformers`, and modules such as `multi2vec-clip` can convert images and text to vectors using a CLIP model.\n\nBut they all perform the same core task\u2014which is to represent the \u201cmeaning\u201d of the original data as a set of numbers. And that\u2019s why semantic search works so well.\n\n## Summary\n\nThis blog explained the concept of vector embeddings, which are at the core of vector databases and enable a modern search technique called vector search. \n\nTo summarize, vector embeddings are the numerical representation of unstructured data of different data types, such as text data, image data, or audio data. Depending on the data type, vector embeddings are created using machine learning models that are able to translate the meaning of an object into a numerical representation in a high dimensional space. Thus, there are a variety of machine learning models able to create a variety of different types of embeddings, such as word embeddings, sentence embeddings, text embeddings, or image embeddings.\n\nVector embeddings capture the semantic relationship between data objects in numerical values and thus, you can find similar data points by determining their nearest neighbors in the high dimensional vector space. This concept is also called similarity search and can be applied in different applications, such as text search, image search, or recommendation systems.\n\nNow that you have a good understanding of vector embeddings, you might also be interested in the following articles:\n* What is a vector database?\n* Distance metrics for nearest neighbor search\n* Why is vector search so fast?\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n\n", "type": "Blog", "name": "blog-vector-embeddings-explained", "path": "blog/2023-01-16-vector-embeddings-explained/index.mdx", "link": "https://weaviate.io/blog/vector-embeddings-explained", "timestamp": "2024-02-08 20:24:13", "reader": "JSON", "meta": {}, "chunks": []}