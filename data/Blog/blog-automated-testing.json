{"text": "\nBut you probably know that writing tests is quite a task in itself, and in many cases running them can also be a chore. For example, the test suite may need to set up and tear down a separate service such as a database, which can be time-consuming and error-prone.\n\nI'm here to tell you that it doesn't have to be that way. In this article, I'll show you how to make testing easier with Embedded Weaviate, and other tips for better automated testing.\n\nAnd doing so, you might just discover the hidden gem that is the value provided by adding tests to your applications.\n\n## Testing and Weaviate\n\nIn this article, we will focus on integration tests. Integrated testing is an important part of the development process, and especially so for complex applications. Weaviate-based apps usually fall in this category.\n\nFor one, Weaviate must interact with the application in a variety of ways. And additionally, Weaviate often interacts with external services such as vectorizers or LLMs.\n\nSuch complexity makes it important to test the application as a whole, and not just its individual components. This complexity also means that arranging the test suite can be cumbersome with a variety of moving parts that need to be set up and torn down.\n\nEmbedded Weaviate makes one part of this puzzle much easier, since Weaviate can be instantiated directly from the client. The following is all you need to do to start a Weaviate server:\n\n\n  \n  \n  \n\n  \n  \n  \n\n\nThis is not only useful for new contributors to the project, but also for experienced developers. Starting anew as a new contributor, or working from a different machine on occasion, can be a hassle. With Embedded Weaviate, you can just run the test suite and be done with it.\n\nBut Embedded Weaviate is not the only way to make testing easier. In the following sections, we'll look at other ways to make testing easier, and how to make the most of Embedded Weaviate.\n\n\n## Scoping tests\n\nWhile you may be familiar with tests and integration tests in general, here are some specific suggestions for Weaviate-powered applications:\n* **Whether to test search quality**: This depends primarily on the model used for vectorization, such as by a Weaviate vectorizer module. We suggest evaluating models separately, but not tested as a part of the application.\n* **Focus on interactions with the inference provider**: Search itself is a core Weaviate functionality that we can trust. So, we suggest any integration tests focus on the interaction with the inference provider. For example,\n  * is the vectorization model the expected one?\n  * if switching to a different inference provider or model, does the application still function as expected?\n* **Other common issues to test** include:\n  * Connection or authentication issues with the inference provider\n  * Incomplete or incorrect data imports\n  * Specifying the vector correctly when bringing your own vectors\n  * Data definition issues, like invalid class names, properties, or data types\n\n\n## Testing with embedded Weaviate\n\n### Set up\n\nEmbedded Weaviate lets us spawn a Weaviate server instance from the client, and automatically tear it down when the client terminates. The data is persisted between sessions, so we recommend deleting your data before each test.\n\nHere's how to instantiate an embedded Weaviate server and perform this cleanup:\n\n\n  Install dependencies\n\nIf you have yet to install the required dependencies, run the following command:\n\n\n  \n\n  ```bash\n  pip install -U weaviate-client pytest\n  ```\n\n  Then, save the code as `embedded_test.py` and run `pytest`.\n\n  \n\n  \n\n  ```bash\n  npm install weaviate-ts-embedded typescript ts-node\n  ```\n\n  Then, save the code as `test.ts` and run `node --loader=ts-node/esm test.ts`:\n\n  \n\n\n\n\n\n  \n\n  \n  \n\n  \n\n  \n  \n\n\nNow, let's walk through some examples of how you might construct integration tests with Embedded Weaviate.\n\n\n### Our first test\n\nAs a simple example, let's create a class and then test that it was created correctly.\n\nWe'll create a class for question & answer objects from the game show *Jeopardy!*, by specifying its name and the vectorizer (`text2vec-openai`).\n\nHere, the integration test will consist of checking that the class was created with the expected default OpenAI vectorization model type, `ada`.\n\n\n  \n    \n  \n\n  \n    \n  \n\n\nAlthough this is a simple test, you can imagine that if you have tens, or even hundreds of classes, tests like these can save you a lot of time and effort. And, if you're working with a team, you can be sure that everyone is on the same page about the expected schema.\n\nAll we've done so far (instantiate and connect to Weaviate and OpenAI, perform cleanup, create the class and test creation), was done with very little code, thanks to Embedded Weaviate. Next, let's see how we might test imports.\n\n\n### Testing imports\n\nOne particularly common issue we see is skipped objects during import due to rate limits from the vectorization provider. So, let's see how we might test that all objects were imported correctly.\n\nIn this section, we'll import a small subset (100 objects) of the original Jeopardy dataset. As always, we'll use batching for optimal speed.\n\nWhile we load the JSON into memory here, you can use other methods such as streaming for very large JSON or CSV files.\n\nThe test is simple; it verifies that all specified objects have been imported by performing an object count and checking it is 100.\n\n\n  Download jeopardy_100.json\n\n\n\n\n  \n    \n  \n\n  \n    \n  \n\n\nSuch a test would provide a simple, repeatable, way of ensuring that all objects were imported correctly.\n\nAnd now that all data has been imported, let's see how we might test queries.\n\n### Testing queries\n\nSemantic (`nearText`) searches may be one of the most common (if not *the* most common) searches our users perform.\n\nSo let's see how we might test semantic searches. A semantic search requires vectorizing the query, so a test will validate the integration with the vectorizer (`text2vec-openai` in this case).\n\nWe'll run a query for \"chemistry\" and check that the top result is about \"sodium\".\n\nDepending on your application and usage, you may find that the top result changes over time - for example, if your data changes over time. In this case, we will assume that the top result is immutable, or that you will update the specific test over time.\n\n\n  \n    \n  \n\n  \n    \n  \n\n\nA test like this can be used to ensure that the vectorizer is working as expected, and that the data has been imported correctly. For example - if the top result was something completely unintuitive and wildly dissimilar to the concept of \"chemistry\" - this might be cause to investigate further.\n\nYou could also test additional aspects, like the number of results returned, or the order of results.\n\n### End-to-end code\n\nSo far, we've seen how to test the following:\n* Create the collection\n* Import data\n* Semantic search functionality\n\nThe code below brings together the setup and tests we've implemented so far - if you haven't done so yet, try running it yourself \ud83d\ude09.\n\n\n  End-to-end code\n\n\n  \n    \n  \n\n  \n    \n  \n\n\n\n\n\n## CI / CD\n\nFor many of us, Continuous Integration and Continuous Deployment (CI/CD) is a critical part of our development process. It allows us to automate the testing and deployment of our code, and to ensure that our code is always in a deployable state.\n\nWe use GitHub, and they offer GitHub Actions which we like at Weaviate. While we don't have space to cover it in detail (check out their docs if you're interested), we want to highlight that you can set up a YAML file to automate running of tests on GitHub. The YAML file could look something like this:\n\n\n  \n\n```\nname: Run Python Automated Tests\non: [push]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Set up Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: 3.11\n    - name: Install dependencies\n      run: pip install weaviate-client pytest\n    - name: Run tests\n      run: pytest\n```\n\n  \n\n  \n\n```\nname: Run Node.js Automated Tests\non: [push]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v3\n    - name: Use Node.js 18.x\n      uses: actions/setup-node@v3\n      with:\n        node-version: 18.x\n        cache: 'npm'\n    - run: npm ci\n    - run: npm run build --if-present\n    - run: node --loader=ts-node/esm test.ts\n```\n\n  \n\n\nWith this step, you'll be able to run your tests on every push to the main branch.\n\nAnd did you notice that we didn't have to spin up an instance of Weaviate through GitHub Actions? That's because we're using Embedded Weaviate, which means we can run our tests without worrying about the Weaviate server instance.\n\nThis might not seem like a big deal, but it lowers the barrier to entry for running tests, and makes it easier to run tests locally, or in a CI/CD pipeline.\n\nAs a result, your application will be more robust, and you'll be able to deploy with confidence.\n\n\n## Closing thoughts\n\nIn this post, we've seen how to write an integration test for an application using Weaviate &mdash; Embedded Weaviate in particular.\n\nWith just a few lines of code, we are able to verify how we import a data set, vectorize it, then export the vectorized objects. The test can be extended with search, insertion, updates, deletes, and other operations that are part of the user journey.\n\nWhat's more - because we were using Embedded Weaviate, the journey from the start to finish was far easier, not to mention portable.\n\nSo what are you waiting for? Try out Embedded Weaviate - and add those tests to your application that you've been putting off \ud83d\ude09.\n\n\n\nWhat other aspects of integration testing would you like to learn about? Let us know in the comments below!\n\n\nimport WhatNext from  '/_includes/what-next.mdx'\n\n\n", "type": "Blog", "name": "blog-automated-testing", "path": "blog/2023-07-18-automated-testing/index.mdx", "link": "https://weaviate.io/blog/automated-testing", "timestamp": "2023-11-02 11:59:34", "reader": "JSON", "meta": {}, "chunks": []}