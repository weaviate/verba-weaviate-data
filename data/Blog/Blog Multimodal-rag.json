{"text": "\n![The wonderful world of multimodality](./img/hero.png)\n\nThe average human hears and learns from about 1 billion words in their entire lifetime. This might be an over-approximation, but it is in the correct ballpark because 1 billion seconds is about 30 years and we don\u2019t hear more than a few words per second. Accounting for sleeping, eating, and other activities; doing some back-of-the-napkin calculations, we can arrive at the [above number](https://youtu.be/Ckz8XA2hW84?t=2286).\n\nThe issue, however, is that current Large Language Models(LLMs) are trained on trillions of tokens, many orders of magnitude more data than we ever see in our lifetime, and yet they still don\u2019t have as vivid of an understanding of the causal relationships that exist in the world. From this, we can infer that the way humans learn is fundamentally different from how our current state-of-the-art models learn.\n\nHumans have a remarkable ability to learn and build world models through the integration of multiple sensory inputs. Our combination of senses work synergistically to provide us with rich and diverse information about our environment. By combining and interpreting these sensory inputs, we can form a coherent understanding of the world, make predictions, acquire new knowledge, and establish causal relationships very efficiently. Not only do humans capture and use multimodal representations of information but given a task we can also incorporate any of these modalities as context to help us guide our answers.\n\nIf you\u2019d like to explore this line of thinking further and the potential problems that need to be addressed when getting computers to take advantage of multimodal data, have a look at my previous [blog](https://weaviate.io/blog/multimodal-models) where I cover the topic in detail.\n\nIn this post we\u2019ll touch on:\n- **Contrastive Learning** - One particular approach to training multimodal embedding models that can understand images, audio, video, text, and more\n- **Any-to-Any Search and Retrieval** - Using multimodal embedding models to perform any-to-any search and scaling these multimodal embeddings into production using Vector Databases (*With code examples!*)\n- **Multimodal Retrieval Augmented Generation(MM-RAG)**: Augmenting the generation from Large Multimodal Models(LMMs) with multimodal retrieval of images and more\n- **Code Demo of RAG**\n\n## Contrastive Learning\n\nOne way to train a model that understands multimodal data including images, audio, video, and text is to first train individual models that understand each one of these modalities separately and then unify their representations of data using a process called contrastive training.\n\nThe idea behind contrastive training is that we can unify the vector space representation of models by taking embeddings across modalities and pushing them further apart or pulling them closer together depending on whether they are similar or different conceptually. This is demonstrated in the image below:\n\n![tripletLoss](./img/image_a.png)\n*Source: [Schroff et al. 2015](https://arxiv.org/abs/1503.03832)*\n\nThis process was carried out in MetaAI\u2019s [ImageBind paper](https://arxiv.org/abs/2305.05665) to unify vector spaces across 6 different modalities including images, text, audio, and video. To successfully perform contrastive training they used multiple labeled datasets of positive points across multiple modalities and randomly sampled for negative points.\n\nTo get a better intuitive understanding of how this process works imagine you embed the image of a lion into vector space using a vision model. The concept behind this object is similar to the audio of a lion roaring, so the audio object embedding can be used as a positive sample and the contrastive loss function works to pull these two points together in embedding space. On the other hand, the embedding of an image of a salad is a negative example and therefore needs to be pushed apart. Have a look at the modification of the above visual to account for cross-modal contrastive training:\n\n![crossMCLR](./img/image_b.png)\n*Source: [Zolfaghari et al. 2021](https://lmb.informatik.uni-freiburg.de/Publications/2021/ZB21/)*\n\nIf we can continually do this for a large enough dataset of labeled points then we can tighten the representations of data objects in embedding space and even unify the models of different modalities. Another benefit, that ImageBind demonstrated, was that of using frozen image model representations to bind other modalities with cross-modal contrastive loss training - hence the name ImageBind. Embeddings that start differently can then be pulled towards the image representations and thus all of the similar concepts across modalities can be unified such that a concept across all modalities will have similar vectors - demonstrated in the image below. To learn in more depth about contrastive representation learning I would recommend this [blog](https://lilianweng.github.io/posts/2021-05-31-contrastive/).\n\n![unified_emb_Dark](./img/unified_emb_D.png#gh-dark-mode-only)\n![unified_emb__Light](./img/unified_emb_L.png#gh-light-mode-only)\n*Shows a unified embedding model that captures meanings from any modality that was fused during the contrastive training step.*\n\n## Any-to-Any Search\n\nOnce we have a unified embedding space we can take advantage of this to perform cross-modal object operations such as cross-modal search and retrieval, meaning that we can pass in as a query any modality the model understands and use it to perform vector similarity search in multimodal embedding space, getting back objects of any other modality that are similar in concept. You can also use this unified embedding space to perform cross-modal embedding arithmetic, for example, you can answer questions like what an image of a pigeon and the audio of a bike revving look like together.\n\n![cross_modal](./img/cross_modal.png)\n*[Source](https://imagebind.metademolab.com/)*\n\nIn the Jupyter notebook [here](https://github.com/weaviate-tutorials/multimodal-workshop/blob/main/2-multimodal/1-multimedia-search-complete.ipynb) we show how you can use the `multi2vec-bind` module in Weaviate to use the ImageBind model to add multimedia files to a vector database. Then we can perform any-to-any search over that data.\nYou can also use this diagram explaining any-to-any search to get an intuition of how the following code leverages the unified embedding space previously generated to perform the search.\n\n![any2any_Dark](./img/any2any_D.png#gh-dark-mode-only)\n![any2any_Light](./img/any2any_L.png#gh-light-mode-only)\n*Any-to-any search: Shows that any of the modalities understood and embedded by the multimodal model can be passed in as a query and objects of any modality that are conceptually similar can be returned.*\n\n### Step1: Create a Multimodal Collection:\n\n```python\nclient.collections.create(\n    name=\"Animals\",\n    vectorizer_config=wvc.config.Configure.Vectorizer.multi2vec_bind(\n        audio_fields=[\"audio\"],\n        image_fields=[\"image\"],\n        video_fields=[\"video\"],\n    )\n)\n```\n\n### Step 2: Insert Images and other media\n\n```python\nsource = os.listdir(\"./source/image/\")\nitems = list()\nfor name in source:\n    print(f\"Adding {name}\")\n    path = \"./source/image/\" + name\n    items.append({\n        \"name\": name,\n        \"path\": path,\n        \"image\": toBase64(path),\n        \"mediaType\": \"image\"\n    })\nanimals = client.collections.get(\"Animals\")\nanimals.data.insert_many(items)\n```\n\n### Step 3: Performing Image Search -\n\n```python\nresponse = animals.query.near_image(\n    near_image=toBase64(\"./test/test-cat.jpg\"),\n    return_properties=['name','path','mediaType'],\n    limit=3\n)\n```\n\nFor a more detailed breakdown refer to the complete [notebook](https://github.com/weaviate-tutorials/multimodal-workshop/blob/main/2-multimodal/1-multimedia-search-complete.ipynb) and supporting repository.\n\nThe use of a vector database to store and perform fast and real-time retrieval of object embeddings allows us to scale the usage of these multimodal models to power multimodal search in production and to integrate into our applications the cross-modal operations that we've discussed.\n\n## Multimodal Retrieval Augmented Generation(MM-RAG)\n\nRetrieval augmented generation(RAG) allows us to pack retrieved context into a prompt so that a language model can read relevant information before generating a response. This allows us to scale the knowledge of large language models without having to train or fine-tune them every time we have updated information.\n\nBy externalizing the knowledge of a model, [RAG can provide](https://cs.stanford.edu/~myasu/blog/racm3/) benefits such as:\n- **Scalability**: reducing the model size and training cost, as well as allowing easy expansion of knowledge\n- **Accuracy**: grounding the model to facts and reducing hallucination\n- **Controllability**: allowing updating or customizing the knowledge by simply performing CRUD operations in a vector DB\n- **Interpretability**: retrieved items serving as the reference to source in model predictions\n\nHowever, the issue with the current RAG workflows is that they only leverage retrieved text source material. This is because most LLMs only understand language and so any information that's retrieved had to be in text format \u2026 until now!\n\nRecently, there have been a group of large generative models, both closed and open source, that understand both text and images. As a result, we can now support multimodal RAG, where we can retrieve images from our vector database and pass those into the large multimodal model(LMM) to then generate with. This simple two-step process, illustrated below, is the main idea behind Multimodal-RAG.\n\n![mmRAG_Dark](./img/mmRAG_D.png#gh-dark-mode-only)\n![mmRAG_Light](./img/mmRAG_D.png#gh-light-mode-only)\n*Shows the two-step process of MM-RAG, involving retrieval from a multimodal knowledge base and then generation using a large multimodal model by grounding in the retrieved context.*\n\nMM-RAG was [presented earlier this year](https://arxiv.org/abs/2211.12561) by a group at Stanford. They showed a workflow of multimodal models, one that could retrieve and another that could generate both text and images.\n\nThey also discussed the advantages of MM-RAG:\n\n1. It significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks.\n2. It requires much less compute while achieving better performance (\n  Using OpenAI GPT4-V\n\n```python\nimport requests\n\ndef generate_description_from_image_gpt4(prompt, image64):\n  headers = {\n      \"Content-Type\": \"application/json\",\n      \"Authorization\": f\"Bearer {openai.api_key}\"\n  }\n  payload = {\n      \"model\": \"gpt-4-vision-preview\",\n      \"messages\": [\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\n              \"type\": \"text\",\n              \"text\": prompt\n            },\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{image64}\" #base64 encoded image from Weaviate\n              }\n            }\n          ]\n        }\n      ],\n      \"max_tokens\": 300\n  }\n  response_oai = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n  result = response_oai.json()['choices'][0]['message']['content']\n  print(f\"Generated description: {result}\")\n  return result\nGENERATED_DESCRIPTION = generate_description_from_image_gpt4(\n    prompt=\"This is an image of my pet, please give me a cute and vivid description.\",\n    image64=SOURCE_IMAGE\n)\n```\n\n\n**Generated description:** This adorable image captures a charming French Bulldog sitting obediently against a vibrant red background. The pup's coat is predominantly white with distinctive black patches around the ears and eye, giving it a look of natural elegance. Its expressive, wide-set eyes gleam with a mix of curiosity and anticipation, while the slight tilt of its head and those perky bat-like ears contribute to an overall image of endearing attentiveness.\n\nThe cuteness is amplified by a handwritten sign hung around its neck with the words \"FREE KISSES\" and a little heart symbol, extending a sweet and whimsical offer to all who come near. The sign, coupled with the dog's innocent gaze, conjures up feelings of warmth and companionship. This tiny ambassador of affection sits proudly, almost as if understanding the joy it brings to those around it. With its compact size and affectionate demeanor, this little canine looks ready to dispense unlimited love and puppy kisses on demand.\n\n### Use Text to Reconstruct the Image from DALL-E-3 (Diffusion Model):\n\nCurrently, GPT4-V can't produce images so to generate an image from the above description we will use the new DALL-E-3 model instead:\n\n\n  Using OpenAI DALL-E-3\n\n```python\nfrom openai import OpenAI\ndef generate_image_dalee3(prompt):\n  openai_client = OpenAI()\n  response_oai = openai_client.images.generate(\n    model=\"dall-e-3\",\n    prompt=str(prompt),\n    size=\"1024x1024\",\n    quality=\"standard\",\n    n=1,\n  )\n  result = response_oai.data[0].url\n  print(f\"Generated image url: {result}\")\n  return result\nimage_url = generate_image_dalee3(GENERATED_DESCRIPTION)\n```\n\n\n\n**Generated Image:**\n![generated puppy](./img/puppy_dalle.png)\n\n## Conclusion\nIn this blog, we covered how we can extend the concept of RAG to include retrieval from a multimodal knowledge base. We also explained how multimedia can be embedded into a unified vector space and consequently how we can leverage vector databases to power any-to-any search. I hope you found this article useful! I'd love to connect on **X** at [@zainhasan6](https://twitter.com/ZainHasan6)!\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n\n", "type": "Blog", "name": "Blog Multimodal-rag", "path": "blog/2023-12-05-multimodal-RAG/index.mdx", "link": "https://weaviate.io/blog/multimodal-RAG", "timestamp": "2024-05-08 10:51:53", "reader": "JSON", "meta": {}, "chunks": []}