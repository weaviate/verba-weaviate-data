{"text": "\nCover, Building Multimodal AI in TypeScript\n\n\nFor a lot of people, Multimodal AI and its derivatives are strong contenders for technology of the year. The promises of Multimodal applications of artificial intelligence are exciting and speak to behavioural changes in media consumption and general interaction with the internet. I'm here to show you how you can get a piece of the pie. In this article, we\u2019ll look at how to build Multimodal applications in Typescript and dive into everything that needs to happen in between.\n\n\n\n## Modalities \n> A modality is a particular mode in which something exists or is experienced or expressed.\n\n### Single Modality \n\nHistorically, search has been over text, emphasizing the relevance of keyword search. This type of search was focused on matching text search terms to large datasets of text and returning the most relevant. This was the single modality; text search through data and get back text.\n\nEven with the onset of multimedia; images, videos and audio. We leveraged metadata and attached text to these more complex modalities and essentially ran keyword searches. Still a single-modality search. \n\nEarly versions of large language models followed suit. With most being single modality models, with a single model inference.\n\nSingle Modality Architecture\n\n\n\n### Multi-Modality\n\nIn today's world, not only are people optimizing for TikTok searches but the video hosting service is the search engine of choice for a majority of Gen Z. Multimodality enables search queries with multiple media types over multiple media types. Enabling you to text search over datasets of video or mixed media. \n\nMostly enabled by vector and hybrid search, Vector databases by design are uniquely adapted to storing, indexing and enabling efficient retrieval for multimodal use cases. On the machine learning model side of things, we\u2019re seeing the rise of multimodal models that can infer numerous modalities. \n\nThis means we can interact with an LLM via chat and have the LLMs generate text, audio or video. By definition, this would enable interaction with similar models via text or even video.\n\n\nMultimodal Architecture\n\n\n\n## Building our own Multimodal Search\n\nNow that we have a better understanding of multi-modal search, let\u2019s build a search application with Weaviate and Next.js\n\nSpecwise, we want to be able to run a text search through image, video and audio data without leveraging the file's metadata. \n\n\n### Requirements\n\nYou need the following to go through with this tutorial. \n\n- An LTS version of Node.js\n- Docker\n- Git\n\n> The project that this tutorial is based on is available on Github if you\u2019d like to give it a try before going through with the tutorial. \n\n\nCreate your Next.js application that comes with TypeScript, Tailwind CSS and the App Router with the following command.\n\n```bash\ncreate-next-app  \u2013ts \u2013tailwind \u2013app \n```\n\n### Getting Weaviate Running\nIn the newly created folder, create a `docker-compose.yml` file and paste the following code into it. \n\n```yml\nversion: '3.4'\nservices:\n  weaviate:\n    image: semitechnologies/weaviate:1.23.4\n    restart: on-failure:0\n    ports:\n      - \"8080:8080\"\n    environment:\n        QUERY_DEFAULTS_LIMIT: 25\n        AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\n        PERSISTENCE_DATA_PATH: '/var/lib/weaviate'\n        DEFAULT_VECTORIZER_MODULE: 'multi2vec-bind'\n        ENABLE_MODULES: 'multi2vec-bind'\n        BIND_INFERENCE_API: 'http://multi2vec-bind:8080'\n        CLUSTER_HOSTNAME: 'node1'\n  multi2vec-bind:\n    image: semitechnologies/multi2vec-bind:imagebind\n    environment:\n      ENABLE_CUDA: '0'\n\n```\n\n\nThen run `docker-compose up -d` in your terminal to start an instance of Weaviate with ImageBind, our Multimodal model. This also enables us to use the multi2vec-bind module, the tool that enables us to store vector embeddings of video, text, audio and image data. \n\n### Importing Data\n\nNext, we will create a Weavaite collection and import data into it. In your `./public` folder, create three folders called *image*, *audio*, and *video*. These will store media corresponding to their folder names respectively. You can then add your data or use the data I added to build out the original application. \n\nWe then install the Weaviate TypeScript client and other project dependencies with the following command.\n```bash\nyarn add weaviate-ts-client use-debounce rimraf \n```\n\nTo import our data, we need to create a folder called `src` where our import scripts will go. \n\nTo start, create a file called `client.ts` and paste the following code in it to initialize the Weaviate client. \n\n```typescript\n\nlet client: WeaviateClient;\n\nexport const getWeaviateClient = () => {\n  if (!client) {\n    client = weaviate.client({\n      scheme: 'http',\n      host: 'localhost:8080',\n    });\n  };\n  \n  return client;\n}\n\n```\n\n\nNext, still in `./src` we create a file called `util.ts` and paste the following code in it. \n\n```typescript\nimport {readdirSync, readFileSync, } from 'fs'\n\nexport interface FileInfo {\n    name: string;\n    path: string;\n}\n\nexport const listFiles = (path: string): FileInfo[] => {\n    return readdirSync(path).map((name) => {\n        return {\n            name: name,\n            path: `${path}${name}`,\n        }\n    });\n}\n\nexport const getBase64 = (file: string) => {\n    return readFileSync(file, { encoding: 'base64' });\n}\n\n```\n\nThis file defines base64 as the encoding for the files we are importing and lists all the files in a directory.\n\nWe then create a file called `collection.ts`, where we define our Weaviate collection and its properties.\n\n```typescript\n\nconst client: WeaviateClient = getWeaviateClient();\n\nconst collectionExists = async (name: string) => {\n  return client.schema.exists(name);\n}\n\nexport const createBindCollection = async (name: string) => {\n  if(await collectionExists(name)) {\n    console.log(`The collection [${name}] already exists. No need to create it.`);\n    return;\n  }\n  \n  console.log(`Creating collection [${name}].`);\n\n  const bindSchema = {\n    class: name,\n    moduleConfig: {\n      'multi2vec-bind': {\n        imageFields: ['image'],\n        audioFields: ['audio'],\n        videoFields: ['video'],\n      }\n    },\n    properties: [\n      {\n        name: 'name',\n        dataType: ['text'],\n        moduleConfig: {\n          'multi2vec-bind': { skip: true },\n        },\n      },\n      {\n        name: 'media',\n        dataType: ['text'],\n        moduleConfig: {\n          'multi2vec-bind': { skip: true },\n        },\n      },\n      {\n        name: 'image',\n        dataType: ['blob'],\n      },\n      {\n        name: 'audio',\n        dataType: ['blob'],\n      },\n      {\n        name: 'video',\n        dataType: ['blob'],\n      }\n    ],\n    vectorIndexType: 'hnsw',\n    vectorizer: 'multi2vec-bind'\n  }\n  \n  const res = await client\n    .schema.classCreator()\n    .withClass(bindSchema)\n    .do();\n\n}\n\nexport const deleteCollection = async (name: string) => {\n  console.log(`Deleting collection ${name}...`);\n  await client.schema\n  .classDeleter()\n  .withClassName(name)\n  .do();\n\n  console.log(`Deleted collection ${name}.`);\n}\n\n```\n\nThis file contains a function to create a collection, a function to delete a collection, and a function to check if a collection exists. If there are any details we want to change that would define our collections, we can do so in this file.\n\nNotably, we create a new class and define media types of image, video and audio to our `multi2vec-bind` module. We also add the following properties to our collection: `media`, `name`, `image`, `audio`, and `video`. Think of these as properties we will be able to query against when we start to make queries or import data to Weavaite.\n\n\n\nNow we create a file called `import.ts` that uses Weaviates `ObjectsBatcher` to batch import files.\n\n```typescript\n\nconst sourceBase = 'public';\nconst sourceImages = sourceBase + '/image/'\nconst sourceAudio = sourceBase + '/audio/';\nconst sourceVideo = sourceBase + '/video/';\n\nconst client: WeaviateClient = getWeaviateClient();\n\nexport const importMediaFiles = async (collectionName: string) => {\n    await insertImages(collectionName);\n    await insertAudio(collectionName);\n    await insertVideo(collectionName);\n}\n\nconst insertImages = async (collectionName: string) => {\n    let batcher: ObjectsBatcher = client.batch.objectsBatcher();\n    let counter = 0;\n    const batchSize = 5;\n\n    const files = listFiles(sourceImages);\n    console.log(`Importing ${files.length} images.`)\n\n    for (const file of files) {\n        const item = {\n            name: file.name,\n            image: getBase64(file.path),\n            media: 'image'\n        };\n        \n        console.log(`Adding [${item.media}]: ${item.name}`);\n\n        batcher = batcher.withObject({\n            class: collectionName,\n            properties: item,\n            id: generateUuid5(file.name)\n        });\n\n        if (++counter == batchSize) {\n            console.log(`Flushing ${counter} items.`)\n\n            // flush the batch queue\n            await batcher.do();\n      \n            // restart the batch queue\n            counter = 0;\n            batcher = client.batch.objectsBatcher();\n        }\n    }\n\n    if (counter > 0) {\n        console.log(`Flushing remaining ${counter} item(s).`)\n        await batcher.do();\n        \n    }\n}\n\n\nconst insertAudio = async (collectionName: string) => {\n    let batcher: ObjectsBatcher = client.batch.objectsBatcher();\n    let counter = 0;\n    const batchSize = 3;\n\n    const files = listFiles(sourceAudio);\n    console.log(`Importing ${files.length} audio files.`)\n\n    for (const file of files) {\n        const item = {\n            name: file.name,\n            audio: getBase64(file.path),\n            media: 'audio'\n        };\n\n        console.log(`Adding [${item.media}]: ${item.name}`);\n        \n        batcher = batcher.withObject({\n            class: collectionName,\n            properties: item,\n            id: generateUuid5(file.name)\n        });\n\n        if (++counter == batchSize) {\n            console.log(`Flushing ${counter} items.`)\n            // flush the batch queue\n            await batcher.do();\n      \n            // restart the batch queue\n            counter = 0;\n            batcher = client.batch.objectsBatcher();\n        }\n    }\n\n    if (counter > 0) {\n        console.log(`Flushing remaining ${counter} item(s).`)\n        await batcher.do();\n    }\n}\n\nconst insertVideo = async (collectionName: string) => {\n    let batcher: ObjectsBatcher = client.batch.objectsBatcher();\n    let counter = 0;\n    const batchSize = 1;\n\n    const files = listFiles(sourceVideo);\n    console.log(`Importing ${files.length} video files.`)\n\n    for (const file of files) {\n        const item = {\n            name: file.name,\n            video: getBase64(file.path),\n            media: 'video'\n        };\n\n        console.log(`Adding [${item.media}]: ${item.name}`);\n        \n        batcher = batcher.withObject({\n            class: collectionName,\n            properties: item,\n            id: generateUuid5(file.name)\n        });\n\n        if (++counter == batchSize) {\n            console.log(`Flushing ${counter} items.`)\n            // flush the batch queue\n            await batcher.do();\n      \n            // restart the batch queue\n            counter = 0;\n            batcher = client.batch.objectsBatcher();\n        }\n    }\n\n    if (counter > 0) {\n        console.log(`Flushing remaining ${counter} item(s).`)\n        await batcher.do();\n    }\n}\n\n```\n\nThis file reads out images, video and audio directories for media and then encodes them before sending them to Weaviate to be stored.\n\n\nWe then create a file called `index.ts` where we run all the files we created above. \n\n```typescript\n\nconst collectionName = 'BindExample';\n\nconst run = async () => {\n  await deleteCollection(collectionName);\n  await createBindCollection(collectionName);\n  await importMediaFiles(collectionName);\n}\n\nrun();\n\n```\n\nIn `index.ts` we can change our collection name to our liking. When this file is run, we delete any existing collection that exists with the name we specify, create a new collection with the same name and then import all the media files in our `.public` folder.\n\n> It's important to note that we need our Weaviate instance running with Docker before going to the next step. \n\n\nTo run our import process, we need to add the following scripts to our `package.json` file.\n\n```js\n  \u2026\n\"scripts\": {\n      \u2026\n    \"preimport\": \"rimraf ./build && tsc src/*.ts --outDir build\",\n    \"import\": \"npm run preimport && node build/index.js\"\n        \u2026\n  },\n  \u2026\n```\n\nWe can now run `yarn run import` to start the import process. Depending on how much data you're importing, it can take a little time. While that runs, let's create our Search interface.\n\n### Building Search Functionality\n\nNow we need to create a couple of components that will go in our web application.\n\nCreate a `./components` folder and add a file called `search.tsx`. Paste the following code in it.\n\n```typescript \n'use client'\n\nimport { useSearchParams, usePathname, useRouter } from \"next/navigation\"\nimport { useDebouncedCallback } from \"use-debounce\"\n\nexport default function Search({ placeholder }: { placeholder: string }) {\n    const searchParams = useSearchParams()\n    const pathname = usePathname()\n    const { replace } = useRouter()\n\n    const handleSearch = useDebouncedCallback((term: string) => {\n        const params = new URLSearchParams(searchParams.toString())\n        if (term) {\n            params.set(\"search\", term)\n        } else {\n            params.delete(\"search\")\n        }\n        replace(pathname + \"?\" + params)\n    }, 300);\n    return (\n        \n            \n                 Search \n                 handleSearch(e.target.value)}\n                    defaultValue={searchParams.get(\"search\")?.toString()}\n                    className=\"w-[400px] h-12 rounded-md bg-gray-200 p-2 shadow-sm sm:text-sm\"\n                />\n            \n        \n    )\n}\n\n```\n\nWe have an input tag that takes search terms from users. We then pass these search terms to our URL params and then use the `replace` method from `next/navigation` to update the URL. This will update the URL and trigger a new search when the user stops typing.\n\n\nOptionally, you can import the footer and navigation components. We don\u2019t really need them to demonstrate Multimodal search but they make the app look nicer! \n\n\nIn the `./app` folder, paste the following code in `page.tsx`, \n\n```typescript\n// // import Search from '../components/search'\nimport Link from 'next/link'\n\nimport weaviate, {\n  WeaviateClient,\n  WeaviateObject,\n} from \"weaviate-ts-client\";\n\nconst client: WeaviateClient = weaviate.client({\n  scheme: 'http',\n  host: 'localhost:8080',\n});\n\n\nexport default async function Home({\n  searchParams\n}: {\n  searchParams?: {\n    search?: string;\n  }\n}) {\n  const search = searchParams?.search || \"\";\n  const data = await searchDB(search);\n\n\n  return (\n    \n      \n        \n        \n       {/*   */}\n\n        \n\n          \n\n          \n\n            {data.map((result: WeaviateObject) => (\n              \n\n                \n                  \n                    {result.media}\n                  \n\n                  {result?.media == 'image' &&\n                    \n                  }\n\n                  {result?.media == 'audio' &&\n                    \n                      Your browser does not support the audio element.\n                    \n\n                  }\n\n                  {result.media == 'video' &&\n                    \n                      Your browser does not support the video element.\n                    \n\n                  }\n\n                \n\n              \n            ))}\n          \n        \n         {/*  */}\n      \n    \n  )\n}\n\n\nasync function searchDB(search: string) {\n\n  const res = await client.graphql\n    .get()\n    .withClassName(\"BindExample\")\n    .withFields(\"media name _additional{ certainty id }\")\n    .withNearText({ concepts: [`${search}`] })\n    .withLimit(8)\n    .do();\n\n  return res.data.Get.BindExample;\n}\n\n```\n\nHere we define a function `searchDB()`, which makes a call to Weaviate to search for the given search term. We then map the results to the UI. Weaviate lets us run a vector search from our `BindExample` collection with `withNearText()` and pass the user-entered search term as a query. We can define properties we want to search on with `withFields()` and the number of results we want to return with `withLimit()`. We then pass the query response to be displayed on the UI.\n\n\n### Final Result\nThen you should be able to search like this \n\nthe final multimodal search project\n\n## Conclusion\n\nWe just saw how to leverage a Multimodal AI model to power our Multimodal search in Typescript with Next.js. A lot is happening in the space and this only just scratches the surface of Multimodal use cases. One could take this a step further and enable search via image, video or audio uploads. Another avenue worth exploring is Generative Multimodal models to either enhance results or interact with existing datasets. If this is something you find interesting or would love to continue a conversation on, find me online at malgamves.\n\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n\n", "type": "Blog", "name": "blog-typescript-multimodal-search", "path": "blog/2024-01-23-typescript-multimodal-search/index.mdx", "link": "https://weaviate.io/blog/multimodal-search-in-typescript", "timestamp": "2024-02-08 21:23:00", "reader": "JSON", "meta": {}, "chunks": []}