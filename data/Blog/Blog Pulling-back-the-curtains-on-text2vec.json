{"text": "\n![Pulling back the curtains on text2vec](./img/hero.png)\n\nYou probably know that Weaviate converts a text corpus into a set of vectors - each object is given a vector that captures its 'meaning'. But you might not know exactly how it does that, or how to adjust that behavior. Here, we will pull back the curtains to examine those questions, by revealing some of the mechanics behind `text2vec`'s magic.\n\nFirst, we will reproduce Weaviate's output vector using only an external API. Then we will see how the text vectorization process can be tweaked, before wrapping up by discussing a few considerations also.\n\n## Background\n\nI often find myself saying that Weaviate makes it fast and easy to produce a vector database from text. But it can be easy to forget just how fast and how easy it can make things.\n\nIt is true that even in the \u201cold days\u201d of say, five to ten years ago, producing a database with vector capabilities was technically possible. You *simply* had to (*inhales deeply*) develop a vectorization algorithm, vectorize the data, build a vector index, build a database with the underlying data, integrate the vector index with the database, then forward results from a vector index query to the database and combine the outputs from both (*exhales*).\n\nThe past for vector searching definitely was not a \u201csimpler time\u201d, and the appeal of modern vector databases like Weaviate is pretty clear given this context.\n\nBut while the future is here, it isn't yet perfect. Tools like Weaviate can seem like a magician's mystery box. Our users in turn ask us *exactly* how Weaviate does its magic; how it turns all of that data into vectors, and how to control the process.\n\nSo let's take a look inside the magic box together in this post.\n\nIf you would like to follow along, the Jupyter notebook and data are available [here](https://github.com/weaviate/weaviate-examples/tree/main/text2vec-behind-curtain). You can use our free [Weaviate Cloud Services](https://console.weaviate.cloud) (WCS) sandbox, or set up your own Weaviate instance also.\n\n> Note: The vectorization is done by [Weaviate \u201ccore\u201d](https://github.com/weaviate/weaviate) and not at the client level. So even though we use Python examples, the principles are universally applicable.\nLet's get started!\n\n## Text2vec: behind the scenes\n\nWeaviate's `text2vec-*` modules transform text data into dense vectors for populating a Weaviate database. Each `text2vec-*` module uses an external API (like `text2vec-openai` or `text2vec-huggingface`) or a local instance like `text2vec-transformers` to produce a vector for each object.\n\nLet's try vectorizing data with the `text2vec-cohere` module. We will be using data from `tiny_jeopardy.csv` [available here](https://github.com/weaviate/weaviate-examples/tree/main/text2vec-behind-curtain) containing questions from the game show Jeopardy. We'll just use a few (20) questions here, but the [full dataset on Kaggle](https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions) includes 200k+ questions.\n\nLoad the data into a Pandas dataframe, then populate Weaviate like this:\n\n```python\nclient.batch.configure(batch_size=100)  # Configure batch\nwith client.batch as batch:\n    for i, row in df.iterrows():\n        properties = {\n            \"question\": row.Question,\n            \"answer\": row.Answer\n        }\n        batch.add_data_object(properties, \"Question\")\n```\n\nThis should add a series of `Question` objects with text properties like this:\n\n```text\n{'question': 'In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger',\n 'answer': \"McDonald's\"}\n```\n\nSince we use the `text2vec-cohere` module to vectorize our data, we can query Weaviate to find data objects most similar to any input text. Look for entries closest to `fast food chains` like so:\n\n```python\nnear_text = {\"concepts\": [\"fast food chains\"]}\nwv_resp = client.query.get(\n    \"Question\",\n    [\"question\", \"answer\"]\n).with_limit(2).with_near_text(\n    near_text\n).with_additional(['distance', 'vector']).do()\n```\n\nThis yields the McDonald's `Question` object above, including the object vector and the distance. The result is a `768`-dimensional vector that is about `0.1` away from the query vector.\n\nThis all makes intuitive sense - the entry related to the largest fast food chain (McDonald's) is returned from our \u201cfast food chains\u201d query.\n\nBut wait, how was that vector derived?\n\nThis is the 'magic' part. So let's look behind the curtain, and see if we can reproduce the magic. More specifically, let's try to reproduce Weaviate's output vector for each object by using an external API.\n\n![pulling back the curtains](./img/pulling-back-the-curtains-text2vec.png)\n\n### Matching Weaviate's vectorization\n\nWe know that the vector for each object corresponds to its text. What we don't know is how, exactly.\n\nAs each object only contains the two properties, `question` and `answer`, let's try concatenating our text and comparing it to Weaviate's. And instead of the `text2vec-cohere` module, we will go straight to the Cohere API.\n\nConcatenating the text from the object:\n\n```python\nstr_in = ' '.join([i for i in properties.values()])\n```\n\nWe see:\n\n```text\n'In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger McDonald\\'s'\n```\n\nThen, use the Cohere API to generate the vector like so, where `cohere_key` is the API key (keep it secret!), and `model` is the vectorizer.\n\n```python\nimport cohere\nco = cohere.Client(cohere_key)\nco_resp = co.embed([str_in], model=\"embed-multilingual-v2.0\")\n```\n\nThen we run a `nearVector` based query to find the best matching object to this vector:\n\n```python\nclient.query.get(\n    \"Question\",\n    [\"question\", \"answer\"]\n).with_limit(2).with_near_vector(\n    {'vector': co_resp.embeddings[0]}\n).with_additional(['distance']).do()\n```\n\nInterestingly, we get a distance of `0.0181` - small, but not zero. In other words - we did something differently to Weaviate!\n\nLet's go through the differences one by one, which hopefully will help you to see Weaviate's default behavior.\n\nFirst, Weaviate sorts properties alphabetically (a-z) before concatenation. Our initial concatenation had the `question` text come first, so let's reverse it to:\n\n```text\n'McDonald\\'s In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger '\n```\n\nThis lowers the distance to `0.0147`.\n\nWeaviate adds the class name to the text. So we will prepend the word `question` producing:\n\n```text\n'question McDonald\\'s In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger'\n```\n\nFurther lowering the distance to `0.0079`.\n\nThen the remaining distance can be eliminated by converting the text to lowercase like so:\n\n```python\nstr_in = ''\nfor k in sorted(properties.keys()):\n    v = properties[k]\n    if type(v) == str:\n        str_in += v + ' '\nstr_in = str_in.lower().strip()  # remove trailing whitespace\nstr_in = 'question ' + str_in\n```\n\nProducing:\n\n```text\n'question mcdonald\\'s in 1963, live on \"the art linkletter show\", this company served its billionth burger'\n```\n\nPerforming the `nearVector` search again results in zero distance (`1.788e-07` - effectively zero)!\n\nIn other words - we have manually reproduced Weaviate's default vectorization process. It's not overly complex, but knowing it can certainly be helpful. Let's recap exactly what Weaviate does.\n\n### Text vectorization in Weaviate\n\n\n\nNow that we understand this, you might be asking - is it possible to customize the vectorization process? The answer is, yes, of course.\n\n## Tweaking text2vec vectorization in Weaviate\n\nSome of you might have noticed that we have not done anything at all with the schema so far. This meant that the schema used is one generated by the auto-schema feature and thus the vectorizations were carried out using default options.\n\nThe schema is the place to define, among other things, the data type and vectorizer to be used, as well as cross-references between classes. As a corollary, the vectorization process can be modified for each class by setting the relevant schema options. In fact, you can [define the data schema](/developers/weaviate/manage-data/collections) for each class individually.\n\nAll this means that you can also use the schema to tweak Weaviate's vectorization behavior. The relevant variables for vectorization are `dataType` and those listed under `moduleConfig` at both the class level and property level.\n\n* At the **class** level, `vectorizeClassName` will determine whether the class name is used for vectorization.\n* At the **property** level:\n    * `skip` will determine whether the property should be skipped (i.e. ignored) in vectorization, and\n    * `vectorizePropertyName` will determine whether the property name will be used.\n* The property `dataType` determines whether Weaviate will ignore the property, as it will ignore everything but `string` and `text` values.\n\n> You can read more about each variable in the [schema configuration documentation](/developers/weaviate/manage-data/collections).\nLet's apply this to our data to set Weaviate's vectorization behavior, then we will confirm it manually using the Cohere API as we did above.\n\nOur new schema is below - note the commented lines:\n\n```python\nquestion_class = {\n    \"class\": \"Question\",\n    \"description\": \"Details of a Jeopardy! question\",\n    \"moduleConfig\": {\n        \"text2vec-cohere\": {  # The vectorizer name - must match the vectorizer used\n            \"vectorizeClassName\": False,  # Ignore class name\n        },\n    },\n    \"properties\": [\n        {\n            \"name\": \"answer\",\n            \"description\": \"What the host prompts the contestants with.\",\n            \"dataType\": [\"string\"],\n            \"moduleConfig\": {\n                \"text2vec-cohere\": {\n                    \"skip\": False,  # Do not skip class\n                    \"vectorizePropertyName\": False  # Ignore property name\n                }\n            }\n        },\n        {\n            \"name\": \"question\",\n            \"description\": \"What the contestant is to provide.\",\n            \"dataType\": [\"string\"],\n            \"moduleConfig\": {\n                \"text2vec-cohere\": {\n                    \"skip\": False,  # Do not skip class\n                    \"vectorizePropertyName\": True  # Do not ignore property name\n                }\n            }\n        },\n    ]\n}\nclient.schema.create_class(question_class)\n```\n\nThe schema is defined such that at least some of the options, such as `moduleConfig`/`text2vec-cohere` /`vectorizeClassName` and `properties`/`moduleConfig`/`text2vec-cohere`/`vectorizePropertyName` differ from their defaults.\n\nAnd as a result, a `nearVector` search with the previously-matching Cohere API vector is now at a distance of `0.00395`.\n\nTo get this back down to zero, we must revise the text generation pipeline to match the schema. Once we've done that, which looks like this:\n\n```python\nstr_in = ''\nfor k in sorted(input_props.keys()):\n    v = input_props[k]\n    if type(v) == str:\n        if k == 'question':\n            str_in += k + ' '\n        str_in += v + ' '\nstr_in = str_in.lower().strip()\n```\n\nSearching with the vector generated from this input, the closest matching object in Weaviate once again has a distance of zero. We've come full circle \ud83d\ude42.\n\n## Discussions & wrap-up\n\nSo there it is. Throughout the above journey, we saw how exactly Weaviate creates vectors from the text data objects, which is:\n\n- Vectorize properties that use `string` or `text` data types\n- Sorts properties in alphabetical (a-z) order before concatenating values\n- Prepends the class name\n- And converts the whole string to lowercase\n\nAnd we also saw how this can be tweaked through the schema definition for each class.\n\nOne implication of this is that your vectorization requirements are a very important part of considerations in the schema definition. It may determine how you break down related data objects before importing them into Weaviate, as well as which fields you choose to import.\n\nLet's consider again our quiz question corpus as a concrete example. Imagine that we are building a quiz app that allows our users to search for questions. Then, it may be preferable to import each quiz item into two classes, one for the question and one for the answer to avoid giving away the answer based on the user's query. But we may yet use the answer to compare to the user's input.\n\nOn the other hand, in some cases, it may be preferable to import text data with many fields as one object. This will allow the user to search for matching meanings without as much consideration given to which field the information is contained in exactly. We would be amiss to not mention other search methods such as BM25F, or hybrid searches, both of which would be affected by these decisions.\n\nNow that you've seen exactly what happens behind the curtains, we encourage you to try applying these concepts yourself the next time you are building something with Weaviate. While the changes to the similarities were somewhat minor in our examples, in some domains and corpora their impact may be certainly larger. And tweaking the exact vectorization scheme may provide that extra boost your Weaviate instance is looking for.\n\n\n", "type": "Blog", "name": "Blog Pulling-back-the-curtains-on-text2vec", "path": "blog/2023-01-10-pulling-back-the-curtains-on-text2vec/index.mdx", "link": "https://weaviate.io/blog/pulling-back-the-curtains-on-text2vec", "timestamp": "2024-05-08 10:51:26", "reader": "JSON", "meta": {}, "chunks": []}