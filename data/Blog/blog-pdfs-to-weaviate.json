{"text": "\nSince the release of ChatGPT, and the subsequent realization of pairing Vector DBs with ChatGPT, one of the most compelling applications has been chatting with your PDFs (i.e. ChatPDF or ChatDOC). Why PDFs? PDFs are fairly universal for visual documents, encompassing research papers, resumes, powerpoints, letters, and many more. In our latest Weaviate Podcast with Unstructured Founder Brian Raymond, Brian motivates this kind of data by saying \u201cImagine you have a non-disclosure agreement in a PDF and want to train a classifier\u201d. Although PDFs are great for human understanding, they have been very hard to process with computers. PDF documents contain valuable insights and information that are key to unlocking text information for businesses. With the latest advancements in multimodal deep learning (models that process both images and text), it is now possible to extract high quality data from PDF documents and add it to your Weaviate workflow.\n\nOptical Character Recognition (OCR) describes technology that converts different types of visual documents (research papers, letters, etc.) into a machine readable format. RVL-CDIP is a benchmark that tests the performance of classifying document images. New models like LayoutLMv3 and Donut leverage both the text and visual information by using a multimodal transformer. These models are reaching new heights in performance because they leverage visual information, not just text.\n\n\n\nDonut pipeline\n Pipeline of Donut from Kim, G. et al (2022) \n\n\n\n## About Unstructured\nUnstructured is an open-source company working at the cutting edge of PDF processing and more. They allow businesses to ingest their diverse data sources, whether this be a `PDF`, `JPEG`, or `PPT`, and convert it into data that can be passed to a LLM. This means that you could take private documents from your company and pass it to a LLM to chat with your PDFs.\n\nUnstructured\u2019s open-source core library is powered by document understanding models. Document understanding techniques use an encoder-decoder pipeline that leverages the power of both computer vision and natural language processing methods.\n\nOn the Weaviate Podcast, Brian Raymond described one of the founding motivations of Unstructured as follows: \u201cHey, HuggingFace is exploding over here with 10s of thousands of models and an incredible community. What if we did something similar to the left of HuggingFace, and we made it cheap, fast, and easy for data scientists to get through that data engineering step, so they can consume more of that!\u201d Now that the stage is set, let\u2019s explore how Unstructured works.\n\nUnstructured simplifies the process of importing a PDF and converting it into text. The core abstraction of Unstructured is the 'brick.' Unstructured uses bricks for document pre-processing: 1. Partitioning 2. Cleaning, 3. Staging. Partitioning bricks take an unstructured document and extract structured content from it. It takes the document and breaks it down into elements like `Title`, `Abstract`, and `Introduction`. Later on you will see an example of how the partitioning bricks identified the elements in a research paper. Cleaning the data is an important step before passing it to an NLP model. The cleaning brick can \u2018sanitize\u2019 your text data by removing bullet points, extra whitespaces, and more. Staging is the last brick and it helps to prepare your data as input into downstream systems. It takes a list of document elements as input and returns a formatted dictionary as output.\n\nIn this blog post, we will show you how to ingest PDF documents with Unstructured and query in Weaviate.\n\nTo follow along with this blog post, check out this repository.\n\n## The Basics\nThe data we\u2019re using are two research papers that are publicly available. We first want to convert the PDF to text in order to load it into Weaviate. Starting with the first brick (partitioning), we need to partition the document into text. This is done with:\n\n```python\nfrom unstructured.partition.pdf import partition_pdf\n\nelements = partition_pdf(filename=\"../data/paper01.pdf\")\n```\n\nNow, if we want to see all of the elements that Unstructured found, we run:\n\n```python\ntitles = [elem for elem in elements if elem.category == \"Title\"]\n\nfor title in titles:\n    print(title.text)\n```\n\n\n  Response from Unstructured\n\nA survey on Image Data Augmentation for Deep Learning\nAbstract\nIntroduction\nBackground\nImage Data Augmentation techniques\nData Augmentations based on basic image manipulations\nFlipping\nColor space\nCropping\nRotation\nTranslation\nNoise injection\nColor space transformations\nGeometric versus photometric transformations\nKernel filters\nMixing images\nRandom erasing\nA note on combining augmentations\nData Augmentations based on Deep Feature space augmentation\nData Augmentations based on Deep Learning\nFeature space augmentation\nAdversarial training\nGAN\u2011based Data Augmentation\nGenerated images\nNeural Style Transfer\nMeta learning Data Augmentations\nComparing Augmentations\nDesign considerations for image Data Augmentation\nTest-time augmentation\nCurriculum learning\nResolution impact\nFinal dataset size\nAlleviating class imbalance with Data Augmentation\nDiscussion\nFuture work\nConclusion\nAbbreviations\nAcknowledgements\nAuthors\u2019 contributions\nFunding\nReferences\nPublisher\u2019s Note\n\n\n\nIf we want to store the elements along with the content, you run:\n\n```python\nimport textwrap\n\nnarrative_texts = [elem for elem in elements if elem.category == \"NarrativeText\"]\n\nfor index, elem in enumerate(narrative_texts[:5]):\n    print(f\"Narrative text {index + 1}:\")\n    print(\"\\n\".join(textwrap.wrap(elem.text, width=70)))\n    print(\"\\n\" + \"-\" * 70 + \"\\n\")\n```\n\nYou can then take this data, vectorize it and store it in Weaviate.\n\nPDFs to Weaviate\nPDFs to Weaviate\n\n## End-to-End Example\nNow that we\u2019ve introduced the basics of using Unstructured, we want to provide an end-to-end example. We\u2019ll read a folder containing the two research papers, extract their abstracts and store them in Weaviate.\n\nStarting with importing the necessary libraries:\n\n```python\nfrom pathlib import Path\nimport weaviate\nfrom weaviate.embedded import EmbeddedOptions\nimport os\n```\n\nIn this example, we are using Embedded Weaviate. You can also run it on WCS or docker. This demo is also using OpenAI for vectorization; you can choose another `text2vec` module here.\n\n```python\nclient = weaviate.Client(\n    embedded_options=EmbeddedOptions(\n        additional_env_vars={\"OPENAI_APIKEY\": os.environ[\"OPENAI_APIKEY\"]}\n    )\n)\n```\n\n### Configure the Schema\n\nNow we need to configure our schema. We have the `document` class along with the `abstract` property.\n\n```python\nclient.schema.delete_all()\n\nschema = {\n    \"class\": \"Document\",\n    \"vectorizer\": \"text2vec-openai\",\n    \"properties\": [\n        {\n            \"name\": \"source\",\n            \"dataType\": [\"text\"],\n        },\n        {\n            \"name\": \"abstract\",\n            \"dataType\": [\"text\"],\n            \"moduleConfig\": {\n                \"text2vec-openai\": {\"skip\": False, \"vectorizePropertyName\": False}\n            },\n        },\n    ],\n    \"moduleConfig\": {\n        \"generative-openai\": {},\n        \"text2vec-openai\": {\"model\": \"ada\", \"modelVersion\": \"002\", \"type\": \"text\"},\n    },\n}\n\nclient.schema.create_class(schema)\n```\n\n### Read/Import the documents\n\nNow that our schema is defined, we want to build the objects that we want to store in Weaviate. We wrote a helper class,  `AbstractExtractor` to aggregate the element class. We will call this in order to grab the abstract element along with the content.\n\n\n  AbstractExtractor\n\n```python\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\n\nclass AbstractExtractor:\n    def __init__(self):\n        self.current_section = None  # Keep track of the current section being processed\n        self.have_extracted_abstract = (\n            False  # Keep track of whether the abstract has been extracted\n        )\n        self.in_abstract_section = (\n            False  # Keep track of whether we're inside the Abstract section\n        )\n        self.texts = []  # Keep track of the extracted abstract text\n\n    def process(self, element):\n        if element.category == \"Title\":\n            self.set_section(element.text)\n\n            if self.current_section == \"Abstract\":\n                self.in_abstract_section = True\n                return True\n\n            if self.in_abstract_section:\n                return False\n\n        if self.in_abstract_section and element.category == \"NarrativeText\":\n            self.consume_abstract_text(element.text)\n            return True\n\n        return True\n\n    def set_section(self, text):\n        self.current_section = text\n        logging.info(f\"Current section: {self.current_section}\")\n\n    def consume_abstract_text(self, text):\n        logging.info(f\"Abstract part extracted: {text}\")\n        self.texts.append(text)\n\n    def consume_elements(self, elements):\n        for element in elements:\n            should_continue = self.process(element)\n\n            if not should_continue:\n                self.have_extracted_abstract = True\n                break\n\n        if not self.have_extracted_abstract:\n            logging.warning(\"No abstract found in the given list of objects.\")\n\n    def abstract(self):\n        return \"\\n\".join(self.texts)\n```\n\n\n```python\ndata_folder = \"../data\"\n\ndata_objects = []\n\nfor path in Path(data_folder).iterdir():\n    if path.suffix != \".pdf\":\n        continue\n\n    print(f\"Processing {path.name}...\")\n\n    elements = partition_pdf(filename=path)\n\n    abstract_extractor = AbstractExtractor()\n    abstract_extractor.consume_elements(elements)\n\n    data_object = {\"source\": path.name, \"abstract\": abstract_extractor.abstract()}\n\n    data_objects.append(data_object)\n```\n\nThe next step is to import the objects into Weaviate.\n\n```python\nclient.batch.configure(batch_size=100)  # Configure batch\nwith client.batch as batch:\n    for data_object in data_objects:\n        batch.add_data_object(data_object, \"Document\")\n```\n\n### Query Time\n\nNow that we have imported our two documents, we can run some queries! Starting with a simple BM25 search. We want to find a document that discusses house prices.\n\n\n```python\nclient.query.get(\"Document\", \"source\").with_bm25(\n    query=\"some paper about housing prices\"\n).with_additional(\"score\").do()\n```\n\n\n  Output\n\n```\n{'data': {'Get': {'Document': [{'_additional': {'score': '0.8450042'},\n     'source': 'paper02.pdf'},\n    {'_additional': {'score': '0.26854637'}, 'source': 'paper01.pdf'}]}}}\n```\n\n\n\nWe can take this one step further by using the generative search module. The prompt is to summarize the abstract of the two papers in one sentence. This type of summarization is very useful when scouting out new research papers. This enables us to get a quick summary of the abstract and ask questions specific to the paper.\n\n```python\nprompt = \"\"\"\nPlease summarize the following academic abstract in a one-liner for a layperson:\n\n{abstract}\n\"\"\"\n\nresults = (\n    client.query.get(\"Document\", \"source\").with_generate(single_prompt=prompt).do()\n)\n\ndocs = results[\"data\"][\"Get\"][\"Document\"]\n\nfor doc in docs:\n    source = doc[\"source\"]\n    abstract = doc[\"_additional\"][\"generate\"][\"singleResult\"]\n    wrapped_abstract = textwrap.fill(abstract, width=80)\n    print(f\"Source: {source}\\nSummary:\\n{wrapped_abstract}\\n\")\n```\n\n\n  Output\n\n```\nSource: paper01.pdf\nSummary:\nData Augmentation is a technique that enhances the size and quality of training\ndatasets for Deep Learning models, particularly useful in domains with limited\ndata such as medical image analysis.\n```\n```\nSource: paper02.pdf\nSummary:\nUsing machine learning techniques, researchers explore predicting house prices\nwith structured and unstructured data, finding that the best predictive\nperformance is achieved with term frequency-inverse document frequency (TF-IDF)\nrepresentations of house descriptions.\n```\n\n\n\n## Limitations\nThere are a few limitations when it comes to a document that has two columns. For example, if a document is structured with two columns, then the text doesn\u2019t extract perfectly. The workaround for this is to set `strategy=\"ocr_only\"` or `strategy=\"fast\"` into `partition_pdf`. There is a GitHub issue on fixing multi-column documents, give it a \ud83d\udc4d up!\n\n\n  strategy=\"ocr_only\"\n\n```python\nelements = partition_pdf(filename=\"../data/paper02.pdf\", strategy=\"ocr_only\")\nabstract_extractor = AbstractExtractor()\nabstract_extractor.consume_elements(elements)\n```\n\n\n\n\n  strategy=\u201dfast\u201d\n\n```python\nelements = partition_pdf(filename=\"../data/paper02.pdf\", strategy=\"fast\")\nabstract_extractor = AbstractExtractor()\nabstract_extractor.consume_elements(elements)\n```\n\n\n\n## Weaviate Brick in Unstructured\nThere is a GitHub issue to add a Weaviate staging brick! The goal of this integration is to add a Weaviate section to the documentation and show how to load unstructured outputs into Weaviate. Make sure to give this issue a \ud83d\udc4d up!\n\n## Last Thought\nThis demo introduced how you can ingest PDFs into Weaviate. In this example we used two research papers; however, there is the possibility to add Powerpoint presentations or even scanned letters to your Weaviate instance. Unstructured has really simplified the process of using visual document parsing for diverse document types.\n\nWe tested a few queries above, but we can take this one step further by using LangChain. Once the documents are imported into Weaviate, you can build a simple chatbot to chat with your pdfs by using LangChain\u2019s vectorstore.\n\n```python\nfrom langchain.vectorstores.weaviate import Weaviate\nfrom langchain.llms import OpenAI\nfrom langchain.chains import ChatVectorDBChain\nimport weaviate\n\nclient = weaviate.Client(\"http://localhost:8080\")\n\nvectorstore = Weaviate(client, \"NAME_OF_CLASS\", \"NAME_OF_PROPERTY\")\n\nMyOpenAI = OpenAI(temperature=0.2,\n    openai_api_key=\"ENTER YOUR OPENAI KEY HERE\")\n\nqa = ChatVectorDBChain.from_llm(MyOpenAI, vectorstore)\n\nchat_history = []\n\nwhile True:\n    query = input(\"\")\n    result = qa({\"question\": query, \"chat_history\": chat_history})\n    print(result[\"answer\"])\n    chat_history = [(query, result[\"answer\"])]\n```\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n\n\n", "type": "Blog", "name": "blog-pdfs-to-weaviate", "path": "blog/2023-05-23-pdfs-to-weaviate/index.mdx", "link": "https://weaviate.io/blog/ingesting-pdfs-into-weaviate", "timestamp": "2023-11-13 10:42:21", "reader": "JSON", "meta": {}, "chunks": []}