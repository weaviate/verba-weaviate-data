{"text": "\n[Vector databases](/blog/what-is-a-vector-database) - like\u00a0[Weaviate](/developers/weaviate)\u00a0- use\u00a0**machine learning models**\u00a0to analyze data and\u00a0**calculate vector embeddings**. The vector embeddings are\u00a0**stored together with the data**\u00a0in a database, and later are used to query the data.\n\nIn a nutshell, a vector embedding is an array of numbers, that is used to describe an object. For example, strawberries could have a vector\u00a0`[3, 0, 1]`\u00a0\u2013 more likely the array would be a lot longer than that.\n\n*Note*, the meaning of each value in the array, depends on what machine learning model we use to generate them.\n\nTo judge how similar two objects are, we can compare their vector values, by using various\u00a0**distance metrics**.\n\nIn the\u00a0*context of vector search*,\u00a0**similarity measures**\u00a0are a function that takes two vectors as input and calculates a distance value between them. The distance can take many shapes, it can be the geometric distance between two points, it could be an angle between the vectors, it could be a count of vector component differences, etc. Ultimately, we use the calculated distance to judge how close or far apart two vector embeddings are. These metrics are used in machine learning for classification and clustering tasks, especially in semantic search.\n\n> Distance Metrics convey how similar or dissimilar two vector embeddings are.\n\nIn this article, we explore the variety of distance metrics, the idea behind each, how they are calculated, and how they compare to each other.\n\n## Fundamentals of Vector Search\n\nIf you already have a working knowledge of vector search, then you can skip straight to the\u00a0[Cosine Distance](#cosine-similarity) section.\n\n### Vectors in Multi-Dimensional Space\n\nVector databases keep the semantic meaning of your data by representing each object as a vector embedding. Each embedding is a point in a high-dimensional space. For example, the vector for bananas (both the text and the image) is located near apples and not cats.\n\n\n\n\n*The above image is a visual representation of a vector space. To perform a search, your search query is converted to a vector - similar to your data vectors. The vector database then computes the similarity between the search query and the collection of data points in the vector space.*\n\n### Vector Databases are Fast\n\nThe best thing is, vector databases can\u00a0**query large datasets**, containing\u00a0**tens or hundreds of millions of objects**\u00a0and still\u00a0**respond**\u00a0to queries in a tiny\u00a0**fraction of a second**.\n\nWithout getting too much into details, one of the big reasons why vector databases are so fast is because they use the\u00a0**Approximate Nearest Neighbor**\u00a0(ANN) algorithm to index data based on vectors. ANN algorithms organize indexes so that the vectors that are closely related are stored next to each other.\n\nCheck out this article to learn\u00a0[\"Why is Vector Search are so Fast\"](/blog/why-is-vector-search-so-fast)\u00a0and how vector databases work.\n\n### Why are there Different Distance Metrics?\n\nDepending on the machine learning model used, vectors can have ~100 dimensions or go into thousands of dimensions.\n\nThe time it takes to calculate the distance between two vectors grows based on the number of vector dimensions. Furthermore, some similarity measures are more compute-heavy than others. That might be a challenge for calculating distances between vectors with thousands of dimensions.\n\nFor that reason, we have different distance metrics that balance the speed and accuracy of calculating distances between vectors.\n\n## Distance Metrics\n\n- [Cosine Similarity](#cosine-similarity)\n- [Dot Product](#dot-product)\n- [Squared Euclidean](#squared-euclidean-l2-squared)\n- [Manhattan](##manhattan-l1-norm-or-taxicab-distance)\n- [Hamming](#hamming)\n\n### Cosine Similarity\n\nThe cosine similarity measures the angle between two vectors in a multi-dimensional space \u2013 with the idea that similar vectors point in a similar direction. Cosine similarity is commonly used in Natural Language Processing (NLP). It measures the similarity between documents regardless of the magnitude.\n\nThis is advantageous because if two documents are far apart by the euclidean distance, the angle between them could still be small. For example, if the word \u2018fruit' appears 30 times in one document and 10 in the other, that is a clear difference in magnitude, but the documents can still be similar if we only consider the angle. The smaller the angle is, the more similar the documents are.\n\nThe cosine similarity and cosine distance have an inverse relationship. As the distance between two vectors increases, the similarity will decrease. Likewise, if the distance decreases, then the similarity between the two vectors increases.\n\n\nThe cosine similarity is calculated as:\n\n![Cosine Similarity](img/cosine.png)\n\n**A\u00b7B**\u00a0is the product (dot) of the vectors A and B\n\n**||A||\u00a0and\u00a0||B||**\u00a0is the length of the two vectors\n\n**||A||\u00a0*\u00a0||B||**\u00a0is the cross product of the two vectors\n\nThe\u00a0**cosine distance**\u00a0formula is then: 1 - Cosine Similarity\n\nLet's use an example to calculate the similarity between two fruits \u2013 strawberries (vector A) and blueberries (vector B). Since our data is already represented as a vector, we can calculate the distance.\n\nStrawberry \u2192\u00a0`[4, 0, 1]`\n\nBlueberry \u2192\u00a0`[3, 0, 1]`\n\n![Cosine Example](img/cosine-example.png)\n\nA distance of 0 indicates that the vectors are identical, whereas a distance of 2 represents opposite vectors. The similarity between the two vectors is 0.998 and the distance is 0.002. This means that strawberries and blueberries are closely related.\n\n### Dot Product\n\nThe dot product takes two or more vectors and multiplies them together. It is also known as the scalar product since the output is a single (scalar) value. The dot product shows the alignment of two vectors. The dot product is negative if the vectors are oriented in different directions and positive if the vectors are oriented in the same direction.\n\n\n\nThe dot product formula is:\n\n\n\nUse the dot product to recalculate the distance between the two vectors.\n\nStrawberry \u2192\u00a0`[4, 0, 1]`\n\nBlueberry \u2192\u00a0`[3, 0, 1]`\n\n![Equation 1](img/equation1.png)\n\nThe dot product of the two vectors is 13. To calculate the distance, find the negative of the dot product. The negative dot product, -13 in this case, reports the distance between the vectors. The negative dot product maintains the intuition that a shorter distance means the vectors are similar.\n\n### Squared Euclidean (L2-Squared)\n\nThe Squared Euclidean (L2-Squared) calculates the distance between two vectors by taking the sum of the squared vector values. The distance can be any value between zero and infinity. If the distance is zero, the vectors are identical. The larger the distance, the farther apart the vectors are.\n\nThe squared euclidean distance formula is:\n\n\n\nThe squared euclidean distance of strawberries\u00a0`[4, 0, 1]`\u00a0and blueberries\u00a0`[3, 0, 1]`\u00a0is equal to 1.\n\n![Squared Euclidean  Equation](img/l2-equation.png)\n\n### Manhattan (L1 Norm or Taxicab Distance)\n\nManhattan distance, also known as \"L1 norm\" and \"Taxicab Distance\", calculates the distance between a pair of vectors. The metric is calculated by summing the absolute distance between the components of the two vectors.\n\n\n\nThe name comes from the grid layout resembling the streets of Manhattan. The city is designed with buildings on every corner and one-way streets. If you're trying to go from point A to point B, the shortest path isn't straight through because you cannot drive through buildings. The fastest route is one with fewer twists and turns.\n\n\n\n### Hamming\n\nThe Hamming distance is a metric for comparing two numeric vectors. It computes how many changes are needed to convert one vector to the other. The fewer changes are required, the more similar the vectors.\n\nThere are two ways to implement the Hamming distance:\n\n1. Compare two numeric vectors\n2. Compare two binary vectors\n\nWeaviate has implemented the first method, comparing numeric vectors. In the next section, I will describe an idea to use the Hamming distance in tandem with Binary Passage Retrieval.\n\nLet's use an example to calculate the Hamming distance. Imagine we have a dataset containing a variety of fruit and vegetables. Your first query is to see which item pairs best with your banana pancakes. To achieve that we need to compare the banana pancakes' vector with the other vectors. Like this:\n\n| Banana Pancakes | [5,6,8] | Hamming Distance |\n| --- | --- | --- |\n| Blueberries | [5,6,9] | 1 |\n| Broccoli | [8,2,9] | 3 |\n\nAs seen above, blueberries are a better pairing than broccoli. This was done by comparing the position of the numbers in the vector representations of foods.\n\n#### Hamming distance and Binary Passage Retrieval\n\nBinary Passage Retrieval (BPR) translates vectors into a binary sequence. For example, if you have text data that has been converted into a vector (\"Hi there\" ->\u00a0[0.2618, 0.1175, 0.38, \u2026]), it can then be translated to a string of binary numbers (0 or 1). Although it is condensing the information in the vector, this technique can keep the semantic structure despite representing it as 0 or 1.\n\nTo compute the Hamming distance between two strings, you compare the position of each bit in the sequence. This is done with an XOR bit operation. XOR stands for \"exclusive or\", meaning if the bits in the sequence do not match, then the output is 1. Keep in mind that the strings need to be of equal length to perform the comparison. Below is an example of comparing two binary sequences.\n\n\n\nThere are three positions where the numbers are different (highlighted above). Therefore, the Hamming distance is equal to 3.\u00a0Norouzi et al.\u00a0stated that binary sequences are storage efficient and allow storing massive datasets in memory.\n\n## Comparison of Different Distance Metrics\n\n### Cosine versus Dot Product\n\nTo calculate the cosine distance, you need to use the dot product. Likewise, the dot product uses the cosine distance to get the angle of the two vectors. You might wonder what the difference is between these two metrics. The cosine distance tells you the angle, whereas the dot product reports the angle and magnitude. If you normalize your data, the magnitude is no longer observable. Thus, if your data is normalized, the cosine and dot product metrics are exactly the same.\n\n### Manhattan versus Euclidean Distance\n\nThe Manhattan distance (L1 norm) and Euclidean distance (L2 norm) are two metrics used in machine learning models. The L1 norm is calculated by taking the sum of the absolute values of the vector. The L2 norm takes the square root of the sum of the squared vector values. The Manhattan distance is faster to calculate since the values are typically smaller than the Euclidean distance.\n\nGenerally, there is an accuracy vs. speed tradeoff when choosing between the Manhattan and Euclidean distance. It is hard to say precisely when the Manhattan distance will be more accurate than Euclidean; however, Manhattan is faster since you don't have to square the differences. You want to use the Manhattan distance as the dimension of your data increases. For more information on which distance metric to use in high-dimensional spaces, check out this paper by\u00a0[Aggarwal et al.](https://bib.dbvis.de/uploadedFiles/155.pdf)\n\n## How to Choose a Distance Metric\n\nAs a rule of thumb, it is best to use the distance metric that matches the model that you're using. For example, if you're using a\u00a0[Siamese Neural Network](https://en.wikipedia.org/wiki/Siamese_neural_network)\u00a0(SNN) the contrastive loss function incorporates the euclidean distance. Similarly, when fine-tuning your sentence transformer you define the loss function. The\u00a0[CosineSimilarityLoss](https://www.sbert.net/docs/package_reference/losses.html#cosinesimilarityloss)\u00a0takes two embeddings and computes the similarity based on the cosine similarity.\n\nTo summarize, there is no \u2018one size fits all' distance metric. It depends on your data, model, and application. As mentioned above, there are cases where the cosine distance and dot product are the same; however, the magnitude may or may not be important. This also goes for the accuracy/speed tradeoff between the Manhattan and euclidean distance.\n\n> Use the distance metric that matches the model that you're using.\n\n## Implementation of Distance Metrics in Weaviate\n\nIn total, Weaviate users can choose between five various distance metrics to support their dataset.\u00a0[In the Weaviate documentation](/developers/weaviate/config-refs/distances#distance-implementations-and-optimizations)\u00a0you can find each metric in detail. Weaviate makes it easy to choose a metric depending on your application. With one edit to your schema, you can use any of the metrics implemented in Weaviate (`cosine`,\u00a0`dot`,\u00a0`l2-squared`,\u00a0`hamming`, and\u00a0`manhattan`), or you have the flexibility to create your own!\n\n### Distance Implementations and Optimizations\n\nEven with ANN-indexes, which reduce the number of distance calculations necessary, a vector database still spends a large portion of its compute time calculating vector distances. As a result, it is very important that the engine can do this not just correctly, but also efficiently.\n\nThe distance metrics in Weaviate have been optimized to be highly efficient using \"Single Instruction, Multiple Data\" (\"SIMD\") instruction sets. Using these instructions, a CPU can do multiple calculations in a single CPU cycle. To achieve this, we had to write some of our distance metrics in\u00a0[pure Assembly code](https://github.com/weaviate/weaviate/blob/master/adapters/repos/db/vector/hnsw/distancer/asm/dot_amd64.s).\u00a0[Here is an overview](/developers/weaviate/config-refs/distances#distance-implementations-and-optimizations)\u00a0of the current state of optimizations; including which distance metrics have SIMD optimizations for which architecture.\n\n### Open-source Contributions\nWeaviate is open-source and values feedback and input from the community. A community member contributed to the Weaviate project by adding two new metrics to the 1.15 release. How cool is that! If this is something you're interested in,\u00a0[here](https://github.com/weaviate/weaviate/tree/master/adapters/repos/db/vector/hnsw/distancer)\u00a0is the repository with the implementation of the current metrics.\n\nimport WhatNext from  '/_includes/what-next.mdx'\n\n\n", "type": "Blog", "name": "Blog Distance-metrics-in-vector-search", "path": "blog/2023-08-15-distance-metrics-in-vector-search/index.mdx", "link": "https://weaviate.io/blog/distance-metrics-in-vector-search", "timestamp": "2024-05-08 10:51:45", "reader": "JSON", "meta": {}, "chunks": []}