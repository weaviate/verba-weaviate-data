{"text": "Karen Beckers, Data Scientist from Squadra Machine Learning Company, gives insightful information about how to use vector ... \n[Music] thank you so much for checking out the eighth episode of the we va podcast today i'm joined with karen beckers a data scientist at squadron machine learning company karen thank you so much for coming on the podcast can you tell us about how you're using the we vv8 vector search engine yeah thank you for having me i'm really excited as you said i'm a data scientist it's quite a machine learning company and we're a company that provides customers with ai and data science solutions on one hand we have our own products um and we focus on suppliers wholesalers and retail and with those products we provide solutions for smart product data management supply chain optimization and as we like to call it ai commerce instead of e-commerce but on the other hand we do projects and these projects we provide data science solutions for any specific task that a customer might have so and this is actually where viviate comes in because for one of our customers an e-commerce customer we're building a search engine and we're doing this a little bit different than the more traditional methods of search so actually um building the search engine based on the image relevance so we started out with the proof of concept of only a thousand products uh just to see if it was actually possible can we get relevant results um results are relevant to the query based on the images and at that point we were just brute forcing it so we were just we calculated the vectors which is brute forcing the the vector similarity and of course that doesn't scale if you're going to larger data set so we had to transfer uh transform to a different platform and um so that's when we we stumbled upon wev8 and um since then it's it's been so much easier the whole process has been so much easier and yeah it's really it's really great to be to use with yet yeah i think that that idea of using an image to search for products is so interesting and i've seen uh demos of say like the fashion mnist data set to give kind of like a illustration of what this looks like where you say take a bag and then hit the data set of bags to find the most similar looking bag what so what kind of data sets are you using to kind of uh bring that to the real world to say like maybe pinterest have an api or like how do you get the data set of real world uh these ecommerce kind of pictures um so the the data that our customer uses he um they they scrape the data from from brands that they they are selling and we're using a zero shot model that is actually able to map textual texts and images to the same vector space um and for search that actually works worked well and we're also using it for classification um but for classification you can do some like after the vectorization you could do a linear probe to you know match the the vector a bit more a bit better to the categories that sounds so interesting like that pipeline of um so for so you could have an image image search a text to image search and then you can also append a classifier with the high level discrete variables of bag shoe or maybe brands like nike adidas or that kind of thing so what kind of um so with the text to image search and the image to image search what kind of are the two different things that go into each of those things um so for the thing if you use text to image then um you can also expand that to text to text and then combine those and the difference with image to image is that you only have that image and um [Music] so we're using the model that i just mentioned that maps text to the same vector space as images but we're also using only text models um so if you if you do text to image then you can use both but if you do image image then you only have that image model so you only can make use of the information that is stored in the image um which is also a challenge but that's interesting of course yeah it's so interesting you can also uh like label it with like you know the symbolic metadata that describes the images as well to have that kind of maybe sanity checking for the image image search i'm curious about how you evaluate the um do you annotate say like the top k and you have annotations of the best nearest neighbor for images or how do you kind of evaluate an image search system at scale yeah that's that's difficult um because we don't have a golden standard of like a set of queries that we we know that these products have to come up in the top 10 results we don't have that so that's difficult um but we're now using a tool called cupid where you um where you can they the tool lists your top 10 results and you can annotate them yourself with either binary battery uh measurements or on a scale and then as you update your algorithm and different results come come up then that automatically updates as well so you at least have a quantitative idea of how good the results are yeah that's so interesting the um so cupid is a labeling software is that correct i'm really curious about these kinds of things and uh can you tell me a little bit more about what cupid does yeah i'm actually just getting into cupid myself we started on that uh i think two weeks ago so that's all that's very new but um it's not not in that sense a labeling software it's um it's maybe more relevancy tool for any search engine so it's actually um [Music] it's pre-built for solar or elasticsearch um but we just um rewrote our our weavit output so that it looks like elastic output so that the tool can actually interpret that um but i think just for any search engine you need a tool to um to score your results to say okay how how well is my my system actually doing and cupid is just one of those tools that that can help you with that yeah thanks so much for that i i love following along with these labeling softwares and i see like the big name things like say um scale ai and snorkel ai the things that i think most people are aware of but also been looking into things like rubrics and learning more about these tools i think it's such an interesting uh part of this so the next question that i love to ask on the web podcast and i think especially for image to image and text to image search is such an interesting different angle is usually we're talking about text to text search where we have all these say sentence transformers and the hugging face model hub i'd say is you know mostly geared towards natural language processing but so how well does say using a resnet 50 trained on imagenet uh just right off the shelf taking those pre-trained embeddings and then using that to image to do image to image search in the transferring domain from you know the thousand or so classes in imagenet into e-commerce how's that off-the-shelf transition from pre-trained model into application um that's a good question um yeah so the the model that we're using it's not not the pre-trained resonant or it is um one of the versions actually resonant based um but this one is it's just trained on the basically what they say is like whole wikipedia whole internet um with the images and the labels the metadata labels that are with the images um so that gives you a whole like a very broad broad scale of what what the the model actually knows so that's that's really great but um for a really funny example actually what we saw that the model was trained up until i think maybe 2019 so pre-covered and so it doesn't recognize face masks so that's actually very funny but of course now face mask you can't you see them everywhere there's such a normal normal view on the street and the model just doesn't know those so that's that's difficult wow that's such an interesting part of the um the updating information and that's one of my favorite things about decomposing deep learning tasks into retrieve and then read because uh when you separate it into a separate retrieval task you can easily update the information compared to having one running supervised learning model and you just try to like update the information by doing further gradient updates with some uh new data set but i love how you could just just you know drop in a new retrieval data set with the face mask to have that adaptation and yeah i think that's such an interesting part so it so is this the open ai clip model that you're using yeah and yeah exactly yeah i really like this paper it's uh they use this weight space ensembling technique where they go from the uh from the zero shot generalization of clip train on all of the internet and then they fine tune it on particular data sets and these are like academic benchmarks it's not like quite like uh e-commerce product matching but they show that if you do that weight space ensembling along the path from the pre-trained clip checkpoint into the fine-tuned representation then you get like the both the zero shot flexibility as well as that like in distribution fine-tune things so do you have uh like particular ways of evaluating that that idea of the out-of-district like the how you would say how flexible it is how robust it is compared to the evaluation on the fine-tuned data set um no that's actually very difficult right now because we we don't have we haven't fine-tuned clip ourselves mainly because we don't have a large enough data set that's actually labeled that was also um the challenge that brought this customer to us their data isn't labeled um and we had a we had a a blog on medium actually about image classification uh so that's how they came uh came to us um [Music] so that was already a challenge for us okay we we can do image classification but it's difficult if we don't have any labeled data so that's why we we turned the clip and then that that that actually turned out to to work really well um and we're all we're also yeah we're constantly thinking um about how can we how can we improve back when we we fine-tune the model but if we don't have any any labeled data then that's really difficult so yeah i see the um like the idea of using transfer learning to label data for some other domain or say um yeah like this idea of maybe you might call it programmatic labeling where you have the pre-trained model that you're now using to label data it kind of reminds me of say like semi-supervised learning as well yeah you know propagate it and go label the unlabeled data and yeah this is one of the things like uh coming back to these say like uh open source labeling software tools the rubrics framework has a great interface for uh this this idea that we're describing if you use the clip model to label your unlabeled data set and i think that's such an interesting topic with like bootstrapping unlabeled data sets and then maybe pushing those to say like the hugging face data sets or something where people can collectively use them and build on them what kind of a vision and language data sets like inspire you to work on them maybe like visual question answering or visual common sense reasoning i see maybe like a lot of vision based robotics where the labeling is uh like follow instructions through some kind of thing something like that what kind of vision and language data sets interest you the most um um i have to say like except for we're not doing a lot of vision vision projects right now so we're mainly using using clip for that we we do use a lot of language models um there was this project um where we trained the team the google t5 model and we we use it to train a paraphraser um and we have we've been using a bird scentsy bird uh um [Music] so um i think yeah maybe those are maybe the more standard models but um i think right now the model that that has the most promise um that has been proven to be the most promising for a lot of projects that we're we're working on is probably sentence bird yeah sentence bird is so cool i love these like the siamese architecture of the decomposition of bird into just something that's intentionally producing embedding so so the um yeah the text description so writing some kind of description that describes the product is that kind of it's an interesting thing to me is whether you want to take like an image of the shoe that you've seen on on maybe you're watching a tic toc video and you see a shoe that you like and you want to just take a picture of it and use that to index like amazon or or whatever to get the product but compared to writing a language description of the thing you're looking for do you think the the language description is maybe a more uh say like robust path for search than image based search not sure actually because you know as they always say like a picture says more than a thousand words i think that probably uh i think that probably is true for this as well right um you can you always describe the the image or the product um from your own imagination and if that doesn't really match um or is just differently nuanced from um how it's represented in vector space then that um that could could be problematic and maybe maybe an actual image is more robust and you could you could probably do both as well right like have the um you send the image and then maybe it's like just inspiration and you're trying to like um i like this sneaker but maybe something that's more so i don't know what kind of textual description would expand on this particular sneaker but the combination of language and text to search for something how would that kind of thing work yeah that's really interesting i thought um i read an article a while back they're saying that google is actually working on that right now that you can upload an image and use a textual description um i'm really interested to see how that how that would work um i don't have any experience with that um i am i imagine yeah i maybe you can combine the vectors in some way say if you if you would would map the vectors to the same vector space and maybe you can calculate an average of the two vectors combined that has the that both information the information of both both entities combined i'm very interested to see how that would work yeah i love that idea and like also maybe combining it with like the symbolic annotation which i want to ask about next but uh yeah i love the google say the landmark detection competition they host on kaggle i think they host that every year where you have an image of say eiffel tower and then you index their database to find the other image of the eiffel tower and i think this is one of the most uh long running nearest neighbor open source competitions on kaggle and you had such an interesting idea of adding on the text to further enhance that like image image text image kind of search thing so i want to talk about symbolic filters and like symbolic search seems to be basically solved where if you have the you know symbolic annotation on the products and the query you can just exactly match them and say they're exactly matched so could you extract the symbolic filters um from the text description so people don't have to be uh like entering their schema and have this kind of fixed schema whereas they can just write natural language descriptions and then from there you can create a symbolic kind of tagging from it yeah we we do um we do that in in some sense um where we have one of the products actually that we we have at the machine learning company is a feature extraction tool where you can upload a product description and then you also upload your your data model so you you know the the whole taxonomy you know what features there are and what the possible values are and then the algorithm extracts um those features from from the text um so we're also doing that for this customer as well and we're trying to enhance it with image feature extraction so um where we accept for example color uh you have a list of colors and then you can say okay what color matches this uh product best that's super cool that because that kind of adds like the symbolic filtering of say regular expression matching in text to images where you can you know find the color exactly by using the um by having just the pixel that appears in the thing uh can you so can you tell me a little more about yeah this mapping from like unstructured data like yeah images or just a language description you describe your product with into structured data where you have that data schema you have that kind of predefined model of what your data looks like how does that mapping from unstructured to structured data look um so on text based we we mainly use regular expressions but [Music] of course you can also provide a list of synonyms and then you have a really elaborate system that can um extract those those elements from the text but then of course there's also the possibility when they're the the product is actually red shoes but then the description says you can also pair this really well with your blue jeans so then extract red and blue and for that we actually again use some vector representation to determine which color is actually best fitted to the product so use the context of the whole text to determine which value is actually true yeah i love that dimension of adding symbolic filters even within the same data domain so so for like searching for images if you search not just for say another shoe but like then a whole outfit to be returned and you add the symbolic filter on to what's being returned and this is kind of like one of the more technical details in wv8 that i really like talking about is the um like the pre-filtering step on h sw where they actually cook these symbolic filters into this massive vector index that can do something like search for through 100 million vectors of pictures of shoes and e-commerce products i think it's so interesting though uh so on that also though coming back to symbolic and structured data and we talked about fusing embedding spaces of image representations and language representations could we form say using techniques like contrastive learning or maybe mass auto encoders representations of the structured data as well and then have that further embedding space alignment between images language and then the structured data as well through some deep learning technique to encode the structured data that's a very interesting idea um [Music] i think if if we can that would of course be very helpful because with the symbolic data you're almost all restricted to to keywords or a list of synonyms and that's never complete so if we can actually uh have some kind of embedding space that incorporates everything um that would that would be awesome of course yeah yeah i think like the multimodal learning the ability to combine all these different kinds of data i i think that's what's driving like what's on the frontier of what will drive so much progress in this technology kind of so with these advances in deep learning for search and e-commerce can you maybe tell me some things that uh not to name any particular e-commerce platforms but some of the things that are missing that really need to be added or on the like the front the cutting edge of like uh new features in e-commerce um [Music] with with deep learning well actually i think um maybe what we're doing and what we're trying to do right now and retrieving products based on the images um i think that would help a lot if that uh of course it's still an ongoing process and it's very difficult to to tune it tune it well and and get the proper results because sometimes you do need textual fields to get the actual good results but if if we can um already establish a baseline based on the images that i think a lot of uh e-commerce platforms uh could benefit from that that if their suppliers uh provide the data then of course there's always noise in that and supplier maybe some suppliers they don't provide the data in the correct format and so if we can enhance that with the images or some some automated uh in an automated fashion then that would really be a helpful thing yeah i love the idea of say when you see some funny thing at home goods and you take a picture of it and then it indexes like etsy ebay amazon and you have this whole like um that idea of going from an image into the whole catalog of all the things on the internet i think is like such an interesting topic with things that might be on the frontier and like new ideas in e-commerce so so i wanted to ask you about what you think about kind of open source generally and the idea of say providing demonstrations of what your technology can do on say hugging face spaces and how there are these easier interfaces to build and share user interfaces with tools like say gradio or streamlit as well as kind of the general topic around say open sourcing data sets and publishing papers um i'm actually not familiar with with gradio streamlit but in general open source datasets and openness models and the whole open source community i think it's it's it's actually for us it's great because a lot of times we get customers that like they're not uh their company isn't that large so they don't have that that a great amount of data and the data is often not not structured well or it's it's really noisy so that is always difficult you know that there is an ai solution that will work for them but if you don't have the data to train that model yourself then that's very difficult so um using those open source models and being able to to base like have a foundation based on those and then being able to fine-tune it with the the smaller set of data that you do have that that is really helpful yeah that like collection of pre-trained models and like coming back to the pre-trained uh siamese bur sentence bert i'm not sure what the s stands for in esper i think it might be sentence for but yeah but but yeah like the the boom in the pre-trained architectures it enables such a like starting point and also like kind of in taking down the cost of training these models because you have the pre-trained checkpoint so you don't need even with your smaller data set generally you don't need to uh tune it for as long do you see the cost as being one of the other like big selling points of these pre-trained models yeah exactly could we uh if you would for example gt2 or even gp3 that's not even realistic to train it yourself with with the data and the the cost we can we can have um so yeah if we if we don't have to do that ourselves and that that's that's really helpful that's that's great we can we can't use that we can't mimic that ourselves as a as a smaller smaller company not not one of the the large larger tech companies in the in the us of course yeah the cost of it is such an interesting part of deep learning because it's yeah to push the cutting edge of it you need to really have expensive computers and you know distributed computers and all the pretty interesting stuff have any recent trends and kind of efficiency gains in deep learning been interesting to you um yeah um there comes one model to mine i actually forgot its name um yeah let me think about it i thought it was really really interesting because of course with all the bigger models that are out there right now um there's a lot more computational cost a lot of more in that sense electricity use and all the data centers that are constantly running and um yeah we're trying to be a little bit more environmentally friendly but data science isn't really and so i read about this this model that um that uses a lot less capacity um that is actually um i thought it was was a multitask some multitask model but it would only um activate those neurons that i would actually use and then and that way it it use a lot less computational uh power i'm yeah i forgot the name about it but i thought that was very very interesting if we could um from a data science perspective be a little bit more environmentally friendly as well yeah i think you're talking about the um the lottery ticket hypothesis and the in the masks that they put on the on the network and it shows that you only need to say the sparse sub network to uh achieve the same performance as the full dense network and yes i think as cool as that sparse masking idea is i think they still have some like hardware optimizations before you can really completely realize the efficiency gains of applying the masking onto the dense architecture because i think right now you're still doing the same computation you just add the mass to it no okay so i think there's still uh some interesting things to be developed in that kind of idea so kind of more broadly with like um data science and then like we v8 vector search and i think it's really interesting maybe this transition and database technology from sql relational databases into nosql the flexibility of json kind of data the way that it lets you just adaptively update your schema and now into wev8 where we have the flexibility of nosql but plus this deep learning functionality what do you think about this kind of trend in this tooling for data scientists uh yeah i think it's really helpful uh especially since we're generally in data science we're moving a lot more towards using vectors than than any other type of data so having the infrastructure to actually use that and make because the i think the previous generation of databases they're they're they're very they're really well established and you they're they're great to use um so going into that new area of vectors is they may be a bit more tricky in the database sense so using a tool like like we've a way where you can actually uh have a solid base um for your data then that that's that's really great move forward yeah i think there's a lot of interesting topics with um how data scientists will use vector search functionality and as particularly as we've been talking about the idea of using unstructured data more so where you just have images or just freeform language descriptions of whatever data you're trying to describe maybe and then you could go into all sorts of things like audio video biological data graph structured road networks all these like unstructured kind of data sources that you can use now for standard kind of data science do you think the downstream functionality as we say describe like nearest neighbor search as retrieve and then read being you take the retrieval part and then use that as input to some kind of supervised learning model do you think supervised learning models will become a more common tool for data scientists i know we maybe do things like regression analysis and look at the coefficients of regressions and do the correlations do you think maybe things like say retrieving from your data and then using a text-to-image model to generate some kind of say visualization of your data or any kind of these creative supervised learning applications do you think that will become more popular with say just like the average data scientist who isn't necessarily like a machine learning research scientist um would you maybe rephrase that i'm not sure i understood it yeah like um the idea of labeling pairs of data for like labeling x y pairs and trying to bootstrap some some kind of task do you think that's something that could become more common with like a typical day in the life of a data scientist or in comparison to maybe say um using mostly say like descriptive statistics looking for like statistics in the distributions of data and the kind of say traditional sort of producing charts and that kind of objective of data science oh in that sense yeah i do think uh that will become more um [Music] we will see that a lot more because i also think that um the models that are out there and the tooling that is out there to use the models is um how do you say it's more accessible than it was before so not only for actual data scientists but also for people um they're not really data scientists but they're more accessible to use um i actually had a header id once that maybe in the future there will be like a plug-in in in excel where you you load your data and you just you push a button in excel and say okay they're now now training gradient descent training simple regression model or even an ensemble method um that and then maybe not even people who know about the data science can use those those methods oh i love that idea even people who don't even know about data science like you could just have a natural language very fuzzy description of what you're trying to do with your data and and it could interpret that and map it into a function i've seen um hugging face added to google sheets and yeah that kind of integration i think is so interesting of adding deep learning functionality into say just like excel spreadsheets but then also as we talked about integrating the unstructured data as well which i think is such an interesting part of of the weevie because with wva you can still have your structured csvs if you want to look through that and have that kind of json data but then you can also add images and language and things that are more uncommon to this so so with your use of uh we v8 how has that been do you have any things that like you know maybe you wish were added to ev8 um well there were actually already a couple of things that um when we started out with wev8 i think it was the end of november um some functionalities that we really needed weren't there yet so for example the the openai click model that we were using they um didn't have it out of the box in in we've yet yet so we of course we could import our own vectors and then use a new vector search with that um but then the downside is that for every query we would have to vectorize the query ourselves so send an api request to our api send it back and all the latency issues that come with that um so that um they already that the clip model is actually integrated in with it right now so that's already great that that's really helpful um but also what we're i've heard that they're that they're not currently working on the beam 25 integra implementation so we're really looking forward to that because as you said we we can use the symbolic data to filter um but then the integration of doing actual keyword search retrieving results based on keyword search and then combining that with the image results is currently very difficult because there's no score coming from the keyword search and um there is a score from the image search but then how how you have to combine it in some way how they're doing that if you don't have scores for both so we're really looking forward to that to that implementation but i've heard they're working on it right now so yeah that's so interesting and uh kind of the first thing you said the the modularity of the different vectorizers or the different embedding models how you have the open ai embedding model in embedding models like clip or say the um the latest cpc code cpc text and how you can put the model into eva so you don't need to have two separate apis and then i love this idea of the combination of different retrieval models in the same api so you can have the bm25 and you can have your dense esper representation all in that same api and then you start getting into this really interesting topic of how do we interpret this kind of fuzzy distance and combine the outputs of tfidf bm25 these lexical sparse retrievals with dense esberg how how are we supposed to fuse the outputs of them do you have any further ideas on this maybe say like um coming back to the labeling software maybe we could use labeling software to start annotating the uh the different scores that come out of our model and then have like another neural layer on top of that that uh interprets the outputs from say five different retrievals where it could be uh tf idf bm25 and then say three different variations of training your esber model because the the ways that you fine tune it right could lead to different behavior too many ideas on i like how we can combine those outputs yeah that's actually very difficult and we we've been stuck on that for quite a while now um but it's very interesting what you're saying we also have that idea that we have all these different components so we have the image image results but we're also doing a text similarity on the product names and we can do text similarity on ever any other field that we have but then also do the keyword search and then all those different components how are we going to combine those and we actually had an idea of training maybe just a very simple random forest and use those weights um actually the or the the the feature importances of that model uh to use as weights for for all the different components um but uh right now we don't have a fixed solution yet so if you haven't have any then ideas always welcome because it's a very difficult topic and it's still ongoing and it will probably be ongoing for a while i don't think it will be solved very soon yeah i've discussed this with some uh of the we vva team and we love this maybe calling this human computer interaction this idea of how we interpret these fuzzy distances from neural searches compared to as bob describes the binary output of when you search for something in a traditional search engine it either contains it or it doesn't compare to this where it's like this is 80 similar to what you said and you're like okay so what do i make of this and i i love what you said with the random forest layer i think maybe in my studying of deep learning i become like overfit to this idea of putting a neural network onto everything but when you just have these uh you know five different outputs you could probably just use a simpler like uh just a random force or uh like really like um not not using a neural network immediately because um the well the whole the whole field of ai where especially the past couple of years everyone's really focused on neural neural techniques but um it's easy to forget the diff the other other techniques are more traditional techniques that are actually very good as well and they are very easy to train do not have use so many computational computational capacities so that that's i always like the ideas of keeping it simple and then having a model that actually works yeah actually worked is important like i think it's because of like the the appeal of using unstructured data is so appealing with deep neural networks that you forget that like once you have a tabular structure once you have a symbolic schema now say like xgboost is the new state of the art and um and deep learning is going to be like harder to adapt and maybe you have things like missing values all these other things that come with like these more traditional kind of data science tools and that thinking on that kind of aspect of it so i think we've covered a ton of interesting topics from uh image search text search and the frontier of applying deep learning for search into e-commerce platforms uh is there anything else sort of that we might have missed with the particular application domain of e-commerce with deep learning for search um [Music] well what's interesting in the in e-commerce is that um so many of the the models the language models right now they're trained on large stretches of text but often in e-commerce you don't have that especially if you look at the queries they're generally like two three words and that's it and that's that's also interesting to look at if the i know i know there are some models that are more trained on on shorter stretches of text um um but there there that is also a challenge for e-commerce there isn't really a standard language model that is especially trained for e-commerce yet so um that will be interesting in the future if someone would be able to train that i think that would help a lot of people yeah i love that thinking about that like um style shift it's like kind of an abstract concept i mean it's very concrete when you describe style as short sequences of text compared to long sequences of text but as we've uh been doing the po we va podcast say um you're training a chatbot model and you're going from reddit conversations into real podcast conversations and kind of that style shift from the underlying domain is such an interesting thing and i guess like with my kind of like academic side of this i i think these data augmentation techniques could be really interesting as i've kind of presented this idea of maybe learning a style transfer model so like a textile transfer model that could augment your data set by using by using this kind of generative model i know a lot of people who really build these systems for real are kind of skeptical about that idea of adding another model into the pipeline of generating training data what do you think about synthetic data and data augmentation as a technique to improve these models um [Music] i think um it's an interesting idea and i think in many cases it can work really well but um i always think it's a bit tricky um if your your data isn't uh great or it isn't isn't that clean and it's very difficult to um well if you keep training with the data then uh it will only become more noisier so that yeah i think that that's definitely something that you have to look out for but in general um yeah using some data augmentation techniques can be really really helpful of course yeah yeah i think especially if you're trying to use say generative adversarial network scans for data augmentation then you have this problem of mode collapse and then if you sample from that you're just adding that mode collapse bias and clustering your your data even more on that one point so it and then just generally that idea of like compounding errors of your generative model makes an error and now you put it into your discriminative model and it just runs away with the air i like techniques like maybe variational auto encoders or these diffusion models that maybe have these explicit objectives around encouraging diversity in the generative model i think with these language models the way that you do the tree structured decoding like how with like open ai's codecs they generate like a thousand different potential generations and they call this repeated sampling where they traverse the generation tree several times and then put that into the compiler to see what doesn't like have an error because this is a model that generates code so these kind of maybe tree structured decoding strategies could be a way to achieve that diversity in the generative model yeah that's very interesting yeah you know that like idea of the generative model but i think also just like the way that retrieve then read adds that interpretability is also such an important part of building these systems and so are you like inspired by say these recent advances like deepmind's retro model maybe facebook's internet augmented generation or openai's web gpt where we're seeing maybe less hype around this idea that like let's just build trillion parameter language models and maybe more of a recent idea into let's decompose the task into information retrieval and then supervised learning yeah i think um i think in general maybe i'm not such a believer in like one great ai that solves everything i think um dividing it into subtasks it will be a much more effective than uh than one one that one-size-fits-all yeah it kind of comes back into this like end-to-end like people like this end-to-end idea where you know instead of featurization and learning we just put it into one pipeline and maybe everything could be end to end but maybe sometimes we need to have a task decomposition and say like multi-task learning doesn't really always work that well maybe if you have things like as you mentioned the t5 framework that unifies these tasks into at least you kind of have like a similar gradient scale when you're trying to do multi-task learning that idea of like decomposing the end-to-end pipeline into subtasks what do you think about that kind of idea of just like let's make this whole thing end to end make everything continuous anything discrete let's put a continuous relaxation on it and just try to do gradients through this whole thing uh i'm not sure i don't it would be cool if it works but i i uh i have my doubts actually do you have similar doubts in that kind of scope of one model that performs all the tasks to the idea of off-the-shelf pre-trained embeddings like one model that say covers all the distributions of data do you think those two kind of ideas are the same the idea of end-to-end i can learn every task this gpt-n is the language model that does everything and then this idea that say like clip and is the embedding model for everything um [Music] well one of what i've learned from my experience with clip is that it is very useful as you as i said we don't have the data but um if you have any labeled data and you can find units for your specific task it's always better in my in my experience um but it's it's it's very useful to have the the general general model general embeddings as a baseline um but if if you can fine-tune it to any specific task then yeah that would always have my my uh my preference also for example the gt3 model i think gt3 is amazing and all the the all the capabilities it's really great but um with with the prompt engineering it's it's very difficult if to to steer it in your exact direction it can do so much but um what we've seen a lot a lot of times that it um it will generate great texts that are like you can't tell if they're they're human written or ai written but um it will also just make up stuff and since it's so well written you don't know that it's actually made up but a lot of times information is not true and then there's a is the problem like how are you gonna check if it's not true or if it is um and so it's it's difficult with such models to to steer it in into a direction that you you actually want so if you can fine-tune anything of course you can now find your gp3 as well and i think that that's also a great move forward to still leverage the capabilities of the model but then actually steer it into your direction yeah that topic of fine-tuning language models or steering yeah i like that steering word too it's really a really nice way of like trying to think of like ah move this big powerful [\u00a0__\u00a0] towards some useful direction and yeah i like maybe like two techniques like as we mentioned this idea of like generating a massive tree of potential decodings and then putting a discriminative model that parses that tree or maybe we've seen recent advances from openai on using reinforcement learning to train a reward model and then use that to kind of fine-tune the model or maybe you could just have supervised learning where you have some sequence to sequence data set and you um fine-tune it on that but i wanted to come a little bit back to this idea of fine-tuning versus off-the-shelf models particularly with these embeddings and i wonder like if you think that maybe there is a fine-tuning bias where as we say that like having labels it generally improves it but is that maybe because like the train test split is from the same distribution of labels and that might kind of bias your thinking into saying that well clearly i have this train test split and it's better on my test set once it's been fine-tuned on this train set compared to maybe sacrificing the robustness and the zero shot flexibility that is kind of like maybe like the key advertisement of these models like as you fine-tune it and you evaluate it and you say these labels have made it better is that because you know you're evaluating it on the same distribution of labels and you're not really doing say like data augmentation corruption tests or whatever like a domain shift test you could like i don't think there even are great tools for that do you think like you're not like maybe people aren't properly evaluating how robust their models are when they say that the fine tune model is better than the off-the-shelf model um well i think i agree with what you're saying that of course if you're you're fine-tuning then you're you're losing some of the the zero or the few shot capabilities um yeah of course that's true um i've never actually like fine-tuned on a general data set that wasn't specific to a task so i don't know if that would have helped like still like maintaining the zero shot capabilities um but i think um if you're fine-tuning for a specific task then that's uh if you you lose that the the other capabilities then um [Music] of course you should be aware of that but i don't think that should be problematic um but yeah you should definitely be aware then that if you're functioning for it for one task it won't scale to a different task yeah that's that's the that's something you will lose yeah do you think maybe those the off-the-shelf way to then the fine-tune model could be combined in that abstraction layer where we talked about putting a random forest layer on top of different outputs of retrieval or do you think like yeah like having some kind of random forest on top of the two from the off the shelf and the fine tune do you think that would be useful or is that just reaching for something um so then you would put the embeddings in a random forest and maybe i think i think even just the output of the distance to the query maybe like as we talked about just having like five distances returned from tfi fbm 25 and the esper it's like the two distances of the pre-trained and then the fine tune is it earlier i mentioned the weight space ensembling technique i think this paper was from the allen institute in the university of washington maybe others i'm i don't exactly remember but it's like this idea that you do an ensembling of the weights as you go along that path from the clip pre-trained checkpoint that open ai releases down into this fine-tune thing and you could have some way of combining the outputs from each of those two checkpoints and maybe generalizing that with our idea of the random force abstraction on top of these retrieval models yeah it's a very interesting idea not i i'm not really sure how that would work actually but um [Music] yeah i i think it would yeah it would be interesting i'm not i'm not really visually visualizing it right now yeah i think those diagrams and maybe the retrieval pipelines it's like maybe they need to be interactive diagrams or you can like yeah press plus and add more retrievals and stuff like that so as we've covered so many topics i have maybe like a meta question for you is just asking like um what are your favorite sources of information your favorite strategies to improve your skills as a data scientist um well there are a lot actually i i really love working with with people for example now for this customer that we're building a search engine we're working with people from all over the world and people who have a lot of experience in search and just being able to work with those people and get the knowledge from them uh doing some pair programming uh sessions with them and that that helps a lot and that's that's very interesting and i think that that's maybe my my favorite way but also just um like reading vlogs on on medium or um what watching the videos of the scale ai conference for example i think that's that's very interesting as well so yeah i think the i prefer it to be like in person that that's my my favorite way but they'll there are so many ways to to get new knowledge and it's it's unending [Laughter] yeah i agree the in-person thing definitely adds like having a conference or definitely adds a lot to it and it's funny also mentioning the scale ai conference that was such a superstar lineup of uh talks they had this year it really was if people are looking for maybe something to watch that scale ai conference you have like clement de lang richard sacha like a lot of these leaders of these massive companies that we're talking about most of the time so yeah that was a super cool thing so thank you so much karen i learned so much from uh talking to you about all these topics and uh good luck with these projects on ecommerce such an interesting application the squadron machine learning company it all sounds so exciting and cool yeah thank you very much connor [Music] ", "type": "Video", "name": "Weaviate Podcast #9 \u2022 Karen Beckers about the role of vector search in eCommerce", "path": "", "link": "https://www.youtube.com/watch?v=brUP1OEtQ5U", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}