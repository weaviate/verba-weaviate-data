{"text": "This meetup by Etienne Dilocker is all about the horizontal scalability of the Weaviate vector search engine, a feature that became ... \nthanks okay then officially welcome to today's meetup that has the title horizontal scaling uh which is yeah the the 1.8 release that we released today where we released the release candidate today and really today is going to be just about deploying vv8 in a cluster in a horizontally scaled fashion uh on a on a kubernetes cluster and uh the last meetup that we had was a lot of theory and i'm sorry the last meetup that i held was a lot of theories sort of around all the the architecture um that makes up vvate and today is sort of the counterpart to that like um we have taken all of that architecture and we have put it into vba and now we just want to want to use it so today also marks uh the as i just said the the day that we released the 1.8 uh release candidate which is i think one of the biggest if not the biggest um pull request that we merged on bv8 so far so 115 um commits there uh over three months in the making uh yeah lots of lots of effort lots of of help from a lot of people very cool stuff in it and something that i'm also super proud of is we when we started our architectural roadmap and i'll get to that in a second uh one thing that we said is that we will release this by the end of q3 and today is september 30th and hey we actually made that and when that when does that ever happen in software development that you estimate a date and um yeah and it actually works out so um let me quickly as i said today is going to be mostly sort of very practical but i want to very quickly sort of recap which is six slides or so a very quickly recap where we are on that roadmap so we started um the first thing was was just improving the performance of our hnsw implementation that we had way back in version 1.4 um and then we implemented the ls entry storage which made importing a lot faster because actually both of those releases made importing a lot faster uh but 1.5 was sort of the point where we said like yeah um riding is no longer the bottleneck and and this is sort of the the start to our scaling roadmap because we said uh scaling horizontally only makes sense if what we're scaling is already efficient because otherwise well what what good is it running on five nodes if you could get the same performance out of a single note so that's how we started this roadmap um saying sort of yeah first we want to get the max out of a single node and then we want to scale horizontally for that uh we had the sharded indices milestone which was completed sometime in august but it wasn't actually released yet mostly because the added value of charting your indices really only becomes apparent when you have a natural cluster which is the current milestone today we released one 1.8 um or the release candidate for 1.8 which has horizontal scaling uh in in um in parentheses here without replication that's the next step on the roadmap and i'll get to the the differences what that means in a second um yeah so so this roadmap is still ongoing but today um we have the first release that's horizontally scalable and replication and dynamic scaling is coming a bit later then so yeah uh that brings me to to talking quickly about uh sharding versus replication and maybe the limitations that we have here and that is also a perfect point to switch over to our docs which have also been updated for the release so i'm on the if you start out uh you end up on the the current release i'm on the the 1.8 release already which because it's just release candidate you get this it's wearing a warning here um yeah we've revamped this page and there's a lot more i'm not going to go into this into detail now but um there's a nice new overview about the architecture and we have a whole new section about scaling and in here we have sort of the the two major motivations of um why you would want to scale your your bb8 cluster um the first one and this is sort of the one that i'm mostly going to be showing today um is if you want to have a larger data set size than a single node can fit um the motivation is basically to chart your data set onto multiple nodes and then essentially if um yeah if onenote could only fit i don't know so so today an article came out by one of our community members who is running i think 60 million objects 60 million vectors on a single node so let's for the sake of arguments say that 60 million is the limit that one node could handle then you could add two nodes and you could have 120 million and and so on so this is the the major motivation um of sharding your data the second motivation would be higher a query throughput so if you have a single node and let's say you can handle making this up completely a thousand queries per second on that single note but your traffic increases then the motivation would be to scale to match that um that higher throughput and that is what will be possible with replication so this is something that's that's important to know as you chart your data set you don't actually gain anything on the querying side but you gain stuff on the importing side so so um you have more resources to import so that gets faster but not on query with replication that's going to be the other way around and then the the third one which is also a benefit of replication is high availability so if you have your data set running on um yeah let's say replicated on three nodes if one of those nodes nodes fail then you can still um yeah use the cluster normally the only limitation is going to be that throughput is slightly lower but then you could scale sort of for a high availability and we also have a summary of that over here in that section where you can say advantages when increasing sharding you can run larger data sets and you can speed up your imports the disadvantage is that even as you add more resources to your cluster you don't actually improve the query side and then the other way around for for replication the system becomes highly available and you can scale your query side linearly where the disadvantage is that even though you add more resources to your cluster the import actually doesn't speed up because now it has to be imported or replicated onto multiple nodes so this is the the sort of intro to that there's a lot more things that we can get into but yeah i wouldn't tackle those as part of the live demo which yeah which will dive right in basically um so for the first part let's let's see i have prepared a kubernetes cluster um and in there i've created a namespace and this namespace is currently completely empty so if i look at all the resources in that first connect always take some time let's see if the cluster is still reachable that would be nice that really took long i think subsequent queries should be much faster um yes so i have a kubernetes cluster and this namespace is completely empty and now i want to deploy bb8 into that namespace for this we have a new release of our helm chart which let me also show you that there we go helm is the the german word for helmet so i always get like these helmet things here but i actually want the helm chart uh which has releases so there we go we have let's also release candidate because it points to the vivian release canada um for 14 which works with horizontal scalability so i've already downloaded that so you can see here i have this this v8 pgc which is just the latest home chart and i have a very very simple value cml file um which is basically meant to to deploy this so i'm overriding a few things i'm overwriting well actually i don't even need to override this anymore because that's now part of the release that i had to during development and i have replicas and i actually want to start with a single replica and then right here this is just some some firewall configuration so i'm basically whitelisting my own ip address in the firewall and uh yeah we're starting with a single replica and this this is tiny this is not a size that makes a lot of sense in in real life probably with just two cpus on a single node but for now for demo purposes that's absolutely absolutely perfect so this is my values yaml and i have a deploy script it's almost almost doesn't make sense to call it the script because it's very simple we're basically just running this item potent uh command to upgrade in my namespace with that value cmo um yeah so sorry that's a release name and down here is the the namespace so let me simply run this and we should deploy a vba cluster that currently has a single node so ignore those warnings i'm running with the the gke autopilot cluster so basically as i deploy more stuff on that cluster it automatically provisions uh new infrastructure in the background which is kind of cool actually so if we look at the parts so it's okay it's just maybe this for for cube ctl maybe easier to understand if they actually type those out uh so we have currently a single um v8 note right here which is currently being provisioned as you can see container creating so this is mostly just the kubernetes cluster which has to to download uh the image and maybe just watch this when it's actually it's actually running already so that's cool so just to to prove that this works we can look at the service definition and here we see a service that has an external ip take a look at that and now if i contact this there we go so this v8 instance is running and right now it has a single node which is not the point of this meetup so let's take our values yaml and as with any configuration that you change in um your values yaml that will be applied once you deploy it again so let's say we go for three replicas for example so let's do that and i'm just running my deploy script takes a second clean that up and now again if we watch cube cpl get pods you will see that they are being created in a stateful set in an sort of ordinal fashion so we're starting with the vba 0 and then we're currently creating instance bb81 and once that is done and ready then it will create bb83 so let's just wait for that for a second something to note while these are um so you can see the one is pending right now that's that means that the cluster needs to provision the infrastructure was very fast uh one thing to note about um dynamic scaling so as we said on the roadmap dynamic scaling is actually not yet not yet present so let me go to the roadmap yeah dynamic scaling is actually the last point on the roadmap so some things of dynamic scaling so as you're seeing right now i'm actually adding notes when the cluster is already there that already works but there are some limitations basically as rule of thumb if there's no data in the cluster yet you can do whatever you want if there is data in the cluster and you increase its size that works however if you um there's basically no way yet to to drain a node or to tell a node that is being decommissioned and that whatever load is on the node should be moved to a different node so that is something that will come in the dynamic scaling milestone so basically if you have data imported on a three node cluster and you reduce it to two nodes that that will break at the moment because that's yeah not not the dynamic scaling part yet um but coming back to to um our cluster which is now ready so that was basically all we had to do and we see all of this is ready and of course you don't have to worry about something like such as load balancing because it's running on kubernetes so the service that we have in front of it will already load balance across those three uh deviate notes there their pods in kubernetes speak in in v8 speak their their notes because they're not tied to two kubernetes that just makes it much easier so if we get that service again here it has that external ip and that basically has the load balancer in front of it so if we contact it again well there's nothing in it yet so if we look at for example the schema you can see right now it's empty so let's import something into that let's get started with we have the news publication data set this very simple data set um we run that also in our documentation with slightly different configuration but that doesn't matter but we can just use that and we can just import it so let me just quickly import that that is python 3 import that point then i always need to check what the order is deviate url first that one cache here it's cache english and batch size i don't know let's go for something like 128. okay so that is running that is importing okay now i need a different uh terminal tab here um so now what we can do let's i want to show something um let's quickly look at these email that we have yeah schema is there cool and i know that one of the classes in that schema is called article so let me take a look at this one we maximize this so here you see a different difference to previous vba versions first of all we have this whole sharding config thing that didn't exist before but more importantly i didn't change anything about my import script this is exactly the same import script that we have been running um on yeah on regular pv8 instances but you can see that it said the desired count and also the actual count for shards to three so the desired and actual count at the moment is always the same because um yeah charts are basically or the creation of shards is basically atomic at the moment in the future this is going to be an async process with regards to to replication for example so let's say you have a replication factor of one and then you set you increase this to three replicas then data has to be replicated from those one node to the the other nodes so that that is basically when that process becomes asynchronous right now it's synchronous so desired and actual is the same however what i wanted to say is basically um that yeah it automatically adjusted the amount of shards to my cluster size and in the second demo that i'll show later we can also see that it doesn't have to be that way you can also control this if you don't want to set it to that size okay ideally i've been talking enough for this to finish yeah that's cool so um let's take that url and let's maybe just connect to that cluster and for this we can take the vv8 console yep that downgrades because it's not an https connection okay still the same url cool so that should be i probably have it in my history somewhere but let's maybe just start from from scratch so let's just send a query any kind of query as you're used to sending to vva so let's go for article and then we can go for a title and summary maybe and then to make it interesting let's set it limit and let's set a near text so that basically means this is going to be a vector search because i'm running with the contextionary you can of course also run with transformers for simplicity sake so we don't need gpus in this demo i've just picked the contextionary which works fine and i've asked bob before for an example uh that because he uses this data set a lot and he said housing prices is one that that he typically uses so let's go for a vector search of housing prices uh the content itself doesn't actually matter just to to sort of as a proof of concept it does find stuff relating to to house it housing prices and uh housing shortages etc so the vector search is working uh the part that i want to show is basically if i run it again it's still working and the the interesting part here is because we're running with three nodes in that cluster and we have a load balancer in front of it we cannot predict which node is hit on that search but if i run it again and again and again and again it will always return the same results so basically no matter which chart we hit whether that chart has all the data or parts of the data deviate internally makes sure that it contacts all the other charts that um yeah that have data or um that it needs data from and this is also why we said before in the limitations of sharding that the queries per second actually don't go up if you increase the the amount of chart even even if for each new chart you add a new physical node that has new hardware um because your data set is now spread onto different nodes we don't know which node contains your data um and and in this simple case and this is maybe also a good point to to talk about this there are a few more other fields that we see here so right now we are sharding on the key id which is the with the underscore this is the internal or not the internal id but the the id um the the uuid that every um object in vv8 has and we're currently having a strategy which means hash so essentially we're creating a hash from the id and there's just a specific hash function in this case it's a murmur 3 and i think it's a 64-bit hash the interesting part about this is so so right now this is fixed and this is actually immutable but in the future and this is why it's in the country the plan is to um open that up and so that you could for example uh set this to a different field um so um sorry if for example you have a specific property in your data and you know that you usually set a where filter on a specific property you could set that here and then you could already chart according to that and then you could be in situations where basically a single chart can answer your uh query and then you do actually benefit from from sharding as well with regards to the throughput in this case right now because the ids are basically meaningless from the perspective of such a search right now i just have to hit all three shards and basically combine the results um okay that speaking of combining results that's basically the perfect segue into an aggregation so let me aggregate an article again and let's maybe start with just a count yeah account count is also account isn't actually that interesting for what i wanted to show let me just quickly get this what i wanted is i think there's a property convert count yeah which is just an inf so this is not vva counting anything that is just a property in that data set where each article has the specific word concept and here let's say we go for i don't know a maximum and a minimum um so if we run that so first of all if i run it again same result that's important because we never know which which node is being um which node is serving that query basically um yeah so so if we have something like a maximum or a minimum that is easy to aggregate like one chart will say my highest number is i don't know 15 000 one chart will say my highest number is uh 100 and the next one will say my highest number is 1680 16852 this is easy to aggregate for maximum we just pick the highest for minimum we pick the lowest et cetera if we go for something like a mean the value becomes being an approximate value so what we're doing in the case of a mean for example is we simply take the mean of each chart's result which would be the real mean if um data was perfectly evenly sharded but in real life it's never going to be perfectly even so this is something you have to keep in mind if you're working on a sharded data set the mean is going to be an approximate volume but yeah the important thing is no matter which note you hit you're always going to get the real results so let's come back here this was the first demo which was cool but also only a tiny tiny data set so as we saw with it no aggregate article meta count you can see this is really just just a 3500 data points in this data set so i also want to demo a second case with a larger data set and especially on this one i want to show you that as you add more shards and therefore add more resources to your cluster um you also increase the import speed so for this let me just switch my kubernetes context to a second namespace which i already prepared and here i've already actually deployed vba and in this case if we take a look three attempts if we look at this specific namespace we can see that we actually have 12 v8 nodes running so they're still relatively small but there's 12 nodes just to show that yeah you can scale them as you wish there's also an importer job which was still running from my test before let me quickly delete this rate it again so in this in this cluster um i also have so the same thing applies here i don't have a service here this has a different ip so just to show before we start not localhost i'm so used to typing localhost um if we look at the schema this instance is currently empty so if it has no schema it also can't have any data because data is held in in classes so to show an import on a larger cluster i have prepared a very simple import job which runs a script so let me go for this import outstanding that is the wrong one that is folder let me go for let me go for this kubernetes job just the kubernetes job if you're if you're new to kubernetes a job is basically just a pod that just runs once or if it's a cron job and it runs like a crown job but basically um it's not like once it finishes kubernetes doesn't make sure to restart it which is um yeah i'll post to the deployment or stateful set where if it goes down kubernetes should make sure that it goes back up again this script basically just randomly generates vectors or objects that have a a vector attached to them which is random and i can just configure it i can ask it to have this many dimensions this is just where it should set to send it to total size which is currently set to a hundred thousand patch size set to one thousand and charts which i'm going to set to one for the first example so even though i have so this is i said this before um that uh the the shards are picked to the size of the cluster unless you explicitly configure them so even though i have a 12 node cluster right now i'm explicitly telling this class should only uh be chart or should only have a single chart which means it can only live on a single note and i'm doing this on purpose so then we have a baseline comparison basically so let me cube ctl apply minus f import job that creates the job and now if we look at our pods and we can see oh it's already running that means it was scheduled on a note that already had the the image locally that's nice so then we can just attach to the logs which is uh let me type it out cube ctl locks minus f and the name of that thing and then we can see it's currently importing it's not super fast because of course these these nodes are sized uh very small it only has i think in this case four cpu so two more than in my previous example and as you can see basically as with any data set in the beginning the the batches are basically artificially fast because there's no not much data imported yet um but as this imports i think this will sort of eventually go to around five seconds or something per per batch and then it will stay relatively constant at that rate um i'm not going to keep it running all the time um because i think this job in total i think it would take about i don't know this thing about six to seven minutes or so to complete um but what i want to do is connect to that bb8 instance and just show you that stuff is happening just to sort of show that it's um yeah not just printing random stuff but actually importing something so let's log out here let's log out into a login into this one and then let's run a simple aggregate query call the demo class yeah let's just count it and there we go we currently have thirty nine thousand and forty thousand now so batch size was one thousand so that makes sense and if we look at our logs again they should now be at yeah forty percent so i think yeah the first one starts at zero percent so the highest would be 99 so that that matches perfectly and yeah as i said sort of as these uh batches will average around five seconds eventually i think it will take six to seven minutes maybe maybe eight minutes or so to complete um but i'm actually going to stop it because something lead oh it's very delete the job let's delete it okay and now let's modify that same job and as i said all we have to do and this is i think also something that's that's really cool all that we have to do to make use of that cluster is just change the configuration in the in the class and oh by the way i haven't shown sort of where this is in the class um yet on this one i'll show you um when this is running right now so for now let's maybe go for for a chart i'm not going to go for the full 12 yet just for for eight for now um and then let's apply that job again and other job by the way um first i need to delete the job uh no i think it already did yeah all right good so i can apply it the job itself will delete every v8 class um so it always starts basically so if we look at the pods it's already running again that's nice so let's attach to it and now you can see this is way faster already so we're now talking about batch times they're taking yeah i don't know 300 milliseconds or something because we're now using a cluster of eight nodes where previously we were only using a single note so also let me go here again and show you and look we've already surpassed the the other one so yeah this one i think we can actually let that finish because it's going to going to be finished very so let's wait for yeah another i don't know 20 seconds or so so funny thing as i um sort of tried this out and this is also the explanation for why i went for eight shards right now not for the full 12. um as i tried this out yesterday we found out that these batches are starting to become so large that the bottleneck right now is actually not deviated itself but we've had clients so we tested with this is running with the go client because it's a bit more efficient than the python client but we're seeing right now that basically in the batch where you send the entire data which in this case is a 300 dimensional vectors so that's quite a lot of data that's being sent there and you get the response in the response you get the same data back because um well it's just sort of the the idea of creating an object then you get the same object back because some fields might have been filled by by vv8 for example and this is currently entirely done in json and yesterday i uh checked the size of i think a batch of ten thousand that is just running a thousand and for ten thousand objects the batch was actually 27 megabytes and turns out at 27 megabytes just sending the batch over http and sort of parsing the json is actually quite slow so this is becoming the bottleneck and this is basically um yeah sort of preventing the times from going down linearly so if we run this same job again so let's delete importer and run it again this time with the full 12 shards so in ideally sort of from a vva perspective now we're adding 50 more resources so this should in theory uh speed it up uh even faster but it doesn't actually because the bottom line right now is the the client so um that's why i picked these these steps of sort of one eight and no twelve because between one and eight it's it's still more or less linear but as we sort of add more power to this cluster yeah the client becomes the bottleneck so let's keep cal applying my job again and let's take a look at it okay this time it landed on a note that didn't have the the image locally yet it seems so it actually has to actually downloading uh but now it's on and running that's good so if we attach to the logs of that one again and you can see it is a bit faster um but yeah it's definitely not 50 faster so right now there is um yeah sort of a saturation in this example which is in no way representative but just for for for this particular um case um sort of around uh eight no eight notes in this case um so let's see i think the total time on the other one was 52 and yeah this is barely any faster this is 48 so adding 50 more resources in this case didn't actually pay off but as said this is something that's currently the limit is in the client so we measured the time of sort of from vba's perspective how much work it has to do as opposed to the clients and we're seeing that the clients are the biggest overhead which is something we can easily fix various ways compression maybe a binary protocol on the request um what would probably also help is just to make the the response maybe a bit more efficient that the vector would only be sent to the flag is set or something like this um but yeah the cool case is the vv8 cluster itself it's still yeah it still works very nicely even with 12 notes one other thing that i wanted to show on this particular one before we wrap up um i set the um yeah the sharding configuration in my script i wanted to show the effect that i actually had and let me quickly do i still have the ip here no to get that again service ml class here you can see um that because i said that the charts to 12 minutes here they ended up at 12 and if we were to change that again to a different value let's just go look for the job and let's maybe that back to one and apply it again and now we need to wait for it to actually run it's already running that's cool so now we can look at the classic and and now we can see that it's just one so that was basically the change that i was doing a point being that just in your configuration in your schema configuration that you're already sending you now basically have control over how that class should be spread um amongst your your vivian notes yeah that wraps up the second live demo and as you can see here i'm still importing now on a single uh note much slower now than it was on the on the cluster um yeah before we move to two questions because we have still have a bit of time i just want to very quickly walk you through it through the new documentation because well it's actually not new documentation it's just a few new pages in the architecture section um but yeah as we said we have the roadmap which i talked about in the beginning already and horizontal scalability is now released next up is replication which i think the benefits we outlined before also if we go to the overview we have this this graphic here which i think is also really nicely shows how oh wow that's there we go um shows sort of how everything ties together in vb8 especially with regards to modules um also yeah from a scaling perspective so if you were to run a module that needs gpus for example that module runs in its own container so you could run that on separate hardware which is gpu accelerated or the vb8 core which is now horizontally scalable as the stateful part is completely cpu only speaking of cpu let me zoom in a bit again of cpus and resources we also added a guide on um sort of how to how to size your cluster or how to size your nodes that's that's the same thing so a question that that i received today was if i'm running vva in a cluster um how does that change my my um yes or i think it was specific to memory but in general my resource requirements and the answer is it really shouldn't so basically if you're running with 24 cpus before and now you say i want to run on three nodes then you should be able to run three nodes of the eight cpus each and the same is true for for memory uh but in general we have this new section in the documentation um yeah sort of outlining the role of cpus what do they do why would you add more cpus same for from memory um also something about yeah potential bottlenecks how garbage collection in go would affect your bb8 thing what you can do to potentially reduce memory requirements if it's too high uh caching vectors on disk the role of gpus etc so this is really cool new guide that should definitely help you and of course the one on horizontal scalability with everything that we talked about right now um what other points didn't we oh yeah there are two two more points that i didn't actually three more points that i didn't talk about at all um one is re-sharding so resharting is also something that um we put in the dynamic scaling milestone so it means right now the shard count is immutable you cannot change it um the reason for this is basically not actually the reason for it mostly that it's going to be in a later milestone but still there is something that you need to to keep in mind with regards to recharging um this h and sw index that we built up you can't just cut that in hot half or cut that into individual parts so basically a resharing process is basically a new import so what that means is sort of in the worst case this is not how it is in vv8 but this is how it could be let's say you're going from one chart to two shards and you would have to sort of build up everything again that would essentially just mean you need to re-import your entire data what we have in vv8 is a virtual chart system which is very much inspired by we we talked about this in depth in the um in the architecture meetup where we went through all the theory which is inspired by cassandra's virtual notes and the idea here is that basically we assign data or we assign yeah objects basically to a virtual chart which then belongs to a physical chart and if we change the number of physical charts only those virtual shards or only some virtual shards basically have to be moved so this means that if you go for example from four shards to five shards you don't wanna have any movement between the four existing shards the only movement you wanna have is from those four charts to the new chart so this is this example here um that if you have so here we have if it takes as an example if it took you 60 minutes to import your data into four shards and now you're adding a fifth chart um then each chart will have to transfer 20 of its data which means that the resharding process would take roughly 20 of the import time which would be uh 12 minutes in this case so important to keep in mind resharding it is on the roadmap for dynamic scaling um or is it actually i think here it says it's it's sort of it's on the roadmap but not even in that point it's just something that that we can do in the future but there is a cost to recharging because of the h is w index uh next point that i did not talk about yet and i also won't talk about right now for more than a sentence or two um how do notes discover each other uh basically you can see that in when i showed the services here there is a vvate headless service which is part of the new um of the new uh helm chart that we deployed or that we released today and basically what that does is it just resolves the ips of the individual nodes and then the nodes themselves uh yeah they use that information to communicate and as long as as any node in the cluster has contact with any existing nodes they will discover each other because they are using a gossip protocol or gossip-like protocol and this is not something we invented um and also not something we implemented but something we just are using a hashicorp's member list go library for this and and this is also that this gossip protocol also would communicate stuff like a failure scenario for example so if one node is um assumed dead then basically that news will spread like like gossip um yes this is all the important thing to say about this is basically that you don't have to do anything as long as you use the the vba helm chart so as you could saw as you could see with the um the first example where we started from scratch having nothing but a kubernetes cluster all you have to do is configure the replicas and it will be deployed uh next thing is also something that we don't have in right now but that um yeah might make sense in the future which is something such as node affinity so as i said before i could tell vva to only create one chart or eight charts and these one or eight charts were basically distributed among those 12 nodes physical nodes kubernetes parts that we had but we can't currently control which nodes should get something so this is something that we might add in future like a label or rule for example so let's say if you had async node sizes um some nodes bigger than others then you might want to sort of put more charts on one node or maybe you have more critical nodes or something like that so in the future we might add labels here and if you're if you've used kubernetes before i think the term node affinity is also one that they use so that's definitely um inspired by kubernetes and meant to to work with kubernetes yeah other than that we've talked about the stuff that isn't implemented yet which is basically what is on the roadmap so replication will be part of the next release probably will have another meetup where you can show that something that i'm already looking forward to to show is um yeah actually putting query load on the cluster and getting it to a point where um it's maxed out and then just adding new notes and um yeah and increasing the query uh load or the query capability once we have replication so that is currently under development well it is going to be after today because today we finished um the horizontal scaling without replication milestone which is ready to use as you can see 1.8 i think it's currently not yet in the configurator let me check actually i don't have to check i know that we haven't put it in yet but um let me nevertheless demonstrate yeah it's not here yet but we'll add it tomorrow probably as a as a release candidate yeah that wraps up the live demo those actually wanted to to go to this one that wraps up the live demo ", "type": "Video", "name": "Weaviate Meetup \u2013 Horizontal Scaling, Sharding, Kubernetes, and more in Weaviate v1.8.0", "path": "", "link": "https://www.youtube.com/watch?v=gIIsZ21hdfk", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}