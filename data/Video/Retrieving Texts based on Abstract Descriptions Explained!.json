{"text": "This video explores a new paper exploring the use of summarization chains to represent long texts and use (original text, ... \nhey everyone thank you so much forwatching weave on YouTube this videowill dive into a new paper titledretrieving texts based on abstractdescriptions quickly before getting intoit if you like these kind of papersummary videos please let us know byhitting the like button and subscribingto the channel here's a quick overviewof the paper in case you're in a hurryand don't want to hear the fullexperimental details so with thisconcept of retrieving text based onabstract descriptions abstractdescriptions what drew me to this paperis it's similarity to this concept ofsummary indexing so summary indexingdescribes maybe at the chunk level sofor each text Chunk say you're you knowyou have full podcast Clips orparagraphs of text instead of justvectorizing that full podcast clip you'dinstead use say a large language modelto write a summary of of the text andthen vectorize that summary of the textwe'll talk about how economical it is touse a large language model for this taskbut that's kind of the general setup isyou know you summarize the full thinginto some whether you know how you writethe summaries whether it's an abstractdescription that kind of blur out thespecific details or just a summary ofthe you know the info informationcontent then vectorizing and indexingthis content is quite an interestingidea it's also very interesting for thisconcept of a top level index so say yourdata is you knowpodcasts or books something that haslike a high level structure and then ithas is made up of these Atomic chunks soyou you use like a summarization chainto summarize the chunks and then youvectorize this summary so now you cansearch through the you know at thepodcast level and then search within thechunks once you have the matchingpodcast another interesting idea so thiskind of concept of summary indexing andthen abstract descriptions that's kindof like the the semantic match that Drewme to this paper but they have thisreally interesting new connection whereyou use these summaries to trainembedding models and they publish thesenew embedding models they make some bigclaims about the quality of them we'veintegrated them in weviate and they dolook like they're pretty interestingmodels so it's quite an interestingpaper that actually ends up beingrelated to these works like in pairspromptigator you know this kind of ideaof use large language models to generatesay queries or in this case descriptionsthat you can then use to you knowbenchmark the performance of say youknow different zero shot embeddingmodels or train your own embeddingmodels so this concept of summaryindexing I have to give a quick shoutout to Jerry Lew Alum index he's readinga lot of great content about you knowthis kind of idea of using thesesummarization chains to then representyou know high level objects based ontheir summariesthere's also really uh relevant for whatwe've been doing with generativefeedback loops generative feedback loopsgenerally describing taking data fromwevia sending it to an llm and thensaving that generated data back toalleviate so you can semantic searchthrough the generated results so youknow you write the summary and then savethe summary back to Evie and then youindex the summary to search through thesummaries so here's a quick explanationof what they do in the paper they usethis prompt let's write abstractdescriptions of sentences you know thefew shot prompting idea where you giveit a sentence and an example of anabstract description so in this case anabstract description it doesn't mean asummary of long text it means atransformation of the text into theabstract thing it represents and sowe'll see an example of that in a secondalso in the prompt you have this kind oflike note you know some more metainformation about the tasks this kind ofprompting strategy then one otherinteresting thing about the promptingstrategy is that you have this writefive good and then five bad descriptionsand then output a Json file with thekeys good bad this is a reallyinteresting prompting trick that you canuse to kind of get like you know 10generation out of one prompt and thenyou know you index the keys and givethings like say the Lang chain outputparser camel case this like one exampleof how you structure these outputs tomake it obey the Json file I mean theselanguages models are really surprisinglygood at respecting this kind of Jsonfile structure and then you can you knowparse it and send it along the chainso here's an example of what they meanby abstract descriptionssay you have a sentence like dopamineconstitutes about 80 of thecatecholamine content in the brain youthen transform that specific sentenceinto a neurotransmitter found in thebrain in high concentrations becausethis abstract description is probablymore likely to match the query so likeimagine a user and their informationneeds they come and they're like curiousabout neurotransmitters in the brain andso this kind of abstract description itprobably like vectorizes and indexesbetter than this specific sentence andthat's kind of the key Insight but theother key Insight is that they alsogenerate bad descriptions and then theyuse this to train embedding models soit's a really interesting likegenerative data augmentation and thenself-supervised way to train these nextgeneration of embedding models and youknow they've publish new embeddingmodels so staying on this conceptquickly of how embedding models aretrained this is something at weavierwe're always trying to stay on top ofand give people the best advice of wherethe best text embedding models are whichyou know vectorization models use so thegeneral idea is that you want to havegood negatives when you're training soin your contrastive learning objectiveyou're trying to make the vectorrepresentation of this meerkat similarto the vector representation of thisalternate view of a meerkat but then nota similar Vector to this picture of NewYork or this golden retriever so theidea of good negatives versus badnegatives is that you know contrastingthe the vectors of these two meerkatpictures with a dog is is better forlearning a semantic embedding space thancontrasting it with this picture of abuilding so we previously had looked atthis paper from Ori ROM and hiscollaborators about uh the spideralgorithm so the spider algorithm is away to look at overlapping engramsbetween passages in Wikipedia articlesand so you know like if two passages oftext you have the same engram like thepriesthood for himself and his maledescendants that'd be like a goodexample of a positive and then you justyou know use the other Snippets of textin the Wikipedia page as negatives so sowe're always on this hunt for how can weself-supervise how can we you knowautomatically annotate positives andnegatives as scale with web scale dataand that's kind of like where the zeroshot embedding models come from so thispaper is presenting a really interestingnew way to you know instead of lookingat these overlapping engrams fromWikipedia Pages you're contrasting thegood descriptions with the baddescriptions so this is the anchor thisis the positive and this is the negativeand that helps with you know therepresentation learning task and youknow they generate quite a large scaleamount of these roughly I think like 169000 that they use to do this so you canimagine you know scaling this up likecrazy and it's pretty interesting so soin the end they're gonna you knowcombine two loss functions the tripletloss triplet losses where you have youknow the distance between the anchor andthe positive and the distance betweenthe anchor and the negative this kind ofthing and then you also have that inbatch negative thing and then you knowyou weight the two loss functions withthis Alpha then in the end the umthe way that they evaluate this you knowthe the state of evaluating embeddingmodels is you know you have the beerBenchmark which is a pretty solid thingbut generally it's sort of hand wavy youknow this this is sort of the Pinnacleof that but that you know they're gonnathey have a human study survey and theyyou know they do show that the humansrate the humans in the study rate thismodel to perform better than these otherbass lines like all mpnet based twothat's a really strong you know modelavailable in hugging face sentenceTransformers or say bm25 which is thekeyword matching so you know five is soyou know we'll get more into the detailsbut basically they show the uh the humanannotators five search results uh they amix of five from one of the models andfive from another of the models and thenthey rate the top five results and soyou know they're saying that they chosethis one five most of the time and thensay bm25 was chosen like zero out of thetimes at most times so this thing thatall these models perform better thanbm25 really in that you know this modelis the best quickly before divingfurther into the details of the paperquick thank you to March Nantes foradding these models into Eva's texteffect Transformers images so if youwant to run these yourself here is theimage for adding these new models so youknow if you want to just stop the videonow and just see what these models cando with your weeviate example then thisis the image to do so so thanks Martinso following that quick description ofthe paper let's kind of walk through ita little slower so the background isthat this paper is marrying these ideasof summary indexing with this Generalconcept of large language modelgenerated training data forself-supervising embedding modeltrainings so this idea of summaryindexing has been one of the mostpowerful kind of ideas around Lang chainllama index and these kind of llm toolsso this article from Erica Cardenaspublished February 21st is you know whenLang chain first came out one of themost powerful things was were thesesummarization chains we also you knowwe've been building on this Generalconcept of sequential chains you knowlike where the name Lang chain comesfrom where you chain together otherlanguage model calls such that theoutput of the first call is the input tothe second call and so on summarizationchains have been the most interestingkind of uh example of this so soprobably my favorite one is this refineidea this is probably the best way tosay summarize a podcast so you take eachof the of the clips of the podcast andyou say write a summary you know you'llreceive these clips one at a time so itwrites this intermediate summary andthen it keeps taking the clips updatingthat intermediate summary untileventually it's looped through all thepodcast clips and ends up with a finalsummary there's also like this mapre-rank and mapreduce I see these asreally interesting for say questionanswering and I think this kind ofrefine ideas really interesting forsummarization there's also an idea thatI think originated in Lama index thatwas like this um tree summarized so yousay line up all the text chunks you knowin a list and then you kind of couplethem in twos and build up the tree withthe summary that way and you knowaggregate it to the top so but generallythis concept of you know applying TheseChains to summarize content and and thenyou have this summary of content thatyou could then say vectorize and buildindexes with or use and you know that'skind of the question is okay now what dowe do with the summaries so this is alsohighly related to the concept ofgenerative feedback loops that we'rereally heavily exploring at weviate thisis the general concept of where we dothese retrieval augmented Generations wesend specific data from weviate to thesegenerative models and then we save theresulting generation back in weeviate tothen maybe you know access for a futureretrieval augmented generation or inthis case you know semantic searchthrough the generated results so when wefirst published our blog post usingAirbnb listing examples we were takingthe symbolic properties of the Airbnblike you know how much it costs how manybedrooms it has and we took all that andwe synthesized a text description of itthat we would then index and searchthrough so it's kind of a similarconcept to this summarization index youknow this idea of transforming data tofacilitate searching through it the nextreally interesting idea and that's whatthis paper is that's why I think thispaper is so interesting is is kind ofmarrying two different schools ofthought and you know I love this whenyou had there's some connection that youhadn't originally thought of is thisidea of large language model generateddata for training search models so thisis a picture from the in pairs paperyou take a document like we don't know alot about the effects of caffeine duringpregnancy on you and your baby so it'sbest to limit the amount you get eachday so you know imagine you have thisbig collection of text chunks it's likeif you're building a tech search appwith weave that's probably like thesituation you find yourself in youprobably don't have these questionsalready so you know having a query logalready is more of like a big companything to already have this kind ofestablished thing so you can do is youcan take the large language models andgenerate questions you know what are theeffects of caffeine during pregnancy andthis document you know would say be thebest answer you have I don't know it'slike specific enough for this questionbut like this this would be like a wayto pass this into the language modelprompt in some way to generate queriesnow you have like query document pairsto you know either say compare theembedding models if you want to say youknow I don't want to use the openembedding models because it's 15 36Dimensions per you know vector and thenI can't afford it's more expensive so Iwant to use say the 384 dimensionalembeddings from say the sentenceTransformer DPR models you know and thenso like you you want to ask thesequestions of which embedding modelshould I use but you don't have data toBenchmark this is a really interestingsolution to that to give you data tothen you know understand what thedifference in the models are going to befor your search app and then also it'sinteresting for training customembedding models for your data whichwould also probably give you betterresults so two extremely interestingthings that you can do with this kind ofllm generated search data so here's themotivation behind the novel Insightinstead of generating specific queriesthat match specific information wegenerate these abstract representationsso for example a lot of theseinformation retrieval data sets and thenthe embedding models that are trained onthem like say they're trained on thesquad you know question answering dataset it asks questions like what is theatomic number of oxygen the answer iseight or like what year did this Kingrule over you know France for and thenit's like some specific date so most ofthese data sets that have been createdand you know published are like queryand then like specific thing that comesout of the query rather than this kindof like abstract information need andthe authors motivate this by saying theuser is not interested in a definitionor a single answer but for sentenceswhose content is a specificinstantiation of their query I thinkthat this is a great quote because it'sit's more in line with like this kind ofsemantic search concept you're lookingfor like you know semantic matches notlike specific matches to questions inthe thing so you know it's interestingthat if you're interested in this kindof thing there's also the set ofembedding models called the instructormodels where they're really digging intothe different information needs and it'squite an interesting topic but they givethis example of say you have this querysubstance abuse in animals right likejust you know this kind of abstractquerythat would be a better match for theStudies have shown that a subpopulationof primates chronically consumeintoxicating amounts of alcohol so it'slike this kind of query compared tosomething like which primate cons it hasbeen found to show like and then itwould be like some specific kind of youknow species of it that kind ofdifference in what these data sets looklike so another quote from the paperjust sort of laying that out moreexplicitly systems that are trained toretrieve passages that contain answersto questions for example train mostly onSquad and data sets like this Beyondbeing focused on questions rather thanassertions are also focused on specificanswers rather than abstract situationsso it's a you know it's a bit metadigging into thinking about informationneeds and information retrieval but Ithink it makes a lot of sense you youknow usually with these kind of queriesyou're like fuzzly looking for some kindof uh similar concept rather than theexact match to your question with theembedding model so digging further intothe paper this is the prompt that'sbeing used to generate these valid andinvalid descriptions per sentence so westart off with the main prompt of let'swrite abstract descriptions of sentencesexample so so this is kind of the newthe structure of problems these days isyou start off with some description ofthe task then you have a few shotexamples of what input output shouldlook like for the task so sometimes youcan omit this but generally you knowI've done a few interviews with peopleabout you know who work on like agentsin production this kind of thing andgenerally the consensus seems to beright now that keeping these few shotexamples of you know task descriptionsor say how to use a tool is still quitea productive thing to improve theperformance of the language model so youhave the sentence you know Pilates rolein events leading to the crucifixionlent themselves to melodrama eventragedy and plotty often has a role inmedieval mystery plays so then yougenerate this abstract description adescription of a historical religiousfigure's involvement in a significantevent and it's later portrayal and artso you know you see how you blur out thespecific uh you know the specificdetails of the of the text and then youinstead have this abstract descriptionof what the text is about so then inaddition to the few shot exam apples youhave this kind of note to the languagemodel and how to complete the taskdescriptions can differ in the level ofabstraction granularity and the part ofthe sentence they focus on somedescriptions need to be abstract Whileothers should be concrete and detailedso then you have kind of the outputprompt so you're saying for thefollowing sentence right up to five goodand Standalone independent descriptionsand five bad descriptions which may berelated but are clearly wrong and thenhere's another really interesting detailI'll put a Json file with keys good andbad so or get you know good comma badbecause it like might even help a littlebit more with the Json file but so thisdetail is quite interesting because thisis this concept of say you knowLangston's recently released outputparser output parser's camel case likethis concept of how do we make languagemodels behaveobey the kind of API syntax to then kindof chain out the outputs and you knowparse them with sayyou know just like just a basic kind ofpython parser for how you would takethis kind of output and then save eachof the generations and then this kind ofidea of using one prompt to generate 10Generations this way you can alsoimagine that there's some kind of crossattention across the 10 examples it'sgenerating so it's kind of like listingout things it's a super interestingprompt and then you pass along the Chainby having it follow this structuredoutput so here's the next detailed paperthat I think is extremely interestingthey're going to take the output of themain prompt and then feed it into themake more abstract prompt so in my ownexperiments of generating queries forthe podcast I've also thought of doingthis kind of thing where say yougenerate you know five queries and thenyou have to ask it you know to takethose queries as input and then say youknow is this query specific to thedocument because you know it'll generatesome queries that like aren't specificto the document so that's kind of whatI'd found in my little like how I'vebeen investigating this a little bitso I think it's really interesting thatthey're formalizing this make moreabstracts prompt in this kind of promptchaining with degenerative feedback loopfor the llm generated training data sowhat they do is out of the five validdescriptions they're going to take eachone of them and they're going togenerate three more from them by feedingthem into this next prompt so rememberagain you have the Json file so youindex the Json file with the good keysand then you know you have so this newprompt so you have the sentencedescription pair and then you have avery abstract description so again wehave this few shot example thing wherewe're giving the large language model anexample of what we want this to looklike and then you know we roll with thatwhere we plug in the sentence we plug inthe we plug in the original sentence weplug in the description the validdescription that was generated in theyou know first thing and then we get avery abstract description so it was areally interesting way of kind ofchaining this along and refining theoutput of the um you know curating thesedescriptions or summaries so here'sanother really important detail of thisthey do this at pretty large scale so ifif you were skeptical about whetherthese models would be any good you knowwe're checking them out with say thesentence Transformers Library I thinkthis is a reason to be convinced tocheck them out so you know for each of165 960 Wikipedia sentences theygenerate five dollar descriptions andfive misleading descriptions so you knowdoing the math that's like roughly like800 000 of each and then they are alsogoing to do that and make more abstractprompt from you know three of seventythousand so so for each of these youknow 165000 roughly Wikipedia sentences they'regonna have you know five to eightadditional Pairs and then fivemisleading pairs so is the train is umyou you have this kind of like localcomparison where that's the way they didthe metrics but so anyway so so it'sinteresting to see the scale of itreally you know doing the million ofeach thing and you know doing that kindof scale it it's it's a large enoughscale to think that these models wouldbe like quality zero shot embeddingmodels of course then you you know couldimagine scaling this up further furtherwith more Wikipedia sentences but it'spretty interesting so quickly before weeven dive further into the human studyevaluation which is quite interesting inits own way let's quickly touch on howembedding models are trained justquickly in case you're not familiar withit basically you have this kind oftriplet loss or you have this info nceloss they're going to combine both ofthem in multitask learning the way thatyou do the triplet loss is you take theso this is the vector encoding of theanchor this is the vector encoding ofthe positive Vector encoding of thenegative whereas with the batch negativeyou Loop through the encodings of allthe other examples in the mini batch soyou know the way deep learning modelsare trained is you sample a batch ofdata points and so with contrastivelearning info nce you you have youranchor and positive and maybe also youwould have a negative in there that youcould kind of mix of this but you couldin so you have the anchor the positiveand then just every other anchorpositive pair that's in that mini batchis used as a negative and you know youLoop through and you take the you know eto the you know the distance and this ishow you do this kind of likedifferentiable loss function and scaleit by doing this stuff like taking thelog of it and that kind of idea sowith the triplet loss you kind of likeexplicitly compare two points you havethe margin hyper parameter that is youknow then plus the vector distance theso the L2 distance between the anchorand the positive vectors subtract thatwith the distance between the anchor andthe negative and then you normalize italso by having this large batch so thiskind of law I think just the quickcommentary is like this kind of tripletloss thing this is pretty friendly thisyou know not too ugly to train withwhereas this kind of info nce thing Ithink this is a little more prohibitivewith people training embedding modelsbecause this is a bit more of a chaoticthing to orchestrate with how you haveto like you know have this like diagonalmatrix to get the labels so this iswhere I think a lot of the the skill andthe expertise in training deep learningmodels comes in and yeah just sort of aninteresting detail if you're interestedin you know exactly how the embeddingmodels are optimized so here's anotherpretty interesting detail of this paperinstead of reporting the performance onof their model on Ms Marco beerbenchmarks they're using a human studywith Mechanical Turk workers so youbegin by instructing the humanthe workers about what the task is andSo you you're shown a description in 10sentences you're asked to choose thefive most relevant descriptions so thenwhat that ends up looking like is youknow a period of difficulty and sorrowfor an individual and then you chooseyou know you rank the top five bychoosing five from this list to submitis which you think are the retrievesentences that best fit this descriptionso in the end you see that they chosethe new abstract Sim models you know onthe average they would choose 3.78 outof the five selected sentences wheneverthey did a head-to-head comparisonablating each of these two modelsagainst each other in like you knowtournament style combinatorics so youknow you had pairs of abstract Sim 5versus all MP net based V2 5 and youknow that made up to ten so each ofthese two have been compared with eachother five you know in this kind ofamount of times and then you see kind ofthe bar chart the performance how manytimes it was chosen out of the 10 andshowing that you know these modelsperformed pretty well in the study sohere's some examples you know if youwant to go through it you see kind ofthe difference in howhow it captures the specifics but youknow how it captures this kind ofabstract concept with this newgeneration the kind of better resultsthat leads to some specific examples ofthat if you want to dive into it so hereare some final takeaways I had afterreading this paper retrieving text basedon abstract description so firstly I'mthinking heavily about this kind ofsummary index concept and I think thatwhat we're seeing is the cost of largelanguage model inferences gettingcheaper and cheaper such that this willbe something that's you know more commonto do as we you know we're seeing allthese open source models I think Falconbeing the latest one at the time of thisrecording but this we're probably goingto see cheaper and cheaper largelanguage model inference such thatsummarizing all of your documents ismore of a you know economically sensiblething to do based on your app orwhatever you're building so it's veryinteresting with this respect to customdata for your document so I imagine alot of weviate users have you know justthe documents and rather than thequeries because you're building kind oflike a new app you don't already haveall this like user data in this kind oflog of query so so now I see there'skind of three things that you get out ofdoing this llm generated data first youget the summary index you can use tokind of index the data to search throughyou can also use then the original textand then these abstract descriptions andthen how well you're able to retrievethe full text based on the abstractdescriptions to compare the performanceof zero shot models and you can also usethat as data to train custom searchmodels so kind of three use cases youget out of this llm generated data youbuild a summary index you can ablate thedifferent embedding models and then youcan maybe train your own custom modelsif you want to go for that so then Ithink this General concept of summaryindexes is super interesting for thisconcept of top level indexing so youknow if I have a whole podcast I want tosummarize the whole podcast by doingthat create and refine summarizationchain through each of the clips and thenonce I find the podcast that matches myquery then I will search within thepodcast so I'm just generally interestedin that kind of phenomenon I didn't Idon't think it the paper particularlymade me think more I believe about thisbut I just think it's something that'svery interesting so then there wasanother paper that came out recentlywhich was Voyager this Minecraft paperwhere you learn skills and then youinterestingly what they did also is thatthey had an abstract description of theskill programs and that's what gotindexed when they were retrieving soit's just an interesting thing of howyou would summarize some kind of reallyabstract thing to then index the summaryof the thing and Vector search crosssummaries I think is really interestingso maybe something that's just like anidea is maybe we could retrieve withabstract descriptions and then applythese re-rankers like cross encoders tothe original full text as an idea andthen another thing is I just think thiskind of how do we judge the quality ofthese llm generated descriptions I thinkthis is quite an interesting topic sowith respect to this last point of howdo we judge description quality we VAPodcast 48 I had the pleasure ofinterviewing Professor Laura Dietz and Ilove this paper perspectives and largelanguage models for relevance judgmentsand I think it's really interesting torelate to this so you know what wepreviously had you know human judgmentthis you know like with say like theTrek benchmarks for informationretrieval like humans would just youknow annotate the relevance of queriesor say like you know we would divide upour team so everyone on our team writes10 queries for each of our likedocumentation or something like this inorder to Benchmark our search systemsnow we say have ai assistance where weuse the you know the llm generatedsummary of the document then we havelike a human filtering layer where thehumans like look at the output and kindof rate them or maybe we do just likedifferent tasks generally I thinkanother interesting idea is two llmseach generated judgment and a humanselects the better one so this is kindof similar to me to the make a moreabstract prompt where you say take theoriginal five valid descriptions andthen you generate three more you couldalso Imagine an intermediate layer therewhere the LM takes the five generateddescriptions and says you know you canonly take three which of the three youknow best abstract summaries of thiscontent that kind of idea of likechaining away with the language modeland soyou know it's just like a reallyinteresting you know the times and thingthis whole spectrum of how we'rethinking about using large languagemodels to you know annotate say querydocument pairs relevance or say alsojudge the quality of generated text thisis like a problem as old as generativemodeling is how do you judge the qualityof generated images of generated textand so with images we had things likethe freshay score Inception distance forsay like Gans and now with text it'slike when you write a summary of uh youknow some text how are you gonna youcan't like what they have now is likeRouge scores blue spelled b-l-e-u scoreswhich is like engram overlap to somehuman annotated gold summary of the textso it's like how are we going to beannotating this kind of abstractdescription so I really enjoyed thispaper and I think this is this isanother interesting perspective on llmgenerated data that we can use forsearch so one more thing before wrappingup I'm working on a new demo of abstractdescription summarization indexes usingthe dog fooding of the Wii VA podcasttranscription so if you haven't seen allthis project already please check it outand stay tuned for generative feedbackloops part two exploring generatedsummaries for each of these podcastclips and the summaries of the podcastthemselves exploring writingpersonalized summaries of the podcastfor different people writing biographiesof people based on the podcast contentand also say automatically findingchapters from the podcast by promptingit so exploring further how we can uselarge language models to transform ourdata and then save that transform databack into our database for some kind offuture purpose thank you so much forwatching this paper summary video ofretrieving texts based on abstractdescriptions please subscribe to thechannel for more content around you knowVector surge retrieval augmentedgeneration approximate nearest neighborsearch and all these cool topics pleasecheck out check out weaviate at weeviateIO or open source on GitHub at wegateweeviate and follow us on Twitter atweeviate IO thanks again so much forwatching", "type": "Video", "name": "Retrieving Texts based on Abstract Descriptions Explained!", "path": "", "link": "https://www.youtube.com/watch?v=mn5P79n541Y", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}