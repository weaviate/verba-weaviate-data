{"text": "Hey everyone! Thank you so much for watching the 75th Weaviate Podcast featuring Tanmay Chopra! The podcast details ... \nhey everyone thank you so much for watching another episode of the weeva podcast I'm super excited to welcome tan Chopra tanay has had an incredible career in machine learning from Tik Tok to neva and now building his own startup we have all sorts of exciting topics to discuss like rag versus fine-tuning and all sorts of cool topics around search and machine learning Tay thank you so much for joining the podcast thank you so much for having me here I'm excited to to chat with you today awesome so could we maybe uh tell this story of your kind of career in machine learning maybe like a helicopter View then we could dive in yeah absolutely so I actually chanced upon machine learning about seven years ago now um actually pretty soon after the the big transformers paper that that folks talk about all the time um and and entirely out of chance I was I was doing this internship back in Bangalore uh my freshman year of college um and somehow uh the manager sort of ended up giving me this machine learning project to work on um because no one else in the the value chain kind of above me wanted to do it so it kind of fell to the intern almost um right as as a lot of great things do um and so I was actually fine-tuning YOLO V3 to detect hard hats on construction workers um and and that's kind of how I got started I got so excited you know doing this project that on during the week I would sort of do this job and then on the weekend I would try to tr train these binary classifiers on my uh very very old Mac none of the M1 M2 stuff um and so it would overheat all weekend um and then get the week to kind of recharge um but yeah that's how I chanced it upon ml loved it so much that I kind of just stuck with it um the rest of my my life till now that's super cool I I started that same way where I wanted to build like a basketball app and I was doing object detection to detect the rim it's like a part of my pipeline I had the 4 gbyte memory uh GPU I can't remember the like 39 and it wasn't a 39 I think that's a pretty good one but I can't remember the the particular anymore but awesome so that led you then to Tik Tok where you and I I understand you know I I think Tik Tok has a lot of reputation is you know their recommendation algorithm is like super Advanced and so maybe if you could talk about just kind of the machine learning culture at Tik Tok and how that you know helped shape your career yeah absolutely so I actually joined Tik Tok right out of grad school and I was on the trust and safety machine learning team um recommendations for us you know by then was a pretty Advanced and your organization but on the trust and safety side we were kind of building it out from scratch uh so we were able to tap into a lot of their um best practices but also at the same time develop new ones uh with all the new knowledge that we' we'd had over the the time um I think the big thing that really made Tik Tok move quickly and deliver sort of Cutting Edge ml uh falls into kind of two buckets right the first is just the practice of iterating a lot and very quickly um instead of sort of front loading on hypothesis development and planning and um sort of trying to understand how the world Works before touching a single line of code uh we'd say you know we have these five or 10 hypotheses let's go test them one after the other um and and at the root of that sits very strong experimentation software um that's really what's key um and the other thing is um continuous Improvement and this is actually really key to machine learning as a whole um you know when you start out whether that's a prompt today or or a model you know three or five years ago um you're not going to be in a great place um but over time you can get significantly better um and primarily by learning through learning from the interactions um that your systems are having with your users with your customers um you fundamentally want to think about where you will be two or three or six months from now uh versus sort of being in this Paradigm of this is all we've got right our first hit is our only hit um and so I think those two things really made us um deliver ml that folks seem to enjoy yeah it almost makes me want to skip ahead into diving into the fine tuning and the new work you're doing so interesting this kind of iterating and but if we could just kind of continue on the story so from Tik Tok then you join Nea and I think Neva is super fascinating that I remember this whole idea kind of I think there like the cohort is kind of like u.com Nea a perplexity I kind of would put in that category in the early days of you know bringing this kind of uh search engine taking out the technology building a new search engine and you know that whole like subscription based business model some of the aspects of Neva and then eventually acquired by snowflake but you know I remember following along with Neva and the blog post and there's so many interesting machine learning projects so you know maybe if you could then transition into Neva and how that made you think about search and machine learning yeah absolutely um so I spent quite a while at Tik Tok you know we went from four or five of us when we started to uh 402 200 sort of uh scale in terms of just machine learning Engineers we have you know really large machine learning Aug uh multiple large machine learning augs um and really wanted to go back to that sort of small fast um hungry World um and Neva to me the mission was really exciting because it was essentially and I think all of the search companies are tackling this mission is that can we reimagine the interface between humans and the internet that's really what it is um right whether it's Neva perplexity oru.com or even Google with sge that's the question we're asking um right can our interaction with the the worldwide web be better um and what does it mean to be better um so I think at neeva we thought a lot about um search as a business and we thought a lot about sort of search as capabilities um one of the big things that we focused on and I think a lot of these companies are also Focus focusing on the new crop is what happens when you start treating the user as your customer versus the user user as your product um and businesses as your customers um right and and then you can start earning trust with users and actually make it feel like the internet the the interface of the internet is their home uh versus sort of a venue that they're a guest at right everyone has the same experience when you go to some of the existing uh search platforms um and the question is can you make it feel like you're sort of at the center of that Universe versus just being exposed to that universe um one of the big challenges in in ml when you do search is speed um right latency really sits at the heart um of of the search experience there's a lot of worlds in which folks will take a not as great experience if it's fast um and we saw that firsthand a lot of times um and that's a big challenge with llms today uh so we spent a lot of time working on model distillation um and we'll talk more about um that in in the context of Emissary as well once we get to it um but really for us the question was how do we get it to be as good but faster um because the Baseline experience was pretty solid um there were a couple of other things on top of this right one of the challenges was um hallucinations which search this becomes a really really key problem because if folks don't trust the outputs you're giving them uh they're going to start clicking into the links and once you start clicking into the links like then we're back to the conventional experience um so there are a couple of these like really interesting challenges one of the big fundamental challenges and and please stop me if I'm sort of going all over the place but um is is temporal reasoning uh language models are actually really bad um at at reasoning temporally um and normally that's not as big a problem if you think about it a lot of like documents and a lot of the things we do on a daily basis uh are temporally independent um right but search is very very temporally dependent um one of the queries that we we guessed with all the times is um all the time is is is Queen Elizabeth alive um right and this is actually a really common question that that a lot of ml folks test llms with uh because when you look that up on the internet you'll get searches that are like oh yeah she's alive oh no she's passed away oh she's alive and now you really have to reason over dates and times to be able to generate an answer to that question um and you'll see actually the way you frame that question influences the answer so strongly um so there's a lot of these like fundamental cracks that that we're working on and and spent a lot of time working on at at Neva as well yes I I really want to keep diving onto temporal reasoning in a lot use a lot of great points and yeah we're definitely going to come back to knowledge distillation and Emissary but this kind of temporal reasoning this is something we see all the time in like our weeva community channels people asking hey I want to uh search with you know relevance as well as time and how do I mix the two and so my prescription now is just you know get the top 10 semantic results and then just sort it by the date but maybe there's more you can and then and then when you're prompting it to then hand the search results to the language model you'd kind of template it like you know on date you know in the curly brackets yeah so so so would you say like there is an idea to you know maybe waiting the score like you have some kind of recency score you know we've seen this kind of exponential decay on the relevant score with the time or maybe yeah 100% and and we actually tried this that that I think you're mentioning um what's what's interesting is it's not just at one level right so the date or time you have is the date or time that you've created the embedding or uh index the document like there's a lot of these like that that's a separate date from the dates mentioned in the content um and so that's just not it's just not one level it's level at which you discovered information and then the dates referred to in said information um and so one of the things we tried was we we do this thing where it was like on date this um especially for news articles you always get the date um this content was was discovered um but even then what it really becomes a very complex problem at that stage because the the llm is now thinking oh on date this two days ago this happened so now I have to create an implicit mapping of two days ago from this state and if you're doing this over three or four articles it becomes really challenging for language models um right it's a really really complex reasoning problem um I think this is still I don't have a good answer um or solution to this problem yet um I think retrieval in general um is a very very complex space and there's some really smart people working on it um but I I think the community is going to start getting a lot more interested about retrieval um once we start getting held to higher and higher standards of accuracy and factuality um which will happen over time right it's very natural where in our early stages um I think retrieval is going to be a very interesting space um and and maybe we8 will do some really cool things right um in in that space um so I think a lot of this specific question is still something I don't have a good answer to yeah I think you brought up the dates within the content that's quite yeah that's quite funny as well and I I guess like um I think a lot about like if you have new information that disagrees with old information in your knowledge base if you want to try to like search for these conflicts and then sort of resolve the conflict and remove the old one from the whole index or you know move it to maybe like an archive category or something like this but so if we if we could I I really would like we have two other kind of topics that we've you know prepared on and and I'm just so curious because aneva you know I imagine so much experience on this so so this kind of other topic of retrieval diversity I I really want to you know dive into this is like right now I guess we just have semantic similarities to the query and then you just get like the top 100 nearest neighbors compared to some kind of like clustering and so how do you see that kind of uh you know divers I know Hast give a quick mention Hast stack and uh deep deep sets Hast stack they have this kind of diversity ranker where they're trying to uh you know look for the Clusters and so how do you see that kind of like diversity in retrieval that's actually a really good question and and also linking back to the previous thing you said and this um disambiguation um right being able to to pull out different threads um from from clusters of information um is is going to be really key um we're just starting to play around with this stuff um and Ne also we were kind of playing around with the stuff some of the guys on my team were doing really great work um because sometimes when you look for a person and this is where we discovered this problem there might be two people with the same name um and so you're now retrieving two different threads of information but they're blobbed together um so how do you disambiguate the different threads um and and you can think of them as clusters right um uh so I think these are problems we need to dive deeper into um that's one thing the other is you then have take an even bigger hit on latency this is something to keep in mind right realtime use cases every time you're adding a stage um you need to ask the question of how do we not just do the stage well how do we make sure this stage is not a statistically significant change in in uh latency uh because the user doesn't care that you're doing all these complex things at the end they asked you a question they expect an answer in a certain amount of time um so in the search Paradigm this is going to be really really challenging um but I think we're we're going to find ways to to get better at it um coming to your second question of retrieval I think an easy win is hybrid search um right you definitely need to be looking at the keyword matching World um we were very lucky at Neva that they built out a search stack before we started doing them out um right and this is this was a huge benefit that folks sort of off often will will Overlook the reason we were able to do such great machine learning on top of uh of of search was because we had incredible search people um and that's what I really say that like um try not to reinvent everything um right if there are things that building blocks that are really strong even outside of AI um we should really be trying to tap into them um right one of the bigger sort of manifestations of this is folks trying to use llms to do discriminative modeling right if you need a value that's yes or no um you should be using a discriminative model uh not sort of an auto regressive system of any sort um right and so there are a lot of these things where sometimes I think we need to take pause and say is this problem solved already and in a better way than than the way we're trying to solve it um when it comes to retrieval 100% diversity is going to be key um but again there is that trade-off um right you don't want to use too much of your context window uh with context um so you don't want too many results but at the same time you want these results to be offering different information one of the things I've been thinking about and playing around with it is can you represent this information in the form of logic um right and linguistically independent logic um and then basically just take the key facts um so you take all the context that you have um right break it down into facts and then push the facts or the claims into the actual llm context um and what that lets you do is avoid repetition of information uh because if an llm sees a lot of the same sentence written in different ways it's going to lean on that piece of information more than the others um so that's that's again an additional step so I'm stepping on my own toes here um but there is there there are ways where you can um get more intelligent about retrieval um a lot of them involve picking up more than you need and then sifting things down that's usually a good like framework to think through yeah that last idea is what really inspired me in this paper called mem GPT where M GPT is like prompting at like here the search results what particularly do you want to add to your working context and yeah that the facts I think that's extremely profound yeah retrieval diversity it's such an interesting one because it's like you know if you cluster the search results now you have this slowdown where you're clustering or maybe you calculated it offline but you know you have to have like adaptive clustering I've seen some ideas where maybe you search with you know you have your top 100 results maybe you search with the seventh and that you know that gives you diverse queries or or I guess you could rewrite the query and search in parallel yeah I think a lot of these are challenging because you don't want to miss the good results right um that's the challenge with diversity is like sure if you start sampling from the top 100 um theoretically you have a wider knowledge base that's covered but are you going to miss the key question that that the user asked um right because that could have been in search result to but two was too close close to one um and so you don't want to be bad at standard deviation two uh from the mean just to try and capture tail queries or or head queries um and that's something we thought quite a bit about at Neva is like we don't want to compromise u mean plus standard deviation one and two uh just to to get on to head and tail yeah amazing I think we could just dive into retrieval diversity for so much top so much to it but if we could kind of rounding out the the search topics this kind of uh query recommendation sort of we've seing this with chat gbt now you come to chat gbt and it's got some you know questions like how about you want to see the top 10 I don't know you know recipes for lasagna like how do you think about this kind of uh query recommendation especially as you're seeing more personalization you know like chat with your notes chat with your docs is kind of what queries you want I I could I see a ton of potential in that yeah absolutely so we actually had a beta product back at uh Neva um called gist um where I actually built out the query recommendation system for this product um and a lot of llm uh capability is integrated into that um I will say a couple of things about about query recommendations right so there's two two kinds of queries um there's one that you think of as as Evergreen queries right and the idea here is they're exciting today they're exciting tomorrow they'll be exciting five years from now probably uh so they're more temporally stable in terms of in terms of like their their interest level um and then there's sort of your temporal queries right so news related outcome related maybe sport related um and usually users will enjoy a mix of these things so this is again where diversity is really interesting right um it's it's one of those places where uh retrieval diversity again comes into play um one of the things that that I found valuable is thinking about that balance but also um when it comes to query recommendations I think personalization is very very key um right it's really simple uh to say we're just going to give you a bunch of suggestions and if someone has nothing to do they'll click on one of them um but the question is can you create a query recommendation system that people will want to come back to right um can it be their source of entertainment or their source of infotainment that's really what we went for um right we want users to feel like they're learning something but also having fun um and that's a really hard Niche um but it's a really really valuable Niche um and so a couple of tips there I think categorization helps a lot right when you start off the cold start problem of of any sort of recommendation system is really hard um so a good place to start is to categorize um and then go from category to query Generation Um right so for each qu category can we start generating a bunch of queries and then you can start doing retrieval on the Generation Um so I would say the right way way to to sort of play around with this is to have a lot more queries recommended uh some way to guarantee and we did this at Neva as well that for any of those queries you have good results um and then which queries do I retrieve for which user so there's really sort of three stages to that process um and I think that's going to be a really really big differentiator uh between a lot of the search folks um going forward uh so it's definitely worth thinking deeper and deeper about yeah I I also kind of guess I see two perspectives between uh like Neva where you're searching all of the web and then maybe you're writing queries for like because I really like this chat with your notes especially at we kind of it's kind of like I think a lot about people having their personalized knowledge and these kind of systems for your personalized knowledge and so I I love this idea of more offline you know process my notes and help me discover something from my notes that I hadn't considered before uh so so are you thinking about you kind of would go through you know each maybe text Chunk of your notes and saying what would be an interesting query for this and then and then and then from there you have you know let's say a thousand queries you embed them and then maybe you use something like you know we we have this thing called refc where it's like you have three vectors and you a like if if I'm if I'm on the search platform and I enter three search queries I would average the embeddings of those three queries to represent Connor and then I would do the vector similarity to Connor with maybe these a thousand queries to recommend the new queries does that kind of sound like a a system design for this kind of thing yeah very very similar design um so if you think about it's the same sort of relatively similar stages right the only thing you're that's missing is the categorization um but um that's because your category is sort of your personal notes although you could also do this characterization there um and be like Oh is Connor at work right now because these are his work queries um is he having fun right now because those are those queries um and that's really really helpful right because that takes it from something it's that's interesting to something that's useful um could you give me um suggested queries related to the meeting I'm having an hour from now um is what goes from cool to holy crap that's useful um right and and so there are these like small unlocks that you can do um and that's where temporality comes in what am I doing today am I sitting down and opening up my financial uh landscape right am I doing financial planning today great can you start suggesting um queries that'll tell me what my current state of finances is um and so my notes of like oh what am I going to invest in a year from now just pop up um that's what makes it like to me magical um right um and I think there's a lot of scope to do this here there's a lot of application layer businesses that can be built um around connecting my current state to my base of information inform yeah I love what you just said about the whole what am I doing today it makes me I love all these like redesigns of note taking software with you know it sounds like a super ripe opportunity for it and yeah to guide the query recommendation with this to-do list so you know hey I'm having a really serious day don't don't give me anything fun you know only useful uh awesome so tamy that was such an excellent overview and I I think you know I'm already so impressed with all the knowledge you have on machine learning and and now so this transition from Neva starting your own company can you kind of tell this intro journey of like the initial problem that you set out to face and and maybe we go from there yeah absolutely um so I'm sure you're probably familiar Neva got acquired by snowflake um a short while ago um and and to me I think the the AI Market is so exciting right now and it's so nent um that there's there's a kind of magic to it that I really wanted to stick around I'd also just come from you know a really large tech company uh to a startup and and really enjoyed that World um and so I set about sort of asking the question of where can I create value in this ecosystem um right and I I I will be very transparent I made a lot of um interesting mistakes uh going down interesting Pathways maybe not mistakes just not best suited Pathways for me explored a lot of application layer capabilities um and what I really realized was my knowledge base um and and my comfort level um was a really good fit for infrastructure more than application uh you know one of the things I toyed around was with e-commerce um and and very quickly realized while there's a lot of value to be had there I don't shop much online and so I don't really understand the the pains and in experiences of folks but I've spent you know pretty much all of my adult life around machine learning Engineers um and and software Engineers doing machine learning that's that's a pain space I understand pretty well um and the the Delta that I found there was folks have really good ideas um but are relearning a lot of the mistakes we learned in the ml World in this new gen cycle um and so can I accelerate that process um can I at some level make AI make sense um right that was the big bet um and from my time at Tik Tok one of the big pains that I'd realized um was really that the biggest value ad for ML um is just retraining old models on new data um it's as simple as that continuous iteration is one of the biggest value drivers in in machine learning um but it's also the most painful thing to do um no one gets valued um in in the team usually for any ml team to retrain old models on new data most of the value you get is for you know building cool new architectural systems um and so that kind of just gets swept under the rug um but it's so value creating um and so I asked asked the question of you know can I make this simpler right can I automate that whole process out so machine learning engineers and AI Engineers can go do cooler things um but that's obviously a while away for for the Gen Market um the bigger question for Gen Market the Gen Market is how quickly can we go from idea to production ready implementation um right and there's a couple of steps there the first step is turning your AI prompt into an API endpoint um and once you do that you're basically creating a lot of infrastructure around that so you're creating some sort of data capture infrastructure right how do you keep your vector DBS fresh if you're doing rag um you're creating monitoring and logging infrastructure um and all solutions for these things exist but they don't exist in one place um and so that's really what we're creating an emissary um our goal is really to create the cockpit um for AI native teams and AI native companies um right can they be in one place and monitor all their API end points monitor all of their like vector DBS um because no one's generating one prompt um right and so when you come to mser you can turn an AI prompt into an API endpoint you can do versioning over all of it you can do ab experimentation over all of it logging monitoring everything just exists um you hook us up to your uh regular drive and we'll provision a vector DB for you and keep it fresh um and when you're ready and this is really key for us um we transition you out of the large language model you're using to your own proprietary model so model distillation is really where we come and add value because there's a lot of platforms where you can you know monitor your prompts that's great um for us we think the whole prompt journey is a transitional process no one's going to stay in the prompt world you come to the prompt world you experiment you iterate and when you're ready you go to your own Fleet of fine tune models because that's where you're like unit economic Mak sense it doesn't make sense to pay 4 cents a request if your system's getting used like 100,000 times a day um and if it's not getting used 100,000 times a day uh you know user adoption is your big problem right now um and so you can basically go from this world where you call this huge model that can do a million things down to this one model that does one thing really really well but that's the only thing you want it to do um so that's that's the platform and then once we distill the capability you want into a small model we keep that small model fresh over time uh so because it's a closed environment we're able to do continuous iteration for you just right off the bat um and that's so for us we're really trying to create this like operating system for the AI world that's what we're excited about um and and we're taking it step by step you know uh but that's the the product in a nutshell amazing there's so much in that and then operating system I love that kind of phrasing that's definitely involving is you know a great so so okay so for my and I also wish in the introduction I I had mentioned that tan May's bullish on fine tuning and because you know I asked this question a lot what do you think about fine tuning versus zero shot and and I get a lot of zero shot people because I I think the zero shot has done so much for evangelizing AI systems you know yeah and so I and yeah but I I can definitely see this kind of fine-tuning the models to get the better performance and there's so many kind of particular examples of this so so yeah so there's so much in that description to unpack but if we could start with I think I want to if we could cover this kind of um rag evaluation is something that I think is picking up a lot as you know more and more people are using it they're wondering how is it you know like and so I I I'm curious about this sort of uh maybe AB monitoring and the kind of tooling behind it that maybe um you know if you could have a really simple API that could update your embedding model for for you and then you also have this kind of observability tools is a common thing that like arise AI uses you know they have a great observe how your rag system is going and ragus is another cool one and so so yeah so I I guess that kind of if we could start on that kind of like embedding ranking maybe search model for processing you know search because you know I know we both you have the search background I'm very biased in thinking about search frankly but but like this kind of like just an API that makes it extremely simple for people to you know switch to their own custom model and manage it could we maybe dive into that a little further yeah 100% um and and I completely agree with you by the way on on zero shot prompting sort of evangelizing the space um and I'm very very glad that it exists because you know I've been doing ml a pre pre the Gen hype um and I've never been this excited about the number of people in the market right um it's really unlocked this uh market for for everybody and for software Engineers which is who we're really excited about as well um we really want to see software Engineers succeed at pushing AI capabilities into production um and and that's really the key unlock the analogy that I love there to that and that's that's one thing I'll say to that debate is is the invention of the 3D printer right um a 3D printer if you think about it can do anything it can like print anything out and it's like cataliz this interest in building things that never existed before a 3D printer um but at the same time when you want to mass produce something you create a mold out of it and you send it to the production line um you use the 3D printer to iterate you use the 3D printer prototype um you'll even use it to maybe make your initial sales um right and we think that this is exactly what um llms are to the geni market we want folks to experiment we want them to actually even get over the cold start problem because for us pre um llms it was really hard to get your initial batch of data and so it was really hard to train your first model um but that's the big unlock that that llms have been able to give us um so what we always say to folks is come with your prompt right um hook up your prompt to the system we'll give you a feedback hook and when it comes to evaluation this is why I'm I'm bringing this up um let your users be the judge right um let them give you the feedback on what's good output and what's bad output and once you start learning from that um let us take care of distillation so that same AP endpoint will now become a distilled model that you own um this is proprietary AI that you've built by just coming up with a prompt um and the biggest Advantage there is cost and quality so it's a very simple win on both fronts um right you don't have to worry about output variability once your model is distilled um it's harder it's it's you'll have to redistill when you change the prompt um but while it's in production it's good um right you can trust it you won't get a 3 a call and this has happened to me by the way um that like oh the pipeline broke down like what's going on dude um right because the output format changed so once you start putting llms in pipelines you have this whole other problem of you no longer get to just say the output is cool um it's not a final end user product um so there's a lot of these implicit problems that pop up um and when it comes to evaluation I think what you really need to look for is um a continuous metric right the more continuous and implicit your signal um and and we'll also talk about sort of the existing rag metrics but when you're talking about collecting feedback from users one thing I'm very very very bullish on is implicit feedback um right the number of people that actually click a thumbs up or a thumbs down is very low um but what you can look at and and this is what was our big unlock at Tik Tok is things like stay duration um right I don't need you to press like if you watch the whole video it means you probably didn't dislike like it too much um right and that's kind of one of those big things that I'm hoping software Engineers will start thinking more and more about if you can come up with an idea of what you want to prototype and how you're going to measure whether it's succeeding or not that's all you should need to do um that's where this infrastructure um should come in and say great you came up with the idea you came up with the initial prompt and you know how to measure the success everything else don't worry about it um right and if you want to worry about it as you're learning one of one of the things we focus on on the platform is is also helping people develop more intricate knowledge of metrics and and of models and things like that open up the back and start playing around with these a experiment if something's better you know stick with that thing um but you need the tools to be able to do this all in one place if you want to do this um I think a lot of what folks are doing in the rag evaluation space is is contingent on and please correct me if I'm wrong here um sort of evaluating llm outputs with other llms um that seems to be a very common metric and I don't think it's a it's a bad idea um I think nothing beats user feedback um the only m in ml um I I won't speak for for AI just yet I'm still learning a lot um the only one ml used to be interactions um right um we called it data but it was really just interactions because everyone can go buy the same raw data um but can you collect your own uh proprietary data used to be a real mode um and I think interactions through feedback is also going to be a very meaningful mode uh for the AI Market yeah again so much in that just fantastic and so I okay so the ver that 3D printer analogy just really stood out to me I haven't heard that before and I really like it but well so I think with this kind of fine-tuning of the language model if we kind of stay in that category instead of you know also putting fine tuning embeddings and ranking in the same kind of category and sort of making it you know in clear what we're talking about but this kind of like dis distillation of model the model it has the obvious benefit of now it's smaller less parameters and thus your margins are better it's cheaper easier all that and that's fantastic and a really interesting thing that Jerry Lou had published with llama index is like evaluating the open source language models like um there are these Advanced query Engine things in index like SQL router or multi-hop question decomposition and Jerry was saying you know here's how all these open source models the smaller sheber ones here's like the green check Red X of what of what query engines they can follow and I I've been very interested in tool use API formatting with like the gorilla large language models and so that's another case where you can compress it and then it's way more economical but the the thing I really want to dive in on in the 3D printer analogy is this kind of um like if I'm trying to get my language model to be able to have this conversation right like speak to me this intelligently about machine learning like am I going to be able to fine-tune a model you know compared to like gbt 5 that I think that's the big question because the I I don't know if the 3D printer you know because it's you know it's not like printing little plastic like gears it's you know it's quite the language model so yeah that that to maybe pause the question before handing it back is is um yeah can we find- tune models that are as powerful as gbt 5 say really good question um I think part of that I can't comment on because we don't know what gb5 will look like um right but we can ask the question of of what does it mean to have a new generalized system and and you know we both started our careers in in object detection so I'll lean into that a little bit um YOLO are great right there was a V3 when I started um there's now like a V10 um and they're not identical uh to to generative models but they're they're pretty generalized when they come out of the bat they can do 10,000 classes or used to be able to 10,000 classes um right and even then every new YOLO didn't mean um that your verticalized models were no longer useful what it did was set a new Benchmark from which your verticalized models could build on top of and that's what we tell a lot of folks uh who who are thinking about these decisions is um your distill model doesn't stop learning so if G gbt 5 comes out tomorrow and it's so much better than everything else great route it back to gp5 for the first 10 20,000 requests um you know obviously start by AB experimenting don't just move your 100% of your traffic to gb5 um but start by learning there and then this distilled model will now become the new it'll it'll build off that new benchmark um the advantage that you have um with with distilled models is they're not frozen in stone um they can always get better on top of the most Cutting Edge llm uh but also when five comes out if it if it is bigger and and that's you know a supposition um it'll be more expensive um there are fundamental costs associated with running llms um that that are setting a a floor on the cost right now a lot of these operations are subsidized um right even at for sents a request or whatever it's probably still not breaking even is is what I'm hearing I could be completely wrong um but it's it's still not even breaking even and so how valuable do you think your AI feature is going to be to your users um is a really important question to ask not AI all AI features are built equal um it's very unlikely that someone would pay $5 for a single summary um right um so how much can you charge LGE and then how much is it going to cost you to run these large models you have to be doing that napkin math um maybe you don't do it today but you do it in six months um and so there is that big concern of um are we doing this in a manner that's sustainable um and and I I would always encourage people to ask that question yeah another podcast that I'm really proud of is when we had roit agar wall from Port key who's doing like llm load balancer between the apis and you know that kind of AB testing the particular tooling I find that really fascinating yes so I I don't mean you you're really painting the picture and I agree with the knowledge so I don't mean to be I agree with you actually with my I'm just trying to like get the other arguments out and and I really want to say like so I think the so this kind of um integration of the user feedback with gradient descent it it sounds very like obviously that would be super strong right but then I'm also curious about like if you can kind of tune like there's this other idea that you can tune the prompts or maybe add more knowled to your retrieval database in Rag and then that's that's like another way to tune it with the feedback it's not you know it's not as direct is putting gradients through it but you could Pro there's probably something you can do right where you just kind of tune the promps yeah yeah 100% um and I think those things are are quite promising I will say um one of the things we do on our platform um and and I encourage everyone to do is is actually look at all options uh right fine tuning might not always be the right option and and a lot of folks are are pointing this out now which is a good thing to do um the thing we want to do is Empower people to understand the difference in uh cost and quality that's it um right I want you to be able to say this is my total cost of ownership this is my total cost of ownership um how much do I want to pay this is the quality I'm hitting at here and this is the quality I'm hitting at here there will be use cases I think we're trying to generalize the whole AI Market a lot and and I might be guilty of this too um but there will be a world where fine-tuning will be cheap cheaper and better um there will be a world where prompting will be cheaper and better um what we need to do is have those options on the table um which means having this infrastructure to distill having this infrastructure to AB experiment and having this infrastructure to capture feedback um if we can bring these things together um then we can start making more Intelligent Decisions about these things um I think a big part of building an infrastructure company is not being highly opinionated about the outcomes of AIC specific use case um and so we just want to give people the tools to make those decisions um and yeah I think um sorry am I am I missing uh something from from what you called out or oh no that was perfect and I I agree completely with what you're say Yeah in use case dependent that makes sense there's the already just the formatting the tool use like the gorilla kind of lesson that already that already makes your case for that thing and I think the only question that people are still trying to wrap their heads around is like kind of the general reasoning engine thing and and I think that one is like because that um that Falcon 180b I guess maybe you would have thought that they would or even like the Llama models that they would fine-tune that and then now we've got back to the old world world of like bioert legal Bert where it was like the domain expertise Bert like I I don't think we I don't think that quite happened with the general reasoning yet but yeah that kind of prompt to model I find it so fast yeah I think here I I will say one thing uh the market is still and I I keep Haring on this for for a very specific reason uh the market is still so nent um that we're not really looking at the standards we will be held up to to commercialize a lot of this um right demos are fantastic um and they create excitement and they create a lot more research and uh you know they catalyze activity um but willingness to pay comes with a very much higher standard of of performance um so even though we might not be seeing it today um and and this is not my term I I kind of stole it from somebody else um but but what that person said and I really value this is there's a lot of tuition money floating around in the AI market today um right when you pay tuition to go to to one of these um colleges um six figure tuitions or or private schools um you're not paying for for what you getting you you're paying for being able to say that you went there um and right now a lot of businesses are in there more this mode where they're paying to be able to say they're in AI um right or using AI or evaluating AI um the question will come pretty soon of like can you charge your customers for this um right um and and that's where we'll see a lot of performance squeezing um happen um right now we're exploring possibilities uh but once we find the ones that are valuable we're going to see a lot of performance squeezing come into play um so I think there's two sep separate stages uh of of this journey and we're still to reach a big part of the second yeah so I'm really sold on it you think you make extremely great points and so now my question is the fleet of models do you know we're very curious about this like what's the future of databases and so you know hugging face Brands themselves as the GitHub of machine learning models I know that you have some ideas about Laura fine tuning and how you have the sparse weight Matrix what's the best way to manage a fleet of small models well hoping it'll be us uh that's that's the hope uh I can't I can't promise it'll be us but I do hope so um I think both on the data side and the model side um we hopefully will start asking this question a lot more of uh why right um we we are going through this journey of um it's great to store everything you can store um but hopefully we transition to a world where we say okay we need this data for X um right and and usually you use data for two big reasons one is to generate insights and the other is to generate experiences um and so maybe we'll transition to this world and and this is more a hope than than a prediction um of we need to create this experience what data do we need for it let's start collecting it we need this Insight right we need to know are people um happier in the morning or evening let's start measuring their happiness in the morning and evening um instead of saying oh why don't we also measure every 60 Minutes um their their happiness which is kind of what we do today um because once we start thinking about we're collecting data for this purpose the way we structure and store that data will become so much more important right if you know why you're using data right now and and this is laughed at across the ml space right most ml Engineers uh spend most of their time data engineering um right or or most data engineering teams are larger than ml engineering teams and there's all these funny graphs about data scientists spending time cleaning data um all of this exists because we think about why we're going to use data after we store it um if we can invert that flow um right and and that's a big bet for us at at msri as well we're saying look we'll give you the data we'll store the data for you in a manner that you can use um in this case the purpose is fine tuning models or or measuring feedback Deltas um we're not just going to log everything we can log uh because that's just going to be a dump um and actually at at Tik Tok and a lot of big machine learning companies that have been doing this for a while even though you have large logs of data before and and this is actually part of our project um design right before you train the model you collect the data in the format that you expected to go into the model for a week or two weeks or three weeks um so we're al already implicitly doing this practice at the places where this has been happening for a while because you need to be able to guarantee that when you train TR this model on X data X data pipeline is already reliable and consistent um so that's my bet for for how that data storage Market or model storage Market uh transitions yeah I I really like that um you know that kind of uh that's why I really like the kind of GitHub for machine learning analog you know how you have like the GitHub actions and all that good stuff and so I imagine a new machine learning model will go through a similar kind of um you know checks and and all that and yeah I guess like because uh the like that kind of question like with we8 it's kind of like with vectors and it's like you know we vectors are the new data type and so we're designing a database for this new data type and I'm curious like if model weights could be a new data type or or maybe you know it's another kind of vector that has a s sort of annotation to how you would you know project it into the Transformer and yeah just so many interesting ideas sort of Tammy wrapping up the podcast sort of I'd love to ask you this kind of futur looking question like like what on the horizon excites you the most yeah um I think I'm really excited to see um which application layer businesses uh transform uh life experiences it's it's a it's a bit vague um but I'm I'm very bullish on on AI actually being able to make people's lives better um I think there's a lot of fear around and and uh very Justified fear around um but to me this vision of AI making everyone a supervisor is really exciting right can we promote everyone one layer up their job um right so instead of now summarizing an article if that's what you do on a daily basis you tell them a sign or delegate a model to do that and you delegate a model to do everything else and you basically become a verifier um so that's a future I'm really really excited about um and that is contingent on us having these building blocks um that are super super reliable um but if we can do that you know my hope is we can fundamentally improve The Human Experience um so that that that's what I'll I'll leave you with love it tan May thank you so much for joining the podcast there's so many interesting ideas in here and concrete and just really an amazing end to end thank you so much thank you so much for having me it was such a pleasure ", "type": "Video", "name": "tanmay_chopra_on_emissary__weaviate_podcast_75", "path": "", "link": "https://www.youtube.com/watch?v=iIy2pbXXxX4", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}