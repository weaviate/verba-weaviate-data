{"text": "Thank you so much for watching the 23rd episode of the Weaviate Podcast! This episode dives into a new technique for ... \nhey everyone thank you so much forchecking out the wva podcast today i'mhere with ori rahm the lead author oflearning to retrieve passages withoutsupervision i think this is such anexciting paper about self-supervisedlearning particularly for training theseretrieval models and retributionretrieval models again with wva this iswhat we use to produce the embeddings ofsay text data particularly for thispaper that we use to retrieve to findthe nearest neighbor with our query andthis new work is a super exciting way totrain these models self-supervisedmeaning that we don't need tolabel data and this can work with justsay you know wikipedia or archive or youjustdrop in a huge text corpus and it canbootstrap a loss function to optimize adeep learning model to produce highquality representations of data soenough of that for me ori can youexplain what's new about this paper andthe ideas behind spider recurring spanretrieval and all these cool ideasyeah sure so uh first thanks forinviting me i'm super excited to be hereum yeah so regarding spidersorecently we released my colleagues and ifrom tel aviv universitywe released a paper onunsupervisedlearning of dance retrievaland basically the main idea of ourtraining system is to use the notion ofrecurring spans which are engrams thatappear more than once in a given contextor in a given document for example onwikipediaum to uh to construct pseudo examplesfor uh for contrastive learning but in atotally unsupervised fashionumand uh yeah actually we were uhi guess we'll talk about it but yeah wewere quite surprised by how far we canget uh dance requires like how good canthey be without using any label examplesduring training at allum and yeah that's that's spiderbasically um yeahso can yousowithout kind of getting too much intothe whole research story i really wantto hear about your perspective aroundself-supervised retrieval models just sowe can kind ofstay on that topic a little more compareit with say supervisor retrieval modelsagain the overlapping engramsyeah sure so um you know for theperspective side uh in the perspectiveside story umso i think that when we started thisproject um there were mainlyeither supervisor recruiters in thedense side of the retrievaland obviously unsupervised ones in themore lexical spar side of uh retrievaland yeah uhsupervisor troopers were ma were reallygood for thefor their data set the data said theywere trained onbut theybut they started i meanuhwe started to observe and papers startedto come out and show that they'reactually not that good for out ofdistributionand uh you know even in the same corpusbut other types of questions uh thenthey fail quite uhquite miserably i'd say um and ourintuition was that if we can uhif we can come up with a good signal forself-supervised retrieval uh then maybewe we will not overfit to specificdistributions of questions and corporaand all that and and be able to be morerobust and andyeah soi think that actually like during theproject we we didobserve thisi mean we did confirm this hypothesisi'd say thatum i meanwe'll jump to the results i guess uhlater butum spider for example is better than dpruh in the you know in the zero shotsetting without seeing any examples atall uheven you know in very close settings towhere dpr was trained on for example ifit was trained on natural questions andyou evaluate it against trivia qa thenin this setting spideran off-the-shelf retriever that didn'tobserve any uh um labeled example duringits training is just better so umyeah so that's i think uh umthat's a hypothesis that we confirmedand and also other papers like paralleluhto us i also showed very similaruh observations and umanother uhand and from another uh perspective uhthey're also these pre-training methodsare also very crucial for the success ofthe umalso in the supervised setting so forexample if you take aspider and fine tune it you know to uhto specific data sets then you wouldjust get uh better models for this dataset and bettertransferable models that better transferto other data sets as wellso umyeah so we were really excited by thisuh you know by this direction and westill believe[Music]a great believers i'd sayyeah i think um so so from myunderstanding of the experiments we'retraining on wikipediaand we have these different questionanswering data sets so say we havenatural questions trivia qa webquestionsthese are mostly question answering datasets derived from wikipedia is thatcorrect yeah yeahso dense passage retrieval which is thesupervised learning baselineis trained on say labeled open domainquestion answering data sets sobasically what the way that this worksis the the data labeler will receivesome context like say it's a paragraphfrom the wikipedia article about oxygenand they'llthey'll come up with a question and ananswer from that contextand then the information retrieval taskthen becomes given the question can youretrieve that context so that's thetraining data for the supervisedbaselinesoit sounds like the spider models trainedon wikipedia with the self-supervisedlearning task are able to outperformin in the same uhdata distribution sort of if like so dprhas the train test split and dpr trainson this train set of questions this testsaid they're both being evaluated onthat same test set yeahsoi feel like that's a huge resultespecially because you cankeep going with the datawith spider right like yeah yeahyeah yeah exactlyso spider doesn't outperform dpr on thesame data set that was trained on but itdoes beat it quite consistently in otherdata sets over the same corpus yeah soyeahyeahand i think that's just absolutelyremarkable and um i think quickly beforegoing a little more into say the spideralgorithm recurring spanner treatmentwe're going to talk about the querytransformation how this mimics bm25lexical as well as this kind ofuh you know neural network style fuzzymatching but uh yeah i kind ofand i've lost my training thought alittle bit i don't remember all thesetopics but i want to talk a little moreabout the the scaling of databecause it's self-supervised so it'slike you know we can we can grab a buncha bunch of more data sogoing from wikipedia to say archive orout of legal contracts like yeah or justweb scripts what happens as we scalethis algorithmum that's a great question so actually ithink about it quite a lot umso we didn't uh try that on othercorpora mainly due to you know resourcesbecause we're in a university and allbutumi believe that um it willwork on other umdomains as well especially onon domains likenewsand archive and stuff like thatumyeah i i i don't think that there'snothing uh uh verywewikipedia-ish about uh this uh trainingparadigmum except for the part that maybeperhaps wikipedia has likemore repetitivenature than maybe other domains butumi don't think that that's the case fornews for example andmany domains thatrequire retrievalespecially for open domain questionansweringsouh yeah that's a great question and wemight um i mean we do think of uhgoing like on a larger scale and maybetraininganother spider on more dataumbut i i don't have a concrete answer toyour uh to your question but i dobelieve that it will be pretty goodon other domains as wellyeahyeah i can from kind of the vvaperspective i'm so excited about howthis interface lets people say say youhave like a blog orlet's use the blog example and you havelike 100 articlesand you want a specific retrieval modelfor your blog and your specific languagei love how these self-supervisedlearning algorithms let you just youknow have a way to train it withoutlabeling the data without retrieving thecontext deriving questions and howeverlong that might take to get like ahundred thousand to push an answeryeah i think that would be a great timefor us to dive into how recurring spanretrieval works soyeahumokay i can i can tell you my head ohsorryyeah yeahgo ahead go ahead and ask your questionoh so i can tell you my my understandingof it so far is we look for theseoverlapping engrams within the samearticle and so i think that's anotherreally interesting use of inherentstructurein these in say documents and that's thething about self-supervised learning isyou're trying toexploit structure and labels that arealready kind of in the data so you takean article like oxygen and then you useoverlapping engrams so say they sharethe snippet uh atomic number is eight oryou know some engram like that and youuse that to form the positives and thenegative is some you know some pat somepassage 100 word chunk that doesn't havethat engramand then there's a query transformationthat's applied so50 probability flip of a coin you eitherdelete the engram from the positive oryou keep it and that's kind of the ideabehinduh having this sort of like fuzzyreference where you're sayinguh i don't know who wasblanklike kind of referring to it in thatkind of way so uh is thatyeah exactly um yeah just one uh oneminori mean you describe it really well butuh theum the masking that the deletion of thespan is is not from the positive passagebut rather than uhin the queerthe passages remain the samethroughout our trainingand yeah the yeah the idea behind thismasking is that you want to be able tomodel both lexical overlap whichuh which is encouraged in the case wherethis uh engram does appear in the queryand also appears inthe positive passage and then we can sayokay there's a term overlap between thequery and the passage uh but when youmask the when you delete this span fromthe query umthepre-training test becomes more semanticin a way and more about the context ofthe of the query anduh yeah in a way we try to you know tomodel both this these complementaryskills umthat you know uh umone is uh attributed uh to denserecruiters and one is more attributed tosparse retrievers but eventually ibelieve that we want all of them to bejust like in a single vector or maybemultiple vectors but in one model thatis able to modeluh this interaction both in a semanticway and in a lexical way between thequery and thecorpusso that's what we try to to modeland the query transformation is mainlyit isboth about this and about um being moresimilar in distribution to questions orto queries ofsearch engines or whateverbecause we don't want it to look like apassage like a long test if we want itto look like arelatively small window short window uhso we can control we basically controlhow long is the query we sample it fromour uniform distribution between i think5 and 30 tokens around the recurringspan and then we just randomly eitherdelete it or keep it in the queryso yeah but the passages both thepositive one and the negative one theyremain the same throughout trainingso one more and a lot of ideas in that ican't wait to get back into sayhybrid retrieval where we combine bm25as well as dense retrieval models andget into that discussion one vector torule them all compared to the multivectors and nothing but i one more kindof detail to the algorithm i wanted toget behind is the span filtering so whenyou do the overlapping engrams youprobably get a lot of like uh you knowas theisright like conjunction oryou know like phrases that don't haveany real meaning and thus positives theywould be useless so how do you you knowprocess all the engrams yeah totally soyeah that's a great question umso we did want uh to to filter out spansthat we don't think that uh bear anymeaningum and but on the other side we didn'twant thisthis filtering to be like a modeldependent you know because many works uhuhthat used salience fans in in all sortsof ways they mainly used they mainly didsomething like you know named entityrecognition or something like that andthen they took the the entitiesuh and and treated them as aliens fansbut we wanted something that is uhtotally unsupervised doesn't depend on atrained model in any wayandthus are filtering included uh first uhremoving any engram that is umthat it only uhincludes stop words so for example yourexample with as the and all these wordsthey they don't cut they don't uhcontribute to the um to this uhengram uhuh processand uh we didn't take any engrams thatany any unigrams uhuh butwith thatuh welooked only on spans that are uh twothat are by grams or longer than thatum and i think that's it maybe we hadsome more filters i i don't recall butum these were the the main ones umbecause as you say i mean you caneasily get with like uhof theuh in themand they will dominate the wholedistribution of your uh stands andthat's not what we want to achieveum so yeah but i do think thatuhfurther thatwe can further improve these uh filtersand and you know have and be moreprecision oriented maybe then recalloriented that that's a trade-off thatthat is reallyimportant and interesting to to explorebut umwe kind of stick to ourinitial filters and and kind of uhuhused them in a they were constant uhthroughout our researchso yeahcan we get a little i want to get alittle more details about these salientspan masking how does that so you haveanother model that classifies somesalient span that would be useful forpositive what would be the thinkingaround that kind of algorithmyeah so maybe like in a general waysalient span masking is used for exampleuh for training t5 forclose book open domain questionanswering andbut not in the context of uh recurringstandsso these are kind of orthogonal thingsright i mean recurring spans are in awayuhlike a self-supervised way to obtainsalient spins i'd sayum soin the context of spider for example uhyou could think of uh of a process thatuh you knowlooks only on theoverlap of some salient span from anamed entityrecognizer for example andits overlap with recurring stands andonly taking those as like a higherqualityset ofspansand be more precision oriented in thatmanner than reoriented butandit may even improve i i'm not surebecauselike any other self-supervised method itis very noisy but you know you use largebatches andand in a very longtraining process andyou just uh you hope that you know thethe signal to noise ratios is just uhyou knowthat it's uh that it's good enough andin our case i believe it was good enoughbut but then againuh uhmaybe you can improve it even more anddue to the fact that it'sself-supervised and you can do it likeon very large corporabeing precision oriented doesn'tnecessarilyumhurt you in any way maybe you can justlook at what on a larger scale of dataand be and be very picky on what on whatspans you take from ituh and you can still train for very longand very large batches andmaybe your signal would beeven betterreally interesting so theceiling span masking idea is like whenwe're picking that token we're going tomask out or even we're masking out awhole span in models like t5 you don'tjust replace a single mass token youhave like five words that it's gonnayou know decode so you so you try tofind the phrase that would be you knowthe most information dense and almostsounds like an active learning kind ofway where it's like what would be themosteffective next training update yeahthat's so interesting how theself-superviseduh overlapping engrams can bootstrapwhat's the most selling thing so i'm soexcited to get into the training detailscould we could we start withuh you use the info nce loss right whereyou have thebatch of negativescould we start with comparing info ncewith say triplet lossyeah sureso umyeah so we were kind of inspired by thedpr framework uh in our uhin our training uh so what they did issimply takethey tooklarge batchesofquestionsand each question hadits positive passage which is labeledand the hard negative passage which camefrom ahigh-ranked bm25 passageuh which doesn't contain the answerand thenbasically their training looks like foreach question you have like amulti-class classification problemwhichtakes these two passages and all theother passages of the other questions inthe batchandtry to optimizethe model such that the representationof the question will be similar in uh inthe dot product senseto the positive testage andand far away from all the otherall the otherpathogen beddingssothat's uh i mean this this framework waskind ofreused in manysettings in machine learning and somecall it as you call it uh influenzaesome call it uh the contrastive losstoday because[Music]uh you know uh i guess uh since thesimclear paper or orsomethingumsoyeah and and that's and that's verynatural because in deep learning we weuse the cross entropy loss all the timeand this contrastive loss is basicallyjustcross entropysuch that you know every questiondefines a multi-class classificationproblemwhich if you could you you would want todo over the whole corpus but you can'tbecause it's too costly so you justestimatethe partition function from thefrom a sample ofnegativesfrom the batch and this hard negativethat you umthat you bring uh uh you know um thespecific uh uh hard negative uh tries toumto be like thelike like it's called the harder uhlike making your task harder umbecause if you just sample from thecorpus then you uh basicallythe test can get uh too easy and we alsoobserved it in the paper we haveablations on that uh in the paper andthey have it in the dpr form of as wellumso yeah uh in comparison with thetriplet loss so i think that it's justmore natural in deep learning to use thesomething like the cross entropy andcontrastive loss so people use it moreoften and we didn't compare these twobecause we just you know umwe didn't try to applythe objectivesbut i do believe that uh they'llprobably work better than the tripletlosses uh but i i'm saying it onlybecause that's you know the commonwisdom of deep learning i guessand yeah so taking from so so this isthe dpr framework and we wanted to andit and it was very umuh very effectiveso we try to think uh can we take thisframework and replace those uhquestion passage pairs which which areexpensive to collectand areveryhard to collect in many domains and manyscenariosand can we replace them with pseudoexamplesandand indeed ourpre-training scheme looks very similarto dpr in that sense where uh as wediscussed before we have uh this uhpseudo query uh which replaces the dprquestion and we have this positivepassage which replaces the positive thedpr positive passageand our hard negative isuh doesn't um we didn't uh fetch itusing pm25 or something it's just fromthe same document and due to the factthat it's from the same document as thepositive passage it makes the task a lotharder and a lot more semantic in a waywherelexicalclues on their own can't get you thatfarin terms ofdiscriminating between these two so uhyeah i think in table four in our paperwe show that removing the hard negativepassage from the documentand from the batchresults in a in a muchin a in a highsignificant drop in performanceat least in the zero shot setting i'mnot sure about the fine tuning settingbut in this in the zero shot settingit's pretty significant it can get tolikei think 6.7 points of uh recall k whichis prettyuh significant sothe presence of power negatives isextremely important for this contrastivelearning framework both in thesupervised setting and in a unsupervised[Music]yeah i think the hard negative is sointeresting the the use of the inherentstructure in the documents compared tothese other kind of things where youknow say the the positives areoverlapping engrams and then thenegative the negatives are just fromother articles so it's like totallyirrelevant it would be like two passagesaboutoxygen and then it's like lebron jamesyeah eiffel towerwithout that structure yeah so so youwill get something out of it but um thisuhfine-tuning uh hard negative isso important for the semantics of allthe of the model yeahthat kind of like reasoningihad one more likekind of just nitpicking my understandingofinfo nce multi-classsoto me what makes it not multi-class isyou have toyou do that like batch matrix multiplewhere you have all the dot productsright so you have this big matrixthat has all the uhi'm trying to do but you have all thedot product scores of everything in thebatch yeah so then my question is sowhen you have this big matrix andin the paper you describe eight gpusscaling that out with the horizontallywith the gpus how does that work how doyou distribute this big batch matrixmultiplication that then gets compressedthen to multi-class classification ohyeah soyeah so the so the scaling part the thedistributed part is mainly in thetransformer part of it so um so we'redoing like a distributed data paralleltraining where uh you have the same youhave the youryour model copied around uhacross these eight gpusand thenyoudivide the the batch to like eight partsandand uhboth the questions and thepassages are divided to these gpus andthen the transformersarein parallel they umthey compute the you know therepresentations and all and then thehorror presentall of the representations are gatheredto the same gpu to gpu 0and thenthismatrix multiplication happens only oncein one gpu but it's very umit's very cheap compared to theto the transformers encoding your inputsso that's not such a big dealand it the matrix is uh something likeit's actuallyits size is like your batch sizetimes uh twice the bedside because eachuheach question has both uh likea positive and a hard negative one so inour case it's like 1000over 2 000 so it's not such asuch a big matrixanduh yeah from what i recall uh it's it'sreally this step of multiplying therepresentations is quite cheap comparedto[Music]thetransformers calculationssowhat goes into the the code with thatwitheight transformers in parallel sync upto one node to do the batch and then getthe loss and then send it back to eachof the transformers how is the code whatextra challenge comes with doing thisumso it's actuallynot that i mean it's it's much simplerthan what it sounds i guess umso torch has you know it'suh it's uh distributed umuh framework and basically you have thisuh gather operation where you just yousimply tell the gpus umfetch meplease fetch the this tensor from allthe gpus and i want it concatenated oversome dimension and then you just get andyou know the whole back pro and all thatit's justimplemented for you and you don't needto evenumyou know to worry about it souh yeah i guess it's much simpler thanthan what it sounds and umumyeah that's it i guessso from your expertise i'm reallycurious to get your opinion on do youthink the abstractions implemented inpytorch fordistributed training for this particularidea is is it enough that it's easyenough to use compared to sayneeding some new model training softwareto say make you know more people able todo this kind of trainingumso i guess that uhlike there are manypapers that implemented contrastivelearning in many ways for manyfor many tasks for many scenariossoumi think that you know the initial umeffort that you need to put in order toum to set up your own framework is is souhyou know it's so small compared to evenwhen where we were uh only like you knowfour or five years agowhere you needed to implement yourtraining pipelinefrom scratch for any task that youwanted towork on and today you have all theseinfrastructures likehugging phase transformers and huggingface data sets and many othersand everything becamemuch easier than what it wasnot so long ago soyeah so i believe thatthe effort is is not veryumverysignificantand one of the so i'm really amazed bysay the google collab gpu everyone's gota gpu nowone gpu though yeah could these modelsbe fine i know the the fine tuningexperiments happen with say eight rtx'scompared to eight a100s could peoplefine-tune these models for their datasets with one gpu yeah um that's a greatquestion uh i think the answer isabsolutely yes umso for example in our fine tuningexperiments i'm not sure if we mentionit in the paper we used onlytwoquadro rdx gpussoeven if you have only one you can lowerthe batch sizeworst case you can do some trick likemaybe gradient checkpointing orsomething like that and maybe thetraining would be a bit longer butyou'll get the same effector justuh or justdecrease the batch size andin the supervised setting it's not thatuhcrucial what is the batch size that youuse it very far correctly in the dprpapers it's really aminorimprovement from like 32 questions to128um so to your question definitely youcan fine-tune such models onona single gpu and also there are othertricks for example like just do weightsharing um from the question to thepassage use the same weight in thequestion and passage encoder that thiswill umthis will the model will be two timessmaller but the results are arepretty much the sameand umyeah so i think fine tuning is is reallyuh efficient and very cheap compared topre-training where we had to use8800 gpus which are prettyexpensiveand other papers like the contributorfor example also used uh you knowthey used i thinkeven more gpus i don't remember exactlyhow many butduring pre-training the umthe size of the batch is much moreuh has much more impact than at finetuning timesouhso in terms of fine tuning you don'thave to worry about the gpus butat pre-training i'd say that it'sit's a lot more important to havethislarge scale both in terms of the dataand in terms of the computeum yeah and but perhaps a way toovercome this issue isthrough something like moco or somethinglike use a memory bank of passagesand umjust have like umkind of like an artificialuh batch in a waythat's how i think of it and that's alot morea lot cheaper than justcalculating the batch umeach timesoyeah that's that's also another optionto how to like umdecrease thethe compute that youthat you use and probably get the sameresults i believeyeah it's super interesting andi suppose one detail on the fine-tuningthing is i'm imagining you know wev8users who are using the vva searchengine for say their specific text andtheir blogsdo you see say for themuse spyder as an off-the-shelf modelsimilar to how we're using a lot ofthese mini lms in the sentencetransformer libraryjust kind of you know using that in thezero shot and then so if we think aboutfine tuningshould they fine tune with the engramsthe recurring span retrieval ideaor should they just you know make theirown little question answering data setusing the labeling scheme we describedwhere you you know pop up a contextderive a question derive an answer andwhat would be the best way forthat's a great question umso if theyif their corpus is wikipedia then idefinitely say just take spider off theshowmake sure that the format of thepassages uh isexactly asas it was during the free training ofspider which is 100 wordsand you'll definitely be okay and you'llhave a pretty good retriever and maybeeven use a hybrid retriever overuh like take a spider and vm 25 andyou'll get a retriever which is betterthan both of themif you have the ability to even annotateuhas few as i don't know hundred examplesthen you probably get uh a significantboost in in the results umspecifically for thefor the distributionof your annotation uhexample of your annotated examplesumyeah so i think that's thethat's the best recipe if you haveanother corpus if if you're not uh uhusing wikipedia as your purpose then iwould say that you can take spider andjust fine tune it with repairing spannerpeople on your specific uh corpusand ummake sure that your passages duringpre-training are just formatted as uhspecificexactly like you intend to use them uhat inference time and if that's the casethen i believe that uh it will be areally effective way to get a goodretriever to your domain andand again another boost willprobably beobtainable by uhby using a hybrid approach with 1.5yeah so now i want to dive into thehybrid quickly i do want to see say thatyeah that query transformation id isreally clever how you you know thewindow length makes it more like a querywith search engines and i think thatwhole thing is super interesting ideabut so can we talk about how we combinespider with bm25 what's the idea behindhybrid retrievalsure so i think okay so uh first we'renot the firstto show that using both of theseof these models togetherresult in in a better model thatimproves over both of them quiteconsistentlyso i think a few months before our paperwas uma paper by uh uhsome authors from uh university of uh uhum[Music]actually i'm not sure where they're fromi think maybe waterloo but i'm not sureuh the jimmy liens leftuh so umso they showed uh this uh phenomenon fordprand they showed that when you use it uhin ain a specific way you get uhyou get an improvement over dprin contrast to what the dpr paper showedwhere they uh where the hybrid model wasuhwassub-optimal compared to dpr itselfumso but the only difference was that umthey use the hyper parameter that scalesthe scores of bm25 anddpr and they find and theyand theyperform hyperparameter tuning for thishyper parameter on the deficit but inour case we didn't want to do thatbecause uh we wanted to be in a zeroshot setting where you don't assume youhave any labeled examples so we just uhuh so we decided to uh assign this uhhyper parameter tothe the value one and and uh that meansthat you basically sumuh some of the scores ofspider and bm25 and uh their sum is nowyour hybrid model scoreand this approach uhis very uh is very effective turns outand it improves over both of themconsistently and quite significantly soi'd say that if you have a retrieverover your own domainumbe sure to to use both of them if youwant if you want to be if you if youcare about your retriever be to be asgood as you canthen i definitely say use both of themand if you can annotate some examples ofyour own then it would even get betterand then again use it in a hybridfashion with bm25 and that's basicallythe best model uh that you can get todayso i have a couple questions about howthis works and first i want to get intoso we we're just going to sum the bm25score with the uh with the dot productor l2 distance cosine similarity fromour dense modelis there any maybe normalization of thescores that should go into this i alsosaw a devil in the details in the paperthat say say a passage wasn't in eitherof the um bm25 or let's just say a denseyou know i'm sure you knowyeah yeah you justyeah that's an important detail so whatwe did is uh and we took it also fromtheir paperis that obviously the set the top k uhretrieved results of both retrieverswon't be the sameso one approach you can take is simplyrecalculatethe scorefor uhfor the second retriever for for exampleif you have a passage that came from vm25 and was it in the dpr set or spiderset you caneither explicitly calculate its scoreits dpr score but then you need toumyou need toto feed it to the to dpr and allso instead of that we just used a verysimple trickuh in which you just take the uh in theexample that i just gave forthat you have a bm25 passageum but it wasn't in the spider score inthe spider set sorry then you just takethe lowest uh set the lowest score inthe spider set from the top k reviewresults and you and you just assign thispassageuh it's its spider score will be nowthis lowest score and that that's justgood enough you don't you don't need todo any more than thatbutumit would be important toto test whetherexplicitly rescoring thesecandidateswillwill result in a betterin a better rankingi'm not sure about it butit does require some more effort sothis simple approach that i justdescribed is is quite effectivesoso sort of the in you know maybe tabulardata pre-processing for machine learningwe do this min max scaling with each ofthe features because they have differentranges soyou know that kind of x minus x maxdivided by x min something like that tokeep it on zero one for the dead scoresand the bm 25 scores that kind ofnormalization of the values of thesethings before we then have that linearaddition without the you know usefulum that's uh that's a great idea i'msure we didn't think of it umyeah maybeit can work maybe even better than whatwe didwe didn't try it butyou do need toi meanbm25 hasthis property that it's um its scoresare length dependent right uhas the query is longer and the passageis longer you will probably get higherscoreswhile in dpr this is not the case youwillalways have pretty much the sameuh the same scores because you justyou have these vectors in the same spaceand just multiply them andthey don't really depend on the lengthof your query or length of your passagesoumbasically you will need to think how youumhow you normalize it in a way that isrobust across different lengths of thequeryandthat will work both for short queriesand for long queriesbut uh i guess that it's somehowsolvable i believeyeah that that the the query passagelength i do that's such an interestingdetail with how the bm25 works i've seensome variants like you know bm25 plusthere's like little different ways ofkicking this work can i and well quicklycan i ask about the reciprocal rankfusion idea where we combine the scoresnot uh combine the ranks from bm25 anddpr or dens retrieval not by the scoresbut by the rankssorry can you repeat the questionah sorry so so instead of doing a linearcombination of the scores bm25s producedyou know spiders produce we might lookat the just the ranks solely you knowyeahum yeah that's also aninteresting ideawe didn't try that but it mayit may be veryvery effective i believe umi'm not sure how exactly like uh uhyou would do that maybe as you said withtheir proton rank or somethingbut umyeah i mean that sounds like a very niceidea toto check out yeahumyeahlike howbetteruhi mean it's really interesting to to uhto do some sanity check of like how uhbetter can you get likehowmuch of theyou know the union of the two sets areyou able toum to squeeze into the top k thatactually is interesting for you uhgiven that you don'tfor example in open domain questionanswering you want the top 100 or top 50or top 20 to beas good as possible and don't reallycare about the ranking inside themthat'soften the caseso it's really interesting to check outwhetherhow much room can you like uh given thatyouuh fixed the retrieved set from bm25 anduh and dpruhuh howlikeuh how many of the umlike how often does it happen that youhave some good passages that are leftout in the um when you do linearcombination and are left outin theoutside the top 20 and then you youunderstand whether you havewhether you want to improvethe combination or the hybrid model in away like you describedbuti'm not sure whether there's a lot ofroom to improve in thataspectyeah i think thei think it canmaybe like yeah like as we design thesepipelines maybe you're doingwell the idea that you've just presentedsay we want to have some diversity inthe result it's not necessarily aboutmake the number let's get the number onething to be the most accurate thing wewant to have some diversity if it's saygetting pipelined into a questionanswering model soso i guess that kind of changes thedownstream application case changes abit how you want touhcom combine ranks you know is it asearch application in which i want togive you the absolute best thing numberone then yeah let's optimize it likecrazy whereas if it's qa you might wantautomated qa2 you might want toyeah it's only relationship i totallyagree it's it's reallyuh application dependentand that's why i believe that in in manyumsearch scenariosyou you want you want to like uh to runacross encoder or re-ranker in order toto get the the best result in number oneor number two and not in the top 20.likelike the open domain case where youdon't really mi you don't really careabout the ranking inside the top 20 or200umso that's what that's something that across encoder uh or a re-ranker uh cancan get you like uh uhcan is still much stronger than just asimple retrieversoso so that's normalso that's another reason why you canclaim that it's not that important tooptimize forfor the top one of the retriever becauseif youum if you intend to run a re-ranker andin in any case then you don't reallycare about the ranking of its like theinitial ranking of its uh inputsrightyeah the cross encoder is potent thatthingbut it's expensive to do a bunch ofpairwise classifications so wewant to try to reduce that so on thesubject of efficiency uhwhat goes like at wva we're super i likeat least me personally i'm super awareof this h sw vector index structure fordoing the retrieval and find the nearestneighbor with the retrieval i'm kind ofcurious with bm25 how do how do you haveefficient search or is it's just ascoring metric could you tell me alittle bit more about the speed ofcomputing bm25 scores between query andthenmassive collectionyeah that that's a great question so wekind of used uhuh pi sirini of the show for bm25and from what iobserved it seemed like it's a bitslower than dense retrievalbuti'm not an expert to you know theefficiency umto the efficiency aspects of danceretrieval versus sparse retrieval i knowthat many peopleumi guess have like different opinionsabout that uh topic and in danceretrieval you also have um you canpretty much control the the efficiencyof your umuh of your index by uh like in theso hnsw can uhum like you can do like a more coursegrade search or you can do exact searchor you can douh you can do all sorts of things andobviously doing exact search is is a bituhis a bit slower but it's not that uhit's not that bad i guess like overwikipedia it takesi think something like umuhsome hundreds of milliseconds uh for onequerybut then if you do uh an approximatesearch then you will get like uh justthen it will be very very fastuh in terms of bm25 i'm not sure whetherpiscerini is like the most optimizedlibrary because i think it's a bit moreresearchoriented thanefficiency orientedi would want to say like uh i mean maybemaybe not maybe i'm wrong but uh that'sthat's how i see itsoas far as i can tell and from what iremember ityou have likeitsis something like umsome dozens of queries per secondsomething like thatumyeah but specifically our work we wedidn't try to optimize this aspect ofquery efficiency because our you knowour test sets are relatively small so wedidn't really care about thatbut there are many scenarios where youdo careobviously like if you want toi don't know do retrieval in a verylarge scale orjust have like a very quick uh responseto a useruh but theni'm not sure what what is the best wayto go in that senseyeah i thinkman i'm not an expert on the bm25 howexactly i know that i think it hassomething to do with inverted index iknow there's things like elasticsearchthat have built up the you know reallyhow to do this efficiently and justhoping to get a little nugget and keepgetting my understanding ofyeah efficiency on that uh i believethat gpus can also accelerate thedance retrieval as well so we used cpusfor our dance retrieval system but umso we didn't try to use gpus for thembutuh i do think that i saw in some papersthat if you uh if you doum store your index on gpus then you canget quite a boost and that makes senseand also maybe you canpair uh you can do like multiple queriesin in parallel and just um and then youcan scale it like on many gpus and andbasically get to the whateverperformance you wantumyeah i think i can comment on thisquickly and then maybe you know eddieand or someone from wva can clarify thisis correct but withwith hnsw we have a hierarchy graphwhere you're it's connected based onthis it's a proximity graph that islayeredso the gpu idea is you want to do abunch of vector calculations at once sowe can trip so we cantraverse a big set of nodes in the graphthis visualization is doing it forpeople but you knowthat's how that idea of i think gpuaccelerated distance calculation alsoplays with the hierarchical structure ofthe nation sw proximity graph but um soall right thanks so much for thesetechnical details i think this is youknow such interesting so manyinteresting details to this could maybestep outside and could you describe tome sort of the meta around your researchcareerputting this project together sort ofyour plans future work that whole kindof thinguh yeah sure umyeah so i thinkthat this work connects to to some otherpapers that i umthat i worki was part of sorry um i think i'muhthat i'm super excited by uh byunsupervised learning andself-supervised learning and futurelearning and basically learning fromvery few examplesor not at all no examples at allandyeah i don't like to work on supervisedtasks that muchbecauseit's it'stoo often you get very minorimprovements from modeling[Music]um changesandin the self-supervised uh setting orflue shot settingthen everything is amplified and um youcan really seethe signalmuch clearer and in a much moreumyeah in a much more amplified wayuh and that that's uh and that's why ireally like it um and also there's ithink there's more room to likecreative ideas and how to use thestructure like you said before thestructure of language structure ofimages i don't know structure ofwhatever domain you're working on andrather than just fitting your data intouh into a model in a supervised waywhich isokay but uhumthinking of uh of what is the specificaspects of your task of your task thatthat you can leveragein a in an unsupervised way or in aself-supervised way uh really excites mepersonally umand i'd say thatrather than that i alsoi'm really excited as as we all bylanguage models and large language modsand whatever you can do with them whichis quite a lot umuh soand now i'm super excited by thei'd sayintersection of language models withretrieval and with dance representationsand there's like a bunch of new work onhow you do use large language models inorder to rank or in order toproduce good representationsand it's uh it's very exciting andthis whole direction of generativeretrieval and stuff like that and itmakes uh and it really makes sensebecause we now have very powerfulgenerated modelsthat we can useandum the question of how to use themfor your task isvery similar to what i said before likehow do youhow do you take advantage of thespecific aspects of your taskuh in a way that language models cancan contribute to soi'd say that the intersection ofuh unsupervised learning future learningretrieval and language models isbasically what iuh what i'm most excited aboutawesome that's so exciting and wellreally quick i just want to get a littlemore on the on on how you see retrievaland language modelsyeah this a little moreso do you see putting these largelanguage modelsin the search pipeline so you know a fewshot learning say we have a query intentclassifier you provide a few examples ofuh the query is hey i'm looking for ashirt that would match these shoes so doyou want shoes to be returned that you'dsay like it's a compliment compared tosay a substitute or an exact search likequeer intent consultationand with the language models you couldprovide a few demonstrations of this andthen the language model can make futureinferences soit so are you thinking about it from theangle of you put the language modelsinto the search pipeline to enhance thesearch experience orthe sort of wreck deep mind retroretrieval augments of language modelinference where the retrieval is likehere's the information you need and thelanguage bundle is like all right i canjust generate anything uh well that's agreat question i i'm a great believer ofboth directions actuallyumso ii doi do like um the retrieval languagemodelsdirection because i think that there'suh there's a limit to where you can getwhen you put the entire uh like yourentire uhumyou trust only the parameters of yourmodeland indeed you get to i don't know 200billion parameters but i'm not sure thatwe need to get there uh if if we use ourdata morecorrectly and thatincludes retrievingthe relevant stuff to what you want todo and justhave your modelbe conditioned on this relevantinformation and then on the the otherdirection that you said is also veryuh exciting in my opinion and it's howyou do like uh the opposite right howyou use language models to improve aretrieval andi do think that it's um it's verypromising but i'm not sure that itshould bewithin the pipelinerather thanuse it asa supervision to train for exampledancer quiverslike umlike recent work from facebook wherethey use the uh the re-ranking of t0 tosupervise umto supervise uhactually it's unsuper it's anunsupervised model uh which uhyeah whichand and it's only in a zero-shot mannersoyeah i do think that we have so muchknowledge in language models and andlikedistilling it into retrieversi thinkpersonally i think it's the futureumand uhyeah so so both directions thatyou mentioned i think that they're veryuh promising and i believe that they'rethe future of language models and thefuture of retrieval and that they'll beuhlike interleavedyeahsuper interesting ori thank you so muchfor coming on the web podcast is ilearned so much from this conversation ireally enjoyed it and thank you so muchthanks for inviting me connor", "type": "Video", "name": "Ori Ram on Learning to Retrieve Passages without Supervision - Weaviate Podcast #23", "path": "", "link": "https://www.youtube.com/watch?v=IWAxkHspjEM", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}