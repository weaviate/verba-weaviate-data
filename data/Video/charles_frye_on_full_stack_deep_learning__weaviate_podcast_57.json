{"text": "Hey everyone! Thank you so much for watching the 57th Weaviate podcast with Charles Frye! Charles is an educator at Full Stack ... \nhey everyone thank you so much for watching the wevia podcast I'm super super excited to welcome Charles Frye Charles Frye is an educator at full stack deep learning and Charles is someone who I've been following along with just for me it feels like my entire deep learning career I've always been seeing what Charles been posting on Twitter and I always feel like I've been learning things from Charles so I think also just the timing of this podcast is perfect because you know full stack deep learning and we have all these new tools Vector databases large language model apis what has happened with zero shot learning and I think this is going to be such a great podcast of covering what what is full stack deep learning now like ml Ops and then full stack like how we usually use the phrase full stack and software engineering and how has all this evolved so Charles firstly thank you so much for joining the podcast yeah thanks so much for having me Connor and thanks for that really uh really kind introduction maybe uh before we kind of dive into it could you give listeners more of a background on just like how you came into deep learning and sort of your personal Journey yeah yeah I came into um came into machine learning and and deep learning through a path that's increasingly deprecated these days um I did neuroscience and biology first um in undergrad I did some research on uh giving people MDMA and asking them how they felt and shooting lasers into Mouse brains to see what the patterns in their biological neural networks look like um so uh but then when it was time to look at doing a PhD I looked around and it seemed like the ml world was really picking up this was 2014 or so so uh sort of the early applications of convolutional networks to really hard computer vision problems um and that looked like it was going to be like successful in the coming decade and so I did a pivot and did a PhD in the Neuroscience department at UC Berkeley but studying artificial neural networks studying initially some like applications to neuroscience and then eventually kind of the mathematics the optimization of neural networks and then from there from the PHD at Berkeley have gone on to kind of end up in this role as an educator in the ML and deep learning space sort of sharing ideas research and how and sort of combining them with ideas from engineering and the development of applications initially at weights and biases it's kind of like a educator developer relations kind of person uh and then now at full stack uh teaching um you know teaching people um most recently about how to build stuff with large language models yeah amazing and I think um Also to clarify for listeners full stack deep learning is kind of the key conversation topic and also the actual name of this course which is awesome and and yeah I that is coming from a PhD in neuroscience at you know a school like Berkeley and then coming in that's just so impressive and I think it's also interesting I I mean maybe I'd like to stay I think this would be a great transitioning topic because the your background in weights and biases and exploring convolutional networks the way I understand weights and biases and I'm sure you're gonna you know help me with this is like this kind of hyper parameter tuning because I remember in the early days of training convolutional networks like the learning rate bash size how many layers the number of hidden units all of this was so sensitive and it was about finding this horrific combination of these things maybe I'll quickly explain a little more about your perspective on ML Ops at uh weights and biases yeah um so definitely that was my view of it when I kind of like first came across it um that was something that really stuck out to me this sort of like hyper parameter sweeping stuff um but as I like worked there and saw the product develop and saw the use cases for ML develop like the thing that really really mattered is monitoring and observability for um for experiments for the like experimentation phase of building an ml power product I think they've like expanded since uh I left like two years ago um and they recently announced a production monitoring beta so it's not just for that um but the thing that I saw was this like um with you know a small amount of instrumentation a few lines of code being able to get like system metrics model metrics all that in one place alongside with like what code was I running I remember I ran into a distributed systems like Heisenberg like a bug with um a bug that sometimes happens and sometimes doesn't uh because distributed training is just not deterministic um or is not deterministic unless you've spent all of your time engineering it to be so um and uh and ran into what turned out to be a a Heisenberg a random bug but I wouldn't have believed it unless I could compare two ways and biases runs right next to each other this was the state of the disc this was the state of the disc they're identical one has the bug one does not have the bug um so that kind of like that's the sort of like software engineering like bug smashing kind of side that bleeds into all the way up to the hyper parameter tuning being able to see what happened what your uh what your configurations those hyper parameters where what the downstream uh results were hmm yeah absolutely fascinating and we're certainly diving back into monitoring and observability the topic and distributed training the topic but let's maybe uh let's set the stage for our listeners of um you know full stack deep learning maybe currently I'm just you know we're thinking about this kind of like data loaders Vector database retrieval augment to generation to large language models how do you see kind of the most common you know end-to-end stack for building deep learning applications yeah yeah that's a great question so yeah so we originally had the you know full stack deep learning course and actually yeah Josh Tobin and Sergey Kari have my fellow instructors um put that together when they were uh grad students at Berkeley um and that course treats full stack as meaning I can train the models and I can like put them into production and then maybe close the loop and take insights from production and turn them into uh stuff that is used to train the model further so it's sort of like in like Andre carpathy's vision of the like data engine and Tesla where it's like you get inside you like collect data by the models being out there in the world that gets added to your training process in some way either as part of evaluation or directly as something you calculate gradients with um that the full stack deep learning engineer envisioned in that class is the person who can cross that divide or sort of go through that entire Loop um so being able to do enough you know enough Pi torch and GPU training and distributed training stuff um to work at a reasonable scale maybe you know not able to implement uh Megatron LM from scratch uh but uh at least uh capable of consuming that um all the way up to putting something in a product and understanding um what makes it a good product versus a not good product um and uh that everything that comes in between that with large language models the full stack developer actually starts to look maybe a little bit more like a full stack web developer where a full stack web developer is somebody who can design a user experience but can also like talk to databases and can talk to external services and so it can build like something that looks like a more like a web application station and at least for now when so much the capabilities are in models that are trained by somebody else available as a service don't require fine tuning and only require like adjustments to prompting or adjustments of program and control flow around them the the role of that that the beginning part that part that looks more like training models that looks more like um you know thinking about uh thinking a ton about Pi torch pytorch lightning hugging face uh that part uh becomes a little bit less relevant I'd love for it to all come back together again um in the coming like year or two as we get better open source models as fine-tuning returns um as maybe even training from scratch in certain domains returns um I think we'll see that uh that come together again and then the full stack is now um uh once again all the way from the the gpus to the databases to the user research uh and back amazing it that's kind of exactly what I'm hoping to kind of figure out my in my understanding is how this can come back to that and how the the new things can still relate to the old ways of thinking about ml Ops and so beginning with the I remember a quote from one of our earlier podcasts with Chris Dawson he this was back when it was just gbt3 and he described it as it's the perfect MVP tool like minimum viable product you you just plug it in and you can see like what this could look like because you mentioned like the data engine that Andre carpathy describes to a lot of people that's like that's a lot of overhead to just get running with that right like so maybe we can just it would stay on this a little more like usually to start building something with deep learning you had you were had a data collection task like a wildly difficult data collection and labeling task yeah yeah I totally agree with that I think these Foundation models are clearly extremely good at building demos uh because now my Twitter timeline is 90 demo um but and then there's like a gap to get to to the final product and I think one of my favorite examples of this came from Nat Friedman talking about the development of copilot he said they like stood up like four different like distinct language models for coding you know with ins inside vs code GitHub so four distinct products that like in a weekend um like they just tried they tried autocomplete intellisense they tried like a chat bot type thing and they tried like a PR bot type thing and um they got all of them to like MVP extremely quickly and then there was like an extended period of turning them into uh like a sticky actual product and the only one that actually made it that far was co-pilot initially they're they're going further with with copilot X now um and I think that's that's like illustrative I think it's kind of common in the development of applications that that mvp is easy and the rest is hard with Foundation models I think part of what makes that eventual development into user product hard is that eventually you're gonna get have to get to the point where you have unique data that is driving unique Model Behavior and for a long time maybe there there were fewer even demos because it was harder with the original gp3 API to like include context um and the patterns hadn't really developed and now that we have this clear pattern for injecting into prompts via retrieval um or uh injecting into the model's Generations via tool use we now have ways to with again without training models and without really needing to have to set up a full model creation pipeline uh let alone like an actual data engine you can get a much better product and that starts to fill in kind of the middle area between just prompting to like training a model entirely from scratch um you know based off of uh proprietary data or or data specialized to your problem um so yeah I see it definitely see the emphasis on the kind of beginning part but it's also still very much early days in the development of applications we could find that there's just the right patterns that lead to prompted Foundation models building the the whole application themselves yeah I think from the first example you gave with trying out four different code completion models I think that's the stage a ton of people are in right now where you you're trying MPT 30 billion alpaca you know vicuna you're kind of rotating out these cheaper inference models which one of these will work for my uh it was four it was it was different products right the same models were being used underneath like say model family um and of course they had a problem of deciding like which model to choose and they ended up going with a small fast model for autocomplete because an auto complete pick a the way copilot works it doesn't matter if you're wrong because users will just ignore your suggestions and people are used to ignoring autocomplete they're used to ignoring intellisense um whereas if you do it if you're a chat bot like people don't like being told wrong things um if you're doing a PR bot people don't like reviewing PRS that turn out to be like fatally flawed uh and so the like quality needs to be really high um and so that was that was their model evaluation problem that they had there um but that's like nested inside these like like what even product are we building are we building this um uh are we building a are we building PR bot are we building uh um a autocomplete uh and that you know key takeaway is the MVPs for all those were easy the products some of them are still under development years later yeah it reminds me of like uh like the early days of like when Jasper started getting popularity and I think Facebook they started having papers where it wasn't about just the the writing assistant wasn't just about writing more texts about like editing your text and what you mentioned like how exactly do we want these language models to interface and and I think that's the question now with Lang chain and yeah and that's was being so something you said earlier I love this quote you when you said uh unique data to achieve unique Model Behavior and then we discussed uh better retrievers and you know we're getting into of course how we VA interest is this like retrieval to the large language model and so this is the kind of the question that's on my mind more than anything else is to achieve unique Model Behavior with our data we've plugged it in we've we've figured out the user interface we've started collecting data should we fine-tune the retrieval models or the language models or both yeah that's a great question um fine-tuning the language model is a little bit more challenging and I would and there's a little bit less to go on in terms of what it means to make it better like you might need user preferences you might have some like cases where you have specific behavior that you want um but you know even despite the advances in efficient fine tuning I still think that's going to be challenging and then you have a model serving problem uh it's a lot easier when there's only one version of a model um much simpler version system um but for so for retrieval I think of that as like a very broad problem right this is like a search or retrieval problem where there's data that lives in some big slow thing like a disk or a database and we need to load it into some like very memory sensitive place um and that memory sensitive place is the like context window of the large language model and so retrieval via embeddings is one very good way to do that it's a good way to do that because retrieving information from a vector is how a Transformer computes and so it's likely to be the case that the things you calculate with uh with Vector similarities with DOT products uh are are likely to be uh similar to what the language model will do internally uh but that is just one of many ways to get information out of a large store and into uh the small hot cache uh where it is needed and so yeah so I see that as like probably the first like the zero if place to start is like your data your pre-processing as always nobody wants to talk about it but that's the place where you should spend your time how do I parse these documents am I pulling garbage out of my data store uh so just like classic elt and data validation problems uh so definitely that's like that's step zero and then it's like look at the retrieval process of that increase that's increase its quality probably maybe it involves passing some gradients through a retrieval model but honestly I think you can do a lot with like metadata filtering and and other classic information retrieval techniques and then those things uh that improves the inputs to this Downstream uh Downstream model yeah definitely a lot to unpack I think um maybe if we can just stay on the the last topic of you know the metadata and how much you can bootstrap we when we originally released hybrid search where we're combining say the zero shot open AI models with bm25 and that maybe you could also say re-rank that with a zero shot cross encoder which could also kind of push it a little further and it's it's like yeah maybe you can just kind of get pretty far with these zero shot models when you orchestrate it like that especially because now the large language models you can kind of prompt them with the filters that they can add to make it filtered Vector search or you could just entirely separate your data out into different indexes and ask it like which one to search and llama index they call that the composable indexes and yeah yeah so there is kind of maybe two distinct patterns one is retrieval augmented generation in the like more classic sense feels silly to call something classic that's only like a few years old but we'll call it clever um but yeah there's like a classic version of it which is loading information into the Transformer at like boot time you might think of it like you're starting to run your Transformer on its uh on the on the context it's running um and you're just saying what do I need to load into that things like Ram once the program starts running um so that actually it's really more ROM what do I need to load into the ROM of this thing for it to run like this is like Commodore 64 style like I'm just like I'm punching the tape and saying like you know this is um you know this is the documentation of my of the library that I want you to answer a question about and that is um the distinction there is that part we think of that as like separate from the behavior of the language model and that it so it's amenable to like software 1.0 or traditional ML and IR techniques and then the other direction to go is to say no language models are like reasoning engines language models are executors of of like complex program flows and retrieving from information store is just the program flow so like you know make it this this me this is like retrieval via tool use um and I can see there being applications where one or the other is more valuable I think the more constrained your application is like the less open domain its questions are and the more straightforward the questions are to answer once you have the context the more it looks like peer retrieval augmented generation um and so like for the main little chatbot uh uh retrieval augmented generation application that I run which answers questions about our course material and about language modeling papers uh the goal is for it to just be a Searcher of that stuff um and we and so the the open-endedness is a little bit less and so there's less need to be like oh like tell me which of these like 50 possible tools and maybe run a Google search to get information Etc but for very open-ended maybe things that are starting to tickle the edge of being like an agentic system then you might want something where retrieval is just one of the many tools that the language model has access to yeah I think that's just completely brilliant is thinking about retrieval as a tool use compared to well so I think to break this apart a bit I think you mentioned several times a Transformers attend over vectors and you know I hope I think our listeners probably most of them are aware of neural architectures and how they work generally so you know we have say 12 layers of Transformer and so one idea is instead of just retrieving to put it in the input you retrieve vectors to put it in say layer 10 out of 12 or 8 out of 12 and and it differentiates on the memory you could you could have ideas where you differentiate through the retriever or even just you keep that as semi semi-parametric and you just you know go through the one one to twelve with the with it injected into eight so so that kind of architecture to me that diverges from just retrieval as tool use just because I don't see how any other tool could be put in the middle of an arc and and then I think it's extreme like yeah yeah yeah I would say that the that sort of retrieval augmentation that is baked into the architecture which goes back to at least retro um for this crop of of language models and um I'm sure there's some uh Burt paper that that did it way earlier and and there's a Schmidt Uber paper from 93 or whatever um but yeah but that style of doing it where you are tightly coupling the internals of the model and the retrieval system I would believe that for narrow tasks you can you like you can differentiate that and then like get really high performance for really low model sizes and really quick latencies um but the that like tight coupling there makes it harder to be like oh well now I want to do a Laura fine tune it's like well how does the Laura fine tune interact with a retrieval thing um and it's like oh I want to use shortcut Transformers I read this thing about shortcut Transformers where you can stop four layers in and then like predict the next token immediately and it's like oh well how does that interact with it so like the more you start to adjust the internals versus treat this thing as an interface where you uh where either you're interfacing with its inputs or its outputs like are used as inputs to your interface the the more you like break that abstraction and start injecting into the internals of the model the more Fragile the system becomes the like harder it is to adjust and right now at the very least there's just so much open field in terms of uh where where you just think in terms of those external interfaces so for like I like until I see like many papers demonstrating or or many applications demonstrating that that's like gives you 10x 100x quality at decreased latency or something I would strongly recommend like not not not opening that Pandora's Box um uh and sticking to the interface yeah it would have to be produced by something like an open AI I mean I think just the last two days we've seen two massive fundraising announcement I think like um I think recalabs inflection AI raised like 1.3 million which is funny enough the exact same number so that's kind of funny and so yeah it would have to be one of someone producing a foundation model that comes with the retriever end of it and yeah Laura early evening definitely topics that we're going to come back to but first I want to stay on this tool use kind of concept a little further so I was in uh Berkeley as well last weekend just didn't go there just I went there to see it and I and I met the authors of gorilla and I thought gorilla gorilla is a large language model that's been fine-tuned to use particular tools so it's like um it's still black boxy you know like it's not we're not talking about putting the latent space of this tool into the middle of the Transformer but we're talking about giving it some examples of how to say how how to Surge or more like how to use like GitHub CLI to like that's one of the examples that I was just like wow that's pretty cool like this whole like um where the tool is to execute code or to file a pull request or to review another pull request like pulling all those levers I I find that to be so fascinating so that kind of idea like I remember web GPT had like search actions kind of like you know discrete black boxy kind of how to use it what do you think about that idea like fine-tuned to use tools kind of yeah I mean I think that we have already seen it like you know if you ask me that question a month ago I would have said oh geez what we really need is a highly capable Foundation model that has been fine-tuned on some kind of tool use pattern and to be honest what tool use pattern would I like I'd like something that looks a little bit like an open API spec um so this is something I spent some time working with um the folks at Speakeasy who do API tooling development and and um uh just sort of like helping them understand the llm landscape and what that could mean for them so I got super into this a couple of months ago and learned a bunch about the sort of API tooling world but it's a way to clearly describe what is available via an API um if you've ever used Fast API and you've seen that little like free docs page that you get um if you use gradio for a long time they also showed you that docs page um that the you know it's the like brightly colored thing with like different types of requests and paths available um it's like it's such a well-specified thing that you can build automated tooling off of it uh and so like I would have said uh a month ago like yeah something that's like close open API spec as like how you describe your tools uh and then like the model knows how to produce a request that goes into that API and so you fully separate out the execution of the tool from the model and you train the model how to work with the with that specification which means you aren't training it how to use a specific tool right like tool former and Tom both kind of are like oh yeah let's like you know let's fine-tune some model to like know when to use a calculator and know when to search Wikipedia right and react uh as a pattern is also like let's use few shot prompting examples to describe the format for a tool uh and then the model will try and pick it up and both of those approaches work okay but they're not extremely they're not super flexible and like what you want is the is like like a flexibility of tool use with model fine tune right so react has a lot of flexibility um at the cost of of token budgets um and then a little bit of uh reduce reliability like a tool formery pattern is going to be like extremely reliable but you have to have examples you have to fine tune on a specific tool uh so fine-tuning on a tool specific on like broad examples from a tool specification that happens to also be a spec broadly used by people to describe services that they are making available seems like the right move and with Json schema powered function calls open AI has done exactly that um and so there's like an incredible opportunity here with tool use that I think you know we're just beginning to scratch the surface of even the people who have been thinking about this like constantly since it came out yeah that was such a great tour of all I love how you brought back tool former that I think if you're mentioning gorilla it's good to cite that one as well and then yeah the react tool use and um maybe I so yeah the open AI functions thing you mentioned your experience in open in the open API specs and I I do find that whole Json formatting it functions the universal interfaces and what openai's accomplished with that I really like that but if we could transition just one I want to talk about a specific kind of tool which is this kind of like text to SQL kind of idea where you would fine tune a model to generate SQL queries I think just if we could stay on this little microcosm of just this one example and think through like is to me it sounds like and it's something that appeals a lot to wevia and is like you know like having a say there's this paper called tiny stories where the headline of the paper is how small can we make these models and they still are able to talk to us and so it's like if we could have a you know a 30 million parameter model that we could just wrap up in a Docker can container served on a CPU I think you could probably Onyx and makes neural magic make it run fast but like that kind of thing of something we could put into eviate and I'm sure there are other examples out there and other kinds of software yeah yeah I definitely think um yeah so the the flow that I just described is very much something that you need a large scale model to handle um because you're going to be picking up semantic information from the natural language in a spec it's sort of like it's like it's reading documentation while also kind of like parsing the contents and then we're generating this uh like in this particular format and we probably also want to be able to like interact in natural language as as part of building those things yeah so that's very like large very capable um uh language model territory So you you're expecting to get that from a foundation model provider uh so with but if you have a specific domain where you want to uh where you want to use a language model but you want it to like sort of interact with symbolic software uh more directly than you yeah you get and that that domain is fairly narrow you're only generating SQL you're only generating python code um then maybe you can get away with much much smaller models I would worry a little bit about I think if you took an extremely large model and you somehow collected a trillion tokens worth of SQL and natural language paired with SQL it's like you know interesting to think what kinds of capabilities would you get out of that like could you just ask it to write you know um to effectively like write your whole uh your whole database um like as opposed to just like writing one one one query um but yeah uh that's that's definitely the the like narrower tool use can involve narrower interfaces than just some like generic thing like open API or Json schema or whatever and can just be like yeah we want something that that generates SQL will run that SQL yeah I mean this conversation is really inspiring my interest to run this experiment where you would like use gbt for whatever to generate a big training data set to then distill the SQL thing into I guess like every input would take in the schema and the natural language question you know like the kind of like special token separation like you do with like Bert and stuff and yeah I mean I think because I don't know if we've had this kind of experiment where you where you scale the data without scaling the model and then into a narrow domain I'd like to call that like generative data augmentation some kind of phrase like that yeah definitely people have started to look at that I think there's like with instruction fine-tuning that you know which includes the like crop of uh South American camelid named um instruction following fine tunes like llama uh or sorry like uh alpaca vacuna guanaco Etc I think we've run out I think we're out of uh uh people will have to find a new mammal to name things after um but uh like that crop if you look back even before that people were looking at generating data with GPT to even gpt3 um uh to generate instruction look like instruction type data um and have found that to work like pretty well I wanna I forget whether the flan data set is constructed in that manner it might it might not have been but super natural instructions natural instructions and Supernatural instructions both constructed in that manner so there's like a proof of concept with one of the sort of like biggest changes with language models which is the switch over from like modeling texts to modeling responses to instructions so I can imagine that that kind of like scale like make a big data set um uh with a capable model and fine-tune a really small model I can see that working uh working pretty well in a lot of domains yeah I don't want to get too off topic but yeah when I first saw that like um I think it was called like decision Transformer I think it's also a Berkeley paper where you where yeah you um it shows how similar like medical treatment decision modeling is to language it's still like sequence to see the key difference being the sparse reward compared to like the here's the supervision episode but here so I think I want to stay earlier we had mentioned this General topic of what I think is the biggest question in full stack deep in like well I say like full stack retrieve augments or generation maybe is a better phrase for it but like should you fine-tune the retrieval model should you fine-tune the large language model or both and what is this and so this is one topic as we're talking about generative data augmentation we're talking about sampling data from large language models to train smaller models so I love this idea where you use the large language model to generate a potential query for a document and then that's what you use to supervise the training of the embedding models I think it's so profound because you know I remember in the early days of wevia we were talking about like we were building site search it was like a thing before no we have it now but before people would troll us on Twitter and say like we've made us new app search for their documentation so we were like okay how are we gonna build this and one idea was like all right let's everyone write 10 questions for each of the pages right and that's how we're going to train our embedding model is so so that kind of like it solves the data problem of you know the language model can write queries potentially yeah and I would say that it solves the cold start problem for data is a better way of thinking about it it's like your data engine needs a cold start um and the language models are one really good way to do that they might you know I've you know a lot of there's this like emerging intuition about the highest capability language models like ubd4 and Claude that they are like a um High percentile crowd worker uh it's like so so actually better than your median crowd worker on many tasks uh and but with crowd workers it's not like people just like hooked up a direct um data stream through Kafka from Mechanical Turk that just like spat out um data that went straight into tensors and turned into gradients like you know there were systems around it and there were and user data and like data that you purchased dog food data um like you described for weediate those were all additional sources um and so language model generated content is an additional data source it has its own uh issues like language models really like language model content so they might give you Rosy evaluations of your performance uh on that um and I think you might lose you're gonna sample well from the like high probability regions of documents at least high probability regions of documents produced on the internet that end up in the in these pre-training sets when it the like long tail of requests is going to be really challenging I know Joe bergam um at Vespa has talked about how there's like the bulk of queries and the long tail of queries and the long tail of queries like is in ends up being nearly as important as the bulk because you know um in this in the same way it is for requests right you look at your requests you look at your your you're you're running an API on your request and you're like 99 of my requests are resolved in 100 milliseconds or less that's great like all 99 of my users are seeing a Snappy website it's like no no hold on one percent of the time while you're doing some tasks in this website you hit something that takes maybe five seconds um and a user like that even if it's sampled at random if a user is like taking many actions they're gonna see it pretty soon um and so similarly even though like the long tail of potential requests is is rare thought of as individually it might be every user has a request that they make that's in the long tail and so language models in generating data are likely to generate things that look like the bulk of queries not things that look like the long tail uh and there's some research on this curse of recursion um uh that that has modeled this uh phenomenon and and shown it empirically and that's going to be a problem for getting to that sticky really high quality product um and so I would suspect like um actually fully solving the like the the problem of needing data train models is always going to involve a data Engine with many diverse sources not just ask gpd4 to generate you 100 new data points yeah I love that and also that curse of recursion that's an awesome title yeah I mean it it um I give a quick shout out to a preview a previous podcast with Professor Laura Dietz on perspectives of using large language models to annotate relevance judgments as an awesome paper and also I've got this Haystack cup here I unfortunately I put it in the dishwasher and that suck it down a little bit but but they have this tool called Cupid for uh labeling query it's just like some interesting things for people listening maybe it could be interested in referencing it but yes this long tail thing this is so interesting and I'd hate to kind of I don't want to get too off topic with generally this kind of retrieval language generation thing but like I've kind of thought of like class imbalance and the inability to perform well on minority sets of data is kind of the Crux of self-driving cars I know I know I'm pivoting topics yeah but like like do you think like like I I've started to think that self-driving cars with deep learning will never be realized because of this problem yeah I definitely have entertained that hypothesis in the hypothesis in the past and now you know there's probably a video of me on YouTube Somewhere saying that um but uh there's two things there one is that in you know empirically I can get in a cruise automobile and take a ride across um across San Francisco with no driver it like I can pay money to do that and so it's kind of happening um and there's lots of deep learning involved at lots of different parts of the stack there's also lots of expert systems involved but um you know and lots of traditional control and and linear quadratic regulators and stuff but but there's deep learning in there for for for especially for perception um so so it's it's it is happening it's it took twice as long as people expected typical for an engineering project um and took a lot more Capital infusion than than people expected but it seems to be happening um I would say there are still long tail issues um and people complain about behavior of the cars um and that is like resolving that is still a hard problem I would say that language maybe helps a lot with this long tail problem in that um I don't know if there's like a figure in the gpt4 white paper from openai that shows how multimodality helps um and it's like asking it to describe the content of a picture and the picture is of a man hanging off the back of a taxi cab doing ironing and if you are a self-driving car and all you ever do is like consume lidar you will drive many many billions of miles before you encounter a man hanging off the back of a taxi cab doing ironing and so like you or or even yeah like you you might not even encounter any ironing uh you know for for billions of Miles uh but if you have a if a component of the stack that's that's part of the perception is trained on a broader data set like the breadth of the entire internet hundreds of billions of trillions of tokens then it has a chance to find out about all these smaller Concepts that can be composed together and also to like knock out further into the tail the things it has never seen before um and so yeah like that that I think will help with that kind of problem I think there um are problems with using really large Transformers uh you know in a uh in a car from from power consumption and um uh to like latency and all these other things but at least that feels like an in principle way to um to help a lot with uh the long tail problem as it appears in self-driving cars EFS and I definitely could talk to you for a while about self-driving cars it's so interesting but let me kind of bring this back to kind of Surge and like people building chat Bots and things like that with our thing is like I guess I think about this like a lot of these large language models they've been trained on like you know internet scale text or maybe like Wikipedia you know the breath is enormous but another thing about the fine-tuning like you know if the you know full stack Discord is a particular pocket of that space right like it to me just the intuition is that it would make so much sense to fine-tune it to put it over into that space yeah yeah definitely I think um the Laura paper is truly incredible the low rank adaptation paper from uh from Microsoft research and it's like like people are aware of this like fine-tuning method um but they like and they're aware that it's good and they should use it or whatever um but I feel like people haven't seen the paper has a bunch of really great insights in it um and one of those one one class of insight that's in it is that um they actually like analyzed what the hell is going on in these fine-tuned layers like what are they doing and what it looks like from what they have in that uh in that little uh more sciency section it looks like what they're doing is reordering the like eigenvalues of every Matrix so they're taking like Loosely speaking they're taking the concepts that the language model already has so like operations language model already likes to do and re-ranking them and saying no actually this behavior is really important and it's behavior that we used to think was important is less so like let's shrink our behavior in One Direction and like increase our behavior in another Direction um so like more so than say like discovering an entirely new direction to focus in um and that sort of explains how you can get really profound changes in the model's Behavior without necessarily increasing their intelligence or capability which is what we've seen from like uh Lima and the false promise of of imitating uh proprietary lens paper that you aren't necessarily making and even the guanica paper like you aren't necessarily making the model that much smarter by fine-tuning it you're just like changing the manner in which it produces outputs you're revealing things that it sort of was already capable of doing that was in the probability distribution over the vocabulary at the end that was present inside of those um inside of those big residual stream vectors inside the model but was like relatively down weighted because it wasn't important during pre-train or was it important during rohf and supervised fine-tuning but in your uh final Downstream fine-tuning case it's super important to uh to like you know use this particular style of diction or to um like refer to a particular noun and so we can just sort of pull those things out and reduce um uh relatively other things yeah it's amazing it's so there's there are kind of two things to that I want to pick apart I mean quickly there's recently we had the voting podcast Stephanie horvachesky and gunjan butter are you where we talked about how you can put image embeddings into the latent space of like level lens and somehow that works like somehow it's able to to do that and like mentioning the eigenvectors so maybe for people listening you know weight Matrix you take that Matrix and you decompose it with singular value decomposition you have like USV that kind of thing and then the eigenvectors like that diagonal and like the rank of it is like how many so like the diagonal of this eigenvector is is like uh how import how the important components of it I you know thinking about projected and residual components is always a yeah yeah I I'd say the like the like short version of that um the like very software engineering version of that is that the eigenvectors Define the independent things going on inside of a matrix um so you could think of them as like like a bunch of asynchronous operations all going on um and because this is linear algebra each one has a different like weight on it a different importance and so those are those are the weights of those uh of those eigenvectors actually you know for the real math nerds these are all singular values and singular vectors so I'm not committing a math crime uh here uh they're they're real valued um and and maybe even positive um but uh yeah so they each of these little asynchronous procedures has an importance weight attached to it and with when I say that we're like re-ranking them I'm saying that we take some like one of the processes was like simulate what Homer Simpson would say in this situation and that's not that important in web pre-training because only a small fraction of documents on the web have Homer Simpson in them but you do still need to learn it if you want to get to the quality of language generation the gpd4 has but if you're fine-tuning the model to sound like Homer Simpson all of a sudden that process becomes really important and so you increase its weight you increase its rank um so that's a a hand hand wavy whiteboardy uh uh software Engineers approach to understanding what that means yeah it's so there's definitely applications of this in Vector search as well like you know there's this paper called hsw finger and it looks like there is a lot of opportunities for us to look at like you know projecting vectors onto other vectors but I do stay up like I I've always thought that like disentangled representation learning was just like an unsolvable problem like even if it's the eigenvector where you you know disentangle the components of it I I I don't know like because sometimes people when you're explaining Vector search to people you'll say like this dimension of the vector says how much of Homer Simpson is this one says how much of this it is and it's like no that's actually it's totally like entangled across the vector yeah yeah and so yeah the um the goal of eigenvector G composition is to like find a basis in which you know to sort of rotate the thing around so that you can actually see oh this is the Homer Simpson Dimension and this is the plural versus not plural Dimension or whatever and there's been limited success in doing those kinds of things to understand the internals of Transformers um but yeah definitely uh individual neurons individual components of those vectors don't seem to have a huge amount of like independent meaning um and honestly I don't think you would necessarily want that to be the case you can if you try to store information in those like n Dimensions you're stuck with the N dimensions of your embedding space and that n is pretty large for a large trans former I forget what it is for gpt3 like low thousands probably um maybe 10 000. but like real like it's a it's that number you and you're and you're and it's the amount you can store if you pick different dimensions to store different things is linear if you relax that and say I don't want to have like n completely orthogonal directions um I just want them to be like mostly orthogonal to each other I want their dot products not to be exactly zero but I'll let them be like a little bit different from zero um and that I'll still consider that or yeah like I'll still consider that like um those two unrelated vectors now all of a sudden you have exponentially sized memory storage um and so it's like what people have found from looking at Transformer internals is that's what they're doing with that with that Transformer uh residual stream there's like you know if you want to know what's in it you have to look at every Dimension and do like a DOT product um with some query rather than looking at some specific Dimension to figure out whether a piece of information is like true or false or what value it has yeah I think without a question there's more like that it's so interesting and such a great topic but I think like if we could pivot into maybe explaining Laura a little more and how low rank adaptation works so again so you can take the weight Matrix and you can decompose it into eigenvector like you can Recon construct The Matrix from the decomposition and so the idea is you can just update these uh the low rank projection with the fine tuning and then you and then you have like this sparse find and it's like the latest update in sparse fine-tuning where to update like a 12 billion parameter I don't know why I said 12 like big model to you can update a subset of it and then if a billion needs to count as as big you know even though like Megatron Ln paper I want to say is like low 10 billions in scale but now that's like oh that's like a small Fun open source Model 12 yeah um yeah I think something that you had told me about in our brainstorming session is this and something like something for I think more casual like people who are just kind of hacking together applications with alleviate in the opening API in this idea that if the Laura fine tuning thing becomes more accessible one idea would be if we have a Discord chat bot you have a separate set of weights for Connor and a separate set of weights for Charles you know and so on what do you think about that idea can you tell me more about it yeah yeah it's really cool I think we don't have the infrastructure place to do that like and there's a bit of a bearish so a bit of a bit of a bearish signal on that front if you look at the um image generation world uh which uh I I you know I spend a lot of my time thinking about the language modeling World um and that you know that's what I'm spending a ton of my time and thought but I recently built like a little QR Code art generator and so I like checked out what's what's going on in stable Fusion world and um they have been productizing Foundation models for way longer than the language modeling world has because they had their moment of like broad accessibility earlier right we still haven't quite had our um stable diffusion moment in language models uh which is to say like an extremely highly capable model with completely open weights that can be run on consumer gpus at like and all these uh like all these things we're still just a little bit short we need like um we need something as smart as text DaVinci 2 but that you can run on like 140 GB card um and so maybe even by the time this podcast is released that'll already be true like who knows it could happen any second um but yeah so so in the in this world they people have been people who have been building stuff and they have discovered different uses for fine tunes and like control net as a way of like merging multiple fine tunes together but but so far as I can tell they don't have anything that looks like that dream that you just uh put out there of like I have one set of Weights that's very small megabytes in size that adjust the behavior of this language model for one user and then I have another one equally small that I can hot swap in actually with like a Cuda operation it's just Matrix math to swap out a Laura fine tune um and then so like on the Fly switch over and now I'm doing it for a different user different user Persona a different region whatever your your approaches for segmenting out your fine tunes um that doesn't seem to be the way things work it seems like mostly people like download the entire weights for every model that they want to use and then like even if they are related by some lineage um so it's a bit of a barrier signal for whether this is going to actually get adopted in the language modeling world you need some kind of like shared manner of expressing where these weights should go um and and making it possible to like get merge uh like one fine tune with another um or uh yeah or all these things that you would want to make it really um uh really good so like right now there are probably people doing that internally who've built uh some Jank version of that but there's no like open standard um that everybody can use that that solves that problem yeah I I I guess I think like there's certainly potential for that in the future I guess for now we're seeing like you kind of do like role playing prompting or like personalized prompting where I'd be like here's an example of how Conor writes his emails kind of and like or like you know kind of likes basketball could you tailor this advertisement to Conor and then it's like hey come play basketball yeah prompting yeah as always prompting takes like 30 seconds to try and gets you 70 of the way there yeah yeah and it works like retrieval augmentation Works super well um and then like you know hammering down that last uh bit then takes you know you have to you have to move over to using fine you have to move over to like redoing the pre-training um yeah uh whatever and uh yeah so that's uh like a pretty pretty typical pattern um and so starting off with like per user prompts is a good way to at least start seeing okay what do users need before they prefer a user um uh a user specific model to the base model um or what are the right ways to break this down like what information is user specific versus what is generic and maybe like you can imagine like a hierarchy of like you have individual users all the way at the bottom you have a single model for all users all the way at the top and then you have different ways of partitioning your users and having a specific prompt or fine-tune um or or other kinds of different Behavior at each point in that uh in that power set of partitions and uh like uh figuring out what the right ways to think about that are with prompting is a great way to save yourself a lot of pain and time uh and any 100 time yeah I mean I think it's kind of coming into like it's like the the Grand Arch theme of our podcast is like you know ml Ops full stack deep learning and I think this is coming into this kind of monitoring and observability sort of topic with respect to like uh embedding space visualization where like it's hard to just segment Charles and Connor you know however many people because we're so abstractly defined but we have this like visualization of clusters and yes maybe you just set the stage how are you thinking about like yeah this cluster visualization I think of it as just a super powerful way for people to start collecting data and then for more people who are you know software who don't who don't come from like training optimizing Cuda kernels like people who are just trying to add it to their apps I think this idea that you can start collecting some input output examples start labeling them on what I like what I don't like and then maybe just see those colored green and red when you put those inputs back into the latent space and I think these kind of charts I think that's a huge middle ground for going from this zero shot rag stack we're talking about into the you know the ml Ops World the weights and bias all that like I think this middle part of this Vision that I think that could be a huge evangelist for this yeah yeah I think um I personally have benefited a little bit from that like um I had a little uh for the uh for the Discord bot um I pushed just like pushed it through T Snee actually using the like built-in version of it inside of weights and biases upload just like upload some vectors from the vector store with their metadata attached and then just project them look at them in 2D and it immediately revealed that there was like a large chunk of data from some PDFs that looks like it's like mangled Unicode bytes or like somebody like included their latec program inside of their PDF in a weird way and so it's just garbage it's just like garbage disgusting it's basically a failure of my like data loading and validation pipeline to not recognize that that was not suitable input and now because dot products are are pretty good um like I'm not ever seeing that in production like I'm not seeing those getting pulled in and like mangling my context but I'm wasting money embedding those I'm wasting uh database space holding on to them and maybe there is good information in there if I had a different pre-processing pipeline to pull that out so that's like you know that that's the kind that like very base level Insight that comes from not even full-on observability but just the like barest forms of monitoring um and the like bearish forms of dashboarding and visualization there's a ton of value to be unlocked there um and that's yep that's definitely what I saw when uh when working at weights and biases that it's just like just being able to see the GPU utilization really easily and being able to be like wait a second it's like way lower than it was three weeks ago what the hell just happened like that is will give you a ton of gains that going further requires you to do a ton of additional instrumentation and a lot of hard work that often is like not very long lasting and needs to be like adjusted and and has needs like constant engineer maintenance and these like simple things like just take the vectors and tissue them and look at them every once in a while gets you a huge chunk of the way yeah it's a it's just amazing like when you're curating data sets for language modeling you have so much data you really have no clue what's in it so this kind of yeah Unicode clusters and I've seen that for myself it's pretty funny obviously I I would love to transition into this topic of GPU utilization a little more right admittedly know very little about it but this kind of yeah like what maybe if you could just set the stage with any topic that interests you and we could go from there oh yeah I mean I don't have that much super interesting to say about it and now that people are mostly using things via um you know that that story and that comment came from uh my time awaits and biases and at a time when there was much less Foundation model use um and a lot more a lot more training and running things yourself um so I don't know that I have too much uh interesting to say there except that if you've never run a profiler that gives you like both profile level stats and critically like a trace view um uh for a GPU accelerated job just like stop what you're doing and like get even just like an end this tutorial run it and then I you know ideally also a model that you care about you have running uh in production or or that you're you're working on just to like it uh substantially improved my intuition for what pytorch was what Cuda was uh like What GPU acceleration means for neural networks just to see this um uh just see this trace of uh like a you know like a flame graph type Trace with uh for the CPU for the GPU in its streams and to relate that back to the kinds of metrics you might see in Nvidia SMI or in weights and biases um it's uh something that web developers um like don't even know how good they have it like they can be running their application in a browser they play like like three buttons and all of a sudden they have a perfectly recorded replayable visual trace of of uh of of what happened in their program and like it's that's a similarly awesome experience for understanding the browser the Dom the uh like the event Loop like all the pieces that make a web application work um you can get a ton of insight from looking at those traces same thing applies to uh to tracing your GPU accelerated uh workloads yeah that's that's been one of the from joining weeviate and seeing the inside of like a legit engineering company compared to like my PhD that's been one of the big eye opening things for me I've ever seen those like stacked Trace graphs and thinking what is that or it's like you're spending this much time on the distance computation this much time like updating the end like all this kind of stuff with the hsw so with GPU profiling do you usually look at like I didn't know about like idle uh like idle time between when you're kind of like sending the gradients and you're looking for the next like just batch of data kind of pretty abstractly but do you maybe profile it like layer by layer like how fine-grained does profiling GPU utilization get yeah um so the profiling tends to be like here's the operations that you ran like with some level of granularity here's how long they took like on average and here's like some some uh quantiles so that that can be useful but I find I I find and that is especially useful when you're like really knocking down like and just continuing to go and like make something faster and faster and faster so that's very much a post product Market fit kind of like this is a really working feature and we're now spending 10 of our time running this workload let's make that let's try and get that 10x faster um we don't it hasn't changed in a quarter we don't expect it to change this year let's spend some time on it um with tracing where you just see like a single or a small number of executions that is more useful as like a thing you do anytime you're you're running something because you can catch these very gross features that are like unintuitive about the behavior of your program um just like as an example um like I was Consulting with somebody and they were like I'll take a look at my deep learning stack and I um like just quickly ran their model through this profiler and it was like part way through their forward pass the CPU stops the GPU stops there's this like giant block of like something is going on not in pytorch but elsewhere in Python for like a third of every forward pass and it turned out they were dynamically constructing a python class on the forward pass just like you know in Python you don't often think that hard about like am I dynamically constructing this class or am I constructing it ahead of time and then like you know uh like referring back to that definition it's like normally you don't have to like that's not slow enough that it slows down the execution of your program but once you're in a hot Loop like uh a model training Loop you know 100 milliseconds matters um 100 milliseconds can be an eternity even if um you know depending on on what you're running uh and so just like simply moving the definition of this class from like oh it's like dynamically constructed in the middle of the forward pass to oh we construct it ahead of time and and just reuse that definition just like yank yanked that entire 100 milliseconds out and saved a third of like shaped 30 off their training time um and so that's the kind of insight like you can maybe see it at the level of Kernel utilization which is the usual kind of metric people look at but diagnosing it requires looking at traces and profiles to be like oh aha here's a surprising like place where time is being spent and this is what's going on when those kernels aren't being utilized uh that's is amazing I think it I think it's a really amazing topic I feel like this is the thing that really separates these like super valuable AI companies and I think a lot of like I think a lot about training embedding models and I think kind of two perspectives of like how you initialize the loss function yeah like you might create a class the loss function is something that people do and with these um you know with the contrast of loss and how you how you build that in I think also just like the the data loaders and how you're going to sample the positive negative Pairs and I think there's probably just a crazy amount of opportunity to optimize these kind of things but yeah so so I I think that was a really really interesting discussion but I want to kind of step back into a more high level space and maybe if you could tell me about your experience with building the um the full stack deep learning uh Discord q a bot oh yeah yeah um yeah so I put that together so what what is this thing um it's q a over full stack deep learning the course full stack llm boot camp um and then also my uh llm paper lit review notion database uh so all those are our sources and um we got some YouTube videos we got some web pages we got some PDFs and it's basically like natural language search over those documents natural language question answering over those documents and I first built it back in January for the scale AI hackathon which was a really sort of like fun there's now like a hackathon every two hours in San Francisco um like yeah uh got a text from a friend like oh do you want to go to this hackathon tonight I'm like uh unfortunately I'm going to a different hackathon like three blocks away um but yeah this was back in January they um they had the hackathon and I was like well what you know what can I what can we do right now q a was starting to emerge and like retrieve augmentation of studying through marriages like a common application pattern um and the um and so it was like clear could be done pretty straightforwardly and you know we get a lot of questions in our Discord where it's like oh you know let me point you to this lecture where we talked about that oh here's the paper that's relevant to that question and it's like oh I'd love to automate away that part of my job um so that I leave like if there's a super interesting question maybe I can follow up on it or um uh if the question isn't easily answerable I can follow up on it um so that was like the vision and it was possible to put together like a a very Jank demo in like an hour and then to have like an actually decent ux by the end of the hackathon like um like it had you know embeds and I want to say I even also shipped the like give Emoji feedback feature of like thumbs up thumbs down uh within the hackathon um but the the GitHub history can tell you whether I'm I'm uh lying at this point or not uh but uh but yeah so then it was that's that is we've stuck with that we've continued to run it and and it's sort of one place where I get some experience with like what is it what is it like to have one of these things running in production so like on the delightful side um when I switched it over to gpt4 oh it got it on smarter and also it's now multilingual so I woke up one morning to find that um somebody had been asking questions about radio in Swahili and uh from what I could tell as a non-speaker absolutely just using Google translate and and chat GPT it was responding in fluent Swahili with uh with like correct answers and sources and links to my videos um so uh that's you know if there's like delightful X experiences of users discovering features of these models and ways to use the ways to use the application I would have wouldn't have anticipated um and then there's also um you know the sadder Parts where it's like oh geez Like You Know It uh sometimes just like completely made up and like sometimes users ask it really weird stuff and it responds in a weird way back to them uh and uh oh yeah and sometimes it goes down sometimes open AI goes down um sometimes the modal goes down um like uh yeah it turns out it's you know interacting with a with a database is hard and the code you write in an hour is not good code for interacting with the database so those kinds of those kinds of discoveries um how exactly that happens in the LOM workflow have also come out of sort of running this thing I think the first question I want to ask you about this is you know as a content creator and I feel like that's kind of the mutual bond we share is making deep learning content for a long time and you know understanding sort of the like how you try to grab everyone's attention and all that kind of good stuff is like the is like this ability to you know automate the content creator turn the content creator into this artifact I mean you mentioned sometimes you get bad responses but it's like it seems like this will just completely change how we think right now of content creators like uh you know I don't know Mr Beast is like if I don't know if he's a good example of one but like you know these these you know Educators people who I think Thomas Frank is a is a YouTuber famous for notion tutorials or like General lifestyle advice and so it's like turning himself into a chat bot shortly this will change just social media broadly right yeah yeah that's an interesting idea I think yeah um framing it as like automating myself um is more fun and cute than it is like a legitimate statement of what these things are capable of I definitely have been you know I've been experimenting with the uh with audio generation with uberduck and 11 Labs um to like generate um you know things that sound like me on a podcast um I promise I'm not using that right now I don't know how I can prove it to anybody listening to this um but uh the uh so like you know you there definitely seem to be some pieces that are there but like full end-to-end automation is always challenging right if you look even at like very sophisticated software shops there are limits to what they're able to automate even in this like in principle Halcyon world of software where everything is bits flowing through machines that should behave deterministically um and so I would say I'm a little I'm bearish on the sh in in the short term on things like fully automating yourself um but finding ways to like take things that you are doing with your time that are low value that are repetitive but context dependent or like require interfacing with natural language um you know uh that are in the end like kind of low effort things that you are doing um taking those and replacing them uh so that you then take that time you're spending on that and then can re-index onto the higher effort stuff that feels like a more um that's like achievable right now like like there are there are wins you can find right now for that and so that's that's how I'm my current mental model for it um the point in which uh content creators are defined as as points within embedding space plus a small neighborhood um you know that uh that feels uh feels fairly far away yeah I guess it's just like I think about yeah like exactly like you say like the um you know I the automating the more you know repetitive low-level tasks to let you think in higher levels of abstraction is like always amazing I just think also kind of like the multi like you mentioned the how do I fix this radio bug in Swahili like I think the way it unlocks the global market it it also can create like hyper competitiveness now because like you know the top five percent can also scale like they can deliver the personal touch that through the language models and stuff like that yeah and yeah yeah so I guess it just I feel like the content creators have actually the market is growing and yeah definitely definitely and and yeah I would say it's like people already from the like last generation of machine learning people this idea of like drudgery or or certain kinds of like menial tasks could be automated and like you know many of them could have been automated with a spreadsheet um uh much less or much less gpt4 but there's all also the the you you suggested this initially as like you know a content creator automating themselves as a chatbot they're that is like that's maybe not the same thing as like full automation like full replacement of the role served by a content creator with a machine but it does it is like a much less machine like um thing that people were doing was like you know interacting with a person understanding what they want and trying to give them what they want um it's just that often when you are doing that you only need to do it at a very like superficial level or it's you know once you have the context of the document that has the answer it's like as straightforward as copy pasting with a little bit of editing like that's much further along the Spectrum from uh what we think of as machines to what we think of uh what we think of as labor for machines to what we think of his labor for humans um and uh so definitely there is a push in that direction from from highly capable language models yeah I agree 100 I think like there's this other concept of like the language models that produced the novel content like the language model you know through maybe retrieve augmented generation where it like retrieves this part of the passage as part of this tries to come up with like novel insights I don't know exactly like but like you've tuned the language model however it does this it produces some kind of blog post and that makes me think even more of the power of like your personal brand because it's like as the content platforms become flooded with this kind of content it's like that Charles tweeted it that or you know however the medium platform that's like what that that stamp of human brand is going to be so valuable yeah yeah I would say um the essential post-modern artist is the DJ whose Artistry is in test and selection and presentation of what a previous uh mode of Productions artists created and it with uh language models perhaps we can effectively all become DJs of text um mixing together things generated by um generated by models uh with with maybe just adding a nice light four on the floor beat underneath them as we as we mix them together yeah I I mean like for me my opportunity has mostly been not in producing novel papers but more in just echoing what I find is interesting and like that's through experimenting with the algorithms and what is successful like if I review like a paper from Sergey Levine on offline reinforcement learning gonna do better than me just saying hey this is these are just my thoughts on this topic yeah so I wanted to ask you also a technical question about this which is you mentioned notion page YouTube the official course material you have these different uh sources of data and then you put it into like a vector database how do you think about like the schema design for different sources do you have like a yeah um well uh I've been using mongodb for my uh for My Level of storage which is a commitment to not have a schema I I you know the you you can you can have a a loose an application layer-defined schema which is uh easier than a database layer defined schema um but the the real thing there is it's like still figuring out what exactly does this look like what should what must it have what might it have in a document um yeah I would say like your questions when you have like multiple sources hmm there's like a certain scale at which you can no longer be very precious about your handling of sources and you have to be very generic I think you end up getting like worse results if all you're doing is saying like um documents are going to be like annotated with what are their H1 and H2 headings um but like you know coming from markdown or HTML but that that means something very different in different websites like it has a particular meaning like H2 has a particular meaning inside of our online course notes but that's like not what it means on some other page um and uh yeah we have chapters in our YouTube videos I use the chapters as like metadata during processing and later and that's also like chapters can have different meanings when you're consuming different types of YouTube videos and like that like injecting a little bit of knowledge there can go a long way to increasing the quality of generations and fundamentally like why do we like why do we have this it's because we need to take documents that are longer than the context length and eventually get them down to the context length and you want to start I think with like splitting up your documents at their joints like splitting them up by sensible like ways to split them chapters headings whatever sub document type thing and it's and then only after that when it's time to decide what's going to go into the language model do you actually chunk it to something smaller like maybe a few hundred tokens at a time but you just make sure you don't cut across any like really serious seams in your document um so like that's one of the that's one of the like primary things to think about um and then I guess the other thing it's sort of like a um application architecture kind of level is like I think of the vector the like embedding vectors and Vector storage as an index into this information so an index in a database is something like we've already stored the data and then we add an index right so we might add an index that's prefix reg X based and this is it like pre-competes a bunch of stuff for us and stores it in a data structure that then allows us to make certain queries really really fast and that's effectively how we how like every application that I have used and like embeddings um for has been to do that right and like when I when you think of a database it's about a lot more than just like an index it's like I want to like I want to version these things maybe I want time travel maybe I want stored procedures like it's a whole bunch of other stuff um and so Vector storage like feels as as I have used it in a rag pipeline a lot more like an index um and so like you kind of want to separate out this is the current state of this document Corpus may be split into subdocuments this is the current state of it and then this is the index over it using a particular strategy of chunking a particular embedding model whatever that then is what the application directly touches um yeah yeah I think like my current understanding is that the best way to do this would be to have one class for just text chunks like all every Chunk from every Source however long it ends up being like 100 tokens or you Max it out at the 512 tokens all lives in one vector index with symbolic properties for like notion YouTube or like title if the page has such a thing and then so from that one index where you build one hsw graph with like you know however much you have possibly you also kind of would add that extra structure in separate classes so like you know this is like the master chunk class and then and then you have like the notion class or like the YouTube class I kind of think like that kind of symbolic separation is the best way and then you can kind of like we have this one thing and we've made this called the group by filter where it's like a book has like it let's say we have two different books and one has ten chunks another has eight that match our query like in the top 100 of our query you know so you would group it and then you would rank the at the book level based on how many passage passages have matched it so I think that's like one way of just using metadata again in search that we talked about earlier again but there's this other idea of like maybe using graph structure to flow embeddings through the graph it's kind of like a Knowledge Graph idea or like maybe by graph graph convolutional networks that could like that would like take each chunk and use its kind of context in the graph of like you know blog posts sites this other blog posts in addition to just this kind of like where it Source from in addition its relation to other things I just think there's like some cool things we can do by thinking further about this I'm sorry that's not too concrete but no yeah I don't know as much about the um Knowledge Graph world in the graph database world I think like definitely the and like people in NLP have been thinking a lot about this it is kind of it feels to me a lot like the branch of NLP that has been um like basically obliterated by the rise of large language models um but that doesn't mean they can't have a triumphant return um so I I have talked definitely talked with some people who are thinking about knowledge graph approaches and um yeah I can see that working pretty well for being like Oh what are the related what are related documents maybe they don't embed the same maybe they are semantically very different from the perspective of embedding but they're related by this by this graph and yeah that's uh yeah that's that kind of uh link based ranking is obviously very good for search or else um you know uh yeah I wouldn't have been using Gmail to communicate with you about a Google meet um yeah uh but yeah so uh yeah I see there's there yeah the the thing that I said about like there's an index and a separate document store like that's uh you get you can enhance that a lot by creating a full-on database with integrated search that uses metadata that metadata is used to construct indices that have both like a vector hnsw type index but also have the metadata as like um additional flags that are used to construct indices that allow like very rapid querying on on sub components yeah I I guess just like broadly like with this topic I think that there is a really interesting opportunity for Vector databases and graph databases to be used together more I think the low hanging fruit is you totally separate them entirely and you think of graph database more in like the graph analytics kind of way like think about like a Twitter like graph like Conor like Charles's tweet that kind of graph where you where you then compute like page rank kind of centrality metrics for like a particular tweet and then you use that as a feature for re-ranking where that's like one of the tabular features for XG boost yeah and like so I think that's the most low-hanging fruit but I I don't know like there's there's another idea to like deep walk no Tibet where you like walk along the graph to find the positives and negatives for the contrastive optimization but yeah definitely I mean there are there is some interesting I want to say it was a blog post but at the very least it was like a Twitter thread like Lang chain has has started to incorporate some graph databases and it's starting to think about like craft databases as tools like there's uh you know graph databases have not been as successful as maybe you might have predicted in 2015 at the height of nosql and social media you might have been like graph databases are gonna are gonna replace SQL and like it's the only thing anybody will ever use um and that's like the opposite has happened like SQL databases have gotten better at storing graphs and gotten better at storing uh uh documents and other things but um but the at the very at the bare minimum representing graph structures inside of databases and using that as part of the querying searching ranking process very like very clearly a ton of opportunity there yeah yeah so it's so I have so many different ways of just conceptualizing this in my head like you could be talking about like a neighborhood of houses something where there's like a graph structure that maybe you embed into a vector or like a molecule where you embed that graph structure into a vector or yeah or the graph is itself like something that you derive out to contextualize the vector and yeah yeah yeah yeah I just think like I don't know there's this one paper I saw that was called Prime kg from Harvard biomedical informatics Institute that was like disease drug Target relations and then each one has has like unstructured Text data about it that's using and embedding and there's like something about classifying sub graphs as well that are like affiliated with some particular like metabolic pathway like there's just something to this and I and the reason I'm trying to connect this back with the Discord bot is I also like to think about like I'm you know I have a weeviate podcast transcriptions data set that you'll soon be in Charles and like I like to build a graph of this with the wegate blogs and the weeviate documentation and I'm thinking like how do I design this data schema and that's kind of what I've settled on now is like I just have the chunk master class and then I have links to these kind of you know podcast transcripts blogs like separate they have their own index as well but this master thing is really where the meat of it is interesting because it I mean just yeah it's just pretty I think pretty interesting thinking about the data schema with that with like you can use symbolic properties to filter your search through it uh but it's yeah I don't know it's just like digging into it further yeah yeah there's a yeah there's definitely a lot there and I'm I'm sure yeah the people coming from the ml world have a lot to learn from the people who've been thinking about search and retrieval um with and without uh ml involved at all and uh yeah part of what is exciting about this moment of like rapid iteration and Rapid building of like full stack applications is that now people from all kinds of different disciplines of software are looking at this paying attention to it and being like wait a second I recognize that pattern that's what we've been doing for 30 years you're still doing the dumb version of it let me let me give you the benefit of like three decades of thinking about this problem here's like here's concentrated nuggets of better ways to do things um and so you can see that sort of like happening with um yeah with retrieval with tool use with uh yeah language models for code generation with uh yeah you with ux and and design with um yeah with uh constructing defensible businesses and thinking about the like the whole uh the whole business around the features or products you're building with language models you know amazing well Charles I'm so happy with our coverage of the full stack deep learning I think we really you know explaining kind of the zero shot MVP get started iterate on the ux instead of the model and then this data engine what might be the middleware for realizing that is it worth realizing it and all the new I think it was such a great coverage let me kind of wrapping up this podcast ask you like a broad question of like some of the directions for the future I know this is this question can kind of put people on the spots like what you know predict the future but like what is maybe not necessarily something that it could be something that just like you can answer this however you like of course but like whether it's something that you just want to see happen like llm inference costs like nothing basically or it could be you know I don't know just a just like a dream like just what you oh sorry or it could be something that you just think very practically this is just the force is headed in this direction so kind of like Dreaming or practically however you want to take the question yeah yeah um I appreciate the breadth of question um yeah yeah uh I'd say that thinking on let's call it like yeah on like a one to three year time scale the phenomena that I think are most important are the decreasing cost of inference and the incorporation and multimodality and and like maybe the longer end of the like three years up to maybe five years the combination of those two things unlocking um applications in robotics hmm so cost of inference is going down like it's just we are in we're in like a uh super Moore's Law like with the cost of cognitive capabilities like you set some you said some Benchmark of like I want to be able to do at least these 10 things with a model um and then you can just watch that plummet the cost just absolutely plummet I think the Ada embeddings from openai have gone down by probably 10x since uh since like November um and uh like yeah like with GPT uh three two turbo that was uh our 3.5 to 3.5 turbo like that was a a 10x reduction in cost and like six months or something um and they're not done uh so that is I expect that phenomenon to continue there's competitive pressure there's Hardware Innovation there's all the work being done to get like things down to in three inference and you know um and shrink these things down put them on consumer cards there's just like oh and like always the Dark Horse of um of like specialized accelerators um and like people can just like you know you the architecture's been fixed for long enough though like attention has been on that for long enough people can just like really just keep cranking that number down and um we have seen while retaining like the same level of capabilities uh so I would expect the like we're just going to continue to see the cost of inference going down obviously we'll continue to push the frontier of capabilities and so the frontier of capabilities might always cost like the same amount or might go up in cost to get the frontier of capabilities but those capabilities are always they're moving up it's like you know you have to be in the right reference frame to think about this and so in a future where it is um like gbd 3.5 turbo capabilities are 10x cheaper in a year and then 50x cheaper in two years like that's a different world than the one we're in right now so I've had people ask me like when am I going to get chat gbt NPCs in my video games and it's like not today unless you're unless you're willing to Shell out like a dollar a minute to play a video game but a dollar a minute um becomes like a Pentium in it in like two or three years and that's like very very like people like that unlocks applications of these models that are currently blocked um I think you know multi-modality is one of those things that we are getting into smaller and smaller models we're getting into higher uh higher quality uh and faster inference Oakland Flamingo V2 came out like within the last week uh of this recording and that's like an improved um multimodal model people have been finding better and better ways to do image language multimodality um and those models as mentioned um earlier like the gpt4 self-driving example like you unlock a lot of uh uh transfer learning opportunities between image and vision that make the image models or the vision models uh or sorry the image models or the language models both better um and so the trend that brings those two things together that I'm excited about um on this time Horizon is robotics um the general discipline in which self-driving technically should be put um where if you can run GPT 3.5 turbo in milliseconds for you know on something that's drawing 10 watts of power um then you have yourself like a robot that you can talk to to deliver commands um so long as that language model can interface with the the robot body as like one of its tools um perhaps via lower level reinforcement learning uh style policies that um that's just like we don't have general purpose robots for a number of reasons but in a couple of years um they can't be smart enough and they aren't as uh easy to interact with when they're doing tasks as humans that excuse is gone um the servos and the uh what do you do when when it swings around and breaks somebody's nose uh those kinds of questions of the maintenance uh those questions remain hard um but not the intelligence yeah super compelling picture for the future I inspired by all those things as well I I think with the maybe one thing to stay on I mean I think um this this kind of like NPCs in video games with super cheap L alums one idea that's always stuck with me that I find to be really fascinating is um I had you Shang woo on the podcast who discussed chat Arena where it's like you know Charles and Connor speaking in simulated conversations like a million simulated conversations between us and like this idea of just like creating this like I like to think of it as we take generative models and we create the output Space by sampling from them and then we use Vector search to look in that uh the latent space of that generated output space to see like as any of these like now is like more to search through or something like that interesting yeah yeah definitely language models are useful for their ability to allow us to like explore language um like like people people focus a lot on the like the Practical applications people focus on like oh I can use the fact that this generates language and requires language as one of its expensive inputs to like accelerate and amplify like but those are like Downstream of this like fundamental thing which is we now have machines that are like capable explorers of language space capable producers of utterances like um yeah and that is maybe like very long term uh at the scale of like decades probably a very interesting phenomenon about language models how does it change the way that we speak how does it change the way that we interact with language that now that um uh the production and exploration of language is no longer bottlenecked on a human being um producing yeah it was fascinating it yeah I think like there's this thing that one of the big dangers of language models is that they're so persuasive that you like similar to how a recommendation engine now is already kind of persuasive for extreme ideas by like keeping you in interest that the yeah yeah the whole exploring the language but I love think about that and then also like when you draw when you draw that to code as well how it can like explore the space of what code can do by writing all sorts of code and you have to have abstractions that you share with other llm programmers and they're like okay I like the subtraction like yeah yeah we've barely begun to see what the impact of these things on programming is going to be right like um yeah changes in Technologies produced by programming um like reflect back and change programming languages so like the rise of developer tooling like really really high quality developer in editor developer tooling is changing the way people think about languages like rust would not be nearly as easy and fun at least in the in the good times to write um were it not for like the various forms like intellisense code completion like the compiler and the analyzer uh are like directly interfacing with the doc man like if I had to write it on a punch card you know or like write it in a static document and it also compile there then get back an error message as opposed to this is the type I'm inferring you get it on Hover like that that has already changed the behavioral languages python is getting typing over um its own dead body but uh and that part of that is because people love this developer experience of being able to like see those things and previously the fact that python didn't have types was like its selling point um and and that's just that's that's just a very humdrum technological changes around like the like language server protocol and um and like improvements in um in in developer environments and text editors so to imagine yeah what's possible now that production semantically meaningful text is not bottlenecked on humans um I think we'll see some pretty rad programming languages in like a 10 to 20 year timeline yeah Miss and that's a funny like I also like came into python like I remember the JavaScript like you don't have to write the types I'm like oh nice like typescript it's like oh no it's back yeah but yeah Charles thank you so much for joining the weba podcast this is truly one of my favorite podcasts we covered so many topics and I learned so much thank you so much yeah thanks so much for having me Connor this was a this is a blast ", "type": "Video", "name": "charles_frye_on_full_stack_deep_learning__weaviate_podcast_57", "path": "", "link": "https://www.youtube.com/watch?v=TztVMhZxORQ", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}