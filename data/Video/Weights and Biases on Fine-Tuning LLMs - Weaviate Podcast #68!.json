{"text": "Hey everyone! Thank you so much for watching the 68th episode of the Weaviate Podcast! We are super excited to welcome ... \nhey everyone thank you so much forchecking out another episode of thewevia podcast featuring Morgan Derek andThomas from weights and biases this wassuch a fun podcast diving into allthings deep learning model training butparticularly these days training largelanguage models has been so excitingwe've recently completed the weviategorilla project where we fine-tunedllama 7B using hug and faces peftLibrary exploring sparse fine-tuning andall these exciting things so in thispodcast we dive all into the currentstate of the tooling around fine tuningor you know maybe training a largelanguage model from scratch which wasshown to you on the screen right now andwe also discussed things like why youwould want to fine-tune a language modelsay the difference between Rag andfine-tuning and how these are actuallycomplementary Technologies as well asthat General kind of idea like domainspecific fine tuning that's been aroundforever or this kind of task specificdistillation like you only need yourlanguage model to perform summarizationquestion answering or formatting APIrequests so really quickly before divingin I wanted to hide like this awesomenew project from waste and biasestraining tiny llamas for fun and scienceif you've seen the weights and biasesreproduction of Dolly with Dolly miniit's kind of similar in spirit to thatwhere it shows the weights and biasesreports going through different detailsof the uh the Llama architecture sothere are more details in here that Ipersonally know about but you can seefrom here hopefully it'll ground thecontext for our conversation a littlebit how weights and biases it helps youexplore these different configurationsfor training so you know sometimes thatis super in the details like you knowsay you have these hyper parameters likesoftmax has some normalization parameterand it takes on this particular valuethis has been a lot of the kind of hyperparameter tuning and that's a you know awhole box to open as well so I hope youenjoy the podcast I had so much fun youknow meeting and talking with DerekThomas and Morgan so with that saidlet's get into it hey everyone thank youso much for watching another episode ofthe weeviate podcast I'm super excitedto welcome Morganand Thomas cappell from weights andbiases is going to be such a fundiscussion I've been having so much funexploring the space of large languagemodel fine tuning of course I've beenaware of weights and biases for a superlong time and so you know finallyteaming up and doing something like thisis so exciting guys thank you so muchfor joining the podcastthanks for the invitation Hannah thankyouawesome so maybe could we do a quicklike a quick round to kick things off ofhow we're currently thinking about uhllm fine-tuning such an exciting topicand weights and biases is such anawesome tool for doing thisyeah hello everyone I'm Thomasum machine learning engineer at weightsand biasesum yeah what we see from from the opensource community and from Enterprisecustomers is that everyone is loving andusing weights and biases to to followtheir experiments the metrics and thetraining logs and keeping everythingtightum I have been mostly following the theopen source Community because they havebeen doing like crazy experiments uhinviting people to participate and wesee that yeah most of the repos outthere already are baked in with weightsand biases and as simple as like turningit on and they are sharing thosedashboards and people are commenting andyou can you can see those training lossGoing Down live it's pretty exciting I Ithink I just just adding because likewe're talking about fine-tuning colorlamps but uh I think it's important tosay like what what we actually mean byan llm are we talking about like a 70billion uh decoder model like Llama OrGPT 3.5 like we've been fine-tuning butlike models encoders for like ages and Ithink that's still going on I think thequestion like since uh chat GPT or PRsince we have like these huge modelsthat actually perform zero shot and afew shots like reasonably well like dowe want to fine-tune this type of modelsand I think this is like what we aretrying to learn as a community kind oflike what works actually which use caseswe want to fine-tune these models versusjust going zero shotyeah yeah the other thing I'm likeumreally interested in seeing is like thenumber ofsoftware Engineers you know who gotpulled in back in like October Novemberwith like 3.5 and then gbd4 and thenlike gamma 2 who are now like intofine-tuning or comments like figure outhow to fine-tune without like an mlbackground and that's really interestingto see like a lot of the like thetraining scripts and Frameworks thatlikekind of like you know will probably getyou like quite a good results withouttoo much ml knowledge like that'sprobably the last 10 percent that wespend some time with it but that's kindof cool to see for me as wellyeah I think that the open sourceCommunity I was mentioning is it'spacked with people like this like theydon't come from the traditional mlbackground and they are yeah pullingthose scripts and creating data setsfrom merging all the data sets or justusing it made more data and thenlaunching those fine tunes and expectingto get good results sometimes breakingstuff it's pretty excitingum amazing there's so many cool thingsin that I love uh Thomas what you saidin the introduction about kind of theexperiment tracking and sharing yourlearning curves with people and Derek Ilike how you broke down the um uh sorrythe um the the different categories ofmodels to training like LMS as well asmaybe embedding models or cross encodersyou know extractive QA in our searchWorld of thinking and Morgan I reallylike that um like just thinking about uhthe abstractions around how uh how easyfine-tuning is becoming and how allthese software developers who maybe theway that I see it is I think the zeroshot models like we mentioned you knowChad gbt or like even zero shotembedding models that's like reallyevangelize this for software engineersand now tools like weights and biases Ithink hugging face has done anincredible job with their abstractionsaround training kit can maybe talk alittle more aboutum yeah like the current state of thetooling for llm find and we can talkabout other models too but I think llmfine tuning is just super exciting andlike kind of the tooling for it in yourexperience with it there's a lot of codebase out thereum most of them build on top of huggingface Transformers library but with theirown like saws and colors and and puttingeverything into yamls or or yeah or orusing structures on like then likerefactoring this into like a supersimple scriptum we are still not with a proven bestof flops per second whatever script thatmakes everything work but there areplenty of bushes out there from leadllama from Axolotl using paths or likethe hugging phase even hugging face hasmultiple libraries on top of Transformerthat would let you like fine-tuneefficiently and yeah and we have thisGPU shorter so like you cannot fitwhatever model you want like you need tomake some tricks toso like efficiently Trim in someparameters another full model and yeah Idon't know the tooling is still evolvingsuper fast and people are breakingbreaking the code bases regularly andI'm I'm actually running right now somesome fine-tuning windows andand yeah I suppose that as Morgan saidlike the silver Engineers out there arenot afraid of like launching a Dockercontainer and just like executing stufflike that us machine Engineers that comefrom Academia or research are like maybethose tools are more frightening to usandmaybe this will be beneficial foreveryone we'll have better softwareengineering practice baked into thismachine learningumyeah I don't know Cal alsoyeah that's awesome yeah I I think thatthat pep Library the way they'veabstracted the sparse fine-tuning withlike Laura and I think there are othertechnique that's all really impressiveand yeah so we'll definitely come backinto the tooling can we talk about Ireally want to get your perspective onuh like you know why fine tune a largelanguage model generally like um I kindof see two schools of thought where it'slike we saw this wave before you knowthe most recent kind of thing where itwould be like uh biomed Bert or like KemBert and the idea was basically you takethe same gbt Uber algorithm and you justdomain specific training data and youknow that's what you're going to need tohave a language model reason about yourparticular kind of stuff and thenthere's also kind ofum like Morgan I saw you do someexperiments with the gorilla things andit's like kind of the the distillationof LMS into task specific models so it'slike that's kind of how I see why wouldyou want to fine tune is either or youhave this kind of more classic way ofthinking where it's like you need todomain at adapt it to your thing oryou're like trying to distill languagemodel that does like all these tasksinto one specific kind of taskyeah I think there's a maybe a thirdcategory there as well as thatopen AI seem to be pushing with their3.5 fine tuning which is just aroundlike style like justum you know speaking you know you knowmore with like your company's voice andlike a support agent spotter or you knowusing yeah maybe using the right likedifferent terminologyum which was like interesting to see foropen air question and also kind of alittle disappointing because obviouslytheum potential of fine-tuning ridersaround like instantly new knowledge intoyour model and they they seem to likede-emphasize that a little bit with atleast a 3.5 fine tuning they're morelike you know don't worry about it knowseverything it needs no just like it'lltweak the style but but yeah like withthe gorillaum fine-tuning I didum to make a task specific like it didyou know I did like learn new knowledgethere so like I think it is possible butyou know maybe they didn't want to likemake any crazy claims about how goodthat that could beum hmmyeah that's so interesting I mean the umlike we're obviously quite bullish youknow we V8 Vector database on the wholeretrieval augmented generation andthat's like you know that we love thatand so it's like the um the have itlearn new knowledge is kind of like um Iguess there's like this entanglementwhere it's like you think of the llm aslike the reasoning engine and you justkind of pack enough information in itand it'll figure it out especially as weimagine the trends seem to be headedtowards really long context models youknow we can imagine maybe that lost inthe middle thing is kind of alleviatedwith some retriever aware tuning and youknow and then we imagine just the nextiteration of gbt5 where it's moretraining data or you know however thenew advances and so I I'm kind of alwaysthinking about like retrieval augments ageneration and then this you know styleyou mentioned or skill like an examplewe had on the last podcast was likeyou're a lawyer right and if I'm yourlawyer I could have the best contactsever I could have all the perf laws tomake your case but you're still probablygoing to jail because I don't know howto reason across the things whereasthat's I really like that kind ofperspective of you know how how do youguys see rag versus Ln fine tuningI think if if I can take it I I feellike like rock is definitely stayingthere in terms of like being a knowledgebase like I don't think we ever want todepend on llams as knowledge basisbecause that's that's not what they aredesigned for like we definitely see llmsas a as a reasoning engine over theknowledge you get from from the rag anduh going back to this fine-tuningquestion I I feel like in the beginninglike when you're exploring a new usecase it makes sense to to start withlike zero shot gpt4 combined with dragbut once you get more mature especiallyas you want to scale you know imagineyou're like a big e-commerce and yousort of like millions of requests likelike gpt4 may not be feasible at this atthis scale and then you need to figureout okay I have like a very compellinguse case and I want to solve it at scalelike at a reasonable cost like I need tofigure out some alternative solution andthen maybe that's a that's a case forfor fine tuning but yeah it's it'sexactly that I'm also I will add that ifyou start with a 0 shot model like fromopen AI or a vendor then you can collectdata to afterwards fine-tune your mallwith the good answers like from fromfrom your retrieval so it's like maybe atwo-step processyeah I I love that you brought that upbecause yeah that's what we're seeing alot of is people get output responsesfrom gbt4 and then they're using the newthree-point gbt 3.5 fine tuning API toyou know gbt4 compress it to 3.5 and Iyou know especially as machine learningpeople I think you know that knowledgedistillation idea where you can compressthe models by copying the output of thelarge models how do you guys think aboutthat like knowledge distillation itshould be like a you know a gold minefor compressing the models and thenmaking inference cheaper and all thatgood stuffyeah I think it still hit the umlike you you probably still want to likehave access to like a knowledge base anda ground truth there at least until likesomeone figures out hallucinationbecause it's the smaller like distilledmodel or stillum you know it still will be prone liketo the sameissuesum there was I think I can recall nowbut some paper that's showing that likedistilled modelsumyou know do do really good at like theumuh on like certain benchmarks um and onthe like bulk of the data set but likereally struggle and fail on the outlineand those areas only account for likeyou know point five percent or a percenton a benchmark so it's fine it doesn'tmatter but you know in the real worldlike out of distribution data sets likethat you know that might be happieryeah yeah so they're like Distillerydistillation one is likereally interesting I wonder kind of butyou know go like like Derek said it'sall youumyou definitely want to like move to likesmaller faster cheaper models and I'mkind of curious if someone will figureout how todistill like not yet like not the theknowledge but like a subset of like thereasoning ability like say like yeahyou're an e-commerce site like itdoesn't need to knowum how to like reason over like medicaldiagnosis or something like that yeahyes this product customer you know didthese things and might want thisrecommendation or something or this textum likeyeah that's what I love about that's whyI really love how um you know um Morganyou and I are both working on thegorilla project and it gives us somekind of thing to ground this with Ithink our discussion and like you knowlike so with my experiments with GorillaI'm I'm currently using the Llama 7billion parameter model and it's exactlythis is like um I have some specificthing I don't need it to reason aboutmedical conversations as you mentionedthat's not useful for this so it's likethat kind of I just find that example ofyou know learning to format API requesta couple reasons I like it so much isobviously for one it helps like usground our conversations in LMfine-tuning and compressing other modelsbut I also think generally people willfind this pretty interesting like if youhave a domain specific language you knowapis you know weights and biases hasapis we get has apis and it's like youknow your software company probably hasapis so it's like I feel like thisparticular fine tuning task ofinterfacing the LMS with your softwareis quite uh generally appealing yeahmaybe be a great thing to just openlysay Morgan what Drew your interest tothe gorilla projectumyeah it's a good question I rememberwhen it came out and I like I thought itwas super cool I know I think it wasprettyopen AI functions rightum so like yeah get getting likestructured outposts the call apisum by the way can you explain gorillafor for folks that may not be familiarwith this data setoh yeah I'll give a quick tldr the theoriginal paper from Berkeley is um theycreated a data set of of deep learningmodel apis so they took models fromhugging face Hub torch Hub andtensorflow hub and then they would youknow so you have like image segmentationmodels image generation models or maybeuh text classification modelsand then you use self-instruct promptingwhere you have gbt4 write an instructiona natural language instruction for whensomeone would want to use the particularmodel so then you um retrieve so then atinference and then you train the modelto write the API requests for thedifferent hugging face models and thenat inference time you retrie you take anatural command like I want to see catsdancing in celebration and then youretrieve the API reference usingretrieval to get like the hugging faceuh image generation you know the APIreference and then it formats therequest to instantiate the hug and facemodel and so then with weavi we uh it'sa natural command of like uh you knowsearch through podcast clips thatdiscuss llms in Search and it formatsthe graphql and then it hits the vectorsearch database and then gives you yoursearch results so that's kind of theextension from the original paper toeviate and you know with weights andbiases I imagine like you could youcould have a natural language likewhat's the status of my training runright and or like my training runs howis back size impact impacting my lossand then it and then it would format theweights and biases apis and then do thething for youexactlyumyeah sorry so gorilla why yeah so thatwas the um I was curious when the papercame out and then but didn't didn't pickit up then and then yeah 3.5 fine tuningarrived and yeah I just wanted to like Ikind of remembered that was the thingand I was like yeah can we kind ofimitate functions a little bit with 3.5umyeah that's the main priority and thenit was like pleasantly surprised if theresults as well I think it was prettycoolum but then like going to Step Beyond itlike like you have and like fine tune insome of the smalleropen source llms is uh yeah really theinteresting things there because if youcan get them to give you like consistentlike API outputsum that's like yeah pretty specialyeah I think it's really cool because uhlike weights and biases well yeah okayso I see I I definitely want to talkmore about how weights and biases andopenai fine tuning play together I'vealways found that integration to bepretty interesting but yeah I have theopen source models I I do really likethis topic because I've always thoughtof like weights and biases and huggingfaces being kind of like neighboringthings where it's like this likepopulation of all these models we likeyou know hug and face brain themselvesis the the GitHub of machine learningwhere and then weights and bias is likethat layer where it's like well how'straining the models going right and so Ithat kind of Open Source models maybe ifwe could maybe we could come back to theopen Ai and weights and biases finetuning and talk more about Open Sourcellms how you see the vision of you knowI think we have Falcon 180b on one handof like we're getting these big capableones and then we've got like uh Phi 1.5which I think is like an extension ofthat tiny stories paper where they'resaying how small can we make them andthey're also like able but talk to usand then of course we have like llama uhcode llama I think deci coder is aninteresting model as well but yeah likeum how are you seeing that proliferationof Open Source Lums and at times hasbeen playing around with some tinyllamas he could maybe chat about thatyeah I can I can I can say one or twothingsum yeah you have like thisthere are basically uh a bunch ofTransformer layers stacked on top ofeach other and they have some slightlydifference andand yeah you may they are notinterchangeable so yeah and we don'tactually know the datasets where theywere trained onso yeah every mall has their own likecapabilities and I don't think we havetested themtruly so to us as the actual performanceI was discussing the yesterday actuallywe with oneone person that worked on Falcon and hewas saying that yeah why you preferllamawith yeah versus Falcon I was like okayI don't know I feel llama has but fromwhat I have tried llama work better andhe was like yeah but have you triedspeaking in another language to Lama andhe I was like no I haven't and yeahFalcon speaks pretty well all thelanguages and llama doesn't so they aredifferences and probably if you're gonnafind tuna Mall you will have to actuallydeep dive on those difference to choosethe good backbone to start your finetuning depending on the task you are youare going to try to solve on yourcompany data on your corporateenvironment so I haven't asked myself aquestion I was like looking at thoseokay they all look basically the sameall of them have flash attentionsome of them are like saving somecompute on the projections or not butthey are not trained on the same dataandthat's also the open source part wherepeople are trying to build those opensource data sets so we know exactly onwhat are they train onbut even if they are using open sourcedatawe realize afterwards that they aretraining on the test data setsafterwards like oh they you cut like theuh someone was busting on Twitteryesterday I would like to take thequestion you cut half and the modelcompletes the rest so basically even ifthe data set I open source they are sobig they are now we are not even capableto know what's in thereso yeah there's a trade-off of exploringwhat's out there on the data sets andarchitecture-wise they are not thatdifferent soyeah I just wanted to make that pointabout thatwe need to explore them way more thanwhat we're doing now that the benchmarksare notdeep enough to exploit thosewho will need some goggle like hiddendata said thatwe know for sure that it's not leaked onany training datait's like in a storage container inIceland and yeahyou have to say the hard drive and onemonth later you can do it results bypaper you know like they postal officeyeah I mean that's an interesting pointthough is um you know I I wonder ifpeople out there are thinking abouttrying several different like opensource Foundation models to start theirfine-tuning with like you know mepersonally I've just llama 7B is likeyeah yeahdefinitely a cool opportunity I mean Ithink another this is another greattransition into I know recently uhyou've had Jonathan Franco giving alecture series on weights and biases andyou know I'm Jonathan's biggest fan I'mwatching anything he's doing and he'sbeen talking a lot about uh you know theimportance of the data cleaning and allthis kind of thing uh what kind oflessons have you been learning from fromthe lecture seriesyeah this is uh this is a great pointand thanks a lot to Jonathan for uh forjoiningum our training and fine Tunica Adamscourse uh he just delivered a lecture ondata sets I think one um question thatwe actually asked Jonathan is like umyou know we see two schools of thoughtlike one school of thought is like youneed to give a model as much data aspossible so that's a competition likehow many trillions of tokens you use forfor pre-training of an llm and and andthe the premise here is like the thebigger the better but then on the otherhand you also have like this textbooksare all you need uh small curatedsynthetic data sets I think what I whatat least I felt from Jonathan is he'smore of a Believer like more data isbetterum at leastum that's what we've seen kind of likeworked over over the longer term uh Ithink what we can see um and maybethat's that's more of my personalopinion but what we can see is like someof these models trained on smallersynthetic data sets they are pretty goodon on specific benchmarks and you knowyou can debate like the thecontamination aspect but I think whatthat means is they can be made to workpretty well on specific tasks so if youhave like a specific task that you wantyour model to work really well and ifyou have a budget to like create asynthetic or or like a filtered data setfor that task then that's I think that'svery promising correction if you wantlike a general purpose model thenprobably the volume of data stillmattersyeah that's so fascinating I mean I Iremember from my conversations with theJonathanum like kind of his journey ofevangelizing pre-training on your datalike you know your company like let'ssay your ev8 and you like concatenateall your blog posts all yourdocumentation like just every texteverything you can find that might berelevant and then language model thatand then maybe then maybe from thereyou'd fine tune it with rlhf or distillit to whatever the specific task mightbe but yeah it's such an interestingthing I mean this kind of synthetic datathing you bring up that's my experiencewith the gorilla wevia so far is I onlyhave 2 300 synthetic examples producedwith self-instruct and you know bysplitting that into 1800 train is 500tests do the math quickly but likethat's enough really and it seems towork pretty well but I yeah I mean ohit's such an interesting it's kind oflike what that textbooks is are all youneed like um if your synthetic data isso clean follows this template maybe youdon't need more data whereas yeah theconventional wisdom and machine learningis go find as much data as you possiblycan right and yeah that's alsointeresting I'm I'm very curious um withweights and biases and how you thinkaboutum data set versioning and maybe likethe the like fine-grained evaluationlike I'm training my um my like mygeneral biochemistry Bert and then Ihave one subset of generalization whichare questions about like themitochondria something like this likehow do you think about that kind of likeyou know the entanglement of data set uhdata sets the details of them maybesymbolic uh identifiers on subsets ofdata as well as monitoring training runswith weights and biasesone thing so so yeah you can um versionyour yourdata sets in weights and biasesum but we see a lot ofcustomers doing and it's like you knowit's not obviously exclusive to weightsand voices um a lot of um continuoustraining so they when they would uhcollect a lot of like production datasend it off to labor labelers um andthenuh augment their existing data set likewith a new version with their new labeldata and then they kick off like a newtraining run and like that can all belike automatedwithin weights and vices which is likeyeah it's pretty standard likeLoop although I guess might be new tosome of the um software Engineers likecoming into the spaceumbut yeah they they did likenot great devices specific but like thethe work um the folks that together weredoing with the like red pajama and umthe guys that normic and gpd4all wherethey they really emphasizedum we returned to them a couple of weeksago on some spaces and they they reallyemphasized the umcurriculum aspect of their pre-trainingof their llmswhich I thought was super interestingand like obviously makes sense um andthey would so you know they obviouslyscoop up all of this like open sourcedata um and then like take differentlike slices from it and then you knowthe order of those slices essentially asyour model earnsum does have a big impact so there'skind of it was interesting that therewas like two aspects to it right there'slike the general like cleaning to removelike all of the junk that's in like webscript data but then it's all the orderof how you you show thingsum is important and that's alsoum I saw folks you know saying when theywere talking aboutfor even like fine tuningum some models youdo you want you want to like yeah um tryand like maybe instill some like newknowledge or like companies specificum knowledge into your model but it's agood idea to like throw in some of theyou know non-company specificinstructionum instructions like data in there toojust so it doesn't you know degrade onlike its General ability to like answeruh questions or instructions well um andI I thought like that was like superinteresting I'm sure that like peoplehave done likemore structured resource on it than likeme browsing Twitter but umyeah there's like it's just I feel likeit's the same thing we always keepcoming back to like in Emma like everyyear it's likeso much of it is like the data rightit's like like amazing you almost wantto like train and tweak newarchitectures and play with new gpus butlikedata Maybe the data the order of thedata you present the model to is likewhatever 70 80 of the workyeah I can I can add something that Iwas in a in a pan on a panel last weekaboutno this week about this topics and andand the day that the data was likepeople were saying like hey yeah peoplethink about gpus GPU poor people GPUreads I need a cluster like why we don'tget 1000 h100s andyeah and then someone came like yeah butthat's the expensive part is not thateven if the gpus are expensive andrenting them like it's curating a highquality data set uh I don't know how howmuch money put openly eye on doing thator or Facebook or or all the like llmcreators but at least people seem toagree that it's like one or ten ordersor two orders of magnitude higher thanthe GPU cost of training them alldo you see on the on the like um Vectordatabase side of things like uses acustomer spending a lot of time incurating their their likeum yeah they're like knowledge basethey're like what what they're likeputting into the vector databaseyeah I think like the the way thatpeople think about the vector databasewell like the vector database stuffmostly emerges from the search enginestuff and in the search engine stuffpeople think aboutum adding like symbolic uh categories ontop of their data so you would have likea filtered Vector search or a filteredhybrid search where you mix Vectorsearch and you know bm25 scoring so sothat so I think peopleum mostly think of there there's a lotof cool ideas like where people thinkabout like multi-vector representationlike how do I have a vector for my titleas well as a vector for my content andlike in in bm25 and you know coming fromelasticsearch and the evolution ofsearch engines into now this newergeneration it's like uh bm25f is apopular algorithm where you score bm25for two properties like title andcontent so that kind of like have a richstructured data object for researchthat's a pretty popular one there's alsokind of like the the nested retrievallike parent child hierarchy and tryingto like search within the Childs to thenretrieve the parents and then re-rankingwith the parents grouping this kind ofthing but um in terms of the thelanguage modeling sent like um not toomany people with Vector databases havethis kind of like uh well yeah as I meanweb scraping is definitely becoming morepopular as people are web scraping thempopulating their Vector databases withthatespecially as um rag is evolving so yeahI do I do think it's something that umthat that are these two worlds are goingto intersect more it's quite aninteresting topic I hope that answerhelped a bit but yeah yeah so it's likeI guess it makes sense right that likethere people already starting with likea higher quality data set to begin withlike like docs or you know like internalmaterials or customer reviews orsomething as opposed to yeah randomescaped junk yeah I guess it would beinteresting though tooif if people are like doing some likeadditional processing but maybe theydon't need it yeahyeah there's maybe there may be likethat little like stop words removalstemming all that like good jazz thatcomes with the like the NLPpre-processing but yeah definitelypretty structured data sets and I meankind of coming back that curriculumlearning I love that idea it's always socool when you have something that's likekind of human learning intuitive likethe idea of learn this first then learnthat I'm very like so with with weightsand biases is it likeyou can be an abstraction like you couldyou know tweak uh I guess like um for mecoming into this I had done some workwith determined AI where we were lookingat like um mostly like learning ratebatch size maybe like how manyparameters the model is and then youwould do like you know the hyperparameter search algorithms like Ashahyperband or like just random searchgrid search keeping it easy sometimesand and like um so can you interfacethings like Active Learning algorithmscurriculum learning and weights andbiases is that something you've exploredI think I think way to biasis is notopinionated on that like you can use anyframework of your choice for for activelearning for curriculum learning like wemake sure to integrate easily so thatyou can track a lot of experiments youcan monitor the progress like look atyour loss look at your metrics samplelike one thing that we see especiallyfor llms and like generative AI likesampling from the model throughout thetraining and looking at this samplesbecomes like super important so like weum we do all of the the stuff aroundlike the training so that you can getinsights into what's working what's notworking so that you can compareexperiments and then if you come backlike if you if you go on vacation andcome back after two weeks you understandwhat happened or that before that timeso uh so this is where our our tuninghelps for sure but like we are not intolike actually like the trainingalgorithms or Frameworks that we workwith with python to work with tensorflowand all of thethe the different tools that you haveavailable for thatI can I can add that I have seen likeannotation tools integrating with uslike they have real Integrations to touse our data lineage so they can connectversion data set with the model and thenat like perform inference maybe labelsome of the bad data on their tool andthen like push back that to our artifactsystem so it's totally doableyeah that's that's really excitingbecause I think that's maybe likeanother opportunity for us to keepworking together after the podcast islike umthey're kind of like Active Learningwhere you search for maybe the nearestneighbors to high loss points that couldbe like one application of theapproximate nearest neighbor surgetechnology and yeah that whole likethinking about the weights and biaseshow it abstracts across all thesedifferent like like uh Morgan youmentioned like um you know mixing thecurriculum or or when you're fine-tuningfor a particular skill like gorillamixing that data with you know like umGeneral reasoning input output Pairs andso that's there's like a hyper parameterwith that right where like you know 32minus K okay and so then you searchthrough that um so I'm also um you knowso with with our gorilla project I'vebeen working with Sam selinga outside ofweviate and so you know we've we've wehave this problem of like umcommunicating what happened and trainingtogether you know and having tocommunicate together and and use thesekind of tools so that's another thingthat makes us really interested inweights and biases uh can you tell memore about that kind of like you know Ialways thought it was so cool likepeople on Twitter would be like you knowI have them on my phone my loss or mytraining curve like what's been theevolution of the kind of uh trainingreports prod product and like yeahyeah I I I don't know if you if you sawthe there's been a couple of screenshotsof people looking at their charts andthey're like Apple watch as wellthat's awesomewell you know like reports is likea really nice tool so so it's um you canthink of it or just you know like aGoogle doc with yourum that has access to all of your loggedweights and biases data and charts andyou can create new charts within it umas well as you know do all the textformatting and embed you knowYouTube tips or tweets or like a wholebunch of stuff as well so it's reallypowerfulum productum a lot of our customersshare it religiously either to likeshare you know even like small bits ofanalysis like it doesn't have to be asuper polishedum report it can be like one chart andthree sentences um or it can besomething that you know a team that'sbeen working on a project for like threemonths are like sent into like SeniorManagement to like reviewumso super flexible like that um a littleunderrated I I think likeum people you know need to get overtheir home but I can spend like you know10 minutes to like put something inthere like share it with a colleague andlike it's pretty amazing that like theycan you know the person you send it tocan then go inum and modify the charts that arealready in there and do their own likeyou know maybe subset of like analysisor like investigate like a particularportion of training curve or whateverum there's the the current uh and likeThomas knows more about this than me butthe current tiny llama projectum where they're like training a modelfrom scratch exactly how is a public wayto moist as a reportum out there at the moment and and soyou can watch you know their trainingloss uh drop in real time over like youknow they're they're training up to likeI think three trillion tokens I thinkthey're likemaybe I don't think they're past atrillion yetum yeah right yeah well um like one butthere are about a 500000 I think or sorry 500 umbillionum so yeah it's like I don't know I'm ahuge fan ofreports but it's likelike everything like within YouTubeum sometimes you have to like spend youknow a little bit of time with it butI think once likeeven to share like notes for yourselflike I find it super valuable likethere's only so much context you canstore in your brain and you come back tolike a project in like a week or a monthor six monthsyou know you look at your workspace andwhat you you know knew intuitively youcould remember was like this was thebest run this one changed this on that'swhy that Spike happened there like allof that's like forgotten and if youhaven't documented it somewhereum you know there's going to be a bit ofa painful like relearning so yeahwe did we did the experiment with Thomasactually a couple of months ago whenwhen we work on the analogues course andthe experiment was like I start workingon a project and then I I disappearedand I go on vacation like whenever likeI don't no Handover no documentation andthen Thomas gets to pick up the projectlike in real life can you imagine thishappening like with weights and biaseslike actually that experiment workedpretty well like he logged into theproject he was able to check like whatcode I used uh what was the result likewhat were the different experimentshyper parameters which machine I ran onuh what was the environment so actuallylike Thomas you were able to pick it upand keep going with the project withoutlike zero Handover yeahI can add that tiny llama the theexperiment you were talking about Morganjust crashed like yesterday and theywere able to resume and the the lastlike match perfectlyso I was like oh good good experimenttracking thereyeah wow that's amazing I mean that it'ssuch a compelling Vision I I mean andalso that tiny llama project I rememberwhen you when there was the dolly minithe reproduction of that that wasamazing as wellyeah I mean like because it's like soI'm curious about so so you know thefour of us are working on a Modeltogether and um you know maybe Morganhas a hypothesis like you know let let'smaybe try to prioritize training on thissubset of data now like like what kindof umlike intermediate questions do youexplore kind of like is it mostly likeum you know like like let's try thisconfiguration of hyper parameters or isit like let's try to train more on thisdata like what kind of decisions arebeing made over that question isI if I can take it like I guessum I guess this is what would probablybe like uh ml training is still more andmore art than science like we actuallyhave a product that can do it in ascientific way which is sweeps and thenand there you can like Implement likethe Asia search over like a range ofhyper parameters like and then you kindof Let It Go but but that that exploresjust like a small subset of thepotential improvements you can make on aModel because like we said like it'slike it's all about the data like youcan add data you can change the orderyou can filter itum there's a bunch of things you can doactually like on the on the code sideyou may need to implement like adifferent augmentation scheme like maybethat's that's relevant for llams but uhyou know you can like there's a bunch ofthings that actually require someimplementationand then this is like where um I guesslike like like actually you need to runlots of experiments and then uh likewhat we typically do is we we just runlots of experiments and then likelogging all of them we can compile likeas long as you have like a goodevaluation mechanism which is uh alsolike a bit of an art like how to doevaluation properly but once you havegood evaluation you can then comparethose experiments and figure out likeokay which of these things worked Iguess priorities I think is umis also it's also a bit of an art hereyeah well all that is so it's so amazingto me I mean I guess one of the so Ihave one other question I've seen umhugging faces put together I thinkthey're calling it collections where youhave like uh like one uh repository forthe model the data set and the paper andthen I I also Imagine like the weightsand biases sweeps adds quite a lot tothe scientific present like a lot ofthese papers you know they have somecombination of loss functions and it'slike Alpha and it's like well whathappens is you ablate the alpha how doeskind of sweeps fit into that picturewhat are your thoughts on that kind oflike you know trying to combineeverything that this machine learningpaper took into one kind of centralizedPlace yeah we love we love the themachine learning teams that use weightsand biases because uh first like you canactually reproduce that research quiteeasily if if it was locked in weightsand biases like it definitely addscredibility like you can see like heythis this happened for you like I can goahead and check the uh the trainingclass I can look at the the wayevaluation was done so it it definitelylike I love research that that is donewith weights and biasesum I think this also helps like researchteams be more effectiveum and then in terms of documentationyeah we've seen some projects likedocuments like all of that experimentswith uh with with reports and and ifthey've done if they use sweeps and inthe project then that also like comeslike with like really coolvisualizations and I think that thatgives you more of a you know like apaper like academic paper is like asnapshot and it's like a snapshot andthen it's like this this Iceberg whereyou have like you know thousands ofexperiments and then you like make asnapshot of like the best one and peoplelike assuming you're a genius becauselike you produce this you know perfectlyworking thing but you don't see like allof the things that didn't work so if youactuallyum you know if you actually courageousenough to to publish also like theresearch that did not work right that'sI think that it's so much more valuablefor practitioners to see like how youhow you came to this uh to this piece ofwork rather than just the the the finalsnapshothmmyeah it's also like that that idea of umyou know the four of us are training amodel and uh Thomas has this hypothesisthat he runs off with and and we havethis like report of of how our thinkingevolved and yeah it's also fascinatingum yes I think that was a really awesometour of all these topics around uhfine-tuning let me ask you kind of thisopen-ended question of kind of like umyou know what's the thing that's on thehorizon that that you're the mostexcited about maybe each you could dooneI I guess maybe I I will start so I'mwell I when I first saw uh GPT 3.5 andgpt4 I I did not believe like opensource can ever catch up but I thinklike open sources is surprising me in alot of positive ways so I'm I'm lookingforward to like seeum you know like more developments likemore ideas being implemented in the openso that we can learn from themum that kind of like making this thescience accessible to toum say the average machine learningengineer so I'm super excited about thatlike experiments like this Moe uhmixture of experts models like I I amexcited how it works outthat's an awesome one I mean I yeah kindof like it it comes into like kind ofalso talking about like um you know likefine-tuning it for a gorilla for somespecific task this the open sourcemodels are getting so good and that's soexciting I mean so so like this kind ofidea of this is an I love this topiclike the closed Source models versus theopen source models for the mostgenerally capable so you so you think uhor like you know like do we think thatthe Falcon models are really catching upto gb24or whatever the I don't know what the ifllama is the latest state of the art orfalcon or yeah I think I think look atleast looking at the benchmarks likewhat like we have like like there aredifferent ways of evaluating fromcompatic Models like you have the Ithink on any Benchmark that you thinkthink of like the the classic benchmarkslike mlu like human eval or like thechat Arena like the the ELO rankingslike gpt4 is is winning all of them Ithink uh and I think commercial modelsactually are on top of this thisbenchmarks but I guess I guess more thanlike the performance on the benchmarks II think like I like the momentum that isin the open source and the fact thatpeople are sharing like learning newthings and sharing sharing them broadlylike that's what keeps me exciting likeI love the like I love as as a personthat is fascinated by Machine learning Ilove you know understanding kind of whatworks what doesn't work I've seen newexperiments so that that keeps meexcited even if like they're like we'restill catching uh catching up likeum you know understanding like what'shappening is is is quite interestingwhat do you think about like um you knowlike with Chad gbt being a product thatthey're getting to have this feedbackloop of where they they collect all thisdata of their Model Behavior like youcould you know like if they let's saythey continue the conversation thatmeans this is a good training example orsomething like that what do you thinkabout that kind of data engine thatwould kind of fuel the closed SourcemodelsI I think it's pretty cool and I hopethey are using itum I'm sure they're using it but othercompanies also have that like I supposeTesla could have a data engine that ispretty complex from all the cars dataand I remember like kapati showcasingthat oh this is a super complex trainingexample and we can search our immensedatabase and find similar ones like andthen active learning that and improveour malls and those were pretty complexlikeyeah image miles connected toTransformer at least what they showed usumso yeah that part and to add from darkI'm also excited about this multimodalstuff that's probably coming like theyteases us with cha gpd4scanning a paper and like or picture ofyour fridge and understanding what youcan prepare from thatum I'm pretty sure that's gonna happensoonumand yeah this is super powerful yeahagreed I I was gonna I was gonna say twoand like oh yeah one of one of them wasmultimodal I think when gpd4 came out soI think someone in in our internal Slackasked like what is your like uh I thinkcontroversial opinion on like yeah ml isgoing this year or something and I saidthat umyeah like like llms are like coming forthe computer vision folks I think likeas well as the fomo like a lot of thatCommunity are sitting pretty and we'relike you know not taking our jobs westill needed like you still need ourlike object detection like skills butlike I don't know like we say Geminiwe'll see like what two people canmanage to doum yeah I think like llms are likecoming going to come harder likecomputer vision quicker than peoplerealizeum well I technically write it won't bean llm it'll be like llm plus it's someCB model but yeah you get dude the otherthing I'm likesuper excited to see and like like tocontinue seeing is like the um on theinference side of things like actuallylike making these work in productionum which is like no small feat and likethey're like gdml was like amazing likellm and mlc and all these like newinference libraries they're like ourawesomeum you know where wherewe're surely going to be releasing a umllm monitoring product to like to youknow like monitor all these like inproduction but like reallyall this training all this fine tuningin all this like open source energy isgreat but if you can't actually serviceto like real users in you know whateverso second so any second like latencylike yeah it's it's not going anywayright so yeah it's just that like havingthat you knownow having like the entire softwarecommunityhelp us as in these and like not justtrying to figure it outI remember a tweet where it's like themachine learning Engineers arediscovering mmap and it's like thisobvious like they're laughing at ourinability to yeah and uh yeah that's soin like the what's your hot take like umyou know I had a friend who who posed itas like tell me something that'soverrated like you have to say somethingis overrated now by the premise of thequestion but yeah I I love all thosetakes those it's also interesting andyeah I do agree that the llm uh computervid I mean that um that paperpre-trained Transformers a universalcomputation engine showing you could putthe latent space of a computer visionmodel into an LM that was like you knowI never would have thought that wouldwork but you know yeah I I it's soexciting I mean the gbt4 I think alsofrom our search perspective the abilityof that to um do like the data ingestionlayer where you can like visually lookat a document and then extract thestructure into putting it into one ofthese search databases that's justincredibly exciting for us butyeah Morgan Thomas Derrick this was anincredible podcast I learned so muchit's been so much fun talking to youguys thank you so much for joining theweekday podcast thank you very much", "type": "Video", "name": "Weights and Biases on Fine-Tuning LLMs - Weaviate Podcast #68!", "path": "", "link": "https://www.youtube.com/watch?v=9wJuza0_ix8", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}