{"text": "Hey everyone! Thank you so much for watching the 68th episode of the Weaviate Podcast! We are super excited to welcome ... \nhey everyone thank you so much for checking out another episode of the wevia podcast featuring Morgan Derek and Thomas from weights and biases this was such a fun podcast diving into all things deep learning model training but particularly these days training large language models has been so exciting we've recently completed the weviate gorilla project where we fine-tuned llama 7B using hug and faces peft Library exploring sparse fine-tuning and all these exciting things so in this podcast we dive all into the current state of the tooling around fine tuning or you know maybe training a large language model from scratch which was shown to you on the screen right now and we also discussed things like why you would want to fine-tune a language model say the difference between Rag and fine-tuning and how these are actually complementary Technologies as well as that General kind of idea like domain specific fine tuning that's been around forever or this kind of task specific distillation like you only need your language model to perform summarization question answering or formatting API requests so really quickly before diving in I wanted to hide like this awesome new project from waste and biases training tiny llamas for fun and science if you've seen the weights and biases reproduction of Dolly with Dolly mini it's kind of similar in spirit to that where it shows the weights and biases reports going through different details of the uh the Llama architecture so there are more details in here that I personally know about but you can see from here hopefully it'll ground the context for our conversation a little bit how weights and biases it helps you explore these different configurations for training so you know sometimes that is super in the details like you know say you have these hyper parameters like softmax has some normalization parameter and it takes on this particular value this has been a lot of the kind of hyper parameter tuning and that's a you know a whole box to open as well so I hope you enjoy the podcast I had so much fun you know meeting and talking with Derek Thomas and Morgan so with that said let's get into it hey everyone thank you so much for watching another episode of the weeviate podcast I'm super excited to welcome Morgan and Thomas cappell from weights and biases is going to be such a fun discussion I've been having so much fun exploring the space of large language model fine tuning of course I've been aware of weights and biases for a super long time and so you know finally teaming up and doing something like this is so exciting guys thank you so much for joining the podcast thanks for the invitation Hannah thank you awesome so maybe could we do a quick like a quick round to kick things off of how we're currently thinking about uh llm fine-tuning such an exciting topic and weights and biases is such an awesome tool for doing this yeah hello everyone I'm Thomas um machine learning engineer at weights and biases um yeah what we see from from the open source community and from Enterprise customers is that everyone is loving and using weights and biases to to follow their experiments the metrics and the training logs and keeping everything tight um I have been mostly following the the open source Community because they have been doing like crazy experiments uh inviting people to participate and we see that yeah most of the repos out there already are baked in with weights and biases and as simple as like turning it on and they are sharing those dashboards and people are commenting and you can you can see those training loss Going Down live it's pretty exciting I I think I just just adding because like we're talking about fine-tuning color lamps but uh I think it's important to say like what what we actually mean by an llm are we talking about like a 70 billion uh decoder model like Llama Or GPT 3.5 like we've been fine-tuning but like models encoders for like ages and I think that's still going on I think the question like since uh chat GPT or PR since we have like these huge models that actually perform zero shot and a few shots like reasonably well like do we want to fine-tune this type of models and I think this is like what we are trying to learn as a community kind of like what works actually which use cases we want to fine-tune these models versus just going zero shot yeah yeah the other thing I'm like um really interested in seeing is like the number of software Engineers you know who got pulled in back in like October November with like 3.5 and then gbd4 and then like gamma 2 who are now like into fine-tuning or comments like figure out how to fine-tune without like an ml background and that's really interesting to see like a lot of the like the training scripts and Frameworks that like kind of like you know will probably get you like quite a good results without too much ml knowledge like that's probably the last 10 percent that we spend some time with it but that's kind of cool to see for me as well yeah I think that the open source Community I was mentioning is it's packed with people like this like they don't come from the traditional ml background and they are yeah pulling those scripts and creating data sets from merging all the data sets or just using it made more data and then launching those fine tunes and expecting to get good results sometimes breaking stuff it's pretty exciting um amazing there's so many cool things in that I love uh Thomas what you said in the introduction about kind of the experiment tracking and sharing your learning curves with people and Derek I like how you broke down the um uh sorry the um the the different categories of models to training like LMS as well as maybe embedding models or cross encoders you know extractive QA in our search World of thinking and Morgan I really like that um like just thinking about uh the abstractions around how uh how easy fine-tuning is becoming and how all these software developers who maybe the way that I see it is I think the zero shot models like we mentioned you know Chad gbt or like even zero shot embedding models that's like really evangelize this for software engineers and now tools like weights and biases I think hugging face has done an incredible job with their abstractions around training kit can maybe talk a little more about um yeah like the current state of the tooling for llm find and we can talk about other models too but I think llm fine tuning is just super exciting and like kind of the tooling for it in your experience with it there's a lot of code base out there um most of them build on top of hugging face Transformers library but with their own like saws and colors and and putting everything into yamls or or yeah or or using structures on like then like refactoring this into like a super simple script um we are still not with a proven best of flops per second whatever script that makes everything work but there are plenty of bushes out there from lead llama from Axolotl using paths or like the hugging phase even hugging face has multiple libraries on top of Transformer that would let you like fine-tune efficiently and yeah and we have this GPU shorter so like you cannot fit whatever model you want like you need to make some tricks to so like efficiently Trim in some parameters another full model and yeah I don't know the tooling is still evolving super fast and people are breaking breaking the code bases regularly and I'm I'm actually running right now some some fine-tuning windows and and yeah I suppose that as Morgan said like the silver Engineers out there are not afraid of like launching a Docker container and just like executing stuff like that us machine Engineers that come from Academia or research are like maybe those tools are more frightening to us and maybe this will be beneficial for everyone we'll have better software engineering practice baked into this machine learning um yeah I don't know Cal also yeah that's awesome yeah I I think that that pep Library the way they've abstracted the sparse fine-tuning with like Laura and I think there are other technique that's all really impressive and yeah so we'll definitely come back into the tooling can we talk about I really want to get your perspective on uh like you know why fine tune a large language model generally like um I kind of see two schools of thought where it's like we saw this wave before you know the most recent kind of thing where it would be like uh biomed Bert or like Kem Bert and the idea was basically you take the same gbt Uber algorithm and you just domain specific training data and you know that's what you're going to need to have a language model reason about your particular kind of stuff and then there's also kind of um like Morgan I saw you do some experiments with the gorilla things and it's like kind of the the distillation of LMS into task specific models so it's like that's kind of how I see why would you want to fine tune is either or you have this kind of more classic way of thinking where it's like you need to domain at adapt it to your thing or you're like trying to distill language model that does like all these tasks into one specific kind of task yeah I think there's a maybe a third category there as well as that open AI seem to be pushing with their 3.5 fine tuning which is just around like style like just um you know speaking you know you know more with like your company's voice and like a support agent spotter or you know using yeah maybe using the right like different terminology um which was like interesting to see for open air question and also kind of a little disappointing because obviously the um potential of fine-tuning riders around like instantly new knowledge into your model and they they seem to like de-emphasize that a little bit with at least a 3.5 fine tuning they're more like you know don't worry about it knows everything it needs no just like it'll tweak the style but but yeah like with the gorilla um fine-tuning I did um to make a task specific like it did you know I did like learn new knowledge there so like I think it is possible but you know maybe they didn't want to like make any crazy claims about how good that that could be um hmm yeah that's so interesting I mean the um like we're obviously quite bullish you know we V8 Vector database on the whole retrieval augmented generation and that's like you know that we love that and so it's like the um the have it learn new knowledge is kind of like um I guess there's like this entanglement where it's like you think of the llm as like the reasoning engine and you just kind of pack enough information in it and it'll figure it out especially as we imagine the trends seem to be headed towards really long context models you know we can imagine maybe that lost in the middle thing is kind of alleviated with some retriever aware tuning and you know and then we imagine just the next iteration of gbt5 where it's more training data or you know however the new advances and so I I'm kind of always thinking about like retrieval augments a generation and then this you know style you mentioned or skill like an example we had on the last podcast was like you're a lawyer right and if I'm your lawyer I could have the best contacts ever I could have all the perf laws to make your case but you're still probably going to jail because I don't know how to reason across the things whereas that's I really like that kind of perspective of you know how how do you guys see rag versus Ln fine tuning I think if if I can take it I I feel like like rock is definitely staying there in terms of like being a knowledge base like I don't think we ever want to depend on llams as knowledge basis because that's that's not what they are designed for like we definitely see llms as a as a reasoning engine over the knowledge you get from from the rag and uh going back to this fine-tuning question I I feel like in the beginning like when you're exploring a new use case it makes sense to to start with like zero shot gpt4 combined with drag but once you get more mature especially as you want to scale you know imagine you're like a big e-commerce and you sort of like millions of requests like like gpt4 may not be feasible at this at this scale and then you need to figure out okay I have like a very compelling use case and I want to solve it at scale like at a reasonable cost like I need to figure out some alternative solution and then maybe that's a that's a case for for fine tuning but yeah it's it's exactly that I'm also I will add that if you start with a 0 shot model like from open AI or a vendor then you can collect data to afterwards fine-tune your mall with the good answers like from from from your retrieval so it's like maybe a two-step process yeah I I love that you brought that up because yeah that's what we're seeing a lot of is people get output responses from gbt4 and then they're using the new three-point gbt 3.5 fine tuning API to you know gbt4 compress it to 3.5 and I you know especially as machine learning people I think you know that knowledge distillation idea where you can compress the models by copying the output of the large models how do you guys think about that like knowledge distillation it should be like a you know a gold mine for compressing the models and then making inference cheaper and all that good stuff yeah I think it still hit the um like you you probably still want to like have access to like a knowledge base and a ground truth there at least until like someone figures out hallucination because it's the smaller like distilled model or still um you know it still will be prone like to the same issues um there was I think I can recall now but some paper that's showing that like distilled models um you know do do really good at like the um uh on like certain benchmarks um and on the like bulk of the data set but like really struggle and fail on the outline and those areas only account for like you know point five percent or a percent on a benchmark so it's fine it doesn't matter but you know in the real world like out of distribution data sets like that you know that might be happier yeah yeah so they're like Distillery distillation one is like really interesting I wonder kind of but you know go like like Derek said it's all you um you definitely want to like move to like smaller faster cheaper models and I'm kind of curious if someone will figure out how to distill like not yet like not the the knowledge but like a subset of like the reasoning ability like say like yeah you're an e-commerce site like it doesn't need to know um how to like reason over like medical diagnosis or something like that yeah yes this product customer you know did these things and might want this recommendation or something or this text um like yeah that's what I love about that's why I really love how um you know um Morgan you and I are both working on the gorilla project and it gives us some kind of thing to ground this with I think our discussion and like you know like so with my experiments with Gorilla I'm I'm currently using the Llama 7 billion parameter model and it's exactly this is like um I have some specific thing I don't need it to reason about medical conversations as you mentioned that's not useful for this so it's like that kind of I just find that example of you know learning to format API request a couple reasons I like it so much is obviously for one it helps like us ground our conversations in LM fine-tuning and compressing other models but I also think generally people will find this pretty interesting like if you have a domain specific language you know apis you know weights and biases has apis we get has apis and it's like you know your software company probably has apis so it's like I feel like this particular fine tuning task of interfacing the LMS with your software is quite uh generally appealing yeah maybe be a great thing to just openly say Morgan what Drew your interest to the gorilla project um yeah it's a good question I remember when it came out and I like I thought it was super cool I know I think it was pretty open AI functions right um so like yeah get getting like structured outposts the call apis um by the way can you explain gorilla for for folks that may not be familiar with this data set oh yeah I'll give a quick tldr the the original paper from Berkeley is um they created a data set of of deep learning model apis so they took models from hugging face Hub torch Hub and tensorflow hub and then they would you know so you have like image segmentation models image generation models or maybe uh text classification models and then you use self-instruct prompting where you have gbt4 write an instruction a natural language instruction for when someone would want to use the particular model so then you um retrieve so then at inference and then you train the model to write the API requests for the different hugging face models and then at inference time you retrie you take a natural command like I want to see cats dancing in celebration and then you retrieve the API reference using retrieval to get like the hugging face uh image generation you know the API reference and then it formats the request to instantiate the hug and face model and so then with weavi we uh it's a natural command of like uh you know search through podcast clips that discuss llms in Search and it formats the graphql and then it hits the vector search database and then gives you your search results so that's kind of the extension from the original paper to eviate and you know with weights and biases I imagine like you could you could have a natural language like what's the status of my training run right and or like my training runs how is back size impact impacting my loss and then it and then it would format the weights and biases apis and then do the thing for you exactly um yeah sorry so gorilla why yeah so that was the um I was curious when the paper came out and then but didn't didn't pick it up then and then yeah 3.5 fine tuning arrived and yeah I just wanted to like I kind of remembered that was the thing and I was like yeah can we kind of imitate functions a little bit with 3.5 um yeah that's the main priority and then it was like pleasantly surprised if the results as well I think it was pretty cool um but then like going to Step Beyond it like like you have and like fine tune in some of the smaller open source llms is uh yeah really the interesting things there because if you can get them to give you like consistent like API outputs um that's like yeah pretty special yeah I think it's really cool because uh like weights and biases well yeah okay so I see I I definitely want to talk more about how weights and biases and openai fine tuning play together I've always found that integration to be pretty interesting but yeah I have the open source models I I do really like this topic because I've always thought of like weights and biases and hugging faces being kind of like neighboring things where it's like this like population of all these models we like you know hug and face brain themselves is the the GitHub of machine learning where and then weights and bias is like that layer where it's like well how's training the models going right and so I that kind of Open Source models maybe if we could maybe we could come back to the open Ai and weights and biases fine tuning and talk more about Open Source llms how you see the vision of you know I think we have Falcon 180b on one hand of like we're getting these big capable ones and then we've got like uh Phi 1.5 which I think is like an extension of that tiny stories paper where they're saying how small can we make them and they're also like able but talk to us and then of course we have like llama uh code llama I think deci coder is an interesting model as well but yeah like um how are you seeing that proliferation of Open Source Lums and at times has been playing around with some tiny llamas he could maybe chat about that yeah I can I can I can say one or two things um yeah you have like this there are basically uh a bunch of Transformer layers stacked on top of each other and they have some slightly difference and and yeah you may they are not interchangeable so yeah and we don't actually know the datasets where they were trained on so yeah every mall has their own like capabilities and I don't think we have tested them truly so to us as the actual performance I was discussing the yesterday actually we with one one person that worked on Falcon and he was saying that yeah why you prefer llama with yeah versus Falcon I was like okay I don't know I feel llama has but from what I have tried llama work better and he was like yeah but have you tried speaking in another language to Lama and he I was like no I haven't and yeah Falcon speaks pretty well all the languages and llama doesn't so they are differences and probably if you're gonna find tuna Mall you will have to actually deep dive on those difference to choose the good backbone to start your fine tuning depending on the task you are you are going to try to solve on your company data on your corporate environment so I haven't asked myself a question I was like looking at those okay they all look basically the same all of them have flash attention some of them are like saving some compute on the projections or not but they are not trained on the same data and that's also the open source part where people are trying to build those open source data sets so we know exactly on what are they train on but even if they are using open source data we realize afterwards that they are training on the test data sets afterwards like oh they you cut like the uh someone was busting on Twitter yesterday I would like to take the question you cut half and the model completes the rest so basically even if the data set I open source they are so big they are now we are not even capable to know what's in there so yeah there's a trade-off of exploring what's out there on the data sets and architecture-wise they are not that different so yeah I just wanted to make that point about that we need to explore them way more than what we're doing now that the benchmarks are not deep enough to exploit those who will need some goggle like hidden data said that we know for sure that it's not leaked on any training data it's like in a storage container in Iceland and yeah you have to say the hard drive and one month later you can do it results by paper you know like they postal office yeah I mean that's an interesting point though is um you know I I wonder if people out there are thinking about trying several different like open source Foundation models to start their fine-tuning with like you know me personally I've just llama 7B is like yeah yeah definitely a cool opportunity I mean I think another this is another great transition into I know recently uh you've had Jonathan Franco giving a lecture series on weights and biases and you know I'm Jonathan's biggest fan I'm watching anything he's doing and he's been talking a lot about uh you know the importance of the data cleaning and all this kind of thing uh what kind of lessons have you been learning from from the lecture series yeah this is uh this is a great point and thanks a lot to Jonathan for uh for joining um our training and fine Tunica Adams course uh he just delivered a lecture on data sets I think one um question that we actually asked Jonathan is like um you know we see two schools of thought like one school of thought is like you need to give a model as much data as possible so that's a competition like how many trillions of tokens you use for for pre-training of an llm and and and the the premise here is like the the bigger the better but then on the other hand you also have like this textbooks are all you need uh small curated synthetic data sets I think what I what at least I felt from Jonathan is he's more of a Believer like more data is better um at least um that's what we've seen kind of like worked over over the longer term uh I think what we can see um and maybe that's that's more of my personal opinion but what we can see is like some of these models trained on smaller synthetic data sets they are pretty good on on specific benchmarks and you know you can debate like the the contamination aspect but I think what that means is they can be made to work pretty well on specific tasks so if you have like a specific task that you want your model to work really well and if you have a budget to like create a synthetic or or like a filtered data set for that task then that's I think that's very promising correction if you want like a general purpose model then probably the volume of data still matters yeah that's so fascinating I mean I I remember from my conversations with the Jonathan um like kind of his journey of evangelizing pre-training on your data like you know your company like let's say your ev8 and you like concatenate all your blog posts all your documentation like just every text everything you can find that might be relevant and then language model that and then maybe then maybe from there you'd fine tune it with rlhf or distill it to whatever the specific task might be but yeah it's such an interesting thing I mean this kind of synthetic data thing you bring up that's my experience with the gorilla wevia so far is I only have 2 300 synthetic examples produced with self-instruct and you know by splitting that into 1800 train is 500 tests do the math quickly but like that's enough really and it seems to work pretty well but I yeah I mean oh it's such an interesting it's kind of like what that textbooks is are all you need like um if your synthetic data is so clean follows this template maybe you don't need more data whereas yeah the conventional wisdom and machine learning is go find as much data as you possibly can right and yeah that's also interesting I'm I'm very curious um with weights and biases and how you think about um data set versioning and maybe like the the like fine-grained evaluation like I'm training my um my like my general biochemistry Bert and then I have one subset of generalization which are questions about like the mitochondria something like this like how do you think about that kind of like you know the entanglement of data set uh data sets the details of them maybe symbolic uh identifiers on subsets of data as well as monitoring training runs with weights and biases one thing so so yeah you can um version your your data sets in weights and biases um but we see a lot of customers doing and it's like you know it's not obviously exclusive to weights and voices um a lot of um continuous training so they when they would uh collect a lot of like production data send it off to labor labelers um and then uh augment their existing data set like with a new version with their new label data and then they kick off like a new training run and like that can all be like automated within weights and vices which is like yeah it's pretty standard like Loop although I guess might be new to some of the um software Engineers like coming into the space um but yeah they they did like not great devices specific but like the the work um the folks that together were doing with the like red pajama and um the guys that normic and gpd4all where they they really emphasized um we returned to them a couple of weeks ago on some spaces and they they really emphasized the um curriculum aspect of their pre-training of their llms which I thought was super interesting and like obviously makes sense um and they would so you know they obviously scoop up all of this like open source data um and then like take different like slices from it and then you know the order of those slices essentially as your model earns um does have a big impact so there's kind of it was interesting that there was like two aspects to it right there's like the general like cleaning to remove like all of the junk that's in like web script data but then it's all the order of how you you show things um is important and that's also um I saw folks you know saying when they were talking about for even like fine tuning um some models you do you want you want to like yeah um try and like maybe instill some like new knowledge or like companies specific um knowledge into your model but it's a good idea to like throw in some of the you know non-company specific instruction um instructions like data in there too just so it doesn't you know degrade on like its General ability to like answer uh questions or instructions well um and I I thought like that was like super interesting I'm sure that like people have done like more structured resource on it than like me browsing Twitter but um yeah there's like it's just I feel like it's the same thing we always keep coming back to like in Emma like every year it's like so much of it is like the data right it's like like amazing you almost want to like train and tweak new architectures and play with new gpus but like data Maybe the data the order of the data you present the model to is like whatever 70 80 of the work yeah I can I can add something that I was in a in a pan on a panel last week about no this week about this topics and and and the day that the data was like people were saying like hey yeah people think about gpus GPU poor people GPU reads I need a cluster like why we don't get 1000 h100s and yeah and then someone came like yeah but that's the expensive part is not that even if the gpus are expensive and renting them like it's curating a high quality data set uh I don't know how how much money put openly eye on doing that or or Facebook or or all the like llm creators but at least people seem to agree that it's like one or ten orders or two orders of magnitude higher than the GPU cost of training them all do you see on the on the like um Vector database side of things like uses a customer spending a lot of time in curating their their like um yeah they're like knowledge base they're like what what they're like putting into the vector database yeah I think like the the way that people think about the vector database well like the vector database stuff mostly emerges from the search engine stuff and in the search engine stuff people think about um adding like symbolic uh categories on top of their data so you would have like a filtered Vector search or a filtered hybrid search where you mix Vector search and you know bm25 scoring so so that so I think people um mostly think of there there's a lot of cool ideas like where people think about like multi-vector representation like how do I have a vector for my title as well as a vector for my content and like in in bm25 and you know coming from elasticsearch and the evolution of search engines into now this newer generation it's like uh bm25f is a popular algorithm where you score bm25 for two properties like title and content so that kind of like have a rich structured data object for research that's a pretty popular one there's also kind of like the the nested retrieval like parent child hierarchy and trying to like search within the Childs to then retrieve the parents and then re-ranking with the parents grouping this kind of thing but um in terms of the the language modeling sent like um not too many people with Vector databases have this kind of like uh well yeah as I mean web scraping is definitely becoming more popular as people are web scraping them populating their Vector databases with that especially as um rag is evolving so yeah I do I do think it's something that um that that are these two worlds are going to intersect more it's quite an interesting topic I hope that answer helped a bit but yeah yeah so it's like I guess it makes sense right that like there people already starting with like a higher quality data set to begin with like like docs or you know like internal materials or customer reviews or something as opposed to yeah random escaped junk yeah I guess it would be interesting though too if if people are like doing some like additional processing but maybe they don't need it yeah yeah there's maybe there may be like that little like stop words removal stemming all that like good jazz that comes with the like the NLP pre-processing but yeah definitely pretty structured data sets and I mean kind of coming back that curriculum learning I love that idea it's always so cool when you have something that's like kind of human learning intuitive like the idea of learn this first then learn that I'm very like so with with weights and biases is it like you can be an abstraction like you could you know tweak uh I guess like um for me coming into this I had done some work with determined AI where we were looking at like um mostly like learning rate batch size maybe like how many parameters the model is and then you would do like you know the hyper parameter search algorithms like Asha hyperband or like just random search grid search keeping it easy sometimes and and like um so can you interface things like Active Learning algorithms curriculum learning and weights and biases is that something you've explored I think I think way to biasis is not opinionated on that like you can use any framework of your choice for for active learning for curriculum learning like we make sure to integrate easily so that you can track a lot of experiments you can monitor the progress like look at your loss look at your metrics sample like one thing that we see especially for llms and like generative AI like sampling from the model throughout the training and looking at this samples becomes like super important so like we um we do all of the the stuff around like the training so that you can get insights into what's working what's not working so that you can compare experiments and then if you come back like if you if you go on vacation and come back after two weeks you understand what happened or that before that time so uh so this is where our our tuning helps for sure but like we are not into like actually like the training algorithms or Frameworks that we work with with python to work with tensorflow and all of the the the different tools that you have available for that I can I can add that I have seen like annotation tools integrating with us like they have real Integrations to to use our data lineage so they can connect version data set with the model and then at like perform inference maybe label some of the bad data on their tool and then like push back that to our artifact system so it's totally doable yeah that's that's really exciting because I think that's maybe like another opportunity for us to keep working together after the podcast is like um they're kind of like Active Learning where you search for maybe the nearest neighbors to high loss points that could be like one application of the approximate nearest neighbor surge technology and yeah that whole like thinking about the weights and biases how it abstracts across all these different like like uh Morgan you mentioned like um you know mixing the curriculum or or when you're fine-tuning for a particular skill like gorilla mixing that data with you know like um General reasoning input output Pairs and so that's there's like a hyper parameter with that right where like you know 32 minus K okay and so then you search through that um so I'm also um you know so with with our gorilla project I've been working with Sam selinga outside of weviate and so you know we've we've we have this problem of like um communicating what happened and training together you know and having to communicate together and and use these kind of tools so that's another thing that makes us really interested in weights and biases uh can you tell me more about that kind of like you know I always thought it was so cool like people on Twitter would be like you know I have them on my phone my loss or my training curve like what's been the evolution of the kind of uh training reports prod product and like yeah yeah I I I don't know if you if you saw the there's been a couple of screenshots of people looking at their charts and they're like Apple watch as well that's awesome well you know like reports is like a really nice tool so so it's um you can think of it or just you know like a Google doc with your um that has access to all of your logged weights and biases data and charts and you can create new charts within it um as well as you know do all the text formatting and embed you know YouTube tips or tweets or like a whole bunch of stuff as well so it's really powerful um product um a lot of our customers share it religiously either to like share you know even like small bits of analysis like it doesn't have to be a super polished um report it can be like one chart and three sentences um or it can be something that you know a team that's been working on a project for like three months are like sent into like Senior Management to like review um so super flexible like that um a little underrated I I think like um people you know need to get over their home but I can spend like you know 10 minutes to like put something in there like share it with a colleague and like it's pretty amazing that like they can you know the person you send it to can then go in um and modify the charts that are already in there and do their own like you know maybe subset of like analysis or like investigate like a particular portion of training curve or whatever um there's the the current uh and like Thomas knows more about this than me but the current tiny llama project um where they're like training a model from scratch exactly how is a public way to moist as a report um out there at the moment and and so you can watch you know their training loss uh drop in real time over like you know they're they're training up to like I think three trillion tokens I think they're like maybe I don't think they're past a trillion yet um yeah right yeah well um like one but there are about a 500 000 I think or sorry 500 um billion um so yeah it's like I don't know I'm a huge fan of reports but it's like like everything like within YouTube um sometimes you have to like spend you know a little bit of time with it but I think once like even to share like notes for yourself like I find it super valuable like there's only so much context you can store in your brain and you come back to like a project in like a week or a month or six months you know you look at your workspace and what you you know knew intuitively you could remember was like this was the best run this one changed this on that's why that Spike happened there like all of that's like forgotten and if you haven't documented it somewhere um you know there's going to be a bit of a painful like relearning so yeah we did we did the experiment with Thomas actually a couple of months ago when when we work on the analogues course and the experiment was like I start working on a project and then I I disappeared and I go on vacation like whenever like I don't no Handover no documentation and then Thomas gets to pick up the project like in real life can you imagine this happening like with weights and biases like actually that experiment worked pretty well like he logged into the project he was able to check like what code I used uh what was the result like what were the different experiments hyper parameters which machine I ran on uh what was the environment so actually like Thomas you were able to pick it up and keep going with the project without like zero Handover yeah I can add that tiny llama the the experiment you were talking about Morgan just crashed like yesterday and they were able to resume and the the last like match perfectly so I was like oh good good experiment tracking there yeah wow that's amazing I mean that it's such a compelling Vision I I mean and also that tiny llama project I remember when you when there was the dolly mini the reproduction of that that was amazing as well yeah I mean like because it's like so I'm curious about so so you know the four of us are working on a Model together and um you know maybe Morgan has a hypothesis like you know let let's maybe try to prioritize training on this subset of data now like like what kind of um like intermediate questions do you explore kind of like is it mostly like um you know like like let's try this configuration of hyper parameters or is it like let's try to train more on this data like what kind of decisions are being made over that question is I if I can take it like I guess um I guess this is what would probably be like uh ml training is still more and more art than science like we actually have a product that can do it in a scientific way which is sweeps and then and there you can like Implement like the Asia search over like a range of hyper parameters like and then you kind of Let It Go but but that that explores just like a small subset of the potential improvements you can make on a Model because like we said like it's like it's all about the data like you can add data you can change the order you can filter it um there's a bunch of things you can do actually like on the on the code side you may need to implement like a different augmentation scheme like maybe that's that's relevant for llams but uh you know you can like there's a bunch of things that actually require some implementation and then this is like where um I guess like like like actually you need to run lots of experiments and then uh like what we typically do is we we just run lots of experiments and then like logging all of them we can compile like as long as you have like a good evaluation mechanism which is uh also like a bit of an art like how to do evaluation properly but once you have good evaluation you can then compare those experiments and figure out like okay which of these things worked I guess priorities I think is um is also it's also a bit of an art here yeah well all that is so it's so amazing to me I mean I guess one of the so I have one other question I've seen um hugging faces put together I think they're calling it collections where you have like uh like one uh repository for the model the data set and the paper and then I I also Imagine like the weights and biases sweeps adds quite a lot to the scientific present like a lot of these papers you know they have some combination of loss functions and it's like Alpha and it's like well what happens is you ablate the alpha how does kind of sweeps fit into that picture what are your thoughts on that kind of like you know trying to combine everything that this machine learning paper took into one kind of centralized Place yeah we love we love the the machine learning teams that use weights and biases because uh first like you can actually reproduce that research quite easily if if it was locked in weights and biases like it definitely adds credibility like you can see like hey this this happened for you like I can go ahead and check the uh the training class I can look at the the way evaluation was done so it it definitely like I love research that that is done with weights and biases um I think this also helps like research teams be more effective um and then in terms of documentation yeah we've seen some projects like documents like all of that experiments with uh with with reports and and if they've done if they use sweeps and in the project then that also like comes like with like really cool visualizations and I think that that gives you more of a you know like a paper like academic paper is like a snapshot and it's like a snapshot and then it's like this this Iceberg where you have like you know thousands of experiments and then you like make a snapshot of like the best one and people like assuming you're a genius because like you produce this you know perfectly working thing but you don't see like all of the things that didn't work so if you actually um you know if you actually courageous enough to to publish also like the research that did not work right that's I think that it's so much more valuable for practitioners to see like how you how you came to this uh to this piece of work rather than just the the the final snapshot hmm yeah it's also like that that idea of um you know the four of us are training a model and uh Thomas has this hypothesis that he runs off with and and we have this like report of of how our thinking evolved and yeah it's also fascinating um yes I think that was a really awesome tour of all these topics around uh fine-tuning let me ask you kind of this open-ended question of kind of like um you know what's the thing that's on the horizon that that you're the most excited about maybe each you could do one I I guess maybe I I will start so I'm well I when I first saw uh GPT 3.5 and gpt4 I I did not believe like open source can ever catch up but I think like open sources is surprising me in a lot of positive ways so I'm I'm looking forward to like see um you know like more developments like more ideas being implemented in the open so that we can learn from them um that kind of like making this the science accessible to to um say the average machine learning engineer so I'm super excited about that like experiments like this Moe uh mixture of experts models like I I am excited how it works out that's an awesome one I mean I yeah kind of like it it comes into like kind of also talking about like um you know like fine-tuning it for a gorilla for some specific task this the open source models are getting so good and that's so exciting I mean so so like this kind of idea of this is an I love this topic like the closed Source models versus the open source models for the most generally capable so you so you think uh or like you know like do we think that the Falcon models are really catching up to gb24 or whatever the I don't know what the if llama is the latest state of the art or falcon or yeah I think I think look at least looking at the benchmarks like what like we have like like there are different ways of evaluating from compatic Models like you have the I think on any Benchmark that you think think of like the the classic benchmarks like mlu like human eval or like the chat Arena like the the ELO rankings like gpt4 is is winning all of them I think uh and I think commercial models actually are on top of this this benchmarks but I guess I guess more than like the performance on the benchmarks I I think like I like the momentum that is in the open source and the fact that people are sharing like learning new things and sharing sharing them broadly like that's what keeps me exciting like I love the like I love as as a person that is fascinated by Machine learning I love you know understanding kind of what works what doesn't work I've seen new experiments so that that keeps me excited even if like they're like we're still catching uh catching up like um you know understanding like what's happening is is is quite interesting what do you think about like um you know like with Chad gbt being a product that they're getting to have this feedback loop of where they they collect all this data of their Model Behavior like you could you know like if they let's say they continue the conversation that means this is a good training example or something like that what do you think about that kind of data engine that would kind of fuel the closed Source models I I think it's pretty cool and I hope they are using it um I'm sure they're using it but other companies also have that like I suppose Tesla could have a data engine that is pretty complex from all the cars data and I remember like kapati showcasing that oh this is a super complex training example and we can search our immense database and find similar ones like and then active learning that and improve our malls and those were pretty complex like yeah image miles connected to Transformer at least what they showed us um so yeah that part and to add from dark I'm also excited about this multimodal stuff that's probably coming like they teases us with cha gpd4 scanning a paper and like or picture of your fridge and understanding what you can prepare from that um I'm pretty sure that's gonna happen soon um and yeah this is super powerful yeah agreed I I was gonna I was gonna say two and like oh yeah one of one of them was multimodal I think when gpd4 came out so I think someone in in our internal Slack asked like what is your like uh I think controversial opinion on like yeah ml is going this year or something and I said that um yeah like like llms are like coming for the computer vision folks I think like as well as the fomo like a lot of that Community are sitting pretty and we're like you know not taking our jobs we still needed like you still need our like object detection like skills but like I don't know like we say Gemini we'll see like what two people can manage to do um yeah I think like llms are like coming going to come harder like computer vision quicker than people realize um well I technically write it won't be an llm it'll be like llm plus it's some CB model but yeah you get dude the other thing I'm like super excited to see and like like to continue seeing is like the um on the inference side of things like actually like making these work in production um which is like no small feat and like they're like gdml was like amazing like llm and mlc and all these like new inference libraries they're like our awesome um you know where where we're surely going to be releasing a um llm monitoring product to like to you know like monitor all these like in production but like really all this training all this fine tuning in all this like open source energy is great but if you can't actually service to like real users in you know whatever so second so any second like latency like yeah it's it's not going anyway right so yeah it's just that like having that you know now having like the entire software community help us as in these and like not just trying to figure it out I remember a tweet where it's like the machine learning Engineers are discovering mmap and it's like this obvious like they're laughing at our inability to yeah and uh yeah that's so in like the what's your hot take like um you know I had a friend who who posed it as like tell me something that's overrated like you have to say something is overrated now by the premise of the question but yeah I I love all those takes those it's also interesting and yeah I do agree that the llm uh computer vid I mean that um that paper pre-trained Transformers a universal computation engine showing you could put the latent space of a computer vision model into an LM that was like you know I never would have thought that would work but you know yeah I I it's so exciting I mean the gbt4 I think also from our search perspective the ability of that to um do like the data ingestion layer where you can like visually look at a document and then extract the structure into putting it into one of these search databases that's just incredibly exciting for us but yeah Morgan Thomas Derrick this was an incredible podcast I learned so much it's been so much fun talking to you guys thank you so much for joining the weekday podcast thank you very much ", "type": "Video", "name": "Weights and Biases on Fine-Tuning LLMs - Weaviate Podcast #68!", "path": "", "link": "https://www.youtube.com/watch?v=9wJuza0_ix8", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}