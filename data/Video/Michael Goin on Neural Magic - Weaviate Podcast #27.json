{"text": "Thank you so much for watching the 27th episode of the Weaviate Podcast! This is truly one of my favorite podcasts we have ... \nhey everyone thank you so much for checking out the wevia podcast I'm super excited about this episode we have Michael going from neural magic uh neuromagic is such an exciting technology we've plugged it into Wii V8 to see how you can do this uh make faster predictions with your models so they call this inference acceleration this General space of trying to make machine learning models make predictions faster they use all this amazing technology around sparsity and quantization it's just such an exciting technology overall and I'm so excited to welcome Michael to the podcast yeah thanks for the warm welcome Connor I'm super excited about what we're enabling here together um particularly at in Vector search and partnering with we V8 um yeah we're pretty new to the space um in general we have a lot of experience in uh computer vision and natural language processing um in terms of you know question answering classification your usual sort of NLP but um yeah the whole Space of vector search information retrieval things like this are really exciting to us and seems to be a lot of space for optimizing the future of databases yeah and it's so incredible to just you know change out the text vectorizer and point it to one of these sparse Zoo models and just see how fast it can turn text into vectors turn images into vectors and then also these other Upstream things like as you mentioned question answering text classification uh so could we kick this off by describing uh what is neuromagic like how is this faster model prediction achieved yeah so maybe if I just give a bit of a background on just where where we're coming from uh and in our technology so at neural magic we're building what we consider to be the software uh or really the foundation for software delivered AI um and this is because as machine learning models grow larger and larger and particularly in the last couple of years with Transformers and large language models we start like doubling the numbers of parameters every uh even multiple times a year from like four to eight months is just the usual Cadence now of talking about doubling up parameters and so this like massive growth in model parameter size means that we've had to use specialized Hardware accelerators in order to keep up with these state-of-the-art results however using these specialized Hardware accelerators can be incredibly difficult for organizations to manage because really if you want to optimize these models and get the best price performance you can out of them it can quickly become a big challenge to determine which models you can run on on which hardware and how to optimize each model for that hardware and this is the exact opposite of most application infrastructures and where we've already gotten really good at managing cpu-based infrastructure that you can simply elastically scale what virtual Hardware you have based on the actual application demands uh I mean you know can you imagine if every time you wanted to build uh like a mobile app or a website you had to wait for your application server module or an extra thing you need to plug into your data center or get uh get access to from your cloud provider there's a reason this doesn't happen anymore um and the industry always evolves into simple open commodity hardware and software and so that's what we're bringing here with neural magic we're providing the open software piece and enabling all this commodity Hardware you already have available such as your CPUs um to to start thinking about AI inference for those platforms um so really excited about that and through that we're uh you know we do the usual sort of inference acceleration things of oh you want to quantize your models bring them to lower precisions and get that extra memory and speed benefit but we also focus on sparsification so this is a process where we're pruning away individual weights of the model while keeping its overall structure but think of it as just another compression algorithm you have all these zeros and your weights now and you can just simply choose to not represent them and not calculate them so this results in a memory and speed savings now the research for sparsification has been around for several decades um people have been specifying the models mostly for size uh you know compression benefits but in terms of seeing actual inference feed from it um particularly on the hardware side just hasn't fully been explored and so we focus on making smarter algorithms and heuristics for executing these sparse models in a sparse native way rather than just throwing you know dense matrix multiplication at the problem and kind of brute forcing these models like traditional Hardware accelerators do um yeah yeah that's incredible and I think there's a lot in there to unpack I I love this argument around say the the CPU the the commodity computers and how you know having the gpus is this bottleneck compared to how you package software applications uh hearing about the pruning and getting into the size now we're definitely going to uh unpack that a little more as well but like we also hit a little more about this um just kind of like these numbers and some of these claims that like I came across neural magic I saw these uh images where it's like Burt large uh distillber and this large run is a sparse large running just as fast as the Silver and now that I've plugged this into wavy and I've seen it myself firsthand and so can you tell me about just like these numbers that you've all been able to produce because I think these just jump out to people when you're scrolling through Linkedin and you see one of these bar charts it's like five seconds on CPU six seconds on a 100 and say 13 seconds on T4 yeah so uh yeah great pulling out all the results there's a there's a lot to go through we're coming up with new results every day um but I especially find that first example you brought up really interesting of uh and this was actually uh research that we did in collaboration with an Intel research lab so Intel themselves are interested in sparsity even if they don't have uh you know uh inference acceleration for it there's still interested in the research it's another form of compression alongside quantization and so actually Intel research Labs over in uh uh their Israel base um uh have been working on greater various Varsity uh achieving greater and greater sparsities and uh you know contacted us for using our engine as a as a sort of runtime for these models they're producing and they published this paper called uh prune once for all um and so it's basically this idea that um uh you know what we call sparse transfer learning um but the essential idea that you can actually prune your pre-trained Upstream model so just like you would take a burp Bass from hugging face that's been pre-trained on Wikipedia or some large Text corpus and a sort of unsupervised fashion then you take this pre-trained model and fine tune it for your specific task on a much smaller set of data um so what we found out with them is that you can actually sparsify these models in the pre-training stage so now you have a sparse architecture um that's pre-trained and you can just maintain the sparse architecture we found of it and fine-tune it onto all your data sets and results that they've published and results that we publish from our research lab as well is that the sparse transferring generalizes quite well across a variety of data sets and sparseity can actually help the generalization of models which is really interesting to think about um in that maybe your first intuition isn't that but the intuition we've kind of come across is not having you know those millions and hundreds of millions extra parameters means that the model has to make each parameter count a lot more and take a sparser more General uh uh um a representation in order to maintain the accuracy of the whole model so it's really interesting this Dynamic play of um you know maybe starting with dense models that work with too many parameters that work really well with our Stark very stochastic methods of training where we throw more data at it but then getting to a good base accuracy and then working on sparse architect finding sparse architectures to you know go through the step of optimization before we just consider uh deploying these these dense models um so yeah in in the case you brought up yeah we were able to get a large model um to 80 sparsity with quantization and get its speed to the same level of a distilbert model which has I think uh eight times less parameters um or maybe even 10 times less parameters um and uh at maintaining essentially the same accuracy of the original Bert large model so staying within we usually Target 99 of the dense models accuracy as a as a as a place for sparsity to keep us um and yeah getting that 8X Improvement on Bert large to uh keep its accuracy much higher than distilbert but get to that same latency um so yeah really opening up what size models we can or what accuracy models we can really consider for deployments nowadays yeah and I I want to like firstly congratulate you all on having this like science and product it's so interesting the The Bert surgeon this uh uh prune once for all from getting the name right it's so cool seeing the science and the product together and and uh so so I really want to get into uh so the sparse zoo and how these models come to be the difference um starting from the sparse transfer learning I find that to be absolutely super interesting the way that say you sparsify a language model and then you know what we do in Vector search a lot is we train with contrastive learning we have maybe uh triplet losses or we have this uh multiple negatives ranking loss so we you know we can take a sparse model and then fine tune it with our kind of contrastive learning objectives to produce vectors yeah so what is the state of the Sparks Zoo how do we you know sparse transfer learn get new sparse models on the zoo what does that all look like yeah so great question so sparse zoo is a website you can go to smartstood.com and you can check out um the 100 or so models that um we host both their their dense sort of baselines and then all of the sparse and sparse quantized models we produce since there um uh you know for a variety of tasks across computer vision and NLP um so definitely encourage you to check that out um but yeah within there this is where we have these sparse Upstream models um or even these sparse Downstream models um that are that have been produced by our research our ml researchers and engineers and the key thing is that everything we push up there is reproducible we have commands ready for you to make them with the rest of and we use this concept of recipes which were really just yaml files but controlling all the hyper parameters and pruning modifiers quantization modifiers basically laying out a whole schedule of training for this model or optimization depending on where you're starting um and and that all folds into our open source machine learning library called sparse ml that plugs into and builds on top of common Frameworks like pytorch ultralytics pug and face Transformers things like this so uh you know both with like uh you know in the in the hug and face Transformers case you know we have our own trainer class that we've inherited from the Transformers class and added sparsification to it and so you should you know you once you pip install sparse ml you can build your ml application with this trainer exactly in code or we have CLI commands like sparsaml.transformers dot question answering for example and you can just point it to a model you want to train reference the recipe in spark Zoo to follow the same path we've gone choose your data set choose your hyper parameters use essentially like you would any Transformers call and and we really hope that helps people reproduce their results and put bring them onto their own custom data sets it's all open source so you can contribute your own custom pipelines um and and maybe training losses things like this um and so yeah uh sort of all scattered across readme's and documentation and on sparsu itself there's uh commands and guides for each of the models that we that is part of the process of uploading a model you need to um have the sparse ml commands for training it evaluating it validating the accuracy and the Deep sparse commands for uh validating the speed that we measured as well or you know acceleration um so so yeah that's kind of an overview at of our ml side and then of course on the inference side we have the Deep sparse engine um which is free to use uh for personal and and research um so we're really excited about what people have been building with that um you know in our slack communities and on GitHub issues um people using it for varieties and models we haven't even uh decided to sparsify ourselves yet they've found success in medical imaging super resolution um even uh you know subclasses of popular NLP um tasks that we haven't gone for yet like multi-label um uh classification yeah super interesting and I I definitely want to get into um benchmarking as we've talked about how we you know clock these things in the difference between say 128 sequence lengths or 384 sequence lengths and I want to get into that but um on this topic of recipes and the sparse ml trainer I first kind of became familiar with this kind of recipe thinking of what goes into a trainer when I was learning about Mosaic ML and their composer and they have all these different regularizers that you you know package into a recipe to train your model with like so I know a little bit about pruning you zero out some weights I know there's like iterative pruning maybe second order pruning is also something you do is what is the recipe for the esparzano yeah so um yeah recipes in sparse smell can can hold a lot of information it it you don't have to use them for pruning or optimization in general they have very simple building blocks with them simple things like learning rate modifiers um so how you want to modulate your learning rate over time or introduce um ways to boost accuracy like distillation from teacher models or a dense upstream or even dense Downstream models um these are all things that help improve the accuracy improve the robustness of training your model um and so so yeah really a recipe is just uh that the base thing to it is you define a start Epoch and a end Epoch and then you list out all of the the modifiers all these little um uh modules essentially you want to hook into your training pipeline um over or you know over the training time of the model um so yeah and we focus a lot on Pi torch we have some Integrations with Keras and tensorflow as well um but yeah it's totally mentioned that you know you can make a sparse smell uh manager essentially and just hook this in as a callback function into your training framework of choice and and it can start um you know overriding things like your learning rate um uh all sorts of things like this other Integrations as well as uploading your experiments uh automatically to weights and biases and and comment uh um tensorboard common uh common ways to for ML Ops to view the status of your experiments and uh you know early stop them uh continue them back up from checkpoints um things like this you know made made for the ml practitioners but also all wrapped up and command line scripts so that um uh you know data scientists and data hackers can also get started on just trying a new data set trying a new experiment they have in mind super interesting and I yeah the whole the way that this enables sparse models and getting away from needing these crazy expensive dense models and towards this more exploiting the inherent sparsity of neural networks to make this more economical run on CPUs Etc about continuing in sort of the Practical stack so now we have our sparse model and we want to evaluate it and say how much faster it is the things like say moving the data to the GPU or all these things what goes into benchmarking these inference accelerations and saying this is faster than pytorch be you know yeah yeah great question it's and it's really difficult to it's unfortunately pretty difficult to Benchmark across all these different types of hardware and Frameworks they all want to do it in different ways but we try to abstract this away as much as possible especially on since most of our models are based in pi torch back ends um we we put a lot of this work into sparse ml you know in addition to the validation process also measuring how fast are we validating the results you know what what's our iterations per second things like this and so this allows us to measure at least you know some CPU and GPU results on Pi torch and measure that but um you know most of our benchmarks are comparing um CPU how much faster can we make CPUs and so from that perspective we like to run and compare against other uh engines that were built for inference such as Onyx runtime or openvino um that uh and and that's part of our process of actually getting to inference time so once you you've trained a model or taken a model from sparse or sparse ml in in your framework of choice we usually export that model into an open uh model format called Onyx and this is just a graph representation where each node in the graph it's a different operations such as convolution matrix multiplication relu pooling things like this so maybe just your one multi-head attention block or Transformers is broken into 20 different nodes that are all connected uh but the key thing this is a an open format you know uh uh spearheaded by Microsoft Facebook Google um so all the Frameworks can export into this and and can even uh take Onyx in as a as a a as an input uh back in and so that's the main thing that the Deep sparse engine takes and uh usually once we get this Onyx then we can bring this in to all of the other inference engines as well and start to measure their performance um and so yeah getting into the details of performance and and what are the different benchmarks that can really cover accurately the the realistic scenarios that you're going to use once you once you deploy models there's there's a couple major ones so um the first sort of obvious one is when you have just a single image or a single you know query coming into the model you want to run this as fast as possible and you want to get that result back to feed into your pipeline and we call this a synchronous or a latency based scenario so we're talking about batch size one and you know a single image a single sentence or set of tokens um representing one request and our goal there is to see how fast can the whole system or section of the system uh uh return this result back to you so this is really common and a part where actually gpus uh or Hardware accelerators in general struggle a lot as they usually can't access the memory on the CPU which is where this result usually starts from in terms of you know you are are getting a an HTTP request someone's giving you an image and asking for a result that networking request lands on the cpu's memory and then now if you want to run it on a GPU you need to pass that image over pcie express onto the GPU copying onto its memory it then executes on that then you need to copy the result back over PCI Express back onto the CPU memory it can put together the network response and then send that back out so when you're really talking about a handful of milliseconds per request and you you don't want the ml model to be the bottleneck in your application pipeline this is where latency is a really important scenario to look deeply into um so that's that's a basic one we do there's also a throughput or batched scenarios where say you're working on an offline task say you have you know a million things you want to embed before the end of the night the question is just how quick or how cost effective can you can you run through this and so here you find a good batch size that can fit within your memory available or utilizes the system well and then you distribute all these batches across your available Hardware as fast as possible and so this allows you to amortize some of the communication costs you still have a lot of data to move around but it should be a place where you're really Bound by um compute as long as everything fits into memory and so this is pretty good for offline tasks and then another example that we hear a lot about from our partners and what they're interested is um is sort of a model server or a multi-stream asynchronous case and so think of it just like a web server uh and and we uh make it very easy to start up model servers so just think of we're presenting a model to the world um as a restful endpoint we commonly use fast apis this is just a super easy to use thing that you can just ping its doc endpoint get information about what format your your image or tensor needs to be in send it over and you you also have a schema to tell you how to use the output and so in this case you know you're serving multiple users at a time like you would at a web server and each request that is coming into your server you need to dispatch this efficiently across your Hardware you don't want each of these responses running into each other and causing each other to slow down and you know the goal is to serve as many users as possible and many requests as possible while hitting you know the latency requirement that you have for your application and your users um so this is something we spend a lot of time on as this is where a lot of application deployments end up in terms of you know either it's it's sort of latency or offline in terms of making very small you know vcpu or Cloud instances so this works well if you're going in say a kubernetes cluster and you you know maybe like each node is working on a single inference but often it can be a lot more cost efficient to just have a single larger node that this model server is hosting up a model or even multiple models on one node and uh just depending on the request coming in dispatching those models and there's requests across this single system um which can really help drive down cost and also make your application complexity um or you're really really your uh your ml infrastructure much simpler so hope that was helpful those are sort of the major three scenarios yeah it's incredible the uh the node server and how you can have each node going in so I have my question to kind of build my understanding of this and I'm sorry to listeners if this is a dumb question but it is it like you would want to do one by one inferences on the CPUs compared to where the GPU I think what you you batch it up into say like 64 predictions at a time because it's this big GPU so you like you know take a lot of data at once is it with the CPU thinking say you have like 16 cores is it like inference inference inference like like a multi-threaded programming way of thinking about inference um yeah definitely in the multi-stream case it can be really it can be usually pretty efficient to uh you know have these like multiple streams where each one of these like batch size one inferences are pinned to individual cores um to really help so it's kind of like an embarrassingly parallel problem each core is working on different problems um but you know in terms of uh getting the best possible performance this is where it's really in your best interest to use an inference engine that's designed for CPUs like deep sparse where we're aware of especially the the unique topology that Hardware topology that CPUs have in terms of having multiple levels of cash layer one layer two layer three Cash sometimes even a layer four cache and your main memory of course and uh particularly with these sparse uh quantized models you can for instance on these large models we've produced you know uncompressed as a dense fp32 model they're commonly at one or 1.2 gigabytes on disk and even more in memory and once we've sparsified these uh large models we've been able to lower them by um at least an order of magnitude getting them down under 100 megabytes for instance once sparse quantized and so um this now uh is really interesting as like oh maybe your L3 cache on the CPU is 70 megabytes so maybe we can fit you know half of the network just in the L3 cache of this section of cores and so let's just have this section of course with this L3 cache just running this model and so all inferences are going to go through this section of cores now we don't require you to have to have this sophisticated understanding of the hardware you're running on but these are some of the optimizations we think about when you're building your application with our we with our engine and our especially our model server that can intelligently make these decisions based on the hardware available and and the models you provided as well compiling the model down to fit the hardware available so it's a really interesting optimization problem and we have a lot of systems and math Engineers that work here who find that problem interesting as well um but uh but yeah generally you know CPUs can see a benefit from throughput as well it's not that it's just completely lost on them you you get more throughput and generally the scaling is better as you become more compute bound obviously because we're removing you know 90 of the compute from the model when we prune it to 90 percent uh so it really is about efficiently using all subsystems of the hardware from the compute you have available to the memory to the networking um and and even the the larger ml pipeline around the model um you know we we offer this um uh you know similar to Transformers this idea of pipelines that um will wrap the model around pre-processing and post-processing that's needed um both for ease of use but also with good defaults for Speed so um you can just host up a whole pipeline instead of hosting a model and you can just feed that straight text instead of needing which tokenizer do I need in order to tokenize the words into the embeddings and then provide these tokens to the model um so that you can just send your text and get your sentiment out of the model um directly without needing to know all the nitty-gritty of ml pipelines and how to do it in the right way efficient for the hardware so so it is is really about bottlenecks at every single layer both inside and outside of the model yeah those abstractions of pipelines are so useful like from sentence Transformer import sentence Transformer and model Dot and code and then just the string and then you've got the vector and that abstraction having that similar thing um I really want to ask about from the beginning uh the CPU cache hierarchy and sort of generally the CPU what what it offers but uh quickly um so to access is you know running it from pip install deep sparse from Deep sparse import pipeline is and then is that the best way or is there say like a Docker container uh that would put this inference engine I don't know these things too well but like what's the difference between you know running in and python or say is there like some container I think the question to ask yeah yeah yeah no it yeah totally understood um yeah I mean I think definitely for developers that like to get their hands Nitty Gritty um understand the the ux of the the code itself and uh understand the code behind our benchmarks as well um you know it's in your best interest yeah pip install it run it locally use the pipelines use the models without the pipelines um and and and read all the code that's on the Deep Source GitHub um now yeah for people that are more thinking about how can the scale um in my infrastructure um particularly things like kubernetes yeah we we offer um Docker containers you know most commonly um yeah we have this deep sparse server that is that the docker containers usually built around and then it's just the case of what config file do you give that Docker container and the config file um just defines yeah what model do we want to use what's the pipeline that it's using uh you know what IP address and port or what port should I host it on simple things like this how many streams should I work on how many cores should I use uh this is all sort of in a text file that you can provide um but yeah we actually have some examples uh up to date on uh sagemaker and we're working on ones for AWS Lambda and Cloud run to to show you examples uh or even on just you know local kubernetes examples I think we're doing the mini Cube as well um to scale across eks or gks so that yeah you can you can just treat your document container as another um yeah web application that uh you know you have this uh restful API into um and so it's just the case of how many pods do I put this Docker container on put a load balancer over it and build your application without worrying about um uh without worrying about scale it's just managed for you um and I think that's really interesting uh just in the case that you know we are running on CPUs we can run on vcpus at the smallest level because we're just looking at what instructions are available you don't need to set up any fancy drivers um like Cuda or special environments um it's really just running on Linux looking at the CPU information and uh you know able to scale down all the way down to a a task uh or a pod on a kubernetes cluster all the way up to you know uh uh bare metal multi-socket servers that are you know uh uh uh hosting 20 models at a time yeah super exciting I think the like right now the way I've plugged in neural magic to aviate is through our text to VEC Transformers container you just replaced the docker image with one that has the neuromagic pipeline and uh yeah it's exciting because I think there's still even more optimizations we can do to get this like even faster and so so kind of the next topic I think we've kind of really covered the um the like how to use it the practical application and uh so I was listening to uh near shavid on Jana kilcher's podcast taco and it seems like a whole reinvention of the model architecture like things like say you can access a terabyte of memory and then the human brain is sparse I can tell me about like this kind of vision of like like sparsity is it is it more than just like layer by layer one quote from the podcast that stood out to me is something around thinking yeah like generally like this thinking of like layer one layer two layer three that there's some other way maybe it's like a path that doesn't wait for this one to finish other ways of traversing this graph do you think about this kind of thing yeah yeah I mean we think about this thing internally um obviously you know from our background uh you know from MIT you know co-founders near shuvit professor at MIT really accredited wonderful individual um and and his post-doc uh Alex uh Mata matiev um that uh that they co-founded this company together you know they were working in the in a connectomics lab together and uh working with brain researchers as the computer science people to help them and yeah they sort of from the problem they were working on they were trying to image and or pattern out brains and these are obviously sparse architectures so they came up with sparse algorithms to map onto the brain and so um yeah totally you know the the road we're going down in terms of uh or the road we are down uh in terms of training models and deploying them very much works on the idea of having dense layers feeding into dense layers larger and larger models um that are at fully connected layers um and like I said this works well because we found a golden bullet that works for now stochastic gradient descent you just throw more data at it you regularize it enough you showed enough data at a time and and it can just continue and continue to keep learning um but that doesn't mean that it is the best way um uh and I think like in the future um you know this the scaling especially as Moore's Law um you know struggles to continue forward um we need to find more efficient ways of training and running these models um and and this is what we hear exactly from our customer engagements they a lot of times they have models running in production or they're gearing up and then they're getting some load and they're realizing yeah this doesn't make sense uh but the value is there for their uh for the people using it it's just too expensive operationally to run all these models um and that's because most the time they're just training the model on the GPU and then they're like well it's accurate uh how do we run it and then they go run it on the same GPU that they that they trained on um and you know haven't taken the thought of from training to inference let's have a step of optimization in there and that's really what we're what we're um working on now introducing that optimization step to more and more people um now of course like you probably shouldn't be training dense models in the first place but um it'll take a lot of research and progress in science to get there um but I'm sure we we can get there um you know purely from the idea yeah the brain is a is equivalent to a cell phone of compute with a petaflop of memory um it's it's not the other way around where you know we we don't have exit flops of compute in our brains um with it with a you know yeah certainly we don't have teraflops of compute with just you know 16 gigabytes of memory attached um we we have a lot more memory um and and access it really efficiently so that uh we don't need to eat all the time um so so yeah that that's that's definitely how we think about it um but also you know just in case of talking about this asynchronous execution um that that we've built in our engine as uh deep tensor columns um it's not like we're only like we're the only ones working on this um you know a great example actually is the uh the really famous Google paper um from I think about a year ago Pathways um or even their sparse Transformers paper before that where they train you know I think a trillion parameter model even larger than gpt3 um but the key thing was based on the input coming in you know you would have say multiple uh multi-head attention or fully connected layers multiple essentially experts you could choose from and based on the input coming in it would route it through a different pipeline so you have you know 100 times more parameters than than you would actually use for each individual inference but you're sampling them over time for different inferences um so this is obviously at a very structural large scale but uh it you know it makes sense this is what the brain does we're not accessing we're not running all of the weight setting parameters and neurons and synapses in our brain every second it's localized at certain pieces that we need to run every single millisecond or pieces that we need to run we need to remember a nice memory or something like that so so yeah it's it's really interesting yeah that's super interesting I guess I was thinking of that yeah those two angles where one it's like you're realizing the lottery ticket hypothesis where you know you theoretically can train this bar sub network from scratch and having this quantization aware training where you're pruning during training is like getting closer to that kind of idea where it's like be mindful of the sparsity during training and then like this Pathways thing which is like a total rethink of how deep neural networks are gonna I'm sorry I'm gonna process data to make their predictions um so one other uh thing that I think is really interesting is uh Edge applications like predictions at the edge putting it in phones or embedded sensors things like this can you tell me about like your motivation around Edge applications say like maybe robotics or like apps that I guess don't have to network like what what's the thinking around Edge case applications of deep learning yeah so uh you know the most obvious thing is power you want to keep power on these things as little as possible um so it's obvious we want to make the machines as efficient as possible um while also maybe paying the least amount to get them on there but um in addition to that there's also the cost of of the networking uh of the bandwidth you pay um and uh you know we've heard this a lot um from from retail and Manufacturing um in that uh you know they they don't want to send uh you know 200 cameras from every store out to AWS they don't want to pay the Ingress costs every day um and it would be much better if they could at least um uh you know just send out the high level analytics every day instead of doing to process all of that in a data center you know hundreds of miles away and also use that bandwidth of the store that their their you know employees and customers would would like to have good Wi-Fi at um so it uh yeah it really is like a problem uh a problem with more problems at every level um but yeah I mean it's really from the case like you know you think about um a camera or say like a you know a camera running uh at your front door at your doorbell for uh you know to look at the delivery driver or um you have to essentially replace your doorbell many people have these um and they wouldn't really like every single thing being shipped out to um uh to the people that make them um and so each one of those cameras I bet you a hundred percent has a CPU attached to them um maybe with some you know video decoding stuff next to it but the CPU has to be communicating that thing and so if we can make the models and things um that are that are useful for this data small enough and compressed enough but still accurate enough to run on these devices um at the edge and then the camera doesn't communicate with anything even even on your local Wi-Fi using that bandwidth um uh even outside of external bandwidth um unless there's a warning or something serious that should be looked at or every once in a while um that's just a better experience for everyone involved and and plus the the box that you have um plugged into your wallet integrated into your Mainframe you don't need to have the compute there have the power there have it be noisy and uh um and things there so definitely we hear the asks every day um and that's why obviously a lot of people are interested in what we have to do um so yeah it's really just about um figuring out the whole pipeline Beyond just optimizing the model to make it easy to build these applications fit them into smaller devices that's part of the reason why you know the Deep Source engine we focused on x86 CPUs to start with so made by Intel and AMD because those especially were the most common four years ago uh uh which we the company was founded at but now arm is is running everywhere both in the cloud on the edge on this MacBook I'm talking to you right now from um so so we're working on arm support right now and hoping to have an alpha by the end of this uh this year and hopefully a demo at neurops in December where we'll be presenting um so uh so so yeah looking forward to getting sparse networks onto more types of hardware yeah that's a super interesting example of the um the doorbell like I think it detects the package deliveries and you might not necessarily want to have a doorbell of your front door like streaming to the cloud service all the time and all this motivation around say like Federated learning and that kind of thinking and I think it's interesting too with with we did if you want to have like a vector-based approach to this where uh say it's a security camera and it's kind of like doing outlier detection so it has the you know it takes an image of the of the front of your yard vectorizes it on the CPU with something like neuromagic and then it builds up this Vector index to do the outlier detection I think it's very interesting and also say like this vision of like uh the Tesla bot like where you can have a household robot that you get I think I think a lot other than Tesla I think a lot of startups believe in this kind of thinking that we're going to have like a five foot eight uh robot that you say uh make me some eggs and then it will go make eggs and similarly in that case I could imagine that you'd want it to not do this kind of bandwidth uh sending and have it be like yeah you don't want it taking up a a person's worth of uh someone streaming Netflix on your Wi-Fi constantly right hook it all up yeah certainly so yeah we've heard that as well yeah awesome well thank you so much Michael I you know I think the technology of neuromagic the way it fits with weeviate is so exciting and you know sparsity the visions of this the sparse Zoo sparse ml all of it is just so captivating and thank you so much for uh doing the weave podcast yeah thanks so much for having me and it's been great talking to you and and learning from you over the time really excited to uh yeah learn more about the use cases for Vector search and uh contribute some some cool demos ", "type": "Video", "name": "Michael Goin on Neural Magic - Weaviate Podcast #27", "path": "", "link": "https://www.youtube.com/watch?v=leGgjIQkVYo", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}