{"text": "Thank you so much for watching the 27th episode of the Weaviate Podcast! This is truly one of my favorite podcasts we have ... \nhey everyone thank you so much forchecking out the wevia podcast I'm superexcited about this episode we haveMichael going from neural magic uhneuromagic is such an excitingtechnology we've plugged it into Wii V8to see how you can do this uh makefaster predictions with your models sothey call this inference accelerationthis General space of trying to makemachine learning models make predictionsfaster they use all this amazingtechnology around sparsity andquantization it's just such an excitingtechnology overall and I'm so excited towelcome Michael to the podcastyeah thanks for the warm welcome ConnorI'm super excited about what we'reenabling here togetherum particularly at in Vector search andpartnering with we V8 um yeah we'repretty new to the spaceum in general we have a lot ofexperience in uh computer vision andnatural language processingum in terms of you know questionanswering classification your usual sortof NLP butum yeah the whole Space of vector searchinformation retrieval things like thisare really exciting to us and seems tobe a lot of space for optimizing thefuture of databasesyeah and it's so incredible to just youknow change out the text vectorizer andpoint it to one of these sparse Zoomodels and just see how fast it can turntext into vectors turn images intovectors and then also these otherUpstream things like as you mentionedquestion answering text classificationuh so could we kick this off bydescribing uh what is neuromagic likehow is this faster model predictionachieved yeah so maybe if I just give abit of a background on just where wherewe're coming from uh and in ourtechnology so at neural magic we'rebuilding what we consider to be thesoftware uh or really the foundation forsoftware delivered AIum and this is because as machinelearning models grow larger and largerand particularly in the last couple ofyears with Transformers and largelanguage models we start like doublingthe numbers of parameters every uh evenmultiple times a year from like four toeight months is just the usual Cadencenow of talking about doubling upparameters and so this like massivegrowth in model parameter size meansthat we've had to use specializedHardware accelerators in order to keepup with these state-of-the-art resultshowever using these specialized Hardwareaccelerators can be incredibly difficultfor organizations to manage becausereally if you want to optimize thesemodels and get the best priceperformance you can out of them it canquickly become a big challenge todetermine which models you can run on onwhich hardware and how to optimize eachmodel for that hardware and this is theexact opposite of most applicationinfrastructures and where we've alreadygotten really good at managing cpu-basedinfrastructure that you can simplyelastically scale what virtual Hardwareyou have based on the actual applicationdemandsuh I mean you know can you imagine ifevery time you wanted to build uh like amobile app or a website you had to waitfor your application server module or anextra thing you need to plug into yourdata center or get uh get access to fromyour cloud provider there's a reasonthis doesn't happen anymoreum and the industry always evolves intosimple open commodity hardware andsoftware and so that's what we'rebringing here with neural magic we'reproviding the open software piece andenabling all this commodity Hardware youalready have available such as your CPUsum to to start thinking about AIinference for those platformsum so really excited about that andthrough that we're uh you know we do theusual sort of inference accelerationthings of oh you want to quantize yourmodels bring them to lower precisionsand get that extra memory and speedbenefit but we also focus onsparsification so this is a processwhere we're pruning away individualweights of the model while keeping itsoverall structure but think of it asjust another compression algorithm youhave all these zeros and your weightsnow and you can just simply choose tonot represent them and not calculatethem so this results in a memory andspeed savings now the research forsparsification has been around forseveral decadesum people have been specifying themodels mostly for size uh you knowcompressionbenefits but in terms of seeing actualinference feed from itum particularly on the hardware sidejust hasn't fully been explored and sowe focus on making smarter algorithmsand heuristics for executing thesesparse models in a sparse native wayrather than just throwing you know densematrix multiplication at the problem andkind of brute forcing these models liketraditional Hardware accelerators doumyeah yeah that's incredible and I thinkthere's a lot in there to unpack I Ilove this argument around say the theCPU the the commodity computers and howyou know having the gpus is thisbottleneck compared to how you packagesoftware applications uh hearing aboutthe pruning and getting into the sizenow we're definitely going to uh unpackthat a little more as well but like wealso hit a little more about this umjust kind of like these numbers and someof these claims that like I came acrossneural magic I saw these uh images whereit's like Burt large uh distillber andthis large run is a sparse large runningjust as fast as the Silver and now thatI've plugged this into wavy and I'veseen it myself firsthand and so can youtell me about just like these numbersthat you've all been able to producebecause I think these just jump out topeople when you're scrolling throughLinkedin and you see one of these barcharts it's like five seconds on CPU sixseconds on a 100 and say 13 seconds onT4yeah so uh yeah great pulling out allthe results there's a there's a lot togo through we're coming up with newresults every dayum but I especially find that firstexample you brought up reallyinteresting of uh and this was actuallyuh research that we did in collaborationwith an Intel research lab so Intelthemselves are interested in sparsityeven if they don't have uh you know uhinference acceleration for it there'sstill interested in the research it'sanother form of compression alongsidequantization and so actually Intelresearch Labs over in uh uh their Israelbaseumuh have been working on greater variousVarsity uh achieving greater and greatersparsities and uh you know contacted usfor using our engine as a as a sort ofruntime for these models they'reproducing and they published this papercalled uh prune once for allum and so it's basically this idea thatum uh you know what we call sparsetransfer learningum but the essential idea that you canactually prune your pre-trained Upstreammodel so just like you would take a burpBass from hugging face that's beenpre-trained on Wikipedia or some largeText corpus and a sort of unsupervisedfashionthen you take this pre-trained model andfine tune it for your specific task on amuch smaller set of dataum so what we found out with them isthat you can actually sparsify thesemodels in the pre-training stage so nowyou have a sparse architectureum that's pre-trained and you can justmaintain the sparse architecture wefound of it and fine-tune it onto allyour data sets and results that they'vepublished and results that we publishfrom our research lab as well is thatthe sparse transferring generalizesquite well across a variety of data setsand sparseity can actually help thegeneralization of models which is reallyinteresting to think aboutum in that maybe your first intuitionisn't that but the intuition we've kindof come across is not having you knowthose millions and hundreds of millionsextra parameters means that the modelhas to make each parameter count a lotmore and take a sparser more General uhuh um a representation in order tomaintain the accuracy of the whole modelso it's really interesting this Dynamicplay ofum you know maybe starting with densemodels that work with too manyparameters that work really well withour Stark very stochastic methods oftraining where we throw more data at itbut then getting to a good base accuracyand then working on sparse architectfinding sparse architectures to you knowgo through the step of optimizationbefore we just consideruh deploying these these dense modelsum so yeah in in the case you brought upyeah we were able to get a large modelum to 80 sparsity with quantization andget its speed to the same level of adistilbert model which has I think uheight times less parametersum or maybe even 10 times lessparametersum and uh at maintaining essentially thesame accuracy of the original Bert largemodel so staying within we usuallyTarget 99 of the dense models accuracyas a as a as a place for sparsity tokeep usum and yeah getting that 8X Improvementon Bert large to uh keep its accuracymuch higher than distilbert but get tothat same latencyum so yeah really opening up what sizemodels we can or what accuracy models wecan really consider for deploymentsnowadaysyeah and I I want to like firstlycongratulate you all on having this likescience and product it's so interestingthe The Bert surgeon this uh uh pruneonce for all from getting the name rightit's so cool seeing the science and theproduct together and and uh so so Ireally want to get into uh so the sparsezoo and how these models come to be thedifferenceum starting from the sparse transferlearning I find that to be absolutelysuper interesting the way that say yousparsify a language model and then youknow what we do in Vector search a lotis we train with contrastive learning wehave maybe uh triplet losses or we havethis uh multiple negatives ranking lossso we you know we can take a sparsemodel and then fine tune it with ourkind of contrastive learning objectivesto produce vectors yeah so what is thestate of the Sparks Zoo how do we youknow sparse transfer learn get newsparse models on the zoo what does thatall look likeyeah so great question so sparse zoo isa website you can go to smartstood.comand you can check out um the 100 or somodels that um we host both their theirdense sort of baselines and then all ofthe sparse and sparse quantized modelswe produce since thereum uh you know for a variety of tasksacross computer vision and NLPum so definitely encourage you to checkthat outum but yeah within there this is wherewe have these sparse Upstream modelsum or even these sparse Downstreammodelsum that are that have been produced byour research our ml researchers andengineers and the key thing is thateverything we push up there isreproducible we have commands ready foryou to make them with the rest of and weuse this concept of recipeswhich were really just yaml files butcontrolling all the hyper parameters andpruning modifiers quantization modifiersbasically laying out a whole schedule oftraining for this model or optimizationdepending on where you're startingum and and that all folds into our opensource machine learning library calledsparse ml that plugs into and builds ontop of common Frameworks like pytorchultralytics pug and face Transformersthings like this so uh you know bothwith like uh you know in the in the hugand face Transformers case you know wehave our own trainer class that we'veinherited from the Transformers classand added sparsification to it and soyou should you know you once you pipinstall sparse ml you can build your mlapplication with this trainer exactly incode or we have CLI commands likesparsaml.transformers dot questionanswering for example and you can justpoint it to a model you want to trainreference the recipe in spark Zoo tofollow the same path we've gone chooseyour data set choose your hyperparameters use essentially like youwould any Transformers calland and we really hope that helps peoplereproduce their results and put bringthem onto their own custom data setsit's all open source so you cancontribute your own custom pipelinesum and and maybe training losses thingslike thisum and so yeah uh sort of all scatteredacross readme's and documentation and onsparsu itself there's uh commands andguides for each of the models that wethat is part of the process of uploadinga model you need toum have the sparse ml commands fortraining it evaluating it validating theaccuracy and the Deep sparse commandsfor uh validating the speed that wemeasured as well or you knowaccelerationum so so yeah that's kind of an overviewat of our ml side and then of course onthe inference side we have the Deepsparse engineum which is free to use uh for personaland and researchum so we're really excited about whatpeople have been building with thatum you know in our slack communities andon GitHub issues um people using it forvarieties and models we haven't even uhdecided to sparsify ourselves yetthey've found success in medical imagingsuper resolutionum even uh you know subclasses ofpopular NLPum tasks that we haven't gone for yetlike multi-labelumuh classificationyeah super interesting and I Idefinitely want to get intoum benchmarking as we've talked abouthow we you know clock these things inthe difference between say 128 sequencelengths or 384 sequence lengths and Iwant to get into that but um on thistopic of recipes and the sparse mltrainer I first kind of became familiarwith this kind of recipe thinking ofwhat goes into a trainer when I waslearning about Mosaic ML and theircomposer and they have all thesedifferent regularizers that you you knowpackage into a recipe to train yourmodel with like so I know a little bitabout pruning you zero out some weightsI know there's like iterative pruningmaybe second order pruning is alsosomething you do is what is the recipefor the esparzanoyeah soum yeah recipes in sparse smell can canhold a lot of information it it youdon't have to use them for pruning oroptimization in general they have verysimple building blocks with them simplethings like learning rate modifiersum so how you want to modulate yourlearning rate over time or introduceum ways to boost accuracy likedistillation from teacher models or adense upstream or even dense Downstreammodelsum these are all things that helpimprove the accuracy improve therobustness of training your modelum and so so yeah really a recipe isjust uh that the base thing to it is youdefine a start Epoch and a end Epoch andthen you list out all of the themodifiers all these littleum uh modules essentially you want tohook into your training pipelineum over or you know over the trainingtime of the modelum so yeah and we focus a lot on Pitorch we have some Integrations withKeras and tensorflow as wellum but yeah it's totally mentioned thatyou know you can make a sparse smell uhmanager essentially and just hook thisin as a callback function into yourtraining framework of choice and and itcan start um you know overriding thingslike your learning rateum uh all sorts of things like thisother Integrations as well as uploadingyour experiments uh automatically toweights and biases and and comment uhum tensorboardcommon uh common ways to for ML Ops toview the status of your experiments anduh you know early stop them uh continuethem back up from checkpointsum things like this you know made madefor the ml practitioners but also allwrapped up andcommand line scripts so thatum uh you know data scientists and datahackers can also get started on justtrying a new data set trying a newexperiment they have in mindsuper interesting and I yeah the wholethe way that this enables sparse modelsand getting away from needing thesecrazy expensive dense models and towardsthis more exploiting the inherentsparsity of neural networks to make thismore economical run on CPUs Etc aboutcontinuing in sort of the Practicalstack so now we have our sparse modeland we want to evaluate it and say howmuch faster it is the things like saymoving the data to the GPU or all thesethings what goes into benchmarking theseinference accelerations and saying thisis faster than pytorch be you knowyeah yeah great question it's and it'sreally difficult to it's unfortunatelypretty difficult to Benchmark across allthese different types of hardware andFrameworks they all want to do it indifferent ways but we try to abstractthis away as much as possible especiallyon since most of our models are based inpi torch back endsum we we put a lot of this work intosparse ml you know in addition to thevalidation process also measuring howfast are we validating the results youknow what what's our iterations persecond things like this and so thisallows us to measure at least you knowsome CPU and GPU results on Pi torch andmeasure that butum you know most of our benchmarks arecomparingum CPU how much faster can we make CPUsand so from that perspective we like torun and compare against other uh enginesthat were built for inference such asOnyx runtime or openvinoum that uh and and that's part of ourprocess of actually getting to inferencetime so once you you've trained a modelor taken a model from sparse or sparsemlin in your framework of choice weusually export that model into an openuh model format called Onyx and this isjust a graph representation where eachnode in the graph it's a differentoperations such as convolution matrixmultiplication relu pooling things likethis so maybe just your one multi-headattention block or Transformers isbroken into 20 different nodes that areall connecteduh but the key thing this is a an openformat you know uh uh spearheaded byMicrosoft Facebook Googleum so all the Frameworks can export intothis and and can even uh take Onyx in asa as a a as an input uh back in and sothat's the main thing that the Deepsparse engine takes and uh usually oncewe get this Onyx then we can bring thisin to all of the other inference enginesas well and start to measure theirperformanceum and so yeah getting into the detailsof performance and and what are thedifferent benchmarks that can reallycover accurately the the realisticscenarios that you're going to use onceyou once you deploy models there'sthere's a couple major ones soum the first sort of obvious one is whenyou have just a single image or a singleyou know query coming into the model youwant to run this as fast as possible andyou want to get that result back to feedinto your pipeline and we call this asynchronous or a latency based scenarioso we're talking about batch size oneand you know a single image a singlesentence or set of tokensum representing one requestand our goal there is to see how fastcan the whole system or section of thesystem uh uh return this result back toyou so this is really common and a partwhere actually gpus uh or Hardwareaccelerators in general struggle a lotas they usually can't access the memoryon the CPU which is where this resultusually starts from in terms of you knowyou are are getting a an HTTP requestsomeone's giving you an image and askingfor a result that networking requestlands on the cpu's memory and then nowif you want to run it on a GPU you needto pass that image over pcie expressonto the GPU copying onto its memory itthen executes on that then you need tocopy the result back over PCI Expressback onto the CPU memory it can puttogether the network response and thensend that back out so when you're reallytalking about a handful of millisecondsper request and you you don't want theml model to be the bottleneck in yourapplication pipelinethis is where latency is a reallyimportant scenario to look deeply intoum so that's that's a basic one we dothere's also a throughput or batchedscenarios where say you're working on anoffline task say you have you know amillion things you want to embed beforethe end of the night the question isjust how quick or how cost effective canyou can you run through this and so hereyou find a good batch size that can fitwithin your memory available or utilizesthe system well and then you distributeall these batches across your availableHardware as fast as possible and so thisallows you to amortize some of thecommunication costs you still have a lotof data to move around but it should bea place where you're really Bound byumcompute as long as everything fits intomemoryand so this is pretty good for offlinetasks and then another example that wehear a lot about from our partners andwhat they're interested isum is sort of a model server or amulti-stream asynchronous caseand so think of it just like a webserver uh and and we uh make it veryeasy to start up model servers so justthink of we're presenting a model to theworldum as a restful endpoint we commonly usefast apis this is just a super easy touse thing that you can just ping its docendpoint get information about whatformat your your image or tensor needsto be in send it over and you you alsohave a schema to tell you how to use theoutputand so in this case you know you'reserving multiple users at a time likeyou would at a web server and eachrequest that is coming into your serveryou need to dispatch this efficientlyacross your Hardware you don't want eachof these responses running into eachother and causing each other to slowdown and you know the goal is to serveas many users as possible and manyrequests as possible while hitting youknow the latency requirement that youhave for your application and your usersum so this is something we spend a lotof time on as this is where a lot ofapplication deployments end up in termsof you know either it's it's sort oflatency or offline in terms of makingvery small you know vcpu or Cloudinstances so this works well if you'regoing in say a kubernetes cluster andyou you know maybe like each node isworking on a single inference but oftenit can be a lot more cost efficient tojust have a single larger node that thismodel server is hosting up a model oreven multiple models on one nodeand uh just depending on the requestcoming in dispatching those models andthere's requests across this singlesystemum which can really help drive down costand also make your applicationcomplexityum or you're really really your uh yourml infrastructure much simpler sohope that was helpful those are sort ofthe major three scenariosyeah it's incredible the uh the nodeserver and how you can have each nodegoing in so I have my question to kindof build my understanding of this andI'm sorry to listeners if this is a dumbquestion but it is it like you wouldwant to do one by one inferences on theCPUs compared to where the GPU I thinkwhat you you batch it up into say like64 predictions at a time because it'sthis big GPU so you like you know take alot of data at once is it with the CPUthinking say you have like 16 cores isit like inference inference inferencelike like a multi-threaded programmingway of thinking about inferenceum yeah definitely in the multi-streamcase it can be really it can be usuallypretty efficient to uh you know havethese like multiple streams where eachone of these like batch size oneinferences are pinned to individualcoresum to really help so it's kind of likean embarrassingly parallel problem eachcore is working on different problemsumbut you know in terms of uh getting thebest possible performance this is whereit's really in your best interest to usean inference engine that's designed forCPUs like deep sparse where we're awareof especially the the unique topologythat Hardware topology that CPUs have interms of having multiple levels of cashlayer one layer two layer three Cashsometimes even a layer four cache andyour main memory of course and uhparticularly with these sparse uhquantized models you can for instance onthese large models we've producedyou know uncompressed as a dense fp32model they're commonly at one or 1.2gigabytes on disk and even more inmemory and once we've sparsified theseuh large models we've been able to lowerthem byum at least an order of magnitudegetting them down under 100 megabytesfor instance once sparse quantized andsoum this now uh is really interesting aslike oh maybe your L3 cache on the CPUis 70 megabytes so maybe we can fit youknow half of the network just in the L3cache of this section of cores and solet's just have this section of coursewith this L3 cache just running thismodel and so all inferences are going togo through this section of cores now wedon't require you to have to have thissophisticated understanding of thehardware you're running on but these aresome of the optimizations we think aboutwhen you're building your applicationwith our we with our engine and ourespecially our model server that canintelligently make these decisions basedon the hardware available and and themodels you provided as well compilingthe model down to fit the hardwareavailable so it's a really interestingoptimization problem and we have a lotof systems and math Engineers that workhere who find that problem interestingas wellum but uhbut yeah generally you know CPUs can seea benefit from throughput as well it'snot that it's just completely lost onthem you you get more throughput andgenerally the scaling is better as youbecome more compute bound obviouslybecause we're removing you know 90 ofthe compute from the model when we pruneit to 90 percent uhso it really is about efficiently usingall subsystems of the hardware from thecompute you have available to the memoryto the networkingum and and even the the larger mlpipeline around the modelum you know we we offer thisum uh you know similar to Transformersthis idea of pipelines that um will wrapthe model around pre-processing andpost-processing that's neededum both for ease of use but also withgood defaults for Speed soum you can just host up a whole pipelineinstead of hosting a model and you canjust feed that straight text instead ofneeding which tokenizer do I need inorder to tokenize the words into theembeddings and then provide these tokensto the modelum so that you can just send your textand get your sentiment out of the modelum directly without needing to know allthe nitty-gritty of ml pipelines and howto do it in the right way efficient forthe hardwareso so it is is really about bottlenecksat every single layer both inside andoutside of the modelyeah those abstractions of pipelines areso useful like from sentence Transformerimport sentence Transformer and modelDot and code and then just the stringand then you've got the vector and thatabstraction having that similar thing umI really want to ask about from thebeginning uh the CPU cache hierarchy andsort of generally the CPU what what itoffers but uh quicklyum so to access is you know running itfrom pip install deep sparse from Deepsparse import pipeline is and then isthat the best way or is there say like aDocker container uh that would put thisinference engine I don't know thesethings too well but like what's thedifference between you know running inand python or say is there like somecontainer I think the question to askyeah yeah yeah no it yeah totallyunderstoodum yeah I mean I think definitely fordevelopers that like to get their handsNitty Grittyum understand the the ux of the the codeitself and uh understand the code behindour benchmarks as wellum you know it's in your best interestyeah pip install it run it locally usethe pipelines use the models without thepipelinesum and and and read all the code that'son the Deep Source GitHubum now yeah for people that are morethinking about how can the scaleum in my infrastructureum particularly things like kubernetesyeah we we offerum Docker containers you know mostcommonlyum yeah we have this deep sparse serverthat is that the docker containersusually built around and then it's justthe case of what config file do you givethat Docker container and the configfileum just defines yeah what model do wewant to use what's the pipeline thatit's using uh you know what IP addressand port or what port should I host iton simple things like this how manystreams should I work on how many coresshould I use uh this is all sort of in atext file that you can provideum but yeah we actually have someexamples uh up to date on uh sagemakerand we're working on ones for AWS Lambdaand Cloud run to to show you examples uhor even on just you know localkubernetes examples I think we're doingthe mini Cube as wellum to scale across eks or gks so thatyeah you can you can just treat yourdocument container as anotherumyeah web application that uh you knowyou have this uh restful API intoum and so it's just the case of how manypods do I put this Docker container onput a load balancer over it and buildyour application without worrying aboutum uh without worrying about scale it'sjust managed for youum and I think that's really interestinguh just in the case that you know we arerunning on CPUs we can run on vcpus atthe smallest level because we're justlooking at what instructions areavailable you don't need to set up anyfancy driversum like Cuda or special environmentsum it's really just running on Linuxlooking at the CPU informationand uh you know able to scale down allthe way down to a a task uh or a pod ona kubernetes cluster all the way up toyou know uh uh bare metal multi-socketservers that are you know uh uh uhhosting 20 models at a timeyeah super exciting I think the likeright now the way I've plugged in neuralmagic to aviate is through our text toVEC Transformers container you justreplaced the docker image with one thathas the neuromagic pipeline and uh yeahit's exciting because I think there'sstill even more optimizations we can doto get this like even faster and so sokind of the next topic I think we'vekind of really covered the um the likehow to use it the practical applicationand uh so I was listening to uh nearshavid on Jana kilcher's podcast tacoand it seems like a whole reinvention ofthe model architecture like things likesay you can access a terabyte of memoryand then the human brain is sparse I cantell me about like this kind of visionof like like sparsity is it is it morethan just like layer by layer one quotefrom the podcast that stood out to me issomething around thinking yeah likegenerally like this thinking of likelayer one layer two layer three thatthere's some other way maybe it's like apath that doesn't wait for this one tofinish other ways of traversing thisgraph do you think about this kind ofthingyeah yeah I mean we think about thisthing internallyum obviously you know from ourbackground uh you know from MIT you knowco-founders near shuvit professor at MITreally accredited wonderful individualum and and his post-doc uh Alex uh Matamatievum that uh that they co-founded thiscompany together you know they wereworking in the in a connectomics labtogether and uh working with brainresearchers as the computer sciencepeople to help them and yeah they sortof from the problem they were working onthey were trying to image and or patternout brains and these are obviouslysparse architectures so they came upwith sparse algorithms to map onto thebrain and soum yeah totally you know the the roadwe're going down in terms of uh or theroad we are down uh in terms of trainingmodels and deploying them very muchworks on the idea of having dense layersfeeding into dense layers larger andlarger modelsum that are at fully connected layersum and like I said this works wellbecause we found a golden bullet thatworks for now stochastic gradientdescent you just throw more data at ityou regularize it enough you showedenough data at a time and and it canjust continue and continue to keeplearningum but that doesn't mean that it is thebest wayum uh and I think like in the futureum you know this the scaling especiallyas Moore's Lawum you know struggles to continueforwardum we need to find more efficient waysof training and running these modelsum and and this is what we hear exactlyfrom our customer engagements they a lotof times they have models running inproduction or they're gearing up andthen they're getting some load andthey're realizing yeah this doesn't makesense uh but the value is there fortheir uh for the people using it it'sjust too expensive operationally to runall these modelsum and that's because most the timethey're just training the model on theGPU and then they're like well it'saccurate uh how do we run it and thenthey go run it on the same GPU that theythat they trained onum and you know haven't taken thethought of from training to inferencelet's have a step of optimization inthere and that's really what we're whatwe're um working on now introducing thatoptimization step to more and morepeopleum now of course like you probablyshouldn't be training dense models inthe first place butum it'll take a lot of research andprogress in science to get thereum but I'm sure we we can get thereum you know purely from the idea yeahthe brain is a is equivalent to a cellphone of compute with a petaflop ofmemoryum it's it's not the other way aroundwhere you know we we don't have exitflops of compute in our brainsum with it with a you know yeahcertainly we don't have teraflops ofcompute with just you know 16 gigabytesof memory attachedum we we have a lot more memoryum and and access it really efficientlyso that uh we don't need to eat all thetimeum so so yeah that that's that'sdefinitely how we think about itum but also you know just in case oftalking about this asynchronousexecutionum that that we've built in our engineas uh deep tensor columnsum it's not like we're only like we'rethe only ones working on thisum you know a great example actually isthe uh the really famous Google paperum from I think about a year agoPathwaysum or even their sparse Transformerspaper before that where they train youknow I think a trillion parameter modeleven larger than gpt3um but the key thing was based on theinput coming in you know you would havesay multiple uh multi-head attention orfully connected layers multipleessentially experts you could choosefrom and based on the input coming in itwould route it through a differentpipeline so you have you know 100 timesmore parametersthan than you would actually use foreach individual inference but you'resampling them over time for differentinferencesum so this is obviously at a verystructural large scalebut uh it you know it makes sense thisis what the brain does we're notaccessing we're not running all of theweight setting parameters and neuronsand synapses in our brain every secondit's localized at certain pieces that weneed to run every single millisecond orpieces that we need to run we need toremember a nice memory or something likethatso so yeah it's it's really interestingyeah that's super interesting I guess Iwas thinking of that yeah those twoangles where one it's like you'rerealizing the lottery ticket hypothesiswhere you know you theoretically cantrain this bar sub network from scratchand having this quantization awaretraining where you're pruning duringtraining is like getting closer to thatkind of idea where it's like be mindfulof the sparsity during training and thenlike this Pathways thing which is like atotal rethink of how deep neuralnetworks are gonna I'm sorry I'm gonnaprocess datato make their predictions um so oneother uh thing that I think is reallyinteresting is uh Edge applications likepredictions at the edge putting it inphones or embedded sensors things likethis can you tell me about like yourmotivation around Edge applications saylike maybe robotics or like apps that Iguess don't have to network like whatwhat's the thinking around Edge caseapplications of deep learningyeah so uh you know the most obviousthing is power you want to keep power onthese things as little as possibleum so it's obvious we want to make themachines as efficient as possibleum while also maybe paying the leastamount to get them on there butum in addition to that there's also thecost of of the networking uh of thebandwidth you payum and uh you know we've heard this alotum from from retail and Manufacturingum in that uh you know they they don'twant to send uh you know 200 camerasfrom every store out to AWS they don'twant to pay the Ingress costs every dayum and it would be much better if theycould at leastum uh you know just send out the highlevel analytics every day instead ofdoing to process all of that in a datacenter you know hundreds of miles awayand also use that bandwidth of the storethat their their you know employees andcustomers would would like to have goodWi-Fi atum so it uhyeah it really is like a problem uh aproblem with more problems at everylevelum but yeah I mean it's really from thecase like you know you think aboutum a camera or say like a you know acamera running uh at your front door atyour doorbell for uh you know to look atthe delivery driver orum you have to essentially replace yourdoorbell many people have theseum and they wouldn't really like everysingle thing being shipped out toum uh to the people that make themum and so each one of those cameras Ibet you a hundred percent has a CPUattached to themum maybe with some you know videodecoding stuff next to it but the CPUhas to be communicating that thing andso if we can make the models and thingsum that are that are useful for thisdata small enough and compressed enoughbut still accurate enough to run onthese devicesum at the edge and then the cameradoesn't communicate with anything eveneven on your local Wi-Fi using thatbandwidthum uh even outside of external bandwidthum unless there's a warning or somethingserious that should be looked at orevery once in a whileum that's just a better experience foreveryone involved and and plus the thebox that you haveum plugged into your wallet integratedinto your Mainframe you don't need tohave the compute there have the powerthere have it be noisy and uhum and things there so definitelywe hear the asks every day um and that'swhy obviously a lot of people areinterested in what we have to doum so yeah it's really just aboutumfiguring out the whole pipeline Beyondjust optimizing the model to make iteasy to build these applications fitthem into smaller devices that's part ofthe reason why you know the Deep Sourceengine we focused on x86 CPUs to startwith so made by Intel and AMD becausethose especially were the most commonfour years ago uh uh which we thecompany was founded atbut now arm is is running everywhereboth in the cloud on the edge on thisMacBook I'm talking to you right nowfromum so so we're working on arm supportright now and hoping to have an alpha bythe end of this uh this year andhopefully a demo at neurops in Decemberwhere we'll be presentingum so uh so so yeah looking forward togetting sparse networks onto more typesof hardwareyeah that's a super interesting exampleof the um the doorbell like I think itdetects the package deliveriesand you might not necessarily want tohave a doorbell of your front door likestreaming to the cloud service all thetime and all this motivation around saylike Federated learning and that kind ofthinking and I think it's interestingtoo with with we did if you want to havelike a vector-based approach to thiswhere uh say it's a security camera andit's kind of like doing outlierdetection so it has the you know ittakes an image of the of the front ofyour yard vectorizes it on the CPU withsomething like neuromagic and then itbuilds up this Vector index to do theoutlier detection I think it's veryinteresting and also say like thisvision of like uh the Tesla bot likewhere you can have a household robotthat you get I think I think a lot otherthan Tesla I think a lot of startupsbelieve in this kind of thinking thatwe're going to have like a five footeight uh robot that you say uh make mesome eggs and then it will go make eggsand similarly in that case I couldimagine that you'd want it to not dothis kind of bandwidth uh sending andhave it be like yeah you don't want ittaking up a a person's worth of uhsomeone streaming Netflix on your Wi-Ficonstantly righthook it all up yeahcertainly so yeah we've heard that aswellyeah awesome well thank you so muchMichael I you know I think thetechnology of neuromagic the way it fitswith weeviate is so exciting and youknow sparsity the visions of this thesparse Zoo sparse ml all of it is justso captivating and thank you so much foruh doing the weave podcastyeah thanks so much for having me andit's been great talking to you and andlearning from you over the time reallyexcited to uh yeah learn more about theuse cases for Vector search and uhcontribute some some cool demos", "type": "Video", "name": "Michael Goin on Neural Magic - Weaviate Podcast #27", "path": "", "link": "https://www.youtube.com/watch?v=leGgjIQkVYo", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}