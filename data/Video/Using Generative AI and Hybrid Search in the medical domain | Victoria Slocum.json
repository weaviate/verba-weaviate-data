{"text": "In the AI Renaissance, a period marked by innovation for the good of humanity, medicine holds so much possibility for progress. \nhi everyone my talk tonight is titled uh using generative Ai and hybrid search in the uh to augment Medical Data pipelines um you know data has been become such a big part of our industry and our lives right now and obviously the medical field is no exception to that and AI generally has such a big potential to really help us understand and make use of this data um but before we go into that the theme of this Meetup is the AI Renaissance which I personally love not just because of the Aesthetics um the Renaissance was an explosion of innovation in art and medicine and Science and it's often called The Rebirth of humanity and you know obviously our views of the past are a bit skewed but the Renaissance As we see it was really this focus on creating something more human for the better and I I really truly love this idea you know there's so much potential with AI today to create possibilities for Change and for good and for Innovation and I I'm truly excited to see what we can build with the tools out there today today um so hi everyone Philip introduced me a bit but I'm Victoria I'm currently a machine learning engineer at we8 on the developer growth team led by the one and only amazing Phillip um and uh academically I studied and did some research in the biomedical engineering Fields before eventually getting my degree in linguistics and taught myself programming somewhere along the way there um and I started working in explosion and fell in love with NLP and the tech industry and then I eventually moved to we8 and fell love with vectors and the tech industry um and now I'm here so this image is actually made by chat GPT I gave it a picture of me and said reimagine myself in the AI Renaissance I think it made me a bit prettier than I am but you know what it goes so um in the next 20 minutes there are three main things I'm going to talk about so firstly I'm going to go talk about AI native tools and applications and where this is a bit different from traditional structures that we see often um and where AI native tools can really make a different difference so secondly I'm going to take a t deeper technical dive into three features of we8 um so the modular system generative search and hybrid search uh and then thirdly we're going to take those three concepts and look at how they might provide actual real world value against some AI medical use cases and of course a demo of a totally cool application I'll search that Edward built um but before we talk about AI I wanted to give a basis into traditional data management so data is important obviously but how you store that data is arguably even more important so traditional data structures like spreadsheets are often called structured data because the data stored in tables and list often has a very structured format um but unstructured data like lots of text in documents PDFs videos images audio it's a lot harder to organize into these structured table formats like a spreadsheet and then even harder to use to provide value in real life use cases so Vector databases are a way to organize this information in a way that makes search and retrieval easier for types of AI use cases so like retrieval augmented generation or semantic search all these hype wordss that we're seeing around everywhere um and by turning these datas into Vector vectors we can apply this relational structure into the storage space allowing us to use and build applicational useful pipelines with the data so weeva is an open source AI native Vector database um and that sounds a bit fancy maybe a bit marketing a but what it really means is that we're built from the ground up with AI applications in mind specifically tuned across all of our features to handle things like generative AI scalable database database security and all those us features that allow users to handle large amounts of unstructured data and also handled the specific types of use cases that come with using this data so I can give you so many examples of all of this um like our hssw Vector index that's custom built that doesn't have the limitations present in some other open- source Vector index techniques or our AI ecosystem Integrations everything from LL index coher open AI blah blah blah super easily exchangeable within our database um to our Enterprise ready features like multi- tendency for security and isolation product quantization for memory saving node replication for zero time time processing um so being AI native is not just something we say to get a green check mark we've really thought about the best ways to implement all of our features in order to make sure that our customers can build applications easily and at scale without sacrificing on customizability or functionality so looking at the list of all the databases out there kind of feels like when I was young and I went to a garage and I found a toolbox with like 20 million wrenches of different sizes in it and I was like what do you need all these wrenches for but when you start to look at the things you're trying to build you're going to realize that different applications require a different set of tools and using the wrong set of tools will be at best and efficient and most likely just not work entirely and it's not that hard should built a cool prototype nowadays there there are functionalities when tools that make this easier such as like a modular architecture for fast iteration and testing but where a lot of the struggle comes from where we see a lot of our customers struggling is moving an app from a prototype setting into a production setting and now you're dealing with much bigger much harder challenges like scaling and security and monitoring um all the boring stuff so now not only do you have to find a tool that can help you easily build and iterate on your initial idea but it's also important that it can carry you into a production level setting and this is where AI native Vector databases pay play a huge part in building AI Nate application they're the right size wrench because they were specifically designed to be that so there's also this idea of ease of use versus customizability that I'm kind of in love with um if you take either one of these ideas on their own they're not that hard to design a tool around right if you tune a tool specifically for ease of use it's going to come with a set of preset out of bun functionality where the users can hit the ground running with something that automatically Works however most cases you need some level of customizability um so there's not really a good one- siiz fits-all solution however too much customizability and you're giving users a bunch of parts and a long and confusing instruction manual and that's going to take a very long time to set up and test where the true magic comes in is really balancing these two having an out-of-the-box solution that can make prototyping and testing easy while also still providing control over the defaults to further customize it to your specific use case or even level production and this is where the modular system is super valuable so it really allows you to have this level of ready to use out of the box functionality while still making different parts interchangeable and customizable so our vectorizer models are an example of this we're actually Vector agnostic basically so there's kind of three different options for vectorization so you can upload your own vectors um if you wanted to use your own model and didn't want to give us any of your rawad data um you just upload the vectors and that's it you can use an open source vectorization model from hugging face for example that runs directly alongside your um data and downloads and vectorizes things or you can use one of our built-in provider Integrations like AWS go here open AI blah blah blah there's a long list um basically we don't care how you do it we just want the results um and this is really made all possible by having a modular system it really allows a clear set of easy to use out of the boox functionality defaults while still being completely customizable it allows you to easily build that first iteration of a prototype I don't know using a lower cost model and then further update and customize it as your use case grows maybe moving to a more advanced or specifically trained model and because we integrate so well into the AI ecosystem these are all super easy to exchange so in we you can also interchange different search types depending on your needs so one of the most powerful search types is hybrid search um which is the combination of both Vector search and keyword search um Vector search has created this explosion of exciting new possibilities nowadays by being able to understand the meaning behind a user's query and match it to documents that have similar meaning a lot of the challenges with traditional search was overcome um there aren't the limitations with needing the exact right keywords in the query or even things like typos or and allows us to better handle AI use cases like chat Bots and stuff um but keyword search still has some of his strengths sometimes you really care about that exact match and the response and it can still better handle things like short queries technical Jaron or queries outside his training data so hybrid search is cool because by combining both Vector search and keyword search into one solution you kind of get rid of all the drawbacks of either system and maintain all the benefits so in we8 both Vector search and hybrid keyword search are run simultanously behind the Hood um and then the best results from both are combined using a fusion algorithm so if Vector search were to fail not return the best results keyword search would be able to pick up the best results um and vice versa so this means you don't need to know the magic perfect words to get the best response but you can still get some of the benefits from both systems okay and then in weate this is a vector search query watch here and then that's a keyword search query that's a hybrid search query so there's one thing changing in between these and it's just that little word there so there's it's all built in and interchangeable there's there's no building your own system there's no additional code and implementing this requires changing literally that one word in the query um and yet still if you required further customizability you can add things like a alpha parameter to hybrid search to weight it more towards keyword or vector search or change the type of fusion algorithm because it's possible by the modular system okay so generative search it takes searching even a bit further than hybrid search um so you may have heard of rag retrieval augmented generation as an example of generative AI um this example pipeline of that so um in this pipeline you have a set of documents in your vector database already and then you'll take the user's query and use that to search through your set of documents and return the most relevant results which then goes into your context um so then those documents in the query are synthesized together in a prompt and then those are given to the large language model to generate then a response for the user um there's lots of cool things this can do over just using like an outof the-box llm like chat GPT such as like reducing hallucinations up in context or providing specific information that you want the answered to so normally the code you'd write for the vector database quering part is that and then you have two other functions for those things but the cool thing about we8 is it's a Genera search module which means this is all done in a single query so all the additional code you have to write is reduced into one query and suddenly three different functions in one helping developers write less code um so how are these features like hybrid search and generative search actually going to help out there in the real world so I was watching this great webinar by my colleague Byron and um Travis from innovative solutions and they were discuss discussing different applications of AI native Vector database and Travis gave a super cool example of a situation they had with a customer and how they solved it um I'm going to be paraphrasing here but basically the problem starts with the doctor's off office experience often being super frustrating so the patient usually has to fill out this really long like 20page form about your medical history your current symptoms whatever else is on there and then they go into the appointment and they sit down with the doctor and sometimes they ask a lot of the same questions you just fill it out in the form um frustrating and on the doctor's side obviously they see a lot of patients a day they don't have time to go through that 20 Page form for every patient um so it kind of creates this lose lose situation the patient still needs to to fill out the document for the medical records the doctor doesn't have time to review any of that data and the data isn't really being used for anything it's just left so how exactly did they solve this problem well first they set a vector database weate and filled it with information about different ailments specific to that type of office so the entries would include things like the ailment the symptoms of the ailment and different profiles of patients that had come in and been treated with the same ailment before then they took in the new patient information and queried it through the vector database which then gave a response of a list of suggested ailments that might be wrong with the patient then using the generative search module they were able to take the patient information and the suggested ailments give it to a large language model and have it synthesize kind of a doctor cheat sheet just a few lines about the basic patient information the suggested ailments they should review with the patient and it reduced the doctor doctor's time to read and then the data from the patient was actually being used so this situation kind of avoids all the redundancy of having data and then not using it and the doctor can really then focus on providing a more human approach you know filling in the missing information expanding on things that aren't clear and giving the best diagnosis for the patient okay so demo time um so we or we Philip gave a little teaser into um hell search but this is Hell search it's a super cool app and as Philip said there's a bunch of supplements um ingested into a WEA instance in the back end uh and I'd like to point out not Health advice just a demo um but it could be used I'll explain that later so let's let's just type in one of the default queries here so help for for joint pain and here it's going to generate and the first thing it's going to do is it's going to use an llm to generate a graphql query so you can see here this is just the regular getter products query and it's using hybrid search which means it's using that combination of vector and keyword search here and then we're also querying with a generative search so still using the hybrid query but then we have an additional task for the llm where it's summarizing the products based on this query right so um here we have this overall generated product summary and then here we for each individual product we also have a generated review summary so this product is highly recommended for mild Joy pains great thank you and then if you wanted more information you can also click into the product and get all the reviews and what's cool about this is it's actually highlighting what it's picking up here so sore muscles and aching bones aching bone bones joint pain very similar in our human Minds not similar to a keyword algor Alm but thanks to Vector search we can actually understand that as similar so that's pretty cool um and what I think is really going to help with this um is allowing users allowing patients to have more access to information you know there's a lot of information out there in the world um and sometimes it can be hard to find it and then when you do find it sometimes it can be too overwhelming to actually do anything with and I think generative search and stuff like this can really help us better ingest and better manage our information which is pretty cool okay so there's only going to continue to be more data more information on the internet more reviews to sift through more products to look at and more patients to attend just more so making sure we make informed decisions and take advantage of efficient time management is so important but it's only going to get harder and this is where I'll end with a broader point which is AI really has a dual role in Information Management so AI can definitely be a part of the problem you know spreading misinformation hallucinating being used to manipulate to harm embodying stereotypes based off its training but I also think it can really be used to improve certain aspects of our life making things clearer faster simpler and we can use it as a tool to empower our choices augment our understandings and allow us to make more complete decisions based on accessible and understandable information with our limited time but above all I think it can be really best about taking away robotic manual tasks and allowing us to have more resources spend Being Human for ourselves and for each other so I really like learning about stuff and rambling about it so uh if you're at all interested in technical content or have any questions or want to tell me where you're building connect draw me message I'd love to hear it and uh thank you so much question oh wait a second questions hybrid search traditional text there wait a second sorry with the generative queries from weeva do you have control over which model version you're using in yes for sure was the answer to that for sure yes you can use coh here hugging face open AI we have two so which model provider you can use for hybrid search or for w so there are two separations for w for example you have a part which is doing the you um create the embeddings so there you can choose an open source model to generate uh the embeddings for example gor has a nice embedding model um then for the generation you can also use an open source model like an arama llama architecture yeah so it's the same sort of module system it's just plug in what you want provide the key and then it works yeah how much control have you got over the actual llm that is it is it quite yeah so verba if you're interested in that verba uses uh the generative search and we so kind familiar yeah I haven't I haven't really used the generative bit of it yet yeah so verba has options for like gb4 GPT 3.5 um coh here and something else sentence Transformers fing face yes yeah so it's just like uh even in the front end we have it so you can set it which is super cool and it's super easy to plug in in genome model but only for creating the embeddings thank you hi thanks uh question is um how much control do I have over the uh or the selection and the ranking of the uh of the of the results uh within the generative search good question yeah so we have a ton of also reranking models um so it's also in the module system which means you can plug in all any reranking module in our reranking module yeah so um is currently working on a ranking mod yeah Erica is doing a lot of good content around this but also full control and it's just about what's coming in and what's coming out needs to match and then it's good um but there's also or several features in weade which making it easy for you for example to combine list from traditional text search and Vector search um autoc cut um and generating these kind of rankings or lists and rerank them depending on waight inance yeah yes here's another question then um do you offer keyword search or sparse Vector search or both so we just offer keyword wait a second sparse Vector search is not really not just kidding so we offer keyword search based on an inverted index we're currently doing some research into providing a spar Vector thing but we don't have it available yet and we're kind of weighing the tradeoffs now you already had a question we we now need to be a bit picky hi there uh getting back to the llms um how about self-served uh models for sure yeah it's it's available for you to yeah if you have it available you just pass in maybe you see me after and I can look it up for you I think probably you would add like a file path or something but I can find it in the docs for you pretty easily so you can run it in a VPC and AWS in Google um it's available over over the market store make Market store and you can I mean serving your own model in the end you need to take care about the infrastructure or you're having own distributed server architecture and running fast API as an endpoint and fast API endpoint is serving the language model more yeah I would say two more questions and then we need to move on hi uh would it be possible as a user to sort of do a take out of my data and bring into to somebody else using this database um yes it's definitely possible because you're talking about Pine count they are storing you so one more question a long way it's worth it oh great hi Emma thank you uh I'm just curious if you guys spend any time evaluating different methods of search and if so how you do that uh you mean like keyword versus Vector versus hybrid yeah like in machine learning you would always have an e set right just to see like if what you're do works so do you do similar things when you're comparing different methods for for Vector search or keyword search Etc yeah we currently don't have any like public benchs coming soon but um we do do a lot of work internally to evaluate these systems and we're also constantly working on improving different parts of it right yes cool thank you and Leon is here she's she had worked on um compare or evaluating and benchmarking retrieval techniques so this is also related because in the end this is some kind of a ranking you're getting back and several other metrics she's here later on okay that was the last question for this talk because we want to hand over ", "type": "Video", "name": "Using Generative AI and Hybrid Search in the medical domain | Victoria Slocum", "path": "", "link": "https://www.youtube.com/watch?v=TGfPNVYkix8", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}