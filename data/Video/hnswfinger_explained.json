{"text": "Hey everyone! I'm super excited to present a paper summary of HNSW-FINGER! HNSW-FINGER presents a clever technique to ... \nhey everyone thank you so much for watching this paper summary video of hsw finger finger is short for fast inference for graph based approximate nearest neighbor search the authors present a super clever technique that extends the hnsw algorithm perfectly by approximating distance calculations so we're going to approximate these distance calculations by Computing a lot of values offline using these cool algorithms like singular value decomposition all sorts of details that we'll explore in this paper summary video but to give more of a tldr hnsw finger trades off additional memory for faster Vector search so when using this algorithm that's the trade-off basically so we'll get into exactly how much memory and exactly how much of a speed up you get in terms of the evaluation you're especially going to see the benefit of hsw finger when you're dealing with you know higher dimensional vectors so they find the biggest Improvement on just 960 dimensional vectors but you know say just at the time of recording this the open AI Ada 2 embeddings are super commonly used and they are 1500 dimensional vectors so you can expect a huge speed up on using higher dimensional vectors like that with an algorithm like this and all that will be evident as we step into the math so I had so much fun reading this paper and I hope this video helps you you know understand the math as well it really helped my intuition on approxine years neighbor search as we look into the like the hsw stop condition and all this cool stuff so let's dive into it thanks so much for watching okay so here's a quick overview of the algorithm in two minutes just in case you want to get the high level concept behind it so the core value out of vector databases is approximate nearest neighbor search we take a query we turn it into a vector and we want to find the most similar Vector to our query in our database of vectorized documents we don't want to have to compare the distance between our query and every single Vector in our database so we build up these data structures to facilitate approximate nearest neighbor search so shown on the left is the hnsw hierarchical proximity graph to Route these distance calculations such that instead of say we have you know a billion vectors in our database instead of doing a billion distance calculations we do say you know ten thousand or so or however many but so that's the core idea as we build up these data structures So within hnsw we have the search layer algorithm for how we're going to Traverse the graph and we have this upper bound between the distance of the candidate and then the neighbor that we're exploring and so we use those distances for how we you know append it into the queue or terminate the search so the authors find that over 80 percent of these distances here when we're exploring these neighbors towards the mid phase of the search they usually don't contribute anything to the actual end result so we can get away with approximating these distances so the key insight for how to approximate these distances is we're going to project the query and the neighboring node onto the center node so again the way that hsw search works is we find some Center node like say this blue node shown in my mouse and you're exploring the neighbors of that blue note so this blue note is the center node C so we project the query onto that Center node as well as its neighbors now the cool thing is that we can then compute the projection of the neighbors onto the center node offline and store these offline and then we you just look it up and quickly calculate the distance so that's done by decomposing L2 distance between the query and the neighbor into the separating into the projected and residual components so you know this is this is the key detail that we'll be exploring in this paper summary video is understanding how each of these four terms are computed this last one the uh you know the angle between the residual uh components that's the key meat behind it but you compute these most of it is stored offline and we'll get into the memory overhead but this is the key behind how it works uh so there's also this clever technique of using a distribution matching where you look at the you know the distribution of the angles between the real data and then once you have the low rank projection what the angle then becomes and doing this kind of normalization thing it's a super common thing to see in deep learning so I think it's just cool to see this in general so I I maybe should mention that we measure this angle of the distances by uh with the data distribution so we you know have the SVD a low rank projection by looking at the actual uh residuals in the distances all this will be clear from the paper summary video but just to give an overview we use the data distribution to help with this approximation so then finally with the results I think looking at this just 960 is the most interesting thing you see a huge benefit in queries per second at different recall levels compared to just Baseline implementations of hsw the benefit being not not as big on Lower dimensional vectors 96 100 128 but these higher dimensional vectors I don't really know too much about like the information retrieval benchmarks and how those react to higher dimensional vectors generally but you know as I mentioned in the beginning of the video the open AI ada2 embeddings are 1536 so makes it seems that they would bet that would benefit from this enormously so let's dive into the details further this video will explain the technical details behind the hnsw finger algorithm to give a quick overview of the presentation we're going to start off by exploring the hsw search algorithm and particularly the stop condition and how we're exploring the neighbors to our current Center node and comparing that with the upper bound which is the furthest distance in our working queue so far so the key Insight from that is that over 80 percent of these distance calculations don't contribute anything to the final search result unless we can get away with approximating these distances so that's where the devil is in the detail is in looking at how exactly we approximate these distances so they first show the decom composition of the L2 distance into the projected and residual components we'll get into how exactly we're going to compute our basis by having the SVD of the residuals as we build a matrix of all the edges in our search graph and then you know low rank approximate that and then use that to project the vectors into the space and then have the projected and residual components and so there's a lot of details behind this but I think it's so interesting once you finally wrap your head around it and I think in addition to understanding just what values need to be pre-computed you see the memory overhead as well which is the third point is understanding the details behind the memory overhead that this creates and I think that further just helps you understand the algorithm so then we'll get into this distribution matching technique it's you use the data distribution in that SVD decomposition to project the vectors so it's pretty interesting to you know then align the the real angles with the projected angles by using this kind of distribution matching technique which a super common thing to do in machine learning and so it's really interesting to see it in this algorithm as well so then we'll get into the results I think the most interesting takeaway is mentioned in the beginning is that the you get the best performance from higher dimensional vectors which makes a lot of sense as we look at the math because obviously you're not having to do you know all the distances on a 960 dimensional vector and so it makes a lot of sense for why that is the one that performs the best but I think it's just interesting to see I think we'll talk about some future directions particularly maybe comparing this with product quantization to understand the difference between the two algorithms as well as just maybe what we can do further with this kind of distribution distribution of the data to speed up distances okay so once we've built up an hsw proximity graph we're going to be exploring it by using the algorithm shown here the search layer algorithm so search layer takes its input the query the entry points from the previous layer say Layer Two down to layer one or layer 1 down to layer 0 we start with these entry points that come from searching the top layer and that's where the hierarchy of the approximators neighbor search comes into this so then we have the ef ef is one of the hyper parameters to be looking at as your say like tuning weeviate and looking at you know EF it's the size of the queue as you're searching so you know if you have a super big Q then you're going to have a longer search and so on so and then you have the layer number that you're searching through particularly because usually you'll store the proximity graph you know as like a dictionary where you index it with the layer number and things like this so the output is going to be the EF closest neighbors to the query so we initialize the visited set the candidates and the the dynamic list of found nearest Neighbors The CW The V set so then while we still have elements in the candidates we're going to pop the nearest element from C to Q from that uh candidate queue then we're going to f we're going to peek at the furthest element from W to Q then this is going to be the terminate the search condition if the if the distance of that nearest neighbor is greater than the furthest element then that means that we've evaluated all the uh all the reasonable candidates to be searching through so now we're stepping into uh probably the most important thing to be looking at with particularly this paper is so for each e in the neighborhood of C so we've popped up C it's the nearest element to our query and now we're going to be exploring the neighborhood of C so you know say this red point is our query and um uh we'll be an example let's say oh yeah so so this is our query this red point and then we bring this blue point with us into the layer one so now we're exploring the nearest neighbors uh to the red point which this is our query again so now this node is the closest node to this one so you know this becomes our new furthest element and this ends up being the entry point as we come here or I guess the green one is supposed to be our query but anyway so I think you get the idea the green one is supposed to be the query in the top layer where we you know this is the nearest neighbor on layer two and then this is the nearest neighbor on layer one and then take this into layer zero or so on anyway so that's basically the idea I don't I don't know if that picture is super clear but but basically so once we're there we're going to add this neighbor to our visited set and then we're going to get that furthest element from W to q and we're going to be comparing the distance between each of these neighbors with that upper bound and the furthest element from W to Q so only if e is closer to our furthest element in that Dynamic listed found in the Earth's neighbor so you know say we entered the graph with three entry points from layer one and now we're in layer zero and we're exploring the neighbors back closest one if it's not uh if the distance isn't less than the third you know that third as if the three that higher highest distance then we're not going to add it to the queue and so on so basically what they find is that when you get to the mid phase of the search say you have 10 of these nodes in your queue now and you're looking for that 11th as you're exploring or you're looking to you know add a tenth and then prune out one of the ten what they find is that over 80 percent of these distances you aren't at is lower than the upper bound the distances further I mean so you're not adding it to the queue unless it doesn't it's not even worth Computing the distance so we approximate the distance so in conclusion because most of these distance calculations don't contribute to the search we can afford to approximate these calculations so now it's time for the devil and details the I think the most important thing to understand about Labor so how are we going to be approximating distance so we're going to be showing how to approximate L2 distance with projected and residual components so let's start off by just a quick recap of that idea of projected and residual components so the core intuition is that we have this query vector and then we have the neighbor of the center node C that we're exploring we always have this reference where we're looking at four e Neighbors from C so there's always that kind of relationship as you're exploring nodes so we're going to be projecting the query and the uh my arms have been that well but like to the center node C so we have the projected and residual components so basically I think the key thing to understand is that the projected component is like how much of this Vector is this other vector so it's going to be a scalar times the original Vector so Q sub proj is T times the vector C so you know and similarly D is B times the vector C so that one is pretty straightforward and then the the residual ends up being the Q minus the projected component is how you then derive the residual so with the D projections we can compute all of that offline because we build a big Matrix of the um of all the edges so you know we we already know what D projected onto C is going to be and we already have these edges to compute that with so let's actually let's let's get into the calculation and then we'll step through each of the four key uh terms in the equation and then you know see how it's computed so L2 distance Q minus d uh so we're going to break that into the projected and residual components and then you kind of like multiply that out and you end up with these other terms so the first thing to note so if you'll see my mouse is we you know first we split apart just the basic thing of you know Q becomes the projected plus the residual and then D similarly is the projected times residual and you multiply that minus sign through it I don't know how much of the algebra I should walk through when you're separating it out but so you know then you continue separating it you end up with this term as you see this again comes like here as you're breaking into the LT distance between vectors you'll often write it as like you know a minus B transpose times a minus B and then you multiply that out and you end up with the uh you know the the plus two a transpose t b sorry with the T again but so you end up with this term and because the projected components are orthogonal to the residual components this term just you know exits out but I think the only thing to be real to be seriously interested in is what you end up with which is this final thing so we have these four terms terms that we need to compute so similarly if you're approximating the dot product you don't need to have these additional Norms in the final calculation so you have these just kind of inner products so you know if you're interested in L2 distance versus cosine angular distance personally I in my experiments with playing with you know Vector search for a while now I've never found that you know especially with deep learning produced vectors that are already kind of scale normalized for the sake of optimization I just use LT distance maybe somebody has an opinion about that and I'd be happy to hear it but I think let's just stick with LT distance for now all right so as we looked at the derivation of L2 distance into projected and residual components we're left with these four key terms and this is the key detail to understanding this paper is understanding how each of these four terms are computed offline okay so the first term is going to be the L2 distance between the query projected and then the neighboring node projected onto the center node the projection components of each of these vectors so the first Insight is that both of these query projected and the neighbor projected are some scalar times the center don't see so we have t times C and we have B times C where T and B are scalars so from there we're going to break the L2 distance we're going to take out that c that's common to both and then we just have T minus B squared and then the L2 Norm of C so we pre-compute the the norm of each of the nodes so you know for every node in our database we've computed the note the the norm of it so you just look this up this is a part of the extra memory cost that we'll look into later so then T minus B squared is just a subtraction multiplication to itself so it's not like we don't do any computation but this is you know as you see in the top of the memory reason the arithmetic is counting how many um you know the low level of how many additions and so on that we need to compute so B can also be precalculated by looking at the same equation so you know B is going to be that component of the neighboring node D of C in that you know and you can compute that offline so let's get a little more into this so so this T equals query transpose times C divided by the norm of C that's going to be like the coefficient when you project a vector onto another Vector would just be this then times the C again I think is yeah so so from there you can just uh break down so then remember we already loaded in the norm of C from that we pre-computed so now we just need to do this top part the query transpose times C so that just ends up being the norm of the query again the norm of the center node that we already computed and then we do need to do this L2 distance between the query and the center node but we only need to do that once to explore the neighborhood of this Center node so in these H and SW graphs they often would have like 32 64 edges per node so you know we just compute this once to then explore the 64 neighbors of the center node and node to each of these D's as We're looping through the neighbors so that's how we get the first term and hopefully it's clear that just the uh you know the decomposition into scalars how we're able to pre-compute the nor form and then you know how how exactly we compute the T by having it it's the coefficient of the projection kind of and you know then we break the transpose into this so the next two terms are pretty quick to uh derive so with the residual Norm we just compute that offline we just look it up from memory to add it to our four terms to calculate the uh you know the Q res transpose D res so then we have the norm of the residual component of the query so the way that we do this is again the query equals the projected plus the residual components you've separated the original Vector into the residual and projected components just like post each other and so so to get the residual component we'll just subtract the project the query from the projected component so the projected component uh so again we calculated that t value in step one so we're now going to square T and multiply it by that Norm of C that we've pre-computed and loaded into memory and so now we'll just um you know we just have the projected component that we calculated from t squared times the norm of c and then we just subtract back to query from that of the the norm of the query sorry and again the norm of the query is um something that we've already computed so these are these are scalars as we've computed the norm hopefully that's clear okay so now for the most interesting of the four terms the angular distance between the residual components of the query and the neighboring node so first we're going to break out this transpose dot product into the norm of the query plus the norm of the uh the neighboring nodes residual component times the cosine distance between the residual component of the query and the residual component of the neighboring node so this is where now we're going to look into the singular value decomposition that is used to do this approximation so what we're going to do is first we're going to create a matrix this d d sub res the residual Matrix where each entry in The Matrix is the difference between the vectors so you know if there's an edge from A to B the first entry in The Matrix is going to be a minus B and so and you know then a to F the next one is the difference between a to F so we construct this big Matrix that way and then we get the low rank basis by doing singular value decomposition where a singular value decomposition you decompose The Matrix into the eigenvectors and the you know like the top R singular values they capture most of like the variance in the vector so it's like you know like about project it's kind of like how tsne and PCA and like low rate low dimensional algorithms work it works like that where you say these are the components that capture like the variance in the data so the data being the residual Matrix the uh the yeah like the distance Matrix hopefully that's clear so like that is the you know the the top components of that distance that you then use to project the vectors into that space so okay so now let's look at how we get the other half of that term which is going to be the projected residual component of the query so first what we're going to do is we're going to leverage that we've already computed a lot of the stuff that we can use to derive this without having to do the multiplication so or or having to compute the residual component and so on so so we We Begin by knowing that the um that the residual component is going to be the query minus the projected component so so we decompose That Into You Know acute transpose B minus Q projected transpose B and then from there we expand the Q transpose projected again remember is T times the center node that's the that's what the projected Vector is this T times C thing and T is the scalar of like how much of the center node the projected component of the query is so so then we break that up into again the t is calculated like this where QTC and again we've already calculated that so we already have that calculation and this um so this qtb the the reason that that's interesting to put in the front is because we can just compute this once for the query to then do our entire search with because um none of these are dependent on the center node C you know when we're projecting or the residual component there's always going to be that reference C that we're doing this from whereas the qtb that's just Universal to every single traversal we're going to do so then we get the Q res uh TB from this these subtractions and then we take the sign of it and then we just have the Hamming distance between the residual transpose B that we derived from this and then again the Dres TB that we've computed offline and saved as this binary representation okay so hopefully that's a clear explanation of where we get each of these four terms from then we just combine them by adding them together and then we have the distance of the query and this neighboring node D so they show uh later on when they ablate the comparison between that SVD of the residual and compared to say if you know if you don't have these additional terms as well so you only took the the sign of the Hamming distance between the um you know the residual the query and the residual the distance they'll show some ablations on why having all these terms helps with the stability of this algorithm and the accurate approximation of the distance calculation so let's get more into the memory cost of doing this and I think that'll really help clarify what's happening here so for each node we're calculating the uh the projection of it onto that low rank basis so this is what we would then use as reference in in those CTB so this is this is computed for each node if we're then going to be exploring from that node to its neighbor so this node is going to be the C in our reference and then we're exploring the D neighbors of C so then we also compute the uh the norm of each uh of each of these nodes that would again look up several times in the in the calculation so this is the extra memory for each node now the really the memory comes for each Edge so we have for each Edge we're going to have the sine code so again we have that D res TB so we're Computing that and storing that offline so that's our binary uh representation of the residual projected with the B then we have four bytes of the pro projection coefficient B so again we use that that b to have the T minus B squared in the um in the calculation of the original distance of the projected query in the projected node and then we also store the no the norm of the residual component for each of the nodes for in the edge so the residual component with respect to this Source uh so like if it's a to B and we're looking at it from the perspective of B and sorry a is like our C Center no lowercase C so we're storing the residual of that the norm of that for each Edge that again we look up several times with this calculation putting that together that results in the additional memory being the number of nodes we have the Delta V for vertices graph being made of vertices and edges we have the number of vertices times four R plus one and then the number of edges times R over eight plus eight so what that ends up oh sorry what that ends up meaning is so let's say for example we have the data set that just one million vectors with 960 Dimensions per vector and say we have a maximal 96 edges per node so we end up having one million times and then the four times say we use 64 as the low rank for that projection so we keep the top 64 singular values of that projection and then we and then we add that with the 96 million edges of which we then have in this case 16 additional bytes per each of these edges so so you can see how kind of the relationship between how many vectors you have and how many edges per node you have and how that scales with this so I guess kind of interestingly yeah so you know you have this 96 times 1 million for the edges so maybe when you're using this you'd want to have I guess the more edges you have the more computation you would save when you're exploring the neighbors of that Center node but then you know that would lead to more memory overhead is you have 96 and then you know going on into whatever number but so in the end we end up having 1.7 gigabytes extra added with this finger index so the offline values for the finger index and so with respect to this just example where the it would originally be 3.6 gigabytes for the data points and so you know you're talking about like adding like half so in addition to the memory overhead there is the time overhead of calculating all those values they find their experiments that it adds about 10 extra time so that's really not too bad if you look further into the detail they have the derivation of you know all those memory reads and arithmetics if you want to look into the distance between just doing full L2 distance and then you know the series of steps that we looked at with calculating those four components so so another clever technique that they use is distribution matching between the real distances between the neighboring node and the center node and the approximated distances so oh sorry so what this ends up looking like is shown in the green you have the real distances tend to be gaussian distributed whereas once you approximate them with this projection they end up being a little right skewed so what you end up doing is you calculate the mean and the variance of the real distances with that residual Matrix and then you also calculate the mean and the variance of the approximated distances and so then you would normalize this approximated distance score by having Tu to use the approximate distance not the scalar the coefficient from the projected thing earlier but so this approximated distance becomes you know what it originally was minus the mean of the approximated distances for all of the distances and then you know the variance over the mean of the very audience and then you know plus the new means so this is a way to try to you know align the distribution and make it more like the real and the real distribution of the distances so now let's get into the fun part the results of the experiment so they're going to be testing this with six different data sets and two different uh distance metrics euclidean as well as angular distance so just in case you were worried about that from originally looking at the difference between the L2 distance derivation and say you know cosine distance and so on so they also do test it with the angular distance so I think the most interesting thing to see is the gist 960 shows the biggest benefit shown in the red is the hsw finger what you're looking at is queries per second versus recall so in the a n benchmarks evaluation protocol you look at different hyper parameters where you could you can get really good recall by trading it off speed by say increasing that EF and so on so so they they Loop through the hyper parameters then put a plot on each of these curves with um the recall and then the speed so you always are going to have that kind of like accuracy versus speed trade-off when configuring approximate nearest neighbor search algorithms so anyways I think the interesting thing is that shown in the top right the higher dimensional vectors get much better result that's also consistent with the 784 from the fashion mnist but this one is you know the most significant probably here as well looking at you know you know like 1500 versus uh you know 500 so you know somewhere like in this particular you know measurement of it it's a pretty massive speed up but generally they say something like 20 to 50 faster but you know these lower dimensional 96 100 128 they don't seem to see as much of a benefit from this so I think it's also just worth mentioning that the text embedding uh 802 they're 1536 dimensional so it makes sense to think that they would the finger would look like this with those particular embeddings so some additional results of the authors comparing hsw finger with two other techniques that I personally haven't looked into so I can't speak to you too much but they also add some kind of uh distance approximation on top of the hsw proximity graph they also test the importance of using the finger algorithm compared to say random projection locality sensitive hashing or say you don't do the full derivation of the terms and you just measure that Q res t uh d-rez part at the end so that's what the Asian SW SVD thing is so they do show that you're getting much better recall by using the um the finger as it is where you use the um you know the actual distribution of the data to do the low rank approximation compared to just like random projection where you randomly sample vectors to project it into the space into the lower dimensional space and end ideas like this so here are some of the reflections I had after reading this paper uh firstly I think it's very interesting to consider uh hsw finger and product quantization so product quantization is where you compress vectors by you know you have this vector and then you'll slide these windows and then you'll cluster each of the windows and then you will represent the window with the centroid ID so by doing this you know say you have like zero you know two 64-bit values and then you've compressed it to like an 8-bit ID too that's basically the idea and so product quantization is about saving memory and trying to reduce the memory overhead of hnsw but also kind of what's interesting about it is you end up using it for disk based Solutions so like disc NN is about you you load these pre-computed I'm sorry you load the um the compressed representations and this way you get away with not needing to have so much RAM for doing it and I think hnsw finger could also have a similar kind of application to disk where you don't really need the real vectors anymore for the actual traversal of the vector as we saw you're just doing the distances between these uh pre-computed values that don't require the full Precision or you know whatever the original Vector was rather you just have these components so you know even though you're adding memory to the thing if you want to preserve the original vectors you don't I don't think you need those original vectors for the actual traversal part so this is kind of some quick thinking about I haven't really put too much thought into that particularly but I think the next interesting thing is just measuring the online maintenance of this kind of low rank projection in that residual Matrix you know what happens as you add more data that's kind of one of the big things that separates Vector databases from Vector libraries is you know people continually add data into the index so how often are we going to need to recompute all these values because we would need to like reproject it and so on that's also the problem with product quantization is you're doing that K means to Cluster those centroid IDs and then it's like well how many of these do we need to load before we have a representative sample so I think also that kind of distribution shift could be a really powerful uh lever for that online maintenance because if you just know the distribution you see the the distribution starts to skew you can you know just kind of align it with that normalization trick and then further I just think finally this like using the data distribution in these a n indexes they already kind of you know do that with how the edges are constructed but I think explicitly doing things like compression with the data itself thank you so much for watching this paper summary video of hnsw finger this paper is such an exciting way to speed up approximate nearest neighbor search by pre-computing these values to approximate that distance calculation uh there are a lot of details to this paper some memory and I hope that I got a lot of them right if you find anything wrong please don't hesitate to leave it in the comments and you know help my understanding as well as everyone listening hopefully so again thank you so much for watching if you want to learn more about weediate you can check it out on weevia i o Open Source on GitHub weekend and then on Twitter at webiate underscore IO also join our community slack and discuss these ideas around approximate nearest neighbor search I think it's so exciting to see more research with this and I it's just such an exciting part of Libya is how do we you know compute these Vector distances efficiently so thanks so much for watching ", "type": "Video", "name": "hnswfinger_explained", "path": "", "link": "https://www.youtube.com/watch?v=OsxZG2XfcZA", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}