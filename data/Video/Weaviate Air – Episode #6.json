{"text": "Jupyter notebook for the cursor API: ... \nforeign [Music] I have the whole crew in here we've got Erica I got done we have Zen we've got Connor we've got me uh well Sebastian you can see the neck well this way see the name here and we are super excited for this Vivid air episode which is going to be focused on yesterday's release of vv8 118 and you may have already cut a glimpse of the release blog post and read about it in the documentation but I thought the idea of this episode could be can we maybe discuss some of those features and explain them or maybe we could demo some of those features you know just in case if you didn't quite figure out yet how to use them we can maybe show some of those in action so we are super happy to see you here and thank you very much for watching and listening and if you like this content give us a like because that always helps maybe more people like-minded like you and us can come and watch us so this will be super super exciting and the bonus of being here live like always is you can ask questions as we go so um you can ask questions oh you know like if you don't have a question just say hi say where you're watching us from or you know just say you know some random fact you know just keep it more or less on the topic and we are all going to be fine so the main topic today like I mentioned is we did 118 and we have a bunch of really cool things that we want to talk to you about so first we'll talk about cursor API so Dan will be covering that and I believe that as a a nice little demo that goes over it then Conor will be talking to you about want and we should probably have a whole discussions around it Erica will be in charge of talking about like the new stuff around bm25 and hybrid you know the wear filter and the stop words um then we'll talk about the addition to hnsw PQ and then we'll cover like an update to replication with tunable consistency and Repair on read we'll go into roaring bitmaps and then finish off with the addition to our backups for azure so without any further Ado let's start with the cursor API done the floor is yours thank you Sebastian hi everyone my name is Dan Das kalisco I'm a developer advocate.pv8 and let me introduce you to the cursor API which we launched with version 118. the main reason for this is that private 118 you could only retrieve about 10K objects from a database which meant you couldn't really use it as a source of truth but with version 118 we released the cursor API which lets you paginate through all objects in the database in a class so you can use Vivid as your primary database and the way this works is that we have an after parameter which is based on uuids and it works for both the rest and the graphql apis you pass an ID to that parameter and with yet will retrieve objects that follow that ID in insertion order so that means this API Works only for retrieving objects but not for searching or filters to demonstrate this I use the Vivid demo data set python package which was also published recently by my colleague JP and it has a number of data sets including Jeopardy questions which I imported into a sandbox so we have the console here and are going to demonstrate the graphql cursor API so we're getting Jeopardy question object that starts after the empty string which means we need to return the first objects object it has and we also request the ID among the other fields and this ID will help us resume getting the objects effectively doing pagination so you can notice that we have a limit of three objects and this is the second one so if I'm going to copy this ID and paste it here then rerun the query we expect UV to return objects after this one so starting with the last object then it almost hearty as the answer and we'll see two more objects from the Jeopardy question class so let's see if that's what's going to happen and that in happened as predicted so this is the graphql usage of the cursor API and here we have a jupyter notebook that shows how we can use it for the rest API as well and the code is um simply a while loop in which we call the objects API with this after parameter and again it starts with MP string then we fetch the idea of the last object that was returned and repeat the loop and there is a setting here interval set 100s every 100 objects to display some progress and you can see that um uh the client retrieved 100 objects in about 0.6 seconds this is way slower than typical because it's a sandbox and it eventually finished with reading all objects and um graphql functions in a similar way so we have this query in which we interpolate the after parameter and we pass it via post request to the V1 graphical endpoint again pick up after from the last returned object and repeat the loop and graphql retrieved all the objects even faster than the rest API about twice as fast so this was the cursor API and you can read more about it in our 118 release notes right here perfect this was uh pretty cool and done so basically what was the idea of this like so um with the cursor API is the idea to be able to go through all of the data like in a really fast manner without like writing complicated queries or what's the what's the idea and the main idea is that now you are able to access all objects in a class well previously you are limited to 10 objects for a query so users had to craft these artificial wear filters to keep patient 84 objects but now you simply indicate the last the ID of the last object you received in the after parameter and you get the next limit objects so that lets you dump all objects in database or use it as your primary source of Truth nice nice this this is cool yeah and um definitely like looks like a super efficient way to like access a huge amount of data because I I remember in the past like if you just wanted like test out few objects this wasn't as fast so yeah even like on the on a Sandbox environment uh like the speeds were pretty impressive this is very fast because there's no filtering you simply fetch objects in batteries nice nice so and then is the idea of the cursor API that because you have this after you almost like go like here's my pointer start from here and kind of like fly in that direction is that where it comes up yeah yeah that was my question is um from the uids how are they sorted so when you go from after it takes you to where are they like in order of how they got into the database like it's after it takes you from this uuid how how are they sorted so they are in the order in which they were uh inserted in with gate in the order of creation interesting yeah that's pretty cool nice so I guess like even as the as you query the data and you modify the data your cursor kind of stays consistent in one place while if you are running like specific queries suddenly the next page would have could have changed right if something underneath changed yes because there's some ideas which are immutable the order of objects returned by uh this course or API is guaranteed nice nice yeah I love it I love it yeah thanks for the demo this this made it pretty uh pretty uh to the point and then and and now I know how to use it like uh I can't wait to um like I I I almost said that I can't wait for this episode to end but like I've this this is definitely going to be something I'm going to test out after we go offline so um yeah thanks a lot for the demo done and um Conor tell us about once should I show your screen as well yeah cool is it showing now it is I got the uh because I have this going full screen so I can't see everyone else anymore all right so I'm going to be talking about wand scoring and there used to be this meme on Twitter that was like making gpus go bird and talk about like running software fast and so I had to borrow that for talking about like something that makes something faster hey so is that that's like a bird like like a fast car or a burger I'm cold I guess it's like the sound of the machine like you imagine this like right like a pairing cut machine that's okay I get that nice all right so the tldr of this is is with with bm25 we're scoring each of these keyword terms and you only really need to score the unique terms to most of these queries so that's the tldr is you know if the query is how to catch an Alaskan pollock you probably only need to score Alaskan pollock you don't need to score how to you know and these kind of things so so let's dive into the details of how you calculate the bm25 scores and uh so if you see my mouse like K1 and B these are just like constants like constant values that are used to kind of stabilize the algorithm you might tune them but let's kind of ignore that uh so the key things to look at here are so this is the score of a document in the query is you Loop through the terms in the query and then you have the IDF for that query term the Q sub I and then you know the frequency of the query term in this document you know normalized by the same score and then by the um the length of the document divided by the average length of all your documents and then you also have this IDF thing this IDF thing is the most important thing well let's just quickly explain why it is the most important thing so with with this other part of the equation when you're doing the uh the frequency for the query in this given document this the key thing here is going to be this document length and the average document length so what that looks like is let's imagine like one of our documents is just pawlik it's just one word this is like the most extreme case where you're kind of like gaming that cardinality of D thing so you end up with one over 50 and so you know pawlik appears one so it's one over one plus one over fifty and so again we're going to ignore these constants just to get the key ideas behind like what changes for each query so you end up with 1 over 1 plus 150 Which is less than one so you're always going to have some value that's less than one and thus the IDF is really where the payload comes in because you know you're only getting at most just one which is going to be the same as the IDF from this part of the bm25 calculation all right so the interesting thing is the IDF so the key things in IDF are n the total number of documents you have and then end of the query term which is how many times this term appears uh in this um uh overall in all of the documents so as some examples let's say like you know just we have 20 documents and you know how appears and so this is like the inverted index you have how to catch n and these are the document IDs that like that where it appears like just numerically so you know to sum it up how appears eight times two twelve catch four and ten and then Alaskan only twice and Pollock only twice so when you plug that into the you know n minus how many times it appears over how many times it appears you end up getting like for the rare terms 20 minus two over two is nine whereas for the common terms like two you get like 20 minus 12 over 12 which is two-thirds so this is these this is the IDF of each of these terms how much can it possibly contribute to the vm25 score so once you've been scoring this query how to catch an Alaskan pollock you have a minimum threshold so you score pawlik it adds nine you know to to the the documents that had pawlik Alaskan then adds another nine and then catch adds another four so then there's like a minimum threshold right and so let's say it's four now the minimum distance after you've scored this many so now these remaining terms how and two they can only possibly add 3.17 to the score so the search is over so all in all what this leads to is just less uh scoring calculations for scoring the bm25 and thus a faster you know calculation of bm25 scores with this wand algorithm where you're using this minimum uh threshold to prune it and say no forget it it's not possible for this to add more to the score so also you can check this out if you're really interested in all the details behind exactly how this Implement is implemented uh here's the link to the implementation and going cool so that's right hey Connor I've got a question so if you go back um well yeah here oh no the the next one so when when does this happen where you kind of like score like Pollock Alaskan in cash that's like when we score like the the query level even before the query begins yeah so you can sort them by uh so so you have these lengths and so you sort them in reverse order because you know it's like inversely the less it appears the more it can add to the score so yeah you first you sort it and then you can uh compute the minimum threshold so exactly what how this is done with with the top K I'm not exactly sure of that detail but basically you sort it and then you can like calculate what that threshold would be based on having because you you uh calculate these numbers by you you would have this number from the length of the inverted index basically oh yeah so basically because I'm speaking of like obviously um I'm new still to the field and everything right so I'm basically trying to figure out from the point of view of so when does it happen so does this happen at the time when like we import our data and we create this index okay yeah yeah no change to event is needed it can this can just build right on the inverted index that you already have you don't need to say upgrade V8 1.18 and then re-import all your data you can just 117 or however to 118 and then just or I guess 117 is when bm25 came out but anyways so like 117 to 118 and you can just use this you don't need to re-index oh cool cool yeah nice and then like with this basically the bf-25 surge is just like smarter because it's just going to ignore words that just like keywords that don't really add much to the results right yeah like yeah you have to how I was like yeah that's basically every single document I have right yeah exactly it's nice it's the cool I I have a question so is my understanding correct that let's say you have a huge query this is basically picking out the rarest words uh in that query based on how rare those words are in your database and then you order them from rarest to most common and then you say if I've got this many rare words don't look for any other common words and that removal of all those common words from the scoring process is what speeds things up is that is that the correct understanding yeah that's exactly it and I think um well you you said something with that that made me question my understanding a little bit because if you well oh okay sorry so yeah yes exactly because you do it with the query if you have a this is how to catch an Alaska right like six words so it's pretty easy to visualize it I don't know what it was about you just said that inspired me to think about like querying with like a paragraph but if you're querying with like an entire paragraph I mean that would be an interesting test to run I guess I haven't really done that kind of test but that would be like a stress test of this right yeah yeah because if you have like let's say 20 common words the sum total of that could end up being pretty high that it needs to keep doing it until yeah yeah yeah now this is cool right yeah that's why I like we be there because like we're teaching to each other and then suddenly it's like wait what like let me ask questions yeah we should almost have like a follow-up session just like now testing out ideas yeah nice nice cool um did you did you have anything else to add or what's what's happening next oh uh yeah this is all I have for uh the wand oh perfect thanks um so I want to give give a quick shout just uh before I hand it over to you Erica so like uh Carson uh is uh saying hi from casual in Germany uh so he's looking forward to using we with 118. it's like um yeah like I'm super excited to hear that and let us know how it goes and uh Russ says great stuff so um yeah thanks rash thank you for encouragement so um yeah anybody else just like give us a shout and of course right at the beginning Connor was like stepping in thank you for watching I think that you send it before even I got to say it right so he's such a pro Conor all right so moving on Erica I'm kicking the Baton yeah let's do it right so you will be talking now about bm25 hybrid improvements which is pretty close to the topic of one than the stuff that Connor was explaining right yeah exactly so bm25 and hybrid was released in 117 and yesterday we released 118. but really what we were focusing on was how can we improve bn25 and hybrid so Conor went over one and this the simplest way to start using wand and really apply it to large scale data and applications is just by upgrading a repeat instance which is exactly what Connor said so it's kind of cool that it's that simple to use it and see the performance improvements um but what I'm covering today specifically is we added in stopword removals um it's now indexed whereas previously it was not and then also added we added in the wear filter to bm25 and hybrid um I guess you can share my screen I'm still talking a little bit to like kind of built it up but um it ignored the stopword configuration and scored all of the words so as you can imagine if we are queering with a large paragraph and it has the and what many times it it will be very slow um so ignoring them speeds up queries that contain stop words as they can be automatically removed from queries as well um and that's obviously important for stop words so here I'm using the podcast search demo that I showed previously um Conor a GitHub repository and using it but what I'm showing here is that you can use the brask API to update your stop birds even after your data has already been indexed um oh wait so are you saying that I already have an existing schema and then I can update it oh neat yes yes um so here I have added in so my date I've already uploaded my data it is indexed I have my schema defined but now what I'm showing is that we are adding in a few additional soft Words which is what the an is and this preset is set to English um but you can also have none and then add in additional um stop words in here but then you can also add in oh no I removed it really quick um you can also add in hey this is how we navigate through our documentation just live a little preview um so here you can also have removal so if you do want to include a or the this is where you would Define that all right ah it's like the opposite of ignore it's like included too yes exactly all right so I'm going to uh run this and then I'm going to head over to my B1 schema refresh it and what you can see is I the additional is what and as you can see previously I had added in B and is but because it's already included in the English preset um the only new one that was added in is just what and that's really Erica could you zoom in a little bit just just to make it bigger yeah I'm so sorry this is the preset of English hey does that mean we could um mess up with Connor's example and like add Alaskan and Pollock to to the list and then he would never get the results ever again yes that's a good secret can we do it well you need the database the database to have something about Alaskan pollock in it yeah as in We're Not Gonna necessarily test that query right but like I just want to see how easy it is to just add additional words now now that you've done it yeah so here I've added in beer and release because I know that's talked about frequently on the podcast and then I'm just gonna run this again and then refresh and I could see that it was added in see the stalker variable um so yeah the results would it be in your favor if you accidentally did this you're just too nice for Connor like you didn't want to mess up with these Alaskan Bullocks huh but this is cool how fast is this update so can you can you tell us like what happens underneath right like when we add these additions do these keywords get removed from the sparse vector or sparse is that the right word as well inspectors well now they're stop words but it's different from 118 is that they are indexed but when you are creating a bm25 they are um removed for relevance ranking because if it's going to rank the and and it's kind of going to boost results even if that's not really what you're looking for so the key here is just kind of to remove those um like the relevance ranking for the in and like or beer and release or what this example sounds good yes um all right so and then the next thing that I want to show is the wear filter so we have finally added it in for bm25 and hybrid but just with this example I'm taking the Pod clip and I'm asking a query on what were the features released in 117 and just similarly um to 117 this works right but what is new is the wear filter which is great I will show the ticket that had like I think 30 episodes um so what do I want I know that Conor edian and Parker talked about um the 117 release and podcast 31 so I will just narrow it down to the podnum sorry okay and then operator is equal and then just my value int of 31. so now as you can see all the Pod numbers are referring to podcast31 and this is where we have Connor speaking Connor's speaking a lot oh but then we have Eddie and as well that is related to talking about the features that were released in 117. so you can see that the main focus for that podcast was talking about replication and hybrid search um yeah wow so um we basically combining three kinds of search uh like filters right so we have Vector search like we have the bm25 search because of hybrid and then we have scalar search time that's that's pretty possibilities yeah and what I wanted the Highlight here is that this feature was built off of a request by someone in the community a result in Federer um this had 34 uploads which is insane so I just want to make a point that if you want to contribute to the features that would be included in 119 or even 120 um you can head over to the doc developers docs and then on the bottom here we have the roadmap then you can click on it and you'll be taken to the GitHub proposal and then you can upload it so please this is the time as we're planning for 119 um your voice matters absolutely I mean I I can give you some like a secret uh Intel that I know like so at the end our CTO uh he's actually planning to like a look at some of those items in the backlog and then some of those more popular ones would definitely get a lot more attention from at the end so very much like Erica said like we are very much like roadmap at feedback driven and if you feel like there's something that's very important there give it an upvote give it just like that right Erica believes this one is very important right so uh yeah just just like Erica said but like my pony is like you can do it like right now like even this week because you could influence what will go into 180 19 or maybe 120. so um there's no time like now right cool this is pretty pretty amazing thank you for the demo Erica anyone have any other questions one question I had was how does the removal of stop words work with the um the Rarity of words and then ignoring the non-rare words because I would imagine the stop words would be the non-rare words and then if those are removed then whatever is left over does that does that make Connor's Point even stronger because now you don't have a lot of garbage uncommon words stop words takes care of that now The Uncommon words are even um they're they're I guess removing those is even more powerful now right am I understanding that correctly or I see I mean yeah it's like in the example if we remove the bottom three now the least common I guess would be catch right or the most common that has like the less weight would be catchy so it really puts that focus on Pollock and Alaskan ah so the these two features are kind of synergistic because the the the wand update doesn't need to take uh take care of a bunch of common words the stop word update gets rid of those and then you really focus in on the most important word foreign and correct me if I'm wrong anybody here but like it's almost like it gives us an ability to kind of uh say which words should be completely ignored but let's say you have a furniture company uh so then what Conor was explaining matching to a chair because basically chairs will probably be mentioned like once every 10 times like we have like an item in the catalog so magic to watch wouldn't be so important because of want but at the same time we still still will be a valuable match right but then a specific kind of chair right like that could have like a you know just like Ikea and other companies are doing right like that they have a name so because those would be rarer like in terms of how often they appear they will appear even more but I think with like the the stopwatch itself you could kind of go like yeah just ignore every time somebody says woot or or something like you could even have like terms they go like yeah like I don't value these mattress so much or like at all and I think that could be away right yeah that's a good way of putting it the con the stop words feature gets rid of generally common things and then the uh the part that the tfidf part that Conor was talking about then after having removed the generally common words now it says of the interesting words which ones are rare and which ones are common so it's even yeah that's a good way of putting into question yeah totally so you know like um I'm sure if you haven't been to Ikea or buying a bookshelf so nothing to bookshelf like you give you like a small score but if you say Billy like yeah that would get like a pretty high score right like uh and uh I'm sure that anybody's ever been to Ikea knows what's a daily bookshelf bookshelf no interesting maybe I have too many books great great this this is amazing this is great and um I love the addition of of the wear filter um that's definitely been very needed and uh yeah perfect so any anything else that you want to cover no that's all online I'll pass it over perfect so um who have we got next so we have hnsw and PQ um who was doing that that's me if you can share my screen absolutely there you go all right okay let me show your face again all right so let me jump into it so 1.18 adds this uh new Vector compression feature called Product quantization and it works with uh our uh our hnsw index and it essentially makes it so that you can store your vectors in memory with one-fourth the requirements that one-fourth can change depending on the parameters for PQ that you set so I want to give just a quick five minute intro to what PQ is and how you might be able to use it okay so imagine that is one vector that you're storing so this can be any any dimensional Vector okay what PQ does what we do right now is we store this vector and we have millions like this that are being stored in memory which takes a lot of memory what PQ allows you to do is the very first thing you do is you chunk up your vector into n segments that's one parameter so you choose whether you want eight segments or 100 segments the upper limit there is the dimensionality of the of the vector of course you can't have more segments than you have dimensions uh then you take each segment and let's just have a look at the first segment for now you do the same thing with every segment but let's consider the first segment and let's take the first segment of all of your vectors so I've just drawn two vectors there but imagine these are all of your vectors you take all of the first segments of your vectors and you plot them out right so the first Vector is there the third Vector is there and then all the other vectors contribute their first segments as well now what we're going to do is instead of pay attention to where each Vector is located which takes a lot of space we're going to create neighborhoods of vectors so I'm going to create this neighborhood where these two vectors live together this neighborhood where only this Vector lives this neighborhood where this Vector lives and then so now that we've got these larger neighborhoods these clusters of vectors instead of defining the vector uniquely using its coordinates here what we're in set going to do is Define these neighborhoods using their centroids and this is where the second parameter comes in which is Kate you can decide how finely you want to segregate these uh vectors by saying I want a lot of neighborhoods I want a thousand neighborhoods or you can say I want 20 neighborhoods so there's going to be very very large neighborhoods you're going to have a lot more vectors per neighborhood but the whole idea is that every neighborhood gets its own centroid which has a coordinate and it has its own ID now instead of referring to a piece of a vector this first segment of vector one by the coordinates that are unique to it we're instead going to take those coordinates and we're going to replace it with the centroid ID for this centroid so any Vector that lives in that neighborhood will now not be referred with its unique coordinates but rather with a centroid ID so instead of having to store all of these numbers now you're just storing the name for that centroid and this is where the memory saving comes in so if you have your database to begin with you've got right now this is what you have EQ is going to chunk that chunk this up and it's going to do the same calculation over and over again for every segment and it's going to turn this first segment here into a centroid ID the second segment here into a centroid ID and if you implement this over your entire entire database you'll have just all these segments now chunked up into centroid ID so instead of having to store numbers here you only have to store uh centroid IDs instead and if you understood none of that let me just abstract all the details away and let's let's uh get uh down to just the intuition let's say every Vector is an address that you're storing so you can have the house number the street number you can have which uh neighborhood I'm living in what state I'm in what country I'm in so on and so forth and then all the way at the top I'm living in on planet Earth PQ allows you to zoom out and say I don't really care about the house number I don't really care about the street number I only care about the state that Zayn is living in and now you're going to have a bunch of people that live in the same state so you have a course or representation for each Vector but that course representation allows you to save on memory requirements right so some of the experimentation that we've done allows you to save one save and store the vectors in 1 4 the memory requirement so that's one fourth the ram previously required so that's a speed up but it's it doesn't come for free right so the the one thing that you have to balance when you're using PQ is that the lower the more you compress the vectors the lower you get the memory you're paying for that in terms of recall because now you're summarizing vectors in terms of the neighborhood that they live in using those centroids right so it is a balance between how much recall you need versus how much memory uh savings you want to you want to realize so that's a quick summary of uh of PQ and that works with hnsw you can look into documentation and by default we don't enable PQ and it's experimental right now but if you want to play around with it you can enable set the enable flag underneath PQ in the configuration to true and there's a bunch of other details and settings that you can play around within the configuration you can read more about that in the docs and Sebastian will add the link to the docs in the the description of the video as well so I guess if we want to find like a balance of the higher recall and lower memory is there like a how could we optimize K where we could have like a balance between the two uh so it would depend on the size of your vectors really because the the lower K is you're basically saying if I go back here you're basically saying if you decrease K to one you're saying identify the address of a person uh using one centroid and that'll just be okay this person lives on Earth which is not very helpful and so the recall is going to take a major major hit it's going to be horrible but then you could say Okay I I can afford to have more memory so now I'll zoom in further I'll I'll give everybody their own neighborhood I'll give everybody their address so it's a there's a work there's actually a blog post that's going to come out next week around the details of this so you can read more about the balance between memory saving and recall um there is a cut off that we'll mention there where you can compress it down and then to a certain degree which is this one-fourth after which the the price and recall that you pay is not worth it okay so there's like a diminishing return at some point exactly it does Plateau off so that's a that's the good thing so you can keep on compressing the vectors and then you when you see recall taking a very big hit that's when you know where the cutoff is thanks loved your explanation and this circle it's like okay how many kids great job yeah yeah I want to also hop in and firstly say the slides were gray I love the whole presentation I I love how you put the uh the chunks into the points as well that was awesome we go back to that slide actually with the uh with the two going into the point yeah I love how you animate it in the regions that was awesome yeah I was like if I have a video for this that'd be great but I don't have a video so let's just make a PowerPoint video so yeah we should turn that into a gif and like tweet it yeah um the funny thing was like when initially you had just like those two arrows pointing out uh like and they were not pointing at anything I was like what are those hours pointing at and I was like wait wait those are vectors they're not just arrows right yeah any drops they're not arrows or vectors um hey so you mentioned at the beginning that is the other two parameters or is there one parameter like is there K in the dimensionality we want or yeah so N is a parameter here how many segments uh you want to yeah cut the cut the vector up into and then for each one of those segments the second parameter is this guy right how many centroids to look for for each segment yeah so so like almost okay so let me think about it so if I have my Vector that has 1024 for dimensions and I could say like I want to shrink it down to 32 Dimensions right so that's the end and and uh value yeah would that mean that I would end up with 30 yeah go on for for 1024 dimensional Vector if you want each one of these segments to be 32 dimensional then you would have to set n to 1024 divided by 32. and N has to be a constant multiple so that each you don't have like a portion of a vector Dimension hanging in one segment and and the other half hanging in the other segment right so is there reverse where if I want to actually have 32 segments so I have to do a calculation of was the n then you have to do a calculation of how what the end should be yeah oh no I just want 32 segments right so um okay so if I have done 32 segments do I get a centroid per segment or the centroid is just calculated based on this reduced Vector you get you get K centroids per segment so every every segment is going to have these vectors and then you calculate let's say a 124 centroids per segment so each one of these guys is going to this is going to be 124 centroids this is going to be 100 in separate different 124 centroids and you can also choose how uh what that K is right one 124 or 500 1000. okay okay because I I felt like um the biggest memory saving is in in a space of like how much we compress the the vectors themselves right so if I initially started 1024 and I shrunk it to 32 that's just like a tiny fraction right and yeah so here if you if you have less segments then you will eventually have less centroids overall because here let's say you have 10 segments and for each segment you have 100 centroids you have a thousand centroids in total but let's say you only have two segments then you only have 200 centroids so that's a that's a cost saving in memory but then also how many centroids each segment has if you reduce that that's also a cost saving so you can take the memory down to being nothing and you'll have no recall that's why it's a it's a it's a delicate balance yeah and then okay so that that's this part and then later on when I do run a query unlike on those reduced vectors yeah um so what happens like first we go to a neighborhood based on the centroid of our query we pick all the candidates but then the scoring and the distance calculated based on the full Vector right so that is uh if you read the paper on disk a n that's how it works but right now that full Vector is not uh stored um on disk that that might be a feature that we Implement in the future but how distancing calculations work right now is you can take a vector and then you find out which neighborhood that Vector resides in and then you can calculate the distance between let's say there's a new let's say this is a new query Vector you find out that it lives in this neighborhood for the first segment you you know the centroid you know the coordinates of the centroid you can calculate the distance between the query vector and the centroid and you do that for every centroid in every segment um another future work uh right now is you can actually store the complete Vector representations on disk which is much cheaper and then if you want to do distance calculations you can bring them into into working memory and calculate exact distances if you have the complete vectors but right now we don't have the complete vectors we just have the compressed vectors if you enable PQ cool cool yeah I always mix up that detail of whether the compressed vectors are compressed and my understanding is that they're compressed in memory and then they're fully represented vectors are on disk and then you would um you decompress them with some error and then look for the nearest Neighbors on disk I always mix up that detail movie yeah from what from what I understand the main uh RAM savings come from the fact that your compressed vectors are stored in in working memory in Ram and the full representations are stored on disk so you do your high level search in memory and then once you have your best candidates then you go to disk you read them in and then you can do more fine-tuned exact searching over those limited candidates all right yeah so basically that that's what I meant with my question so yeah because that yeah the saving is like instead of storing 10 million 1024 Dimensions big uh vectors I can start 10 million 32 exactly big so that's like 32 times less exactly uh memory footprint but then once I found the initial candidates based on the uh the neighborhood we can kind of like go in and then score them based on the full Vector right exactly yeah but but then because the first trip is like super fast um we can kind of like uh take the heat of doing a discrete as a one-off and then do that distance calculation yeah this is powerful stuff so I have a question um and then and I realized there were also questions um from from stop words but uh we but first let's let's uh talk about PQ so Raj is asking uh well or first is very cool small question will PQ affect the time taken for Vector search performance on large indexes like 15 mil plus vectors so it um Vector search performance on large indexes this is so the the blog that we're planning on releasing next week actually looks at uh latency considerations as well but mainly the idea behind this is not not to uh it's not going to take longer or shorter but the main idea is that you need less memory requirements okay so PQ is not going to um it might take a little bit longer um because you have to do this uh reading in from memory type of thing that I described which we're looking at in the future but the main trade-off here is not latency um latency and recall which is the which is the main case and let me go back to my slide here in in hnsw the main trade-off is recall and latency here the trade-off that you have to decide is how much recall do you want to trade for being able to save memory so it's more around memory space requirements as opposed to time requirements we've got also a question and probably we don't have this right now right but um how do we know if we're planning on sharing some benchmarks this is what uh we're going to talk about this in the blog post next week where we do a deep dive on PQ so look forward to that and then we'll also have more details then yeah so thank you for questions Raj and vinod and uh yeah just just keep keep an eye on our blog post then uh because uh as far as I I I remember as well the plan is to even post two separate blog posts on the topic because there's so much to talk about uh but thank you for sharing the questions because that also helps us shape some of that content and um yeah next week Tuesday we should have that the first blog post from the series um so this is really cool um yeah any other questions around PQ before we move on there you go thanks rashford yeah no worries uh thank you for asking this is why I love this whole session being live cool okay so let's go back to um the topic with stop words uh if we are done here and so we had a question well first this is not a question you know this is a shout out so uh thank you we really appreciate it makes us even happier uh to to work on it and I would definitely pass it on within the company so let's look at the actual question so are there any performance trade-offs to consider when adding a large number of stop words do we do we know or is that a question we should probably get back to later that's an interesting question but also uh thanks for the compliment and like Sebastian said it'll be shared with the team um but um I'm not sure I mean I know Connor is working on the beer benchmarks um and maybe there is a data set within that that has where you can add in a large number of stop words and then kind of compare the performance um but previously but the demo that I used for a podcast search I was looking into the natural questions which is within the um beer data set um and it's kind of tricky because there aren't really that many stop words and the performance I didn't really see a difference when I've removed it but maybe Conor has something to add to this um I well I think it's I I think the kind of customized stop words is probably best for like a multi like low resource languages languages where you need that kind of domain expertise that's probably where I would where I interpreted as having the most application um because you'd be tuning a really long list of words to try to like speed up the beer benchmarks particularly yeah I I think what we're looking at is because maybe the stock was by themselves are not about like making querying faster but the real true benefit is can they make your results better right because By ignoring the wrong stuff like the stuff that you don't want necessarily to be part of the query you can get better results better results but maybe and that's something maybe we just have to test out and we can come back next time around as vaccine like hey what did what is what would happen if I had like a thousand stop words would that affect the performance right so not so much like would that make it faster but would that could that potentially make it slower that would be an interesting thing to figure out and I'm sure if atien was on the call or like somewhere from engineering they would have had the answers straight away so maybe next time we should have them on online Coco all right thank you for the questions and you know like this is part of being live you know sometimes there are questions we don't have the answers straight away but uh we can always figure out maybe you can add the answers to the documentation uh once we figure it out so that'll be cool perfect so who have we got what have we got next so we covered um bm25 hsw and then now we will talk about the update to the replica yeah replication update with tunable consistency and Repair on read um I believe that's done right then that'll be me indeed so you need to share your screen oh yeah I can show the documentation uh there's no demo but um I can show a few Graphics that hopefully will help you understand replication especially if you haven't been following along uh we introduced application in the previous version 117 and in this version We enrich the future set so to help you understand what happened in this version um I'll briefly go over what replication is essentially you can replicate an object in a class to a number of nodes so in this diagram you can look at the um the class A for instance let's say it's a for article and you see that out of the six nodes here three of them have this green marker so that means every object in the class is replicated on three nodes and three is the application Factor and the replication is useful for a number of use cases first being high availability if a node goes down your setup still functions and then you can linearly increase throughput by adding nodes you can also upgrade with no downtime or you take one node out and the others will still process and return results original proximity is obviously helped and um when you build a distributed database with the application you have to choose two out of three properties one of them is consistency which means you get the latest version of an object that was written on every read There is the address availability it means you always get a response even if some nodes are down and the other is partition tolerance which means the system will still function if nodes individually do not and uh the um the trade-offs between consistency and availability is what you can control all operations now in version 18. so this is called tunable consistency and there are three values for this the first one is one which means that a read or write return as soon as one of the nodes responds Quorum which means that your request will return after a majority of nodes respond majority means half of the replication Factor plus one and then finally we have all which means all nodes in the replica set must respond so we need to use this all for scheme operations to ensure full consistency but for reads or writes over the rest API you can choose between one column or all for graphql we do not support consistency that's tunable yet but this might be implemented in a future version and to visualize how this works we can look at the Quorum consistency level so in this damn you have the replication coordinator node that receives a request from the and this sends it out to n nodes the replication Factor so here is three you have three arrows going out and you can notice that only two arrows come back so that means a majority of nodes have responded the replication coordinator node is happy and Returns the result of the client uh there are a number of scenarios for consistency versus availability that you can choose from so Quorum and Quorum for reason right will give you a balanced write and relatency or if you need fast rights and slow reads you can choose one for rights so you only wait for one node to respond before the right is acknowledged and then um all reads means that you have to wait for all the nodes to respond before you get the result or if you need the fast reads you invert those so you have the all consistency level for rights and one for reason so that would be tunable consistency and then we have repairs so imagine a scenario like this um you write an object with a consistency level one and that note dies before it can propagate your object to the other nodes so in that case when you read the object again with the consistent consistency level one it's possible that you'll hit another node not the one that accepted the right so in that case you might uh not get the object at all or if you had the patch operation you would get the previous version of it but if you had used consistency level all for that read that means vv8 will wait for all nodes to return and it will see that one of the nodes did have your new object or the updated one while the others didn't have it so then it would automatically propagate the changes or the creation to the other nodes and it will return the latest version so this is called the last right wins model for solving consistency conflicts and it's what movie it uses and we explain here in the consistency documentation page what happens when you write an object and then when you want to guarantee that it has been propagated so essentially you need to read it with a high enough consistency level compared to the one that was used when writing the object that's essentially it you cannot check out the blog release or replication and here we cover both number of consistency updates and Repair on read cool um I got a question so where do you add this configuration or like where do you provide like you know say like I want to use Quorum versus all or majority um so this is used um in the rest API uh for every right every read and every right endpoint when you get when you create an object you can specify the consistency level here ah cool is is there a way to change the default behavior I'm not sure how if you went over that um the default is Quorum I should have mentioned this and there's no way to change that you change it per operation okay okay all right I'm supported by all nodes sorry all endpoints which means also creating cross references supported again you can specify the consistency level here and also for batch operations interesting Coco perfect any questions from maybe from the audience hey we got a question so uh Dan what replication protocol algorithm does we would use do you have the answer what replication protocol does it that's something um pretty low level I cannot answer it right now but I'll get back to the user on slack if they are there and we cannot pick a documentation as well yeah that would be a good question like yeah if you're part of like our slack maybe drop it on General and then we could definitely uh come back to it thank you for the question anybody else any other questions in here or from the audience perfect um it definitely shows uh who wrote the documentation on this because uh you you are so good like walking over to the content it's almost as if you wrote it yourself my um nice nicely done cool all right so who have we got next so on the list of topics uh we have roaring beatbox and I believe that is Zen Zen I'm going to share your screen and you can take it away sounds good okay so roaring bitmaps are um we've implemented implemented a new data structure in our inverted index so before I kind of drown you in jargon let's start off at the top so what do these do um so they essentially speed up filtered search up to a thousand X and the Thousand X is the attention grabber here We've ran some uh experiments where we've got a thousand X Improvement then what the heck are pre-filtered searches so pre-filtered searches are basically think of let's say you're running an e-commerce store and you've got 10 000 unique items that are up for sale in your eCommerce store a user comes in and they ask for they have a search query they're looking for furniture you're not going to run that query against all 10 000 unique items that you have in your store you're going to pre-filter the search such that you only look for look at items let's say 2 000 items that are in the furniture uh category so you pre-filter the search such that you don't have to look over 8 000 items you drop those out at the at the outset so how we V8 works is we have an inverted index that has a reference to every object so it's essentially just a list of unique uh item identifiers and then there's the hnsw index that we talked a little bit about when we were talking about hnswpq which allows you to do Vector search approximate nearest neighbor search so the inverted index is what we've modified with this roaring bitmaps update and what it essentially allows you to do is the inverted index is now implemented using the rowing set data structure and the efficiency that this unlocks is if you've got a filter let's say you're looking for items that are of furniture type in my in my store and above a particular price uh those operations are really efficient when done using this new data structure and the other thing the other speed up is that because we natively implement the inverted index using the Roaring set data structure we don't have to convert into the new roaring set data structure that when we have a filter so we save on latency of of that conversion as well right and mainly where we see that thousand X Improvement is if you've got a filtered search where the allow list the allowable or the interesting items to search over so in my previous example if the user had a question about furniture and they were searching over Furniture the allow list would include the 2000 Furniture items and not the 8 000 remaining other items like clothing hygiene products all that stuff right so this allow list where you get that 1000x Improvement is if the allow list is basically your entire database so imagine somebody goes into Amazon and searches for opens up a filter and says search for items that cost more than a dollar your allow list is going to have everything in there because barely anything cost less than a dollar so now your allow list is basically your entire database this is the same as running non-filtered search you're running every query has to go through every single object in your database which is really slow and the problem is that because it's implemented as a filter search you're still going to filter everything and the filter is not going to have any effect it just adds a bunch of latency and this is where the uh the time savings are realized right if you had previously a huge uh allowed list where your filter was all encompassing your entire database we would first of all generate the filter which would be useless but we would still do it and then we would convert that filter and send that to hnsw and it would search over all of those uh all of those vectors and that's where that's what would take a lot of time in terms of what this means if you've already got your inverted index in in the previous data structure in 1.17 you can choose to stay on the old inverted index you don't have to migrate but you won't get the 1000x speed improvements especially for these very large allow list queries but what you can do is you can choose to do a one-time migrate and that will change your inverted index and we'll convert it into this roaring bitmap data structure and after that it's a one-time change and after that you'll have a speedier filtered searches hey Zen um quickly so you said like we can stay on the old inverted index is it by like I can upgrade to 118 and still stay on the old exactly so you can you can upgrade to 118 and you can still stay on on the old inverted index or you can there's a migrate option where you can choose to migrate your uh your inverted index to uh using roaring bitmaps and then your 118 will actually you'll see the uh the Thousand x uh improvements yeah and of course like the the 1000x is really for like a large data set yeah exactly so my data sets have like a thousand objects it's like yeah so your allowance is of a thousand that's not so big when it's like nine million yeah yeah like let me show you an example so this is a a screen grab from uh what Etienne tweeted earlier there's a bunch of details here um that are not as relevant but what I wanted to point out here is let's say you have estimate estimated matches of where you're allow list is almost the entire database of a database of 10 million it's 9.99 million in 1.17 this query would take five and a half seconds the same query in 1.18 takes three milliseconds right and then as your allow list size decreases your your search time decreases but you can see that it's always an improvement it's just strictly an improvement it never makes anything worse because uh the the new data structure is much more efficient in um in the calculations that need to be computed that's everything that I have and I even see some examples because obviously like sometimes it's sort of like stays like there was like seven milliseconds before it's seven milliseconds now and then it's like like 1000 sort of like roughly because yeah going from 5400 that's like more than a thousand but then there's some others when it's not not as a thousand right there so but definitely I guess the key here is like hey you're working on 10 million objects or more like you want to get like a billion objects in yeah that could be super uh super useful yeah and I believe for this one the reason why it doesn't budge is because this is unfiltered search so it's not it's not you're not using the filter uh anyways in this oh yeah so it's fast by default anyway yeah good point good point I didn't look closely enough um but I think that there's like a couple of benefits to it like not only that the queries are fast but also if suddenly your users start throwing loads of queries that have like a huge allow list then you could overload your servers right like suddenly your servers are less responsive because these few people started asking very wide and Broad questions right like show me all those items over one dollar um yeah it's like come on please don't kill my server so like now there is a way to kind of like protect yourself from that in a way nice I love it um do we have any guidance as to how you do that one-off upgrade to to this because like why wouldn't you want that yeah that should that's in the dog so when you uh when you uh upgrade you can in configurations there is a migration option if you set that then your database goes into read-only mode and it's basically uh generating this uh it's converting the old inverted index into this new roaring bitmap implemented inverted index for at that point it's only read only so you can't add anything to it but once it's converted then everything goes back to normal and you realize the speed ups that are shown here yeah Okay cool so uh some comments before we move on um yeah so Rush sends us couple of rockets so uh I guess we we're going to the Moon um Euro has a question is this default for new classes so if we are on 118 and we create a new class um yeah so automatically we'll get that right yeah for new classes this is by default if you're if you have a class predefined before this one then you have to migrate over perfect well you don't have to choose to I should say you don't have to you can choose to yeah or even better you get to you get to get to right um yeah and at the end says hi so if we have like some really tough questions like we could always like uh ask him you know to help out or heck you know we could even add him to the stream and URI has a follow-up question so like how long does the upgrade take so like half a million objects do we have any idea anyone that's it that's in a TN question I guess he's he's run the experiments I'm not sure exactly how long the migration would take hey let's uh let's uh pray to to tuition you know you know can you send us an answer and I hope there is not much of a delay on the on the stream then um yeah but uh what Etienne is thinking and figure out um is that is I don't know if I interrupted you in the middle or if it was towards the end oh no this is everything I think this is my last slide yeah this is it all right I tie my root Interruption well this is super exciting and there's something by the way I want to add like um I was talking to some um uh somebody influential like recently I don't necessarily want to name them right now um but uh like uh they were super excited about roaring beatbox he was they're like do you know that you'll be like the first database to have half roaring bitmaps implemented and um it to me was like really impressive like because I was thinking like wow we really are at the Forefront of like that Innovation you know like somebody came up with that idea uh we we discovered it and then uh you know Bank you know couple of months later uh we have it implemented in viviate and then everybody else can can take full advantage of it you know just like just upgrade to the latest version um and then upgrade your uh index and and here we are this is pretty amazing yeah what's super interesting to me I I was watching the podcast with uh Conor and Etienne on the 1.18 release um this is not just we're not doing it the old way and then we have a roaring bitmap the the inverted index is completely now the internal guts are shifted so that it's implemented using the Roaring set data structure to have that radical radical of a shift and like that that just shows that it's super Nimble super flexible right we realize that there's a better way to do things boom done like the internal guts are completely converted that's that's so awesome it's amazing that you can do that you know uh without like introducing breaking changes or any of that right so um coming back to the original question from uh no no not this one oh missed it so like how long would this take so at the end run some tests and then he migrated 10 million objects and that that took 10 minutes so basically and that's just like on a local local machine well granted a tiens machine is super powerful um but but still it's like um you could sort of estimate you know half a minute maybe for half a million objects so it's not a not a hefty operation to do that um that could be kind of cool now there's a follow-up don't ask why I migrated 10 million objects locally I wanted to import for a demo case of bitmaps but accidentally imported into what 17. nicely done you know living dangerously at the end Living Dangerously perfect and we get a rocket from you right hey we collected three rockets today like uh I hope that by the end we are done with the final segment we'll have more Rockets even who knows all right so any other questions or should we move on I take the silence as a nodding as like the final segment cool so I am going to take uh the final segment and I'd like to talk about uh the backups for Azure so that was like uh one of the uh sort of last minute thing added to to the release it was something that was a little bit in the air and we we didn't know whether this is this was going to happen or not um but we we managed to to add it so marching our senior software engineer um managed to implement it just like few days before release so this is super super exciting so basically first of all what I want to highlight and then point everybody to is like that we basically cover the three major players uh Cloud players when it comes to uh your backup needs so you can store configure with it to create backups for AWS for GCS and then as of this release for Azure storage so you could go to our documentation so it's basically under reference configuration and backups and then you can find all the necessary information and you need you need to basically know stuff like um what is your backup Azure container and potentially you can add the backup Azure path and then basically recovering the the necessary um credentials and the environmental variables so you definitely need to provide like the the storage connection string and your storage account uh alternatively yeah you could also provide the storage key um but from my understanding I haven't tried it myself yet and then given that it landed uh pretty last minute um I wasn't able to to study just yet but this is super exciting because that basically means that if you are on Azure and then preview and then previously you felt like you were missing out because you couldn't set up backups on Azure well now you can um and then that this is super great and powerful so I hope um people will be excited um so this is cool and just to wrap up so if you're curious about this release uh go check out our blog post so basically on on the blog post if you go to our site this is the the first one and we are covering again pretty much the same details so maybe there's some information that we didn't quite get uh into maybe in this session but I believe that we covered most of it so you can learn about all of this replication cursor API backups etc etc so we cover all of this um and and a little like a selfie shout if you are um uh if you if you if you love wivate and you want to follow what we do uh you should definitely subscribe to our newsletter right like uh subscribe to our newsletter and I also like shared some uh cool things about the release uh but but also like you know we give shout to like the community uh so this was really really great to see uh this release actually took 18 different contributors to happen so this is a pretty big deal uh we had seven if I can't count well um one two three four seven new contributors um and then it was actually pretty cool to see like Alexander and and zenyon uh to uh contribute like from the go Community side which is absolutely amazing so if in the future you don't want to also miss out on Vivid air definitely subscribe to the newsletter because we'll give shouts over there uh and then we also share like interesting blog posts or podcasts and then in here for example you can watch a podcast with um Etienne and Connor that are talking about it so if you want to get like the um the perspective of the CTO on on this then this could be also a good place to go and learn more about it I shouldn't have done that perfect so let me just have a one final look uh on an additional note so we have something from Rush so can't wait to get my hands on the new version any details on how a WCS cluster can be upgraded to 118. so it's something that I actually discussed internally um and then at the moment if you're using like the freestyle boxes we don't offer an automated upgrade so if you if you do that like I guess the only ways at the moment is just create a new instance and migrate over your data maybe something that you could use the cursor API to help you with because you could like walk through the data to load to read in one side and then create a separate batch export to uh to do that maybe that could be could be a way but like right now we don't have like a a system to upgrade your instances just yet um and I think if you are a paying customer then probably Byron might be the best person to to check out like I'm sure you know who Byron is if you are paying customer and then he'll probably be able to help you um but that's at least the story for now and then etienne's final words at least it showed the migration works around our time for re-import before the demo but could migrate in time hey this is pretty awesome so yes it's definitely quicker to migrate um well and then re-index your data so unless we get any final questions any final Rockets I think this will be the content that we have for today what do you think team anything else from you no no Zen is thinking a lot of stuff was covered lots of improvements yeah so uh okay so in this case I don't see any final Rockets or any final questions so thank you all very much for watching this was super exciting I hope it was as much exciting as it for you as it was for me and for the team here um and I see like uh Zen is sending us a what is it a chocolate okay you're hungry okay let's uh let's wrap it up so thank you for watching I hope you all are excited uh we are getting some final Rockets So we collected six dollar Rockets this is a record so far at the end is sending us a trophy well a text works as well um thank you Carson thank you all I hope you enjoy Vivid 118 and then please please send us any additional questions over the community slacks subscribe to the newsletter watch our podcast you know be with us and if you have something exciting that you build with vv8 and you would like to join us for an episode maybe next month hey uh second Wednesday of the month we could be there so um yeah so thank you for watching thank you ", "type": "Video", "name": "Weaviate Air \u2013 Episode #6", "path": "", "link": "https://www.youtube.com/watch?v=NUlS-TNEH_0", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}