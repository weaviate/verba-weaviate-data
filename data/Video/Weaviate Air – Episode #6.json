{"text": "Jupyter notebook for the cursor API: ... \nforeign[Music]I have the whole crew in here we've gotErica I got done we have Zen we've gotConnor we've got me uh well Sebastianyou can see the neck well this way seethe name here and we are super excitedfor this Vivid air episode which isgoing to be focused on yesterday'srelease of vv8 118 and you may havealready cut a glimpse of the releaseblog post and read about it in thedocumentation but I thought the idea ofthis episode could be can we maybediscuss some of those features andexplain them or maybe we could demo someof those features you know just in caseif you didn't quite figure out yet howto use them we can maybe show some ofthose in actionso we are super happy to see you hereand thank you very much for watching andlistening and if you like this contentgive us a like because that always helpsmaybe more people like-minded like youand us can come and watch us so thiswill be super super exciting and thebonus of being here live like always isyou can ask questions as we go so um youcan ask questions oh you know like ifyou don't have a question just say hisay where you're watching us from or youknow just say you know some random factyou know just keep it more or less onthe topic and we are all going to befine so the main topic today like Imentioned is we did 118 and we have abunch of really cool things that we wantto talk to you about so first we'll talkabout cursor API so Dan will be coveringthat and I believe that as a a nicelittle demo that goes over it then Conorwill be talking to you about want and weshould probably have a whole discussionsaround it Erica will be in charge oftalking about like the new stuff aroundbm25 and hybrid you know the wear filterand the stop wordsum then we'll talk about the addition tohnsw PQ and then we'll cover like anupdate to replication with tunableconsistency and Repair on read we'll gointo roaring bitmaps and then finish offwith the addition to our backups forazureso without any further Ado let's startwith the cursor API done the floor isyoursthank you Sebastian hi everyone my nameis Dan Das kalisco I'm a developeradvocate.pv8and let me introduce you to the cursorAPI which we launched with version 118.the main reason for this is that private118 you could only retrieve about 10Kobjects from a database which meant youcouldn't really use it as a source oftruth but with version 118 we releasedthe cursor API which lets you paginatethrough all objects in the database in aclass so you can use Vivid as yourprimary databaseand the way this works is that we havean after parameter which is based onuuids and it works for both the rest andthe graphql apisyou pass an ID to that parameter andwith yet will retrieve objects thatfollow that ID in insertion orderso that means this API Works only forretrieving objects but not for searchingor filtersto demonstrate this I use the Vivid demodata set python packagewhich was also published recently by mycolleague JPand it has a number of data setsincluding Jeopardy questions which Iimported into a sandboxso we have the console hereand are going to demonstrate the graphqlcursor APIso we're getting Jeopardy questionobjectthat starts after the empty string whichmeans we need to return the firstobjects object it hasand we also request the ID among theother fields and this ID will help usresume getting the objects effectivelydoing paginationso you can notice that we have a limitof three objectsand this is the second oneso if I'm going to copy this ID andpaste it here then rerun the query weexpect UV to return objects after thisone so starting with the last objectthen it almost hearty as the answer andwe'll see two more objects from theJeopardy question class so let's see ifthat's what's going to happenand that in happened as predicted sothis is the graphql usage of the cursorAPIand here we have a jupyter notebook thatshows how we can use it for the rest APIas welland the code is um simply a while loopin which we call the objects API withthis after parameterand again it starts with MP string thenwe fetch the idea of the last objectthat was returned and repeat the loopand there is a setting here interval set100s every 100 objects to display someprogressand you can see that umuh the client retrieved 100 objects inabout 0.6 seconds this is way slowerthan typical because it's a sandboxand it eventually finished with readingall objectsand um graphql functions in a similarway so we have this query in which weinterpolate the after parameterand we pass itvia post request to the V1 graphicalendpointagain pick up after from the lastreturned object and repeat the loopand graphql retrieved all the objectseven faster than the rest API abouttwice as fastso this was the cursor API and you canread more about it in our 118 releasenotesright hereperfect this was uh pretty cool and doneso basically what was the idea of thislike soum with the cursor API is the idea to beable to go through all of the data likein a really fast manner without likewriting complicated queries or what'sthe what's the ideaand the main idea is that now you areable to access all objects in a classwell previously you are limited to 10objects for a query so users had tocraft these artificial wear filters tokeep patient 84 objects but now yousimply indicate the last the ID of thelast object you received in the afterparameter and you get the next limitobjects so that lets you dump allobjects in database or use it as yourprimary source of Truthnice nice this this is cool yeah and umdefinitely like looks like a superefficient way to like access a hugeamount of data because I I remember inthe past like if you just wanted liketest out few objects this wasn't as fastso yeah even like on the on a Sandboxenvironment uh like the speeds werepretty impressivethis is very fast because there's nofiltering you simply fetch objects inbatteriesnice nice so and then is the idea of thecursor API that because you have thisafter you almost like go like here's mypointer start from here and kind of likefly in that direction is that where itcomes upyeah yeah that was my question is umfrom the uids how are they sorted sowhen you go from after it takes you towhere are they like in order of how theygot into the database like it's after ittakes you from this uuid how how arethey sortedso they are in the order in which theywere uh inserted in with gate in theorder of creation interesting yeahthat's pretty coolnice so I guess like even as the as youquery the data and you modify the datayour cursor kind of stays consistent inone place while if you are running likespecific queries suddenly the next pagewould have could have changed right ifsomething underneath changed yes becausethere's some ideas which are immutablethe order of objects returned by uh thiscourse or API is guaranteednice niceyeah I love it I love it yeah thanks forthe demo this this made it pretty uhpretty uh to the point and then and andnow I know how to use it like uh I can'twait to um like I I I almost said that Ican't wait for this episode to end butlike I've this this is definitely goingto be something I'm going to test outafter we go offline so um yeah thanks alot for the demo done and um Conortell us about once should I show yourscreen as well yeah cool is it showingnowit is I got the uh because I have thisgoing full screen so I can't seeeveryone else anymore all right so I'mgoing to be talking about wand scoringand there used to be this meme onTwitter that was like making gpus gobird and talk about like runningsoftware fast and so I had to borrowthat for talking about like somethingthat makes something faster hey so isthat that's like a bird like like a fastcar or a burger I'm coldI guess it's like the sound of themachine like you imagine this likeright like a pairing cut machine that'sokay I get that nice all right so thetldr of this is is with with bm25 we'rescoring each of these keyword terms andyou only really need to score the uniqueterms to most of these queries so that'sthe tldr is you know if the query is howto catch an Alaskan pollock you probablyonly need to score Alaskan pollock youdon't need to score how to you know andthese kind of things soso let's dive into the details of howyou calculate the bm25 scores and uh soif you see my mouse like K1 and B theseare just like constants like constantvalues that are used to kind ofstabilize the algorithm you might tunethem but let's kind of ignore that uh sothe key things to look at here are sothis is the score of a document in thequery is you Loop through the terms inthe query and then you have the IDF forthat query term the Q sub I and then youknow the frequency of the query term inthis document you know normalized by thesame score and then by the um the lengthof the document divided by the averagelength of all your documents and thenyou also have this IDF thing this IDFthing is the most important thing welllet's just quickly explain why it is themost important thing so with with thisother part of the equation when you'redoing the uh the frequency for the queryin this given documentthis the key thing here is going to bethis document length and the averagedocument length so what that looks likeis let's imagine like one of ourdocuments is just pawlik it's just oneword this is like the most extreme casewhere you're kind of like gaming thatcardinality of D thing so you end upwith one over 50 and so you know pawlikappears one so it's one over one plusone over fifty and so again we're goingto ignore these constants just to getthe key ideas behind like what changesfor each queryso you end up with 1 over 1 plus 150Which is less than one so you're alwaysgoing to have some value that's lessthan one and thus the IDF is reallywhere the payload comes in because youknow you're only getting at most justone which is going to be the same as theIDF from this part of the bm25calculationall right so the interesting thing isthe IDF so the key things in IDF are nthe total number of documents you haveand then end of the query term which ishow many times this term appears uh inthis umuh overall in all of the documents so assome examples let's say like you knowjust we have 20 documents and you knowhow appears and so this is like theinverted index you have how to catch nand these are the document IDs that likethat where it appears like justnumerically so you know to sum it up howappears eight times two twelve catchfour and ten and then Alaskan only twiceand Pollock only twice so when you plugthat into the you know n minus how manytimes it appears over how many times itappears you end up getting like for therare terms 20 minus two over two is ninewhereas for the common terms like twoyou get like 20 minus 12 over 12 whichis two-thirds so this is these this isthe IDF of each of these terms how muchcan it possibly contribute to the vm25score so once you've been scoring thisquery how to catch an Alaskan pollockyou have a minimum threshold so youscore pawlik it adds nine you know to tothe the documents that had pawlikAlaskan then adds another nine and thencatch adds another four so then there'slike a minimum threshold right and solet's say it's four now the minimumdistance after you've scored this manyso now these remaining terms how and twothey can only possibly add 3.17 to thescore so the search is over so all inall what this leads to is just less uhscoring calculations for scoring thebm25 and thus a faster you knowcalculation of bm25 scores with thiswand algorithm where you're using thisminimum uh threshold to prune it and sayno forget it it's not possible for thisto add more to the scoreso also you can check this out if you'rereally interested in all the detailsbehind exactly how this Implement isimplemented uh here's the link to theimplementation and goingcool so that's right hey Connor I've gota question so if you go backum well yeah here oh no the the next onesowhen when does this happen where youkind of like score like Pollock Alaskanin cash that's like when we score likethe the query level even before thequery beginsyeahso you can sort them by uh so so youhave these lengths and so you sort themin reverse order because you know it'slike inversely the less it appears themore it can add to the score so yeah youfirst you sort it and then you can uhcompute the minimum threshold so exactlywhat how this is done with with the topK I'm not exactly sure of that detailbutbasically you sort it and then you canlike calculate what that threshold wouldbe based on having because you you uhcalculate these numbers by you you wouldhave this number from the length of theinverted index basically oh yeah sobasically because I'm speaking of likeobviously um I'm new still to the fieldand everything right so I'm basicallytrying to figure out from the point ofview of so when does it happen so doesthis happen at the time when like weimport our data and we create this indexokay yeah yeah no change to event isneeded it can this can just build righton the inverted index that you alreadyhave you don't need to say upgrade V81.18 and then re-import all your datayou can just 117 or however to 118 andthen just or I guess 117 is when bm25came out but anyways so like 117 to 118and you can just use this you don't needto re-indexoh cool cool yeahnice and then like with this basicallythe bf-25 surge is just like smarterbecause it's just going to ignore wordsthat just like keywords that don'treally add much to the results rightyeah like yeah you have to how I waslike yeah that's basically every singledocument I have rightyeah exactly it's nice it's the cool I Ihave a question so is my understandingcorrect that let's say you have a hugequerythis is basically picking out the rarestwordsuh in that query based on how rare thosewords are in your database and then youorder them from rarest to most commonand then you say if I've got this manyrare words don't look for any othercommon words and that removal of allthose common words from the scoringprocess is what speeds things up is thatis that the correct understanding yeahthat's exactly it and I thinkumwell you you said something with thatthat made me question my understanding alittle bit because if you well oh okaysorry so yeah yes exactly because you doit with the query if you have a this ishow to catch an Alaska right like sixwords so it's pretty easy to visualizeit I don't know what it was about youjust said that inspired me to thinkabout like querying with like aparagraph but if you're querying withlike an entire paragraph I mean thatwould be an interesting test to run Iguess I haven't really done that kind oftest but that would be like a stresstest of this right yeah yeah because ifyou have like let's say 20 common wordsthe sum total of that could end up beingpretty high that it needs to keep doingit until yeah yeahyeah now this is cool right yeahthat's why I like we be there becauselike we're teaching to each other andthen suddenly it's like wait what likelet me ask questions yeah we shouldalmost have like a follow-up sessionjust like now testing out ideasyeah nice nicecoolum did you did you have anything else toadd or what's what's happening nextoh uh yeah this is all I have for uh thewandoh perfect thanksum so I want to give give a quick shoutjust uh before I hand it over to youErica so like uh Carson uh is uh sayinghi from casual in Germany uh so he'slooking forward to using we with 118.it's like um yeah like I'm super excitedto hear that and let us know how it goesand uh Russ says great stuff so um yeahthanks rash thank you for encouragementso um yeah anybody else just like giveus a shout and of course right at thebeginning Connor was like stepping inthank you for watching I think that yousend it before even I got to say itright so he's such a pro Conorall right so moving on Erica I'm kickingthe Baton yeah let's do it right so youwill be talking now about bm25 hybridimprovements which is pretty close tothe topic of one than the stuff thatConnor was explaining right yeah exactlyso bm25 and hybrid was released in 117and yesterday we released 118. butreally what we were focusing on was howcan we improve bn25 and hybrid so Conorwent over one and this the simplest wayto start using wand and really apply itto large scale data and applications isjust by upgrading a repeat instancewhich is exactly what Connor said soit's kind of cool that it's that simpleto use it and see the performanceimprovements um but what I'm coveringtoday specifically is we added instopword removals um it's now indexedwhereas previously it was not and thenalso added we added in the wear filterto bm25 and hybridum I guess you can share my screen I'mstill talking a little bit to like kindof built it up but umit ignored the stopword configurationand scored all of the words so as youcan imagine if we are queering with alarge paragraph and it hasthe and what many times it it will bevery slowum so ignoring them speeds up queriesthat contain stop words as they can beautomatically removed from queries aswellum and that's obviously important forstop words so here I'm using the podcastsearch demo that I showed previouslyum Conor a GitHub repository and usingit but what I'm showing here is that youcan use the brask API to update yourstop birds even after your data hasalready been indexedum oh wait so are you saying that Ialready have an existing schemaand then I can update it oh neat yes yesum so here I have added in so my dateI've already uploaded my data it isindexed I have my schema defined but nowwhat I'm showing is that we are addingin a few additional soft Words which iswhat the an is and this preset is set toEnglishum but you can also have none and thenadd in additionalum stop words in here but then you canalso add inoh no I removed it really quickum you can also add in hey this is howwe navigate through our documentationjust live a little previewum so here you can also have removal soif you do want to include a or the thisis where you would Define thatall right ahit's like the opposite of ignore it'slike included too yes exactly all rightso I'm going to uh run this and then I'mgoing to head over to my B1 schemarefresh it and what you can see is I theadditional is what and as you can seepreviously I had added in B and is butbecause it's already included in theEnglish presetum the only new one that was added in isjust what and that's reallyErica could you zoom in a little bitjust just to make it bigger yeahI'm so sorry this isthe preset of Englishhey does that mean we could um mess upwith Connor's example and like addAlaskan and Pollock to to the list andthen he would never get the results everagain yes that's a good secret can we doitwellyou need the database the database tohave something about Alaskan pollock init yeah as in We're Not Gonnanecessarily test that query right butlike I just want to see how easy it isto just add additional words now nowthat you've done ityeah so here I've added in beer andrelease because I know that's talkedabout frequently on the podcast and thenI'm just gonna run this again and thenrefreshand I could see that it was added in seethe stalker variableum so yeah the results would it be inyour favor if you accidentally did thisyou're just too nice for Connor like youdidn't want to mess up with theseAlaskan Bullocks huhbut this is cool how fast is this updateso can you can you tell us like whathappens underneath right like when weadd these additions do these keywordsget removed from the sparse vector orsparse is that the right word as wellinspectors well now they're stop wordsbut it's different from 118 is that theyare indexed but when you are creating abm25 they areumremoved for relevance ranking because ifit's going to rank the and and it's kindof going to boost results even if that'snot really what you're looking for sothe key here is just kind of to removethose um like the relevance ranking forthe in and like or beer and release orwhatthis example sounds good yesum all right so and then the next thingthat I want to show is the wear filterso we have finally added it in for bm25and hybrid but just with this exampleI'm taking the Pod clip and I'm asking aquery on what were the features releasedin 117 and just similarly um to 117 thisworks right but what is new is the wearfilter which is great I will showthe ticket that had like I think 30episodesum so what do I want I know that Conoredian and Parker talked aboutumthe 117 release and podcast 31 so I willjust narrow it down to the podnumsorryokay and then operatoris equal and then just my value int of31.so now as you can see all the Podnumbers are referring to podcast31 andthis is where we have Connor speakingConnor's speaking a lot oh but then wehave Eddie and as well that is relatedto talking about the features that werereleased in 117. so you can see that themain focus for that podcast was talkingabout replication and hybrid searchum yeahwow so um we basically combining threekinds of search uh like filters right sowe have Vector search like we have thebm25 search because of hybrid and thenwe have scalar searchtimethat's that's pretty possibilities yeahand what I wantedthe Highlight here is that this featurewas built off of a request by someone inthe community a result in Federerum this had 34 uploads which is insaneso I just want to make a point that ifyou want to contribute to the featuresthat would be included in 119 or even120um you can head over to the docdevelopers docs and then on the bottomhere we have the roadmap then you canclick on itand you'll be taken to the GitHubproposal and then you can upload it soplease this is the time as we'replanning for 119um your voice matters absolutelyI mean I I can give you some like asecret uh Intel that I know like so atthe end our CTO uh he's actuallyplanning to like a look at some of thoseitems in the backlog and then some ofthose more popular ones would definitelyget a lot more attention from at the endso very much like Erica said like we arevery much like roadmap at feedbackdriven and if you feel like there'ssomething that's very important theregive it an upvote give it just like thatright Erica believes this one is veryimportant right so uh yeah just justlike Erica said but like my pony is likeyou can do it like right now like eventhis week because you could influencewhat will go into 180 19 or maybe 120.so um there's no time like now rightcool this is pretty pretty amazing thankyou for the demo Erica anyone have anyother questionsone question I had was how does theremoval of stop words work with the umthe Rarity of words and then ignoringthe non-rare words because I wouldimagine the stop words would be thenon-rare words and then if those areremoved then whatever is left overdoes that does that make Connor's Pointeven stronger because now you don't havea lot of garbage uncommon words stopwords takes care of that now TheUncommon words are evenum they're they'reI guess removing those is even morepowerful now right am I understandingthat correctly orI seeI mean yeah it's like in the example ifwe remove the bottom three now the leastcommon I guess would be catch right orthe most common that has like the lessweight would be catchy so it really putsthat focus on Pollock and Alaskanah so the these two features are kind ofsynergistic because the the the wandupdate doesn't need to take uh take careof a bunch of common words the stop wordupdate gets rid of those and then youreally focus in on the most importantword foreignand correct me if I'm wrong anybody herebut like it's almost like it gives us anability to kind of uh say which wordsshould be completely ignored but let'ssay you have a furniture company uh sothen what Conor was explaining matchingto a chair because basically chairs willprobably be mentioned like once every 10times like we have like an item in thecatalog so magic to watch wouldn't be soimportant because of want but at thesame time we still still will be avaluable match right but then a specifickind of chair right like that could havelike a you know just like Ikea and othercompanies are doing right like that theyhave a name so because those would berarer like in terms of how often theyappear they will appear even more but Ithink with like the the stopwatch itselfyou could kind of go like yeah justignore every time somebody says woot oror something like you could even havelike terms they go like yeah like Idon't value these mattress so much orlike at all and I think that could beaway right yeah that's a good way ofputting it the con the stop wordsfeature gets rid of generally commonthings and then the uh the part that thetfidf part that Conor was talking aboutthen after having removed the generallycommon words now it says of theinteresting words which ones are rareand which ones are common so it's evenyeah that's a good way of putting intoquestion yeah totally so you know likeum I'm sure if you haven't been to Ikeaor buying a bookshelf so nothing tobookshelf like you give you like a smallscore but if you say Billy like yeahthat would get like a pretty high scoreright like uh and uh I'm sure thatanybody's ever been to Ikea knows what'sa daily bookshelf bookshelfno interestingmaybe I have too many booksgreat great this this is amazing this isgreat and um I love the addition of ofthe wear filterum that's definitely been very neededand uh yeahperfect so any anything else that youwant to coverno that's all online I'll pass it overperfect soum who have we got next so we have hnswand PQum who was doing that that's meif you can share my screen absolutelythere you go all right okay let me showyour face againall right so let me jump into it so 1.18adds this uh new Vector compressionfeature called Product quantization andit works with uh our uh our hnsw indexand it essentially makes it so that youcan store your vectors in memory withone-fourth the requirements thatone-fourth can change depending on theparameters for PQ that you set so I wantto give just a quick five minute introto what PQ is and how you might be ableto use it okayso imagine that is one vector thatyou're storingso this can be any any dimensionalVector okay what PQ does what we doright now is we store this vector and wehave millions like this that are beingstored in memory which takes a lot ofmemory what PQ allows you to do is thevery first thing you do is you chunk upyour vector into n segments that's oneparameter so you choose whether you wanteight segments or 100 segments the upperlimit there is the dimensionality of theof the vector of course you can't havemore segments than you have dimensionsuh then you takeeach segment and let's just have a lookat the first segment for now you do thesame thing with every segment but let'sconsider the first segment and let'stake the first segment of all of yourvectors so I've just drawn two vectorsthere but imagine these are all of yourvectors you take all of the firstsegments of your vectorsand you plot them out right so the firstVector is there the third Vector isthere and then all the other vectorscontribute their first segments as wellnow what we're going to do is instead ofpay attention to where each Vector islocated which takes a lot of space we'regoing to create neighborhoods of vectorsso I'm going to create this neighborhoodwhere these two vectors live togetherthis neighborhood where only this Vectorlives this neighborhood where thisVector lives and then so now that we'vegot these larger neighborhoods theseclusters of vectors instead of definingthe vector uniquely using itscoordinates here what we're in set goingto do is Define these neighborhoodsusing their centroids and this is wherethe second parameter comes in which isKate you can decide how finely you wantto segregate these uh vectors by sayingI want a lot of neighborhoods I want athousand neighborhoods or you can say Iwant 20 neighborhoods so there's goingto be very very large neighborhoodsyou're going to have a lot more vectorsper neighborhood but the whole idea isthat every neighborhood gets its owncentroid which has a coordinate and ithas its own ID now instead of referringto a piece of a vector this firstsegment of vector one by the coordinatesthat are unique to it we're insteadgoing to take those coordinates andwe're going to replace it with thecentroid ID for this centroid so anyVector that lives in that neighborhoodwill now not be referred with its uniquecoordinates but rather with a centroidID so instead of having to store all ofthese numbers now you're just storingthe name for that centroid and this iswhere the memory saving comes in so ifyou have your database to begin withyou've got right now this is what youhaveEQ is going to chunk that chunk this upand it's going to do the samecalculation over and over again forevery segmentand it's going to turn this firstsegment here into a centroid ID thesecond segment here into a centroid IDand if you implement this over yourentire entire databaseyou'll have just all these segments nowchunked up into centroid ID so insteadof having to store numbers here you onlyhave to store uhcentroid IDs insteadand if you understood none of that letme just abstract all the details awayand let's let's uh get uh down to justthe intuition let's say every Vector isan address that you're storing so youcan have the house number the streetnumber you can have which uhneighborhood I'm living in what stateI'm in what country I'm in so on and soforth and then all the way at the topI'm living in on planet EarthPQ allows you to zoom out and say Idon't really care about the house numberI don't really care about the streetnumber I only care about the state thatZayn is living in and now you're goingto have a bunch of people that live inthe same state so you have a course orrepresentation for each Vector but thatcourse representation allows you to saveon memory requirements right so some ofthe experimentation that we've doneallows you to save one save and storethe vectors in 1 4 the memoryrequirement so that's one fourth the rampreviously required so that's a speed upbut it's it doesn't come for free rightso the the one thing that you have tobalance when you're using PQ is that thelower the more you compress the vectorsthe lower you get the memory you'repaying for that in terms of recallbecause now you're summarizing vectorsin terms of the neighborhood that theylive in using those centroids right soit is a balance betweenhow much recall you need versus how muchmemory uh savings you want to you wantto realize so that's a quick summary ofuh of PQ and that works with hnsw youcan look into documentation and bydefault we don't enable PQ and it'sexperimental right now but if you wantto play around with it you can enableset the enable flag underneath PQ in theconfiguration to true and there's abunch of other details and settings thatyou can play around within theconfiguration you can read more aboutthat in the docs and Sebastian will addthe link to the docs in the thedescription of the video as wellso I guess if we want to find like abalance of the higher recall and lowermemory is there like a how could weoptimize K where we could have like abalance between the twouh so it would depend on the size ofyour vectors really because the thelower K is you're basically saying if Igo back here you're basically saying ifyou decrease K to one you're sayingidentify the address of a person uhusing one centroid and that'll just beokay this person lives on Earth which isnot very helpful and so the recall isgoing to take a major major hit it'sgoing to be horrible but then you couldsay Okay I I can afford to have morememory so now I'll zoom in further I'llI'll give everybody their ownneighborhood I'll give everybody theiraddress so it's a there's a work there'sactually a blog post that's going tocome out next week around the details ofthis so you can read more about thebalance between memory saving and recallum there is a cut off that we'll mentionthere where you can compress it down andthen to a certain degree which is thisone-fourth after which the the price andrecall that you pay is not worth itokay so there's like a diminishingreturn at some point exactly it doesPlateau off so that's a that's the goodthing so you can keep on compressing thevectors and then you when you see recalltaking a very big hit that's when youknow where the cutoff is thanks lovedyour explanation and this circle it'slike okay how many kids great job yeahyeah I want to also hop in and firstlysay the slides were gray I love thewhole presentation I I love how you putthe uh the chunks into the points aswell that was awesome we go back to thatslide actuallywith the uh with the two going into thepoint yeah I love how you animate it inthe regions that was awesome yeah I waslike if I have a video for this that'dbe great but I don't have a video solet's just make a PowerPoint video soyeah we should turn that into a gif andlike tweet it yeahum the funny thing was like wheninitially you had just like those twoarrows pointing out uh like and theywere not pointing at anything I was likewhat are those hours pointing at and Iwas like wait wait those are vectorsthey're not just arrows right yeahany dropsthey're not arrows or vectorsum hey so you mentioned at the beginningthat is the other two parameters or isthere one parameter like is there K inthe dimensionality we want or yeah so Nis a parameter here how many segments uhyou want to yeah cut the cut the vectorup into and then for each one of thosesegments the second parameter is thisguy right how many centroids to look forfor each segmentyeah so so like almost okay so let methink about it so if I have my Vectorthat has 1024 for dimensions and I couldsay like I want to shrink it down to 32Dimensions right so that's the end andand uh value yeah would that mean that Iwould end up with 30 yeah go on for for1024 dimensional Vector if you want eachone of these segments to be 32dimensional then you would have to set nto 1024 divided by 32. and N has to be aconstant multiple so that each you don'thave like a portion of a vectorDimension hanging in one segment and andthe other half hanging in the othersegment right so is there reverse whereif I want to actually have 32 segmentsso I have to do a calculation of was thenthen you have to do a calculation of howwhat the end should be yeah oh no I justwant 32 segments right soum okay so if I have done 32 segments doI get a centroid per segment or thecentroid is just calculated based onthis reduced Vector you get you get Kcentroids per segmentso every every segment is going to havethese vectors and then you calculatelet's say a 124 centroids per segment soeach one of these guys is going to thisis going to be 124 centroids this isgoing to be 100 in separate different124 centroids and you can also choosehow uh what that K is right one 124 or500 1000. okay okay because I I feltlike um the biggest memory saving is inin a space of like how much we compressthe the vectors themselves right so if Iinitially started 1024 and I shrunk itto 32 that's just like a tiny fractionright and yeah so here if you if youhave less segments then you willeventually have less centroids overallbecause here let's say you have 10segments and for each segment you have100 centroids you have a thousandcentroids in total but let's say youonly have two segments then you onlyhave 200 centroids so that's a that's acost saving in memory but then also howmany centroids each segment has if youreduce that that's also a cost saving soyou can take the memory down to beingnothing and you'll have no recall that'swhy it's a it's a it's a delicatebalance yeah and then okay so thatthat's this part and then later on whenI do run a query unlike on those reducedvectors yeahum so what happens like first we go to aneighborhood based on the centroid ofour query we pick all the candidates butthen the scoring and the distancecalculated based on the full Vectorrightso that is uh if you read the paper ondisk a n that's how it works but rightnow that full Vector is not uh storedum on disk that that might be a featurethat we Implement in the future but howdistancing calculations work right nowis you can take a vector and then youfind out which neighborhood that Vectorresides in and then you can calculatethe distance between let's say there's anew let's say this is a new query Vectoryou find out that it lives in thisneighborhood for the first segment youyou know the centroid you know thecoordinates of the centroid you cancalculate the distance between the queryvector and the centroid and you do thatfor every centroid in every segmentum another future work uh right now isyou can actuallystore the complete Vectorrepresentations on disk which is muchcheaper and then if you want to dodistance calculations you can bring theminto into working memoryand calculate exact distances if youhave the complete vectors but right nowwe don't have the complete vectors wejust have the compressed vectors if youenable PQcool cool yeah I always mix up thatdetail of whether the compressed vectorsare compressed and my understanding isthat they're compressed in memory andthen they're fully represented vectorsare on disk and then you would um youdecompress them with some error and thenlook for the nearest Neighbors on disk Ialways mix up that detail movie yeahfrom what from what I understand themain uh RAM savings come from the factthat your compressed vectors are storedin in working memory in Ram and the fullrepresentations are stored on disk soyou do your high level search in memoryand then once you have your bestcandidates then you go to disk you readthem in and then you can do morefine-tuned exact searching over thoselimited candidatesall right yeah so basically that that'swhat I meant with my question so yeahbecause that yeah the saving is likeinstead of storing 10 million 1024Dimensions big uh vectors I can start 10million 32 exactly big so that's like 32times less exactly uh memory footprintbut then once I found the initialcandidates based on the uh theneighborhood we can kind of like go inand then score them based on the fullVector right exactly yeah but but thenbecause the first trip is like superfastum we can kind of like uh take the heatof doing a discrete as a one-off andthen do that distance calculation yeahthis is powerful stuff so I have aquestionum and then and I realized there werealso questionsum from from stop words but uh we butfirst let's let's uh talk about PQ soRaj is asking uh well or first is verycool small question will PQ affect thetime taken for Vector search performanceon large indexes like 15 mil plusvectorsso it umVector search performance on largeindexesthis is so the the blog that we'replanning on releasing next week actuallylooks at uh latency considerations aswell but mainly the idea behind this isnot not to uh it's not going to takelonger or shorter but the main idea isthat you need less memory requirementsokay so PQ is not going toum it might take a little bit longerum because you have to do this uhreading in from memory type of thingthat I described which we're looking atin the future but the main trade-offhere is not latencyum latency and recall which is the whichis the main case and let me go back tomy slide here in in hnsw the maintrade-off is recall and latency here thetrade-off that you have to decide is howmuch recall do you want to trade forbeing able to save memory so it's morearound memory space requirements asopposed to time requirementswe've got also a question and probablywe don't have this right now right butum how do we know if we're planning onsharing some benchmarks this is what uhwe're going to talk about this in theblog post next week where we do a deepdive on PQ so look forward to that andthen we'll also have more details thenyeah so thank you for questions Raj andvinod and uh yeah just just keep keep aneye on our blog post then uh because uhas far as I I I remember as well theplan is to even post two separate blogposts on the topic because there's somuch to talk about uh but thank you forsharing the questions because that alsohelps us shape some of that content andum yeah next week Tuesday we should havethat the first blog post from the seriesum so this is really coolum yeah any other questions around PQbefore we move onthere you gothanks rashford yeah no worries uh thankyou for asking this is why I love thiswhole session being livecool okay so let's go back toum the topic with stop wordsuh if we are done here and so we had aquestion well first this is not aquestion you know this is a shout out souh thank you we really appreciate itmakes us even happier uh to to work onit and I would definitely pass it onwithin the companyso let's look at the actual question soare there any performance trade-offs toconsider when adding a large number ofstop wordsdo we do we know or is that a questionwe should probably get back to laterthat's an interesting question but alsouh thanks for the compliment and likeSebastian said it'll be shared with theteamum butum I'm not sure I mean I know Connor isworking on the beer benchmarksum and maybe there is a data set withinthat that has where you can add in alarge number of stop words and then kindof compare the performanceum but previously but the demo that Iused for a podcast search I was lookinginto the natural questions which iswithin the um beer data setum and it's kind of tricky because therearen't really that many stop words andthe performance I didn't really see adifference when I've removed it butmaybe Conor has something to addto thisum I well I think it's I I think thekind of customized stop wordsis probably best for like a multi likelow resource languages languages whereyou need that kind of domain expertisethat's probably where I would where Iinterpreted as having the mostapplication umbecause you'd be tuning a really longlist of words to try to like speed upthe beer benchmarks particularly yeahI I think what we're looking at isbecause maybe the stock was bythemselves are not about like makingquerying faster but the real truebenefit is can they make your resultsbetter rightbecause By ignoring the wrong stuff likethe stuff that you don't wantnecessarily to be part of the query youcan get better results better resultsbut maybe and that's something maybe wejust have to test out and we can comeback next time around as vaccine likehey what did what is what would happenif I had like a thousand stop wordswould that affect the performance rightso not so much like would that make itfaster but would that could thatpotentially make it slower that would bean interesting thing to figure outand I'm sure if atien was on the call orlike somewhere from engineering theywould have had the answers straight awayso maybe next time we should have themon onlineCoco all right thank you for thequestions and you know like this is partof being live you know sometimes thereare questions we don't have the answersstraight away but uh we can alwaysfigure out maybe you can add the answersto the documentation uh once we figureit out so that'll be coolperfect so who have we got what have wegot next so we coveredum bm25 hsw and then now we will talkabout the update to the replica yeahreplication update with tunableconsistency and Repair on readum I believe that's done right thenthat'll be me indeedso you need to share your screenoh yeah I can show the documentation uhthere's no demo but um I can show a fewGraphics that hopefully will help youunderstand replication especially if youhaven't been following along uh weintroduced application in the previousversion 117and in this version We enrich the futuresetso to help you understand what happenedin this versionum I'll briefly go over what replicationis essentially you can replicate anobject in a class to a number of nodesso in this diagram you can look at theum the class A for instance let's sayit's a for article and you see that outof the six nodes here three of them havethis green markerso that means every object in the classis replicated on three nodes and threeis the application Factorand the replication is useful for anumber of use cases first being highavailability if a node goes down yoursetup still functionsand then you can linearly increasethroughput by adding nodes you can alsoupgrade with no downtimeor you take one node out and the otherswill still process and return resultsoriginal proximity is obviously helpedand umwhen you build a distributed databasewith the application you have to choosetwo out of three propertiesone of them is consistency which meansyou get the latest version of an objectthat was written on every read There isthe address availability it means youalways get a response even if some nodesare downand the other is partition tolerancewhich means the system will stillfunction if nodes individually do notand uhthe umthe trade-offs between consistency andavailability is what you cancontrol all operations now in version18. so this is called tunableconsistencyand there are three values for thisthe first one is one which means that aread or write return as soon as one ofthe nodes respondsQuorum which means that your requestwill return after a majority of nodesrespond majority means half of thereplication Factor plus oneand then finally we have all which meansall nodes in the replica set mustrespondso we need to use this all for schemeoperationsto ensurefull consistencybut for reads or writes over the restAPI you can choose between one column orallfor graphql we do not supportconsistency that's tunable yet but thismight be implemented in a future versionand to visualize how this workswe can look at the Quorum consistencylevel so in this damn you have thereplication coordinator node thatreceives a request from the and thissends it out ton nodes the replication Factor so hereis threeyou have three arrows going out and youcan notice that only two arrows comeback so that means a majority of nodeshave responded the replicationcoordinator node is happy and Returnsthe result of the clientuh there are a number of scenariosfor consistency versus availability thatyou can choose fromso Quorum and Quorum for reason rightwill give you a balanced write andrelatency or if you need fast rights andslow reads you can choose one for rightsso you only wait for one node to respondbefore the right is acknowledged andthen umall reads means that you have to waitfor all the nodes to respond before youget the resultor if you need the fast reads you invertthose so you have the all consistencylevel for rights and one for reasonso that would be tunable consistencyand then we have repairsso imagine a scenario like this um youwritean object with a consistency level oneand that note dies before it canpropagate your object to the other nodesso in that case when you read the objectagain with the consistent consistencylevel one it's possible that you'll hitanother node not the one that acceptedthe right so in that case you might uhnot get the object at all or if you hadthe patch operation you would get theprevious version of itbut if you had used consistency levelall for that read that means vv8 willwait for all nodes to return and it willsee that one of the nodes did have yournew object or the updated one while theothers didn't have itso then it would automatically propagatethe changes or the creation to the othernodes and it will return the latestversion so this is called the last rightwinsmodel for solving consistency conflictsand it's what movie it usesand we explain here in the consistencydocumentation pagewhat happens when you write an objectand then when you want to guarantee thatit has been propagated so essentiallyyou need to read it with a high enoughconsistency level compared to the onethat was used when writing the objectthat's essentially it you cannot checkout the blog release or replicationand here we cover both number ofconsistency updates and Repair on readcoolum I got a question so where do you addthis configuration or like where do youprovide like you know say like I want touse Quorum versus all or majorityum so this is used umin the rest APIuh for every right every read and everyright endpointwhen you get when you create an objectyou can specify the consistency levelhereah coolis is there a way to change the defaultbehavior I'm not sure how if you wentover thatum the default is Quorum I should havementioned this and there's no way tochange that you change it per operationokay okay all right I'm supported by allnodes sorry all endpoints which meansalso creating cross referencessupported again you can specify theconsistency level here and also forbatch operationsinterestingCocoperfect any questions from maybe fromthe audiencehey we got a questionso uh Dan what replication protocolalgorithm does we would usedo you have the answerwhat replication protocol does it that'ssomethingum pretty low level I cannot answer itright now but I'll get back to the useron slack if they are there and we cannotpick a documentation as wellyeah that would be a good question likeyeah if you're part of like our slackmaybe drop it on General and then wecould definitely uh come back to itthank you for the questionanybody else any other questions in hereor from the audienceperfectum it definitely shows uh who wrote thedocumentation on this because uh you youare so good like walking over to thecontentit's almost as if you wrote it yourselfmy umnice nicely done cool all right so whohave we got nextso on the list of topics uh we haveroaring beatbox and I believe that isZenZen I'm going to share your screenand you can take it away sounds goodokay so roaring bitmaps areum we've implemented implemented a newdata structure in our inverted index sobefore I kind of drown you in jargonlet's start off at the topso what do these doum so they essentially speed up filteredsearch up to a thousand X and theThousand X is the attention grabber hereWe've ran some uh experiments wherewe've got a thousand X Improvement thenwhat the heck are pre-filtered searchesso pre-filtered searches are basicallythink of let's say you're running ane-commerce store and you've got 10 000unique items that are up for sale inyour eCommerce store a user comes in andthey ask forthey have a search query they're lookingfor furniture you're not going to runthat query against all 10 000 uniqueitems that you have in your store you'regoing to pre-filter the search such thatyou only look for look at items let'ssay 2 000 items that are in thefurniture uh category so you pre-filterthe search such that you don't have tolook over 8 000 items you drop those outat the at the outset so how we V8 worksis we have an inverted index that has areference to every object so it'sessentially just a list of unique uhitem identifiers and then there's thehnsw index that we talked a little bitabout when we were talking about hnswpqwhich allows you to do Vector searchapproximate nearest neighbor searchso the inverted index is what we'vemodified with this roaring bitmapsupdateand what it essentially allows you to dois the inverted index is now implementedusing the rowing set data structure andthe efficiency that this unlocks is ifyou've got a filter let's say you'relooking for items that are of furnituretype in my in my store and above aparticular priceuh those operations are really efficientwhen done using this new data structureand the other thing the other speed upis that because we natively implementthe inverted index using the Roaring setdata structure we don't have to convertinto the new roaring set data structurethat when we have a filterso we save on latency of of thatconversion as well rightand mainly where we see that thousand XImprovement is if you've got a filteredsearch where the allow list theallowable or the interesting items tosearch over so in my previous example ifthe user had a question about furnitureand they were searching over Furniturethe allow list would include the 2000Furniture items and not the 8 000remaining other items like clothinghygiene products all that stuff right sothis allow list where you get that 1000xImprovement is if the allow list isbasically your entire database soimagine somebody goes into Amazon andsearches for opens up a filter and sayssearch for items that cost more than adollar your allow list is going to haveeverything in there because barelyanything cost less than a dollarso now your allow list is basically yourentire database this is the same asrunningnon-filtered search you're running everyquery has to go through every singleobject in your database which is reallyslow and the problem is that becauseit's implemented as a filter searchyou're still going to filter everythingand the filter is not going to have anyeffect it just adds a bunch of latencyand this is where the uh the timesavings are realized right if you hadpreviously a huge uh allowed list whereyour filter was all encompassing yourentire database we would first of allgenerate the filter which would beuseless but we would still do it andthen we would convert that filter andsend that to hnsw and it would searchover all of those uh all of thosevectors and that's where that's whatwould take a lot of timein terms of what this means if you'vealready got your inverted index in inthe previous data structure in 1.17 youcan choose to stay on the old invertedindex you don't have to migrate but youwon't get the 1000x speed improvementsespecially for these very large allowlist queries but what you can do is youcan choose to do a one-time migrate andthat will change your inverted index andwe'll convert it into this roaringbitmap data structure and after thatit's a one-time change and after thatyou'll have a speedier filtered searcheshey Zen um quickly so you said like wecan stay on the old inverted index is itby like I can upgrade to 118 and stillstay on the old exactly so you can youcan upgrade to 118 and you can stillstay on on the old inverted index or youcan there's a migrate option where youcan choose to migrate your uh yourinverted index to uh using roaringbitmaps and then your 118 will actuallyyou'll see the uh the Thousand x uhimprovements yeah and of course like thethe 1000x is really for like a largedata set yeah exactly so my data setshave like a thousand objects it's likeyeah so your allowance is of a thousandthat's not so big when it's like ninemillion yeah yeah like let me show youan example so this is a a screen grabfrom uh what Etienne tweeted earlierthere's a bunch of details hereum that are not as relevant but what Iwanted to point out here is let's sayyou have estimate estimated matches ofwhere you're allow list is almost theentire database of a database of 10million it's 9.99 million in 1.17 thisquery would take five and a half secondsthe same query in 1.18 takes threemilliseconds right and then as yourallow list size decreases your yoursearch time decreases but you can seethat it's always an improvement it'sjust strictly an improvement it nevermakes anything worse because uh the thenew data structure is much moreefficient in um in the calculations thatneed to be computedthat's everything that I have and I evensee some examples because obviously likesometimes it's sort of like stays likethere was like seven milliseconds beforeit's seven milliseconds now and thenit's like like 1000 sort of like roughlybecause yeah going from 5400 that's likemore than a thousand but then there'ssome others when it's not not as athousand right there so but definitely Iguess the key here is like hey you'reworking on 10 million objects or morelike you want to get like a billionobjects in yeah that could be super uhsuper useful yeah and I believe for thisone the reason why it doesn't budge isbecause this is unfiltered search soit's not it's not you're not using thefilter uh anyways in this oh yeah soit's fast by default anyway yeah goodpoint good point I didn't look closelyenoughum but I think that there's like acouple of benefits to it like not onlythat the queries are fast but also ifsuddenly your users start throwing loadsof queries that have like a huge allowlist then you could overload yourservers right like suddenly your serversare less responsive because these fewpeople started asking very wide andBroad questions right like show me allthose items over one dollarum yeah it's like come on please don'tkill my serverso like now there is a way to kind oflike protect yourself from that in a waynice I love itum do we have any guidance as to how youdo that one-off upgrade to to thisbecause like why wouldn't you want thatyeah that should that's in the dog sowhen you uh when you uh upgrade you canin configurations there is a migrationoption if you set that then yourdatabase goes into read-only mode andit's basically uh generating this uhit's converting the old inverted indexinto this new roaring bitmap implementedinverted index for at that point it'sonly read only so you can't add anythingto it but once it's converted theneverything goes back to normal and yourealize the speed ups that are shownhereyeah Okay cool so uh some commentsbefore we move onum yeah so Rush sends us couple ofrockets so uh I guess we we're going tothe Moonum Euro has a question is this defaultfor new classes so if we are on 118 andwe create a new classum yeah so automatically we'll get thatright yeahfor new classes this is by default ifyou're if you have a class predefinedbefore this one then you have to migrateoverperfect well you don't have to choose toI should say you don't have to you canchoose to yeahor even better you get to you get toget to rightum yeah and at the end says hi so if wehave like some really tough questionslike we could always like uh ask him youknow to help out or heck you know wecould even add him to the stream and URIhas a follow-up question so like howlong does the upgrade take so like halfa million objects do we have any ideaanyonethat's it that's in a TN question Iguess he's he's run the experiments I'mnot sure exactly how long the migrationwould takehey let's uh let's uh pray to to tuitionyou know you know can you send us ananswerand I hope there is not much of a delayon the on the stream thenum yeah but uh what Etienne is thinkingand figure out um is that is I don'tknow if I interrupted you in the middleor if it was towards the endoh no this is everything I think this ismy last slide yeah this is itall right I tie my root Interruptionwellthis is super exciting and there'ssomething by the way I want to add likeum I was talking to some um uh somebodyinfluential like recently I don'tnecessarily want to name them right nowum but uh like uh they were superexcited about roaring beatbox he wasthey're like do you know that you'll belike the first database to have halfroaring bitmaps implemented and um it tome was like really impressive likebecause I was thinking like wow wereally are at the Forefront of like thatInnovation you know like somebody cameup with that idea uh we we discovered itand then uh you know Bank you knowcouple of months later uh we have itimplemented in viviate and theneverybody else can can take fulladvantage of it you know just like justupgrade to the latest versionum and then upgrade your uh index andand here we are this is pretty amazingyeah what's super interesting to me I Iwas watching the podcast with uh Conorand Etienne on the 1.18 releaseum this is not just we're not doing itthe old way and then we have a roaringbitmap the the inverted index iscompletely now the internal guts areshifted so that it's implemented usingthe Roaring set data structure to havethat radical radical of a shift and likethat that just shows that it's superNimble super flexible right we realizethat there's a better way to do thingsboom done like the internal guts arecompletely converted that's that's soawesomeit's amazing that you can do that youknow uh without like introducingbreaking changes or any of that right soum coming back to the original questionfrom uh no no not this one oh missed itso like how long would this take so atthe end run some tests and then hemigrated 10 million objects and thatthat took 10 minutes so basically andthat's just like on a local localmachine well granted a tiens machine issuper powerfulum but but still it's like um you couldsort of estimate you know half a minutemaybe for half a million objects so it'snot a not a hefty operation to do thatum that could be kind of coolnow there's a follow-up don't ask why Imigrated 10 million objects locally Iwanted to import for a demo case ofbitmaps but accidentally imported intowhat 17.nicely done you know living dangerouslyat the end Living Dangerouslyperfect and we get a rocket from youright hey we collected three rocketstoday like uh I hope that by the end weare done with the final segment we'llhave more Rockets even who knowsall rightso any other questions or should we moveonI take the silence as a nodding as likethe final segment cool so I am going totake uh the final segment and I'd liketo talk about uh the backups for Azureso that was like uh one of the uh sortof last minute thing added to to therelease it was something that was alittle bit in the air and we we didn'tknow whether this is this was going tohappen or notumbut we we managed to to add it somarching our senior software engineerum managed to implement it just like fewdays before release so this is supersuper exciting so basically first of allwhat I want to highlight and then pointeverybody to is like that we basicallycover the three major players uh Cloudplayers when it comes to uh your backupneeds so you can store configure with itto create backups for AWS for GCS andthen as of this release for Azurestorage so you could go to ourdocumentation so it's basically underreference configuration and backups andthen you can find all the necessaryinformation and you need you need tobasically know stuff likeum what is your backup Azure containerand potentially you can add the backupAzure path and then basically recoveringthe the necessaryum credentials and the environmentalvariables so you definitely need toprovide like the the storage connectionstring and your storage account uhalternatively yeah you could alsoprovide the storage keyum but from my understanding I haven'ttried it myself yet and then given thatit landed uh pretty last minuteum I wasn't able to to study just yetbut this is super exciting because thatbasically means that if you are on Azureand then preview and then previously youfelt like you were missing out becauseyou couldn't set up backups on Azurewell now you canum and then that this is super great andpowerful so I hopeum people will be excitedum so this is cool and just to wrap upso if you're curious about this releaseuh go check out our blog postso basically on on the blog post if yougo to our site this is the the first oneand we are covering again pretty muchthe same details so maybe there's someinformation that we didn't quite get uhinto maybe in this session but I believethat we covered most of it so you canlearn about all of this replicationcursor API backupsetc etc so we cover all of thisum and and a little like a selfie shoutif you areumuh if you if you if you love wivate andyou want to follow what we do uh youshould definitely subscribe to ournewsletter right like uh subscribe toour newsletter and I also like sharedsome uh cool things about the release uhbut but also like you know we give shoutto like the community uh so this wasreally really great to see uh thisrelease actually took 18 differentcontributors to happen so this is apretty big deal uh we had seven if Ican't count wellum one two three four seven newcontributorsum and then it was actually pretty coolto see like Alexander and and zenyon uhto uh contribute like from the goCommunity side which is absolutelyamazing so if in the future you don'twant to also miss out on Vivid airdefinitely subscribe to the newsletterbecause we'll give shouts over there uhand then we also share like interestingblog posts or podcasts and then in herefor example you can watch a podcastwith umEtienne and Connor that are talkingabout it so if you want to get like theum the perspective of the CTO on on thisthen this could be also a good place togo and learn more about it I shouldn'thave done thatperfect so let me just have a one finallook uh on an additional note so we havesomething from Rush so can't wait to getmy hands on the new version any detailson how a WCS cluster can be upgraded to118. soit's something that I actually discussedinternallyum and then at the moment if you'reusing like the freestyle boxes we don'toffer an automatedupgrade so if you if you do that like Iguess the only ways at the moment isjust create a new instance and migrateover your data maybe something that youcould use the cursor API to help youwith because you could like walk throughthe data to load to read in one side andthen create a separate batch export touh to do that maybe that could be couldbe a way but like right now we don'thave like a a system to upgrade yourinstances just yetum and I think if you are a payingcustomer then probably Byron might bethe best person to to check out like I'msure you know who Byron is if you arepaying customer and then he'll probablybe able to help youum but that's at least the story for nowand thenetienne's final words at least it showedthe migration worksaround our time for re-import before thedemo but could migrate in time hey thisis pretty awesome so yes it's definitelyquicker to migrateum well and then re-index your dataso unless we get any final questions anyfinal RocketsI think this will be the content that wehave for todaywhat do you think team anything elsefrom youno noZen is thinkinga lot of stuff was covered lots ofimprovementsyeah so uh okay so in this case I don'tsee any final Rockets or any finalquestions so thank you all very much forwatching this was super exciting I hopeit was as much exciting as it for you asit was for me and for the team hereum and I see like uh Zen is sending us awhat is it a chocolate okayyou're hungry okay let's uh let's wrapit up so thank you for watching I hopeyou all are excited uh we are gettingsome final Rockets So we collected sixdollar Rockets this is a record so farat the end is sending us a trophy well atext works as wellum thank you Carson thank you all I hopeyou enjoy Vivid 118 and then pleaseplease send us any additional questionsover the community slacks subscribe tothe newsletter watch our podcast youknow be with us and if you havesomething exciting that you build withvv8 and you would like to join us for anepisode maybe next month hey uh secondWednesday of the month we could be thereso um yeah so thank you for watchingthank you", "type": "Video", "name": "Weaviate Air \u2013 Episode #6", "path": "", "link": "https://www.youtube.com/watch?v=NUlS-TNEH_0", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}