{"text": "Hey everyone! We are super excited to share our interview series from the NeurIPS (Neural Information Processing Systems) ... \n[Music] hey everyone we're live from Nur 2023 in New Orleans and we have an incredible collection of interviews in person with our new clion microphones yeah thanks for letting me co-host this week Connor awesome it's been such a fun week and we really hope you enjoy our interview series hey everyone I'm so excited to be here at the coher booth at Nerfs I'm here with Jay Alamar and it's a pleasure to be here and to just learn from you and have a few questions for you great to be here no yeah great so it's awesome conference great to run into you awesome so how has uh nerbs been treating you how overwhelming uh there's so many smart people so many Works uh so many things to catch up on so many talks and poster presentations so yes it's there's a lot to to sort of uh dig get through yeah I get feel like that's motivating for 2024 right that is true that is true yeah like I'm spending a a good chunk of this time thinking about the tools to explore information so visualizations or using llms to you know understand or explore or search things like uh Europe's papers and posters awesome and I guess with the encourage M with Nerfs at the end of the year and it's kind of like what we can expect um in 2024 can you give us like kind of what we can yeah expect to see from uh cohere okay yes so we're very excited about retrieval augmented generation and we also see that the industry is as well um and so that's an area where there's a lot of Promise a lot of people who are just new to llms this past year um they they're starting to to look at that as one of the highest let's say cases that they that they want to think of and I think the last year has been sort of a learning curve for a lot of them in terms of how the importance of search and retrieval and embeddings and um and all of that will come together in in this form of of of rag and retrieval augmented generation so U I would really excited to see a lot more sort of production rollouts of of rag systems this coming year of course awesome yeah I think rag it's like it started out in 2023 but in 2024 I feel like it's really going to be a shift in what we see um and maybe even like with multimodal especially um and you guys have multilingual which is very interesting and like empowering the whole world not just like in English uh I think that's uh true yeah yeah multi multi lingual is is fascinating it gives PE a lot of people in different geographies a lot of a lot of tools that people take for granted let's say in uh in English um so uh yeah the thing with let's say rag is like the robustness of it is takes time uh so it's not do you just search the question that a user gets just throw that at a database and get an answer uh people find that their proof of concept needs a little bit of an improvement and so we've been setting out you know some of the ideas and tools to make these systems a little bit more robust these can include things like query rewriting or citations um and let's say better retrieval uh through things like Integrations with with with we V8 for um embeddings and and uh and reranking yeah especially I think like the we V8 connector um that's like one of the opportunities that we've seen um behind us is the co Coral uh demo um is this also have like the multilingual model uh behind it or so so the rag demo in in Coral is mostly English at the at this point but it shows you a little bit of how we think of of the rag uh use case where if you have a question that you ask the model the model will go and retrieve and give you let's say citations for the answer so this answer it got it because it looked at this paper um and then we did this specifically for nurs so you have web search but here let's say these are internal uh but users will be able to see ne's papers and they can uh they can search that uh as well so these are this is let's say a demo but people can build all of this using the API and I'm actually going to use this opportunity to tease a guide that working on with the Zayn and Sebastian at wv8 to do rag uh with hybrid search with wv8 using all of these uh for somebody to just download download that code running write in in a in a Jupiter notebook and do the the whole cycle of rag with wv8 and cohere I love our web8 coher partnership I think it's like a great story to tell and like the possibilities are endless absolutely absolutely the same love collaborating with you folks well thank you for the interview and yeah that was great incredible thank you so much great raining [Music] into hey everyone we're back with another sick web8 tutorial uh on sparsity on llms uh I'm here with my guest Connor yeah awesome so I think uh there isn't anything more exciting than running llms on CPUs and maybe we could just kick it off by diving into this demo and explaining what's going on with this yeah so uh at neurom Magic we work on compressing models uh and making them as small as possible with a specific focus on unstructured sparsity so uh here at neurs we're showing a paper called sparse fine tuning for infin acceleration on large language models and we actually applied sparse GPT uh to uh MPT llama and mistal models and we're able to prun it in one shot and then further fine-tune those models on Downstream task preserving accuracy compared to the dense model and getting sort of massive speed up 6X 8X uh even you know hopefully some 10x stuff uh uh going on here and yeah we're showing sort of sparse llama uh running on some fun prompts uh on uh running on some AMD a AWS uh compute instances amazing so I know that uh you know privately we have a lot of conversations where you've explained this to me over and over again but if you don't mind uh going again like what does it take to take the dense llama 2 and convert it into the deep sparse latu Yeah so basically you know we have a library called sparse ml uh that's open source and works natively in P torch so it can take your hugging face weights uh you know uh take that uh you know module graph directly and apply uh the latest sort of compression algorithms like sparse GPT or smooth Quan or uh layer activation Equalization all the latest quantization and and pruning techniques and basically uh you know apply all those to Your Pie torch graph and then uh export that to Onyx which is the format we use in deep spar and so when you're ready to deploy you just sort of ship that Onyx and the Deep sparse uh application uh to your server or your local machine and you can get started in our like direct text pipelines from a code perspective or we have say open AI compatible uh you know API server that you can sped off and start pointing your open AI code to yeah I think it's absolutely amazing and I just the so if we could kind of then come into like the implications of this like now you can run llms on CPUs and it's getting way cheaper to run run it uh you know we're big evangelists of rag retrieval augmented generation where you're a lot of people want to run it privately on their laptops and so this kind of thing would just massively take that forward and we're also really excited about things like uh we call it generative feedback loops where it's like the llms take your data and then they you know process it to create more data and maybe as we think about like what do billion scale look like for the average person who has their notes in you know a notion and then they do this kind of thing processing with llms just like generally what do you think like this kind of inference acceleration how will this change the entire state of AI yeah I mean I think like particularly CPU inference and and optimizing the models like this really suits well for uh you know having all these like specialized models um and continuing to like make fine-tune variations uh and and but you know what you find is like sort of the problem with that is that you can't really have a lot of throughput for that you're not getting like 10,000 or 100,000 users a day like hitting like your specific fine tune model so it's a lot to like spin up a GPU and use all that memory just to host like your model or your like various models that you want to play with throughout the day uh and like sort of CPUs with like the much cheaper like memory uh if you can like get higher effective memory bandwidth through sparcity uh and of course like quantization as well um you know you can get like good enough performance for serving your like low batch size use case um and uh and you know just get really good like latency uh for for the applications you're trying to to build and experiment with I guess so parsing that for me I'm curious like if there's maybe a hybrid model where you have specialized models that live on CPUs but you also have gpus that host maybe like the gbt 4 like the so that kind of orchestration of like the general Reasoner lives on the gpus and then you have this kind of specialized models in the CPUs and I guess it also just makes me think about this whole like load balancer for llm API kind of idea and then I think also with weate like like if you can run the llm and the CPU does it make sense to have like some models kind of like in the database like very close to where it's running as well as uh then requests to bigger models and just I guess my question would be just like yeah yeah that kind of orchestration yeah I know I actually I saw this uh saw this great tweet today of like someone saying that they trained a uh you know kind of like a routing or like a gate Network similar to like the mixol stuff that uh it would actually help you choose whether you should ping like TBT 3.5 or four or uh you know Claud or gemini or sort of like which llm is going to be like best suited for the kind of like prompt and task you have ready for it um and I think that kind of idea you know really got me thinking yeah this morning like across those lines of like yeah like what actually how big of an llm do you need to use uh like what's available do you need to stay local with some you know small fast you know essentially free llm to use or do I need use a more like capable API uh in order to get like uh you know the right sort of answer that I'm expecting uh for the given task so I think things like that are going to like continue to develop I think like you know as we have like more apis and like a lot more specific models and of course like hosting of those models with all like the prompting or uh system messages uh uh or you know sort of all the complexities aside from the model weights themselves changing how those apis feel for use I think there's going to be a lot of uh yeah higher level decision- making and routing uh that you're going to want to do um when building like a cost-effective application yeah amazing I I guess like one other thing is um I a trend for me that I'm feel like I'm seeing is more inference API provider than we talk about the inference API but like this I'm curious like uh like I think my understanding is that neural magic is very focused on just kind of like you know building this making it work and then but then do you see like a whole another like uh inference like providers that are built on top of neural magic that offer the apis or will neurom magic offer an API and I think this also go sorry it also goes for fine tuning sorry yeah yeah I mean at the moment yeah we're not really yeah investing in like in inference API and like hosting models for people um we actually like you know have some partners and like uh customers that we work with that like want to build you know want to have us built into their applications like uh you know we had a recent uh release with uh strive Works who makes like an inference optimization and auto scale platform and they you know built Us in as uh uh you know as a provider there um and then obviously we're working with like Cloud marketplaces to to build products there so I think uh yeah from our perspective we want to make like the best possible software and like the most like elegant turistic approach to inference rather than brute forcing it um and yeah like I hope we're like best served using that and and people find that software usable awesome Michael thank you so much I love the hats hope see CPU Rich really clever become more CPU Rich every day there's no reason to [Music] stop hey everyone we have another awesome episode in our live and nerve series with Andre moar co-founder and CTO of vomic AI we have an awesome demo behind us of the uh embedding visualization software so many cool things Andre thanks so much for joining our podcast Conor thanks for having me here awesome so how are you liking the conference so far what uh the the conference is great this is my fourth year here uh my first year is not an academic so it's really inter to see this uh this conference really grow into something that's a lot bigger than sort of like the research papers there's a lot of community here from industry um and yeah the post session are gigantic this year yeah I think there's just so many cool things to this embedding visualization like just with with we8 and people with their vectors and they want to see kind of how their data is distributed it such a powerful tool for that as well as we get tons of questions about people who want to do this kind of like topic modeling or see outliers and I think kind of but the question I really want to ask with this live at nup series for you is uh there's also such an interesting opportunity to help people train machine learning models by seeing the visualization of how their model is mapping the space and I know that you train a lot of models and you have a lot of cool IDE about that how how do you see that kind of uh you know presenting nomic to people who train models yeah so look one of the one of the things so like um the the original Transformers paper uh at nerve 2017 uh this is where it was presented and that sort of like marked this point where I think a lot of research especially on the architecture side in machine learning sort of gravitated towards like you have this one architecture the Transformer and what you do is and you play with the data to figure out how to program that architecture right how to program the weights to be able to solve your end problem so a lot of the work that a lot of people to do nowadays especially in industry it's very data heavy work uh the way you program machine learning models is by manipulating the distribution of the data points so kind of where we come into play here is that we allow it you to be very capable and make it very easy to allow anyone to go and and program the machine learning model by adjusting the data distribution uh and we sort of build like you might might say the data IDE to allow you to be able to adjust that distribution to program your end machine learning model so cool so maybe like uh you see a region I think it's kind like this Active Learning problem and that's maybe one of the big applications of nearest neighbor search generally is like you you have a region where you have a lot of loss in that region so you try to collect more data points in that region kind of and it's hard to just uh have a category to that region with you know like if it's like M classification and you have errors on eight that's kind of intuitive but with more self-supervised you need like this visualization of the Clusters to even you know begin to understand where are the failure modes yeah I mean there's like many ways to evaluate the quality of machine learning model uh if you go to the poster sessions all the way over there uh you'll see that the thing you need to get into a good conference like this is you need good quantitative evaluations numbers that beat previous numbers ideally uh but honestly in many domains it's very very hard to pick good evaluation metrics like how do you evaluate the quality of image generation for instance like the metrics people use there are actually very bad proxies sometimes uh a lot of a lot of things like um like uh like the metrics for evalua in quality of Gans they're they're very poor quantitative metrics um so one way to think about it is if you can do massive observability of the input and output spaces of machine learning models and you are you have very very good knowledge about the domain specific requirements of what you need for your end problem you can find things for instance like anomalies in your data set or output space anomalies where your model might be doing something that you don't even know is a bad property and you could identify those and then trace it back to training data root causes so when you think about what's happening with machine learning nowadays is that a lot of people are realizing that a data Centric approach is the approach that you need because do doing things like being able to find and subsample regions of your data set that might be of more of more importance uh is a thing that is very hard to do especially when the input points to our data data sets our models are unstructured and the output points are unstructured so having basically capabilities to do that um is super critical um and giving those capabilities to people who maybe you know aren't the Geniuses walking around nerps uh is also really critical because there's you know a limited number of those folks in the world yeah yeah yeah I think that argument completely sells it with the sampling regions and I guess um so people with we8 we have this integration done and you can check it out on we8 recipes as well as when we published our podcast together we open it up with the link to visualizing our podcast Clips with nck and so with these visualizations I want to ask you Andre do you have a favorite one uh I would say my favorite one is Wikipedia so that's actually when we were first building out the company that was sort of our Benchmark it was like how fast can we go in and give you a view of the embedding space of all of Wikipedia and when we started out uh it took about a week to be able to generate that and it takes us under an hour now um so all of English wikkipedia um so I think that's one of the most beautiful ones you can literally see all of human knowledge embedded onto one screen uh curated human knowledge especially so I think it's really beautiful um but obviously like there's some really cool things like being able to see 100 billion tokens from a training set of a multimodal LM uh that also is really interesting too yeah yeah definitely and I guess kind of one more like I remember um when we met you I mean like you've trained gbt for all and I think everyone knows it's one of the most famous models and you you really are great this like lowlevel optimization I think if we could touch on the uh the optimizations for running TSN so maybe people listening TSN has this quadratic complexity that you know makes it quite difficult to scale it to Wikipedia scale embeddings um can maybe talk a little bit about the the technical uh yeah yeah yeah so a lot of folks when they look at their stuff they're like hey wow you have this really cool visualization platform um that's fun uh cuz you'll see T representations in like a lot of the papers that get published here it's a it's a it's qualitatively one of the only methods to understand what's happening with the output space of a neural network um but to actually make it practical for real purposes you have to do a few tricks um so in in the back end of our of of of of our software we have a hyper optimized T actually the guy who made tne calls it t on steroids um we have that quote uh and uh there's the tricks you have to basically do uh include putting things on accelerated gpus so you have to have sort of from from scratch custom implementations of a lot of these algorithms and then you have to do you have to do very very care careful systems engineering work uh the machine learning Parts you know uh are arguably not that interesting compared to the kind of system level engineering you have to do like how do you load up 100 million embeddings in memory and efficiently run an algorithm over top of that without you know ooming the machine uh you ideally don't want to be running a several terabyte computer to do that right so there's like all of these system level optimization things you have to do so it really is like a problem of combining ml talent and skill with Talent from individuals and know how to do like lowlevel systems level programming um to to sort of like reach an end solution that may be useful to a human uh past like you know an 100,000 point scale T representation of a data set uh the op Source algorithms that exist out there implementations you can pull it off the shelf for 100,000 data points 200,000 data points um it works fine it's very slow uh we can do a million data points in like 2 minutes so uh that's sort of like the direction we've been working towards amazing I I love that oing the machine really funny and yeah NOA just I think the visualization on top of we8 it's like one of our most powerful Partnerships I think and I hope people listening will check it out Andre thank you so much for joining our live nerve series [Music] man hey everyone we're back with another live at nerves interview I'm here with div gar the creator of multi on super exciting company doing uh you know next level stuff with agents D thank you so much for joining our nerves podcast definitely yeah good to be here awesome so can we kick it off with an overview of what multi on is sure so in a sense we want to build like personal AI agents that really understand you and can take actions on your behalf on the web uh so from like two years from now we don't see it seems like really weird like if you're still going on the web manually clicking and typing what's going to happen is there'll be like AI That's going and doing all of this for you we just talk with the AI and the AI will go do all the things on your behalf and so we want to build this sort of like the next uh the future of AI where like this agents really understand you know everything about you I connect to all your data and all your services and then can act on your behalf to like simplify your everyday life which could be anything from like yeah so this could be anything from like say like ordering food booking flights your daily everyday stuff to like executive stuff which could be like say sending emails LinkedIn uh admin tasks and so we see like this going to be this next big revolution where a lot of the things are stadia take a lot of time and we can go and just make everything like instantaneous like automatic and I think that's going to just uh totally to be like one of the biggest changes ever since the invention of the computer itself yeah super compelling and um so I guess my question for you I'm like a you know I think this agent thing obviously the next step and we're seeing a lot of like I would say like kind of hacker culture around it where with people connecting with the apis and so I'm so curious like um with nurs sort of what what's this perspective of the machine learning research what's that adding to the conversation around this agent so definitely I think like uh even if you look at neps like not many people are working on agents right now because it's just so much uh in the future in a sense where like not there's not even too much research on it um but but I think like it's starting to like become very obvious like this is like what we need and I think like next year I think we will see just like a huge wave everything will be just center around agents there's definitely a lot of like hard problems because even you at chat GB it's mostly a chat but now if you're starting actions a lot of things can go wrong so solving a lot of this hard challenges about how do you make it safe how do you make sure like nothing goes wrong how do you do that at scale supposed have like billions of this agents running like how do you make sure nothing nothing fails yeah I think there's so many interesting opportunities I think with our Vector database we're very interested in multi-tenancy which is like something we're seeing with our production cases where your users some have like you know 10K docs other have 10 million docks and when to give them a Brute Force index versus the proximity graph and so I think that abstraction will go perfectly for multi-agents and then one other thing I start plugging the weate thing quickly but uh this kind of like reinforcement learning is um like reinforcement learning for agents this something I've been having a lot of conversations at nervous with people about is um you know uh how you structure the the loss and learning within agents do you think learning will have a big role in the future of Agents or do you think we'll just kind of use them as zero shot black boxes no say like learning will be very important because like what might happen is you might find like like security vulnerabilities on the fly or you want to like keep updating like user preferences what they like stuff like that so as you discover just like more real-time knowledge you just want to real time update update the agent and so this like sort of like continuous learning will I think become really important thing awesome fantastic div thank you so much and thanks for uh pushing through I know we got a bit of a loud section of our our podcast but thanks D definitely this was a lot of fun thank you [Music] awesome hey everyone I'm so excited to be here at the boxel 51 booth at nurs 2023 here I have Jacob um he's going to give us a demo and then we'll end off with a very interesting question on where we see AI where AI is headed in 2024 thanks for having me Erica it's it's great to be here uh pleasure to talk to the weba folks as always uh my name is Jacob I'm a machine learning engineer and developer of angelist at volx 51 uh I'm going to show you a little bit about the 51 toolkit uh so 51 is an open source toolkit for curating and visualizing data uh images videos Point clouds PDFs any unstructured data generally so we have a data set pulled up in the 51 app right here uh you can browse through your samples these can be images videos Point clouds whatever you can click into them uh you can see all of your predictions all of your ground truth information and inspect it uh you can filter by different properties like I can filter for things that are for instance overcast that I can combine different filters together like I only want to see things that are overcast and happen on the highway uh and then I can get these interesting subsets of my data and everything going on here is backed by a python SDK uh which gives you programmatic control over your unstructured data so we can actually see what that looks like when we hit this bookmark button right here and then we can see that this is represented symbolically so this is a symbolic pipeline of logic that allows us to get the specific data that we want uh but in general this is a toolkit that doesn't just allow you to filter your data it allows you to find hidden structure to find problems and to curate a higher quality quity data set so let me show what I what I mean by that uh so one thing that we do is we have embeddings so you can generate embeddings with whatever model that you so choose it can be an image embedding model it can be a multimodal model whatever and you can use Dimension reduction either umap or tne or PCA uh in order to uh reduce the dimension of uh those embeddings down to two Dimensions so you can visualize all of that so let me click out of this so that we can see all the points here and so now we can see the clustering of our data in the two dimensional space and we can color by whatever property we want so I can color by something like the time of day and I can see the rough structure of the data so mostly this is one time of day and this is another time of day so this is night and this is daytime but there's a couple of points in these clusters that are colored differently than the other ones so we can investigate what's going on here so we can look and say okay okay let me just look at these points so I'm just going to look at the nighttime points in the cluster of mostly daytime images and I can inspect what's going on here and look at these and see if they're edge cases or if they are annotation mistakes so I can see some of them like this one looks like it really is nighttime so so this is a mistake but some of them like this one looks like it's daytime we're just in a tunnel which is an edge case so then you can actually go back and reate the data or add more data for edge cases and so so on and so forth so this is just one example type of thing you can do with 51 but in general allows you to build a better set so you can train a better model very cool so essentially it's labeling but then also visualizing so like with these edge cases you can go back and say like classify it like it is daytime like that's okay and then like you're building up like a richer data set in a way um but you also mention multimodal data and I think right now it's a an very interesting an application right now um where do you see multimodal data and also applications headed in 2024 great question um just clarify one thing it's not labeling uh it is a a way to uh make your data better so we integrate with labeling platforms but we're not a labeling Tool Set uh but uh multimodal is going to be 2024 like 2024 is going to be the year multimodel data uh and models uh we're going to see a lot more of the gp4 vision type models and the lava 13B multimodal assist uh but what I'm most excited for is having all of this stuff uh be real time so uh we're going to see real time uh image editing and instruction guided tasks that are multimodal uh and all that happening uh in a way that enables people to interact and uh create new workflows and and uh value propositions so that's what I'm most excited for not necessarily uh large Foundation models but uh these capabilities uh actually coming to Everyday People so very interesting thank you thank you for the demo and also answering the question my pleasure my [Music] pleasure hey everyone we're back with another live interview at nerves um with determined Ai and Liam Lee uh Liam it's so cool to run into at nerves and meet in person U Liam and Angela were two of the first people I met sort of out of my PhD in the industry and uh you taught me so much about uh hyperparameter tuning and there were so many cool things with Asha and uh and this whole uh like resource allocation stuff and S it's so cool to meet you and uh can you tell us about what you're working on these days yeah you know as with a lot of companies we're also looking in the lmm space and enabling our users to basically fine-tune the open source language models as easily as possible and also know providing like looking to provide easier tools for prompt engineering and evaluation of different llm applications I mean so I I did have a question Li I really wanted to ask you about um so uh when I was learning from you you were T teaching me about this um hyperband algorithm which is like uneven resource allocation you have say you're searching through batch sizes and you have 3264 1228 256 and the 321 after 5 epox is like so bad it's like why spend further why get it to 50 and so I think in our current state of retrieval augmented generation we're searching for like our configurations of which embedding model to choose how to chunk our data do you need re ranking if so which model and and so we also have this kind of like hyper parameter problem and so I'm just really curious if you think these uh like the foundational algorithms of like fault tolerant um uh this resource allocation if this will translate into rag yeah absolutely um I think it maybe really will shine when users have larger rag like data sets that they want to evaluate rag pipeline over so you know you can think of defining a rag search space with all the different techniques that you mentioned and the resource that we talk about allocating in this case will be you know how many examples do we want to evaluate a certain configuration on right so hopefully you don't want you don't need to evaluate on the whole data set in order to get a useful signal for whether a specific rag pipeline is good or not so you know you can use resource allocation in this case to hopefully speed up hyper primier tuning for Rag by allocating a certain number of examples looking at the intermediate average evaluation results and then allocating more resources to pipelines that appear to be doing better from like a rag perspective as already I think uh I I guess I was like I think biased towards a vector database we have this problem of like building up the vector index which is kind of analogous to the llm train like model training where you have like this long process where you're like building the hnsw graph and but I also I really like what you just said about like just the like if you're testing different LMS like I think right now like with the mixt model people are saying okay do I switch from open AI to mixol or how about llama and so that kind of how many examples a test is also yeah massive part of it yeah yeah and I think just more and more I I think there's more room for automl in the whole llm application stack as well because and and also room for foundational automl research because we're looking it seems like people are like going into or designing much more complex pipelines and like workflows with llms and you know in in traditional hyperparameter optimization really only focused on one search space or like a fairly limited search space but with with LM applications I think the search space is only getting wider and wider and it'd be really useful to have autom ml techniques that are much better at navigating this search space and like optimizing maybe like the most important component at a certain time and then kind of like figuring out when to optimize what and going back U or using like incremental hyper parameter optimization um to really like hopefully navigate the complex search spaces better yeah another uh nervs podcast we has with Omar kab who's created uh DS yeah I don't know if that's how he call says it but but yes I love that Library uh it's great I think it's the user interface is what I think we should be moving towards I really hate it when I see people like trying to manually do prompt engineering and um you know they try a prompt and then forget what they tried before that used to work well right I think uh and there's just so many different prompt engineering approaches really hard for a user to know what all those different approaches are and like to try all those and we should abstract it into a much like cleaner user interface something like DSP where you know the user specifi input output and you know have an optimization system that worries about how to get good performance with techniques like prompt engineering fine-tuning and maybe even extending to optimizing the retrieval pieces within DSP so yeah I think I really like that Library so excited to that you brought it up amazing Liam thank you so much it's so great to meet you and having to be a part of our podcast here yeah thanks for having me good to see you awesome [Music] hey everyone I'm so excited to be interviewing Alex CH from semantic Kel Alex how has nerves been treating you it's been good a lot of uh great research a lot of friends um yeah just a good time to gather everyone here together under similar purpose I've seen you at a few events so I get the networking side and just being surrounded by so many smart people it's a lot of fun um can you explain to us a little bit more about semantic kernel yeah I mean semantic kernel is uh an SDK that we at Microsoft open sourced and it's kind of the foundation for many of the co-pilots that you see uh Microsoft create and we built it internally in the office of the CTO kind of and we had early access to the GPD 4 and really kind of uh saw that there were a lot of common patterns a lot of emerging architectures coming out and yeah we decided to open source it in early March and it's kind of grown into its own uh thing in community and we've worked with we8 also to uh to make it uh even better it's always nice to see Zen part of your podcast but also doing work with semantic Kel um it's kind of funny that you said that it was initially like an internal tool but then you open sourced it that's awesome um but with that being said uh 1.0 is being released very soon can you tell us uh what we can expect from it yeah so 1.0 is kind of a pretty momentous thing for our uh project because it signifies that we believe that we have a stable architecture that uh developers Enterprises can yeah really like depend on and I could be backwards compatible and all that so the C version of the semantic kernel will have a 1.0 release um scheduled for Monday that I think that's December 18th so hopefully like fingers crossed it everything goes well but yeah expected to land very shortly awesome very exciting and any breaking changes oh yeah there will be lots of breaking changes there's new planners there's uh kind of like ways of how we kind of handle context we or just like arguments that you pass into the colonel so there's just a lot of like things that hopefully right the changes are worthwhile for people to upgrade to one Flo so and any one feature that you are most excited about in MTO or so we've been yeah we've been pushing uh on the experimental side uh more on assistance and kind of like agents style Behavior we've been very much uh inspired and work very closely with um our friends at Microsoft research and like autogen and similar types of projects so yeah those uh early features are are there right now um you can play with it uh it's marked as experimental but eventually once we uh kind of stabilize that side right it will also uh come out um in the main semantic colel very exciting yeah walking by the Microsoft Booth today I saw that they were advertising for autogen and it made me think about you with Samantha kernel and just if we can marry the two together autogen for sure can own the the cool kid hat right now so but yeah semantic Colonel you know you want to use it for an Enterprise uh production app you can definitely uh use that very nice well thank you for joining this series and it it's nice having you thank you so much thank you [Music] hey everyone we're live with another interview from our uh nurs iners interview series I'm here with Alex from weights and biases Alex thanks so much for joining our podcast hey man this was so cool I just came up and so you guys with these things like I have to jump on I have to say something what's up man yeah awesome we had so much fun at the weight and bias's party it was one of the coolest ones we've seen in n and um yeah we were actually just trying to find you at the booth that we're kind of wrapping up as we're doing this interview so it was so cool to see you coming by and have a we really wanted to feature weights and biases in our series as well um so can you tell me about like uh your impressions of the conference and kind of what you're learning about the state of AI from it it's interesting because like there's a quite a big difference between the papers that happen and like you know vuna paper from the smart for example the folks who like uh fine-tuned on top of outputs right they came out with like a llama fine tune they're presenting a paper and in what we do like I report about AI every week week on Thursday on my podcast like that's so such old news it's still cool research and so you talk to these like uh researchers who like stand with the graph and like the lmc's guys it's so cool that they did this back in March but since then today we have like a 10 billion parameter model that beats GPT 3.5 and it's like one model so a lot of the like to me at least somebody who exists on Twitter somebody who like reports every week from like stateof the art to stateof the art stateof the ey is moving way faster than this is It's it's still super super cool and um the coolest thing about this is just meeting people just the amount of crazy people that I've met um that's super cool yeah without a question that's my favorite part of the whole nurs conference and I think what you're saying is extremely fascinating because um yeah it's I it is it does feel to me like the engineers are moving faster than the researchers maybe and if you're just trying to keep up with the flashiest thing I think the engineers the hackers are ahead of the curve with that but I think that there is a very interesting thing with with so I understand weights and biases is about experiment tracking and you know being rigorous with these metrics but I think the trend we're also seeing is this llm as judge and using the llm to produce metrics that would let this I think would merge this kind of like rapid development of new products with that kind of benchmarking and tracking um what do you think about all that kind of llm as judge research and how it might be integrated with weights and biases well thankfully some of what I do is report on this every day and so today open ey released openi yes I'm not I'm saying this correctly released open source today they released something that uh helps uh judge bigger models outputs with moral models and they came out with something including some code on GitHub like this is the first like I was like whoa Open the Eyes back back to open um where they use they say something around the area of uh GPT 4 that's guided with gpt2 can outperform gpt3 Guided by human which GPT 3.5 guide by like an expert human is like considered like the one of the top like results and so that's very interesting because like you know our lhf got may be behind us at some point we need to scale faster like humans can scale only so far in like AI needs to take over so definitely there's a bunch of that I haven't seen much research here but like it's really like quite hard to follow everything the all the places and and plaques and like all the the talks but that research is moving very fast open something today and open source is not that far behind I have a few friends who do rag and then uh they have several agents and some of their agents are like 70 B llamas and some of their agents like 7even B llamas and they just kind of like they're the routing they're like the the neural core whatever they call it where they make the decision whether or not to go for like that big heavy like logic thing that you have or is it like fine enough to use like a small very capable model as well and the whole thing wraps around all these models getting better and smaller at the same time so like I think we're coming very soon to to like an on device on edge like a router model that's tiny then if you need to go like for the bigger like the very heavy like brain stuff you will like that will rout to a bigger one yeah I love that Vision one of our podcasts in the series is with Michael going and neurom magic and we're doing the interview and behind us we have the neurom magic demo where you have the sparse llama racing through showing how much faster tokens per second it can be running on CPUs and you have these smaller specialized models running fast I think you this year I think we're going to see all sorts of stuff with that and yeah I've also that whole R lhf thing I've heard a lot of debates around nervs about like DPO versus Po and the whole uh from Human feedback versus AI feedback and multi-agents and all that sounds like such an exciting opportunity and definitely something with the weights and biases tracking to play a massive role in it and so maybe coming back to rag just so kind of want to tie our we8 and weights and biases stories a little more I think with rag we have these uh hyperparameters like um which embedding model to use how to chunk your data like token length for chunks maybe do use reranking and so we've we've already actually kind of started uh doing these experiments using the weights and bias visualization so I think there's also the opportunity to you know fine-tune and then plug it into rag for the metrics that are reported in the weights and biases dashboard um maybe if you could just tell me how you see like rag in weights and biases absolutely so definitely definitely there's an area of research and very exciting things going on from the perspective of okay you can do rag but with like a regular llm that's not F to there like instruct fun to do everything around the world basically around logic right but then I I I noticed more and more people they're like okay we need to fine-tune this on specific retrieval stuff the not the embedding model for rag even though that's also possible some people fine tune embedding models for rack specific Q&A uh but fine tune are model like a 7B llama or a 10B I forgot the name that just released today like a new state-of-the-art on hugen face fine- tune that on like retrieval and Tool use as well and so those kind of capabilities fine tune into the model they make the model better at rack for example and fine tuning is all what like w and is all about right you take base model off the shelf or like a coolest new mistol or mixol whatever they dropped uh just a Friday ago and then you start working at it with your data sets on your own task and your own capabilities and that's definitely connecting weights and biases like Rag and wave all that yeah yeah me that's that's how we uh started our waste and biases collaboration is Morgan and I both had this shared love of the gorilla models and um and shout out to Morgan shout out Morgan I I saw he got the whole weights and biases co-pilot bill I saw repet tweeting about it want yeah yeah one bot one bot yeah and we're definitely trying to catch up and have our our we8 assistant on there as well and training it and monitoring it with weight and bias is also cool Alex thank you so much for joining our podcast everyone listening you got to check out Thursday ey podcast thank all the cool stuff Alex and W and Vis are up to thank you [Music] awesome hey everyone I'm here with Sebastian rashka one of the world's top Educators in AI Sebastian it's so cool to meet you here at nerves yeah nice meeting you Connor um totally surprised by you attending here so it's nice to just uh meet each other here at the conference awesome so uh can you tell us a little bit about pytorch lightning yeah pytorch lightning is an open source library on top of pytorch which makes it super simple to run experiments uh for example multiple gpus changing the position it takes care of logging check pointing a lot of things uh it's essentially a a free upgrade for pytorch uh so cool and so I understand you're doing a workshop on um how to train llm I think it's in 24 hours or I I might be mixing the title a little bit but can you tell us a little bit so yeah that's the neurs efficiency challenge that is actually a whole workshop with many many talks uh I wish my talk was 24 hours or maybe not it's just 15 minutes at that Workshop but it's going to be an exciting Workshop tomorrow where we will see uh the Creative Solutions people came up with to train or find tun llms on one GPU for 24 hours like you said and coming up with the best llm in that short amount of time which is I think Super interesting to a lot of people because a lot of people don't have multiple gpus or uh time to train LM so it's important to also focus on how can we make this more efficient and what are the best techniques to make that efficient so yeah if you have time you're around tomorrow it's an exciting Workshop I hope yeah amazing I like I think this kind of like um like L like Lowa low rank adaptation I know you've written so many amazing blog post explaining like how it works and I think this using pytorch lightning to uh like uh control that low a lower level it's all really really amazing stuff um so I guess I'm just really curious about your general sentiment on um like this low-level control with pytorch and maybe how this compares to uh to like buying into the abstractions of like Transformers or maybe even things like I say any scale or like open AI fine tuning where you don't even bother with the uh the model itself and just maybe your experience with um uh who is the most interested in this kind of lower level uh control of the models at this stage that's a good question I would say people like me who are tinkerers who like uh having an understanding of what's going on uh also I would say researchers who want to ask and answer questions so you need to have the model you could use an API like openi uh which works very well but you probably also heard it's changing all the time so you do your research on one version the next day it's a different version and it's hard to control and so with these smaller L&M uh you have more control of it so as a researcher I'm not a person who deploys things I can also Imagine as someone who builds products it's good to have your own uh llm that you can put into your product so you know exactly how it works and you can change it when you want not when I don't know some company changes it so in that sense I think there's a big audience of tinkerers uh researchers and people who deploy things and like to have a handle on how things work awesome Sebastian thank you so so much for joining our quick nervous podcast your newsletter is amazing thank you appreciate it and yeah nice meeting you and I hope I see you at the workshop tomorrow [Music] awesome ", "type": "Video", "name": "Weaviate at NeurIPS 2023 (Interview Series)", "path": "", "link": "https://www.youtube.com/watch?v=xrZxk0H2cmY", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}