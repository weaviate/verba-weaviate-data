{"text": "Thank you so much for watching the 26th episode of the Weaviate Podcast! This is another really special episode! Jonathan ... \nhey everyone thank you so much for checking out the weave podcast I'm super excited to welcome Jonathan Franco back to the weba podcast Jonathan is one of the co-founders of Mosaic ml a company that is just doing amazing work in the space of making deep learning training more efficient faster cheaper and they've recently produced an estimate of costs for language GPT language model language model costs across all these different sizes and price points and just overall helping people get a better understanding of what it takes to have your own large language model and I've been so interested in large language models and thinking particularly with weave and the role that they might have in search so I'm so excited to welcome Jonathan back to the podcast and talk about what he's been up to at mosaic ml thank you so much for having me it's always a pleasure to get to talk to you and I can't wait to talk about this new project awesome so yeah could we dive right into it um so what what's the latest update with um the Mosaic ml cloud and training large language models definitely so you know just to give you a sense of what we do at mosaic very briefly you know our goal is to train models efficiently just in general that's you know that's what we want to do for everybody and there are a lot of ways of making it possible to train a model efficiently and obviously a lot of models that people want to train efficiently so I think the last time we chatted we had just released composer which is our our open source library with all of our efficiency methods built in and I think we had shortly thereafter released our recipe where we gotten like a 7x speed up in training resonate 50 on imagenet over the standard Nvidia baselines which you know I'm kind of jealous I'm not seeing a bunch of people doing lottery ticket experiments I humanly couldn't do because it was taking too long to train and now people are able to do it overnight and I'm super jealous that people get to do that work now um but our latest release is for large language models for GPT 3 Type models specifically and you know it's been a lot of work on our part but we've put together our own software stack for training these models we've put together underneath that in the orchestration stack which is part of our Mosaic Cloud that I'm sure we'll talk about a little bit and the end result is we put out some price you want to train gpt3 today you know you can call me today it's 450 000 to get to gpt3 Quality um we can talk more about where that number comes from and how it compares to other numbers but I'll tell everyone that's the starting point um you know that's the Baseline to beat and first of all if you beat that Baseline let me know we'd love to you know obviously make it better but second of all our goal at mosaic is going to be to drive that cost down as close to zero as possible over the next while I've set the goal informally to the team of getting to 100K sometime the next few months I think it's eminently possible given the kinds of speedups we've gotten elsewhere well so can we get into this stack uh see things like the uh distributed data parallelism uh the chinchilla the optimal compute laws uh kind of like what goes into the package of uh the language model training yeah so they're really three big ingredients um I mean the first ingredient to be completely honest is we put a number out there this is kind of ingredient zero I think a lot of the numbers you've seen previously for gpt3 have just been overblown to be completely Frank I think our number looks very impressive based on the insane estimate some people have put out but those estimates just don't have really any basis in reality um you know this is a good number but it's it's an honest number and I think anybody else who sat down and tried to do this today would get to the same number so you know item zero is just trying to be as direct and clear and honest as possible with the numbers um you don't want to scare anybody off from trying this quick to the contrary I think everybody should be training their own models here there's no reason why you should rent the model when you can buy it um but you know there are really three big technical ingredients that go into this I think the first is the chinchilla scaling laws so for those who aren't familiar you know first of all we have these neural network scaling laws which are kind of I feel like scaling law is a very strong word these are our beliefs about the right amount of compute to use to train on a certain amount of data in order to get a model that you know is the best possible model that you could get there if you have a or you know if you look at a different way if you have a certain budget of compute how should you spend it how much data do you need to really make the most of that compute in order to get the best possible language model what architecture should you even train um there's nothing fundamental or natural about this this is probably a human description of a phenomenon that may be more sophisticated and more complicated and I could talk your hair off about you know how much we should trust scaling loss this is to be fair one of the only places in deep learning where we have a scaling law on this GPT style language modeling you'd never hear about scaling loss for Bert for example um so you know we this may not work everywhere but at least here we can kind of make these guesses and what happened was a couple years ago the folks at openai um Jared Kaplan and collaborators came out with a paper proposing some scaling laws and it was awesome it you know it was the basis for predictions about what size gpt3 should be the problem was actually that it was a little bit off um and as soon as you extrapolate to bigger and bigger models and this is you know keep in mind we're dealing with you see these nice linear plots of scale on a log log axis so being off by a little is being off by a ton and so especially at these larger scales like gbt3 we the predictions just weren't you know weren't as accurate as they could be which is totally expected when you're training on small models and trying to extrapolate if you're off by a little you're off by a lot and the folks at deepmind went through and basically fit the scaling well a little bit better and fit it with more data with more compute I mean years later so they had many more resources it's no flaw at the Kaplan work it's really just you know it's like we developed the James Webb Telescope and now we can observe Stars better than we could with the Hubble telescope and what they found was that for the amount of data gpt3 was trained on the model was way too big it just didn't have to be 175 billion parameters it could have been closer to 33 billion and that dramatically reduces the cost of doing this it's a huge difference because the cost increases quadratically as you get to these bigger models because bigger models you need more data as well and so you know you can get away with doing this much cheaper that way so that's why the um in the cloud I think it's like 60 billion parameters is the 450 000 mark because 60 billion is just as good as the 175 billion in terms of performance and that kind of thing it's actually 30 billion which is to me kind of mind-blowing like we're dropping the size by you know to 20 of what it was before like when it comes to efficiency that's about as good as it gets so you know I don't think that's the end of the story though to be completely Frank we're seeing so so much interesting work on data pruning and curricula and you know questions of whether we you know how we should change what the attention operation looks like in the paper we saw this interesting parallel attention that came out we've got flash attention like there's just so many new ideas floating in the space now about how to train these models effectively that you know we got 5x from just measuring something a little bit better how many other five X's are there so there's probably a lot more here Beyond this but you know for anyone who's doing research right now and really wants to get into this stuff I think the sky's the limit on making these large language models more efficient this was just kind of the low hanging fruit that's super interesting can we dive into what makes it more efficient obviously you're the king of sparsity with the lottery ticket hypothesis and you know there's things like sparse attention uh I learned about say the Ali B attention when I was first going through composer uh what are the you know efficiency things that have gone into the uh gbt3 training from Mosaic um nothing that's the honest answer is that we're using flash attention um from our colleagues at Stanford but otherwise we haven't even put in our speed up methods yet this is I mentioned before this is our Baseline this is you know this is resnet 50 from the Nvidia examples this is the way that I think about it this is the starting point for making things more efficient the first thing you have to do is stand up the stack and compute what your Baseline costs and the answer is wow the Baseline is already a lot cheaper than we thought thanks to the the work of the chinchilla folks and some other things we'll mention um but this is just our starting point at mosaic and I expect you'll see in a few months from us that number dropped precipitously um and keep in mind this is the number that allows us to you know have a healthy business as well um so there's a lot of room for us to keep pushing down that number well and uh so I read another quote in the article around something around um the vision of training millions millions of models compared to say the zero shot generalization kind of way of thinking uh can you tell me more about your vision for uh customized models say it's like you know biomedical domain Financial domain legal domain uh do you see that being like how has your experience with Mosaic been on finding all these specific use cases and then helping people say uh you know convince them what a language model is why it's worth it and then how to say prepare their data like what's that experience been of getting people to train their own language model definitely so I think this is and this is honestly something on the wavy side I'd like to chat about as well um people have a lot of data and you know that sounds like a dumb thing to say but honestly there wouldn't be weeviate if people didn't have a lot of data that they need to organize in some way and I think you can it's the same argument at mosaic that I've yet to meet a potential customer who wasn't sitting on a treasure Trove of unlabeled data in their particular domain and the question is really well shouldn't you be able to take advantage of that or to put it a different way right now we're asking or GPT to do transfer learning essentially we take birth we pre-train it on you know Google Books and we pre-trained it on C4 and then the hope is well you fine-tune it on your data and then it'll transfer why should we even bother to transfer like don't you want to pre-train your model on in domain data you may not have it labeled but one big hypothesis we have in Mosaic and we're seeing this borne out in practice with customers right now is that you should pre-train on your own data and in fact pre-training on your own data makes it much easier to get better performance because you're in domain you don't have to force the model to go across domains and across tasks so it sounds very obvious like why do transfer learning if we don't have to but I think in the academic literature we think of burnt we think it burnt two ways one is you know what do you do with all this unlabeled data you have but the other is that we somehow put the constraint on ourselves that Burke has to be generic that you want to have given that it's so expensive to free train your own bird we have to make a birth that anyone can use for anything um we're successful at mosaic and we push down the cost of Bert pre-training to the point where it's not expensive and I'll argue to you it's actually not very expensive I think we've pushed down the cost of pre-training to the point where it costs what image training used to cost so for those academics out there who want to train on purp I think it's too expensive it's not you know reach out we haven't released a recipe yet but you should expect that next month for birth but you know you don't need a generic bird if you want to build a bird for your one Downstream task or your two Downstream tasks and in that case you really do want a free train on your own data in fact you want to do even more than that you want a tokenizer that was built using your data set because you know the tokens that were used for C4 are on Reddit or for Twitter or you know they have lots of emojis they have lots of words or characters they may not show up in your data you may have characters from a language that isn't included in your data that are wasting your vocabulary and so everything's in the tokenizer it's what you pre-train on you can build a domain specific or domain-specific TPT and I think that's pretty exciting like if you have medical data you don't want a model that's pre-trained on Reddit as your basis you I don't think you want to use an API where you're just wearing a model that was pre-trained on Reddit what kind of medical data are you getting um why not free train on PubMed um if you're working in a legal context again do you want legal advice from Reddit or do you want legal advice from whatever internal data you said you have or from the actual laws from various countries if you've downloaded those laws for some reason in a particular domain so I think over and over and over again or you know even further field if you're working on code or proteins you probably don't want to start with C4 it probably makes a lot more sense to start on GitHub yeah that's super interesting and I thought I thought a lot about in search how you know a lot of the the bm25 the keyword scoring is so popular because it helps for those specific words as say uh you know say you're training it on Mosaic ml slack and you want a language model to talk to you about Mosaic ml you know the word composer has never been seen in Wikipedia so then these keyword scoring can help you adapt to that compared to this but um so kind of in my thinking around language models and large models I guess the reason I always thought that Reddit and Wikipedia were so popular is because it's it like helps the overfitting sort of with you know a billion parameter model is predicting the mass token if you only have uh you know 300 paragraphs I always thought it would overfit to that is that kind of like the regularization techniques help overcome that or what you're thinking around oh I think this all comes down to data volume so the assumption is made that people don't have enough data people have tons of data oh my god like people have hundreds of billions of tokens of language data on their domain internally think of a company that runs like a I don't know a chat service for customer service agents or something like that can you imagine how much in-domain data they have um everybody has tons of data it's been shocking to me how much data is floating around so I think the important part to recognize is like C4s around a trillion tokens but plenty of people have around a trillion tokens of data or more the amount of data that I've heard people have it's just like it's astounding how much unlabeled data labeling labeling is expensive you want to do it as best you can you need it for the downstream tasks but how do you leverage all that other unlabeled data training gpt3 train a bird train you know a Sinclair model on your data and use that as your starting point not an imagenet pre-trained SIM clear not a you know a C4 pre-trained GPT like I think we have plenty of in domain data it may not be in the public sphere if you wanted to go and find a data set of 200 billion tokens of legal data that's hard but a company that specializes in doing legal work probably has that in spades yeah that's incredible and I and like the language modeling algorithm is such a beautiful self-supervised you know predict the master token it's very easy to just drop your text in and I think a lot of and you know simclear also with you have images you can just augment the images and then positive pair and then we have some good heuristics I was recently looking at this uh paper called spider from Ori ROM and we had him on the podcast a way to similarly do the positive negative bootstrapping uh kind of one other thing I was curious about with these laws like say the argument for Wikipedia for Reddit is the pre-training corpuses I always kind of thought like this prompting uh like the way that you can sort of template the input was a result of it of it's seeing this kind of thing on the internet so like you know it's because it's modeling HTML if you do like image source equals or like I don't know how you prompted but like like prompting it with like you know how they'll like do like the title for an article by having title HTML tags and then that's where it generates so maybe do you have any thoughts about kind of prompting and how that might be different when you're doing it for a specific data set definitely I think it again comes down to transfer learning in some sense prompting is not transfer learning you're training the model to make predictions on what comes next for specific tasks or in this case you know a lot of different things and then prompting is just asking it to do more of that um if you want to prompt your model for something that is like what's in your data set if you're you know doing medical work and you have a data set of medical facts and you want to prompt it for medical diagnosis information if it's in domain it's in domain and so you don't need to you know you don't really need to worry about that um and I think you know the happy medium may eventually be a mix one of the things we're doing a lot of playing with right now is mixing you know open source data Plus customer data and seeing whether like what the Right Mix is should you like amplify the customer data more to kind of balance out what's happening but you know maybe it is good to have a little bit of open-endedness like you know if you have a medical data set it's not going to be able to write your HTML for you you're completely right like that I would be shocked if that happens here in a medical context maybe you don't care maybe you care a little bit about the model's ability to just kind of generalize a little bit and then it's a matter of mixing the data properly or doing the pre-training at a specific order I think these are things that like there's paper don't stop pre-training that like looks at this a little bit but I don't know if we have the tools in Academia right now to really look at this closely maybe something like the pile which does have a bunch of separate domains would give us some ability but I'm not even sure we know how to measure success like measuring crafting is hard and I you know their their harnesses out there there are benchmarks out there but I don't know how good they are right now so we're kind of in this tricky place where I'm not entirely sure how we measure and how we make progress in this front scientific we need the benchmarks and you know this is kind of a call to action for the academic world and people who may be listening this is a great project to do you don't need to have a million dollars in compute in fact you don't need to have any compute in order to play around with a little bit of prompting on different kinds of data sets and building benchmarks um I you you mentioned one point that I thought was incredibly interesting this idea of mixing open source data with in-domain data um can you tell me a little more about the approach that maybe you would use your in domain data as a query and then get like the top 10 nearest Neighbors From the pile and then you know then blow it up 10x with that kind of thinking what's been that kind of strategy maybe I mean as a scientist in me my reaction is always what's the dumbest thing you can do first for me the dumbest thing is I don't know you've got 100 million tokens if you're a Dano we've got C4 um C4 is a trillion token so if we were to just sample from both the data sets combined your data would be underrepresented let's you know over sample your data by 5 or 10x and kind of make it 50 50. that would be the dumb first thing I'd try and the other thing you could try is doing stage free training like pre-train on C4 for a while and then free train on your data I don't know if catastrophic forgetting would kick in this is all new science um again for any people listening who are trying to write a paper right now there's so many questions oh my God I wish I were an Academia at the moment to chase after these questions there's so much great science to do um but then you can get into questions of like how do you figure out similarity between things this is where honestly having some kind of if only there were a service that would let you find the similarity between different kinds of objects in a semantic way um you know figuring out what that similarity is and being able to use it so it's we have so many tools at our disposal right now there are a lot of opportunities for us to to play around with these things and really the living limiting factor there does become cost but you'll probably see a lot of the same effects in small scale that you will in large scale so you can study this at 100 million parameters and then test it at 100 billion parameters and see what happens super cool and um so I want to kind of get into the applications of large language models I think uh so like kind of transfer learning for some other task is a very popular one say few shot learning and then you've recently touched on a really interesting idea which is language models as databases you you know query the language model directly you can tell me more about your thoughts on that idea yeah I think this is kind of I I don't think it's too early to call it an emerging consensus but I would call it an emerging zeitgeist that these language models are really kind of shims or interfaces you can kind of honestly look at it two ways language models are databases in and of themselves you can query a language model for the facts that it has contained in it um you know who was the first president of the United States like you know the language model we'll say that George Washington maybe you need to prompt it a little bit with examples of how to take question and receive answer you've seen all this work on Chain of Thought kind of same idea like how do you get information out of the model how do you make the model do math in some sense this is all about the model having information that it's figuring out how to relate or connect in interesting emergent fuzzy ways that wouldn't have been possible in a relational database the other way is to have the model literally interface with data or with some kind of data source and I mean it's an example of that in that sense my other favorite example is the folks in Adept AI who are building models that know how to interface with various web sources um you saw this paper from a few oppress recently where he had a language model set up such that you know it would ask a question it would try to deduce some information by asking question you know it would provide a question and then you know it would either provide the answer itself or a fear to this cool thing where he would just call out to Google and put on the answer from Google and then have it ask follow-up question and then you know just keep using that information but it was queering to Google it was using a data source and then I think there was an example I saw on Twitter today of having a model like write IPython queries that would use the Wikipedia package to query Wikipedia for information and then use that as input so we're getting to this really interesting place where the model itself knows things and the model can interact with data sources in and of itself I think these are really two separate things and I do Wonder from the future we'll train them differently maybe this is what the folks at Adept are already doing but if you want the model to do retrieval and interact with data sources you'll train it completely differently from how you train it if you want it to know things in and of itself but I do wonder whether one cool application may be let's suppose you have a big sequel relational database you just train a language model on that content and have that alongside the database and you know develop ways to query that model for kinds of relationships or kinds of information that are in your database but that you might not be able to suss out from just relations or writing SQL queries it'll find new relations I mean this is what wevia is in some sense and in many ways this is turbocharging it by literally training the model with the data in addition to that so I think the applications are endless in that respect I really want to go back to self ask and I you know seeing your endorsement for the paper and then I checked it out and I just thought it was amazing and but I do I want to stand this idea a little more so is this the idea of say you you know you you take tabular data and you translate it into text like just by kind of parsing it like I read this paper called language interface fine tuning where it's like you know if and then the feature name equals the feature value like you have this template to parse tabular into text is that kind of what the thinking of maybe we move all our data into this text interface for the language models yeah I mean that would be again you you know me I only do dumb things um that would be the first dumb thing I tried what if you just literally take the row of the database format it basically treat it as a sentence like put a little symbol between each of the items in the database maybe give it the field name like put it into a little record format and then those are the sentences you train the language model on I wonder what would happen I wonder if that would be enough given a sufficient amount of data for you to be able to query the model maybe you do need to pre-train it first on something that looks like C4 and then you train this but really the question is just how do you shelve the data into the model and at that point maybe a new relations show up maybe the model has some preconceived notions if you pre-trained it on C4 um but really at the end of the day the question becomes in my mind don't you want your data and your database in some sense I also wonder whether you could have a language model that learns to do SQL queries on a database to extract data out of that the same way we're doing with Google or Wikipedia right now you could kind of go either way but the important part is allowing the model to find these new emergent soft relationships between data like you know I'm totally convinced now of the weeviate mission in that sense like this is you know there's so much we can unlock from our data that we couldn't before by just letting the model figure out how things are connected in ways that are hard for us to write down yeah it's super interesting and um so with this idea of things that are hard to write down I want to come back to the self-ask thing and um Chain of Thought prompting and you know this general idea of language models with tool use I've seen that phrase like tool you set I think that's a pretty nice one uh where it's like say the language model uh queries a calculator or it writes some code and then it runs the code and says okay what's the output from that and and with these search engines I think is just incredibly interesting because we have all this kind of like search pipeline stuff and query formulation things where you might be like uh temperature in Germany and then you query it and then you get the results back and you're like oh actually I meant this particular City or like you or average like you add keywords as you kind of iteratively search oh so where does this self-ask thing this Chain of Thought is it in the training data is it something that is maybe learned from the language how has it learned to be like you know have a prompt that breaks up the question into like the compositional facts and then how how does it learn to do that that's a great question honestly you should invite ophir to chat about that and I I it's a great question I hadn't thought of it that way I don't know where it is in the data and it may simply be that the model is mashing up all this other data that it has in an interesting way the same way that you can get stable diffusion to give you like people sitting around a campfire on an airplane I think I saw go by or like salmon swimming in a stream where it was literal pieces of salmon like you know that was never anywhere in the training except but it managed to mash it up and figure out what to do with it and I wonder if something similar is happening here I what I what I love about all this work like self-ask um Chain of Thought we are developing new querying languages this is like us inventing SQL except that we didn't design the database the database came into being and we have to figure out how to interact with it that example I mentioned about the IPython interaction like that's a again it's a new querying language and I honestly thought the most potent part of the self-ask wasn't even necessarily the self-ask part it was that ophir did such a phenomenal job figuring out a way to measure the complexity of the knowledge that was extracted from the model he gave us a benchmark a ladder to climb a way to measure whether we could retrieve certain kinds of information from models and I think that's going to open the door to a ton more benchmarks and you know what happens when there's a benchmark um we optimize the hell out of that Benchmark to edit coach science forward we develop new querying methods I don't love the idea that we'll go through and manually come up with these querying methods because that just to me seems a little unscientific in some sense maybe it's good enough um maybe some of this prompt tuning stuff will give us ways to discover better prompts or something like that it's still pretty early but you know we've raised really two or three huge questions here one is you know how do you get the knowledge into the model or where does the knowledge come from or even is the knowledge in the model or does the model just know how to interact with data sources number two is how do we query the model number three is how do we measure the efficacy of that query like in SQL you know if you got the right answer like you can go through and manually search the database or what have you do you know if there's a bug in your like database compiler here how do we know how to measure what the model knows is there a world in which actually The Innovation that allows us to dramatically reduce the cost isn't a bigger model or a better training algorithm but a better querying language that allows us to find that actually gpt2 had all the same knowledge or it could do much of the things that we can do today with gpt3 we just didn't know how to ask it for it we didn't know how to get it out of the model I don't know that I like that makes me dream I kind of wonder if a billion parameters may be enough in two years because we'll get better at the querying part but to do that we need better knowledge measurement and if I were a grad student right now those would be all the cool questions I'd want to work on because you don't need a huge model to do it like oh sure told me he used 900 worth of compute on that entire paper which is just insane given that I have graphs that are 10 times worse that's not in some of my papers yeah well that um so fear came up with this like compositional celebrities Benchmark where um in the it's like um uh who I think it's things like who was president the year that Justin Bieber was born uh so you you chained together these facts and then it does that uh multi-hop compositional question and uh Jonathan we brought up the um the uh I think it's salmon swimming up a river and um it's a diffusion picture and and it's like literally like uh literally but it's like the food is like in a river it makes no sense and I'm sure that future without the training set and people have pushed it enough it's not just memorizing laughs but uh so I'm very curious about this like compositional generalization I think like the Gary Marcus angle is it's like um uh a cup of coffee with holes in it it shouldn't be able to exist right is because the coffee would come out of the cup or uh the ball on top of the the like you chained together all those adjectives like a round green rectangle on top of uh uh purple cylinder what do you think about this compositional generalization is is this like kind of the ultimate generalization test I don't know I try to stay away from the Gary Marcus conversation to be completely honest um you know you can listen to old people talk a lot if you want to or you can do real science um that's all I'll say about Gary Marcus right now on Twitter um but I do think it's an interesting question as to what kind of compositionality we want um you know when it comes to video you could get this kind of behavior I think really you know what may be missing is that we're working on pictures and not on video then when you see video you may say well there are still things that can't really figure out um okay fine you know we need to move to some kind of better World model so it's really I think the question of like the coffee cup with holes in it is a little bit separate from the answer of kind of compositional reasoning on the compositional reasoning side I mean I think ophir may have pushed these kind of compositional questions to their limit like once you get into three part questions like who was president in the year that the person who wrote blah blah song was born um like when you get to the third and fourth order questions it just gets like humans wouldn't be able to answer those questions so I think ophir has shown us a way forward and there's going to need to be more Innovation on how we measure the kind of knowledge that's in these models how we find other things that require logic to work their way through or it really it is a throwback to some of the 1970s and 1980s style AI where we might need these giant knowledge bases where we can come up with facts that we or deductions that we know to be true based on these you know these expert system sorts of databases and then you don't need to ask the model to try to reason through and figure those same things out deductively and I don't know I think that's a really cool world to live in and a fierce paper like it points the way toward a future no we have benchmarks that look like that where ophir doesn't have to manually generate those questions but we have you know maybe we dust off some scheme from the 1970s and that may already be there for us to pull on yeah well I that kind of thinking about how we do this like aggregate query in these language model databases very interesting like if we have a collection of articles about hiking trails in you know New England let's say and then we want to ask how many hiking trails are in New Hampshire and then it needs to have that kind of intermediate and the language model is the interface of the aggregation that there's this other paper called like neural databases is something that had like this um you know a symbolic aggregate step explicitly set and I think all that is just incredibly interesting um I wanted to also ask you about a kind of different way of thinking about language models the retrieval augmented language models and you know like the whole idea of uh you have the you retrieve something and then you maybe it's like an encoder decoder where what you retrieve is you know fusion and decoder style rather than just being say prepended to the input and then you know decoder only gbt um do you have any thoughts about things like you know rag Atlas these kind of models a retro yeah I like I love where we are in the field right now um we have models that we can train to memorize data we have models that can use you know we have retrieval augmented models we have the ability to fine-tune models we already have we can prompt models we can do in context learning we can have the models use external tools we're and we can actually literally train the models to use the external tools or we can prompt them to use the external tools probably one or two of these are going to stand the test of time I have no idea which and I love it I absolutely love it right now like this is this must have been how it felt in like 2015 for those of us the 99 of us who aren't in this field in 2015 when Vision models were just exploding and every week it was like a whole different way of doing things Fiji resnet Inception these were all such different models that were trained in such different ways Adam was floating around at that time in different optimization strategies Bachelor and pops up layer Norm pops up it's just this whole sea of ideas and it was hard to know what the best thing would be dancing at wide dress net like it was just and here we have even more diversity and just completely different paradigms for interacting with these models it is a great time to be in the field like I don't know people keep complaining like all the cool stuff was done this is the cool stuff like this is why I'm here because we have no idea what the right answer is and there's so much science to do and for the people who sit down and systematically develop ways to measure this and systematically develop real world scenarios and honestly that's a lot of our work at mosaic unfortunately it uses a lot of customer data um because and we can't release that data um although we try to find ways to share the findings and we can reproduce it on public data sets but you know what a time to be in this field so much happening yeah how much um like how much of this do you think is from the Transformer and just text as being sort of the interface with multimodal domains I feel like a lot of the well I mean yeah there's a lot of breakthroughs to look at but like I feel like the sort of text and NLP to me seems like sort of maybe the biggest I mean um images of course but I think like this text interface is just such a unifying way of doing it and then also for creating user interfaces for uh people to interact with these things uh so many pivoting topics um I heard you know bachelor normalization later normalization um can you tell me about the state of uh composer and all these regularizations I'm so curious about you know the training recipes and making it easier to train models yeah so I'll walk you through a bunch of things so Mosaic the way that we think about this is at the end of the day we have kind of converged around four or five big models so much A lot's built um you know those are things like resnet which is not only for image classification more academic but also as the backbone for a lot of object detection segmentation models that are used in practice today you know we talk a lot about the big Vision Transformers and everything but by and large when we've seen you know good old dressnet is still the old standby then you have you know your segmentation your object detection models we've again converged around a few different heads for doing segmentation for doing object detection so we're kind of you know there are a few different pipelines NLP you know it's Burton GPT that's the show you know there are other sorts of things kicking around there is retro you know kind of a tweak on TPT there's you know things like um she was completely slipping my mind um but you know there are a bunch of other paradigms out there by and large it's broken GPT that run the show um and at the end of the day those standbys really are what works at mosaic a lot of them are focusing on the scientific side is developing really great recipes for training those models so you know we released our recipe back in June that was our proof of concept to basically convince ourselves it would be possible to speed up models this way and we got a 700 speed up over the Nvidia bench bass lines which I'm so proud of to be fair it's an old Benchmark um but even over some of the newer stuff like the Tim models we still were looking at a 2x or 3x Improvement and accuracy levels that had never been achieved on press that 50 on image that without help from outside data sets so we're really proud of that and you know we're doing it for everything so honestly I'll just you know I'll go ahead and spoil it for everybody we're going to be releasing our segmentation model next week I believe that we're looking at the team has told me anywhere between a 5 and a 20x speed up depending on your Baseline um it turns out even the baselines for this model weren't very good and we spent a little bit of time improving those we had a blog post about that over the summer and then once we've improved that Baseline you know we were still getting between 5 and 20x and we hope this will be really useful to the computer vision Community those who need segmentation it's a smaller more Niche group um but it's a Workhorse and it's really valuable and then on the Bert side we'll be releasing our recipe probably in the next month or so you should expect again about a 4X speed up on pre-training on you know things like C4 but also probably on any data set for academic purposes this is going to be huge well you know we honestly I can't talk about one other thing that we did with it because you know I'll just say we're still in the ml Perth quiet period um but you know you'll see more about Bert soon on that front as well and we're really proud of that some of this is even not even with the algorithmic speed up so with really good systems optimizations as well um GPT you saw our Baseline released you can see this pattern Baseline speed up Baseline speed up Baseline speed up um did that for estimate in June the speed up for Baseline was back I think in July for segmentation three months later speed up um you just saw our llm Baseline a couple weeks ago you know what to expect toward the end of the year early next year you know that's what I'm hoping we'll have TPT three for 100k and then you know Bert Baseline we didn't really push the Baseline too hard this summer um but you'll see the speed up out in the next few weeks and so you know if our track record keeps up we're just going to keep doing this object detection Baseline coming up in two weeks you know the drill um I'll start working on self-supervised Vision you know the drill we may work on some diffusion models you know the drill and it's just you know keep cranking away at this honestly on llms I think there's more than 5x there I'd love to get that cost under 100k you know I'm not going to promise it because it's hard but I'd love to get it there and what that also means is that a 1 billion parameter model gets down into a couple hundred dollars for an academic researcher that's what image that used to cost that's huge I'm still the second year PhD student who didn't have as much compute as his friend at least in my heart um I want all Academia all people in Academia to be able to do real scientific research that the folks in Industry aren't doing so you know you can see what we're doing on that front and then a lot of the big promises were building it on Miss Mosaic Cloud which is a platform that is meant to make it really easy to do this stuff you know you know multi-node training where you don't have to deal with any of the issues good integration with our recipes where we can automatically profile your network and figure out which speedups to put in depending on your data set depending on your model those sorts of things we can catch you know lost spikes basically it's you know trying to imagine what do I wish I had had that I was doing my PhD and hopefully you know it's useful to more than just me but you know the hope is that'll allow us to stay in business long enough to keep giving cool things to the community um and my hope is that if you train with us it'll be cheaper than training with any other Cloud because with our speed up methods you know our a100s are magically 5x cheaper than everybody else's so hopefully that'll you know that'll be a nice gift we can give to the community in terms of dollars not just in terms of using the speed up methods yeah well it's obviously you know Baseline speed up is a nice Trend to be following um could we I mean um yeah like you mentioned um you know detect Mosaic will detect a lost Spike and then I think it resets it and maybe orders the data differently um tell me a little more about how uh the the recipes are a composition of like uh you know batch normalization or different attention layers and and maybe like clever ways of doing Dropout like can you tell me how the recipes are unique to each data set or or network or like how how is it not is it not just the same recipe or that kind of thing it's a good question it's something we're always studying and especially right now we're doing a lot of probing for our old recipes figure out how broad they are like all right let's take less than 50 when they mention that because it's a benchmark most people are familiar with what happens if you run that recipe on the places data set does it completely fall apart have we way overfit our recipe damage net have we always fitted what if you changed the resolution have we way over fit what if you change from reset 50 to resident 101 so we're working right now not just on these great flashy recipes that give you a 7x speed up put on nice generic recipes that give you a consistent 3x on a wide range of different things in an area so you know for Burton gbt that's a lot easier because language is language to a large extent for vision that's a lot more complicated because there are many other angles to take so especially if my vision team were here right now they would tell you that's the biggest worry on their mind is how generic are these recipes and how do you make them more generic but at the end of the day you know a lot of what we're doing if you start to look inside it there's some stuff that's a little bit bespoke um you'll see this with our recipe for image segmentation where you know if you're measuring dice as your metric you know we've added dice to the loss because you know that's a really good way to improve your dice and it's something that people have proposed and once you do it it does help a lot um but there are other things we're doing like exponential moving averaging to the model which is pretty generic it works across all of our computer vision tasks so it's hugely helpful in MLP it doesn't necessarily help but it doesn't hurt so there's no reason not to turn it on or something like sharpness aware minimization where there are some hyper parameters you have to set but it's been generally helpful for computer vision so a lot of this or mix up a lot of this is actually pretty generic and I have no reason to believe it won't work on pretty much any computer vision task it's up to us to prove that you know the the burden of proof is on us but so far so good and I'm really pissed like you know it's I hope we'll have more results on that in the next month or so on how generic these recipes are and whether you're released like you know the generic recipe and the one built into our Cloud will be the generic recipe that'll be what you get unless you really want to you know go for it but you know if you can get 3x instead of 5x but it works across the board I think you're pretty happy too yeah well and I it's a very interesting point uh from imagenet to places or you know satellite images I don't know manufacturing images and the thinking of like have all these data augmentations just been developed for say imagenet or does this really work for any kind of computer vision a super interesting idea I don't know yeah I mean um it definitely seems like maybe like some of the cropping uh yeah I mean the whole like think about like the difference between satellite images compared to image I think is very interesting personally yeah if you would ask my vision team they would tell you they're specifically worried about that right now um and so they're working on it and they're getting a bunch of data sets and we're going to find out what the generic recipe is and I think we'll have you know maybe that'll be the third blog post in each series you know it's our Baseline our best recipe then our generic recipe and I think that would be fun um there's one other thing I wanted to throw in here which is really one of the big aspects of llms is actually this fully sharded data parallel Library and I think it's important to discuss for a couple of reasons um so when we train large language models they're too big to fit on a single GPU in fact they're too big to fit on a single node oftentimes and not only that but the activations are too thick so you have to do what's typically called 3D parallelism which is where not only do you split the model across multiple gpus but you also split the data across multiple gpus which is what you usually do data parallel training you also do pipeline parallelism where you send each example through the model in a pipeline fashion you accumulate the gradients as you kind of pipeline the examples back and then at the end you apply the gradients to model and then you repeat so you don't even do it all in one step and you're constantly streaming activations from you know first part of the layer to the next you might even have to do tensor parallelism and split a tensor across multiple gpus these things are huge um and there's some fantastic Frameworks for doing this so in the past one of the very popular ones was deep speed is super popular for doing large language model training we've worked closely with the Deep speed team and they're awesome um they produce some awesome stuff in fact they produce this other cool thing called xero and it's a series of sharding operations for charting the state of things like your Optimizer and your model across the gpus and even in some of the fancier floatations you know even doing activation checkpointing to the CPU and even doing activation checkpointing to non-volatile storage attached to your model this is really cool that's something I'll talk about in a minute because I think that's just the coolest thing in how they do it because if you train a large language model you do so much reading and writing but you'll actually burn out your SSD in the course of one training run but that's fine because the rent is so expensive that's one SSD um but the nice thing about these is that these sharding techniques actually reduce your need to do all these data all these parallelism techniques and so this fully shorted data parallel Library this is out of the folks at Fair um but what it does is it's just data parallel but all it's doing is it's using these zero techniques to Shard the model stay at the optimizer State across all the gpus in the cluster and so instead of needing a ton of GPU memory on a single node to do data parallel training and again data parallel training is just where you have the model you know you have multiple copies of the model in htpu you run different examples through each through the models forward propagate backward propagate and then merge the gradients so it's kind of it's the dumb thing that you would do if I gave you eight gpus and told you to train that's what we're doing on literally everything else if you use pie torch distributed data parallel for example it's a naive thing fstp takes us back to a world where we're just doing data parallelism you don't have to do model parallel we don't have to do tensor parallel we don't have to do pipeline parallelism which is the nastiest but we're doing smart sharding so it requires a lot more Network bandwidth in order to do the synchronization but it's way simpler Megatron is notorious for being hard to customize the models that we released for fsdp it's a one file and you just have to wrap some things in fstp but it's one file which means you can customize it it's pure Pi torch so you can imagine for us at mosaic we want to do speed up methods we need to be able to modify the heck out of the model in various ways that's really hard to do in Megatron it's a piece of cake in the simple notation and I think it's what I like it too is kind of a like a cafe versus tensorflow versus Pi torch sort of thing like we wouldn't have gotten a pie torch if we didn't have some Frameworks that preceded it that got better about coming up with the right apis I think it's the same thing with fstp in some sense it's superseding Megatron or deep speed at least for the moment which are superseding hand rolled Frameworks that you had to come up with yourself um because we've learned how to get the interfaces right we've learned what is and isn't necessary to make things run well and so my argument to all of you is if you like to Pi torch over tensorflow you're probably gonna like what we've put together in fsdp over Megatron and it's customizable and easy to use if you want to tweak things and I think that's resonated with people so you know maybe we'll have the Jacks that will eventually replace this but at least for now this is so much cleaner and so much easier and you don't have to use things like Megatron that are really unwieldy and difficult to work with and I've heard nothing but complaints the same way that in the tensorflow days kind of you know at least in tf1 um it was the best game in town but it was still something everybody complained about I think we still complain about Pi torch but a little bit less and now people seem to love chaps I like to think we're getting to that place with llms and again they are for everybody but we're still getting the same utilization you would get with Megatron you're not giving up any speed yeah well that that's so fascinating and personally I don't have enough experience with these things to really keep the debate going but I I do like completely appreciate the abstractions and the apis it it's just all incredible hearing about all these things and um and yeah so maybe uh pivoting topics like could you tell me about your experience with um building Mosaic as a company you know it's growing like crazy it's so interesting to see this you know company emerging can you tell me about your experience building this it's been hard I think our CEO Naveen Rao will always say you know building a company is one of the hardest things you could ever do um there are a lot of hard things in life this has been the hardest thing that I've ever had to do um definitely takes the cake on my PhD at a PhD you know you have to do good science that's really hard and you have to create something that's never been created before that's really hard um not only do we have to create something that's ever been created before you have to create something that's actually useful to people and useful enough that people will pay for it we have to understand what people want we have to build one big thing together a PhD is very much an individual Pursuit building something together is hard um you know trying to figure out you know you had seen like to get to the base we do the Baseline blog post three months later we do the speed up blog post and to get those baselines it's probably another three months of work which means that we have to predict what our customers are going to need six months in advance if we want to have it ready that's hard this is all really hard in a space that's changing so quickly and in a financial environment that's changing so quickly 2021 was a great year to be raising money um 2022 is not it's been a tough year for startups it's been a tough year for everybody in terms of you know tech companies and so all of that is really difficult keeping a team focused keeping a team you know with our eyes on the ball and what matters trying to serve our customers appropriately it is so hard and I'm sure you can speak to the same experience it is so difficult and I love the challenge I hope they'll succeed who knows um but the experiences I've gotten and what I've learned have just been incredible I really didn't plan on doing a startup that was I mean we've talked about this a little bit more privately you know that that was not my plan in the least um I'm really glad that I had this experience that's super interesting and you've always amazed me with how many things you're able to do in parallel I think when I first met you you were at Georgetown you were at MIT you did Facebook how to like and now you're a professor at Harvard and Mosaic how do you manage them like how was your kind of time think like I'm very curious like how you are managing your time sort of not in your new role in Mosaic and also obviously your role in science it's tough It's genuinely tough I mean I'll tell you two things one I haven't started at Harvard officially yet I get one more year but Harvard has been very effective at finding ways to get free labor out of me so advising students which is a blast um and I'm also managing their compute infrastructure because I'm you know I'm doing that in my day job so it's really useful there it's been a lot of work and then you know between you and me I still haven't finished my dissertation I'm working on it but it turns out mosaic has been a huge distraction my advisor will ping me periodically and you know make fun of me like you know your students realize you don't have a PhD yet right um it's the biggest running joke at mosaic that like you know oh Jonathan he doesn't have a PhD yet yeah my friends find it so funny that I have a PhD and my boss doesn't um it's hard and I'm not sure I'm that successful I've also chosen to prioritize work-life balance lately which has been really nice but also um startups are demanding and they will take all the time they can give them and I work really hard with my team to make sure they're taking vacation I yell at them if I see them on slack on next weekend there are times for that and there are times when we have to push but it shouldn't be all the time and if you're in a place where you have to burn yourself out to be successful I don't know if that success is worth it to be completely honest um there's much more to life than you know suffering for your work and you know so yeah the answer is I don't have the balance I should have but I have a hell of a lot more balance than the last time we talked to the time before that and I'm really glad for it but it's hard um at the end of the day the people I'm most responsible to are my team they're counting on me to make sure they still have jobs they're counting on me to make sure that they're doing the right work and you know when push came to Sheldon it's a matter of taking the evening off or making sure that my team is in a good place and is safe and is happy you know you've got to be people first yeah incredible Jonathan thank you so much for doing the podcast it's so amazing getting to pick your brain about these things there's so many interesting ideas so caught up with it and you're building such an incredible company and and yeah it's it's really cool to hear how you know the people first the balance and all that good stuff instead of some kind of like let's work ourselves to death culture but but like yeah overall want to do the same feedback back to you um you know there's a life outside of work don't you forget that either you lead a pretty busy life yeah well thanks again Jonathan thanks so much for doing the review podcast and it's awesome I'm so excited about these things thank you so much for having me ", "type": "Video", "name": "jonathan_frankle_on_mosaicml_cloud__weaviate_podcast_26", "path": "", "link": "https://www.youtube.com/watch?v=oFyYaZbRviY", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}