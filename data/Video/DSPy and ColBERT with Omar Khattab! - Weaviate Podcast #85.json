{"text": "Hey everyone! I am beyond excited to present our interview with Omar Khattab from Stanford University! Omar is one of the world's ... \n[Music] hey everyone we're here with our first interview at NPS in New Orleans 2023 our first in-person interview ever for the weba podcast and I can imagine a better guest Omar it's so cool to meet you cobbert dpy this latest Aras paper I've been you know so interested in your work and it's really cool to meet you thank you Conor great to be here with you awesome so could we maybe uh sort of the question we want to do in this series at nurs is like um before we dive into your works and further like discussions on that just this general question of like it's pretty broad but just like what's the current state of AI and maybe yeah where are we headed well I'd say it's very clear from the last few years especially the last year that there's something to these language models that's really powerful um and I'd say that um the mainstream has been and it remains that um these language models will are are becoming increasingly these powerful hopefully reliable you know user facing systems and I um don't believe that's the case I believe that they are uh becoming these really powerful abstract text generation devices um that can be used to build much more powerful programs much more easily than we could do before so there are these programming tools that could be really empowering um but they're not reliable systems and so all of the challenges that we see currently on you know what's called hallucination you know reasoning issues um inability to um you know to to follow strict rules um to use tools um and issues around reliability in general um I think the real um uh fix here is not going to come from better RL uh algorithms or data is not going to come from scaling up it's going to come from um just a different way of thinking about using these models uh as devices in in bigger pipelines fascinating so I I've been trying like so I think with dpy it's the perfect example of using these uh models as devices and pipelines and I so I I first I think I first started to understand dpy when you gave the talk in Jerry's llama index webinar and this idea of compiling a program I thought that that stood out to me very strongly and then um reading your paper arrays on uh using llms to evaluate rag pipelines it had this like uh it like I think I'm starting to understand this like when do you when can you use zero shot when do you need few shot examples and then when do you need fine-tuning then I guess like if you could help me connect the dots between uh so there's that scale of zero shot fuse shot fine tuning and then there's also like searching for a program like when do I need multihop query decomposition and uh tool use and so I guess that's for me it's trying to connect like the the query Pipeline with this fine-tuning so in general there is a tremendous amount of progress in the field but there's also this sense of Crisis we don't know how to do evaluation we have these models that are so powerful it's very easy to build kind of half working systems but it's hard to know you know how well they're working and to compare them and to Benchmark them Etc and I'd say the Central Key sort of thing that we need to address here as people building things with these systems and people evaluating them is um what are you trying to build usually you're trying to solve a specific problem a particular task and so once you define your task the question should be what would I do what's a good approach you know um a person someone with with reasonable you know um awareness about the the constraints of the task what would they do so rag is simply like you know retrieval augmented generation is simply kind of the base case here of a lot of these tasks if I'm asked a technical question about something I might not remember exactly the first step I do is search um quite a lot of other tasks need more of a research sort of mindset where you know you need to look for information read it summarize it potentially follow new links browse a bit gather all that stuff before you commit to An Answer other things require you to use tools Etc so if you start thinking of the language model as this you know device that's connecting all of these pieces and you are in charge as as someone expressing well defined programs as opposed to like you know coercing uh um a statistical model um um through kind of PR gestures um um a lot of these a lot of these issues become a lot um you know become a lot more actionable and in an iterative way as well um so you brought up the point of you know few shot you know zero shot Chain of Thought retrieval multihop Etc um what's nice about DSP is it just very concretely takes the stuff that I'm saying now and gives you a a particular framework to think your problem your way so it just sort of lays down a bunch of modules the uh key philosophy here is that the way we want to use language models is very similar to the way we uh started um you know like the way the field sort of reached a consensus around how to build neuron networks you know it wasn't always so clear that you had you know these feed forward layers that are just um you know you had convolutions and uh you know dense layers and and uh convolutions of various kinds and attention and and recurrent uh layers it it was really uh you know a much more organic thing at the start the same way we're seeing with prompting techniques now and what we're saying in the asy is these prompting techniques make sense in general but they shouldn't be these conceptual handwavy things they should be concrete modules that have actual algorithms that are well- defined and we should be able to um Express programs in terms of them so if you go into dsy right now you will find that there is a Chain of Thought module a program of thought module a react agent module Etc and when you're using one of these you say like my task requires a chain of thought that takes questions and generates search queries or takes um you know uh a long document and gives me a summary and you can apply these interchangeably with any of the of the modules that are there declare your modules and then start connecting them into a a larger pipeline you know the way you would normally Co code by calling them as functions just the same way as like you know you would program in in pytorch if you have built neural networks in that before and then um the process of compilation borrows a lot from program syntheses basically we're saying when you have a program like this the missing piece is how should I condition the model on my definition of the task to conduct each of the steps well how can I teach the model to generate good search queries how can I condition the you know teach the model to take my passages and generate a well- grounded rag response and BP's answer to that is automatic compilation you give us a metric of what you're trying to maximize maybe I want my answers to be faithful to the retrieved sources and I'll Define that in a particular way now given a metric like this we can apply very simple sort of uh but really powerful optimization techniques that say what goes in the prompt and what I up what updates I can make to the weights are actually these latent variables that I can sort of optimize and learn um as parameters of these modules and I can do that you know over a whole uh program not like piece by piece um to maximize the quality of your thing and the idea here is that you know there's this debate is prom engineering going to stay or is it going to die and the and the true answer is these language models are powerful precisely because you know telling them to do things differently results in different outcomes now they're too sensitive right now and very small perturbations break them that is not going to stay necessarily if the models become better but the idea that expressing your constraints better and so of um asking the model to solve tasks in a better broken down way will lead to higher quality is here to stay and what we're saying is um as long as you can take care of the program structure of the problem uh decomposition we can almost always find uh very simple automatic um you know automatically generated prompts and demonstrations that make sure these pieces uh work well together and what's really powerful about this is that um you know in as opposed to a high quality prompt for a particular model which breaks when they update the language model this stuff is language model adaptive model changes the prompts get recompiled get new ones um the other thing is that because this automatic optimization is there you are forced to sort of contend with what is it that I'm trying to optimize for do I care that the answers are correct like what do correct mean does it mean it's complete do I care about recall or do I care about Precision do I care about um you know um the the citations are faithful what does that mean for me and um that just generally in in my experience working with people leads to much better systems because people are um you know sitting down and defining what they actually care about and the third thing which is really important is your costs can be cut dramatically people hear me saying there's all this multihop and all these stages and they think my bill is going to be four or five times bigger but the truth is and I have like you know we have a lot of results of of this sort where you know you can actually take something like this and compile it into a much smaller model potentially through fine tuning it's the same code you're just asking a different compiler strategy to be activated um and because the model is so much smaller you know you could run it on a CPU let alone you know um things like you know paying for gp4 or or or whatnot so you know in general it's a it's a holistic Paradigm and we kind of are actively adding pieces here and there we hope to release a bunch of really cool stuff very soon I say release between codes cuz a lot of them is are actually merged we just haven't announced them yet so like if you're actively using there might be features you're using that are not technically um you know not technically announced yet yeah well first of amazing overview of it all and I I guess um oh there's so much to pick apart of that but I I kind of want to just drill into this idea of um so so to me it sounds like the analog is with neural network architecture as we used to be like um you know do I put four layers of convolution eight layers of convolution where do I put my bottleneck to compress it and this kind of design and now we're stepping into this new design space of like do I need a query rewriter do I need a multi-hop and so we're searching through those building blocks of like SQL router like just all these components yes and then once you find the pipeline you could kind of copy the input outputs to fine tune it to a smaller model well you would you could in principle explore like fine-tuning the the pipeline into a model in my experience and that's something I I I hold strongly a lot of the decomposition is is very essential like there's you're never going to in the general case um distill retrieval plus gp4 into one model because that model does not know whatever the retriever was going to do but you can um compile the individual pieces into much smaller model coals that are highly specialized um so the program structure is here to stay although some parts might be fused uh again more analogies to neuron networks there um you you brought up a really key point though a lot of this ends up being what is the right program you know structured what are the right modules that I want to use and there was a a period in in NE Network design where it was older age like you would propose a new architecture that has like two extra layers that do something different and some of that was real progress other was maybe not as real and I and I suspect that there's a period where this is going to be completely essential we have to go through it because we need to discover the right modular pieces but I think eventually like what happened happened with Transformers someone is going to or like a group of researchers over time is going to converge towards some general architecture an agent if you will that can sort of um be uh compiled on Broad data kind of like a foundation language model or Foundation sorry uh program um and um um until then we are forced I believe to uh work with you know these these these modules that we have and in some sense in neural networks it's still matters like you know um the new language models coming out all the time like mixture of experts that's that's a new network change um people are tweaking lost functions to uh not lost functions um activation functions to this day and it still matters um and so the level of abstraction is just rising to um to this what is key that it's it it is U not you know um um the focus is when we build these NE networks you you should never find discussions of like what should the weight of you know the fifth uh parameter in the third mlpb um you know should I should it be a large number or a small number and similarly here it shouldn't be the conversation shouldn't be like did I ask the model to think step by step or did I ask the model to uh take a deep breath these should be automatic automatic decisions um and you know the optimization literature in general has more than enough to start building things in this space there have been some really cool papers coming out recently that that actually look at this from a you know um from a more sort of continuous optimization standpoint but even in just discrete um optimizations work like store um um um from Eric zelikman itala at Stanford really powerful sort of simple paradigms for optimizing um things like that and and DS spy has a lot of these optimizers built in for instructions for fine-tuning for bootstrapping demonstrations for very complex um programs and I think this Paradigm is here to stay so I I've read this paper prompt reader where it's like you evolve discreet prompts and um so I guess this search like the take a deep breath thing is such like a like how would you search for like who or or or what is it where it's like um uh if you solve this is very important to my career I'll give you $100 if you solve it and like and now it's like MML U Benchmark you know plus five so the the the take a deep breath one is actually a good example in the sense that that paper discovered this through automatic optimization they didn't like write it down maybe my career depends on it is more of a manual a manual one I think these are actually I I find these interesting in their own right from a sense of understanding these models and learning about what matters to them what affects them so that we can later automatically optimize it I'm I I'm in no way opposed to like research that's trying to or research or just like exploration that tries to understand them better I'm saying that when you're trying to build something with it though that shouldn't be your approach like that's a cool exploration that's like you're poking it and seeing what happens that's fun uh and also very insightful it's it's not how we want to be Building Systems um so in terms of prompt breather uh language models or optimizers really cool papers have been coming out sort of in a more discrete side of things uh like these um and uh in general what DS Supply sort of serves as is these papers as as as awesome as they are they take the view of the language model as a system so you have one prompt and you want to get like the really high quality sort of prefix before the model starts generating things so that the output is really good on on a particular task um in dpy language model is just a device there's a program in Python or whatever that's actually being executed in a well- defined way and it just makes you know CES as needed to the language model to implement specific steps what we call signatures uh in the program and how do you sort of Nest or compose these optimizations when you have multiple layers like these um is is key progress on optimizing individual bits and pieces like prompt breather or language models optimizers have been really um effective one of the things we built recently was actually based on um language models as optimizers I think it's called opro right so we have uh built you know things based on that um in general beijan optimization is is is an Avenue for this people I also look at like uh you know um a a lot of a lot of other angles of optimization um you know much much broader than this reinforcement learning has a place here for sure um but all of these are solutions that that the that the community has more than enough technical skills to resolve the real shift has to happen in the way we we view these the way um people building products and Building Systems you know make the decision to invest in their in their task definition you know what is it that I'm trying to measure what I'm what is it I'm trying to to maximize and we can't forever keep saying evaluation is hard it's hopeless well if you if you look at the language model as this generalist that should do everything yes I don't know how to evaluate that but you're not you know you're not deploying that you're deploying you know an email assistant or you're deploying a code um assistant even if you're deploying GPT there's still a list of things that people actually use it for in practice like generating code for this or you know checking um you know sources for that or whatever it is and you can start treating these as as metrics or unit tests that you want to pass um and um maybe that's what open AI is doing internally um but also uh in a in a in a sense where um you're focused on um program structure task decomposition um where the language models do what they're really good at which is transform um you know transform text um in a way that fits these um these contexts and these uh signatures as opposed to this old knowing monolithic um architecture well I already feel like my trip to New Orleans was worth it because I got the sign the way that you connected that and brought the signatures thing I think I I got have such a better sense like uh you have this query rewriter uh then maybe you multihop and then maybe you uh then you retrieve and it's like how to search and each uh each module has a signature and then you optimize a discreet like take a deep breath like something like that in each part of it and yeah that's really and so I guess this is so this kind of I think our conversation is naturally going to like you have a specific thing that you want to do so you compile your program to your specific thing but I'm also curious about like uh like an an llm specialized for text to SQL right that's like one of the most common specialized LMS we're seeing do does that one can we just have one text to SQL model or do you think like it should be customized to your schema and then would that concept generalize to like you know a multihop like would we have an it's kind of like areas in the llm for evaluation like can we just have one llm that has a prompt like based on the query query are the search results relevant search results please give a score on one to five or does it have to like be customized to a domain so My Philosophy is you know X is All You Need My Philosophy is use all of what you need like don't unnecessarily constrain yourself uh I'm only for specialized models coming in within this Paradigm where you're building bigger you know scaffoldings or systems around these models um ideally the way you build this is um is modular in the way that's consistent with this abstraction so in particular in dsy I'll use an even simpler example um you know since since I built Kar and is something we we we we use a lot um the default sort of retriever in the examples in dpy is Kar but it's in no way um specific um to coar dpy can work with any you know we have a wave8 um built-in um um um retrieval sort of abstraction um if you look at the way the uh programs are structured there is a dspi retrieve uh module and that module doesn't actually isn't actually tied to any of the particular back ends the same way as like if you're doing pytorch and you say I want um a linear layer you're not saying I want a linear layer layer that runs on the CP with the Intel mkl uh you know back end you just say a linear layer and then when the code is running it selects the you know you or either you manually select or it automatically selects the right back end so similarly here you still have a um um you know a predictor of some kind that takes a signature in this case the signature is maybe uh text to SQL like you know question to SQL quity and um when it's being compiled either an automatic process Maps it to this model because it thinks it's going to do well or tried it and it worked well or you manually say like please map this to that particular model but the idea is that your module itself is is abstract enough that it's focused on what is the behavior I'm trying to express and in what way am I trying to express it the behavior I'm trying to express is the signature and in what way I'm trying to express it is the is the module class itself like Chain of Thought is one uh basic prediction is one um you know react is another uh that has multiple calls and turn um Etc so so specialized models have a have a huge place to play here um but they're not like privileged they're still um compilation outputs for compiling these General uh signature based uh modules yeah I guess like um like I I love this I I I agree I think this like whole like llm is a CPU like Andre Kathy's you know Narrative of the operating system I really like it a lot and then I and then there's the compiler half of that and I guess like I've had um there's I think there's a new paper that's titled like LM compiler that's kind of about like you take a task and then it's sort of like autog gbt style where it like decomposes all the subtasks of it and then it like resolves dependencies and that would be like one perspective of thinking about a compiler this is this is a more traditional use of the word compiler where I believe their goal is to like it's a compiler from a system standpoint to make things more efficient to find paths that could be eliminated uh this is very important and um I think it's increasingly going to become essential as the programs themselves start to have a lot of branches and a lot of like complexity um this is not the main use of the word compiler that we've been using for you know all of this year uh for 2023 where it's really more of a program sentences standpoint you have a program but they're missing pieces like what are the prompts I'm going to use um for example and the compiler's job is to look at your program and then fill in these missing pieces in know in accordance with your metrics with your assertions and other constraints that your program specifies so these are two different things um but what's nice about the word compiler is that like it conveys you give me a program and either will'll give you more effective ones or more efficient ones or hopefully like you know a bit of both amazing well I I thought this whole discussion of DSP DSP was just amazing and the whole mod signatures all of it sounds super compelling I think anyone listening would be convinced of this I have to ask you a question about uh Co I I remember listening to you on I think it was Christopher Manning's podcast where he says coar like Steven coar the actor and I or I think a talk show host and I I always thought it was a Colbert but I just heard cber well it's no big deal well we we we have a it looks like we have a you know a pattern of names that are really confusing dsy or dpy kber or Colbert it's all right yeah but so I I I I remember so I I had kind of like fact checked my understanding of it with Nils rhymers quickly and he said like you know sounds good but I think you know here with Mark be the best so is the idea behind it that you know right now with uh you know vectors we're pooling you know each at the output of ber you have a vector for each of the tokens and then we pull it and that's the vector that goes into Vector databases typically but the idea of late interaction would be you first search with that pulled vector and then you get like say the top 100 neighbors and then you have like this inner product with those other vectors no not quite yeah fantastic although although that could be one way to do it uh but we we we don't happen to do it that way um so it's your your audience is familiar with like buy encoders you know you have a a model could be a black box like could be coher for example or could be like a birth based um thing and it gives them um a vector for each uh document in their data set and then when they have a it embeds it into another vector and does similarity search maybe through a service like yours um on the Other Extreme of things are rankers or cross encoders so here you're saying I want a much more powerful contextual awareness I want to go to the model and say please take my question and my document and give me a score and I'll repeat this with all of the documents that I care about and then I'll sour based on these scores this is known like it's it's established and generally not not questioned this is much higher in quality because well you're relying on a Transformer as opposed to a do product um and um the reason it's more powerful intuitively is all of these interactions between the terms basically the model is able to say like oh I see this word here in this context which kind of looks like that phrase there in that context so it starts to aggregate these um these signals um as it as it builds up the final relevance score so what's why don't we just all use that the problem with this is that every time you want to score a document you have to go to that the language model and give it the quy and the document um and it has to re encode it from scratch so if you have 10 million documents I mean frankly even if you have 10,000 um now you have to call the model that many times and that's just infeasible in in the in the grand scheme of things so people generally resort to embedding things into a vector and you know working to make sure that these representations are as high as are as high quality as possible there's been tons of and tons of work uh you know in this space over the past uh many years Kar takes a different approach it says well why why are we giving up so quickly on these interactions what if we could have late interactions that keep the main source of gains there while avoiding all of the scalability issues so what's great about interactions is that you have representations that can um be fine grained they're at the level of the tokens so instead of one big Vector that's pooled as you said we have we have you know maybe 100 tiny vectors for each document and when I say tiny I really mean they're very small like 20 bytes for example um um for each vector and people have done even have done even smaller um so now that we have representations at the level of all of the words our document is now a matrix it's a set of vectors um and our query is also a set of vectors and we need a way to do the actual scoring so what Kar also introduce introduces is a scoring mechanism that surprisingly to my own surprise manages to preserve the Precision of going through the Transformer and doing the full interaction um and can scale um to massive data sets to millions in of documents in milliseconds that scoring mechanism at the high level is basically saying for every embedding in the quy just find the highest scoring um you know most similar embedding in the document and that's a score and then simply average them across all of the quity vectors so it's a it's a sum or average of Maximum similarity scores this looks like a really simple um function but the cool thing is because of the properties just mathematically of Maximum and sum we're able to um leverage you know the same techniques that allow single Vector models to scale things like similarity uh search indexes like dense dense Vector indexes um which can do search in sublinear uh complexity we can leverage them also to do uh Kar search at Large Scale so we can search things like 150 million um uh documents in 150 um you know 100 something uh milliseconds um um on a CPU without a GPU um with with Kar V2 um for example um so the the the interesting thing is quite surprisingly this manages to perform as well as cross encoders as the as the you know having a full attention um but because it can scale to millions of documents you're able to get much higher recall basically it's as if you were able to take your cross encoder and run it on all millions of documents that you have um um but in milliseconds and um which means that you don't need like a cheaper model that gives you the initial ranking that you are going to rerank um accordingly what's the what's the catch what's the cost for a while the catch was the index was very large cber V2 fixes that and compresses the vectors dramatically the real catch that remains and it's not going to go anywhere unless maybe you guys help us out is that the infrastructure um for this is um um is specialized you cannot simply use off-the-shelf um single Vector retrieval system to to do that it needs its own specialized search because you're working with matrices not with vectors um but as long as you're able to do that um and you know um as long as that you trust the community including us and others to keep releasing you know new kol bear and karik models that you know keep up with all the advances in the field which are tremendous um then I I believe this Paradigm is is is here to stay and is is is is here to transform uh retrieval so I think um and so okay so I'm sorry if I I'm trying to just understand everything yeah the um so we would need a custom coar model to produce that Matrix then so it's not something that you could kind of retrofit a lot of the off-the-shelf kind of zero shot embedding providers that we currently have it would be you know an entirely new category of embedding models to to produce the Matrix that we been it's the same architecture as far as the encoders concerned but it needs to be trained specifically to do late interaction like Kar V2 exists for example we're we're we've been working on Kar 2.5 for sometime um we have things in the pipeline um but I mean the community I was this morning at at the XTR poster from Deep Mind and um Google Deep Mind and they have built this um Kar variant that's really cool um this one is not open source but there's tremendous you know a very large number of Open Source ones um and more to come amazing Omar I am so happy with this podcast our first inperson podcast of recording and it's again so cool to meet you these ideas of cobber and dspi are both like so inspiring and I heard I heard also the the thing on Twitter that I I miss this joke that if you at if you at Omar he'll come out I'll answer your question so I thought I'd work that in somewhere in the podcast in but anyways Omar thank you so much for doing yeah thank you gor awesome ", "type": "Video", "name": "DSPy and ColBERT with Omar Khattab! - Weaviate Podcast #85", "path": "", "link": "https://www.youtube.com/watch?v=CDung1LnLbY", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}