{"text": "This video explains the details behind Active Retrieval Augmented Generation from Jiang et al! This is a super exciting innovation ... \nhey everyone thank you so much for watching weave yet on YouTube this video is going to dive into a super exciting new paper on large language model generation titled active retrieval augmented generation the technique is also known as flare flare is short for forward-looking active retrieval augmentation so quickly before diving into it this is one of the first paper summary videos on WE via YouTube so if you like this kind of content please let us know by giving it a thumbs up hitting the like button and subscribing so with that said let's dive into it so here's a super quick overview of the algorithm flare in two minutes just in case you're in a hurry and you don't care about the experimental details and just want the gist of the new algorithm so typically what we do with retrieval augmented generation is we take a query like generate a summary about Joe Biden we turn that into a vector and then we use that to Vector search for our nearest neighbors or maybe you do a bm25 keyword search with this phrase to get search results that help you write a summary about Joe Biden so now what you're going to be doing in the flare forward-looking active retrieval technique is iteratively doing this retrieval wall generation not just retrieve once and then generate so the way that that works particularly is you have generate a summary about Joe Biden uses that as the query to get the search results then you decode a potential first sentence from the generation Joe Biden born November 20th 1942 as the 46th president of the United States and you use that forward-looking generation as the retrieval query so then you delete this potential generation you just never actually generate it you just sample and see what it would have been and then you use that what it would have been as the search query to get information that might be a little more relevant so you know say it's going to write this is the 46th president of the United States this search query might result in search results that say you know that explicitly say you know what number President Joe Biden is similarly with the Next Generation Joe Biden attended a hallucination the University of Pennsylvania where he earned a law degree and other hallucination you use this next potential sentence as the search query and then that will retrieve search results that say you know University of Delaware Bachelor of Arts and history and political science so there are a couple other interest interesting details about just how particularly you're going to sample this query they ablate an implicit explicit query details that we'll get into in the paper summary video in addition to exploring how to form these queries they also explore when to do these queries particularly when the the log probability the language model put on each of these tokens oh sorry in the theoretical generation when that is lower than a given threshold that's when you trigger the forward-looking retrieval so they Benchmark this technique on four data sets that we're going to get into in the paper asqa hint is the same data set as sqa they compare their result of forward-looking with a baseline of using the previous window as the retrieval this General framework that we're used to of this single time retrieval and then no retrieval at all also in the details of the paper they have a really interesting comparison of this kind of technique with say search agents where search agents is where you're prompting it to say uh you know open square bracket search open uh uh parentheses sorry and then the search phrase you know that kind of search action thing so they have some really interesting ablations really interesting details of the paper but if you want just a quick two minute overview basically the idea is to use the potential next generated sentence as the query to have an active retrieval whilst generating something long like an entire summary of Joe Biden okay so following that two minute overview of the flare technique let's kind of walk through the whole basics of the story of this paper and the context that it lies in so we're starting off with this general idea of retrieval augmented generation this is something that at alleviate Vector database is super excited about this idea or basically what you do is you take the prompt to chat gbt you turn the prompt into a query then you hit one of these search engines and then you get the documents and then you take those documents and you put that alongside the prompt to a large language model such as chat gbt and then this gives you a customized response so you don't need to fine-tune the large language model to you know generate things particular to your data you can just take these documents put it in the input and then the large language model is remarkably good at kind of reasoning across this information that's in the context window so as a quick example that I like to use all the time you say you ask chatgbt what is reftube reftubec is a particular feature to alleviate and you know chat gbt says you know I don't know what ref to VEC is because it's not in the training data but when you do this when you take this same what is ref to vac and you use it as a query to then hit the hit a data set of the wivier blog post then you get this short description of raftavec and then by prompting it saying please answer this question based on the following context now I can tell you rough to Vector over reference the vector is a moduleeviate 1.16 so you see how this kind of retrieval augmentation creates a you know an answer where the language model can reason about your kind of particular information of course the reasoning I'm using that just like hand wave I don't mean like logical reasoning but anyway so this is the general idea is that you retrieve and then it helps inform the generation in this paper the authors are asking about what if we continually gather information throughout the generation process rather than this retrieve once and then generate kind of framework what what does it look like to be iteratively retrieving from our information as we're doing some kind of generation so I think this kind of class of thinking is most similar to kind of Lang chain I don't want to particularly say line chain the software library but this kind of a general you know academic concept of you generate something and then you take the output of what you generated as the input to the next large language model called that kind of idea of continual generation or you do some kind of retrieval I think we've kind of started to see this thing and this paper has a great comparison of flair with the self-ask prompting which I think is a really great uh analysis of these two worlds of whether you're going to kind of chain these language models together or if this is something that's cooked into the first sort of generate call and it's not really so much as like discrete prompts that that are applied in some kind of sequence but just something that is a mechanism in a single generation but we'll dive more into that the authors ask can we create a simple and generic retrieval augmented language model that actively decides when and what to retrieve throughout the generation process so this is why I really want to kind of hit this similarity between the self-asked prompting is we have seen this kind of thing before where you're you know prompted with some kind of tool like hey you have a search agent if you want to use it then open a square bracket search and then you know parentheses the search term so it's not like the first time we've seen this actively decide when and what to retrieve but I think the particular mechanism with this is pretty unique and as mentioned they have a you know they compare these two techniques side by side so then finally they draw with this kind of human reasoning thing I always like these kind of comparisons they say similar to how humans gradually gather information as we create content such as papers essays or books long form generation with language models require Gathering multiple pieces of knowledge throughout the generation process so I really like this because you know imagine that you have a prompt like you know write me three paragraphs about how kubernetes works if you know nothing if you just start with a coup with what is kubernetes as the query as you start generating you know you'll discover some pocket of knowledge that you need to do more research in I think as all of us have done this kind of like exploration of new ideas you find some some sub idea that you didn't originally know about and now you need to go down that rabbit hole and learn all about that thing so I love this kind of comparison to thinking about why we need active retrieval augmentation and why retrieval augmentation isn't just a retrieve once and then generate away kind of thing okay so now let's dive into the details of the algorithm so as presented in the original slide in the two-minute let's overview the basic idea is that we start with an input query such as generate a summary about Joe Biden we use a retrieval technique whether this be bm25 search which is what is used in the paper or vector search Hybrid search you know search with cross encoder ranking or you know however you search so you retrieve some information with this query and then you're going to use the you know the d sub queue the documents formatted with this prompt and the query to to do the Next Generation so just as a kind of a meta comment I think it's so interesting how we see these papers around large language models are starting to annotate the papers with prompt templates similar to how we used to see like pseudo code of algorithms so you see like how the prompt they're like at least five of these prompts in the paper and just a detail I thought was pretty interesting okay so it takes the results and then it generates a hypothetical next sentence and then you're going to use the the log probabilities of these tokens to decide should we retrieve again and then if so you retrieve again and then you discard the original context and you use the new retrieval to better inform the sequential generation in the active retrieval augmentation so the first interesting thing is about when to query so as I think people who've made it this far in the paper maybe you're familiar already with large language models and how particularly they generate things but you know a large language model at each step of generation if it's generating this sentence if all tokens of S Prime sub T have probs greater than equal to Theta at each step it's you know it's like if mask and then if all mask if all tokens mask or you know however it's tokenized it's just Word level is easy to explain quickly and what it does is it puts a probability on that word token so you know it's it's like a say it has a vocabulary size of 30 000 it's doing you know 30 000 label classification on classifying which of the tokens is most likely to come next so you know it might say like if with you know 90 all with like 100 or like 99.9 then maybe like tokens has like 40 so all of a sudden it's like you know uncertain there's High entropy in the labels that is predicting for this next Mass token so when you have like you know thirty percent thirty percent thirty percent then you multiply out the probabilities and then you say if that's greater than Theta then go ahead and keep it but if it's less than Theta that means that we have this High entropy next sentence and that means that the language model is uncertain of it it's like this idea that you can detect hallucinations by the log probabilities of the tokens it's a super interesting technique I don't know too much about all the literature and all the tests on how that's worked but in this paper that's what we're going to do in the paper the authors are going to be experimenting with the open AI large language models and I did not know that you can do this so just showing this quickly maybe this will help somebody else know that you can do this so they're using the log probabilities from these large language models so when you look at a token so it also goes token by token obviously rather than word by word I just like word by word it's easier to explain to someone quickly but so you know FL the token gets 99 here then AR the token gets 100 or like you know C is like 99.999 so confident that you just say 100 and then is only has 63.71 you know so on so so this is like a sense of these log probabilities really really really really really interesting that that openai is showing you this because I think originally I don't I think there was something around they weren't showing you this because you could use this for knowledge installation there's like a lot of things you could do with this but anyway so I try to get an example of something that's uncommon uh Unum unconfident not confident but interestingly I guess uh they have trained on ref to back so restavac used to be my example from opening the video of like something that openai doesn't know about without the training without the retrieval augmentation but and they've updated it still doesn't it's interesting because you can see how this still isn't as good of an answer as when we had um you know given it the context from the blog post but anyway so wow look at that I I'm just seeing this now is that you can highlight it all and it multiplies out the blog probability so this would be the log probability you know for a sentence that we're comparing with Theta so we you know highlight this and then we'd compare the log probability with that Theta kind of interesting but you can see like type here had uh 5.19 uh that's so interesting I get I guess you're also seeing kind of how their decoding algorithm works and how the uh so you know decoding also isn't generally like greedy decoding where you just take the maximum probability of The Next Step they do things like you know beam search and you know this kind of stuff for how they uh how they decode so it's not always just the most probable next token another interesting detail if you're getting into the weeds of exactly how to decode from large language models so anyways I think you get the high level idea that you're looking at the confidence by multiplying out these probabilities and that's how you're determining when to actively retrieve so the next ablation of the paper is quite interesting as well which is how to query so you know we've we've taken this next sentence Joe Biden attended the University of Pennsylvania where he earned a law degree this had you know High entropy and we're saying no we need to actively retrieve with this so then the question is do you do an implicit query it's an implicit query the the you know the naive way to do it would just be take this sentence and use that exactly as the query what they propose is masking out the uh the tokens particularly that have the high uh entropy so you know this phrase the University of Pennsylvania if we go back to the open AI playground we'd imagine that this phrase is highlighted in red and some of it with law degree so we'd you know we'd hope these hallucinations are highlighted in red I'm not exactly sure if that works completely for detecting all hallucinations but you know so the idea is you take the high uncertain phrases and you mask them out such that the query is Joe Biden attended mask where he earned mask or maybe just deleting it to the sentence becomes Joe Biden attended where he earned but that might lose a little bit of the um I don't know maybe like the grammar of it but anyway so so you use this as the query to then retrieve information about uh Delaware and it's a really clever idea because if you use the University of Pennsylvania is the query maybe you'll end up getting more information about the University of Pennsylvania say you're searching Wikipedia or something so this kind of idea of masking out the uncertain phrases is quite interesting then the other idea is do you have an explicit query where you kind of map it into the query space so you know like like into a natural question so ask a question to which the answer is the University of Pennsylvania and then the you know another large language model but then now I you might be saying oh my you know using another large language model to generate each of the queries is going to get expensive but you know you assume maybe you could have some task specific model for this that generates queries given phrases like this but you know that that kind of thing but so what university did Joe Biden attend what degree did Joe Biden earn you turn these into explicit questions to retrieve things and it's a pretty interesting idea you can also have this zero shot question generation you have the user input so far the generated output so far and then given the above passage ask a question to which the answer is the term entity phrase and then say you know the University of Pennsylvania so all of these kind of interesting ways of how you form the query that you're going to be sending to the retrieval engine and then so just this little detail this is about when to mask in the implicit query but what you see at the end is as we'll get into the evaluation results they don't actually seem to find a like a you know a difference between the implicit explicit query so maybe this is just something that's interesting to talk about you well I mean what university did Joe Biden attend compared to Joe Biden attended mask or your mask it looks like they perform similarly on the you know on how they measure that in this paper but I think we'll get more into the benchmarking but it's hard to it's hard to say if these benchmarks are going to generalize to you know the app You're Building you know you're building an app with leviate and uh you know an open Ai and stuff is this is this particular thing going to hold to your results I don't I'd say try it out and see how it works for your particular thing so I think that's a really good transition into how did they measure this what are the experiments that were run so let's start off with the Baseline approaches so the the strong bass line is to use the previous window sentence as flare query well I'd call it a strong bass line because it's sort of like an obvious ablation the next thing is probably the strong based on like the current state of the art sort of way of thinking about this but using the previous window sentence is the flare query I think the benefit of this is you don't have to do that kind of extra generation where you do the forward-looking step and then you query with the forward-looking step if you just use the previous previous sentence then you know if you think about overall how many large language model calls you're making that might reduce that kind of thing but so that's the Baseline then we're going to get into self-ask prompting how that's different from this technique and then they also have a a variant on self-ass prompting called flare indirect so previous window sentence uh you know pretty straightforward window meaning that you use a token window where you're counting the tokens for the you know the previous sentences that are used as a query whereas sentences you use the heuristic of splitting on you know periods exclamation marks question marks to you know say that this is a sentence and then send that as the query so this result is quite interesting they're showing that using the next sentence compared to the previous sentence gives a pretty high uh you know Improvement on exact match F1 Precision recall of a multi-hop question answering data set this is where you multi-have question answering we'll get into it more quickly but I think maybe you might want to have a quick explanation as you're looking at this that's where you have to uh combine like two sources of information so if it's did Aristotle use a laptop you have to first know when did Aristotle live when were laptops invented and then PSAT together into a short answer and the short answer is short answers are great because then these exact match F1 metrics are pretty good whereas you know asqa I I think it's either long form question answering or open domain summarization where you generate a really long answer so these metrics are a little they're a little more difficult to really like capture the quality of the generated response compared to the gold annotation but anyways so what they find that's interesting is Boom huge result by using the Next Generation as the query which is you know that's kind of the whole idea of this so definitely before reading this paper and I still think this is a very interesting technique self-ask prompting was definitely probably the you know the active retrieval augmentation that's how I I would think of it personally so the idea behind self-ass prompting is when you have a question like who lived longer Theodore Hager or Harry Von Watkins you prompt it with are follow-up questions needed here and then if the language model outputs yes you then have follow-up and then you have the language model generate the follow-up question how old was Theodore haker when he died intermediate answer and then so it will you know retrieve with this and then it will I always imagine that this kind of could work asynchronously as well where you could have the follow-up question and then maybe do it but say that you do this sequentially where then intermediate answer Theodore haker was 65 years old when he died and then you have the next token so it could either be you know so or follow so you look at these two follow-up or so and so if it's follow up then you know another search query then another intermediate answer and then so the final answer is Harry Von Walken so this kind of idea of uh self-ass question decomposition has been quite interesting where you ask the language model hey you know is this question too confusing to just like retrieve once and then generate or do you want to break this into sub questions and then have each of those questions answered so this is a pretty strong Baseline it's really interesting that they're kind of putting these two algorithms side by side and then you know seeing what happens so sort of similarly to prompting the language model to our follow-up questions needed here we also have this idea of search actions and when we prompt the agents to use these tools this is this is the whole idea behind tool use you know like the chat gbt Marketplace or say like Lang chain llama Index this idea or the react paper is another really great one where you tell it hey you have like wolf from alpha to trigger it say you know open square bracket you know calculator and then this expression and then it will execute it and you'll see the output so so the idea of how you would do this with search is you you know open square bracket search you know Joe Biden University and then you get the results Joe Biden University and then you know so like a keyword search that kind of thing so so this is another really interesting bass on there comparing it with interestingly they're going to couple these skills so so this is this is the idea of skills tools I kind of personally think of skills and tools as similar things well skills is like you would have like a collection of prompts sort of that like are like describing how to prompt a summarization or how to prompt like a grammatical editing of something this kind of idea whereas tools you think a little differently is like you connect to some kind of external thing so skill one would be like an external tool and skill two would be like an internal thing so skill one and instruction to guide language models to generate search queries and then you have search related exemplars like examples so few shot learning has been you know prior to the uh instruct gbt the you know the rlhf and all that the whole idea of language models was that you would have to give it like four or five examples of the task and then it could fuse shot in context learn the task from those examples it was absolutely remarkable that it was able to do this I don't mean to downsell the ability of this but so a lot of this tool use stuff people are still thinking about prompting it to uh you know prompting it how to use the tool so for example if I'm you know if I'm trying to prompt my large language model to write weva queries or write weeviate schema say I would give it like a couple examples of what we gave schemas look like and then it could attend to that and then learn the skill of how to write a schema or how to write a weba query so what they're doing is they give it some examples of when it used a search term to get a better result similarly it gives it examples of um you know multi-hop question answering so multi-hop question answering this kind of what the task related examples are are like this these kind of examples of decomposition which is needed for multi-hop multi-hop is the whole idea of um you have to like compose facts you can't just have one answer you you know you decompose so did erisa or who live longer at the example on the screen is like you know okay how long did this guy live how long did this guy live and that's how you would reason about who live longer so so this is the prompt a little bit of an interesting detail is that in their algorithm they introduce a bit of extra control by explicitly boosting the probability of the open uh bracket when they want it to search and then or you know down weighting it when they when it's just completed a search and you don't want it to search again you're like you know generate you just searched so to dive a little further into what this looks like uh prompting large language models to have the skill of using a search API as well as the skill of thinking step by step so the authors they showed the like how they prompted with this for uh the so they have four data sets that explore you know two Wiki multi-hop QA strategy QA and asqa so uh so let's look at this quickly so you know what what you do is uh you you have this kind of this again is how you search you have this it tells you to do this action and then as you're decoding from the language model if you see it do this you know open square bracket and then you know you're looking for this kind of thing if you see like these two in sequence that's usually how you would decode a tool use sort of because you could have other kinds of tools like calculator or python code executor or you know any kind of external API all these ideas so so you know you give it some examples of but what are the risks during production of Nano materials and so answer with search search nanomaterial production risk so so you give it a few examples of how to use search for some kind of you know information heavy query like metformin is the first line drug for what so then you have these examples of thinking step by step so this is this Chain of Thought prompting that has been super effective is uh showing the language models how to kind of break down a question so one of the director of the film of the film hypocrite film die so the film hypocrite was directed by Miguel Mora Ada Miguel moreta died on 19 June so you know you're showing it how to take apart these questions and particularly when you're doing this task of multi-hop question answering this is a really useful skill to have so so then you kind of combine it by just answer with step by step in search with the new question and then you know you give it a little more prom now combine the aforementioned two skills first write out the reasoning steps then draw the conclusion where the reasoning steps should also utilize the search API you know search whenever possible so so it's kind of interesting how you're then combining these skills in the end you notice that the few shot props data these don't have search in them so interesting detail to this okay so so then you know you have all these examples so another interesting thing is whether you want to retrieve you shot examples is an interesting idea I haven't really dived it into it too much but just something to know is you might want to retrieve few shot examples that are similar in the semantic space to the current input not sure how well that works so here's another example of the strategy QA so generate yes or no to the font question do hamsters provide food for any animals and then you show it how to do this kind of step-by-step reasoning and then the final answer is no so so strategy QA is maybe the best Benchmark we have in this paper because it is going to be yes no at the end of it which the reason I say that's the best Benchmark is because then you can easily do the metrics I think like you know exact match of question answering is a bit tough because you could you know phrase the correct answer in multiple ways for a lot of questions that's kind of the whole idea behind that asqa data set they explore is that uh if you say like uh where is the where did the Philadelphia Eagles play you could you could have like different granularities of how you answer that like you could name the stadium that they play in you could name like the city they play in that kind of thing so so yeah so I like the yes note anyways so the point of this is here are examples of what these problems look like for how you prompt it to use skills and how to kind of do this few shot reasoning so what I would say that people taking away from this if you're you know building out so say weeviate and large language models is that for your task it probably makes sense and you can probably boost the performance a little bit by having these kind of few shot examples of a skill but then there's the trade-off of uh you know of the input window length so there's we'll talk about this at the end there's a lot of hype now about the Mosaic 65 000 input windows or the anthropic 100 000 input windows and so what that might let you do is really pack the inputs with all sorts of information a ton of these examples of how to use skills as well as retrieval augmentation it is so exciting this idea of longer input windows but as of right now most of the most of the cases of large language models and all those models they're probably going to be expensive or not as strong as the models that are trained densely with the 4096 token like so right now you should be mindful of how long these are going to end up being in your prompts okay so one of the most interesting details of the paper are what data sets did they use to say that this retrieval augmented generation technique is you know better than another so we have four long form knowledge intensive generation tasks slash you know data sets for benchmarking these kind of systems first up we have two Wiki multi-hop QA this you know multi-hop question answering multi-hub question answering Common Sense reasoning long form question answering your open domain summarization these are broadly described as tasks in deep learning literature meaning that they follow a similar kind of input output framework such that you could you know you have multiple data sets kind of domains that follow this sort of annotation interface so multiphap QA means that you're combining you know at least two facts to form the final answer you know again who lived longer this guy or that guy you need to combine the two facts of when they each lived strategy QA Common Sense reasoning quite an interesting Benchmark for getting like sort of sort of say like the general World understanding of these language models that's where you you know you ask a question like will an apple float in water or like what happens if I toss an apple up in the air so that kind of like Common Sense physics like density and gravity so you know it's things like this like you know that kind of General sanity the language model so to say asqa is long form question answering and then Wiki ASP is open domain summarization so these two data sets are quite difficult to measure the final result of so you know if I tell you to write me an eli5 summary of I don't know the density thing like how does density work what whatever the question is like what's what's the summary of this paper that we're talking about it's going to be hard to exactly compare the generated summary with say like the human annotated or self-supervised bootstrapped gold summary so you know say you're doing the long form question answering of what is active retrieval augmented generation about that's the question and then you need to write like a paragraph and say you compare that with the abstract of this paper as the ground truth what you would do is you do things like engram overlap you know how many uh tokens between the generated answer to that question of what is this paper about and then the abstract how many of those tokens do they share and then that's kind of like what exact match that's what F1 Precision recall they all capture that kind of thing but we're gonna get into a couple interesting metrics they use as well like uh disam ambiguated uh are disambiguated uh question answering as well as this uni eval for you know maybe putting them into Vector spaces and using semantic vectors to compare generated responses with ground truth a lot of interesting ideas there so starting off with two Wiki multi-hop QA the way that these multi-hop question answering data sets are constructed is typically use the notion of relational tuples so similar to you know like uh graph database technology where you have entity relation object you're going to take out those kind of triples and then you try to you know chain them together with some maybe like a transitive property or something like that so so you're keeping these uh triples so this is like the transitive thing where you have you know a relation b b relation C so then a relation C stuff like that so you extract these relational tuples and then you chain them together and kind of translate them into natural language and then that's kind of like that's like how they create these data sets to Benchmark how well systems can do this like logical chaining of facts but but then not in the you know that most of these system like a baseline of how you would do this would be translate the paragraphs from the retrieved evidence into all these tuples and then you know maybe have like some intermediate step of chaining the tuples but generally we're trying to see if the language model can just do it without That explicit kind of relational extraction so this is kind of a look at what kind of things the multi-hop reasoning uh Compares so you know comparison question you have these relations like you know you would parse this out into a relation like lived lived is the relation for this song and then you have comparison who lived longer when they share that kind of relation you might need some kind of diction like dictionary of what relations you can stack like this so compositional question is inferring the bridge entity to find the answer so why did the founder of versus die so you need to bridge this concept of versus was the founder of uh or Gianni Versace was the founder of versus and then he died like this so you need to bridge that relation anyway so this is this is the idea behind these multi-help question answering data sets so it looks like the strategy QA data set which is labeled as common sense reasoning has a similar kind of uh strategy for construction as most of these multi-hop question answering data sets with a key difference being that all the answers are collapsed to yes no uh it seems like there's another interesting detail to this which is about implicit facts and then or this kind of like multi-step and then implicit properties of the strategy questions so as an example you would say are sharks faster than crabs and for this one you explicitly need the reasoning steps of how fast their sharks how fast are crabs and then is one thousand two whereas if you ask it something like are more watermelons grown in Texas than in Antarctica you might not even need to break that up into the steps because you you can just say like you know the Antarctica doesn't support growing watermelons so that's kind of the idea I didn't dive too much into the details of this but maybe if you're interested in pausing this or Consulting the paper itself to find out exactly how this kind of data set is constructed but I interpreted it to be similar to multi-hop question answering with the yes no and then yeah maybe something else for how particularly they say that these this is common sense like I'm curious like you know would Hayes and Osiris hypothetically compete for real estate in the Underworld well okay so so yeah that's common sense because real estate in the Underworld I don't know I mean anyway so so the data set this is what it's trying to capture how well the language models do and you know however you set up the system to answer these kind of questions so the next data set is measuring a long form question answering is asqa which stands for answer summaries for questions that are ambiguous so basically the idea is if you ask a question like who is the ruler of France until August 2nd 1830th and there were two rulers of France during this time so so you could kind of have either like the summary could say either thing and kind of be correct so so it's about kind of disambiguation it's a you know pretty interesting annotation data set so uh yeah so that's as much as I really looked into this doing this kind of thing of answering these questions the the general thing I'd say to take away is that it's you have an annotation of long-form answers I guess it's all it's further compounded by this this ambiguation part of the data set so you know definitely interesting if you're kind of doing a tour of all these data sets so then rounding out our tour of the data sets used in this paper we have Wiki ASP so Wiki ASP is aspect based summarization so the key is you know if you say write me a summary about Barack Obama that's pretty vague compared to where if you break it into write me a summary about Barack Obama's early life and career Barack Obama's presidency or Barack Obama's Legacy so you have this kind of notion of a particular aspect based summary so you you know you kind of reduce the search space of the summarization and what it could entail and I think that's a pretty interesting detail for how you construct these you know long summarization data sets so all in all here is a summary of the evaluation data set so even when they have more than 500 examples they will subset the data set to 500 which in the paper they describe is about the cost of running these experiments you know obviously these large language model inferences aren't cheap and you want to ablate the um like the impact of previous next the particular Alpha parameter for the um you know Alpha parameter for when to use the next sentence as retrieval as well as a beta parameter for the masking on the implicit query so anyways it's like as you want to Blade all these details you can't be doing it with like 200 000 questions so you know we have the metrics that are used and then the Corpus using Wikipedia for three the open web and the Bing search engine for the wiki thing uh and then using you know bm25 or like some kind of search engine configuration so say it's like a hybrid search say it's like a you know bm25 Vector search Hybrid search but then also you have like symbolic features that boost it like you know what time the article was these are all the kind of things that would be like abstracted into a search engine like Bing or if we imagine kind of putting weeviate into into the retriever here we V8 would be more like that where you you could you can bm25 with it you can also like combine all sorts of things for ranking search results uh so then the top case how many of the results are put into the prompt so two three three five the number of examples that are used when you're giving it examples of how to think step by step for a given task and then whether you're doing that or not so you only do the thinking step-by-step examples for the multi-hop question answering data set okay evaluation metrics so exact match is you know what is the atomic number of oxygen eight is the answer and then does it exactly match that so this is the thing where yeah so you know that kind of thing take it how you will it's it's a pretty good metric for a question answering when it's a short answer and it's something like eight that's a pretty good example but if it's something that could be more abstract I think it's a it's not the it's a metric that could use a little work it works great for when you have yes no exact match so you know with strategy QA you say so the final answer is yes no and then you can exact match it you know and then it's just like you know binary classification so token level F1 named entity F1 so F1 is uh like a Precision recall waiting so uh you know Precision recall these are like um these are like false positive metrics generally where uh Precision is saying like how many of the correct entities were were generated and then recall is like how many of our Precision is how many of of the generated named entities are correct and then recall is how many of the correct named entities were generated and then an F1 score is a is like Precision times recall over Precision plus recall some I think it's like a geometric mean of those two metrics so token level F1 is where you do that for each of the tokens in the um in the gold answer and the name entity is where you only do that for the named entities in the gold score so the gold summary is the human annotated they or just like the source of Truth where if it's like you know Barack Obama was the 45th president of the United States and then so you you dis named entity F1 you disregard was the and you just use Barack Obama or like 45th those kind of things so then uh Rouge is this like you know Rouge is like the blue score Bleu where it's like it's like a way of doing engram overlap between long passages of text so so yeah so that's that's basically what it is uh so disambiguate F1 uh so what you do with this is you generate a summary so you generate the the long summary or the long answer to the question and then you apply an extractive question answering model like Roberta on that and you see if it can get out the uh the the like the name entities I think from the generated summary and so you have that kind of like intermediate of you evaluate the generated summary by applying a uh like an extractive question answering model on it and seeing what it is able to get out of that uh and then we have this uni eval thing which I think is interesting enough to get its own slide so for example when Google was evaluating their Minerva chatbot originally they had all these different dimensions of how they evaluated the chat pod chatbot like how entertaining is it how factually correct is it so you have these like dimensions of evaluation similarly we had this paper called checklist which was like you know it was more of like a robustness test like does it do well when you add negations when you swap named entities so it's like a checklist of General language understanding and that's kind of I'd say uni eval to me looks like checklist kind of where so you take the generated summary and you have these different dimensions that you've trained task specific evaluation models they describe more about how this is unsupervised so you know I didn't get into details of that but I highly recommend checking that out so you say basically you know how coherent is this summary you know if you're evaluating long form or open domain summarization or like is this summary relevant to the reference so you know looking at these different dimensions of a summary outputting the score and so yeah a pretty interesting way of looking at like multiple linguistic properties of this generated summary so pretty interesting evaluation technique for these long you know when you generate like a paragraph of text how are you going to evaluate that so follow in the description of the algorithm what they're going to be ablating the data sets and then the metrics let's get into the results that they find so starting off with this is kind of the opening result where you compare the flare technique with previous window retrieval shown in yellow and then single retrieval so retrieve once and then generate so we get a huge benefit on multi-hop question answering of the 51 with the exact match so you know exact match of that final question of that final answer 51 compared to 39 which is a pretty big Improvement uh the strategy QA doesn't seem to be as much of an improvement than the asqa you know pretty minor there as well but when you have the hint so the hint is where you give it the disambiguation like the summary should be about this particular who uh like King of France in that example from earlier and then you know performing about the same on the aspect based summarization but probably the most interesting test in my view is the multi-hop question answering because it's probably the most well understood uh you know data set and metrics for this kind kind of thing so this ability of the active retrieval to have this Improvement when you're kind of trying to like answer these complex questions that require retrieving from multiple sources or like asking new questions while you're generating that's quite an interesting Improvement probably the real question here though is how well does flare compare with self-ask prompting where you explicitly prompt it our follow-up questions needed here you know yes and then follow-up questions so we do get a better result with flair it doesn't seem to be a super you know way better kind of result but it's definitely interesting to think about what's more efficient between you know looking at the log probabilities of the next token doing that returning the log probabilities it's very interesting as we saw earlier that now open AI is letting you see that and you know and then so it's going to be super quick to just multiply those together and then say if it's less than the threshold you know compared to the our follow-up questions needed which is more of a black box algorithm so it's pretty interesting when you use the log probabilities to do this active retrieval or whether you just kind of black box it and prompt it to ask it if it wants follow-up retrieval so you know there are definitely a lot of interesting questions this is this is probably the debate that I think is the most interesting with this of the future of this kind of active retrieval augmentation so some additional details they explore they also look you know doing this across the data set so this is again this is again showing you more of the details behind how these plots are produced so you see the different metrics and these different data sets no retrieval single retrieval or the previous window so flare performing the best but you know not too much on these other data sets which is pretty interesting but but again these data sets they're you know asqa and Wiki ASP as we saw these are like these long generation benchmarks and so I don't really think we have the greatest standardization of data sets and metrics for that kind of thing yet uh so then what they're ablating is when to retrieve so this this would be an interesting hyper parameter if you imagine as we build this into alleviate you'd have some kind of hyper parameter called like Theta like similar to the um the alpha in weaves hybrid search where the Theta is saying when like what threshold of uncertainty to do this extra retrieval in so what they find is between 40 to 60 percent uh uncertainty works the best and more than that they find to be actually distracting because you're now like retrieving every single time you generate something and they find that that doesn't work as well as some optimal intermediate value of uh certainty another really interesting detail they explore is that idea of masking the implicit query so when they're going to be doing that implicit query idea which if I can find the slide this is this idea of you you know you take the query is going to be just the next sentence and then you're going to be masking out the uncertain words and then you have a hyper parameter of with what log of probability do you uh mask out the word so they find a bit of a variance with this so you know you know not mattering too much but showing that this 40 thing I guess is the best with masking out those tokens that you send that over so after reading this paper here are some of the reflections that I had on this General concept of active retrieval augmented generation and particularly this algorithm so particular to this algorithm I'm very curious about sort of the generalization of alphago mu zero this kind of like tree search look ahead with text generation so one idea would be you know as you're decoding the next sentence in language models you could have several different potential next sentences that have a pretty high total probability so I think it'd be very interesting to you know Branch out this retrieval augmented generation and have it go along these separate branches of the tree to do these generations and then you know at the end so you have the you know the forward look ahead kind of the Monte Carlo tree search exactly how it is in alphago and the game playing AI is where you you know you're looking ahead to generate and then you kind of see how that works at the end so I think it's very interesting to see that kind of look ahead search in text generation so the second thing I think is really interesting is just large language model evaluation I'm very very interested in this um in this software framework called chat Arena there's another one from UC Berkeley that I don't remember off the top of my head but basically the idea is that language models are given like an ELO rating similar to like like video games where they chat with each other and then you have like a third language model to decide like okay he or that language model was smarter in this uh debate or conversation so it's I think it's a very interesting way to evaluate language models it's you know more hand wavy than the benchmarks where you annotate you know ground truth and compare it but I think it's a little more realistic for how people are you know using these kind of systems and it'll be very interesting to see how that kind of evaluation work evolves so as kind of my analysis of the paper was I think when you have short answer questions and you can do exact match like the multi-hop question answering I think you get a really great Benchmark for that kind of thing and then what these benchmarks can do is they capture the long tail well you can see like the failure modes when you create like a hundred thousand examples and then run it and then you can see like okay it's bad at this particular distribution of questions what's the similarity and then you can maybe add data to fix it that way but I think more so this broad like chat with each other who wins the chat or like I don't know I think the space some evaluating language models also evolving is in a very interesting way the third thing I think is extremely interesting is just this continued development of large language models so firstly we're seeing the latest news is anthropic has a 100 000 inputs and then you know Mosaic ml they have their MPT which is 65 000. so you know I think that we're going to be able to retrieve so much information to put in the context and you know I you could say that I'm biased by working at a vector database and saying this but I think it's unlikely that the best way to retrieve is just to put like some massive document of the same context into the window I think there's still Nuance to how do we use retrieval so you know is the best way to say it's not the greatest specific question but when you're asking that question of uh where did Joe Biden what what degree did Joe Biden do should you just look at the entire Wikipedia article or maybe yeah it's not a good example but like every little passage from everything on the internet about Joe Biden's experience in college if that makes sense like if you're looking at the thing of like what is reftubec how do I use it for my app rather than just taking the one blog post from we've ate on rustavec if you look at like these Snippets like code retrieving from every code base just like you know retrieval from everything that uses ref I still think that's a better way to like pack the problem with information and then I also think this 100K and inputs it'll be very interesting how that kind of skill description so we saw these like few shot examples of how to use a skill like you know compositional reasoning so you'll be able to pack in like a ton of examples of how to use skills and it'll be interesting to see if that kind of like search action gets better with giving it like I don't know 20 examples of how to search but and then I also think this Laura thing is people are fine-tuning their models and their domains that's just another really interesting thing to be looking at how will that impact retrieve augment to generation so then just the final thing of how do we use retrieval I'm pretty curious about like yeah just generally like um you know when it's retrieving so that this is the first this is one of the first things we're seeing on like this like active retrieval you don't just retrieve one so yeah it's very interesting like as we have this kind of like Auto gbt style you come up with an action plan when in the action plan are you going to do more retrievals and I think it's this is very interesting that this kind of like we like this idea of generative feedback loops where you save intermediate Generations as well so like when would you then want to retrieve some kind of intermediate generation as you're like in the middle of executing some wildly complex task so anyway so so here are some takeaways I hope you find this interesting thank you so much for watching this explanation of active retrieval augmented generation the flare technique uh please check out wevia on weave.io or GitHub weviate or Twitter we've yet AO or please leave a like on this video if you like content like this it really helps us know that uh you know creating paper summary videos is something that people like and want to see more of and we're always on the hunt for these kind of new algorithms and experiments that interface with you know Vector search and retrieval winter generation so please leave a like if you like this video please subscribe to the channel for more videos like this and thank you so much for watching ", "type": "Video", "name": "active_retrieval_augmented_generation_flare_explained", "path": "", "link": "https://www.youtube.com/watch?v=IVYwLLDABzI", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}