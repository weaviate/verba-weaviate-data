{"text": "Hey everyone! Thank you so much for watching the 38th episode of the Weaviate podcast! This episode features Leo Boytsov, ... \nhey everyone thanks so much for watching the wevia podcast I'm super excited to welcome Leo boy stuff Leo is currently a senior research scientist at AWS Labs uh prior to this he's written a dissertation at Carnegie Mellon University titled efficient and accurate non-metric k n surge with applications of text matching I started studying Leo's work after seeing some of his tweets that brought a ton of knowledge to the discussion around deep learning for search technology and finally Leo's co-authored an incredible new work in pairs light that I'm just super super excited to dive into so firstly Leo thanks so much for joining the podcast oh thank you very much for inviting me um it's great to be here awesome so I think maybe setting the stage I really want to start with this in pairs light work I think this is so interesting and maybe this background of as we think about the intersection of large language models and search technology I think most of us are sort of going in this direction of you sort of retrieve and read you use the search results to uh to put into the context of the language multi Granite generation but this other idea of using the language model to generate training data for the you know for the search models I think is so interesting so could you maybe start by describing your interest in this um yeah um if you don't mind a little bit of um I would like to start with a little bit like historic uh background uh with uh retrieval maybe I I should say a couple of words about retrieval in general and also about um the difficulty of applying uh machine learning to retrieval and that has been like historically so um the case of it what we usually what what we did before all the machine learning we had we indexed uh text using uh inverted files which basically is your you know book uh book catalog for every keyword you you will get like Pages where this this word occurs and uh the the biggest Advance was the basically the bm25 function that would um tell you uh the score of of a page uh given how many times the the certain keywords appear in in a page and that was a step big step forward compared to the previous scoring function compared to Boolean search where you would have to specify all the words that need to appear and that would experts would do so and that uh being qualify function was really a big standard for quite a while and it was difficult to beat even with machine learning techniques because what happened next like people figured out that bm25 is a good um good signal but there can be other ones and then they um started to use machine learning uh approaches such as boosted regression tees to combine those and now the so-called learning to rank um and that um in in somewhat regretfully in many cases it actually that didn't help much and all like the uh whereas NLP was advancing at you know at you know rather rapid Pace in in information retrieval it was for a long time it was not possible to be Beyond 25 or like slight um you know slightly boosted Beyond 25. and and one reason and and there is actually a real reason for this in my opinion is that in NLP uh like say consider something like a classification task in NLP you look at the page and there are like certain attributes of that page that indicative for example of the sentiment right and they're only typically or often they are only attributes that are specific to that page but when you're doing retrieval you need to look at uh at features that um that are query document features the pairwise features and those features need to be sufficiently generic so the it's it's often just not enough to look at oh oh this is like a trigram or bigram that that appears in the document or in the query it's often not enough unlike NLP so in in general there's like query document um similarity that has been historically more difficult for machine learning approaches and another thing is I I guess that search is also much less precise and much much noisier so it's harder to come up with this you know there is bigger long tail so it's harder to uh come up with with some some features that work for this in in general and again that this is really uh different from a lot of NLP tasks uh say like uh question answering factoid question answering the rock use uh even before neural network uh these systems were able to achieve a human level performance like for example with ABM Watson system that did human Champion yeah um I hope that's not not a very long introduction but historically uh it was difficult uh to beat simple bass lines and when uh deep learning started to you know um to become very very popular in um information table it's fair to say that until we got bird we didn't get any really good Improvement at least not in all the domains um over Beyond 25 and only with Bert which is relatively recent uh development we were able to beat um bm25 like decisively yeah super interesting I think with my path in deep learning I also remember like when I started deep learning the natural language processing was mostly like uh one-dimensional convolutional models but like before the Transformers the Transformers just like busted open like the Deep learning for natural language processing so so could we continue on the cross encoders and the learning to rank um just kind of maybe also just the beginning of it being that the cross encoders are high capacity classifiers that take his input the query in the document and then output a score and so we use that score to re-rank it and you know the vector search will give us like a thousand or you know however many results to begin with and then we'll re-rank that to get a higher quality list uh so can you sort of describe like cross encoders and then I know learning to rank and like the um usually you have like clicks to like user data like XG boost models so it's like a little different right to thinking between cross encoders and like most of the XG boost style learning to rank stuff that's been developed uh yeah absolutely uh so first of all um so when we say learning to rank um although learning to rank is a very generic uh very generic uh term but oftentimes when people say learning to rank they mean uh specifically that old style learning to rank when you have features it can be pairwise features like uh the bm25 scores it can also be um query specific or document specific features such as like query type or like document quality score but these are features so in the end you get a long Vector of features and then you use something like boosted regression trees or it can be actually as simple as a linear function but that's crucially on like on top of features that are pre-computed somehow engineered and with the um with the invention of this strong neural network models such as Transformer base cross encoders and buy encoders we were able to replace most of those at least um most of those features with just this uh similarity score that comes from the neural model and it's also learning to rank of course but it's um somewhat different from the the classic learning to rank so the difference is um you you say that bm25 score maybe also like bm25f score as well as BM 25 score for the multiple properties of some kind of object as well as the vector distance score and use those additional features in the re-ranking model compared to maybe these like content only neural models that just is like query document more fine-grained kind of matching is that like kind of an accurate description of the sort of Distinction of it uh yeah I think it's a good one and it's still possible to plug in Bird score into the bigger model to but when our impression that they say for for the the say query document similarity only people would use more sophisticated feature-based model to achieve birth-like performance but now you can replace uh those with bird uh and in fact in terms of the query document similarity what I also used in the past and that's relatively unknown um similarity function is the so-called um model one uh and it is semantic in terms of in the sense that it can assign scores between query document pairs that are not exact match and in some certain cases in particular for question answering uh for uh if you retrieve document for a question answering system it gives really big boost on top of Boom 25 which actually a few people know but this is the case and um it can improve BMW 24 upon 25 but you would still need to do some fusion like simple learning to rank uh and you can replace it now using birth and typically get much better results so I guess kind of the ending on replacing it with bird is sort of where I where I'm hoping this is headed is that these cross encoders can just sort of again like the query document is input and kind of overcome needing to do all this feature Engineering in the learning to rank that maybe just the cross encoders can re-rank by themselves just based on the based on a more fine-grained query document representation and maybe we could um maybe we could come back to the in Paris light and how you how you train cross encoders for custom domains maybe talk about the zero shot versus the in domain cross encoder in sort of that sort of Direction so uh yeah uh um sure so the uh as I said it can uh the the the cross encoder can largely replace uh some pre-neural Machinery that was using the combination of features and for the cross encoder across encoder and there are two types of neural models that we're using now but they are both Transformer based but basically cross encoder is a little bit more accurate and it um you concatenate the queries and documents and pass it through their multi-layer a relatively large Transformer model which basically produces your score using a projection layer uh uh additional projection layer and you train this model so these models are pretty accurate um there is also a buying color model uh where you encode queries and documents separately and produce uh embeddings which you can compare using cosine similarity those models are somewhat less accurate uh but they permit a faster retrieval because you can pick Computing beddings during the next time so these are two types of models and in terms of the um so those are great but there is one problem so typically uh well that's the perennial probably we need training data for these models these models are relatively especially cross encoders they're relatively data efficient but not like it's not like you can train them on five examples we have a paper where we uh evaluated the few short properties for a number of Collections and yeah it's more like um hundreds uh well if these are sparsely charged queries you need a few thousand maybe hundreds with um densely judged queries with a lot of relevance judgments per query then um you can do few of them it's still a lot of data right and another problem is that um they are trained for a specific type of document you called domain for example question answering over Wikipedia and you try to transfer it to another domain say legal documents and oftentimes it just doesn't work so and we it's not like a big discovery that that was expected we uh we showed that that was the case uh in in fact concurrently with that uh uh now famous beer paper uh then they did it also for they um they did of course much more because they showed that also for buying coders and what they showed that in fact cross encoders transfer beta then uh then then buying quarters um and I perhaps should clarify what we mean by transferring better Wars so in the main mismatch and big problem with the main mismatch is that it's not just performance goes down it goes down so much that this wonderful neural models working so well in the main they become worse than bm25 out of the main which is kind of ridiculous right it go it throws us back uh 15 years ago so yeah I think that in domain out of domain there's definitely a lot to that that we can unpack um but I guess kind of like this idea of having an in-domain model for your particular application like um I kind of wonder about the zero shot and whether you need that like I think it's a great thing to get started but once you have some data set and you have some queries for whether you're building like a finance search app or a Twitter search app or like I'm working on the webiate podcast search app so I'll search through this transcript right so like I I'm thinking like um you know having I could use something like in pairs to generate my data set and then train my cross encoder on that data set and then I can you know re-rank within domain and it kind of makes me wonder and it feels accessible to me because you know like these um cross encoders are like text classifiers maybe you can shed some more insight onto the difficulties of training cross encoders but to me it seems like a you know not the most complicated deep learning optimization task to train a text classifier uh oh yeah uh training cross encoders um I I love to train cross encoders and I hate to train buying coders I I did the uh the cross encoders the um so the cross encoders versus buy encoder you probably should have uh said talked more a little bit about the advantages and disadvantages of both types of models so for cross encoders they are really easy to train but they are slow unless you're using them in their ranking mode the good news is that in many domains bm35 already generates a list of candidates that is pretty good and these models can be some of these models can be pretty cheap to run with buying coders uh they're difficult to train they converge really poorly um and they uh but they get the advantage is that you can potentially uh you know make ritual faster because you don't maybe need that re-ranking step or make it less necessary uh so yeah in terms of the but both models require a lot of data to train um and what was ex it was difficult before this like this data typically needs to be manually created and once again another um another difference between the NLP domain and uh uh information table domain so if you tell somebody like pick a random NLP person and tell them that you have difficulty annotating 1000 queries then I was like oh like what's what's the big deal with a thousand queries one thousand examples basically that's kind of the light bulb that goes on in the mind of uh your typical NLP researcher but no no no you have to annotate the query what does it entail you say for each query you run some candidate generator or use a bunch of retrieval systems that produce results the modern area they need to be diverse and then find some a bunch of documents and ideally you get like hundreds of those some of those are going to be relevant some of those gonna be non-relevant and then an assessor or maybe a couple of assessors those are people who are judging um would go and read these documents at least briefly and for the 100 document so this is a normal salutation effort so I it's even if it's like really sparsely judged it's still like one thousand you need to read a few thousand documents to not take one thousand quiz it's much more expensive so yeah the data is a huge problem and zero shot doesn't always work and moreover what's interesting when the zero shot is working well and that that result comes from um from the great work of Rodrigo Nigeria uh and some of his quarters that some of the models they do transfer really well but they typically need to be large and not just large huge so something like 3 billion or 10 billion parameter models we are not going to put those in production anytime soon maybe not even within a couple of decades so that that's that's a big big issue um so ideally we wanna train models that are small so how do we get data and uh except entertaining it manually and then although there there was some work on self-supervised learning in the IR domain but again things I would remember things are more difficult than they are the main it was much less uh for example the contributor paper there was much and before that we had embeddings which are typically much worse than being 25 like averaged in bearings um they are worse and unless that's a very specific some domains in some domains they do these things do work well but say not for Ms Marco not for legal ritual um and yeah so those Excel supervised approach training approaches they um don't work well and there was recently there were a couple of exciting papers one coming uh from rodigo and his group called in pars and another was like slightly preceding work UPR and shortly after that those propagated paper as a follow-up to eat parse and these over three three papers that showed Oh you can use a large language model to either generate uh training data from successfully or you can use um llm directly as a pre-trained llm as a re-ranker and that was the UPR paper that um we also cite so so and obviously in person propagator they are much more um practical in the sense that they instead of re-ranking using a large model they are basically using prompting prompt engineering to distill the information from the large language model to smaller ranker or retail model and that makes things much more practical yeah and I think there's lots of there's definitely a lot to unpack I think the the distilling part I'm very interested in that and kind of different from the knowledge installation where you use the large model to label the data and then you learn those labels the synthetic data generation thing is a different case where you just generate the data to distill it like in the data space rather than just like the pseudo labeling label space but uh quickly it is you there's one detail you mentioned that I really wanted to unpack a little more and about the relevance judgments for training cross encoders and I know we have like this ndcg metric where we say label the relevance of documents and say like a scale like a three level scale like you know this is a three this is a three this is a three this is a two this is one one one one zero zero right so um like how important is that label like multiple relevance labeling having labeling multiple relevant documents per query for the sake of training these cross encoders oh um that's more than one question um so first of all it's uh it's great point that the distillation through prompting that was uh used in in-person promptegator is so much different from the classic distillation through caled versions that we typically do uh and I would want to address this uh question first because uh it's I I think it's still it's there is work showing that it may still be possible to do the more standard uh distillation as well so the um the UPR paper the um I I hope I not mispronounced the name like by session at all uh what they originally showed that they can re-rank using a big model and uh the uh they have a follow-up paper where they show that you can actually use scale diversions to distill that Knowledge from the main model to the uh to a smaller ranker so that seems to be possible too uh but another uh yeah uh but what the the promptigator and imparts found apparently so it's easier to use prompts to actually uh use prompting as as a generation of synthetic examples and some sort of proxy for this installation of the knowledge well that said I think what's what's important I mean these are sort of technicalities but what's important I think the one important um piece of information one important Insight here is that um we have something to distill and that's a really good question how this happens so I remember I mentioned before that um IR is a difficult domain for machine learning because well for many reasons in this query document similarity features are harder to um to somehow for ML and before uh before all those exciting developments there was a lot of um work on unsupervised uh like dog to deck style word to vaxton embeddings and they don't work well and there were and one reason why they didn't work well because they were trained in self-supervised fashion without direct supervision and we know that it didn't work well for IR moreover it worked mostly mostly didn't work well and it worked really poorly so then Bert appeared at first people tried when Bert appeared bird if people probably people are familiar that with bird bird can they provide embeddings for text as well although this embeddings well a lot of people including actually Nils who was on the podcast they quickly noticed that those embeddings are not good for the table for something like sentence retrieval yes you can use it but but query to documentary doesn't work so self-supervision doesn't work uh then you would need a ton of supervised data and buying coder which is like really tricky to train now we have models that uh a trained using self-supervision and now they work for ritual so something changed and and that that wasn't present before so and we don't know exactly what is it like in scale of the data is it what uh is it the way how these models are trained so yeah so that's um and that's why uh in in that's why for us in in parts light paper which is in fact basically reproduction paper which is very incremental with respect to that much more fundamental work but we try to answer a couple of uh important questions such as can you make it even more practical and another question is um is it really the scale just the scale of that next talking prediction training that makes the model learn something about IR as well which didn't happen before and to do so we need to take um we need to use a purely open source model like Bloom because we can trust that they train it like this way and didn't find unit on any IR data sets or even something like Irish which cannot be we cannot be sure if you take gpt3 do you know how it was trained no I don't know yeah I love those details of the paper like the um the bloom seven billion the maybe the GPT J model so these open source language models to generate the training data which are also a little more accessible because I think like with the gbt3 da Vinci like at the pricing of recording this podcast it's like going to be probably like a cent per you know generation sort of around that so you know 5 000 scaling it so it makes it seems a lot uh very useful to be testing if the smaller generative models can produce good synthetic data as well and then also the smaller rankers because in the end you use the 30 million parameter uh mini LM architecture based rank or model so I maybe could we touch a little more about the inference requirements of the rankers because uh well I guess these just just inference Topic in general the because like the rankers you know if you have like a three billion parameter rank or you mentioned the log profit the language model to get the score to re-rank that either in you're gonna be waiting waiting for the re-ranking forever oh yeah um yeah so so basically uh like our paper although it did get some positive feedback uh it's it's really uh it's really very incremental reproduction paper but what I so so it was uh in fact not my main work project it was a sidekick um I supervise the team of Master students and and CMU and I was doing things jointly with them not just supervising them so they implemented in fact some key component how to generate these synthetic queries and then I took this and um I I ran I ran these experiments most of the experiments and and yeah uh it's originally the question was was that yeah gpt3 is somewhat expensive so can we make it cheaper by using the open source model but that's not not all this story uh that's um this is the part where uh Rodrigo no guerra's team with Bonifacio and all that they scooped us a little bit because they showed in the follow-up paper yes you can replace gpt3 but what they didn't try to do they didn't try to replace the huge model with something that is uh really much more practical and that's something that what we did so the cross encoders they um although they can have reasonable requirements they can be applied on the inference times they can be applied only to um to like small subsets of documents so in even with the I don't remember inference times for a bigger model but even for like 30 million parameter models on just with General pytorch without much optimization it's something like um I I think a few thousand documents per second on the modern GPU which which means you can't afford the ranking more than like a couple hundreds documents received Say by pm45 and it's actually much worse if you try the model that is 300 million parameters you multiply it by 510 and another 10 multiplier if you take the models that uh Rodrigo guerra's team was using with exhibition parameters probably also like five times another five times slower or something like that uh yeah I think it'll be really great like uh sort of like blog post content around weediators getting these numbers out of like how long it takes to re-rank a thousand 100 documents with with each parameter size uh Transformer based cross encoder uh yeah it's super interesting because the the 30 million right it could re-rank it super fast and and uh else maybe because I think like people are used to like I'd say like sub 30 50 millisecond Vector search and bm25 search so uh because I I just like you said to rank a few thousand documents in a second with a cross encoder because I'm just curious if I heard that detail right because with my experiments it's been to re-rank like a thousand documents and probably looking at like you know 10 to 30 seconds uh Which models do you use I it's usually I'm using the sentence Transformers cross encoders and uh yeah maybe like 80 million parameter scale just like uh testing it but uh yeah maybe just for because I think I'm missing a huge part of my knowledge now how are you uh optimizing the cross encoders for inference um I I didn't and I I think it may be uh depended a little bit on the GPU that you're using um but for the uh for the for the 30 million parameter model that I I was using I I did measure the times and it's for 100 documents it's something like 0.3 second for a hundred um yeah so the you can rank one thousand in some cases you probably would need to do so and it's going to be um more expensive but I think that in many uh in many cases people deal with smaller collections we are ranking 100 is enough and in fact that was one thing that we changed between the when we did reproduction but that one thing that we changed between in-parse paper and our reproduction is that they were ranking with thousand documents we were ranking 100 and with somewhat uh with somewhat biggest model like having like 400 million parameters will basically match the numbers on most collections we get in the neighborhood of day numbers while we're ranking only 100 documents so this already shaves off uh an order of magnitude so um and also another thing that going from uh 400 million to 30 million fundamental models is also much more uh efficient although the results are not as good so it's um an open research question somewhat open the situation how to make it even more effective but um what I should have mentioned before I think it's it's a step forward it's not definitely not a solution of the problem uh what is a step forward in particular if you look at the paper over the king Parts they use two models one is as big as our biggest Model A little bit and the the same same order of magnitude um 300 200 to 300 million parameters and with that model they do not beat bm25 at all just except for Ms Marcos parse uh and the track so only on one collection they can do it and we take a 30 million parameter model and we were able to consistently outperform bm45 on all the collections they were using so it's definitely step forward um yeah so um people are in pretty sure people will optimize this recipe even further and we'll get some yeah I think like um because I haven't done things like even just like you know maybe just the Onyx optimizations the maybe compiling into the Nvidia Triton server and we're also really excited about neural magic and the sparse inference acceleration all the kind of things we can do to make here and faster and uh I also think kind of this topic um like I think as we're thinking about this retrieval augmented large language model thing you might be willing to wait a little longer for the re-ranking also because you know you're going to pay for all these Generations so you know if you're searching like let's say I'm simulating my interview with Leo and and I'm uh and I want and I um and I'm and I want to like practice my podcast and I'm talking about in pairs light and I want it to retrieve some passage and I have this big Corpus like I'm willing to wait you know two more seconds for it to get that top result that will help supplement the language model to ground the context and then pretend to be Leo in my use of the language model so yeah so maybe we could talk about just this General space of retrieval augments that large language models sort of pivoting topics a bit um I'm sorry I'm a little lost do you want me to touch what kind of they developed the topic of efficiency a little bit more or yeah if you're interested in doing that for sure I I yeah sorry that was kind of it's like a Loosely tied together transition but yeah this it's like I signed I kind of see like the value of re-ranking is increasing with the large language model oh I see uh well first of all um the the ranking can be relatively cheap so that's that's again um as you mentioned people sometimes people can wait if you're doing an Enterprise search and something within one second is probably uh a possibility um so uh 30 million parameter model is definitely practical and um but you probably need still need to run it on some gpus device so something like I think there may be M1 uh hybrid CPU GPU probably would be required um or like pretty multi-core CPU um so that's that's one thing but the second thing of course uh in can be possible again uh combine it with the retrievers as well I we didn't train retrievers because unlike in parse paper sorry propagated paper who trained retrievers and that can be if you train retriever what happens with the retriever retriever is like buying color model it can be pretty accurate on its own yes every ranker adds something on top of that but usually like 10 15 so if you have um a retriever you can in Factory rank fewer candidate records so instead of say 100 you'd rank on the link 30. so that's or instead of 1000 you would rank 100 so that makes us a little bit more um uh yeah more feasible and with retrievers you can invest more into like use bigger models because you kind of shifted to the indexing time yeah so that's something to try uh and with four four four yeah so um yeah so that's basically uh you can wait a little bit longer you can use buy encoders you can use slightly faster Computing devices and you can use a more optimized um environment and I'm not sure about the neural magic because I haven't uh played with it myself but uh I know that it's like on sip on on the there are cheap gpus now like what things that are like that are comparable in speed total Titan and they have some like they're very cheap and and why not use them if they even cheaper than maybe than CPU why not use them for inference yeah I think um just yeah on the neural magic side of like it's like this combination of things with like sparsifying the models lower Precision taking advantage of like the CPU cache hierarchy uh if people are curious that we have an interview with Michael Goin who explains all the details of that uh but uh yeah uh so yeah super cool and I thought that trade-off you mentioned was so interesting of the um of like how you want to think about putting your resources towards optimizing the buy encoder like vector embedding models with the cross encoder re-ranker models and how if you have really good buy encoders and you would only need to re-rank 30 and then you could have a higher capacity re-ranker like three billion parameters because you're only re-ranking 30 or or this other kind of thing where you're like we're going to throw it all into the learning the rank kind of thinking where you're like let's get like a thousand from bm25 a thousand from Vector search and just into this 30 million parameter ranker which is like the engine to fix it I that trade-off is super powerful I think yeah um absolutely so you can in fact as I said uh the like old style IR can still get like in the in the case of QA collection I'm I'm guilty I haven't published that paper I should uh that showing that actually really can get it's not only like Factory tool that that can be helpful and uh yeah in the advantage although there is some work published on that but the games that I get there they are actually not that big I think you can um can you can I should publish a more compelling paper basically saying that um you can use bm25 it rank just like really a lot of uh bm45 entries using old style Tech or like modernized old style tag which runs on CPU it runs fast and then it gives still you like almost almost like close to that by encoder like style quality and then you can use your ranker and that said I'm not like particularly attached to rankers in cells we had um I think it was more for the paper itself it was more important to show that you can't replace gpt3 and not only that uh we got things for their uh for the in-person paper authors who big thanks uh twin Parts paper authors who uh released all their generated queries so we were able to directly compare regenerate sort of the same using the same documents but using only the generation was done using blue and gptj I think and we were able to compare against the quality and we could see that yes we we get results that are better than GPT 3 results um so that was an important question and answer the second question would we be able somehow get models that are much smaller um that was that was and again we got scooped in part in on both of these research questions by propagator paper or buying parse version two paper uh but I do think there is value to our findings as well and it's also nice to have like independent confirmation of of ideas that's for practitioners it's probably very useful as a as a signal yeah that I love that idea of open sourcing uh the synthetic data sets like the um you know I love hug and face model Hub is they mentioned like the open source generative models being a huge part of this and um like open sourcing the generated data set the the data set Hub papers with data also has like the collection of data sets yeah I've been so interested in wanting to participate in this kind of thinking around the science like we are working on like the beer data sets and so I'm planning on you know putting these in Google buckets and then you can restore weave it instances that have like the bm25 search the hnsw index already but that like publishing of the like gbt3 generated synthetic data set to expand like NF Corpus or you know the beer data system this kind of idea I find it so interesting so so in the podcast I like to kind of like really transition topics um can you tell me about like your sort of your origin story of how you came in to be working on search and then particularly this non-metric space oh yeah um yeah so that that's a little bit the this to my story is uh a little bit unusual if not to say it it's quite unusual because um I'm not like your typical uh not your typical researcher and uh so I I was working in the industry for quite a while um and then I I got my PhD basically very recently um mid-career but one important reason why this happened because the the country from which I originated it it stopped to exist and all these signs that that wasn't that country to is basically there was no funding there was no way you can go and get uh doing research and although the fundamental education like remained strong they're the only like opportunities were like go and you know work on financial systems databases and things like that um but I got bored by this pretty quickly and um talking about how I can automate this stuff like green creation and writing of SQL queries yeah this this stuff is pretty repetitive so probably a lot of that can be automated uh but anyways so I got of course I was interested more like in research and at the moment that was like basically late uh it was like 30 no it was a little bit more than 20 years ago the the only like AI of which um it was basic research and search engines there was a big topic that we we had Google that you know um and that's how I got interested in retrieval algorithms and I was uh doing that that stuff part-time and at some point like I talked with my family uh that they um and they basically supported my decision to go get like a formal degree because unfortunately in our society it's really hard if you don't come from nowhere you're not being treated seriously in yeah you will not just get like good quarters yeah it's uh some there are some stories of people going to I don't know some engineering job and then they emerge as researchers there is one interesting case of uh of um the author of Chain of Thought prompting that's an interesting story they're interesting but they're rare very rare cases that happens so basically yeah um and that's how I got interested in retrieval um not so I I didn't plan to work on retrieval while in my PhD program but I joined a lab um whose Pi like Eric nyberg was participating in that IBM Watson project and for me that was like super flashy and super AI beating human Champions but after like reading and learning about the topic what we quickly realized that oh IBM Watson system is actually is not is is pretty simple it is basically a retrieval augmented model what we have now retrieval augmented NLP that's what it was the retrieval uh retrieval based QA system and that's how I came back to work a little bit more on the duo because I decided that you can retrieve the you can improve the the retrieval component that's that will retrieve the perform improve the performance of the whole system the okay system and now answering your question a little long-winded explanation about how no metrics such came into place so the the non-metric we have like in in the real world we have distance which is euclidean distance and has a bunch of nice properties and it has satisfies that actions of the metric space but there are a bunch of similarity metrics so we don't normally call them distances who are wrong they not do not satisfy our intuitions of the 3D world and they are non-metric like they assigned similarity the kale Divergence they're all but they can still be useful because they're used as a similarity function to compare queries and documents and so the idea was coming from that Old uh feature engineering world so you can compute that similarity you have a data analyst that would compute uh that create this uh the features combine them and that would be some combination that would not be nice and you would need a special kind of system that would uh support retrieval using this similarity function so sorry I'm still trying to wrap my head around it so it's like a learned function for the for the distance because that sounds a lot like the cross encoder to me uh I I know that sounds like really alien to people because nowadays in like recent years we made a huge progress in representation learning so basically you can compute very good representations and compare them using the euclidean distance or they can sign similarity between normalized representation is basically the euclidean distance uh and that's all you need it's just everything else like neural network um job to make this representation comparable using a simple similarity measure but in the old days uh people would create multiple features and the idea was that oh so these multiple features there will be complex function people were contemplating some expensive similarities something like Earth Mover distance which is really inefficient to compute but people like like there is a paper I think there that's using uh this earth mover distance to compute more than bearings with some small gains although Earth more resistance I think it's still metric but you want some some system that what it's it's a geogenetic metric it's much harder to do retrievable with this generic metric because it's not euclidean so it's not even like vector space distance so there was that idea that all that's uh let's help analysts by building providing them with tools that would support retrieval using these complicated similarity measures yeah so I remember studying like uh wasserstein Gan where they you use like that Earth mover's distance um do you mind explaining that from the beginning what what the difference is between Earth movers distance and this is like Optimal transport like what the difference is between that and then just like a KL Divergence between vectors um I have to tell you that I don't remember a very specific nuances of how Earth what was distances calculated but um as far as I remember it is not it is not represented you cannot represented by something like take Vector coordinates just like compare them like in the euclidean distance you can't just you have a bunch of simple distances the euclidean distance the lp Matrix and the decay of the versions they're still pretty simple because you just compare uh pairwise you do pairwise coordinate comparisons comparisons like I quote unquote because you do some operations like either you subtract them on square or I can kill the versions it's a little bit more involved but basically it is a pairwise operations which are often that you just can simulate why are the inner product computation and then it's a simple iteration in the vector space but in a more generic uh way you can consider generic vectors not a vectors by generic space that doesn't have a notion of this simple vectors or they are not compared so easily but they still they would space in school could be called something like a metric space and um it would have some actions in particular metric space satisfies the triangle inequality symmetry and you might be you might Envision searching in this space without using vectors directly just distance computations um yeah think about it like um I don't have like a piece of paper on the desk but like I crumble up a piece of paper and that's kind of how I think about like what a manifold is like a discrete topological structure where you can't you can't let or you know my hand right now is a manifold like I can't do distance from here to here because this doesn't exist sort of a sort of like have been thinking about that kind of thing but uh yeah maybe I know it's getting a little distract a little off topic but do you do you have an interest in things like category Theory and these kind of topics um not really familiar but I think the the manifold example is interesting in that um it's a good illustration that you can reason about this space in abstract ways like you have some certain restrictions you can go from here to here but not from here from another point but at the same time you don't reason in terms of explicitly in terms of coordinates and that makes a huge difference in terms of the search function because your search function is going to be restricted the you have less information about the space and that's that's going to be big restrictions but what's interesting um that there wasn't that that uh sequence not sequence of uh there is that uh a class of algorithms that rely on building neighborhood graphs which surprisingly uh people realized gradually that those algorithms are are sufficiently generic to provide a retrieval in in the case where you don't have those explicit Vector space coordinates and not only that they can oftentimes beat those uh algorithms that I designed uh specifically for this knowing like having this coordinate system and access to individual coordinates and they can be so we have that um yeah and once once I realized that I could use this kind of algorithms and I I I saw the research opportunity which ended uh in the that was a dead end to a large degree unfortunately but it may be useful in the future and we also helped uh helped promote these approaches which were also super useful for uh distances like the euclidean distance as well as for inner product search and cosine similarity bases yeah it's amazing I I think the whole like graph representation learning which is usually like let's try to get the graph into a continuous space let's try to get the graph into a euclidean space so we can like do all these things I I also find it so fascinating the the hsw like the the graph index that's paired with the way that you organize the euclidean space distance measures do you think uh yeah I I don't have like anything super conquer I'm just kind of like thinking out loud with this but yeah so interesting so can we also talk about um so you developed this the flex Newark Library you've built the hsw lib and uh nmslib uh can you tell me about your experience developing a library to organize your experiments and communicate your research this way um yeah absolutely well first of all I'm wasn't like the only the the person working on this and then um the that with respect to that graph based retrieval algorithms I I helped to promote it I was I created the first optimized version of that that um to some degree was was reused but then we of course take our most efficient implementation was contributed by Yuri malkov uh but more generally um that project was uh in terms of organization of the the the the research it's actually quite interesting that this project started as the PHD project of um another person Billy naidan who also contributed quite a lot to it um and at some point like everybody's using the python version python biting he created the first python bitings you can't it wouldn't have been possible to do anything useful without python bindings um yeah but that that started as his PhD project and we had no uh graph algorithms at that point uh these were his interest wasn't this retrieval and generic metric spaces so we did a little bit of of work on this topic and it was basically two of us um and then we had a couple more people joining our the project so it was like you know like sort of random alignment of people on the interests and uh um and their contributions and I was kind of holding that um I'm a sleep project together so uh doing actually a lot of uh the software engineering job the uh some of the running some of the experiments on the published uh a couple of papers of that non-metric search and also maintaining it and it's um yeah doing things like that basically even like encouraging people to to contribute although it wasn't a big project it was a small collaboration it was a little bit it is actually a little bit different with a flexnard because flex No Art is um the retrieval toolkit that uh is basically separate from animal sleep and it's just incorporates all the the good stuff like bm25 and Keenan and search and that was solely at some point it was like 100 my project and what I'm trying to do I'm I work with people and whatever we do jointly for the information tool we trying to um reuse this library and I also like I it it actually started it it started as from it comes from my PhD this is so I used it and I you know it's like I improved a little bit and a little bit and then a little bit more and then um it becomes more useful with time yeah it's really interesting and I I would think that like waviate could be used for a lot of this IR research as well where you could you know you have the you can build off the implementation of hsw product quantization like as disc a n is developed and these kind of things and I guess like my thinking is like the library it adds value and sometimes they have like the data sets are built in so like some of these IR data sets is really easy to load the data because you don't have to clean it it's already clean and sort of a part of the library as well as these indexing algorithms and sort of like the whole ranking flow yes I guess like um what do you think are the this might be too open-ended of a question but like what are the most important things for an information retrieval research library to contribute to the science to you know be used oh that that's uh that's that's a great question and that's a very open-ended one I don't have definitive answers so the uh I think the animal sleep definitely uh was useful and that's kind of funny because I I don't have a key contribution that library and it's still uh wouldn't have existed without me or like the league and then um one thing that it should be useful in some ways it should uh there should be some missing elements like an animal sleep we had methods that uh before us nobody was or like few people uh seriously considered them as um as you know something to be used um that's like one missing missing piece of science it's another thing is the ease of use and I think say within Amazon sleep we um we were able to largely to a large degree achieve this so first the Billy created this Python bitings and then I got help from another person to make it like you know deployment easy and installation on um computers that you don't have to compile you'll get some pre-compiled binary definitely ease of use you just do Pipeline and also on a bunch of operating systems ease of use definitely it should be it should should be no pain for people to install um the it was great point about downloading data sets and I personally underestimated this part for a while and flexnorth doesn't have this ability but I see what pi syrini is doing and I definitely want to appeal for that for them from them I don't want to support a lot of data sets um because it's like you know it's difficult but like basically I say I want to convert beer data sets and Ms Marco and store it in the cloud so people when they use flex not and say bm45 indexes from flexnor they can just basically run a command and then it will download things as well so what else instead of like years of use it should it shouldn't it should be yeah somehow uh it should have some there should be some missing is some missing piece of science or technology that that people need and and last but not least I think I should have said that maybe earlier um promotion makes a huge difference yeah so people should know you should just writing something beautiful and putting it out on GitHub nobody knows uh sometimes Word of Mouth works but you know you compete with Google and Facebook and the others whenever like you get good results from some leaderboards you publish papers you then people you know get people interested so that's I think an important component too yeah super interesting I think to build on the data sets part you've done some work on Long document uh benchmarks and I I think this is a very interesting topic that hasn't really been covered like if we're gonna rank you know if we're going to search through entire scientific papers sort of can you sort of just like set the stage of of the distinction and long document and sort of how this hasn't been studied as much as you know Ms Marco natural questions like that whole tour of the beer data sets yeah so that's um that's that's a great question that's actually one of the uh important limitations of the Transformer models which are currently I feel is being removed now at least like mitigated but originally the the way the birth was created it had to um it still has if you use that original birth model it has a limit on the number of input tokens it could process and it's only 512. and it's not a whole lot right so and if you have a long document you need to classify it or you have to do something and there is a bunch of algorithms that are doing some sort of clever chunking aggregation of results and um that's uh that was something to Benchmark and um compare for the paper um that said I feel like the community has made a really great progress on removing that limitation and with approaches like flash attention I think we'll soon have uh Transformer models that will support very long inputs up to a few thousand a few thousand more than 10 000 tokings um it's I think it's a matter of time but it should happen yeah I think there's kind of like um maybe um yeah so that that idea of like flash attention I've seen like staircase attention I I went to neurops this year and I saw a lot of the posters are like here's our sparse Attention our take on Sports attention right it's like this linear attention kind of technique and and I certainly agree like things like the Sarah Bros chip as being one like big chips that get that like you put in massive inputs to the Transformers that I find that whole thing to be incredibly interesting but maybe if I could pitch you this other idea on what we're thinking with uh with a long document representation where like our so our weva data model is like classes and then like references to other classes so we think about it like article has passage passage and so like article has passage passage passage passage passage and then um and so one way would be to represent the whole article you would have the vectors of each chunk and then you would average the vectors from the chunk to represent the article another idea would be maybe we like cluster the uh passages and then we have like three centroids that represent the article and then a newer idea with chat gbt expensive but it could it's an idea is that you would uh you would you would say like please summarize these passages so like you you go one by one through the passages and then update the summary and then from that represents the article so what do you think about that kind of like graph structured way of representing like abstract long document object with these chunks oh yeah um there is definitely a bunch of way how to uh chunk data and aggregate it and represent in the this phase I am I have to tell you that I have to I have to give a disclaimer that I'm not particularly a knowledgeable person in in for this kind of work uh in yeah basically I did work in the domain much uh but it's my yeah you can do interesting things you can do embeddings and then you can do something probably like graph neural network based on top of that again how well it works I can't tell uh but um I I can tell you that there are definitely efficiency issues because what we see a lot of times that uh things that work best are trained end to end meaning that like you you say you have a graph you have like embedding for each node you embed it and basically your neural you feed a large chunk or like the whole graph sort of into you like the whole document or the graph of the document into that neural network and then embedding the being although embeddings might be somewhat to be computed warmed up but in the end you want to run this end-to-end compute gradients propagate them all the way through the network because if you don't do this your your representations you don't know how good they are right like coming back to that example to bird which is Trend in self-supervised fashion but whose representations are not good enough for retrieval so it can be something like that and then you need and then fine-tune the whole uh thing again and that's expensive so the the modern accelerators they don't have enough memory for long documents so that's that's one limitations that I would I would think of and then you can be clever and somehow split um but how well it works I cannot comment because I don't have first-hand experience yeah that's a that's a such an interesting topic because the graph neural networks to me like the big appeal of it is how well it can handle uh like variable size inputs so like one article might have five passages the next has like 30 and then the graph neural network I think like the end-to-end thing it's very like in the graph neural network sense to stay on that I think it would be like you update the original like you send gradients all the way back to the input would be maybe how you would try to do it end to end and you get like new embeddings that way sort of that are synced up with like the context of the graph but yeah but but then you like I find it very hard to believe that there will be like zero shot graph neural networks that that like represent Aggregates of embeddings for long document representations like I think you'd need to train this on your particular uh thing oh I I totally I totally agree with you that we um the there is a relatively poor transferability of neural networks to new domain and tasks and although uh openai made some great progress on in some domains like with a clip model uh but I think it's still you know because we uh oftentimes it's still like really difficult in many practical situations to do this transformation although there will be some progress there was it's it's an interesting topic and it's a complicated one and I again I have to I can only I I should probably repeat the disclaimer that I'm um that's that's not that's not my you know uh it's not my primary expertise this more structured objects regretfully so I cannot tell you what works and what what not what doesn't work in this uh space yeah I think it's a pretty Cutting Edge topic from my understanding I know there are definitely some graph neural networks uh experts out there in the world but um I think this is a good like not leaving the graph neural network thing in the past talking about the um the in-domain out of domain so you know we're kind of talking about like this idea that you if you want to have a graph neural network that can represent like your cross-reference schema you would need to fine tune it in domain on your problem and one of the big arguments around out of domain in domain one of the key things to be mindful of is robustness this topic of like robust generalization the out of domain models being really good at being robust relative to in domain like there's this great paper called wiseft where they take the clip model off the shelf that's trained on something like Leon I don't think it's I don't think the clip data set is open source as far as I'm aware uh and they fine tune it for particular image Text data sets and they show that like you know like the out of domain model is more robust so uh lee I know you've done some work on awesomeness can you sort of explain what robustness robust generalization is oh yeah sure so the uh well first of all when we talk about robustness we uh typically separate uh it into natural robustness in adversarial robustness and what I worked more on was uh actually adversera Boston's although we did some work on the um national uh natural generalization of natural robustness for the information tool too but basically the the ID is is very simple and I think you explain it it really well those very good examples so you take it something that is trained for one type of data we call it domain and apply it to another type of data which is another domain and things don't work as well and we call it distribution shift although there is no real distribution and some people like oh like what's distribution here but it's a it's a common terminology we somehow envisioned a probabilistic data generation process that generated data in one domain generated data in another domain and in fact there may be some because how is sample is is a generative process and then there is some some mismatch between properties and because our algorithms are statistical so if there is difference in statistics between these domains it's sort of inevitable that you would um you would get this um differences and with adversarial or business we wanna it's actually really I think it's probably going to be um it's not it's going to be a little bit on off topic uh because with the reserve robustness we assume that a malicious users can modify inputs in a way that would completely change model prediction in a way that you want it it's a it's a very huge problem in in like academically and it's not there are no easy ways to solve it apparently but I think it's actually doesn't um it's not clear if it has a big um practical impact because it's difficult to attack these models in practice it's easy in the digital domain but you never know the model you you can't usually control the inputs at the say pixel level or so it's difficult um and with text you don't have access to gradients because that's discrete discrete thing but I think the natural robustness is is a much more important topic for everybody oh that's your robustness of the distribution shift is something that we should really care about at least in the near future awesome sorry there's a bit of a lag in the connection but it's all back together yeah awesome yeah I I like this example of like it's like they have the self-driving car and it's like oh no an adversarial optimized stop sign like that kind of like you look at like the the brain of the network to optimize with gradients some kind of imperceptible thing yeah I also don't am not super interested in that but I'm interested in the general thing of like if uh like say you want to test the robustness of your Tech search system by uh generating paraphrases of the queries and seeing if the paraphrase of the query returns like far different search results than the original query or like the image analog would be you have like an image and you rotate it or you horizontally flip it increase the brightness these kind of augmentations and that produces like totally different search results so how do you see robustness uh impacting search oh um that's yeah that's definitely great question and the I do see robustness uh being a huge topic for neural networks because uh it will frustrate users who do not understand why exactly why they change the inputs a little bit and that would break systems and I and I'm pretty sure that all those systems are breakable there is no easy solution what uh that I am aware of and I think to things that work uh somewhat reliably in as was demonstrated it's it's another you know talking about there is that concept of Peter peel and machine learning that no matter how smart we are just bigger models and and more data helps us so I think in terms of robustness it's probably um we need again the the Practical sort of the the only practical uh remedies that we have now are train more generic models on more data that's what open AI doing and train models that are bigger and of course both Solutions have practical limitations too so yeah like one idea I'm sort of interested in like is um the bm25 vector search Hybrid search is that more robust now because I think of like the sort of symbolic lexical search being more robust and then so like on that layer and running some experiments to try to like as I'm still getting the beer data sets and we've almost finished and then I'll be able to do these kind of experiments and then also I think with the ranking layer like um maybe you can have like you you use the same rank Fusion you use in hybrid search to have some kind of symbolic like clicks or something like that where you where you have a sort as well as the you know neural re-ranking in the new rank fuse those to maybe improve the robustness on sort of those two layers oh yeah absolutely so um so yeah so again uh I perhaps I should have mentioned that that some sort of ensembling usually improves robustness 2 and that's your classic result that Ensemble can produce both variants and bias uh in particular variants so someone sampling in particular like diverse approaches using diverse approaches can work well uh although the information that's again the example shows how unique the information in triple domain is so look we are like it's 2023 neural network Revolution after imagenet it's a decade in Inspirations he will we still use build 45 because we we have hybrid systems now we still need Bill 45. okay so that's difficult uh definitely tells about the uh the difficulty of the retrieval domain and so one problem um uh like providing a kind of more specific answer to your question so Fusion is definitely one way to go uh then um there is a problem with fusion though the you you get bm35 and get results from build 45 you get results from a cross encoder or buy encoder and you get scores of course and their scores are not comparable so how do you fuse them that's yeah so like you can do that round robin right then um but the problem with round robin is that if you have systems that are very different in terms of the effectiveness you may end up like injecting very bad bm25 results into very good uh and the other way around um yeah uh but there are also other Solutions uh probably we can come up with the models that are somewhat more robust in particular cross encoders are more robust than buy encoders so you can't to some degree if you're rank with the cross encoder that's actually one advantage that I could have uh recalled when I asked me why I use cross encoders uh Crossing quarters are somehow more robust like um uh yeah so this that's what takura and and quarters uh founded a beer paper that's actually great and maybe there are other models that are using maybe lexical Clues and other signals kind of combine it together maybe more robust than just when you look awesome colors to um yeah I have to pick your brain a little more on the um on the rank fusion part because I've had so many conversations about this score based Fusion versus rank based Fusion that I'd really love to get your take on this as well um so yeah so if you so like if alpha alpha is the parameter that determines how you combine the ranked list of bm25 and Vector search and then like 0.5 means equal contribution uh so so with tuning the alpha parameter if we tune the alpha parameter of like say on beer like we're going here it is from zero to one point one point two like is is that bad machine learning because you know you're kind of you're kind of tuning a hyper parameter on the test set maybe you could you start with your thoughts on that idea oh it's a little bit change of topics so there's maybe uh uh I I want to make it like um sorry I want to make such the stage for this because I think we jumped to uh quickly so as we talked before about different systems let's consider two just to producing different kind of scores that are incomparable between 25 and say cross encoder and one one approach to combine those just do like Crown Robin uh like for one like basically mix them in the order of uh arriving but another approach is to compute the linear combination of this course and it can be potentially better uh but there are two issues one issue is that you may lose some of the Beyond 25 results of the good because they will be pushed down the total score is not going to be but maybe it's not such a big problem but uh another problem is that you need to have that Alpha coefficient uh the fusion and how do we choose it well ideally we choose that coefficient on the um training set but we have a beer data set which is does not does not have a proper development set and people want to uh want to claim say uh high scores on beer because that attracts like basically as an advertisement of the system and their approaches so if you are like I'll gonna be very straightforward if you just fine tune Alpha on that full beer data set um it's it's gonna be an upper Bound for your system performance but you can't claim that it is achievable like it may or may not be because you may just overfeed it's not such a huge data set right it may be maybe maybe you're just overfitting if you you for example uh you at some point you decide to double the number of entries in the beer collection image data set imagine like we somehow find a more track covet documents and queries and expand it and it will stop working it just don't know so it's not like a good way to say this in general is what I think would be fair to do is to do some sort of cross validation for example uh divide each test set into five compute that Alpha on that and then test on the remaining parts and then it would uh would tell you like oh like this is going to be more um robust more fair estimate of how you can perform on the data set in a few short settings say or something like that yeah it's so interesting I I also had that like let's see what the upper bound would be on a conditional Alpha where for each uh query we go through the whole list and then see which one performed best and then like as a quick like kind of hand wavy thing like on the NF Corpus I was seeing hits at one go from like 1400 to 1800 out of 3200 doing that just is like a quick sense of what you can get from that but I that other idea of like you've trained some model or or a linear combination or some optimization of the bm25 score and the vector search score or the cross encoder score uh and then as you mentioned but then you those scores could shift I guess you normalize them with like it's like the max BM 25 score over the minimum BM 25 score you know do that kind of thing um yeah I think it's very interesting because like if you had like a like a query classifier that like takes this query and classifies how to weight the bm25 and the neuro like how to weight the lexical neural methods yeah I find that whole thing to be very interesting even though I don't uh yeah it's it's I because I usually think of the Cross encoder as like a stage one stage two thing where whatever the cross encoder says is the output is the final right list where yeah yeah so it's really interesting this kind of high research thing and I think all these yeah it's such an interesting topic um so anyways uh I think it's a great coverage of topics that Leo do you have anything else you want to maybe add I just want to quickly follow up on your um on New York uh or the problems that you described because that's something is indeed is a problem with fusing this course yeah we can compute the linear combination of this course but other like these scores actually properly normalized there is a potentially huge variability across queries we just don't know in in in in possibly we don't want to use the the linear model you want to use something like gbrt right boosted regression trees and in fact that's uh one way to go you need a lot of data for this uh that said in practice for like small domains like Ms Marco I played a lot of this when I especially uh when I was active on that leaderboard in the early stages when I was uh and I trained a really strong uh Puri like classic IR model for this and I play for the MS Marco document leaderboard and I played a lot with like fusing different scores from different components and I had two options to use one is linear fusion and another using this non-linear GB model and uh and I only in one case I was able to get uh somewhat um noticeable performance boost on top of the linear model so yeah so linear works well um although with the caveat that that's within a single domain and once you go out of domain you're not only this coefficient maybe make efficient maybe become like known invalid not appropriate and the audio scores may change like a normalization confusions may need to change for another domain it's it's really um I I think robustness is going to be like really huge topic for the coming years yes more data and more more everything will help but it also comes with the efficiency cost and eventually would want to run these models maybe only even on your phone yeah so it's uh it would have to be super efficiency yeah yeah it inspires a lot of thought on like if you have this yeah it's like a very tuned system for this domain of data uh because you not only have the embeddings that come from maybe a particular like if that's if you fine-tune your embedding model on your domain and then you have the rank Fusion is a fine-tuned parameter for your data the cross encoder maybe also is fine-tuned for your data and then that domain shift maybe like don't like uh distribution shift detection I I'm not like super caught up with those methods but maybe that kind of thing could help uh like you know like maybe the idea of you in the vector space you cluster it in like hdb scan can like do a good job of telling you like this is an outlier point in your vector space kind of you know that kind of thinking uh yeah just oh yeah um uh that's that's Again The Good The Good the good reminder that the there is also the difference between more like kind of academic style of um the machine learning where you just compute an average score and be done and the data scientists who work more like a real problem they care much more about outliers they care much more about the what sort of the what's in the data and what's like specific results for the specific they dive much deeper and they care about like the the whole Spectrum yeah but anyways so I I think it's a difficult problem uh I'm I I can't I don't have a unfortunately I have to admit that my crystal ball is not good enough I made a lot of mistakes about what things are going to be in the future so that's why I don't want to even make any like predictions maybe the scale will do a good job for us although we will have to be efficient uh although it may not do much more beyond what we um we see already but it's hard to predict because it all those things there is no Theory it's just like your intuition it can be wrong mind was wrong many times so I don't want to make this prediction about things that are not uh clear but I'm pretty sure that it's become it's not even like it's become very clear that the the Deep learning the the language modeling the generative models it's there is no slowdown there is no like winter per se some may be going down in terms of like I don't know finances some layoffs no but it's not like a real winter some a little chill but that's a huge topic it's probably the area to be in the only thing that I don't want to be really narrow IR researcher I think it's probably um worth trying to be kind of more generic and keep an eye on several sub problem several sub-domains so that's that's probably um what I I could recommend personally as as a researcher and engineer yeah I think that's really great advice as well again I think it can be kind of overwhelming trying to keep a tab on like all the all the little subtopics oh yeah uh somebody uh somebody use like is quite like really really great uh term for this it's uh it's breakthrough fatigue nothing less that breakthrough fatigue so in the old days like IBM where they chess playing machine IBM with the you know beating human champions in Jeopardy and that happened with like you know five to ten years yeah and now like we beat humans and go we beat humans in this and then like our pH systems are super human kinda and the chat GPT is doing so much stuff oh it can't do math yet but I don't know maybe like tomorrow it will be able to do math too so and how soon will it happen you never know and those things that not only just like great papers but all those uh superhuman claims of superhuman performance their density in time is just like really uh yeah you can get a fatigue from just following all that stuff let alone working on on any problems and trying to improve things yeah yeah I've had some interesting time definitely interesting time to be an AI and deep learning yeah for sure yeah I I feel the excitement of like I think the Breakthrough fatigue is very interesting because I also like was like with the stable diffusion thing I don't know what it was about me but I kind of just like was like all right yeah it's cool but but this new Chat gbt thing to me this breakthrough is like I love it it's so fun like it's so much fun to play with it yeah it's definitely nothing like we saw before um it's uh it's absolutely amazing in many ways yeah awesome well Leo thank you so much for joining the wevia podcast I think this was such a collection of information and a tour from the you know the cross encoders the re-ranking the fusion the uh you know the non-metric spaces in Paris light of course and I I'm so excited to play around with in Paris light I think that generating data for the custom Training of people's particular models even just benchmarking on these data sets and facilitating with that I think that idea is just massive so thank you so much oh thank thank you oh thank you very much for inviting me I really enjoyed our conversation ", "type": "Video", "name": "leo_boytsov_on_information_retrieval_science__weaviate_podcast_38", "path": "", "link": "https://www.youtube.com/watch?v=X-VFkq6fHDY", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}