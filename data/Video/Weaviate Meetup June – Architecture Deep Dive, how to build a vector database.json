{"text": "Weaviate is a full-fledged vector database with full CRUD support. In this meetup, Etienne Dilocker of Weaviate explains how ... \nthanks thanks a lot for joining and uh today i will as bob just said i will really give you a bit of a more of a deep deep dive on uh basically how vb-8 works under the hood uh what its architecture is but also what our roadmap is and also focus on the whole aspect of cloud native a bit more so why do we do such a deep dive meetup um so as i just said this is really going to be about more sort of in looking at the inside of bb8 uh if you haven't heard of vba did or haven't used yet um just to let you know there's also a new introduction video out that i bought recorded i think uh one or two days ago and that just went live and it basically shows you all of the things that you can do with uh vv8 in like a 10 minute introduction uh for today i'm going to assume that you have a very rough understanding of what vv8 is so that i can um tell you basically how it works and how it works under the hood so why do we do this first up this is basically been based on on your feedback so based on community feedback and there's been very positive feedback we've heard people say that while the architecture is one of the reasons of why i want to use vba or maybe they've even used something else before but that doesn't maybe isn't compatible uh with what they expect for from a search engine architecture-wise so that's very good for us to hear but there were also critical voices that said well you aren't very open about the architecture you should highlight it a bit more or maybe explain the architecture a bit more so basically this is us being vocal about the architecture and highlighting what's going on under the hood so that you don't have to read the the code to get an understanding another thing is just overall sort of if you look at the ml and lp field uh nlp field um and if you look at this sort of research versus production thing a lot of projects are in a very very early stage where basically um the teams at the company at large enterprise companies every large enterprise company needs to have at least one a team focusing on ml stuff and nlp stuff but a lot of these projects aren't in production yet there's maybe a bit of research going here and maybe it's because the business has other priorities or maybe because the the quality of those nlp tools maybe they're not good enough yet but another part of that is also because getting stuff to production is very very hard there's a lot of things that are currently or that that suddenly get introduced when you want to move to production that simply aren't the case if you're just doing sort of a spike or discovery with some stuff locally so basically you can't just take the jupiter notebook and put it into production and even if you could you probably shouldn't but uh the good thing is that there's solutions such as bb-8 that really are meant for production so basically what i want to convey today is that if you have those production requirements which can mean large scale it could mean that you need redundancy fault tolerance it could mean that you have specific security uh requirements that you want your stuff to be observable then that is really what vba is meant for and also by being open about sort of how we achieve all of that and giving you a bit more insight i also want to convey that basically the people behind vvate that they make conscious decisions and that while we do sort of what we do that we really also know what we're talking about so it gives you a bit of trust in us as a team building a solution that's meant for production so this is a graphic that i just tweeted out maybe some of you have seen it um yesterday and basically this is a very simple benchmark of uh importing a specific data set into vba and i will sort of tie this into our current architecture roadmap of where we are and what's been happening but the main takeaway for this at this point is basically just we've been improving things and things have been going going faster and basically as you can see on this roadmap sort of each new version reduced the time that it took to import a data set into v8 and um yeah you'll see in a second how that ties into our architectural roadmap so before we get to the roadmap itself i also want to talk about how data is stored in bb8 sort of at a very very rough aspect like the 30 000 feet view just to to show you sort of what happens in vba when something gets stored and an important thing or an important distinction between vv8 as a vector search engine and as database if you compare it to for example just a a a n library vba really as a as a yeah as a database it returns the entire object and this object can be on disk and not just maybe a reference to that object not just an id but not just that you can also combine the vector search itself with a structured filter so we call that scalar search so for an example you could say well give me all the users that are close to this specific user which would be the vector search part sort of the fuzzy part in the vector space but only if the account balance of that user is in this example less than or equal to two thousand and basically we can do that efficiently and not um sort of uh what we wanna do is basically do this sort of uh pre uh uh filter where we say like okay first we to know what those users are and then limit the vector search to those users because if you do it the other way around if you do a post filter then you first do your vector search but then maybe none of the results that were returned in that limited search maybe none of those match the filter so basically we want to do this pre-filtered or we do do this pre-filter and also because it's a database any mutation that you do so any import anything that you do in in viviate by the time that we have returned or by the time vba has returned a successful status code to you you can be sure that that data is written somewhere on disk so it basically gives you the right guarantees that you get from a database as opposed to i don't know like a library that would only run in memory for example and as a result this means that um when we store data per index so you see here is basically one index that we have per class and well we'll see this graphic a few times more today um generally there are three parts there's a vector index right now the vector index that we support is h and sw so that's sort of a very common index suits are our purpose very well because it allows mutability it's sort of relatively easy to customize but also because we surf the object we need to store the object so that's the object storage and we need to build up this inverted index so that's basically what allows us to do these pre-filters that we can then combine with the vector index so essentially these are the three things that we store so what you see here is an overview of a roadmap and the cool thing about this is um other than the nice visualizations that we have the cool thing about this is that we're not starting at step zero right now so basically um yeah you've seen the the uh the graphic with the import times that we're going down um we're already so this is already going on so some things are already complete and other things are basically in development right now oh sorry that was that was too quick so here if we start with step one so step one the first thing that we we did and this was also triggered a bit by community feedback so um a few what was it i don't know about two or three months ago i think um we were trending on github for for a couple of days and a lot of new users came into vvad and it was really cool it was sort of the first time that we got a lot of feedback like of course people had discovered vba before already but that was the first time that we got a lot of users in and they were able to give us sort of feedback very different feedback on very different parts some of the documentation some on the implementation and one of the things was basically one of the feedback points that we got was basically hey this is hmsw that's pretty cool but i noticed that if i import a large data set it's kind of slow like it doesn't it doesn't get the kind of performance that i would expect from hnsw and then we sort of took that feedback seriously and compared and saw that okay yeah that this is true like it's it's fast-ish but it's not as fast as maybe hnsw could be uh so basically the first step that we said on this this sort of the the end goal of the roadmap is just scaling in general and as you'll see later scaling includes horizontal scaling but scaling in general for us also just means being able to to work with larger data that's like obviously when the space of of big data so we need to be able to to handle large data sets and one part of being able to handle large data sets is if we're building this sort of expensive vector index at import time it just means we need to do it fast enough so um the first step was basically optimizing the h w index implementation this was mostly fixing a few bucks here and there just general performance optimization viva it is written in go so there are a couple of things that we could do and also one thing that i find particularly cool is if we have one assembly file right now in our implementation uh where uh the the dot product calculation that we use that we also use for for cosine distance of course and distance in vba is just a normalization of the vector and then doing a dot product calculation which is the same thing as cosine distance but it's a bit faster if you only have to normalize once um yeah and this is uh there we have an optimization now for um for avx-2 compatible cpus which doesn't work by default in go so we had to write it in assembly which then you can integrate and go which is i think this was the major difference then if you compared a library like hmswlip which is like the reference implementation is the implementation that that came along with the paper and that's written in c plus plus if you compared that to vb8 before or to vba's h and sw implementation uh while we were just not using these hardware acceleration i'm not these using these avx2 instructions and that's where we we lost a lot of speed so basically we have completed that and this was released with all the other performance improvements in v 1.4 so if you look at the the road map uh sorry no the roadmap at this graphic of the benchmark uh 1.2 uh was basically yeah that was the the controller sort of the base before we started optimizing anything um and it's the time was so high that you can't even see it in this graphic anymore it moved off the the top of the screen and then in 1.3 we took care of a couple of performance fixes and then 1.4 i'm using 1.41 here because it was the latest 1.4 version but that the fixes themselves or the optimizations were introduced in 1.4 alongside with the uh image2vec module by the way so if you want to use um if you want to vectorize images out of the box that has been possible since 1.4 um yeah and there you see the the major drop-off from from 1.3 which doesn't fit on the screen anymore two to one point four so basically these changes were step one on the roadmap but as you can see uh there's something else on the horizon and um yeah and that's basically the next step so let me get into that so when you sort of design a database completely from scratch or when you plan out a database you have to make a couple decisions you have to make a lot of decisions but one decision that you definitely have to make is like is my database gonna be read preferring or is it right preferring and this this has a lot of impact and essentially by the time we decided to build vb8 um h and sw just came out or was just sort of just gained i think it's been out for longer but it just gained popularity in the community and people were using it and we were basically thinking that's a really cool start we need to build a product around this like we need to build a whole database around it so sort of this focus on these fast query performances that you could get with h w was pretty clear for us like okay that's that's the the sort of benefit of a vector search that we need to to offer like it needs to be fast at query time so basically when we started out we were saying okay that's the the first goal that we need to achieve it needs to be fast to to read um but turns out that if you if you do these things that we do with vba which also means that if you also store the object and if you also um build up this inverted index and if you're generally in a space where there's a lot of data it turns out that you also write a lot of data and that they just can easily become the bottleneck so basically what good is the fastest reading database if importing data or ingesting data into that database takes so long that you can never get to a very large use case that the database could basically easily handle from from a read perspective but you just can never get there because it's it's or i don't know because you you in uh you create more data maybe that your your database can ingest for example and um that's basically the point where we said okay now that we fixed some of the performance issues that were going on in the vector index and where that we're building now the actual bottleneck is just storing the objects itself so now we need to address this and this has been a a pretty big change so this it just merged the the pull request that had i think it was exactly 100 commits then they had had to fix something else then it was 101 but so sort of just to put this into perspective this is like a major change that's been been happening in vva and essentially we went from um from sadie's b-bolt which is a fork of the i don't know who it's by the original bold which is a um yeah a key value store that's written in go that uses a b plus tree approach so essentially sort of a very common pattern that you have for for yeah disk access or for for data access on disk that works pretty well this data data store this key value store also supports transactions which is something we haven't actually needed at all in in vba so far and then we made the decision okay we need something now that works better on on writing we write a lot of data we need something that's not the bottleneck there this is why we went with an lsm tree approach and essentially in lsm3 approach it's actually it's actually quite simple but at the same time it's also also kind of genius it's also in use by a lot of databases basically all the databases that you you know that are really good at ingesting a lot of data in in very little time they probably use either an ls m3 directly or something similar to to an lsm tree and the idea is in an osm tree that basically you start out with an in-memory structure so you you import your data and just put it into a memory structure and in this memory structure the only thing that you do when you add something is you make sure that that structure itself is sorted so whenever you have something sorted it's pretty easy to access it like i don't know if you have a sorted list then you can do a binary search and that's pretty efficient to access it so that's the only thing that needs to needs to happen in an uh in this memory store it needs to be sorted once this memory store grows too big you simply write it to disk as is you take this thing that's already sorted write it to disk one go so if writing in one go on onto your disk super fast if you have an ssd disk definitely but even on a spinning disc like just one seek right i don't know 10 megabytes of data pretty fast uh the problem with an in memory structure is that it's in memory so you don't have the the guarantee of having something on disk uh if it's never been written and if we're saying that the efficiency comes from the fact that you import into memory for let's say i don't know 15 seconds then there's also 15 seconds of data that could potentially be be lost if if the database crashes and i just said in the beginning like vb-8 is the the tool that really wants to be the database and that um yeah we can't just say okay oh well sorry you lost the last 15 seconds of um of your data that you wrote so we can fix that by writing into a write ahead lock and basically write a headlock or sometimes also called commit log is another file that we write into that's also append only so we only never seek in that file but basically just start writing into and just write at the end of it so basically this data that we ingest that was sorted in memory we're also writing it into a file where it's not sorted just sort of as a as a backup basically if something happens and then if if nothing happens we can just write the sorted part onto disk create this kind of segment and then start with a new segment and the cool thing is that basically even if you've just written hundreds of gigabytes or terabytes of data when you start with a new segment you basically start from scratch you start with a new file it's a new file that you're writing into so essentially this simply doesn't congest it doesn't slow down it has a sort of constant uh write speed and that is very important because that was exactly the issue that we had with the b plus tree before and good read performance uh but when writing at some point it would just uh um yeah it was basically it would basically slow down so if we look at the the road map that's what we're seeing so in in from 1.4 to 1.5 which is out now by the way as a release candidate you can you can start using it um by the time we we this this goes from the release candidate to the the full release there'll probably be some smaller changes we're going to add one or two features but those are unrelated to the ls entries which is things that we still want to get in the release um that we make um but from the sort of performance aspect you can start using this and and it should work well and if it doesn't then please do tell us that's why we have the release candidate so that we can make sure that if something goes wrong that we can fix it so this makes importing this data about 130 percent faster on on this particular data set of course these these numbers are very specific to one data set um but it's more about the sort of the relative comparison um yeah that made it 130 faster and this is sort of only from writing the data so the the expensive thing that we're doing is basically um it's basically building the agents w index um but as you could see sort of because we're writing so much data that was the bottleneck that we had to to take away so something else that i i want to say is right now uh i was talking about segments in an lsm tree and basically these segments i'm not sure if i even said that but these segments can also be merged over time and then you basically combine a lot of small segments into a larger segment and then you have sort of one sorted file which is again a bit more efficient to to um yeah to to use if you've heard of um for example the hsw implementation in open distro or if you've been following what's happening with leucine then you might not be saying oh segments not not good for a vector surgeon so let me let me explain why on the one hand we have the vector index which um has a o of log in order this is specific to hmsw which has an o of log n time complexity at query time so essentially what that means is bigger is better you want to have one large index and um with this sort of very primitive example calculation where i'm pretending that there's a thousand objects well if you have log of a thousand which is three times one then your cost is three and if you were to now split this intex in index into a hundred small parts that each just contain ten then log of 10 is 1 but you have to do 100 searches so now your total cost is 100. so basically it's a very very made up example with very made up numbers but to illustrate that really on on this kind of vector index it's much better to have one large one than multiple small ones in addition when when you merge something an hms w index unfortunately at least no one has really figured out how to do it efficiently yet you can't really combine multiple h and sw indexes into um into one big one i mean you can of course but is is as expensive as building them in the first place or even slightly more expensive because like the the new index is now going to be bigger than the individual ones so um basically for h and sw that's that's a problem and what we want to do is we want to avoid that our vector index gets segmented so this one we really want to have one one big one now on the ls entry that's on the other side of the screen it's exactly the opposite like we we just show uh showed that basically um these small sectors or these small segments are exactly what keeps that thing fresh and what keeps it fast at right speed so here we do have those segments and um while we have those segments there's a such a thing called a bloom filter which is really cool too it's a probabilistic sort of way of telling whether data is contained in a specific segment and the cool thing about bloom filters is that it never has false negatives it can have false positives so it might be the case that it tells you yes the data is in there and then you open this thing up and you look for it and was a false alarm data wasn't there but it can never have false negatives so you can never sort of misstate it that way and this bloom filter can be sort of used to make looking up data in very many small segments very efficient um and and then even sort of yeah over time you can even merge those segments into into smaller segments sorry into fewer but larger segments and then basically over time uh you have the same as if there were never any any segments in the first place and that's a relatively cheap process because as we said before like each segment is basically just a sorted file and if you take two sorted files and uh try to put them into sort of a next file which is again sorted that's basically like a like a merge sort so that's a relatively uh cheap and easy uh process so on the lsm side we're accepting the segmentation we're saying this is what we're not just accepting it we're really saying this is by design of how an ls tree works and we say that we can also just improve it async but if you look at the entire thing servant this is really what would tell the vvat storage uh apart from for example that the problem that lucine or open distro which is based on leucine are currently seeing is that we don't tie so while this stuff lives closely together and i'll get to that more when you talk about charting we never tie the vector index to a uh lsm3 segment that's very important because then we don't run into these kind of issues that suddenly you have a lot of yeah segmented vector indices that you need to somehow either combine or accept that they're slower so that was step two on the road map so the next step that we want to take is then um sharding so basically sharding or partitioning um i'm gonna explain what that is and i'm gonna start with the motivation of why we would even want to do this once you've defined that you want to do this there are a couple of questions like how do you do it how do you partition how do you once you have those partitions how do you distribute them among something basically like how do you sign ownership how do you give someone ownership of a partition and then once you've done all these these things um yeah what are the effects of those decisions like there are probably trade-offs involved and how does that affect real life data so the main motivation for partitioning data is basically we want to split something up and split it into smaller units that are basically the same as the larger unit but just only contain parts of the data so basically instead of having one database we would have sort of three smaller databases in a very very simplistic way and and one of the motivations for this is basically that if you have those three charts they since they're all self-contained they don't have to live on a single server anymore so you could for example put them on on multiple servers um the advantages of something basically of partitioning your data or sharding your data is yeah as i just said you can spread it among sort of not even it doesn't even have to be servers it might also just be that you have a machine that has a lot of cpu cores and that you're noticing that yeah okay i can i don't know use utilize 8 or maybe 16 cores efficiently but with 32 cores i can't anymore and then if someone for whatever reason if you have a machine with 128 cores then it might make sense to just charge your your workloads so that they can use those more efficiently but as we're sort of moving towards horizontal scalability the main motivation is probably going to be that you want to distribute those shards onto various servers and it could potentially even be on different data centers or something so basically as long as we decided that somehow we're partitioning the data we can then move it into different places however it does not come without disadvantages so the the first question basically that you have if you have let's say you have 100 charts and you want to access something some object with an id uh john doe one two three how do you know where that is like maybe there's rules somewhere maybe you have to test something i've also talked about plume filters before so potentially this is something that that could be used um to to sort of figure it out where the data could be lying but essentially we need some sort of a rule and the other thing is well right now in my example i was looking for a key with id uh john doe one two three but what if you don't know what you're looking for and we're a search engine so basically or vba is a search engine so basically we're dealing with a lot of cases where people don't know what they're looking for so these are the kind of problems with partition data that need to be solved so for partitioning in general i have this very very simple example which is just a modulo function and we're using modulo 2 and there's this nice sort of stream of numbers and some of them are already in buckets and basically what you can immediately see is like bucket zero contains the even numbers but one contains the odd numbers so basically my my hash function or my my partitioning function is a simple modulo two and then i'm just using the remainder i'm gonna put it with the remainder of zero i'm gonna put it into bucket zero and if the remainder is one i'm to put it into a bucket one there is one advantage or one one very big advantage on such a distribution function if i import something or if i'm looking for a key i can just use that exact same function and uh sort of immediately determine where i have to look so my stream here ends at 17 so let's say i'm looking for key 18 don't know if it exists or not but what i do know is that if i just use this key the next one and apply the module function since the remainder is going to be zero i know that i have to look in bucket zero if it's not there then um well then it just doesn't exist so this is this is one of the the advantages however there is also a problem and the problem is basically well what if i say now that instead of two buckets i would want to have three buckets so in this case the the easy thing to do would be well let's say let's just turn this modulo 2 into modulo 3 and all of a sudden the values that can come out could be 0 1 or 2. but the problem is that you don't start from scratch like you might have already imported data and now if we're looking at where the data goes um while zero modulo anything is always zero so that's going to stay in the same bucket but as the numbers go up so um for for one for example that also stays but for two so uh two modulo three is now two that's the new bucket so two would have to be moved even though it was sort of somewhere it needs to be moved um if we go to three that's in bucket one so that was a different bucket but modulo three is now uh zero so this also needs to be moved into a bucket that even existed already so basically what you're saying is if we have such a function and if we ever change how we partition it basically we need to change the entire database and that's a that's a problem because especially if we have such a thing that that's a problem for any distributed database but it's even more of a problem if we have something such an hw index which is costly to to build so we need a solution for that problem now before we get into that solution just in general a shard as i said is a self-contained sort of mini index it's a self-contained unit and um for us i said that we we have sort of this motivation to keep stuff that belongs together very close so if we have some sort of a function we determine um that something goes into chart two then this chart contains everything it contains the vector index the object the inverted index basically everything is in in that chart and all of these charts together form the index so to the user to the outside if they send a query and whether that query touches a single chart or all of the charts uh to the user it will just feel like one index like they won't know whether this was cert or maybe they will know because it's contained in the response um but the response will be the same it will be the same as if this was all one index and they don't care if it came from from yeah from one chart or from multiple shards and whether these were local charts or charts that were maybe on a different server so that's the general idea that once we have a sharded index that um yeah basically an index is made up of those of of basically what currently is an index so these all have the same capabilities of an index and then of course on a search we need to combine them but on a vector search for example we have the the distance which we translate to a certainty and that's something that we can use to to sort them so now for that problem that we have about changing the amount of of charts um basically that we saw with the with the um module function and here this is something that we're calling vba's virtual charts and this is very very much inspired by cassandra's virtual notes so the difference between cassandra and and vb8 is extender is not a search engine so it doesn't need sort of an index by spanning multiple data points which we do so we need something such as such as chart but we can still take some of the concepts that they do that basically help us find a sort of smart way of how to distribute data in the chart so what i have here in this this ring fashion is basically another hashing function that that i just made up that doesn't exist that produces values between 0 and 11 999 the idea why why this odd numbers is basically because it's a ring and um if we were to pretend this is a clock face then we would immediately know sort of where the three is and where the six is and then it just because i thought like only 12 numbers so that's a very bad hash function so i'm going to make it 12 000. so knowing this we can introduce something that we could would call a virtual chart and as of now this doesn't change anything about sort of what we do we don't now go out and produce i don't know 100 charts because as we learned in the in the part about h and sw we want as few large agents w uh index parts as we can so we can't just sort of make these virtual uh shards or we can make those virtual charts uh very small but we can't make the actual charts very small so these virtual charts are basically now sort of randomly distributed on this ring and each chart owns one segment of this ring and the segment is defined like the shard only has an upper limit and it it owns everything sort of from the previous virtual shards upper limit so if we take as an example the the sort of green one around the the 3000 mark here that very roughly that the previous one ends at like 2800 and the new one starts at 3200 something like that roughly so why do we do all of this um basically the the cool idea is now if we were to introduce one new virtual node so once we would add a sorry one new virtual chart if we were to add a new shard then we'd probably add much more than one but let's say for the sake of argument that we would uh add one new chart uh virtual chart and this virtual chart has the sort of upper limit of exactly three thousand then what you would see is that it would basically split this green one here in half and why does that matter this is sort of as we see how we distribute these virtual charts into actual chart so a chart as we said we need to have relatively few shards because we want to have large indexes on there um that means a chart needs to sort of own virtual charts which essentially just tells us that if the sharding function tells us you would go into virtual chart i don't know something then we know basically from a lookup table okay this is the actual chart in this case one and just for the sake of simplicity i've just decided that charts basically own a specific color so shard one would own the color blue chart two would on the color green and and we can go on and then if for for whatever reason we introduce new uh virtual chart we can cut them in half and basically now is sort of now is the major benefit if we look at those buckets with a modular function like even if we add or even though we added new buckets nothing has to be moved across buckets that already existed like yes if we sort of if we go from two to three and we say we wanna populate three yes three will have to take something away from one and we'll have to take something away from two at some point there's an equilibrium and it will stop but it can never be the case that we have to move something from one to two or from two to one so basically a lot less movement has to happen if we ever change these these things and now you might be thinking sort of before i said well one of the problems of an hnsw index is that you can't really combine it or that you can't sort of merge one into two so now i'm talking about potentially changing the shards won't we have that exact same problem and yes we do uh but there's there's two things that to keep in mind here so first of all this is a much more long lift sort of process that is much more rare if you import into a segment you will import for a minute or import for a few minutes and you will already have i don't know probably 10 segments or something like that so that's a very very common thing where this is a problem changing the charts or changing the clusters so most likely the reason why you would change the number of shards is because you've changed the cluster size that's a much rarer occurrence and if it is then it's also much more acceptable to maybe have a background process there to to readjust so this is this is a pretty rare process where the cost of sort of rebuilding an index might be more acceptable that's the one reason to justify this the other one is that hmsw is basically just the one index that we currently have in h in in v8 but we don't know what the future will bring maybe there's a new index at some point where we say oh wow the the cost to build this is just a tenth of that of hmsw making these sort of rebalancing acts uh much more feasible so this is sort of a very core decision in in the design and we want to be prepared to sort of not be not have the restriction of never being able to to change hearts and there are some some databases out there where basically you can only select the the number of shards up front and then you can never change it and we want to sort of be more the cassandra in this in this case where we can say yes um we are prepared to to scale dynamically and we are prepared to sort of do things that maybe you couldn't predict already uh when you initially uh set up your your cluster so when you partition like that so so basically now we've talked about what happens or how you partition um but also another question is what do you partition by so basically you need to take something to make that decision and in in two slides back on that or three slides back on that modulo function we were just using numbers and numbers well i don't know we could say that this was the id or this was the only property um but basically you just need to make that decision of what do you use to partition and what we have in our plans and i'm saying sort of this is the the first version because i think there's there's definitely potential to also extend that in the future uh at first what we want to go by is just the object id so everything in vb8 needs to have every object needs to have an id and um using that id uh yeah we can sort of easily identify we can easily identify that's the point of an id but we can access each object by an id and if the id is also the partitioning key we basically know that on each on each axis we have the partitioning key with us so for example why do you want to access something by id so on a search it doesn't matter so much because the search we said that each chart is self-contained so if we uh sort of combine searches from different charts then it doesn't matter so much because like the shard will already give us the whole object so we just need to combine the search but there is a scenario where we need to do those lookups by id and this is vba's cross-reference feature so since we can't control if a cross-reference ends up on the same node it could potentially be but it could also not be the case we need to sort of when we resolve a cross-reference we need to quickly grab that data and in this case it's much much more efficient if we know if we can do the partitioning calculation in front and if it will tell us okay this particular id lives in this virtual chart which belongs to this actual chart which as we'll get there in the next step belongs to maybe this node and that's much much nicer the disadvantage is that if you have fuzzy information um you will probably touch multiple shards but that that's okay also if we if we look at replication in a second uh that will basically show that um yeah that there are ways around this uh one of the ways of how we could potentially improve this in the future is if we just give that decision of how to make the partitioning if we just give that to you the user because only you really know what you're using vba for and let me give you an example so let's say um we're coming back here this is sort of how this all ties together if we're coming back to vva's feature of using a um a structured filter then you could potentially make the key that you're filtering on or the property that you're filtering on you could make that your partitioning key so for example a made-up example in e-commerce use case and we're saying we're partitioning by a field that is the average shopping cart total so i don't know these are these are past shopping carts and um the total is is what we're positioning by and we're not using now a hashing function such as murmur 3 which by the way is also inspired by cassandra i think it didn't even didn't even mention that but um it works well there so why not also use it in in our case um and but we're using more of a range function where we're saying like if this is a low value it goes into partition one if it's a mid value it goes into partition two if it's high volume it goes into partition three what's low mid high doesn't matter for now so now if we know that our queries will always set this kind of filter we can basically triple the the search capability of our search of our search engine because if we're saying now give me the users that are similar to x and that's the the vector search part that have a high average shopping cart total which would be expressed in the where filters such as i don't know where shopping cart total uh equals more than whatever the threshold is for high then potentially you could be in a situation where this entire query is served by a single partition and that means that if your data is partitioned let's say across 10 partitions which are on living on 10 nodes then you could potentially sort of uh yeah sort of up your your intake or your throughput by 10 because each of those nodes can conserve those queries individually so in the partitioning strategy there there's a lot of potential in in sort of really high usage cases so i think it would be really cool if we could open this up in the future and just sort of let the user define how you want to partition but for now just to get started we just want to go with the the object id so that we have something to sort of evenly distribute the data the major motivation of that um is really clear than in the in the next step so so by the way step three is now under development so step one and two is basically released either as a full release or a pre-release a step three is currently under development and therefore also the next steps so now now that we have those charts um this is probably the part that we've been waiting for all the time is step three like why do i have shards if i can't do anything with them now that we have them in step four we want to distribute them across notes and this will finally get us to the point where all of that load can be spread across multiple nodes in a cluster so for example um if you have a data set that you say like yeah with one ev8 node it just i know just it's too large for for what we can um yeah for for what it could handle or maybe it's not too large for what it could handle but maybe it's too large for for how fast we could import it so there the benefit really is that in this case you can now um once we we are at step four you can distribute that among multiple nodes in your in your cluster and cluster being the the key word here like this is the first time that we can really talk about a vb8 cluster cluster of sort of yeah where we have horizontal scalability and and also where we see that horizontal scalability itself does not necessarily mean high availability so yes at this point the cluster is distributed uh but it's not necessarily fault tolerant yet or at least not a hundred percent because replication is still missing at this point so if out of those three servers one were to die um then we could still serve two thirds of the data set which is sort of i don't know like it's better than nothing but it is not what we want of course we want to be able to serve the entire data set which is then what we will get in step five so so by the way between step three and four this is sort of i only have one slide only saying now we distribute but once we implement this uh there is a lot more things that we need to do there's all the stuff in the background like we need to make sure that a schema changes for example that they are fully consistent whereas other stuff might not have the same consistency requirements so basically this is large enough from an implementation perspective to give it its own step on the roadmap even um yeah if if sort of i talked a lot about step four and then very little about uh oh sorry i talked a lot about step three but very little about step four the next step then and i think this is this is really this is where it starts to get get really cool and really exciting the next thing is then once we have a replication and the way that we want to do so it's a replication in general just means as you can also see in this this graphic so if you compare it to the previous one each node owned like one color the color of a chart here uh and then in the next step each node now so here we would have a replication factor of three each node now owns basically a copy of all three charts so in this setup of just three nodes two nodes could die and you would still be able to serve the entire data set like maybe not at the same throughput because you're you're missing two machines but you can still serve requests and and that's basically the the major benefit of having replication and this is also the point where we can say okay now it's highly available something can happen a server can go down and we can still be able to serve the data set and um as always with these things the replication factor is something that you can control you could potentially go to to some extreme measures where you have maybe 10 nodes and every node contains a copy of every chart then you would have a super highly available setup like at this point it would probably be more more realistic than i don't know you just have an outage of the entire data center uh then that all 10 nodes go down individually so this is sort of the decisions that you can make basically what i'm saying is we as as developers in vva and want to give you that kind of control and just sort of design the system and you can use it um according to your your budget requirements according to your slas according to your your availability requirements um there is also yes also i haven't talked about the fact yet that the way that we're planning uh the replication thing is completely leaderless so basically this means that there is no sort of write-only notes and read-only notes but all notes or all shards that are contained on several notes are completely equal in that case and um that means there's there's no bottlenecks but there's also a potential for something uh that we've tried so far like in the very beginning actually we did an experiment with a distributed uh server or building up an hmsw vector index in a distributed fashion um which works surprisingly well like it needs needs more experimentation but there is a way of sort of spreading that cost out of building the index by spreading that out across multiple servers without then even ending up into multiple charts so the idea is that you basically just have a single chart that is replicated across servers but still that those servers share the cost of building the index so this is a bit experimental but i think there's a lot of a lot of potential but even even without that that idea that the benefit of replication i think is is pretty clear which is uh basically high availability to single or note failures uh with regards to consistency uh the plan is to just have this eventually consistent as we're seeing sort of all of our use cases tend to be in the either in the analytical space or in sort of um yeah these typical search cases where you replicate your data from another data store so very eventual consistency is absolutely fine if we ever see that we need more than eventual consistency uh we could very easily or not easily but we could definitely again sort of copy from cassandra or be inspired by cassandra where they have a model of tunable consistency um where the idea is that you can sort of control the cost of writing data where you could say uh i don't know i for example write with a a quorum of nodes and then i also need to read for a form of nodes but then it's consistent or you could say that i don't know i need a replication of at least x y or c for our right but for now that's not really the plan it's something that that sort of our architecture allows us to do if we see the need for it but for now based on the cases that we're seeing um we're not spending any resources on this um so still in in in the idea of replication as i said before we really want to give you the control of how you set up your your cluster and i just came up with two examples here one is scenario one um let's say that you have sort of a workload where you import your data maybe it's i don't know trained with a specific model and you know that you retrain this every 30 days so basically your typical workload is like import once and then you you start querying which is something that we sometimes see but of course also you get is database and you can mute hit it so it's in no way a restriction uh what you can do then is um you could say okay i want to have one chart per node at import time import my data set and then once the import is done i want to turn up my replication so that let's say we have three nodes that we end up with the picture on the right here where each node is able to handle um an entire request based on data that it has locally and the benefit of that besides high availability is that you have this this massive throughput at query time because each node can serve a search query across all shards um in in isolation without needing a network request and without talking to other nodes scenario two is something differently where you would say okay we have a lot of writes and we have a lot of reads happening simultaneously and there's not an import phase in a query phase and in this case you could just say okay i want this highly available from the beginning i want replication from the beginning and then you'd have something that is highly available at any time so basically as soon as you yeah your cluster is live it will be highly available whereas in the first one is a bit cheaper to build and a bit faster maybe but then only it becomes highly available once you you start turning on the replication the final step on the roadmap is then dynamic scaling so this is sort of the holy grail or maybe the holy grail is multi-data center which is also something that that is possible with this architecture in general um but sort of the next part here is dynamic scaling where the idea is that uh you can change the cluster size based on the demand that you're seeing at runtime and this is also where we sort of come back where we then see the the benefit of this ring thingy here with the virtual chart that if you dynamically change your cluster and you can't predict how you're going to change it then you might also have these these sort of these situations where you need to rebalance and potentially sort of combine shards or split charts up and this is then where the virtual charts will help in sort of minimizing the movement because yes sort of changing something about an hsw index and potentially other vector index types in the future is expensive but if we can at least minimize the the changes that have to be done and then that becomes a lot more efficient and that's sort of the the uh yeah final part on that six step road map where we've seen that one and two um are complete and released three is in in under development and then the other steps we're going to follow so that is basically the overview i've been talking for for quite some time let me know if you have any questions so that's uh thanks session that's a lot to process so i'm curious to hear if there are any any questions also there as well yeah exactly that was also one of the the motivations of saying we need to we need to record this because there is a lot of a lot of stuff crammed into this it was almost an hour wow but let me try to figure out how i can exit views that i can also see you there we go and i think i need to stop presenting yeah so uh so this this was great i think it it gives us an idea of what can be done i think we're still in experimentation mode so uh some of the some of the practical considerations uh that we're having when we when we're trying to implement use cases was so so the first one is okay when we didn't initially look at it as a data store as the database right so persistent storage wasn't at the top of our mind right like okay and and uh it was more response times for your uh for your for your reads right right like so the right wasn't such a big factor but it was still a big problem if you're doing the one-off migration right so i wanted to get my data from wherever my current person's storage is into into vv8 right and and that was a that was a big uh that was a bottleneck right like you know the speed with which you could do that one-off migration right so we were looking at that uh uh alone rightly and for me right like once i had done that then the additional so and then right when i'm adding objects directly to the uh to the instance it's not a problem because it's not you know in future right like you know that load might increase and then it might become a problem but you don't you don't see that when we are when you're using it currently right so that's that's that's one thing the second is the the availability is one thing but also right like in a recovery you know rp rpo right like you know sort of what happens if i lose some data have i got backup how do i recover right like you know if i'm using this as my database right so that one that was a that was a that was something at the back of my mind to say right like you know to prevent me from okay can i use it as a production database right so for me rightly can i say okay my uh you know um what i had in mind was use something like cassandra neo4j for my persistent storage uh but you but you realize that right like you know that's not necessary if you're using vv8 as the database right uh so that's that's one thing and i think depending on the use case the characteristics will differ right like you know we can only we can probably like you know sort of experience that only when we try it and see what happens but that's when right like you know the we need that flexibility right do you need more sharding so obviously i want uh horizontal scalability but uh my uh i might just write like you can have a one-off big migration load and then write like you know sort of my my ongoing right like you know sort of load for adding objects might not be that high right so i might need a different solution to to get my data on one time into into uh to migrate it onto the v8 instance and then ongoing that that wouldn't be a problem so uh the the flexibility right looking at that that's coming will be will be great um and and and of course right like you know sort of the um availability for both of those right-clicking and i think it's it it's for us to see right look at how we can tweak it whether we want faster response times right or right like you know sort of as you are if you've got very less road and you want to write like you know sort of configure it pretty heavily for faster response times that's that that flexibility right like it will be great i said sorry right click and i think i've taken too much of the time because no no it's very cool to to hear sort of how how all this abstract stuff how this this ties into actual use cases and especially sort of two takeaways for me is is one sort of the um it might not be clear to everyone that vvat really is a database and that that there might be something else that you need or maybe yeah sort of in an experimental mode it might also make sense to to sort of start storing the data or to still have the data somewhere else or until you figure it out how you want to use vvat and then you can throw away instances and not not worry so much but in the end sort of in that production scenario it's really cool that the data can be be safe there and the second one yeah i'm sort of this this uh import heavy load this is also something that i've thought about for cases where once we have the really dynamic scalability you could produce a ridiculously large cluster and and just sort of run it only for maybe a day and and like let's say uh if you have a machine that that's 30 times as much as the machine that you end up in in the end but you run it for just one day then it costs you basically one month of serving the data in production and this kind of flexibility i think this is going to be going to be really cool yeah question about that as well uh so do i understand correctly that you're basically saying that your assumption was that you could use with it just for the search functionality but not as a persistent data store as well do i understand that correctly yeah so i i wasn't um you know i i wasn't seeing that um you know basically like you know the horizontal scalability that you get and and the the flexibility of using distributed storage right like you know so so typically like you know sort of you what the the way we do is we we try so we we plan separately for compute and storage right like you know sort of in any cloud deployments and right-clicking on that storage itself is a big cost that we're right like you know sort of there's a there's a whole uh capacity management archival blah blah blah right like you know sort of all of that and rpo objectives connected to that right the backup recovery all that connected to that uh whereas right like you know availability is something different right like you know it's fine if i it's one thing if right like you know if i if if the site goes down right click you know for some time and then it comes up right like instead of it's it's fine that might be okay from an sla point of view but if i lose data if something gets corrupted right like you know then my rpo so rto and rpo are the two holy grails of enterprise sls right so and and we treat each one of that differently so i was fine with the uh rto stuff and and it it was fine right like another of the the kind of use cases that i'm i'm guessing um might not write like a make a business impact if the site is down for a read for for a certain period of time right but if i'm going to use it as a persistent storage i don't want to get to i want to make sure because this is my where my master data is stored right so which is why right like you know sort of till now in at least in the experimentation it was always okay i can i can spin up a cluster migrate the data right uh check out the semantic search and do all that but then right click it i'm fine if that goes down i import it again right or and for now right like once i import it uh the import is pretty painful just now because of the of the time and because of the amount of data for the one-time input but once it's on the vv8 instance then it is just right-clicking on sort of those one-off objects that i add right so typically like you know sort of what i'll do is i'll i'll set up some messaging system some event based something that right like you know that says okay fine yeah every time a user gets added just go and put it into right like you know sort of and create that object there so that's not a big problem till and we've not faced that load yet but that becomes a problem but that's not a big problem just now right but if i am to say right like i'll i'll need to trust vv8 with my production data and that's the data store and all my backup recovery rpo will be tied into that then that you know it's so whatever you presented today gives that confidence but it wasn't there before nice nice thank you yeah that was definitely one of the goals to just sort of convey that we're we're taking these kind of things very very seriously and of course something that that a decision that i can't make for for people that use vba is always like do you use vb8 as your only database or is there a case where you have something like we're not saying that just because vv8 is a database that you should never use another database it really needs to fit the the use case so for example i don't know if you have uh very transactional data then it it probably makes more sense if you have a transactional data store and then replicate into vba so very similar as you would today maybe see in in sort of a hybrid setup of the cassandra and then using maybe an elastic search or something so that's also very much possible with with vv8 and something that just needs to be done decided on it on a case by case basis correct yeah yeah i think yeah makes sense and and probably like you know sort of in that case we just need to think of right like you know sort of how we replicate and you know if i've built my knowledge graph for my enterprise on vv8 and i don't want to lose and it's the same amount of data somewhere else then yeah yeah and of course also backups play play roles or besides that the sort of live replication you can also just do do sort of old school backups exactly which we've by the way just implemented in the uh in the wcs so in the vba cloud service if you use the the hosted uh vb8 service okay i think from the standard tier on uh everything they're just sort of snapshots that are that are currently replaced and you can restore snapshots and it it's a sort of very simple backup system for now um the idea is that in the future we could have incremental backups and all that that kind of stuff um but just in general it's also a nice proof of concept of just having yeah how do you do backups with with vba and of course not just how do you backups but how good is the backup if you never train the restore process but sort of do the the whole the whole intune thing um yeah which we we have there as well and i think our documentation is maybe not up to speed yet with regards to to what to do on on backups and library stores but uh yeah it's a good point and we'll definitely also look into it okay thank you okay thanks a lot for for your feedback was really really cool also it's it's it's always so cool to just get that kind of connection just see vva being used and and see what the pain points are see what the the the cool things are and just i don't know just in general to hear that vivian of course it's being used but like to put a face to to a user is very very cool so thank you very much for for sharing as well cool thank you i think it's like because we went like almost 10 10 minutes over so um there's a few more things hm may be nice to mention because i think you haven't mentioned any uh expected or estimated timelines have you just just ballpark timelines yeah yeah yeah i thought i thought i could get away with it no so we so we do have a a sort of um one date on the timeline that that i think that's also on the website where we said the the end of q3 so basically by the end of september um is where we sort of yeah aim for for this this timeline which definitely means um yeah it needs to be horizontally scalable and i'm pretty sure it also it will also have replication in it um i don't know if dynamic scaling might be in that so so maybe not but sort of the the target point that we really say like is by the end of september we want to be at the i think it was step five out of six where where uh replication is in and then see where it goes maybe also on dynamic scaling so this is pretty pretty soon uh already which i like from a from a feature perspective which every time i check the calendar thing from an implementation perspective well where's the time gone but i mean we're we're not starting from scratch where we've already just released step two of that six step pipeline cool thank you so much and also thank you for your question um i think we're gonna wrap up and so we'll uh publish this also on on youtube because uh you'll see it on our slack channel etc and on twitter people asking for this so they can they have enough material instead of watching a movie they can watch this the architecture thank you so much everybody and um well i hope yeah thanks for for joining also for those that have already left there's a couple of people that left like right right at the one hour mark sorry for using up all the time and thanks for staying ", "type": "Video", "name": "Weaviate Meetup June \u2013\u00a0Architecture Deep Dive, how to build a vector database", "path": "", "link": "https://www.youtube.com/watch?v=6hdEJdHWXRE", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}