{"text": "Weaviate 1.17!! This is a massive release for Weaviate, debuting Replication, Hybrid Search, BM25, Faster Startup and Import ... \nhey everyone I'm super excited to host Eddie and dylocker and Parker Duckworth for the weevier 1.17 release podcast these releases are always so great it feels like such a celebration of weeviate and the hard work of the team to bring these new features uh into weviate uh so today we're mainly talking about replication and hybrid search and uh we're also welcoming uh Parker Duckworth for the first time on the weba podcast so we'll talk about ref devec as well a little bit uh so firstly Parker thank you so much for joining the wevia podcast yeah happy to be here awesome I think this release is just so special because uh you know we all got together in Italy and these having everyone in the semi-technologies team in the same room as our up on the Whiteboard at the slides and presenting these new features edian can you tell us about your experience with that yeah it's absolutely crazy so so we're a completely remote company but being remote doesn't mean that you can't meet up occasionally and this is exactly what we did so we had uh 28 I think uh or 27 or 28 folks in in Italy in the same room and uh we had a demo session planned for the last day of the week to to show our progress and I think by that point it had been about a week that I had last synced up with with Parker and red one who were working on the replication and and I was just sitting there and enjoying the demo and I almost get a bit emotional looking at the kind of progress that we've made because I mean like building a distributed database is the first for me it's probably a first for most people to do that and um seeing that come together and sort of seeing the the seed that that we originally planted when it was a super tiny team and see the the team grow and and come this feature come together and it I mean it's like it's such a such a serious feature in the end basically like when we started I would think like okay once we have replication that is really then we're in the big league then we have these these high availability and failover scenarios these kind of things and yeah seeing that happen absolutely unique experience can absolutely recommend it go to Italy with your company yeah it's just an absolutely incredible experience and and yeah the end at the last day uh Parker and red one gave this incredible lecture about a replication so uh Parker could kind of take it away and uh tell us about replication yeah absolutely so uh when I first started at the company we had um a milestone it was like a the last Milestone of the road map which was replication and it seemed so far away uh such a Monumental task at the time right we didn't really have anything um to to support that at the time that I joined so you know over time we've built towards replication um for example starting with backups uh first we introduced the ability to back up whatever's in your weviate instance um maybe on a single node and then that evolved into distributed backups which allows you to like backup a cluster and all of this was working towards the ability to automatically replicate or to support replication and finally we were able to build on uh the back of all the work that we had done with backups and introduce replication so it's something that we had in mind the whole time throughout the planning and development of backups we knew that we were building towards replication and so we wanted to yeah just build it up incrementally until we we got to this point so uh the really fun and interesting thing is that really the Capstone of the replication work uh I guess you could say was done in Italy so up to the point um up to that point you know we I hadn't met anyone in the team in person you know I'm based in the US and uh the rest of the core team is based um in Europe and other other places and so uh getting to sit uh specifically next to a red one another core team member and work with him in person to finalize this replication um that we wanted to build was super super uh interesting and and um they just made the whole experience so great so the the way that we decided to implement um replication we first looked at you know the cap theorem like what trade-offs do we want to make here you know do we want to prioritize consistency or availability or partition tolerance and so after discussing um many times with the core team and and um yeah red one and I discussing for for a while we um decided to make the trade-off for partition tolerance and availability uh similar to Cassandra um the thing with with weeviate is that um it's super read heavy so oftentimes the use case uh will be where um we'll insert like a large amount of of data up front and maybe there will be more inserts in the future but we want to prioritize uh read um the availability um so that being said um we decided to follow a uh leaderless um yeah replication algorithm so the idea is that a request will come into a cluster of Eva nodes and the node which happens to receive the requests will be um uh promoted I I guess you could say as the coordinator for that request and so um this coordinator uh also considers itself one of the participant nodes or one of the um yeah other nodes that it needs to relay like this this replicated data to so um the coordinator will uh participate in like a two-phase commit with the rest of the nodes including itself so a request comes in let's say a write request for um yeah a piece of data and then the coordinator receives the request and sends out um a broadcast basically like asking every node that is part of the replication replica set I guess you could say um to acknowledge the request has come in uh once that acknowledge one else comes comes back then um yeah the coordinator will actually send out the rest uh of the notes the data that it needs to actually commit or write to disk so um the advantage to going with this uh leaderless um algorithm is that it's more flexible in the case of node failure we don't have like a single point of failure with like a leader follower um algorithm so um yeah just uh makes things more flexible in the event of like node outages and things like that yeah that's so interesting to hear about I I love learning about these new database features in webia for example when the backups came out I had so much use out of that with my research on search features because as we're building out this beer benchmarks and we're going to talk about this later with hybrid search and our evaluation miracle and how we're evaluating this these bench these uh backups sorry have been so useful in I can upload NF Corpus arguana sci-fi all these beer data sets and then just back it up restore it run the tests and then you know we know our hybrid surge performance or whatever we're evaluating them so can you help me understand further uh the use cases of replication um when when people are going to need to use it yeah so the the biggest one is for reliability so typically with replication you want some kind of redundancy so for example if a note goes down and this is a Parker already said uh that from the perspective of the cap theorem where um prioritizing availability and partition tolerance and partition tolerance in a distributed cluster is basically a given so partition tolerance of this means in in this case means a connection to another node could go down or the node itself could go down and like from the perspective of a node the other node is down doesn't matter if it's actually down or if it just can't reach it so kind of a partition tolerance is a given and um then availability would mean that use cases can still continue and the cool thing with our application is that the consistency level that the user wants is tunable so um some of that is going to be part of 1.17 some of it is only going to be part of 1.18 so we're rolling this out in in phases basically but in phases that we believe make sense so it's not that one net 1.17 is like a half finished feature and then it's only complete in one not 18 but basically what you get in 1.17 is is complete and usable and then you get new features that still fall under the umbrella of of replication in 1.18. um so yeah with tunable consistency you can basically say um how much priority do I give to the availability of reads versus uh right so with 1.17 and every ride is replicated to all nodes that participate in that particular class so in in a sort of consistency level terms that's the replication level all and then every read or request that comes in um at least for for searches uh it is the with done with the consistency level of one so in other words to write data into the cluster all nodes need to be available but to read data any node could go down basically as long as you still have one and that's that's a specific configuration so in this kind of setup this would work well for um for example for uh search on an e-commerce application so you would say like hey products are updated only once a day so we only need to be able to write data once every 24 hours but users need to be able to search 24 hours and if something goes down they still need to be able to search so that will be a great use case for for the kind of uh read with uh sorry write with high consistency and read flow consistency kind of cases but there are others you could for example say if I ride with a quorum and read with a quorum then um you could tolerate node failures on on both cases or you could say I both write and read with very little a consistency or minimal consistency basically and then you could tolerate a lot of node failures but also you could risk that data is out of sync so basically you have this eventual consistency kind of aspect where you say okay for my use case I can tolerate it if some data is not there at some point but it needs to be there later on so this is the the high availability kind of use case which from my perspective is the the most requested reason or the most the biggest motivator in in replication but there are others so for example what you can also do is you can use replication to scale your read throughput so again to stick with e-commerce uh Black Friday let's say the kind of traffic that you expect on on Black Friday is five times the kind of traffic that you would expect during during regular day or regular week you don't want to provision your cluster for that Peak load that you have one day of the year you want to have your cluster sized appropriately for the rest of the the year and then scale it up basically just for for that one or two or three day period And this is some things you can do with replication so you could say my replication factor is three for most of the year so I have some redundancy but maybe now I scaled up to five or or 8 or 10 or 15 because then I have 15 identical replicas that could serve the traffic and basically this this gets linearly so 15 nodes could serve five times the traffic that three nodes could serve and finally there's another one and this is sort of more on the roadmap but that's the the multi-data center kind of replication so for regional proximity so you could have two places on on Earth so maybe I think we've used that before in agent SW graph example so let's stick with that we have Frankfurt Germany and um I don't know Boston uh USA so um if you had a data center somewhere in the middle it's kind of a bad example right now because we're in the middle of the Atlantic but that's okay let's let's assume there was Data Center in the Atlantic um then the kind of latencies that users would get would be pretty much the same for the for the users in Boston and for the users in Frankfurt because they have the same sort of regional distance to the data center but no one has a real good latency because no one is close to the data center so we could say let's move the data center to Boston super for the Boston users not so great for the Frankfurt users but now with multi-dc replication which you could also do is you could have a data center in Boston you can have a data Center in in Frankfurt and each of those users contacts the data center that's close to them so they have good latencies but then of course the data centers need to stay in sync as well and this is basically the multi-data center replication this is something that's not yet present in 1.17 also not going to be part of 1.18 yet but it's something that the kind of architecture that we've chosen in vv8 that is supported by the architecture so this is something that if you want to have it I think we have a feature request ticket for it on GitHub already so upload it and then we'll build it yeah I love how you gave the example of kind of the um rewrite trade-offs with consistency levels when when you want to use each thing when I think that's always a super important thing to understand yeah the the distributed systems it's so interesting to learn about it reminds me of our podcast people are you know for a binge on wevia podcast we did another podcast with Eric Bernice and on a running Vector search in production and it reminds me of that topic of yeah what it takes to you know scale out your e-commerce store so you can handle Black Friday I think in general it's so interesting um so also quickly I want to touch on this um this iterative release and Eddie and you've recently written a very popular blog post on product engineering I think now would be a great time to kind of touch on product engineering and you're thinking around these iterative releases and the general kind of philosophy behind how you think about these things yeah so so uh for those of you who haven't heard of product engineering yet it's kind of the the merger of product decision and Engineering decision so it's kind of blurring the lines between traditionally you might have a product department or a product team and then you have the engineering department and these these parties hate each other and they don't talk to each other and don't collaborate um and the in product engineering the ideas that you you soften these boundaries and you have collaboration so in a startup to me this makes a lot of sense because you have small teams and if you have a team with like three developers you can't afford to have a dedicated product manager and maybe a dedicated lead developer and then who's going to do the work basically they have like a a manager to engineer ratio that just doesn't make sense so something that naturally involves in those kind of settings is that yeah you blur the lines and you maybe have an engineer who takes over a couple of product responsibilities or maybe you have a product manager who has an engineering background and can do the the kind of prototyping themselves and um yeah you have easier communication more collaboration and have something that I would say yeah sort of is is is more productive and and feels more natural and you don't have these kind of artificial boundaries between the two um for our replication release um I mentioned that before already we're really really interested in the feedback that we're getting and we're really trying to to provide value early but value but really value like not just sort of give you something half finished so like hey we went for a minimal uh implementation so that you could have it sooner but don't use it because it's not really meant for production and that's kind of not what we're trying to do but instead return to say like okay what is a combination of features that you would use or what you would need to use it in in production and then maybe this only works for 80 of use cases maybe it doesn't work for the remaining 20 yet that's fine because the remaining 20 they'll be covered later on but that's no reason for us to say to the the first 80 hey you're gonna have to wait because we're only going to release it when we once we have 100 covered so that's kind of the idea um of doing that in iterations and of course we get the feedback when something is out we well yeah it gives us the ability to still pivot and for this to to sort of tie this back into product engineering it's super important to have that that feedback cycle and not have like artificial steps in between where someone has to relay a message from like one Department to another department then in the end you have Engineers that never talk to users um but it's the complete opposite basically like everyone in in uh the UVA core team or in other teams can be a member of our or not can be a member but typically is a member of our public slacks so they communicate with Engineers right away and then if if new ideas pop up through that we we discuss the ideas internally and it's not like well no the product manager said we're not going to do that but it's more way more collaborate yeah incredible and I think this is a great transition to start talking about hybrid search and our philosophy and the overall how we've developed it and so on and so maybe let me set the stage by describing what hybrid search is so uh hybrid search describes combining keyword scoring methods with Vector search methods so let's I think we're all pretty familiar with the vector surge part that's where we encode data with machine learning models build up a vector index and search for the approximate nearest neighbors and that but now let's kind of focused a little more on the keyword scoring methods so sort of the foundational algorithm is tfidf term frequency inverse document frequency where you score some sentence like I'm super excited to welcome Ed Ian and Parker Duckworth to the podcast based on the kind of uniqueness of these terms in that query with how unique it is to the collection of documents that I have so then going from TF IDF to bm25 bm25 introduces this binary Independence model it's you don't count how many times the keyword is going to appear in the document just whether it appears or not you similarly normalize it for the length of the the document so it's a bit of a modification to tfidf and it's another way of scoring these documents based on the keywords that has been really successful so then we have these two uh search algorithms and so now with hybrid we're combining the the results from each of the lists so we're going to dive a little further into the rank fusion and then also say bm25 and the extension to bm25f but I want to come back to this um product engineering and Eddie and I want to ask about I thought with this project you did such a great job of leading the team this was one of the projects that I've been a part of since joining we V8 where there's been like it's like a task force almost like this is our project this is your responsibility this is your responsibility and then we've just kind of come together and it's almost finished and it's so exciting so can you tell me about like your initial thinking around the development of the hybrid search Project yeah that's that's great to hear by the way that's that's really nice yeah so so uh for our listeners to to understand a bit sort of how we structure that internally um we have the core team itself which basically builds vv8 which is kind of a lot of what we do but by far not the the only thing that we do and then uh Conor is part of the the research team as well so besides um like the podcast and and other sort of several activities there's also the the research part and what we consider research and research that the term research depending on what your background is this can have very different definitions or it can be have very different meanings um but we use research in the sense that we say we've identified an opportunity somewhere something that we will most likely want to add to alleviate but there is some kind of a question that we need to answer first and this question could be something as simple as what is the best ux to to integrate this into our apis like how would our users want to use it like do we want to give the user a lot of control or do we want to maybe abstract something so this this could be a question it could be a question of how are we going to build it so especially in in um so you mentioned uh rank fusion and then the scorebase fusion and these these terms this is basically something that that you know way better than I do and something where we benefit so much from from um yeah having these kind of kind of collaborations within the company um yeah so this sort of how do we build it what do we need what do we need to figure out how to to be able to build it could be an evaluation also something like does this idea make sense like it looks good on paper but what happens if we try it at scale let's try it with 10K objects a million objects maybe a billion objects thus it does it scale does it fit into into deviate in that sense and this is something where hybrid I think early on WE identified that there's an opportunity and um said like okay let's let's get started let's see what it is let's see um what do we need because hybrid sort of in a sense you need the both the building blocks for hybrid both The bm25 Surge and the the um the vector surge you need to have both Vector search obviously is kind of what bv8 is about so we can safely assume that we have Vector search covered bm25 is something that we gradually started building um it was actually TF IDF in in sort of sort of the the the simple building block for bm25 but I think from the indexing perspective it's actually the same or it's like one or two parameters need to additionally be indexed for bm25 that is something that we actually had in mind in the very very first prototype that we built so we we didn't have any apis for for TF IDF or bm25 but we had the inverted index early on I think over over two years ago we added the inverted index to to bb8 and it already had this this and Parker you may have come across that in the in the code whenever we put those buckets we had like buckets for with frequency and without frequency and the word frequency is so basically for all the text properties we We additionally uh to to indexing the word we also index the frequency and that was in preparation for that whole TF IDF pm25 step so we kind of knew that it might be something that we want to add at some point um but we also have to figure out like what is what is the real value of it and um if I'm a hundred percent honest something think that that I don't know at this moment is Will hybrid search play role five or ten years from now it could easily be the case and then I I don't think anyone can confidently end could be the case that semantic search just keeps improving so much that hybrid search basically is more of a stopgap solution at the moment to bridge a gap and in the Gap being exact keyword match in and out of domain search or it could be that while it still improves hybrid search is just always going to be better because it's the the combination of two things and this is something that that yeah I I don't know and I don't think anyone really knows but something that I find super exciting and yeah quote quote me on this five years from now and let's see let's see how it turned out there seems to have been quite a bit of Buzz about hybrid search in the community as well I think in our our slack Community I often see people requesting this feature or asking when it's going to be available or being excited when they hear that it's going to be available soon I think uh yeah it's always awesome to see people asking for things and then have it delivered that's so cool and I think um so in preparation for weeviate air Eric and I were coming up with a demo and I I think this example of how to catch an Alaskan pollock that query is a great example where you have the semantic meaning of catch you don't mean catch a baseball catch a cold you mean you know fishing and then Alaskan pollock which is a specific keyword and then that kind of fusion I I think what you're saying editing is very interesting about um you know will the vectors be able to contain that kind of keyword Centric focus in the future and I kind of think so also especially with like say the way that Colbert would re-rank with token representations but I think in addition to that uh this rank Fusion thing that we've been exploring will be very useful I can imagine combining it with the wear filter and near Tech search to have this kind of boost so you know saying recommendation or sir like you're in an e-commerce store again uh it's Black Friday and you go for rugs and it's like you know two thousand dollar for a rug three thousand dollar for a rug and so you also want to have where prices less than 300 but then you want to have like the fusion where you also show the extravagant for rugs so you could have that kind of rank Fusion so I think that rank Fusion is a core primitive of search pipelines that we've explored in this particular feature uh one other thing and then I really want to dive into ref devec with Parker and uh is this um how we've been benchmarking hybrid search and it comes back to the backups and I I think this is just so exciting for the development of levia and our features and our connection to the science is we've been uploading the beer benchmarks to eviate and we have the ndcg the recall scores and I think it's just an incredible exciting step for us um so I'm kind of curious editing if I could ask this kind of a question about like your thoughts on sort of the beer benchmarks and just sort of these kind of like academic information retrieval metrics and how they play with feature development yeah so the beer Benchmark is definitely more your area of expertise than the mine but I think this is this is exactly what makes this so great that we we now have something quantifiable as well as opposed to to just sort of it's it benchmarks are always reflective of some sort of scenario so you could set up in a benchmark in a way to produce some kind of a result so so let's say The Benchmark is primarily keyword based and probably a keyword-based algorithm is going to be better on it let's say it doesn't care so much about um like yeah specific unknown words but it matches a domain of a semantic search based model then you're probably screwing it towards that so so benchmarks are yeah or you always need like the asterisk for what what is the Benchmark meant to to show but nevertheless I mean that's not a reason not to to use benchmarks like it's it's very good to to be able to objectively say okay a is better than b whether that matches to a being better than b for a specific use case that is something that that users have to see for for themselves so that is why it's super important to me to have both approaches like the quantifiable approach but also the qualitative approach where he's like okay an actual use case and we we have our customer success team who deals with the the paid cases that we have uh for first semi we have the open source Community who sometimes share data or or give us some insight into what's working for them so the mix of both to me is super important that we don't just make these claims of chair picked examples but that we also don't do the opposite of saying like hey it's nothing is cherry-picked everything is scientific but then the user comes in saying like why doesn't it work for me yeah it's absolutely fascinating especially with uh I'm like kind of coming into the trending topic of the day uh chat gbt I don't mean to distract too much but this kind of ad hoc evaluation I did one query I like the result that means the system works compared yeah and that goes both ways right like you see the people saying like hey it's the best thing ever I only have positive results and you see people saying is the worst thing ever I only have negative examples yeah exactly and I think it's worth kind of knowing that these systems are a little different than the maybe traditional software cases where these Edge like machine learning performance is very like long tail to like hit or miss and I think the beer benchmarks a particular reason why I'm so excited about this particular work is uh the diversity captured in it it has you know papers about covid-19 it has Financial questions like uh are my personal taxes separate from my hobby income and then you have like nutrition questions about like are multivitamins a waste of money so you have this incredible diversity diversity and query length and I think we're also seeing the kind of intense this kind of explorationism This research is emerging as well where you'd say what is the intent of the search task and that kind of expiration um so yeah overall I just couldn't be more excited about the benchmarking I think it's such an exciting step for us so I want to play the topics I'm so excited to have uh Parker especially because he uh was so pivotal to the development of ref Parker could you start by kind of explaining what ref to VEC is and then I really to dive into sort of some of the questions that we've been seeing in our community chat like particularly clarifying um updating the references and how this kind of Cascades backwards uh thinking around like can we have custom aggregation functions but but maybe if we could set the stage can you tell us about what reftubec is yeah certainly so uh ref the back centroid is yeah a new module that we released uh recently and uh the idea of it is is that um an object which is set to be vectorized so to speak by rectifix Android um a vector isn't produced by this object itself but it's produced by like the aggregate of its references vectors so uh the ref to back module will take an object and then grab the vectors from all of its references all of its referenced objects and then we'll yeah compute a centroid with that um set of vectors to to to find something that's yeah similar to all of these things at once and and so the idea is that this is really useful when you want to represent something um as an aggregation of other things right for example um users uh based on their likes right what can we what can we uh show to a user that is something that aligns with what their their Express interests are and the rest of that centroid is something that's that's perfect for doing something like that yeah exactly I think um the the sort of the most obvious use case I think is the kind of bipartite graph recommendation case where you have users products uh the user liked a few products and now you represent the user with the uh with the vector from the products and then that's the query Vector to recommend new products with I think that really helps uh just get the idea quickly um yeah I think um I want to kind of stay on this topic of graphs and weviate a little more I have sort of my story of coming to weviate is um you know I when I had first talked to Bobby's it was Ted talked about how he was really interested in the semantic web in ontologies and then sort of shortly after we had that podcast I went to New York to meet Laura at the knowledge graph conference where their you know companies like tiger graph relational AI where they have these tuples and so I always kind of had this thinking that like we V8 is going to have this focus on the graphs sort of opening this up and maybe adding you could start them can you tell me about kind of the the motivation behind this cross-reference design because I think it's so powerful so under like I don't want to say underappreciated but I think it's maybe not hyped enough like this kind of design of having cross-references the way it lets you do multi-vector the way it lets you do multimodal I think it's such a powerful part of the data schema design and webiate yeah this this goes back a bit to the history of vb8 because there was a phase before we called ourselves or before we called bv8 a vector search engine um because we we we're sort of trying to figure out like what is it that that that alleviate can add or where it can add the kind of value and at that point um bb8 was no database on its own yet but bb8 was sort of thought as a layer on top of other databases and at that point we're actually planning on running the deviate on top of graph databases so we had an integration for what's called genus graph um a tool that I had not heard before and also kind of haven't heard of since but I think it's it's like it's a super Niche tool super good at what it does but also like a like a relatively small Niche and the idea of Janna's graph was that you build this itself on top of other databases so I think at that point I don't know if this is still true it was uh Cassandra and um an elasticsearch I think so sort of like store the data in in Cassandra and then uh query using using elasticsearch and and this enables you to yeah sort of build this like like super large scale graphs and then vv8 was basically the AI layer on top of that and originally the idea was was before we started yeah basically accepting vectors together with with objects to only vectorize the schema so one of the very first use cases was you have this these these knowledge graphs and they have different ontologies so so you would have a graph here any graph there and you kind of know that there's an overlap but because people didn't use the exact same words it was super difficult to to Really match like what is like in these two crafts like yeah they do intersect but you don't know how so the original idea was to use um yeah basically NLP technology early in LLP technology of the time to just figure out what the right schema is and then at some point Bob and I were on a call and this was super early on I think that the team was very very small and we were kind of figuring figuring out this video like what if we tried that same approach not on the schema but in the actual data and we were both like nah that's that's crazy like we can't we can't do that and and then we tried I was like whoa this this works kind of well and that was kind of the the step where where sort of this semantic graph ontology tool turned into a vector data Vector database so we kind of pivoted completely and and that was also the point when we started um don't want to say rewrite vivade because some parts of it like the graphql API for example still is modeled after after that original structure um but yeah it was kind of um sort of Shifting the focus a bit but at the same time our early users already had graphs that they represented with bb8 so they're like okay well we can't just abandon them we can just say like okay V8 now switched from sort of this semantic graph tool to a nosql document only search engine and now you can't represent your graphs anymore okay uh vv8 is probably not going to become a graph database because so if you in in architecture it's always it's always trade-offs I'm like what do you prioritize them said okay search is more important for us than than craft reversal so we kind of uh skewed the architecture towards search so the the agents W index and the inverted index and the way it distributes data across nodes and these kind of things so these are all set up for for search but we said we want to keep the cross references and the the cross references from an architectural perspective they're basically just links and and of course there's some couple of optimizations you can do when you resolve those those links um but yeah that's that's kind of the the history of why they're there and um and and now sort of it's it's an enabler for new use cases basically yeah that's amazing I I've always wondered like the vectorized class name thing now makes so much more sense to me with the context of that and that's so interesting um so I really want to dive into the technical details uh Parker could you tell us about like because I we're seeing this question about kind of people want to understand exactly how if they have a to B to C and they update C uh will it Cascade back like that this kind of question uh it seems to be something that people are curious about yeah absolutely So currently the um the only way to update an object's reference Vector uh is by updating that object itself the parent object which holds the references so let's say um object a references object B and C and object A's uh Vector reference Vector is the centroid of bnc's vectors uh if B or C are updated a is a reference Vector does not change right now we don't have any sort of back Channel mechanism that allows that information to to reach the object which references those vectors and primarily uh it was because this is our first iteration um this is something that could be very computationally heavy if for example um we have tons of these reference vectors around so uh currently the the only way to update an object's reference Vector is to update that object's set of references directly so that can be done either just by posting an entirely new object or I guess you could say put in a new object with the same ID and a new set of reference vectors or deleting some references from that object or updating that object's references one at a time but basically the only way to update an object's reference Vector is to mutate that object set of references on itself whereas updating one of its references directly is not going to affect that parent objects reference Vector yeah and that idea of the kind of yeah when you really chain out these graphs and there's kind of like the bipartite graph I originally designed described where you can have like multiple edges uh you maybe also have it going back and forth kind of if you imagine data like that but like multiple classes chained together I think the aggregate functions are going to be that's going to be something that we can really explore and as edian said in laying the future for what we can do them and maybe I even want to just work this in there because I'm happy to have gone so last night I went to mit's learning on graphs conference and it was firstly yeah it was super cool to be at MIT it's super super smart people and just walking around here like nice I'm at MIT but like um seeing the research and seeing the thinking around graph neural networks it can be so intense about this kind of thing of like what kind of problems can uh deep learning broadly solve sort of connecting to like the Turing machines and uh you know like what problems can be solved like NP completeness can graph neural networks take that on but I think there's a big middle ground but for like the just a graph convolutional Network being used somewhere that's useful and I think just this basic idea of chaining these things together aggregating maybe that could be the use of that and we could similarly have a python app for our module inference the same way we have text-to-back uh you know all these things we have that kind of container for the like pytorch geometric Library so kind of pivoting and I know that the graph neural network aggregation thing is a bit intense but can we talk about like um what would it take to build in like custom aggregation functions maybe starting with just like having biasing it so that the mean centroid is uh most heavily impacted by the most recently added cross-reference yeah currently uh are only uh are only module in the class of ref to vac is rough centroid this was built purposely to be able to be easily expanded um uh into more more um centroid type algorithms or more algorithms to to um yeah calculate this reference Vector however you want to calculate it so uh weaving its module system is by Design very modular and so if we were to want to introduce something like this um most of the boilerplate I guess you could say the groundwork has already been set so it's just a matter of coming up with the way you want to calculate these reference vectors and then um introducing a new module which piggybacks this existing ref the back framework that we build within the weaving module system to to use this new algorithm to calculate the reference Vector so I would say um for any reference or ref the back centroid modules in the future it's um not a whole lot of work to introduce a new one it's just a matter of like figuring out how you want to calculate these these reference vectors yeah I'm just just sorry just thinking out loud um about that that recency bias because I think I'm not 100 sure but basically references in vv8 have a specific order we we don't ever make use of the fact that they have an order but on disk they're they're saved in order so most likely we could use that fact we don't have time stamps for for a reference so right now we couldn't easily do say something like okay from one to two was a very short time difference then from two to three it was a large one but at least we know the order so if we just want to give the most weight to the most recent one it would probably be as simple as giving the most weight to the last element of the array yeah super interesting I think um kind of one other thing that excites me and yeah that I think that the building blocks of that are in place and that will be super impactful just basing on that a little longer you imagine like you want to have recommendation without sort of logging in and having that long Archive of user data you want to just be able to like scroll through Tick Tock or whatever and like quickly have recommendations I think that kind of thing lets you like control it with your by giving the signal of uh recency sort of one other thing that kind of excites me is this idea of clustering the um the embeddings I think that could be super powerful especially for diverse interests so like if you've liked these products and it's like uh Nike shoes Adidas shoes Jordan shoes I think instead of averaging it we could have this uh clustering and then the centroids could be used which brings this topic of how might we represent centroids like and I think the cross-reference thing again is we would use it again to do multi-vector representation and that kind of idea so super cool I think um yeah this overall this is 117 and thanks so much for the discussion on ref to VEC I'm so excited about reftubec I think this kind of graph structure how we can send embeddings through the graphs and aggregate them I think a lot of people are excited about it because I think it's exciting but but anyways thanks so much I think um yeah replication hybrid search and sort of the Italy 117 release all of it yeah yeah smooth a few few smaller improvements as well um we'll mention them in our release blog post but um a couple of uh performance improvements regarding startup time so uh both for for startup times at the time that the application takes to to restart so for example if there was a a node failure and this is something that with replication we have a lot more tolerance for but even with replication you still care about the time that the node is back because there may be a time window when when it's unavailable so there are a lot of improvements around the the um yeah startup time but also and and this was sort of similar uh a similar cost for this we've also improved uh batch latencies and and they're particularly sort of the the P99 or or the max latency so we had a pretty constant write speed already based on the LSM store having a constant write speed but then we had like these occasional Peaks and that those could lead to timeouts and then timeout would lead to a retry and then the retry would put more load on the cluster in all these this kind of chain of events and we have a lot of fixes around those as well that we implemented in one not 17 which is sort of one of these for for me my favorite category like not a very exciting feature but super exciting for those that actually operate VBA clusters awesome well thanks again both so much for the for coming on the podcast and everyone please check out weave 1.17 and thank you so much again for listening awesome thanks Connor ", "type": "Video", "name": "Weaviate 1.17 Release with Etienne Dilocker and Parker Duckworth - Weaviate Podcast #31", "path": "", "link": "https://www.youtube.com/watch?v=nSCUk5pHXlo", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}