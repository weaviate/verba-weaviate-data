{"text": "Weaviate 1.17!! This is a massive release for Weaviate, debuting Replication, Hybrid Search, BM25, Faster Startup and Import ... \nhey everyone I'm super excited to hostEddie and dylocker and Parker Duckworthfor the weevier 1.17 release podcastthese releases are always so great itfeels like such a celebration ofweeviate and the hard work of the teamto bring these new features uh intoweviate uh so today we're mainly talkingabout replication and hybrid search anduh we're also welcoming uh ParkerDuckworth for the first time on the webapodcast so we'll talk about ref devec aswell a little bit uh so firstly Parkerthank you so much for joining the weviapodcast yeah happy to be hereawesome I think this release is just sospecial because uh you know we all gottogether in Italy and these havingeveryone in the semi-technologies teamin the same room as our up on theWhiteboard at the slides and presentingthese new features edian can you tell usabout your experience with that yeahit's absolutely crazy so so we're acompletely remote company but beingremote doesn't mean that you can't meetup occasionally and this is exactly whatwe did so we had uh 28 I think uh or 27or 28 folks in in Italy in the same roomand uh we had a demo session planned forthe last day of the week to to show ourprogress and I think by that point ithad been about a week that I had lastsynced up with with Parker and red onewho were working on the replication andand I was just sitting there andenjoying the demo and I almost get a bitemotional looking at the kind ofprogress that we've made because I meanlike building a distributed database isthe first for me it's probably a firstfor most people to do that and um seeingthat come together and sort of seeingthe the seed that that we originallyplanted when it was a super tiny teamand see the the team grow and and comethis feature come together and it I meanit's like it's such a such a seriousfeature in the end basically like whenwe started I would think like okay oncewe have replication that is really thenwe're in the big league then we havethese these high availability andfailover scenarios these kind of thingsand yeah seeing that happen absolutelyunique experience can absolutelyrecommend it go to Italy with yourcompanyyeah it's just an absolutely incredibleexperience and and yeah the end at thelast day uh Parker and red one gave thisincredible lecture about a replicationso uh Parker could kind of take it awayand uh tell us about replication yeahabsolutely so uh when I first started atthe company we hadum a milestone it was like a the lastMilestone of the road map which wasreplication and it seemed so far away uhsuch a Monumental task at the time rightwe didn't really have anythingumto to support that at the time that Ijoined so you know over time we've builttowards replicationum for example starting with backups uhfirst we introduced the ability to backup whatever's in your weviate instanceum maybe on a single node and then thatevolved intodistributed backups which allows you tolike backup a cluster and all of thiswas working towards the ability toautomatically replicate or to supportreplicationand finally we were able to build on uhthe back of all the work that we haddone with backups and introducereplication so it's something that wehad in mind the whole time throughoutthe planning and development of backupswe knew that we were building towardsreplication and so we wanted to yeahjust build it up incrementally until wewe got to this point so uh the reallyfun and interesting thing is that reallythe Capstone of the replication work uhI guess you could say was done in Italyso up to the pointum up to that point you know we I hadn'tmet anyone in the team in person youknow I'm based in the US and uh the restof the core team is based um in Europeand other other places and souh getting to sit uh specifically nextto a red one another core team memberand work with him in person to finalizethis replicationum that we wanted to build was supersuper uh interesting and andum they just made the whole experienceso great so thethe way that we decided to implementum replication we first looked at youknow the cap theorem like whattrade-offs do we want to make here youknow do we want to prioritizeconsistency or availability or partitiontolerance and so after discussingum many times with the core team and andum yeah red one and I discussing for fora while weumdecided to make the trade-off forpartition tolerance and availability uhsimilar to Cassandraum the thing with with weeviate is thatum it'ssuper read heavy so oftentimes the usecase uh will be where um we'll insertlike a large amount of of data up frontand maybe there will be more inserts inthe future but we want to prioritize uhreadum the availabilityum so that being saidum we decided to follow a uh leaderlessum yeah replication algorithm so theidea is that a request will come into acluster of Eva nodes and the node whichhappens to receive the requests will beumuh promoted I I guess you could say asthe coordinator for that request and soum this coordinator uh also considersitself one of the participant nodes orone of the umyeah other nodes that it needs to relaylike this this replicated data to soum the coordinator will uh participatein like a two-phase commit with the restof the nodes including itselfso a request comes in let's say a writerequest forum yeah a piece of data and then thecoordinator receives the request andsends out um a broadcast basically likeasking every node that is part of thereplication replica set I guess youcould say um to acknowledge the requesthas come in uh once that acknowledge oneelse comes comes back thenum yeah the coordinator will actuallysend out the rest uh of the notes thedata that it needs to actually commit orwrite to disk so um the advantage togoing with this uh leaderlessum algorithm is that it's more flexiblein the case of node failure we don'thave like a single point of failure withlike a leader followerum algorithmsoum yeah just uh makes things moreflexible in the event of like nodeoutages and things like thatyeah that's so interesting to hear aboutI I love learning about these newdatabase features in webia for examplewhen the backups came out I had so muchuse out of that with my research onsearch features because as we'rebuilding out this beer benchmarks andwe're going to talk about this laterwith hybrid search and our evaluationmiracle and how we're evaluating thisthese bench these uh backups sorry havebeen so useful in I can upload NF Corpusarguana sci-fi all these beer data setsand then just back it up restore it runthe tests and then you know we know ourhybrid surge performance or whateverwe're evaluating them so can you help meunderstand further uh the use cases ofreplication um when when people aregoing to need to use it yeah so the thebiggest one is for reliability sotypically with replication you want somekind of redundancy so for example if anote goes down and this is a Parkeralready said uh that from theperspective of the cap theorem where umprioritizing availability and partitiontolerance and partition tolerance in adistributed cluster is basically a givenso partition tolerance of this means inin this case meansa connection to another node could godown or the node itself could go downand like from the perspective of a nodethe other node is down doesn't matter ifit's actually down or if it just can'treach it so kind of a partitiontolerance is a given and um thenavailability would mean that use casescan still continue and the cool thingwith our application is that theconsistency level that the user wants istunable so um some of that is going tobe part of 1.17 some of it is only goingto be part of 1.18 so we're rolling thisout in in phases basically but in phasesthat we believe make sense so it's notthat one net 1.17 is like a halffinished feature and then it's onlycomplete in one not 18 but basicallywhat you get in 1.17 is is complete andusable and then you get new featuresthat still fall under the umbrella of ofreplication in 1.18.um so yeah with tunable consistency youcan basically sayum how much priority do I give to theavailability of reads versus uh right sowith 1.17 and every ride is replicatedto all nodes that participate in thatparticular class so in in a sort ofconsistency level terms that's thereplication level all and then everyread or request that comes in um atleast for for searches uh it is the withdone with the consistency level of oneso in other words to write data into thecluster all nodes need to be availablebut to read data any node could go downbasically as long as you still have oneand that's that's a specificconfiguration so in this kind of setupthis would work well for um for examplefor uh search on an e-commerceapplication so you would say like heyproducts are updated only once a day sowe only need to be able to write dataonce every 24 hours but users need to beable to search 24 hours and if somethinggoes down they still need to be able tosearch so that will be a great use casefor for the kind of uh read with uhsorry write with high consistency andread flow consistency kind of cases butthere are others you could for examplesay if I ride with a quorum and readwith a quorum then um you could toleratenode failures on on both cases or youcould say I both write and read withvery little a consistency or minimalconsistency basically and then you couldtolerate a lot of node failures but alsoyou could risk that data is out of syncso basically you have this eventualconsistency kind of aspect where you sayokay for my use case I can tolerate itif some data is not there at some pointbut it needs to be there later on sothis is the the high availability kindof use case which from my perspective isthe the most requested reason or themost the biggest motivator in inreplication but there are others so forexample what you can also do is you canuse replication to scale your readthroughput so again to stick withe-commerce uh Black Friday let's say thekind of traffic that you expect on onBlack Friday is five times the kind oftraffic that you would expect duringduring regular day or regular week youdon't want to provision your cluster forthat Peak load that you have one day ofthe year you want to have your clustersized appropriately for the rest of thethe year and then scale it up basicallyjust for for that one or two or threeday period And this is some things youcan do with replication so you could saymy replication factor is three for mostof the year so I have some redundancybut maybe now I scaled up to five or or8 or 10 or 15 because then I have 15identical replicas that could serve thetraffic and basically this this getslinearly so 15 nodes could serve fivetimes the traffic that three nodes couldserveand finally there's another one and thisis sort of more on the roadmap butthat's the the multi-data center kind ofreplication so for regional proximity soyou could have two places on on Earth somaybe I think we've used that before inagent SW graph example so let's stickwith that we have Frankfurt Germany andum I don't know Boston uh USA so um ifyou had a data center somewhere in themiddle it's kind of a bad example rightnow because we're in the middle of theAtlantic but that's okay let's let'sassume there was Data Center in theAtlanticum then the kind of latencies that userswould get would be pretty much the samefor the for the users in Boston and forthe users in Frankfurt because they havethe same sort of regional distance tothe data center but no one has a realgood latency because no one is close tothe data center so we could say let'smove the data center to Boston super forthe Boston users not so great for theFrankfurt users but now with multi-dcreplication which you could also do isyou could have a data center in Bostonyou can have a data Center in inFrankfurt and each of those userscontacts the data center that's close tothem so they have good latencies butthen of course the data centers need tostay in sync as well and this isbasically the multi-data centerreplication this is something that's notyet present in 1.17 also not going to bepart of 1.18 yet but it's something thatthe kind of architecture that we'vechosen in vv8 that is supported by thearchitecture so this is something thatif you want to have it I think we have afeature request ticket for it on GitHubalready so upload it and then we'llbuild ityeah I love how you gave the example ofkind of the um rewrite trade-offs withconsistency levels when when you want touse each thing when I think that'salways a super important thing tounderstand yeah the the distributedsystems it's so interesting to learnabout it reminds me of our podcastpeople are you know for a binge on weviapodcast we did another podcast with EricBernice and on a running Vector searchin production and it reminds me of thattopic of yeah what it takes to you knowscale out your e-commerce store so youcan handle Black Friday I think ingeneral it's so interesting um so alsoquickly I want to touch on this um thisiterative release and Eddie and you'verecently written a very popular blogpost on product engineering I think nowwould be a great time to kind of touchon product engineering and you'rethinking around these iterative releasesand the general kind of philosophybehind how you think about these thingsyeah so so uh for those of you whohaven't heard of product engineering yetit's kind of the the merger of productdecision and Engineering decision soit's kind of blurring the lines betweentraditionally you might have a productdepartment or a product team and thenyou have the engineering department andthese these parties hate each other andthey don't talk to each other and don'tcollaborateum and the in product engineering theideas that you you soften theseboundaries and you have collaboration soin a startup to me this makes a lot ofsense because you have small teams andif you have a team with like threedevelopers you can't afford to have adedicated product manager and maybe adedicated lead developer and then who'sgoing to do the work basically they havelike a a manager to engineer ratio thatjust doesn't make sense so somethingthat naturally involves in those kind ofsettings is that yeah you blur the linesand you maybe have an engineer who takesover a couple of productresponsibilities or maybe you have aproduct manager who has an engineeringbackground and can do the the kind ofprototyping themselves and um yeah youhave easier communication morecollaboration and have something that Iwould sayyeah sort of is is is more productiveand and feels more natural and you don'thave these kind of artificial boundariesbetween the twoum for our replication releaseum I mentioned that before already we'rereally really interested in the feedbackthat we're getting and we're reallytrying to to provide value early butvalue but really value like not justsort of give you something half finishedso like hey we went for a minimal uhimplementation so that you could have itsooner but don't use it because it's notreally meant for production and that'skind of not what we're trying to do butinstead return to say like okay what isa combination of features that you woulduse or what you would need to use it inin production and then maybe this onlyworks for 80 of use cases maybe itdoesn't work for the remaining 20 yetthat's fine because the remaining 20they'll be covered later on but that'sno reason for us to say to the the first80 hey you're gonna have to wait becausewe're only going to release it when weonce we have 100 covered so that's kindof the idea um of doing that initerations and of course we get thefeedback when something is out we wellyeah it gives us the ability to stillpivot and for this to to sort of tiethis back into product engineering it'ssuper important to have that thatfeedback cycle and not have likeartificialsteps in between where someone has torelay a message from like one Departmentto another department then in the endyou have Engineers that never talk tousersum but it's the complete oppositebasically like everyone in in uh the UVAcore team or in other teams can be amember of our or not can be a member buttypically is a member of our publicslacks so they communicate withEngineers right away and then if if newideas pop up through that we we discussthe ideas internally and it's not likewell no the product manager said we'renot going to do that but it's more waymore collaborateyeah incredible and I think this is agreat transition to start talking abouthybrid search and our philosophy and theoverall how we've developed it and so onand so maybe let me set the stage bydescribing what hybrid search is so uhhybrid search describes combiningkeyword scoring methods with Vectorsearch methods so let's I think we'reall pretty familiar with the vectorsurge part that's where we encode datawith machine learning models build up avector index and search for theapproximate nearest neighbors and thatbut now let's kind of focused a littlemore on the keyword scoring methods sosort of the foundational algorithm istfidf term frequency inverse documentfrequency where you score some sentencelike I'm super excited to welcome Ed Ianand Parker Duckworth to the podcastbased on the kind of uniqueness of theseterms in that query with how unique itis to the collection of documents that Ihave so then going from TF IDF to bm25bm25 introduces this binary Independencemodel it's you don't count how manytimes the keyword is going to appear inthe document just whether it appears ornot you similarly normalize it for thelength of the the document so it's a bitof a modification to tfidf and it'sanother way of scoring these documentsbased on the keywords that has beenreally successful so then we have thesetwo uh search algorithms and so now withhybrid we're combining the the resultsfrom each of the lists so we're going todive a little further into the rankfusion and then also say bm25 and theextension to bm25f but I want to comeback to this um product engineering andEddie and I want to ask about I thoughtwith this project you did such a greatjob of leading the team this was one ofthe projects that I've been a part ofsince joining we V8 where there's beenlike it's like a task force almost likethis is our project this is yourresponsibility this is yourresponsibility and then we've just kindof come together and it's almostfinished and it's so exciting so can youtell me about like your initial thinkingaround the development of the hybridsearch Projectyeah that's that's great to hear by theway that's that's really nice yeah so souh for our listeners to to understand abit sort of how we structure thatinternallyum we have the core team itself whichbasically builds vv8 which is kind of alot of what we do but by far not the theonly thing that we do and then uh Conoris part of the the research team as wellso besides um like the podcast and andother sort of several activities there'salso the the research part and what weconsider research and research that theterm research depending on what yourbackground is this can have verydifferent definitions or it can be havevery different meaningsum but we use research in the sense thatwe say we've identified an opportunitysomewhere something that we will mostlikely want to add to alleviate butthere is some kind of a question that weneed to answer first and this questioncould be something as simple aswhat is the best ux to to integrate thisinto our apis like how would our userswant to use it like do we want to givethe user a lot of control or do we wantto maybe abstract something so this thiscould be a question it could be aquestion of how are we going to build itso especially in inum so you mentioned uh rank fusion andthen the scorebase fusion and thesethese terms this is basically somethingthat that you know way better than I doand something where we benefit so muchfrom from um yeah having these kind ofkind of collaborations within thecompanyum yeah so this sort of how do we buildit what do we need what do we need tofigure out how to to be able to build itcould be an evaluation also somethinglike does this idea make sense like itlooks good on paper but what happens ifwe try it at scale let's try it with 10Kobjects a million objects maybe abillion objects thus it does it scaledoes it fit into into deviate in thatsense and this is something where hybridI think early on WE identified thatthere's an opportunityand um said like okay let's let's getstarted let's see what it is let's seeum what do we need because hybrid sortof in a sense you need the both thebuilding blocks for hybrid both The bm25Surge and the theum the vector surge you need to haveboth Vector search obviously is kind ofwhat bv8 is about so we can safelyassume that we have Vector searchcovered bm25 is something that wegradually started buildingum it was actually TF IDF in in sort ofsort of the thethe simple building block for bm25 but Ithink from the indexing perspective it'sactually the same or it's like one ortwo parameters need to additionally beindexed for bm25 that is something thatwe actually had in mind in the very veryfirst prototype that we built so we wedidn't have any apis for for TF IDF orbm25 but we had the inverted index earlyon I think over over two years ago weadded the inverted index to to bb8 andit already had this this and Parker youmay have come across that in the in thecode whenever we put those buckets wehad like buckets for with frequency andwithout frequency and the word frequencyis so basically for all the textproperties we We additionally uh to toindexing the word we also index thefrequency and that was in preparationfor that whole TF IDF pm25 step so wekind of knew that it might be somethingthat we want to add at some pointum but we also have to figure out likewhat is what is the real value of it andum if I'm a hundred percent honestsomething think that that I don't knowat this moment is Will hybrid searchplay role five or ten years from now itcould easily be the case and then I Idon't think anyone can confidently endcould be the case that semantic searchjust keeps improving so much that hybridsearch basically is more of a stopgapsolution at the moment to bridge a gapand in the Gap being exact keyword matchin and out of domain search or it couldbe that while it still improves hybridsearch is just always going to be betterbecause it's the the combination of twothings and this is something that thatyeah I I don't know and I don't thinkanyone really knows but something that Ifind super exciting andyeah quote quote me on this five yearsfrom now and let's see let's see how itturned outthere seems to have been quite a bit ofBuzz about hybrid search in thecommunity as well I think in our ourslack Community I often see peoplerequesting this feature or asking whenit's going to be available or beingexcited when they hear that it's goingto be available soon I think uh yeahit's always awesome to see people askingfor things and then have it deliveredthat's so cool and I think um so inpreparation for weeviate air Eric and Iwere coming up with a demo and I I thinkthis example of how to catch an Alaskanpollock that query is a great examplewhere you have the semantic meaning ofcatch you don't mean catch a baseballcatch a cold you mean you know fishingand then Alaskan pollock which is aspecific keywordand then that kind of fusion I I thinkwhat you're saying editing is veryinteresting about um you know will thevectors be able to contain that kind ofkeyword Centric focus in the future andI kind of think so also especially withlike say the way that Colbert wouldre-rank with token representations but Ithink in addition to that uh this rankFusion thing that we've been exploringwill be very useful I can imaginecombining it with the wear filter andnear Tech search to have this kind ofboost so you know saying recommendationor sir like you're in an e-commercestore again uh it's Black Friday and yougo for rugs and it's like you know twothousand dollar for a rug three thousanddollar for a rug and so you also want tohave where prices less than 300 but thenyou want to have like the fusion whereyou also show the extravagant for rugsso you could have that kind of rankFusion so I think that rank Fusion is acore primitive of search pipelines thatwe've explored in this particularfeature uh one other thing and then Ireally want to dive into ref devec withParker and uhis this um how we've been benchmarkinghybrid search and it comes back to thebackups and II think this is just so exciting for thedevelopment of levia and our featuresand our connection to the science iswe've been uploading the beer benchmarksto eviate and we have the ndcg therecall scores and I think it's just anincredible exciting step for us um soI'm kind of curious editing if I couldask this kind of a question about likeyour thoughts on sort of the beerbenchmarks and just sort of these kindof like academic information retrievalmetrics and how they play with featuredevelopmentyeah so the beer Benchmark is definitelymore your area of expertise than themine but I think this is this is exactlywhat makes this so great that we we nowhave something quantifiable as well asopposed to to just sort of it's itbenchmarks are always reflective of somesort of scenario so you could set up ina benchmark in a way to produce somekind of a result so so let's say TheBenchmark is primarily keyword based andprobably a keyword-based algorithm isgoing to be better on it let's say itdoesn't care so much aboutum like yeah specific unknown words butit matches a domain of a semantic searchbased model then you're probablyscrewing it towards that so sobenchmarks are yeah or you always needlike the asterisk for what what is theBenchmark meant to to show butnevertheless I mean that's not a reasonnot to to use benchmarks like it's it'svery good to to be able to objectivelysay okay a is better than b whether thatmatches to a being better than b for aspecific use case that is something thatthat users have to see for forthemselves so that is why it's superimportant to me to have both approacheslike the quantifiable approach but alsothe qualitative approach where he's likeokay an actual use case and we we haveour customer success team who deals withthe the paid cases that we have uh forfirst semi we have the open sourceCommunity who sometimes share data or orgive us some insight into what's workingfor them so the mix of both to me issuper important that we don't just makethese claims of chair picked examplesbut that we also don't do the oppositeof saying like hey it's nothing ischerry-picked everything is scientificbut then the user comes in saying likewhy doesn't it work for meyeah it's absolutely fascinatingespecially with uh I'm like kind ofcoming into the trending topic of theday uh chat gbt I don't mean to distracttoo much but this kind of ad hocevaluation I did one query I like theresult that means the system workscompared yeah and that goes both waysright like you see the people sayinglike hey it's the best thing ever I onlyhave positive results and you see peoplesaying is the worst thing ever I onlyhave negative examples yeah exactly andI think it'sworth kind of knowing that these systemsare a little different thanthe maybe traditional software caseswhere these Edge like machine learningperformance is very like long tail tolike hit or miss and I think the beerbenchmarks a particular reason why I'mso excited about this particular work isuh the diversity captured in it it hasyou know papers about covid-19 it hasFinancial questions like uh are mypersonal taxes separate from my hobbyincome and then you have like nutritionquestions about like are multivitamins awaste of money so you have thisincredible diversity diversity and querylength and I think we're also seeing thekind of intense this kind ofexplorationism This research is emergingas well where you'd say what is theintent of the search task and that kindof expiration um so yeah overall I justcouldn't be more excited about thebenchmarking I think it's such anexciting step for us so I want to playthe topics I'm so excited to have uhParker especially because he uh was sopivotal to the development of ref Parkercould you start by kind of explainingwhat ref to VEC is and then I really todive into sort of some of the questionsthat we've been seeing in our communitychat like particularly clarifying umupdating the references and how thiskind of Cascades backwards uh thinkingaround like can we have customaggregation functions but but maybe ifwe could set the stage can you tell usabout what reftubec isyeah certainly so uh ref the backcentroid is yeah a new module that wereleased uh recently and uh the idea ofit is is thatum an object which is set to bevectorized so to speak by rectifixAndroidum a vector isn't produced by thisobject itself but it's produced by likethe aggregate of its references vectorsso uh the ref to back module will takean object and then grab the vectors fromall of its references all of itsreferenced objects and then we'll yeahcompute a centroid with thatum set of vectors to to to findsomething that's yeah similar to all ofthese things at once and and so the ideais that this is really useful when youwant to represent somethingum as an aggregation of other thingsright for exampleum users uh based on their likes rightwhat can we what can we uh show to auser that is something that aligns withwhat their their Express interests areand the rest of that centroid issomething that's that's perfect fordoing something like that yeah exactly Ithink um the the sort of the mostobvious use case I think is the kind ofbipartite graph recommendation casewhere you have users products uh theuser liked a few products and now yourepresent the user with the uh with thevector from the products and then that'sthe query Vector to recommend newproducts with I think that really helpsuh just get the idea quickly um yeah Ithink um I want to kind of stay on thistopic of graphs and weviate a littlemore I have sort of my story of comingto weviate is um you know I when I hadfirst talked to Bobby's it was Tedtalked about how he was reallyinterested in the semantic web inontologies and then sort of shortlyafter we had that podcast I went to NewYork to meet Laura at the knowledgegraph conference where their you knowcompanies like tiger graph relational AIwhere they have these tuples and so Ialways kind of had this thinking thatlike we V8 is going to have this focuson the graphs sort of opening this upand maybe adding you could start themcan you tell me about kind of the themotivation behind this cross-referencedesign because I think it's so powerfulso under like I don't want to sayunderappreciated but I think it's maybenot hyped enough like this kind ofdesign of having cross-references theway it lets you do multi-vector the wayit lets you do multimodal I think it'ssuch a powerful part of the data schemadesign and webiateyeah this this goes back a bit to thehistory of vb8 because there was a phasebefore we called ourselves or before wecalled bv8 a vector search engineum because we we we're sort of trying tofigure out like what is it that thatthat alleviate can add or where it canadd the kind of value and at that pointum bb8 was no database on its own yetbut bb8 was sort of thought as a layeron top of other databases and at thatpoint we're actually planning on runningthe deviate on top of graph databases sowe had an integration for what's calledgenus graphum a tool that I had not heard beforeand also kind of haven't heard of sincebut I think it's it's like it's a superNiche tool super good at what it doesbut also like a like a relatively smallNiche and the idea of Janna's graph wasthat you build this itself on top ofother databases so I think at that pointI don't know if this is still true itwas uh Cassandra andum an elasticsearch I think so sort oflike store the data in in Cassandra andthen uh query using using elasticsearchand and this enables you to yeah sort ofbuild this like like super large scalegraphs and then vv8 was basically the AIlayer on top of that and originally theidea was was before we started yeahbasically accepting vectors togetherwith with objects to only vectorize theschema so one of the very first usecases was you have this these theseknowledge graphs and they have differentontologies so so you would have a graphhere any graph there and you kind ofknow that there's an overlap but becausepeople didn't use the exact same wordsit was super difficult to to Reallymatch like what is like in these twocrafts like yeah they do intersect butyou don't know how so the original ideawas to useum yeah basically NLP technology earlyin LLP technology of the time to justfigure out what the right schema is andthen at some point Bob and I were on acall and this was super early on I thinkthat the team was very very small and wewere kind of figuring figuring out thisvideo like what if we tried that sameapproach not on the schema but in theactual data and we were both like nahthat's that's crazy like we can't wecan't do that and and then we tried Iwas like whoa this this works kind ofwell and that was kind of the the stepwhere where sort of thissemantic graph ontology tool turned intoa vector data Vector database so we kindof pivoted completely and and that wasalso the point when we startedum don't want to say rewrite vivadebecause some parts of it like thegraphql API for example still is modeledafter after that original structureum but yeah it was kind ofum sort ofShifting the focus a bit but at the sametime our early users already had graphsthat they represented with bb8 sothey're like okay well we can't justabandon them we can just say like okayV8 now switched from sort of thissemantic graph tool to a nosql documentonly search engine and now you can'trepresent your graphs anymore okay uhvv8 is probably not going to become agraph database because so if you in inarchitecture it's always it's alwaystrade-offs I'm like what do youprioritize them said okay search is moreimportant for us than than craftreversal so we kind of uh skewed thearchitecture towards search so the theagents W index and the inverted indexand the way it distributes data acrossnodes and these kind of things so theseare all set up for for search but wesaid we want to keep the crossreferences and the the cross referencesfrom an architectural perspectivethey're basically just links and and ofcourse there's some couple ofoptimizations you can do when youresolve those those linksum but yeah that's that's kind of thethe history of why they're there and umand and now sort of it's it's an enablerfor new use cases basicallyyeah that's amazing I I've alwayswondered like the vectorized class namething now makes so much more sense to mewith the context of that and that's sointeresting um so I really want to diveinto the technical details uh Parkercould you tell us about like because Iwe're seeing this question about kind ofpeople want to understand exactly how ifthey have a to B to C and they update Cuh will it Cascade back like that thiskind of question uh it seems to besomething that people are curious aboutyeah absolutely So currently the umthe only way to update an object'sreference Vector uh is by updating thatobject itself the parent object whichholds the references so let's sayum object a references object B and Cand object A's uh Vector referenceVector is the centroid of bnc's vectorsuh if B or C are updated a is areference Vector does not change rightnow we don't have any sort of backChannel mechanism that allows thatinformation to to reach the object whichreferences those vectors and primarilyuh it was because this is our firstiterationum this is something that could be verycomputationally heavy if for exampleum we have tons of these referencevectors around so uh currently the theonly way to update an object's referenceVector is to update that object's set ofreferences directly so that can be doneeither just by posting an entirely newobject or I guess you could say put in anew object with the same ID and a newset of reference vectors or deletingsome references from that object orupdating that object's references one ata time but basically the only way toupdate an object's reference Vector isto mutate that object set of referenceson itself whereas updating one of itsreferences directly is not going toaffect that parent objects referenceVector yeah and that idea of the kind ofyeah when you really chain out thesegraphs and there's kind of like thebipartite graph I originally designeddescribed where you can have likemultiple edges uh you maybe also have itgoing back and forth kind of if youimagine data like that but like multipleclasses chained together I think theaggregate functions are going to bethat's going to be something that we canreally explore and as edian said inlaying the future for what we can dothem and maybe I even want to just workthis in there because I'm happy to havegone so last night I went to mit'slearning on graphs conference and it wasfirstly yeah it was super cool to be atMIT it's super super smart people andjust walking around here like nice I'mat MIT but like um seeing the researchand seeing the thinking around graphneural networks it can be so intenseabout this kind of thing of like whatkind of problems can uh deep learningbroadly solve sort of connecting to likethe Turing machines and uh you know likewhat problems can be solved like NPcompleteness can graph neural networkstake that on but I think there's a bigmiddle ground but for like the just agraph convolutional Network being usedsomewhere that's useful and I think justthis basic idea of chaining these thingstogether aggregating maybe that could bethe use of that and we could similarlyhave a python app for our moduleinference the same way we havetext-to-back uh you know all thesethings we have that kind of containerfor the like pytorch geometric Libraryso kind of pivoting and I know that thegraph neural network aggregation thingis a bit intense but can we talk aboutlike um what would it take to build inlike custom aggregation functions maybestarting with just like having biasingit so that the mean centroid is uh mostheavily impacted by the most recentlyadded cross-reference yeah currently uhare only uhare only module in the class of ref tovac is rough centroid this was builtpurposely to be able to be easilyexpandedumuh into more moreum centroid type algorithms or morealgorithms to toum yeah calculate this reference Vectorhowever you want to calculate it so uhweaving its module system is by Designvery modular and so if we were to wantto introduce something like thisummost of the boilerplate I guess youcould say the groundwork has alreadybeen set so it's just a matter of comingup with the way you want to calculatethese reference vectors and thenum introducing a new module whichpiggybacks this existing ref the backframework that we build within theweaving module system to to use this newalgorithm to calculate the referenceVector so I would sayum for any reference or ref the backcentroid modules in the future it'sum not a whole lot of work to introducea new one it's just a matter of likefiguring out how you want to calculatethese these reference vectorsyeah I'm just just sorry just thinkingout loudum about that that recency bias becauseI think I'm not 100 sure but basicallyreferences in vv8 have a specific orderwe we don't ever make use of the factthat they have an order but on diskthey're they're saved in order so mostlikely we could use that fact we don'thave time stamps for for a reference soright now we couldn't easily do saysomething like okay from one to two wasa very short time difference then fromtwo to three it was a large one but atleast we know the order so if we justwant to give the most weight to the mostrecent one it would probably be assimple as giving the most weight to thelast element of the arrayyeah super interesting I think um kindof one other thing that excites me andyeah that I think that the buildingblocks of that are in place and thatwill be super impactful just basing onthat a little longer you imagine likeyou want to have recommendation withoutsort of logging in and having that longArchive of user data you want to just beable to like scroll through Tick Tock orwhatever and like quickly haverecommendations I think that kind ofthing lets you like control it with yourby giving the signal of uh recency sortof one other thing that kind of excitesme is this idea of clustering the um theembeddings I think that could be superpowerful especially for diverseinterestsso like if you've liked these productsand it's like uh Nike shoes Adidas shoesJordan shoes I think instead ofaveraging it we could have this uhclustering and then the centroids couldbe used which brings this topic of howmight we represent centroids like and Ithink the cross-reference thing again iswe would use it again to do multi-vectorrepresentation and that kind of ideaso super cool I think um yeah thisoverall this is 117 and thanks so muchfor the discussion on ref to VEC I'm soexcited about reftubec I think this kindof graph structure how we can sendembeddings through the graphs andaggregate them I think a lot of peopleare excited about it because I thinkit's excitingbut but anyways thanks so much I thinkum yeah replication hybrid search andsort of the Italy 117 release all of ityeah yeah smooth a few few smallerimprovements as well um we'll mentionthem in our release blog post but um acouple of uh performance improvementsregarding startup time so uh both forfor startup times at the time that theapplication takes to to restart so forexample if there was a a node failureand this is something that withreplication we have a lot more tolerancefor but even with replication you stillcare about the time that the node isback because there may be a time windowwhen when it's unavailable so there area lot of improvements around the theum yeah startup time but also and andthis was sort of similar uh a similarcost for this we've also improved uhbatch latencies and and they'reparticularly sort of the the P99 or orthe max latency so we had a prettyconstant write speed already based onthe LSM store having a constant writespeed but then we had like theseoccasional Peaks and that those couldlead to timeouts and then timeout wouldlead to a retry and then the retry wouldput more load on the cluster in allthese this kind of chain of events andwe have a lot of fixes around those aswell that we implemented in one not 17which is sort of one of these for for memy favorite category like not a veryexciting feature but super exciting forthose that actually operate VBA clustersawesome well thanks again both so muchfor the for coming on the podcast andeveryone please check out weave 1.17 andthank you so much again for listeningawesome thanks Connor", "type": "Video", "name": "Weaviate 1.17 Release with Etienne Dilocker and Parker Duckworth - Weaviate Podcast #31", "path": "", "link": "https://www.youtube.com/watch?v=nSCUk5pHXlo", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}