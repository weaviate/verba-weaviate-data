{"text": "Introduction and demo by Bob van Luijt during FOSDEM 2020 about the Weaviate Vector Database. More info: ... \nyes thank you so much well thank you for having me so it's indeed it's a loss of the day so it's like outside and I saw like a lot of people already drinking beers and stuff so it's like I hope that there's still people in the room so thank you for listening so this is like yeah last year my my colleague HN was here any he presents for the first time we've yet and what we're trying to do and we've done a lot in the year so today I wanna I wanna show you actually an update and way more things about we v8 and a lot of things have changed also the things that we've changed into so quickly agenda what I want to talk about so first of all it's about what it is because they work up you know we're new we're new on the scene so yeah so what is it secondly I want to talk about what has changed since last year or presented here and this year I want to talk a little bit of the about technology and last but not least of course I want to give it demo I want to show it in action all right so that's the that's what I'm gonna do so first about the about we've yet so we hit it an over soared smart growth that's what it is and what we mean with that well the fact that it's open source that is the usual suspects right so source lives on get up but you can use that using docker docker compose and communities so we have said all out of the box for you it is smart because of something which we call the confectionery and if you never heard about the context unary that's fine because we invented it and I'm about to explain to you what it is and what it does and I want to use I'm gonna use one buzzword sorry for that but what I what is it that serious it's it a serious notion so sometimes people talk about like the AI first architectures so like can we build systems that have like these machine models built into them that we can build new solutions and ask you new ideas and that's what we try to solve with semantic models so what makes it in our case what we like to call smart is the building semantic model and the graph is well I don't have to in this room I don't have to you know share to people what the graph is but the we we've chosen to use graph QL and the reason why I've done that is because we see of course there's like a lot of graph experts you know you sparkle or sorry for those kind of things but sometimes when we have to fail poor to find a little bit more difficult we see that they really like to work with graph QL so we really embrace graph GL and it's completely 100% graph QL based so that's how we define what our smart graph is and we're the smarter if you can do three things first thing is semantic search so what you can do and that's also what I will demo to you what we know from traditional search if I may call it like that we search for keywords right so if we write about the company Apple we actually need to search for Apple but in refute you don't necessarily have to so if you add company within an apple but you search for example what's the company the business related to the iPhone it will still find your data object company with the name app we also can do based on that it's automatic classification so we can automatically make edges in the graph based on the semantics of your that object and last but not least we can do knowledge view presentation and this is often what's always also refer that's like nowadays it's like it's the knowledge graph but it's like I wouldn't say we're necessarily in notes graph but tea you can create those similar representations so those are the three things the three use cases that we can help you with so something important to share a base on what we did last year so the best way to explain is a little bit on time and what I mean with that is like that we saw of course a lot of data bases in the past there were more relational base right so just you know I wrote column structure work on structures and tables and then of course we got these graph databases so for example had the people from naio are in room so they made a beautiful beautiful database like that and we chose a year back to store our information with Yun's and what we did was that we had that semantic element that but I will explain the confectionary note I will explain what that is we had that as a feature but last year we decided to just double down on that semantic element so we got completely rid of the implementation yarn\u00eds and we created basically everything ourselves and we that means that we now only have that context nary to store that information in and we actually this is a crown so we actually figured out like this is actually we're really happy that we made the decision because now we could really bring something new you know to need to stay something different and a different way to handle your data objects and work with them so we store everything in a semantic space that's what we do and now if you go like wow semantic space I'm gonna explain what we mean with that so but just keep thinking when I talk about semantic space I'm talking about the context in ER so imagine it like this if you go to a I'm compliance as you can see let's say if you go to a grocery store and you have shopping list and on that shopping list you might have four items a banana washing powder you're looking for an apple and a piece of bread so if you go in the supermarket then you find a banana you know that an apple is gonna be closer by than the bread or at washing powder and if you move towards the bread you know that you're actually getting closer to the washing powder I'm moving more away from the fruit that's how we store data in the space and so that's the metaphor to actually imprint the problem that we solve and we do that to something that we that we call the confectionary and I'm gonna give you a little bit of background where we're coming from and what's different to other solutions that around here so if you go all the way back again time to the to the 1950s there was like a famous quote and I said like a word can be characterized by the company it keeps and but it basically means that you would say that the word Paris would be more closely related to France then it would be to Holland for example or the US and New York would be more closely related to the US then for example Spain and it basically went for all words and a lot of research was done there and then we jumped for it and then with the whole machinery boom we saw there was a lot of work being done with with based on machine learning and these were embeddings and then we also sort of us and it was like we got first word to fact got gloss and nowadays had what they call in the academic realm which they call a state of the Arts Cobert but if we now put on our engineering hat we really fell in love with glove and why did we fall in love it laughs because beard has multiple representations of a word in a data set but glove doesn't cloth has one vector representation for every word and the critique that often got was that well if you for example aim Apple Apple can mean of course fruit but it can also mean the company and but we wanted to solve the problem in a different way but it's engineers we were very happy with this because now we could index it we could index those words in a storage mechanism so if you start a we if yet you can imagine it like this it's an empty space that you start you choose a language not a programming language but a spoken language so let's say for example English and the space is filled with all those words so for example if you find Apple you find nearby fruit you find nearby company and you might also find iPhone and these representations that we store they have a 600d mention 'l v presentation which is very fancy but it has to do with compression that kind of stuff and the thing that we did is this if you store a data object like this so for example the class company with the name apple founded in 1976 etc and in when i demo i will show you how the lexie works but if you sell information what we've here does that it creates a string of the words and the concepts that are in there and it takes those concepts and it funnels them down how does it do that it says like it takes the lydian distance it combines that with e it's this sounds very fancy but with the logarithmic function based among the occurrence of the word so for example company might occur less often than the word Apple so then we say the word company is more important and then we even work with word boosting so that you can say certain words are important in my data object but what we do with that is that we create our search our first object in our graph and it gets its own factory presentation so now if we have an empty v8 with those words in that and we store our first data objects that you can see there there it is it lifts that's where it lives in the vector space that's what we mean so now if we query for let's say a company an iPhone it can look in the nearby space and find that that object so now without even having I phone into that object we can still find it and that's the thing that we created then we thought okay this is this is our thing right this is our golden goose egg because now we have different ways of creating graphs and actually query truly so this example for example might look something like this so if I have a data set with companies and this would be my graph QL query where I say that yet give me things which are companies and I'm gonna have their names but explore them by iPhone I might forget this result Apple and as you see iPhone it's nowhere in the dead object and if I have that same data set and we say well a little bit more abstract organize these companies on the concept of Redmont I might get back Microsoft and that's how we structure our graph so and so that's basically what what we view this and as a developer vd8 comes with a few features so the first things that's that confectionery comes all out of the box you don't have to do your training you don't have to set it up whatever it comes all out of the box or basically a lot of the container I should say adding data happens to the HTTP API but calling data to the graph QL API it's completely containerized so you can run it wherever you want and because we use that vector space it's very very scalable you can scale that space very very tremendously big so I think the the biggest one that we ever try it was a few was a few billions it gets pretty big and something that we have in the pipeline and maybe I can show you that next year but is that we also can create peer to peer networks of v v8 so that we can point to semantic elements in different graphs so that we don't have to agree any more on our ontologies or on our schemas but that's like that's that's in the me so a little bit about graph QL so EV so when I demoed it so how we structure our you graph here this is the the UX if you will of our graph QL so we have a get functions first the other one is an aggregate function but I going to talk about the gasp function we have a semantic kind which we make a distinction between singing selections nouns and verbs they have the class the clauses the property a property might have reference and then the property itself and what you can do you can have these semantic filters on top of that so there here you see for example the explore a filter and they can search for concepts but you can even move away from concepts of move towards concepts and I will demo that to you in a bit demo so now we get to the demo so now you might want to said well how does it actually look so what I did is that I spun up a a docker container if you want to do it yourself if you go to our websites and without technology and then you simply click we v8 and then you find all the documentation the installation gives you just a we v8 but what I'm going to demo to you is this news publications that are set and if you click that one there's just one simple docker compose comment that you can run that you can play you know around with it yourself so there's a meta endpoint which I'm just running just to make sure yes so that it's running well so let's first look at a schema so this is an example of schema so here you see I have the class publication and I have I have the name names or the name of the publication but for example as I said the headquarter G location which is our geo coordinates has articles etc etcetera this is how we structure the schema this key that's important because we will see that back when we use graphical to query and the things that we actually store look like this so for example you see an article the article has an ID a beacon is a reference in our graph but why do we call it beacon because we do it in space so it's a beacon in the space and then here you see for example a summary of the article the title of the article and the URL where the article actually comes from so that's how it's structured so we've created a simple GUI that you can use to actually you know look through and search to the to the graph so you can visualize stuff but I wanna I want to dive into the the graphic your queries so let me show you simple query so if I say get thinnies and I want to have I wanna have a publication and I want to see the name this would be valid query that you see rise in folk Financial Times Wyatt New York and economist etc now what I now can do is that I can say well I want to I want to explore for the concepts let's say business and I'm gonna limit it to three results just that it's easier readable so I know as I do the same query but explore based on the concepts of business so if I now run the square you see Financial Times The New York Times International times etc but you see the word business the meditech it's nowhere there that's what comes from the confectionary or if I would say fashion and I would run it then you see it starts for example with VOC so that's how we've structured it and how it works but that's not not only going for like small strings but also for a lot of larger text objects so for example if I would say get things article and I would say show me the title of the article and we run the query so now you see all these articles about a variety of topics you see brexit so you can see when we when we actually pull to that end but it's just a variety of topics in here but if I now say well I want to have those articles I'm gonna use the explore function again and again I say well I want to explore for the concepts let's say music and I'm gonna limit the results again just for the sake of readability so same query but now based on the articles so if a run that's you say like you see fair enough the first one has the word musical but then it's about Gwen Stefani and then it's about John Lennon so you see the word music is not necessarily in there but it organized like that automatically so now even if you want to filter further in this in this graph what you can do is that you can say well the question we have actual how do we do pagination because if you have the 600 dimensional space what would be the next page so what we've done is this so we said well we can actually for example move towards a concept so I can say well move towards the concept for example of The Beatles so I guess you already know what will happen if I do that so and I give it a certain force the force is how strongly you want to push towards this concept inside the vector space so let's say it's a little bit arbitrary but let's say 85% so if I do that now you see that John Lennon the article with John Lennon comes first and if I say like I'm more like a stones person I hate the Beatles then of course you can also do move away from the concept of the Beatles same query and we see John Lennon is done and now the question is like ok so what makes it so like now the traditional graph people are like yeah but I haven't seem to do the graph in action yet so that's just very simple because you could say for example has authors and we can say on author the author is a name so this is how we structure the graph so now you see huh so you see the graph object here that says first the title of the article and if it has authors and on actually the authors that are related to this and them to this article I think if time allows it there's one more second I still quickly show a more thing yes so the Isle is wrong within which I completely forgot because there's another problem that we also try to solve for this and I want to show you that going back to the publication's so quickly going back to the publication's things publication and then I say the name of the publication when when you glanced over this you might have noticed that we have all these publications but we have the International New York Times the New York Times Company and the New York Times in there three times which is a problem because of course you want to represent concepts but in a database we have the same concept represented three times so we have something for that which we can do is that we can say what we want to group concepts together so let me say I'm gonna do the type merge them together I've gotta give a four so how big do we need to look in the vector space before merging and then I can say well do that with 5% so if I now run this query you'll see that it merged together the International New York Times New York Times Company and the New York Times but if I now do a graph query so I say it has articles on article the title of the article it even now merges together those different articles from those different publications into the same concept so that's what we have that's how it works Oh way more features the automatic classification I didn't even get to show you that but you can play around with it because well we're at first M so this software is open source you can play around with it you can set it up yourself you can create your own graphs of cementec graphs I should say that's my story in a nutshell thank you all for listening and if you like it and if you go to our website then I just have one question so this is our website if you go here that you can sign up to our newsletter cylinder or you can click on the get up star button if you want to don't promote a little bit so that's about this is our website thank you very much for listening [Applause] why are you sure about it there's a great question so let me let me start with the first answer to the first question so and sorry if I went to open it quickly too quickly because it's this whole everything where that tolls also on the website in detail but that's what's happening here we always know use the same algorithm so if you would have a data object with a longer sentence like for example the summary or the title the articles that you've seen it applies the same algorithm so it says like first I take all these individual words then I find this center position between those words then I weighed them based on the occurrence so certain words are seen as more important than other so it weighs it towards that and then we have this optional word boosting that you can say in this specific case this was very important so move more towards that that's how we create those vector position so regardless if you're querying or if you're adding data that's how we do it so that's why we also became agnostic about the fact that loves about single words and because we learned that if we saw with the first prototype we did way back what's very simple we say okay I have the word Apple show me what's nearby and then as you would expect as glove does it says like well iPhone iPhone but I also found fruit and then we did something very simple which okay now go and sit in the center between Apple and iPhone show me again what happens and then you see actually that that successful and if I if I may I can actually quickly show that yeah I'm the lost okay so like ship I should have waited Elliott said go ahead and thank you so there's a confectionary endpoint where you can say well concepts so if I now literally do what I just said so if I would say Apple then you see your Apple iTunes Google preview on and now of course in this example we don't see of course the fruit but let's say if I would do Apple and not based on the company but on fruit so I concatenate them Apple and fruit you see how it now starts to get better and better in those results so and that's how it the algorithm works so you can play around with this also yourself on this on this end point and the other question yeah sure so we were of course we also liked half we were also a business so we have like so the core is like open source but we built like a shell around that so we currently have six companies using this on a large scale in a variety of industries also retail oil and gas all those kind of things and these graphs get pretty big and especially if you scale the communities cluster its fast which is something I'm now I could say that we fence li-like architected debt but it was actually something we got for free because we just the Dynamo is just vectors only vectors and that's of course very fast to scale and to search through so the answer that question is yes I don't know and I love the idea so we're definitely going to try that out I don't know I don't it's a great idea we haven't we haven't tried that yet so what we currently do is that we just have we've hidden in a language so darts French English of course but we haven't tried that yet so if you don't mind then it would be fantastic to or of course you can try this yourself or we can do it together but that would be fantastic it's a it's a great idea and I don't know thank you I hope you all enjoyed it enjoy Boston tomorrow and show the evening of course thank you [Applause] ", "type": "Video", "name": "weaviate_vector_database__fosdem_2020", "path": "", "link": "https://www.youtube.com/watch?v=3NfcAF4qm2k", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}