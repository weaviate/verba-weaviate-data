{"text": "Weaviate v1.15 Release! Thank you so much for checking out the Weaviate podcast -- here is a summary of what is new in ... \nthank you hey everyone thank you so much for checking out a new weviate release podcast releasing webiate version 1.15 this release is shot with all sorts of exciting new things from cloud native backups improved stability for high memory setups faster Imports for ordered objects more efficient filter aggregations two new distance metrics two new Eva modules and then smaller improvements and Bug fixes so it's a long list of exciting new things to me I think it's so interesting seeing these Concepts in computer science like red black trees and how do they impact database systems how has it been added to alleviate and all these other exciting things Eddie's written this awesome article on the go mem limit and how that's improving the stability for high memory setups and then this Cloud native backups so Eddie and thank you so much for coming on the web podcast to discuss uh version 1.15 hey Connor thanks for having me yeah it's a super exciting release um so many cool new features when we initially set out to to build this 1.15 release uh the the main plan basically was around backups but now we have this long list the features and I think every single one of them is a great reason to to upgrade but yeah let's maybe start with backups uh there the the kind of Point yeah everyone needs backups right I mean it's kind of a requirement to run your your database in production or any state full loads such as database and uh you could do backups before you could kind of do them before because you could do them at an infrastructure level basically manually deviate writes its files on disk so you could take a snapshot of that particular disk and store it somewhere dude with your your cloud provider but that that's not a really smooth process this would basically would give you a vendor lock-in for that specific vendor so you couldn't really migrate you would be kind of left or you would leave vv8 in this kind of weird state where it wasn't really ever prepared to to take copies of files or unrestore you would get these like weird error messages did vv8 crash instead trying to recover um yeah so it was It was kind of possible but it just wasn't a very good user experience and deviate is all about user experience so now we've kind of done the exact same opposite and now we have a backup feature with the user experience that we're absolutely proud of with the the 1.15 release you can backup and restore it to any a cloud provider or to your local file system so we have support for AWS S3 or any S3 implementation actually doesn't have to be the AWS one it could also be an open source one for example um for for Google Cloud GCS for a local file system and basically these can all be be plugged in using v8's module system and then you can just do a single command we have support for this and all of our our clients and all of our language clients or you could just send the raw HTTP request to start a backup and and that's it basically in bb8 will do all the cool things in the background and it's been engineered to be minimally intrusive basically so you can keep using Vivian in fact we encourage you to to do this in production and basically keep running vv8 in production send requests have your users use the machine they can even send right requests in the way that it's architect architected without any kind of impact and yeah the backups will just run in the in the background and then you can restore them either onto the same machine or even onto a different machine if you like yeah and I think that the ux the design and the documentation that shows you how to do it is so well communicated and really you know massive credit to how you've written the how how we V8 stays running all the time how you can select certain classes and I want to get right back to the ux but this little story about not backing up your data I really like this uh this HBO show Silicon Valley I imagine like a lot of our listeners maybe have seen this show and enjoyed the show and there's a scene where where Richard is he hasn't backed up the data they have like this insurance client Dan Melcher and and they're all panicking that we you know we have no backup of our data in them and it like they're they try to like migrate their server to Stanford and it falls apart but then the systems back itself up on the refrigerators and it's this really funny scene in that show but kind of communicating that urgency of like you need to have your data back exactly you do and nvv it gives you a proper way to do that now no refrigerators involved at all yeah and so one thing about the ux is I was curious about this notion of backing up specific classes so I imagine I have like 60 classes and I only want to back up some of them and the ux makes it so clear of how you include certain classes and can you tell me about the design decisions behind that part of it yeah yeah absolutely so this is actually uh mainly based on user feedback so the entire feature was uh developed around users input like of course you you ask your users before you write the first line of code um and then you try and try and get feedback and this is something that that was very very heavily influenced from Community feedback so basically the idea behind the the backup system is I I think there are so many use cases that you can do like you can even use it to to migrate between environments and these kind of things but generally I see two main use cases for a backup like one is the disaster recovery case where basically the disk is lost or or someone deletes it or this kind of basically to to take all the data be very agnostic of what data is on there and just take the whole thing basically back it up and restore it that's one option the other option is basically more at a yeah logical level where you know what your specific classes mean and you maybe you want to create a backup not so much to protect against Hardware failure or or cloud provider failure but against user errors so for example um a user could accidentally delete data just through the API so if you want to protect against that then you would do a sort of uh yeah I would also do a backup but it would be more to to protect against user actions and more this kind of application Level backup and there you might not want to back up the entire database for example of course vv8 can be used for for all kinds of different use cases and then I know of some that use classes basically as their their isolation unit for multi-tenancy so they would have their own service that that runs for their customers and each customer would produce one or or not Direct but basically their application would would create one or more classes inside VBA that belonged to exactly one customer so maybe they have a requirement maybe for for um sock 2 or gdpr or something that customer data needs to store be stored in a specific way so that would be one option for example to just create a backup for a specific class and the API is designed in a way that gives you three options basically you can provide no information at all this way it would just take a backup of the entire instance with everything that's that's on there or you could set an explicit list of classes so then basically only what's included in the class would be backed up or you could do the other way around where everything except basically we have this exclude field and you can say like okay um don't exclude this or don't include this class basically exclude this class because it's sensitive for something and then it would create a backup of all the other classes um so see I said like different classes might be you need more secure more like let's back this up every hour or so and then other things it's like well we need to back this up put it behind some security because this is particularly sensitive yeah I think that's really interesting great Point yeah think about all different the timing timing plays a bit role like you could have a class that rarely changes but is maybe very large why would you back it up all the time or you could have another class that yeah that frequently changes and you need much more frequent updates so yeah that's also a nice way to decouple this a bit basically and just give you all the flexibility to to Really um yeah from a technical perspective it's just a single call but you can do a lot with it you can customize it in whatever way you want uh one thing I'm kind of struggling to wrap my head around is um so when you back it up what is the kind of state look like and does it differ from like S3 because S3 is just like a file storage thing right or uh so when it's backed up is it like uh Json or file of data what what is the backup thing the the backup is almost an exact copy of um what you would have on your disk in VBA normally so it's basically the the binary file formats for for the LSM trees for the the hnsw index for other index types that we have so so for bm25 for example there's a a simple index that tracks a property length because you need these like average property lengths for the vm25 uh combination and and other basically metadata that we need for for maintenance so it's almost an exact copy of those files and then there is a snapshot I think it's actually called snapshot Json but that's that's basically just an implementation detail that gives you a bit of info about the the backup itself so it's like what what kind of machine was it running on um when was the backup created what's the the ID of the backup and these kind of things and that's basically the the metadata that that makes this backup a sort of self-contained backup I would say and and what that what I mean with that is basically you could create so let's say you have two instances one is your production instance um and the other one is let's say your local instance which is completely empty and now you want to migrate basically using using backups your data from the reduction instance to the cloud provider and then basically pull back the the um the backup from the cloud provider then all you have to do is send the request from the production instance put it into let's say S3 in this S3 bucket and then on your local machine as long as it's configured to to point to the same S3 bucket you can restore that exact same um backup just by specifying its ID and the the local instance doesn't need to know anything thing about your production instance so you don't have to do any like pre-configuring basically the the backup is completely self-contained with everything that it needs to to be runnable this so one thing I'm very curious about is with say the collection of demos and on uh GitHub we've yet examples we have things like you know uh clip demo with the with the UI and searching through movies searching through wines all these examples and I love this idea of getting wevia demos of here's a graphql API that starts through Wikipedia search through archive maybe complete with the UI also with with this backup because I'm thinking about how I think right now you clone it locally and then spin it up and have it running would instead just kind of restoring from a backup be an easier way to run the demo yourself yeah absolutely so so basically because the the backup and restore is yeah it is just copying of files that also means any kind of index building that you have to do that's already that's already contained um so let's say you have this this massive instance where you have hundreds of millions of objects and took you maybe over a day to index it if you create the backup the backup is just copying the files expensive we have all the the index structures that have already been built and then if you want to restore it so let's say you want to yeah run a demo case and and we've basically just put the demo data set into a backup and you restore from that that would yeah that would make it a super fast process because you you basically just wait for the time it takes to copy the files and once they're there you can start it up and you can use it yeah wow that's incredible then you avoid the import time and just back up wow so I think that was a great coverage of backups and for people listening we have you know these seven different things that I outlined in the beginning that are going to be chunked up in chapters and Dirk is going to be coming on in the second half to discuss uh the red black trees in this thing but the next Topic at Ian could we talk about the go mem limit you've written this great article how does this help leave you yeah thank you for for bringing up the the article um the the go mem limit is one of those features where we really benefit from the goal language Community being active and improving go itself so a lot of the improvements that we do in viviate they're typically code that we ride where we've done something that maybe wasn't ideal before and now we've we found a better way or we've just improved it in in general so so for example the the um memory uh improvements or allocation improvements for filtered aggregations that you're talking about with Dirk later um they are are something where basically we've just written better code but go mem limit is one of the those few things basically where we didn't have to do much other than um compile the binaries for um for the new 1.15 version with the latest version of go because this is a new feature that go actually introduced and go memed is basically a soft memory limit and soft means that that go cannot prevent your memory from growing beyond that limit so it's not a hard limit basically the kernel has a heart limited does an out of memory kill if the memory is exceeded and that's basically that's the hard limit but it's it's a Target and a Target is super important for a um for a garbage collected language because by default in go the target would always be twice of what you have so to put that in simple terms let's say your your application currently uses two gigabytes of memory and and you want it to use to give us like that's not accidental or or temporary allocations but that is let's say you want to keep your vectors in memory and this is two gigabytes of vectors then with that Target being to double so it's called 100 in in Gold language basically it's a go GC equals 100 and that would mean that your instance can grow all the way up to four gigabytes and that's relatively small numbers so that's that might still be okay you might have a four gigabyte instance but now let's say you have this massive setup and your stable memory is 200 gigabytes most likely you don't have a 400 gigabyte instance just to have a buffer for for temporary allocations you would have let's say maybe 20 overhead and you would have a 240 gigabyte machine now if you set your your um uh go GC instead of to double to 20 that would work for this particular case but a you have to know exactly where you end up which is the 200 gigabyte you have to know exactly how much you have and that means that to get there you already had a basically very aggressive garbage collector because it was set to to 20 which means you've spent a lot of time on garbage collection which might not be something that you you would want to to do and go mem limit basically is the the missing turning knob here it's actually not turning up to maybe sort of sort of a knob that turns itself because it basically turns the the other knob the gochi Cena which basically in a um in a single sentence the closer your memory usage gets to the Limit the more aggressive it makes the garbage collector so in the beginning on your 240 gigabyte machine you have all the memory in the world there's no reason to save memory so go can for example double the Heap every time that's fine but at 220 gigabytes or 200 gigabytes with 240 gigabyte limit you can't double anymore and this is exactly what go limit does in the background it would then make the the go garbage collector quite aggressive and then you would never sort of run out of memory again when you think you shouldn't have because you've calculated your stable memory requirements and it was all fine and um yeah long story short all you have to do is set a single configuration environment variable which is this go mem limit and you just put it to however memory you have and if there is a way that the go runtime can can prevent it by basically making the garbage collector more aggressive it's got your back and it will will take care of not going over there what it can do of course is if you if you keep on importing and you have vectors that require 260 gigabytes of memory on a 200 gigabyte a 40 gigabyte machine that will still run out of memory but for those cases where you kind of accidentally ran out of memory it will prevent those and I think that's that's super cool for having to do nothing but but yeah upgrading to the latest version and setting a new environment variable yeah super interesting I I'm not going to claim to be a knowledgeable about go really but I've seen like these Cuda om errors where you try to put a big gradient through the thing and add a memory programs crashing and so I can I can Reason by analogy there where you're trying to import too much data and then it crashes compared to this thing that can manage it and I I feel like that's such a huge thing that things would say um like gradient accumulation in deep learning is just kind of an analogy that I'm going to be using to reason about this like the way that it prevents you from the oom error has been such a game changer with training big models with you know big batch sizes and it sounds like a very similar thing with this and so from my understanding it sounds like um you know with the aggressive garbage collector it might slow it down when you get to that limit maybe because it's uh so I think it's what you do is maybe you over you have like a 32-bit about like placeholder right the temporary allocation is trying to kill those to make them for more data yeah yeah so the slowing down is is kind of an effect what it does is basically it speeds up the garbage collector itself so so basically the garbage collector the or the point of the garbage collector is to to for for any kind of memory that was used temporarily um in a garbage collected language is not freed immediately basically the garbage collector just runs in intervals and collects all that memory so that it can be reused again and the longer it it doesn't run basically so the longer the pass between two cycles the more memory piles up and if you're getting close to your memory limit then it basically has to run more aggressively and that kind of automatically does that trade-off where you only have this many CPUs and if your CPUs do more garbage collection that means they do less other things so yes you you kind of trade off a bit of of compute uh Power for more garbage collection power and and therefore prevent running out of memory if it's preventable so does this help with say uh like large-scale load tests get a billion vectors into Eva does this kind of thing facilitate that yeah yeah exactly so so the the higher your your regular Heap use or I think I use the the term uh long lift versus versus temporary so basically the the higher your long lift Heap or memory usage isn't that is exactly what happens in in such a billion scale case because those billion vectors they will use I don't know including the index something like one or two terabytes probably depending on the dimensionality so I think that the Sif 1B data set is something that we've played around with and I think it's it's a billion uh vectors at 128 Dimensions um yeah and that's that's four bytes for per Dimension so that is not going to calculate them in my head on the podcast but that is a lot and I think if you take into account the the um the space for the index itself we ended up with something like one terabyte or 1.2 terabytes and yeah if you have these kind of kind of massive limits then it really helps if you can set your go mem limit so that the garbage collector doesn't need this massive overhead because even just 10 is 120 gigabytes at 1.2 terabytes and that's a lot wow yes super interesting you've got three main topics to discuss on improving the performance of Eva so Dirk thank you so much for uh coming on the podcast to explain these new ideas yeah thank you for inviting me happy to be here so could we start with uh what is a red black tree how's it different from a regular binary tree and how does this help with vva's performance uh I would just stop by suspending what the binary tree is so let's say you have a bunch of numbers and you want to find out if a number is on your list and um like classical approach would be you just have a long list and you go through it and check everyone at every number and that's slow because you might have to check each entry before you know it and what you can do is you um sort this into a tree so let's say you have numbers from 0 to 100 you pick a middle note let's say 50 as the root note and then if you want to check if 7 is in your in your tree you go in and say okay yeah fifty Seven is smaller than that so I take the left entry and the left entry might be 33 and we go okay seven is smaller than 33 I go again in the left three until you are at the bottom and they're found to seven or not and that lets you check if the number is inside with a lot less steps then compared to just going through um to a list now um if you create these trees you can have problems um if you enter objects in order let's say the tree is empty and you start entering entering numbers in it and let's say you start with with one then one is your first object the root of the tree then you enter two then okay two is larger than one so you go to the right and the right child of one is now the two and now if the three comes you go in three is larger than zero and so larger than one go to the right note which is two three is larger than two you go to the right and enter it there and you can continue on that until you're at whatever numbers you are and then your binary tree is basically a list attitude to find any object you start at the first and you're the right to write the right to write the right right until you're there and so it doesn't improve the uh performance when when looking up things and also when writing objects into the street and now the uh well black tree is a self-balanced tree so there are a bunch of rules associated with the tree and then you rebalance the tree when certain things happen as of course the example for example the example I just gave it's you have then you enter the tree so you have done three nodes on the right and zero on the left so you're just like this rotate it around that the two is the new roots and then you have on the right the three and on the left the one and then you have a gaps of one instead of a depth of two and that makes it then faster than entering and reading notes from it super cool and yeah so around the 1.15 release I will have a blog post it'll probably be an in description of this video and it's released that'll visually describe this concept of how you rotate the trees the red red black coloring and maybe you're aware of things like visual algo these these ideas of um you know the red black coloring how that lets you balance a binary tree before we go a little deeper into how binary search trees help with databases to look into IDs can you tell me about your experience implementing this in weediate um like how what what do you change what kind of like in the code base the kind of Open Source database engine this kind of topic um so we already have a binary tree in there where you when you insert the objects at first and um basically the change was was limited to the existing binary tree implementation that we already have so basically I implemented checks are the rules um I just can't think afterwards is any of the rules are not developed anymore and if yes do the appropriate rotation changing values around um to to have it balanced so the red red reflectory itself is an idea I think from the 70s or the 80s so it's around since quite a while um but it works great so yeah so all this is implemented in go have you been how long have you been programming and go what was your journey like learning it two months so I started I started working on really AIDS um yeah early July and I I started I don't know maybe two three weeks before it was my first girl college um but but it's just surprisingly fast like I was um answer it first I'll just put the government like it's really quick that you get you took out that's a great experience super cool I think that's really interesting that it takes took you two months to learn go and this kind of Open Source database you know upgrading little Parts like binary tree to red black tree these kind of updates can we can we maybe step a bit into the motivation of the red of the binary tree uh so so we say we have these uuids they uniquely identify every item in our database could be images you know passages paragraphs whatever you put in your vector in the web Vector database so how does so the uuids they get sorted in in the tree is that the key idea for when you like how you use this kind of structure um would be an example let's say you have a uid that is um just a counter that goes up and then if you insert it you would get exacted behaviors I described earlier that you insert the first object that is your root insert the object that's your right side that object it's your text right child and so on and that just really degrades the performance when you add objects and makes it makes it very slow because every time you use an object you have to go through all the objects you added before before you can write it and for the next object it's even longer so it gets really slow and yeah so I kind of have two things I want to take this in but first so so say um you know if you're looping through your data in on the client side say you've got python code and you're looping through the data items and you're using that Loop counter to construct the uuid is that the kind of idea where your one two three four and such that you get this right except for anything going on the right now does this also play with say we have an inverted index and we have uh I don't know age uh or let's say some numeric value with a broader range than that let's say maybe uh um maybe we have uh income and some database I don't know we have a big set of numbers in the inverted index does the binary tree also help with that kind of lookup uh I'm sorry I don't know that like as I said I just started so I I don't fully understand the journey sorry let me set the stage better that was a bad example so so with the inverted index it's like um those the foundational idea is say you have a textbook and you're looking for biology you go to the back of the textbook it says biology is on page 99 page 305 inverted index so um so I think with numeric values we can also kind of have this uh inverted index where say let's say it's age and it's one to a hundred you know you'd say 80 and then your database is indexed where you have 80 and then the customers or whatever it is that have that value 80. could we imagine numeric values like that his age say it's like zero to 100 I think you need something like a key that identifies an object so you can like you can store as a value whatever you want so you could have 50 values with a given key um but I'm I'm not sure if you could use it for for what you have in mind so in general is you have a bunch of objects that haven't have a unique key and you want to insert or get them out of a structure quickly that's I think that that's where you use these these binary trees and red black trees are then if you have um ordered rights helps you to keep the performance up right yeah I can certainly imagine how to say um with wevia when you're doing a near object search and you pass in the ID of the object to reference the vector of that object having this kind of structure to quickly get it grab the vector search with it obviously sounds super useful yeah I think like I'm not totally sure about this but there is this I've got what is it called uh h n s w and did I get that right um it works kind of into the same direct obviously it's a lot more complicated but in I get I would say the idea behind it is similar but obviously it's more complex and um helps you to to find find it but this is like the implementation where I work on the 3D to to you get objects into vv8 you first store them and then you do other things later it's I don't think it takes part in the in the search or in the indexing data yeah that's such a fascinating connection between say tree structures for fast search or the proximity graphs hierarchical proximity graph structure of hsw the way that we have these like symbolic structures to organize data for fast lookup super interesting and I guess my understanding of it is you know if you do a near object search where you pass in an ID you can use the balanced red black balance binary binary search tree to quickly get that ID quickly get the vector then you take that Vector on the road to the hnsw index to now do that approximate nearest neighbor but so kind of stepping out of this topic can we now talk about the second topic on our agenda um fewer memory allocations on filtered aggregations could maybe begin with the filtered aggregations and and kind of understanding the current memory overhead and then kind of what's been done to help with that yeah um yeah so maybe we do it for the aggregation so we're looking for for a value that's for example Nia another like sorry an object entry in the database it's near another entry and um there was a lot of overhead involved in temporary allocations while doing this um I think something like so we we did a benchmark where we added one million objects to bb8 and then aggregation rate for example would just count how often is this property there and with this we had I think 200 gigabytes of just temporary allocations that would happen while looking for by checking how many other and um yeah there were a lot of different inefficiencies I would say in the code that that you could optimize um in general like this the the stack and the Heap um where you can store memory and go well in general and um if you start on this you can store things on the stack where you a know how big it is and B it doesn't escape so let's say you have a function you allocate some memory there and then you return it up the stack to whatever column you don't know what's happening afterwards so you need to have it at the place where it can dynamically grow or Shrink or the the lifetime is not limited and so it goes onto the Heap uh if you have a variable that you create in a function you only use it in that function and afterwards it's gone you can put it on the stack where it's much cheaper to create and to uh read and um so I would say there were three or four different patterns that I noticed the first one is um we the the objects we store them in Long byte areas so it's just a long area of bytes and then if you for example want to check or I want to read all the properties from it um then there was a library that was used it was called second binary.read I think and this just created lots of allocations for this process yeah so if you say oh I want to have um this is the the buffer from which I'm reading where the object is stored in I want to read the first 20 bytes because they mean the the ID of the object then you could tell that to the to that library and it would give you those first 20 bytes um and would create a bunch of temporary allocations by doing that um it's nice to use but that makes makes it slow and so I basically created a little low level library that just um reads from underlying buffer how many bytes you want without any temporary allocations and that's yeah then use that library at different places and and make it yeah somewhat user friendly um [Music] yeah yeah so I'm sorry to be you know I'll be re-watching this and hopefully wrap my head around the full the idea but um maybe so a filtered aggregation is say we're we have a hundred million passages in Wikipedia say we're trying to uh average out the word counts of them or something like that right is that it so that's a filtered aggregation uh yeah so so each Wikipedia Passage uh with its respective properties like word count is stored as like a byte string yes so you can better hash out the thing that you're trying to filter on um so basically you know do you have to hold object like the complete object is one long bite string and there you have to to get to the to the right play so you you basically get that long thing and then you know okay the first eight bytes are the ID of the object then there's a bunch of other things and then there is um let's say the length of the of the key of the object so you first you first need to go to the place where the key is that you read the key then you know okay the next 53 bytes are the key if you need it you read those if not you just jump over it and then you do that until you're at the place where the information is that you're looking for and basically with the old library you had to read everything and every read created many allocations and now this new library basically allows you okay um I want to read like I know the next value like the next thing I'm reading is let's say an unsigned in 64. so you do reads unsigned in 64. you get that value back and the internal pointer the points where in the buffer eyes jumps forward and you do that until you are at the right place in the buffer where the information is that you need super cool so it sounds like it would be especially useful if you have say a lot of properties like data objects that have several properties but especially benefit from this kind of technique and maybe one more thing I'm missing in my understanding is so so the temporary allocations so is that um so you copy the whole thing could you take me through it a little a little more sorry to be okay okay let's start from a bit let's say we have a an object that and the the buffer where everything is stored is 10 000 entries long and then what was done before is those 10 000 entries were given to that binary dot read library and they would say Okay read me the first four bytes read between the next 20 by three million next five bytes and so on and each of these calls the library created in the library so not in our code but in the library code a bunch of temporary allocations where they just created buffers for whatever reason like I haven't looked at their code so I don't know it and um basically what's happening now is you have two choices a is move the position inside the buffer forward without reading anything so if you know all the first 80 bytes don't matter because it is information I don't need you just say move position forward and the internal number that says omx by 15 is just moved 80 by 80 entries forward if you then say Oh I know now I'm reading uh answer integer 64. you call the read answer integer 64 call and it then reads the next uh eight bytes interprets them in a way that doesn't use any temporary allocation and returns you the value so having that Precision on um knowing that you're going to read a 32-bit value that that reduces I think that makes sense I so that reduces the uh because otherwise you need to allocate the 64-bit no do you always no um you always need to know what comes next so you have a you have a format and you know in in my long long white area that describes the um the the object like the first eight bytes are some ID then some other thing comes then this comes and then um the length of the t comes which isn't unsigned 32-bit value and so on and um so you always know what to read it's just are you doing it in efficient way or unefficient way basically and this this library was doing in an unefficient way I don't know why and like I haven't looked in that code but it um yeah it probably does a bunch of other things and for example it works for files and for any buffer which makes it really nice to use you can use the same functions on uh like reading a file and reading some byte area probably all's Network also I don't know that and now I basically I created some specialized function which does a subset of the the spinal.read library does but for us it does what we need at this place and it does it more efficiently wow so so I guess the high level takeaway is that you can you know do these aggregations with less memory requirements but yes I'm so fascinated at how you're able to kind of go into the database code and make these modifications even with you know two months ago and replace this library with this thing it's really opening my thinking of about this kind of like open source database and the different components of the databases that you can improve on you know from The Balancing trees for the sequential rights to the memory allocations on these aggregations is super interesting uh so we're not going to do our third topic um so with less memory construct consumption with importing many parallel bashes I think um you know just importing data I love this topic trying to better understand it myself so I'm so excited to learn about what's new with this yeah so um what we were doing up to now is uh like you have your clients and then you for ads data in batches to to vv8 and you for example do it over Network so you just have five clients that you run in parallel that each send their batches to deviate and that can processed there and uh what was done up to now is that for each batch that gets imported a number of go routines are started they do the actual importing so if the first batch arrives um we will start basically a number of CPU cores go routines and those girl routines then work through the to the objects that are in the batch and and add them to the actual database and the same happens for the second and the third the fourth and so on so if you have 10 batches in parallel you start 10 times your number of course goal routines that work in parallel importing those objects and um each worker has a memory cost associated with it so every worker that started um has certain certain things it does and does it memory so if you have then 10 batches you have that 100 times and um so I've got that number from etn I haven't measured it myself but like he tried to import many objects I think he like I think he said the billion but I'm not totally sure if it was a billion and at some point on a really big machine um like 40 of all memory that's used was from these workers and at a certain amount of time a certain point like the import doesn't get faster if you add more gold routine so you just add more memory that gets allocated and used but the total throughput doesn't get higher anymore yeah wow this is incredible the I mean the thinking around if I want to import a billion vectors into vva that I can distribute it across say 10 like uploaders could we talk about that idea a bit more so um because it would speed it up it kind of reminds me almost of like distributed training with deep neural networks and the same kind of you know distributed data uploading what else goes into a distributed data uploading with I mean imagine the red black tree balancing it isn't it probably needs to be some kind of synchronization step right like I mean the the that's a different point so first you you kind of Center the objects to vv8 and then you um put each object into vb8 and um and you have clients let's say the python client and that just has overhead and you you can't send the data from one client fast enough to to saturate like a big server but you have BBI running and so if you instead of just having one client sending data you have two or three clients then um you come to a point where you really use up all the resources that the the um the the server has so in my local test on my machine there is no real point in having two or three in parallel that doesn't make it like maybe a little bit faster up to a point but then it gets lower but if you have this this remote setup so you're sending over over Network you have latency you have throughput issues then it can help to have multiple batches in parallel so so maybe we could um Step through so the the memory overhead of each of the clients what what is it that the clients need to store that creates the overhead oh it's not the clients but um it's the these workers that take the object and actually add it to um to to vv8 and they have some temporary structures where they where they uh like that's one of the parts I have looked at so I I don't know it yet but um they have some temporary errors where they check oh um I already looked at a bunch of other objects and now the new object is different but yeah I can't really expanded the story about that one and um so basically what we've done is now instead of starting a number of go routines with each batch that comes there's a shared a pool of girl routines that that works on the object of all batches so let's say you have 10 clients sending data you're sending batches in then all the objects from these batches get added to one queue and then the shared workers just take the first increase at the database as soon as one is done it takes the next one so you always have the same number of of workers adding objects and it doesn't like the number of broadcast doesn't get up uh doesn't scale up if you have more batches and so you you are limited in the growth of of memory usage so if you have many batches because you don't have this many parallel operations that are going on could you tell me Limited in the um in the memory of the of the patch can you explain that one a little more yes um so each worker that adds the objects to the database create some temporary structures as I said before I'm not totally sure what they contain but they they need some memory to do their work so if you have 10 you have 10 times those temporary structures if you have 100 you have 100 times those temporary structures so um if you know say okay I'm limited it to 10 or 20 workers that go in parallel you limit the amount of memory they use for while importing it and um probably that as kind of a trade-off so if you have more goal routines up to a point it gets faster let's say you would change just one worker one gold routine work in parallel then it would be much lower then you have five if you have a five core machine but if you are going to 50 then on your five call machine the 50 is going routine won't help you adding things faster but it will still consume memory but you can't use any other things so the the client is Freer to go get the next day so the network part is faster and so when you have 50 that um no like the the clients stay the same the networks stay the same what's different is how deviate like in deviate how vv8 accepts the objects and then adds it to the database so vva test instead of having a hundred goal routines working paralleling the object that only has 10 or 15 or whatever you put in uh okay because it needs less temporary structure yes you have less gold routines working parallel and deviate so you have less of these temporary objects adding uh use so you have less over memory consumption so then as a result can you add even more data to it like um it scales a bit better if you have um if you have many batches then it's a bit faster not too much but a little bit I think the most important thing is if you're adding lots of objects in parallel then just your memory is is growing so much that you're running out of memory at a certain point and like this is this is what goes away or it's lower so you basically you can import more objects with the same amount of memory super cool so so I think that was a great coverage of the three topics of you know self-balancing red black trees to improve the sequential rights fewer memory allocations and filtered aggregations and as we just finished up on less memory consumption when importing many parallel batches um so quickly touching on a couple other uh topics in the 1.15 release we have two new distance metrics uh hemming distance in Manhattan distance uh Eddie could you quickly explain what they are and how community members have contributed this yeah yeah so so first of all the fact that Community members have contributed I'm super proud of that fact because for an open source product it's so nice to get these kind of outside contributions and um yeah users having those use cases and knowing that they they need them and then um they don't just have to put in a feature request but they can also just do it themselves that's really nice and it's really cool to to see that that kind of community usage uh yeah so so Manhattan distance that that's one that I haven't actually used myself before um but I like that I like the naming I think an alternative names like taxi cap distance and the idea is that you you have a grid or like you City grit as you would have in Manhattan and you can't just walk straight through a block so basically the only way to navigate from point A to point B is to actually walk the the uh the grid basically in the grid then would be your your axes in the coordinate system and that's basically what the distance is so instead of taking the diagonals you would in in euclidean distance you just take the sort of walk along the axes and um yeah I have no idea what use cases are are there for what models use them but apparently there are some because community members like them um then Hamming distance is one that I'm a bit more familiar with because that's also also used for binary passage retrieval um we're basically Hamming distance just gives you the different uh bits or bytes or depending on on what it runs on if it runs on numbers then gives you the difference between the numbers of each vector and um yeah on binary passage retrieval the the general ideas that you compress this entire Vector into this kind of binary thing and then instead of let's say with cosine calculating the entire angle basically you just compare you just walk through all the bits and see are they the same are they not the same and if all the bits are the same then the distance would be zero the vectors or the vectors the binary vectors would be identical and basically the the worst possible score that you could get is that every bit is different um so I guess it's in computer science terms it's basically an xor kind of calculation between the the two um vectors and uh yeah if all are different than your maximum distance basically the number of bits or if it's on a number the the vector Dimension and then that would be like the the most farthest apart vectors um yeah um binary passenger retrievable Ascent would be one of the the use cases yeah the new distance metrics are so interesting I think so I think we've covered five things now with uh you know cosine similarity being the foundational one and then kind of dot product where you remove the normalization from cosine similarity and euclidean and Manhattan which are similar kind of analogs of each other and now this Hamming distance thing which I think is pretty unique I remember doing bioinformatics classes where you look at like the RNA of mutated coronaviruses and so you'd have this edit distance with the acgt vocabulary and so the edit distance and yeah the maybe the binary passage retrieval may be looking for these connections and you know Eric has been really studying these distance metrics and trying to prescribe things for webia users like you know when to use which one when and so looking at things like accuracy and speed I think you have the little reasoning around the speed where the Manhattan should be faster because you just have the differences you don't need to square them some of the dot product you don't need to normalize them so it should be like a little bit faster but generally I think the prescription is uh you know just try it see and see what your feeling is our current thinking so we also have these uh two new hugging face modules or two new modules that I think integrate with hiking face correct me if I'm wrong and saying that they're kind of hugging face modules but can you explain what I knew about the new Wii date modules uh yeah yeah so one of those is actually uh completely hugging face specific so hugging face has an API where you can basically dissipate service from from hugging face and uh basically they host the model for you and they do the I think it's called the the hugging face inference API and that's exactly exactly what it does um so with vv8 modules or before all that you could do with with eBay modules basically spin the module up yourself but if you maybe already have a hugging face a subscription you just want to integrate with that because you're yeah you're already using that and you're happy with it um the new module basic features gives you an integration point to their API so this is we call this from the mediates perspective it's basically a third-party integration um because it's it's from a technical perspective it's just a network called to um this third party API um but yeah it gives you completely new flexibility because every model that is supported on unhiking face um you can run it through vb8 and um you can basically make use of all of the optimizations that the hug and face team has been been doing already so if you are for example self-hosting vv8 but don't want to self-host the inference part that would be one option and of course also with the VBA cloud service um you can integrate it as well so you can if you want a fully managed uh option then you can integrate as well and have basically the database part being managed by semi and the um inference part being managed by hugging phase and then the other module that we have which is yet another Community contribution which I'm I'm super proud of um it's a summarization module which basically it runs at at runtime or at query time so similar to our q a module for example um which I think we call the category readers and generators um and yeah basically it takes your results and if you want to it can summarize results for you into something into a shorter segment or so so if you um run your search results for example on very large documents but you want to preview them let's say in in yeah your search results page maybe that that's a very simple and very much search related example you could just generate such a summary on the Fly and display it awesome well thank you so much Eddie and everyone we have again this recap of cloud native backups improved stability for high memory setups faster Imports for ordered objects with this self-balancing red black tree more efficient filter aggregations and then the two new distance metrics and two new Eva modules uh so I'm like a kind of like a MC of a show now so following this this is the conclusion of our version 1.4 15 release uh now following is going to be some more information about Dirk uh how did he become working with edian on these performance improvements what is his background like what led him to be thinking about these things so I I hope you'll be sticking around to watch that part as well thanks for having me Dirk I'm really curious like how how is your kind of career development been that you you know you can identify these performance bottlenecks and develop things like this um I'm I was a physicist uh so the the physics Mazda PhD individual postdoc and that basically wrote um simulations about various barriers topics and it's just natural to to look for these performance optimization when you write the simulations and then my my last job was at Deutsche Barn which is like a train company in Germany and uh it's part of a team that wrote the simulation for microscopic train simulations so um yeah I did a lot more more architecture and general design things there was always this performance thing in the back or a part of of the work can you tell me a little more about uh like simulations simulation code and and kind of what what the experiments look like for that uh it really depends so let's let's I would I would go to university again because I think I'm not allowed to tell too much about my last job um so I did there was like cell simulation so how to sell moves how do they interact and so on and um in one of these projects for example we had a lot of of noise how the cells would move so that like we we were looking at the collisions between cells and um you know you have them on a 1D stripe and they collide with each other and we had a little bit of noise so they don't always go head-on but like this sometimes like this sometimes so they rotate a little bit and so you need a lot of um with the same parameter sets you need many runs so you can do some statistics if you just the wallet could be random the outcome but if you do a thousand for this parameter setting you know then oh this place happens 20 times this case happens 50 times then um you can put a statistic and you can compare different parameter sets so your code needs to be fast enough to like explore the relevant parameter space and have enough runs so you can build up statistics and so yeah it needs to be fast enough for that well yeah can I ask about um what inspired your interest in we get uh it's a cool project so I I talked to this with Etienne about it and it sounded really interesting and um I thought like I did simulations for 10 years 12 years something like that and it's time to move on and do something different and um like I was very like close to machine learning in my last project like didn't do it myself but with the simulation it was then used with machine learning and so I feel like I'm keeping this closeness to machine learning but not doing it myself and yeah also very interesting going from the from the big corporate company to a small startup oh yeah I'd love to talk about that as well if I could stay on one more thing with the what is the role of machine learning in those simulations that you describe um so University there was no machine learning so I did experiment a little bit with like detecting the outcomes but it was just like very very very simple um and then my last job basically the the goal of the project was to automate scheduling like long-term planning and short-term planning um so long-term planning sense of the next schedule how all of our trains drive to the network for the next half a year and we have these requirements of training companies that try from here to there from data here and so on and get a schedule where everything fits in and the second part short term let's say three Falls over you can't drive all those tracks anymore we need to reroute everything how can we do it um by staying as close as possible to the original schedule and making our passengers arrive as as close to original time as possible and so I was building with my team um this micro microscopic train simulations and then there was another team that used reinforcement learning to try to learn with the simulation how to create conflict-free schedules and to react to risk disruptions okay is there any scenario in which that would be like the the vva data uploading would be so chaotic that it would be an analogous kind of system or is it I thought a bit about it but I I I don't think like I I don't think there's a like a direct connection um I think possible learning like not too much into it but it's it's I think very Niche and special like I think their profit rate fits great and many way it doesn't um yeah so sorry so it's like these really complex scheduling algorithms is I guess I'm curious if like leviated they say there's like a million clients uploading data and then you got to synchronize it all get in the databases that maybe analogous or is it I don't think so no because um but basically the the the the uh the problems you have in the with the train system is that trains cannot easily overtake each other so the the um the order trains are in at a certain point in time is really important and you can have a train like I'm not using German cities but let's say one train starting in Munich another train starting in Berlin which is 500 kilometers away and they would need to use the same piece of drag in five hours so the the the order which will derived at that point like if the high speed train comes first so if the slow cargo train comes first really can mess up your schedule because if the high speed train is behind the the cargo train which drives really slow and can't be overtaken then you have to ISP train behind it going really slow forward and um that that's a real problem that can't be solved with analytical methods so that's why we went the the AI route I think this vv8 is uploading is you have a bunch of data you need to get it into the database and the order doesn't really matter and you just need to make it efficient and um yeah what do you think about this idea say we're we have like video data right this video data could be like you know a 30 second video is bullet train like it's you know it gets ready compared to like a three hour video where right would that kind of thing maybe be similar um I don't I don't I don't see a connection there sorry um yeah I um I think this this like this scheduling is so difficult because you have everything affecting every everything else so each decision you do at one point can have a day later an effect somewhere else that you don't know immediately and with with like adding things to bb8 the objects don't depend on each other you need to get them in without having bugs in your code like without overwriting anything else but besides that they're independent from each other and it doesn't really matter if you add a 30 minutes video or a 30 seconds video like the the process of writing it will probably take longer because you have more data but um I think besides that that it's not to pick off a connection maybe with the cross-referencing there could be some kind of because I think with the cross referencing the current way that you need to do it is you kind of have like a parent-child import where first you import uh you know say it's articles and then they have their paragraphs you need to first get the articles in there and then you reference the paragraph So maybe with these graph structures there's some kind of scheduling to how you orchestrate the referencing with uploading that could be like if it's really complex and you have like circular things that you need to break up and there it could be um well maybe let's say you're you're creating objects on the Fly and then you need to uh make sure that everything is in that you need and then a new object appears and then you need to reorder everything if you have that then it might be but um I'm not sure how realistic like how often that happens maybe but super cool so can you tell me about your experience at semi-technologies and you know just kind of earlier topic we previewed of working in a big Corporation compared to the startup yeah um so I was like in the in the startup in the big company but then the or should I say the big company came into the startup and and so yeah meetings corporate overhead and that was sometimes a bit too much and now I would say it's it's very lean you really work most of your time obviously can't you talk to your colleagues sometimes when you're there but you really have have time to work so that's that's a um a very nice nice change and like you still have a feeling that you can that you know everyone so like in the old like the crew like I think when I started with 60 people even over there left it was 250 so I kind of you lose you don't know everyone in the company anymore and now I have to play I know all the names and all the faces um that's the difference if I ask one more question about um sort of your your motivation and kind of do you do you maybe have like an application of we V8 like personally I love the idea of searching through scientific papers and I I find every little grounding every little thing I'm working on in that application to be very motivating uh do you have that kind of thing or is it just about the technical of you know where can I find a performance like I like personally I really like to to coach and to complicated problems I I guess um now when you set statistic papers if if you have a lot of results from science like you do experiments or you do simulations and you have just a bunch of of results that you can't really look at anymore you'll have a possibility to just like send it to bb8 and do some some basic analytics in there like as I said with my my simulations where you have um where you have the cells crushing at on and I really I look at a lot of videos and I wrote like by hand the script that oh if the cells are after colliding at this far away in this case and yeah that was a bit annoying so if you could automate that and happy read in there that would be really cool and I I'm sure in larger projects I I did it I mostly worked alone with one or two colleagues so we didn't add that much data but if you're in a big collaboration let's say um at 7 or something where they have billions of terabytes if you could help there I think that would be something that would really fascinate me yeah well it does not as thing I thought about but that kind of yeah like how cell simulations I've heard of like maybe like docking simulations I I don't really know too much about this but this kind of you're simulating a bunch of like um is it like electromagnetic interactions like that kind of thing so we read it like effective interaction so we didn't go into the the like the physical details of the electrons that that um don't want to be the same place but basically if the one cell is here and the other side can't be here and they have some chemical attraction to each other and um yeah repulsed each other at certain places so it was very um how do you say like high level so we didn't go into into the details because it's already like the the uh the models already complex enough so you really need to think about what can you put in what's like the the minimum level of detail that you need to have in there to to kind of simulate what's happening in the experiment um wow so you have like uh like a big molecule and some kind of chemical score to it rather than like an atom level now we did um like real like biological cells so and um we we simulate them as a as a blob and we used it's called phase field approach so it's basically a field that's between one and zero where it's one there is the cell where it's zero that's not and it goes like it's one one one one one and then it goes down to zero in like very small small um area and in this in this fuzzy in between phase that like it's the boundary of the cell and um then the these places from different cells can get close to each other and when kind of their boundaries touch you have like physical interactions so they they repulse each other and attract each other in a certain way and then um based off some chemicals instead of the cell you have additional chemical interactions and then if you let them Collide you have based on these interactions you have then the cells behave differently and and there were like interesting experiments about it and that's what we try to to reproduce those experiments yeah well that sounds super interesting like that like hierarchy of biology and the um that's the whole thing is super interesting well Dirk thank you so much for uh describing the new performance changes in we V8 version 1.15 and I really enjoyed getting to learn more about um you know your background and the things that interest you I really enjoyed this podcast thank you very much which I did too ", "type": "Video", "name": "Weaviate v1.15 Release with Etienne Dilocker and Dirk Kulawiak - Weaviate Podcast #24", "path": "", "link": "https://www.youtube.com/watch?v=8lyA3mf7FjY", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}