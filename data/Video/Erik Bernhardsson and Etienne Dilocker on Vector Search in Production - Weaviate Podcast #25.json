{"text": "Thank you so much for watching the 25th episode of the Weaviate Podcast! This is a really special episode with Erik ... \nthank you hey everyone thank you so much for checking out the wevia podcast we have a super exciting episode today we have Eric burnerton and Eddie and dillocker Eric is one of the early thought leaders in approximate nearest neighbors creating the annoy library at Spotify an incredibly interesting algorithm for achieving this approximate nearest neighbor Surge and I mean these two are just such talented Engineers with so much experience in building these kind of systems and we're here to discuss the main topic of vector search and production running in production what does it take so Eric thank you so much for doing the weba podcast hi thanks for uh being here it's it's gonna be a lot of fun I hope yeah thanks for for uh yeah having me and of course Eric thanks for for joining um super cool topic and I think we can we can go into all kinds of different directions for what it means to run a n in production I think the first one to start is basically um that the history like uh Connor you already mentioned uh Spotify annoy and um yeah Eric you worked at Spotify quite some time ago I think so yeah do you want to start maybe telling us a bit about what you did back then sort of one of the the first pioneers of of approximate news neighbor search yeah totally so I spent almost seven years of Spotify I said 2008 to 2015. and a big part of what I did at Spotify I was building the music recommendation system and I don't want to get like super detail like how that works but like roughly speaking like what we ended up realizing or what I ended up realizing building this is that uh various types of vector models worked really well so we would you know do collaborative filtering on this extremely large sparse Matrix of what users listen to and then you know basically some sort of Matrix factorization would then lead to vectors and then once we have those vectors you know you could do all these sort of you know they have this like nice property that you have like a fairly low dimensional space and you can use nearest neighbors to find recommendations for people so you kind of project users and tracks and artists and albums into this we typically it's like 40 dimensions and then in that space it turns out like if you want to like find good recommendations for a user you just look at like similar near vectors for vectors that are close to that user if you want to find similar music to a different track you take that tracks vector and you you look at uh in this 40-dimensional space What are some other vectors that are close right and so the problem now is like you have to figure out how to do this like you have to find nearest vectors uh fairly fast right and at Spotify we had a few million tracks at that point that was indexed in the recommendation system so about I think two million tracks or something like that now it's probably more it's like 10 20 million tracks that are indexed uh and uh I looked at a bunch of different approaches and a bunch of different existing algorithms um for various reasons like one of the things I realized pretty early on was uh nothing deep into this but like I I wanted the functionality to end map it because what we wanted at Spotify was we had a fairly static data set but we needed to do a lot of recommendations very quickly and you when you deploy this you want the ability basically like you know create an index load it into whatever many processes as you have CPUs and then you know do all this like here's neighbor search so I basically built a static file index based on M map and it built a Noy and noi like basically works you know kind of splits the like that the point space recursively into a bunch of trees does that a few times to get a forest and then in that Forest you can you can search I do want to point out I think actually today there are a bunch of better models I think I know I like Stills is useful and since I think it's like one of the few like kind of like simple like it's just like a file embedded file and it's like uses M map uh but of course like today there's DV8 and many other ones uh that I think probably do a much better job at finding uh nearest vectors very quickly cool thank you super cool cool summary and yeah it's super exciting to to hear basically how how early you were in in doing these things that are now everywhere and of course recommendation being a a big case um and and I I noticed two things I think that we can talk about a bit more um one is you said essentially the the uh recommendation system or the the recommendation approach was just doing a uh near Vector search so basically um do I see it correctly that you never had to do online model inference that you basically that the model inference once built your index and then sort of when the user listens to a song or when they select the song you really just care about the nearest neighbors and never had to deal uh with the inference or did you have to do any inference sort of at what we would call query time in VBA but I guess in Spotify I don't know play time yeah yeah I think of it as like online versus offline inference or like batch inference right like we did basically zero online inference when I left Spotify I think they do a lot now uh but we would actually essentially pre-compute almost everything right like so we would do these the collaborate filtering models we would recompute every month or something like that because they were very large and took a lot of time to do it now that would give us a track embeddings right like a few million track commandings users taste ships faster in particular you have this problem that new users come in and you want to give recommendations to them relatively quickly so we would recompute those vectors every night and then every night we would basically go through every user that had some activity last 24 hours and then pre-compute recommendations for those users and I don't know if that's like pretty efficient approach it just turned out that like the way we built had the system set up at that time made it a lot easier to do everything sort of a bachelor in that way and so that might also explain some of the design decisions of how annoy worked it's quite optimized for more like throughput rather than like sort of online latency in particular that you know talked about the M map you know it's quite optimized for for doing a lot of well using multi-processing and doing a lot of things in parallel uh but um there's probably many other ways you can do it today and I'm sure Spotify today does a lot more like sort of real-time stuff right like as you start listening to the music probably I'm guessing they you know update that Vector in real time and do inference but I mean there's still like batch different stuff going on in Spotify right like Discovery weekly you know is only recomputed every week so I think there's still you know use cases for both online inference and offline inference yeah cool so super hope this this um the sort of the the yeah doing the inference in batches and then having that static data set that you mentioned um I I think this is yeah as you said as well it's shifting a bit but I think this is for me one of the typical cases where you can easily make this this distinction between a search Library an a n search library that typically is built for that static case so beyond beyond annoy if we look at uh phase or Google scan or all these libraries typically they're built for those sort of build once then surf kind of use case um and and this is for me a nice point to sort of draw the distinction or do the distinction between what is a library and what is a database because when we started with vv8 one of the things that we we wanted to give our users basically from the get-go is an experience as they would have in any other database so not So Much from the like okay you're starting with a library and now you're trying to to sort of get the library as a service but the other way round you're starting from a from a database like you may be you're used to it to using mySQL database or or no signal this doesn't matter but basically any database in the world you can incrementally sort of build up your data set you can do updates you can do filtering these kind of things and that is something that that we yeah basically that led our decision um to start with hsw as the first Vector index we definitely did look at annoy I think we even had prototypes before uh with with annoy um I used it also in this this confectionary that we have till this day which is basically like a very simple a simple Battle of words model which is sort of static it's built on on a glove or fast text so we have this kind of static and if you just want to search within there we used a noise for that um but yeah that that kind of sort of online changeability that that was definitely one of the the reasons why we looked into into hsw and yeah definitely one of the the parts that make sort of deviate a database as opposed to a library I would say I think there's kind of maybe two things going on at the same time right like there's like the sort of online offline distinction right like and you look at like a machine learning model uh you know you can either use that machine learning models of an online inference or an offline inference right and and and and then the separate aspect is is the sort of the crowd support like supporting like you know crud like what is it was a stuff for create read update delete like the ability to mutate the data set right and and I think you know like my sort of but I think the sort of Ideal here is like a service that does all four combinations of those two attributes right and and also does it with zero sort of performance loss because I think you know the the online case is a lot harder than the offline case in many cases right because you need to lower light latency and I think I you know it's easy to sort of give up a lot of the throughput by optimizing for the online case but actually don't think you have to uh I also think that sometimes we talk you know when I look at like machine learning as a whole people generally tend to think of like model inference as Premier like an online thing but if you actually like there's like you know 5000 startups that do like you know model deployments right but if you look at actually how people use models in you know real life I would say you know it's fairly evenly split between batch inference and online inference right uh batch inference generally has a lot more you know uh it's more important that you have a lot of throughput so I I think but but I actually think there's ways you can build your system that you get you know the same throughput even in an online mode you just have to be smart about uh you know batching and other things uh so so to me that's like the dream that and you know and I think we've been hopefully it delivers in those things I haven't done any benchmarks uh but but I think it actually does yeah I think on the on the the model inference part uh there's definitely room to to grow but on the latency part on on the search definitely um but yeah so so batching I guess sort of when we think of of batching in in the ml sense it's typically like this offline batching but you can still do like a a mini batching approach right where you just do the batch just big enough to basically have the benefits of say running on a GPU which paralyzes them uh enough um but still have it sort of in a real-time approach where maybe instead of sending uh or instead of doing a hundred single inference tasks that wouldn't have to run sequentially just batch those 100 up which happened maybe I don't know and then over the course of 50 milliseconds or so and then have them batch and and basically get the throughput this way like trade up a bit of latency for for the higher throughput I I think this is especially important when you have like Matrix algebra going on in your system right like we either you know especially when you're using gpus right because like gpus benefit a lot from batching if you don't do that then you might as well just like run everything with separate threads and it's like kind of fine you know you have some locking issues but that's fine um but I I think the sort of mini batching or micro batching that you see in some high performance model serving Frameworks uh they do benefit quite a lot when uh when you're using Matrix algebra yep cool yeah so media at this point is a good good point to to tell a bit about the different options that people have or that users have to do inference with with vv8 um because as you said some like to do it offline and just sort of only basically do the re-index part with V8 so this would be the typical case where the users provide their own vectors and they're not using bb8 end to end but basically batch indexing under do maybe doing some kind of a blue green deployment where class a so-so classes are the the isolation units in VBA where Class A would be serving uh with the old model you can prepare Class B build up the index in the background and then basically just switch the load balancer from one to another to to do this this kind of shift so this would be like the weekly example um but then also for the the live inference Parts with vv8's module system which basically gives you the ability to plug in um yeah any kind of kind of model really and not just any model but also any model provider to do the the end-to-end inference and there what we recently added is support for for the hugging phase API so basically one of those hosted inference service types so that you can really basically do do all three variants that you want to do and then of course if we're talking about the combination with crowd all of them them can be combined with with the crowd part of of the database but you could basically bring your own vectors which um sort of gives you full control about how you do the inference you could you could do it live you could do it or you could do it online you could do it offline you can do it with um sort of hosted together with vv8 so vv8 module system then just brings a small inference container there you can you can still sort of split this up into do you take what what serve what Eva provides and just plug in your model or do you replace the entire container so for example if you say it I can do this better I can optimize this better or something I can do it or now the new option is basically if you want the no Ops approach just put use use one of those those third-party providers where we're supporting a hugging face coher I think and oh an open AI also so with those those three um basically yeah gives you the ability to plug those in I think this is super cool I I mean I don't know like kind of reflecting on this as like you know as someone who like spend time on this like a decade ago or more right like I when I discovered this like vector approach like I like I was actually just talking to someone the other day but like I actually feel like that's like one of the most profound insights I've had in my life was like this like vector approach you know when I realized that back in like 2008 I'm like this makes so much sense like you have this like nice space and like you can think of things in terms of you know euclidean you know geometry right and and you know and that sort of mental leap for me was like realizing like once you embed all these things into the space like a lot of sort of you know relations become more like natural the hard part is the query right and like 10 years ago or 15 years ago when I was doing this like it was very hard to record it so I basically like had to roll up my sleeves and like build this like C plus Library with with python bindings uh that I could import and like use you know for my uh jnk Hadoop jobs uh but I always like was a huge believer in sort of vector models and and so I'm like one part of me it's like extremely excited if the world has come around to like realize like the sort of power of vectors yeah I'm very excited about you know the sort of the new databases coming out the embedding models and and sort of a general just like acceptance like you know sort of embracing of you know the fact that we're in a vector space like you can use these like vector models for a lot of stuff and and like what I what I'm excited about is like I look at the you know I look at all these like technology in the past you know like you know a very search algorithms or various types of ways we did like recommendation relevance in the past like I think there's like a huge opportunity to take a step back and like rethink so much of that from from ground up right uh and uh and so pretty excited you know we've yet is portion of that kind of stuff and but also many many other players right like you know in particular like you know it's not outside of search there's also a lot of like work on bedding some other things I think is really exciting so that's fantasy yeah and yeah yeah couldn't agree more and we're basically even though you mentioned 2008 right so even though it's 14 years later I think we're we're still basically we're still at the beginning like it's it's such a massive Trend and it's it's gonna yeah it's Gonna Change surge but but goes beyond search and then we're really just at the beginning but yeah what I also really like is this kind of growing together with the space and I think that was something that we tried to be be like from the get-go um while bv8 can be used end-to-end like that end-to-end user experience is super important we never said we need to entirely own the chain end-to-end like we would say I don't know you could only use it with models that we train or something and all of a sudden we're competing with these specialized startups and then not just startups also Google and Facebook and then all of them are meta now um but but really just say that okay we want to integrate with those players in the space and basically grow grow viviate with the space but also help grow the space with VBA basically partnership I think that's a good approach I mean in general like I would say when I look at machine learning like I I think you know it's kind of bifurcating a little bit where like you're gonna have the you know the Googles and Facebooks and you know the other big providers like training these like enormously complex very large models open AI Deep Mind and cohere and a few other ones right and most people want to want me to train a model like they could just like take these models and use the embeddings that come out of these models or use the predictions or whatever right like embeddings are often like the penultimate or the the you know sort of some intermediate layer in these models uh and and then like do their own you know sort of fine tuning on top of that so I think that's like a big shift in like how people work with machine learning also in the last five ten years five years maybe even like three years I want to say uh but I think it also opens up you know sort of makes a lot easier to to use these models right like in the past when I did things I had to do the whole thing end to end right like now I can just like download a transforming model from hacking face and and do you know two-thirds of it and they can like throw some like simple stuff on top of it which makes it a lot easier to do these things and with great quality too there are so many like pre-trained models for English and and computer vision and I mean any language really and and those models make the life a lot easier for a lot of these for people trying to build the recommendations or classification or any sort of you know industry machine learning application yeah and it's and it's basically getting better day by day so another topic that I want to talk about is benchmarks you're of course um famous for for the a n Benchmark website but before we go into that um we talked a bit about about what you did at Spotify but I'm also super curious about what you're doing right now and um yeah yeah I'm working on this uh startup called modal I I started it uh I would say about a year ago and it was still pretty early days but the idea is I'm sort of basically like I guess like the best way to put it is like I'm building a data tool I always wanted to have for 20 years and so I I found that like when they look at data teams how they operate and how to productionize things and how to scale things out and schedule things like and how they work with infrastructure and the tools they have it's all kind of janky like there's you know there's so much like time wasted on like you know dealing with infrastructure product you know setting things up like dealing with kubernetes and Docker and you know AWS and terraform and Helm and all this stuff right and and I think as a as a result you know like building code and running data applications today you know takes five times as much time as it should do uh I I mean I literally like you know I've been wasting like the last three days like trying to configure like vpcs enabled yes and like I really think that's like hell for developer to ever have to deal with uh and um and so so what is modal like yeah model basically like you know we built our own sort of Lambda style like infrastructure provider for data jobs right like we make it very simple Tech code and productionize that in the cloud like we are a cloud provider in that sense like we take people's code put it in containers and run it in cloud and we can do it at extremely fast because we ended up building our own uh container runtime and a bunch of other stuff from file system and many other things and so the end result is like you know if you want to do especially if you're an early stage tech company you don't want to set up all this like traditional you know all the infrastructure uh using model you can you can immediately you know do everything from like you know uh schedule retraining of your jobs to deploying your friends to scaling out like large scale like batch inference like you know a large scale mapping over you know various types of sort of embarrassingly parallel tasks like trust coding web scraping simulations and back testing and and other things and and do build a lot of end-to-end infrastructure for machine learning but also like other like you know more like mundane data tasks like reporting or whatever uh and do that very easily without having to think about infrastructure so sort of you know kind of selfishly building a tool I was wanted to have but uh you know also sort of think that many other people would benefit from this tool as well really cool I I saw it I think it was a screenshot or a short video on on Twitter where you shared basically the API and do I understand it correctly that I can basically write python code and I just annotate a function and then it runs in the cloud yeah and I think the it's sort of this idea that I think other people have had too which is sort of people sometimes referred as the self-provisioning runtime like the idea that like everything is just code right and the code itself specifies what sort of infrastructure it needs including the container specification the parallelism like all these other things and then when you run the code the code just like gets those resources in the cloud for itself so it's a sort of idea of you know you take the app code they also put in all that you know container code and environments and and chrome jobs and and credentials and all the stuff and just like right all of it in code right like there's like no config in model like everything is code and um which makes it very concise makes it very easier to run and um uh it makes it a lot easier to maintain right and so yeah like everything in code uh is something I don't know if you ever looked at palumi but it's sort of the same idea for taking even further me only does the infrastructure in code like we also do the we do everything like the infrastructure and the app code together all in Python really really cool can can users try this out like can I just could I just register right now or in in what state is it or would I have to wait a bit sure no but but we're working on so we're still in early data stage test right and we have a bunch of users using this uh and a few paying customers and so we're you know we're slowly scaling up and and you know if you want if you're interested in model feel free to go to model.com and join the wait list um you know it would sort of definitely gonna make more announcements in the next few months and and gradually open up and add more beta testers so if you're interested in trying out definitely go to model.com and setup or put your name on the white list cool nice yeah so as I mentioned before we we talked about model I mentioned uh benchmarks and of course there is a n benchmarks which um is I would say the de facto standard for comparing um yeah comparing a vector surge I don't want to say either libraries or databases because I guess you technically have a mix of them in there but maybe I'm just going to say algorithms and their implementations would that be correct yeah I think that's correct and it's mostly libraries like it sort of started out at almost you know five ten years ago and back then it was mostly libraries around I mean a m Benders marks was like one of these projects that like I got annoyed because I would read like various like papers at conferences and they all had their own like obscure like Benchmark setups and and of course like every live Eminem also biased because I built the noi but like every Library maintainer I felt at that time would build their own Benchmark suite and you know the world's like their library was the fastest um and and I found a lot of you know very frustrating but also people sometimes would come up with new types of like a n algorithms and like and not Benchmark them right like one of the things I was always was frustrated was like this research and locality sensitive hashing well calories let's imagine like basically it doesn't work in my opinion like it's a terrible algorithm but like people keep releasing papers about it because they had this like nice mathematical properties but like if you actually run the benchmarks on it it's just like awful and so yeah so there's a couple of different reasons why I was frustrated and ended up building my own benchmarking Suite initially was kind of janky and you have to to like install all the stuff and it barely would work but uh a few years in I actually set it up in a nice way to sort of containerized everything it made it easy to reproduce I added more libraries uh and so yeah so it's going pretty extensive Benchmark speed I think it's showing a little bit of age in the sense that it doesn't support crud it's very you know all it does is like you know a kind of static offline uh batch inference and really just measures throughput uh so kind of going back to the first thing we talked about it doesn't handle you know the crud case it doesn't really Benchmark that it also doesn't really Benchmark latency uh so so some libraries or databases May optimize for latency at the cost of throughput and sorry and Benchmark doesn't really optimize for that or it does really take that into account um but I think all in all like you know to me like it was something at that time and maybe still uh there was very much needed in the space because there's only like libraries out there making claims about you know their performance uh and until any other benchmarks I basically didn't feel like there was like a sort of a neutral semi quasi neutral because again I'm the author of annoy too but but you know sort of Benchmark Suite that actually try to compare all of those yeah yeah and then I mean the the fact that you're not trying to sell something with it I think that that sort of gives you a lot more credibility like if if we as semi would basically put out some sort of a comparative Benchmark right now where we would Benchmark let's say bb-8 against I don't know our competitors or something I think that would be you would always have to take that with a grain of solvent I mean you should because um so yeah it's really yeah I agree yeah and there's all these like weird ways you can like kind of fake it too like one of the things I I kind of was you know felt with the noise with sort of a m like it's kind of hard too because this is sort of a weird trade-off between like okay what is the desired recall and what is their desired throughput right like if you if you want to maximize throughput you can like you know make the recommendations really terrible and vice versa if you want to you know make the recall extremely good then you can like you know uh you know make it very slow and so one of the things I also did it in benchmarks I pushed for a lot is this sort of you know you have to look at the the whole Frontier like the whole Pareto Frontier of like you have to actually make a 2d plot and like you know plot the trade-off curves and do that for every single Library and what that means is like for every Library you have to run all these like different parameters and then compute that sort of you know the trade-off curve after the fact like after you have all that data so so it's sort of it's a little bit tricky to do that thing you actually have to run you know last time I ran it it took I think something like two weeks to run all the benchmarks uh and uh it was a fair amount of compute that goes into the rebuilding those benchmarks so if anyone is asking like why haven't I haven't updated in a while is because it just takes a little bit of time to set everything up and then run it but I should do that at some point again yeah yeah definitely it's super cool yeah and I think that that Frontier curve I think everyone who's looked at some kind of a n and Benchmark or or a sort of a non-comparative vector search Benchmark everyone knows those kind of graphs where like top and to the right is is best so that's that's yeah it's nice to hear that you came up with that because um basically the first time I looked at an a n Benchmark which was a n benchmarks that was just there so I didn't didn't even think about that yeah someone had to come up with that at some point yeah I don't know if I necessarily invented that I'm sure there was in some older papers too but I definitely you know I do think that Hayden had benchmarks like pushed for that instead of popularized that and you know made that you know the sort of the hopefully the de facto way to compare things is like you know looking at the 2D Frontier you know making that trade-off very explicit I think is very important yeah yeah absolutely so in in our benchmarks that we have which are not comparing bb8 against something else but we're just basically showing like this is how bv8 performs under these circumstances stances we also have that and and it's I think this is a good tool as well this this kind of Frontier just to to educate users because um and I I've had a couple of conversations with users if you're saying like oh I I want a n this is so cool um I I'm using this a n system and then they're like why doesn't it have perfect recall and then a conversation typically goes like do you know like are you aware what a n is like the a and a n stands for approximate nearest neighbors that's basically that trade-off between recall and if you're like oh okay yeah that makes sense but then having those kind of graphs and seeing like even in an approximate nearest neighbor situation you can still achieve High recall you just need to need to basically give up a bit of latency and throughput and just use the Benchmark as a as a guide basically of how to set parameters and how to configure it I think that's that's also super super helpful so that's maybe not a coincidence maybe some kind of subconscious decision fun fact one of my daughter's name is Anne Ann but it's not it's a pure coincidence we had that name in the family and uh although she was born after we had benchmarks so maybe subconsciously influenced do you also have a child called k n but uh yeah maybe maybe one day yeah the fact the funny thing is I haven't really thought about that until now it's kind of weird maybe I did name her after approximately there's neighbors I'm gonna tell her that which is when she's older yeah so so um benchmarks um basically we've talked a bit about the the value that they provide and um I think in a in a sense it's um it's really good to to have or to to give use these kind of insights in um into how these these algorithms perform and at the same time I think as a user how much do you care basically like is there a point where you go okay it's just fast enough or do I need to like of course everyone wants to have the the best but um yeah what is sort of if we're talking about let's say um a two millisecond versus a four millisecond latency like is this something that matters to the user I mean my opinion is like you know it like going to 100 I don't think is ever worth this because there's like a point where it just like takes longer and longer to like really you know essentially just you're just doing like exhaustive search but like if you can get to like 99.9 percent you know that's like usually like pretty much like good enough right that's like almost as good as like 100 and you know going to 99.9 or like whatever like 99 uh that usually ends up being like a 10x or 100x improvement over exhaustive search or maybe even more a thousand X if you have like you know very large data set and so I I think that's kind of The Sweet Spot is like you know for for most users if I just had to pick a point for them I'd probably pick you know the nine you know the the three nines 99.9 that percentile and you know that's close enough to 100 that practice you know won't be a problem uh but still you know implies like a pretty you know substantial performance Improvement but I'm curious what you think at the end actually yeah yeah so um definitely like I I kind of looked at this from the perspective of latency and not so much recall but I I like the way that you're looking at this of sort of optimized for recall first and then see what what latency you you get and uh the point that that I was going to make is basically um that let's say these two numbers that she said like two milliseconds or four milliseconds I think for the kind of slos that users have to meet for for whatever they're doing on top of of vector search they're probably not serving pure Vector search to the user so they're probably building some kind of application on top um let's say they have 100 100 millisecond SLO for for um the end-to-end thing then it's probably not going to matter if it's two milliseconds or if it's four milliseconds however this you can't really look at latency in isolation because latency probably also dives into throughput at least for the same same kind of Hardware cost like assuming that that um it scales linearly across threats and you have a single threat and on that single threat it's two milliseconds or four milliseconds that's basically twice the throughput and um that I think is is much more interesting and there I think if if we if we take this one step further and think about how do you actually run uh a n in production and um for example using the vviad cloud service where we now have usage-based pricing then it it sort of it I think it becomes a matter of of good enough like I need this kind of kind of latency SLO and I need this kind of throughput but I can basically get the throughput through horizontal scaling which of course increases your infrastructure cost but if you have usage based pricing then you don't really care so much about the the infrastructure cost so that's basically where I think yeah you need to kind of prove that it's fast and fast enough and that it's never going to be too slow to achieve your your goals but at the same time yeah at the end it's basically it's more kind of an optimization that we have to do internally to to run more profitable on the V8 cloud service than than what the user should ultimately end up carrying about yeah and it is of course use case specific right like what is the cost of a true of a false negative right so so basically like you know the fact that like there was something in the result set that you know the the a n algorithm didn't find right and it's it's going to be very high for search right like you know if users searching for something and you don't find it like that's kind of a bad experience right for recommendations like not so much right like it if you're just making a recommendation if you want to recommend like you know 20 tracks you know to someone at Spotify and it turns out like actually you know if you have done an exhaustive search you know that you know the 21st track wouldn't have been there because it would have been another track Like No One's Gonna notice right so I think it's extremely you know so for maybe like recommendations like you don't have to go to like you know 99.9 maybe it's fine to go to like 95th percentile like I don't know right and and that's you know those are the trails you have to think about yeah yeah great point yeah yeah and I think also you should take into account how good is your model actually at predicting like even if you have 100 recall that doesn't mean that that from a user's perspective the the match is good if the model just created bad and Bendix basically like it could be the the true nearest neighbor but just yeah it was just the wrong embedding basically yeah and I also wonder to what extent you know people then end up like you know doing sort of a multi-stage re-ranking anyway like at Spotify what we ended up doing was we actually had an ensemble method although I think they changed that later uh but but so what we would do was we actually had not just one vector model but we had you know I don't know half a dozen Vector model plus a bunch of other stuff and and so what we would do is like we would use each of these Vector models uh you know to search for candidates and so if we you know if we had to generate 100 recommendations from user we would ask you know each Vector model like give me a thousand that you know tracks right and then we'd pull them and then we end up with like 5 000 tracks and then we would you know we basically re-rank it using an XT boost uh gradient boosted decision tree based on a bunch of other features too uh uh that that you know all kinds of like you know some of them are like not at all like collaborative filtering Vector based like someone that were just like is it Christmas then you know we should boost crisp like there's like all these like you know weird features right and then we would re-rank based on that and and and I think I don't know I think it depends on the use case like I think you know sort of advanced you know the companies machine learning practitioners like Spotify or you know larger like e-commerce applications I think they're always going to have this like multi-stage approach um but of course like you know for a lot of like early stage startups or a lot of you know customers that don't have you know a whole machine learning department they might just want to take the the output from the embeddings and just return that to a customer and I think that's going to be good enough in many cases yeah or do some some minimal kind of re-ranking with them so for example then we're we're getting in the direction of hybrid search where um so for for our listeners hybrid search is basically the the idea that if we're in the context of text you have your dense Vector model that basically yeah creates your your vector embeddings but you also have traditional search so to speak so bm25 bm25f these kind of algorithms that um boost keywords or boost keyword matches or phrase matches Etc and um yeah with hybrid search the idea is to to combine both of them and um combining can be independent combining could mean that you do some sort of minimal get re-ranking or boosting based on on keywords based in like do the vector search for Bluestone keywords or do it the other way around um and um that can already also help basically overcome limitations of either the a n algorithm if the recall isn't high enough or the model so a text model standard Transformer models for example they perform pretty bad out of domain for for exact matches so that is something that can be overcome with with hybrid search and there I think we're also trying to to Really lower the barriers of Entry um for yeah as you said for for like smaller startups that don't have a whole AI Department working on this by making this easier and easier with with bb8 yeah totally and I'm kind of curious about like the evolution I mean this may be a side topic but like I feel like you know 10 years ago like you know you would just start with like you know something like elastic doing like inverted indexes and that would be like you know you would productionize that and then you would have search right and then you know now today it feels like okay you start with like maybe you know inverted indexes or you start with Vector databases and then at some point you know you start adding features and you start breaking it up into like a multi-stage pipeline you have like both vector and you know inverted indexes working conjunction then you throw in a bunch of other features too and then you throw it like you know throw in like extra boost on top of that or whatever and you know you have this like you know three four stages of you know candidate generation candidate re-ranking filtering right like I I'm kind of curious uh you know what do you see in the world like you know of search and relevance today is that sort of an accurate way to think about it oh yeah absolutely absolutely and I think you have all stages basically you have companies doing just one stage two stage you have multiple stages like we have use cases where we're from our perspective as semi basically we don't even see what's happening like all we know is the customer tells us well we use VBA for Canada generation and then we have that that kind of Pipeline on top of it and of course that that is something that's that's specific to them and and probably we're all there magic is happening and something that they're not going to be very vocal about because that's that's kind of their their secret sauce so yeah sometimes we as the database provider as the vector search provider we only see that that first stage the candidate generation stage um but even in the candidates generation stage I think we can so so for example with hybrid search we can incorporate like a bid more on on of that part and who knows maybe uh later on it could be even even higher if you could do four stage ranking or or search with yeah I think that makes sense because like if I think you know users will generally start with something that you know does that the simple thing the sort of you know pure Vector approach right but like you know over time you know has to get more and more advanced like you know it's sort of nice to sort of offer those things too like as to start to think about you know re-ranking and and you know multi-stage pipelines um I think like offering that you know as a black box or not not actually as a black box but like it's kind of a gray box like you know you can feed some data into it but you know 100 sure how it works and to me that makes a lot of sense as a product yeah one one uh sort of two-stage uh pipeline approach that we we've actually had for a while but I keep forgetting that it's a two-stage approach is that the question answer extraction that you can do with VBA where basically you just have the the dense or whatever a search you use as as candidate retrieval and basically use the the question answer extraction model that also returns a score as a re-rank step or even if you even if you don't do the re-ranking just to extract the the uh the kind of information from that text method that's a very simple but it's also a two-stage kind of pipeline that you can also do out of the box basically with uv8 yeah thanks a lot says at Spotify an incredibly important uh sort of one station felt in the pipeline was was just like removing tracks that the user had already listened to which kind of makes sense like if you're recommending music you obviously don't want to recommend tracks the users already listened to so so that was the last step in the pipeline we basically used Bloom filters for that uh because it's very space efficient uh so we would recompute Bloom filters every night and then use those as sort of binary representations and to filter out the candidates very quickly so we didn't have to so it actually was at the last stage we would we would generate candidates remove everything that was already in the bloom filter and then and then re-rank the resulting ones and Bloom filter also has uh false negatives right like Bloom filters but occasionally uh flag something as belonging in the I guess you should call it false positive but from the uses point of it turns into false negative they will they will sometimes think that you know sometimes because the bloom filter has is approximate we would think that okay the user had already listened to this track but actually they didn't it was just like a false positive so we would then remove that turning it to false negative where the user wouldn't actually get that recommendation because you know the bloom filter thought that the user already had listened to it but yeah I mean I'm just like mentioning as like you know sort of one example of like you know one stage in this like multi-stage pipeline I'm talking about and I think everyone everyone's pipeline will look a little bit different depending on the use case yeah yeah when you think of of stages it always sounds like super complex but can be something super simple is that what I really like is that you're essentially using database technology there are plenty of loom filters in in vva just for the the object store which is an LSM based store and and super cool that you were yeah using using Bloom filters there as well um so what I find super interesting about removing this in the last part um if I understand it correctly that is essentially a post-filtering step so in the worst case you could run into a situation where you would run out of candidates like if the if the user has listened to everything then everything would be removed from the search yeah so that that's another a situation where where I think the the pre-filtering that you can do in bb8 would be super helpful these days because you can just remove them before generating the candidates yeah I don't know if that was in practice I think it's in theory would was a problem at Spotify but in practice we generated so many candidates that it kind of we I mean we were generated like 10 000 candidates for every user right like plus you know and do that you know times like a number of different models that were all Vector based so so in the end like pooling those like we're gonna end up with like 20 30 000 candidates and then we rank them first like filter out using blue filter then re-rank them right uh so I think in practice like for you know occasionally they would end up with like you know maybe one less recommendation but but I think in almost every case like we had it more than planning yeah great example of basically you need to do whatever is right for your use case I could imagine search cases where that would be very problematic yeah thank you so much for joining looking at the time to see where we're approaching our our end this was super fun super super nice to hear yeah here yeah hear about Spotify and Noy hear about um yeah a n benchmarks and then of course about modals so do check out modal as we heard you can't register yet but you get well you can register for the for the wait list right so do do that and uh check out VBA as well if you've if maybe Eric was the reason you got here and not not deviate if you haven't heard of vb8 then check out that as well check out our other videos and uh yeah thank you so much for for coming had a great time yeah someone's going to talk about this stuff thanks so much Eric ", "type": "Video", "name": "Erik Bernhardsson and Etienne Dilocker on Vector Search in Production - Weaviate Podcast #25", "path": "", "link": "https://www.youtube.com/watch?v=gXPuhyM11_k", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}