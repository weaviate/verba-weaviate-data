{"text": "Tuana Celik, a Developer Advocate at Deepset, presented many exciting ideas around Question Answering! We began with her ... \nhey everyone thank you so much for checking out another episode of the we vva podcast today i'm here with tuwanachilik a developer advocate at deepset working on things like haystack and she's created an incredible hugging face spaces demo of a game of thrones question answering bot and i just really wanted to talk to her to understand this further understand deep said haystack and the things that they're working on so tawana thank you so much for joining the we va podcast thank you very much for having me so to kind of kick things off could you tell me about the game of thrones uh demonstration uh what went into it the motivation kind of these kind of these kinds of things sure so the game of thrones demo is really a demo to display an extractive qa pipeline that you can very easily set up with haystack so it's a very simple uh demo it uses in memory document store and the whole idea is really to to to show people how you can set something up super quickly and yeah i've uh hugging face space has been around for not really long but still quite a while um and i really wanted my to get my hands on it so this is what i came up with so there are a few things i would unpack with that but first could you tell me about the the role of haystack in it the kind of the nodes and the pipelines and and what that looks like for this sure so i'll start off by talking a bit about deep set so deep set is the company and we have a open source nlp framework and that is haystack and what haystack really is it's a open source um and the whole idea is to be able to quickly build an end-to-end you know production-ready nlp application so that could be question answering that could be summarization translation a whole bunch of things um and it comes on the it's based on the idea of nodes and pipelines so it gives you the flexibility to sort of uh switch and sort of construct your own pipeline based on the task you want to achieve so for me in this scenario i had an in-memory document store and then i had a retriever which would then based on the query you know give the reader model or the question answering model um the most relevant documents and then a question answering model um but yeah so haystack is based on this idea of being able to sort of um like lego blocks build something up from scratch based on what you want to do yeah that's super exciting and i love that kind of retrieve then re-decomposition uh can you tell me a little bit more about the in-memory document store and how that might differ from other kind of uh sort of like back-end retrieval store options that are available and i mean with the plug-in with we've a we can imagine with having the yeah like a vector database is the back end can you maybe explain for the difference between the in-memory thing and other options so in-memory document is basically what it says on the tin it's an in-memory document store so there's nothing fancy going on um and it was the easiest to set up just to get going with but you can imagine a scenario where you have lots and lots of documents like like residing somewhere whether that be we v8 or open search or elasticsearch any other document store and then based on what those are you can also have more flexibility to then work with maybe vector representations of text data and which is going to make finding similar documents to the query you ask a lot faster maybe so um it's really about how efficient you want your system to be and where your documents were in the first place um so in this example i didn't want to be bothered with connecting up anything else so i just went with an in-memory document store um but yeah you can imagine a scenario where i think we already even um we recorded a demo with laura from semi technologies your colleague actually where we use uh wev8 um document store with embeddings so uh with vector search um with a question answering pipeline yes so i really want to get back into the details of the question answering pipelines and uh the different retrieval models the extractive question answering these questions but really quickly could you could you tell me about hugging face spaces and uh kind of when you first saw it did it like what kind of did it capture attention right away like how do you see this kind of hugging face spaces platform hugging face spaces is i i quite like it i really like it actually because it's a it's a really nice way to prototype and demo demo applications to people um so i yeah i've i've quite enjoyed it actually there are two options right now as far as i know so you can build an application with streamlet or gradio so i built this one with streamlit it was very you know quite quick to get going with to be honest um and yeah it's it's a very nice way for you to be able to show code to people where it's not just pure code and they can already start interacting with it and you know inspire people to like this is what you can achieve in x many lines of code um so i'm i'm really enjoying it uh and yeah so it's nice to show people prototypes of certain very like um isolated tasks so in this one we are doing question answering hopefully and i already have a pdf summarizer one which i'm not super happy with the performance of but it still can it's it's great to be able to have an application giving people the idea of what you could achieve yeah yeah i think the way that it lets you visualize it and then have uh user interface input output and yeah gradio streamlin the way that they've simplified creating a front end for machine learning people i think is just super powerful and exciting and it also kind of has like a content platform flare to it where it has like the uh the news feed trending this week it kind of helps people discover other applications and maybe like integrate it with science and publications and you you know you please cite my work you link to your paper with your demo and i think overall it's creating like more community more engagement with the deep learning research crowd but so transitioning from face spaces which i love and i'm happy that we're both excited about this and i you know i i'm really excited about the future of hugging face bases uh can we dive deeper into the question answering part of this and i know um deepset has a roberta question answering model that's on hugging face uh the model hub and it's one of the most popular question answering models i think out there can you tell me more about the question answering model that's used in the game of thrones demonstration and uh generally your thoughts on uh question answering uh yes so we do have quite a few question answering models by deep set on hugging face actually so roberta bass squad 2 is one of the most popular question answering models but we also have what we now call distilled versions of these models which are way lighter hence a lot faster to work with as well and still perform relatively um you know comparably good to the original the teacher model let's say and yeah so these models are generally they are trained on squad data sets so question answer pairs um and the nice thing about this is where i feel like a lot of people don't realize this is how well pre-trained models can actually generalize to unseen domains so we often get the question uh with from people who want to sort of start diving into question answering so how long did it take for you to train your data train your model on this let's say they see the game of thrones one on the game of thrones data set and the answer to that is actually i didn't train a model at all i use a pre-trained model question answering model and these models are trained to look search for answers which is why we refer to question answering as a search task they are trained to search for answers in unseen data so that was how i was able to just take a pre-trained model off the shelf and use an in-memory document store put all my game of thrones documents there and basically told the model this is all the documents i have this is what the retriever says are the most relevant to the question that was asked to me um now now look for the answer in these documents um so yeah yeah that's incredible the the ability of these pre-trained models to do something like that i i never would have guessed like i wouldn't have suspected that a question answering model would be able to adapt to all the domains like this but it looks like this retrieve then read decomposition has offered a flexibility that is really exciting and and so the retrieval model also doesn't need to be fine-tuned right like it's a sentence transformer trained on like wikipedia and it's a hugging face model that part also doesn't need to be fine-tuned or specialized yeah i did not fine-tune the the sentence similarity model either um i can imagine you know scenarios that maybe you might um can't really think of one off the top of your head because what the retriever model is really doing is or let's say what the retriever component of the pipeline is doing is it's getting a question and it's saying okay i'm going to use a sentence similarity model to to decide which documents in the document store are the most relevant for the reader model to even bother looking through because the the challenge we have here and this is where you guys get in this is where vector and certain yeah vector optimized database is getting is that reader models at the end of the day they are limited by input so um imagine you're telling a reader model to list let's say go through hundreds of documents that's going to take a lot of time for it to decide what the most you know what the the best answer to the question was whereas if you were to have a retriever before that you can say okay you don't have to bother with that 100 you can bother with the first top five top three um which the sentence similarity or the the um yeah the retriever sets uh step has decided is the most relevant yeah it seems like that uh like going beyond 512 input tokens or maybe 1024 things like sparse transformers do you see that as being a big limitation of this retrieve than read way of setting up the task or do you think we usually can fit enough information in that 512 token input window that you know it's not that big of a problem with it with a strong retriever say um in practice it seems that we're doing quite well with a retriever reader pipeline again there are different types of retrievers and this this isn't just about the trend the model you decide the sentence similarity model you decide to use it's also about how efficient the document store you're using is so uh document store like what you guys have like we've eat where it's optimized for vectors um is probably going to perform way better for vector search than another one so there's a lot to consider not just not just the models you use yes i i'm kind of i'm thinking about this is inspiring my thinking about say the difference between large language models like gbt3 of course and like the thinking around in context learning and then the comparison with the retrieve then read decomposition where uh it seems like the retrieval model could be you know a pretty small model like say 100 million parameters in the sentence transformer seems to be like around the range of most of these and then the question answering i think roberta base is like 300 million right so you really cut down the size of these models and achieve a sum of performance do you think a lot about that kind of large language models versus this retrieved and read decomposition we actually use them both in common combination combinations right word right yeah so um i i already mentioned briefly distilled models but yeah like i said we have um so for roberta bass squad 2 for example we have a distilled model called tiny roberta squad 2. um this is a way smaller model don't quote me on any parameter size i'm not the most savvy about this type of stuff but it is a distilled version of roberta bass squad 2 i believe which is way lighter than roberta-based squad 2. um and we know that it performs not as well but nearly as well so it becomes a question of give and take and we when we do use tiny reverter squad two we then also use it in combination with a retriever reader pipeline um so what yeah we you can combine a lot of steps that that is going to make your whole question answering pipeline let's say way faster rather than picking one or the other um to achieve a application that is sort of seamless and nice to interact with yeah i think that's a fascinating idea that you could have the knowledge distillation go from say like a 100 billion parameter language model all the way down into the question answering model and have interfaces for that knowledge distillation i think that's a really interesting kind of area to explore further another thing i'm curious about is say i think right now the way that we mostly think about training or taking retrieve and reader models is that we train them separately we have our sentence transformer it's trained to contrast neighboring paragraphs or next sentences heuristics used to determine positives for that kind of contrastive similarity task and then we train our question answering model like separately what do you think about like the idea of joint training where you have like gradients the gradients from the question answering go back into the retrieval model do you think that's maybe like unnecessary complexity that kind of idea i think they use it i think this gets into the to a branch of nlp that i don't consider myself to have the most knowledge about but i can imagine a scenario where maybe training both of them with an understanding of what domain it's going to be used in um hand in hand might make sense so i can imagine so we even though i say these question answering models generalize quite well there are obviously situations where you will want to consider training or let's not even say training maybe fine tuning of question answering model to your domain i can imagine that you would probably want to do that for a sentence similarity model as well so if you decide you want to let's say do a question answering for i think one of the ones that came up was biomedical data would it make sense to also in line with that decide you're gonna use a retriever so maybe you should have a retriever with a sentence similarity model that's been trained for that purpose i don't know i think it might it might make sense but don't don't quote me on this yeah i think the the scientific text and we recently published our we've a podcast on scientific literature mining and i think what kyle was saying is that the the vocabulary is so different the terms are so different that you have such a domain shift that it it's hard for the wikipedia train books corpus trained models to get over there so so on the topic of domain adaptation domain transfer in nlp do you think there are some quick tools we can use to get a quick sense of what kind of degradation we might expect maybe like some kind of i've heard terms like semantic drift where maybe you look at certain tokens that have like a totally different meaning in some other corpus or maybe you look at like the tokenizer mismatch like certain words are being mapped to the unknown token and you just lose information do you think there's maybe a way to get a quick sense of domain adaptation as we're like building yeah i would as a haystack person i would say why not try fine tuning a model to your data and then evaluating your model um again you can you can do a lot of these via with haystack too so i would say go for it have an evaluation set and see how it performs for you on this topic of fine-tuning uh like sentence transformers so is fine-tuning something that's built into the haystack suite sort of is can you tell me more about how that works yeah so you can yeah you can um you can download a model straight from hugging face haystack has a hugging face um integration um and then say okay now continue training for x-men epochs with my my data set where where which is at this location um it's actually i think it's on the haystack documentation it's a surprisingly easy process i did it the other day is that different from um so is that for training the retriever models or the reader models this is this is for the reader model that i'm talking about this is for the question answering model i think that's the most common um place where you would want to fine-tune a model so is that fine-tuning it with the retrieval attached to it so it kind of gets used to having this is just so the reader model like like you mentioned before is a totally separate entity to the to the retriever so the only thing that the reader really cares about is i've got this context this bit of text somewhere and i've just received a question and i'm going to look through this bit of text to find an answer for it whether it gets that bit of text from a retriever or whether you input it via the hugging face inference api for example it doesn't care do you think maybe there's something to adapting the question answering models to i mean i think like with squad right it's already trained with the context like what is the atomic number of oxygen it gets a paragraph from wikipedia as a part of the input do you think maybe fine-tuning it with the kind of noisy retrieval end where you know it might not it might give you that paragraph from wikipedia but it's also going to give you something about maybe like i don't know something about oxygen for underwater i don't know like some like it might give you noisy context do you think maybe you should fine-tune that reader to even if you're not putting gradients back to the retrieval but like to get the reader used to like i have this kind of noisy my sidekick here is kind of going to give me these funny results and maybe i can get used to it a little bit but the the point of of the reader model isn't to know the answers to to something that the point of a reader model is really to find or search for the answers for something so um it really is a matter of how how well can you how well can you deduce the answer from a given bit of context let's say so i wouldn't see as the point is to be able to look through unseen data the only the only thing that it has to manage is to be able to be as good as possible at looking for the the right answer i think that's the best way to summarize it it's like i i um described this uh the other day um as the following so the question answering model is a thing a thing anything that has been trained to do just that look for answers um and what the retriever is doing or what providing a bit of context and the hugging face inference api is doing is literally just providing it with the playground that it should do that in or or you wanted to do that in so so this idea of um question answering is search i think that's very interesting and i i think kind of all these language tasks are kind of like very similar to me like it seems like question answering could encapsulate sort of the setup for any of these uh tasks if you phrase you phrase any task into a question like uh you know it would say like fact verification you'd say do these claim does this evidence support this claim right and now it's question answering so do you think kind of all tasks could be mapped into question answering or maybe say language modeling also that's an interesting question and i don't have a straight answer for it but it isn't it's an interesting way to think about it um the reason why well actually one thing that i should point out is that since the beginning of our chat we've really been talking about question answering in the context of extractive question answering we haven't really been talking about generative question answering the reason why we call question answering search is yes simple because the idea is that an extractive qa model looks for the answer which means that it doesn't generate the answer it also means that you have it can only answer uh a question if the answer exists in the playground you've provided it with now whether everything else can be described as a question answering task that's a tricky one because then i'm thinking about a lot of other a lot of other tasks in nlp in general summarization is a question answering i don't think so um [Music] but yeah yeah i'm really glad you brought that up the extractive verse abstractive thing and and um maybe like as kind of a quick thing i think maybe the extractive models are much more like ready for these kind of production systems like you know haystack we va as we build these tools that we're trying to push into like a more like a software engineering crowd i feel like the extractive models i feel like they make a lot more sense you can debug them a lot easier but then the abstract and models are so exciting and they have kind of hacks on them like maybe like the beam search decoding or recently a paper called rank gen they there's like kind of like strategies to make them more interpretable sort of but like in your thinking like do you think extractive question extractive models generally with classification labels compared to this idea where like you're generating each prediction has a set of 30 000 possible things and you unroll that sequence like 50 times do you think that's maybe too grandiose for like the production systems that say a lot of these like open source deep learning meet software engineering kind of companies are pushing out there i would say so um for a very simple reason um the the training process of a generative bear in mind i am not an nlp engineer i've really been learning along the way but this is my understanding an extractive qa model is trained very differently to what a generation generative qa model would be trained as um and then the question of oh how long did it take for you to train your model to answer these questions that actually becomes very valid for for generative qa models because that this this time around the model isn't you're not giving the model data for it to search through a generative model doesn't need context to search through a generative model is generating answers from the things it's been trained for now you can imagine in a scenario where you're a company let's say and you have all these bunches of documents text data lying about and you want to provide that via a question answering application on your website you have two options you either go with a extractive version and you use a model either you use it pre-trained off the shelf or fine-tune it to that data you have maybe if if necessary but then you just provide this and you provide the data for it to search for answers in a document store and that data can grow you still use that same model if you're going for generative where there isn't a concept of searching through context for it to still be valid in let's say five months time when you have all this new information you're going to have to think about growing and growing and growing that model so in a real life scenario where you just want a simple question answering application somewhere i i from my experience so far i would say an extractive qa model is going to be a lot makes a lot more sense now obviously i don't know maybe in the near future hopefully we get a lot more a lot cooler things out there with generative models and everyone starts using it but this is my understanding so far yeah that reminded me of when uh malt from haystack was also on the wva podcast he gave me three reasons to favor retrieve then read that you reminded me of and the first of which being that it's easier to update the models and i think that's such a compelling point like you need to update the language model to say like you want to change a fact or something like that or you're trying to just continue the auto regressive modeling task and it might not forget it it's hard to say if it did right and so that particular point of the ability to update definitely sounds very compelling and then the two of the things he mentioned was the interpretability if you see the context the reader model is using to predict so that can help you out exactly that's another thing and that's actually a really good point because with an extractive qa model you ask a question and you're not only getting the best answer the the model can come up with you're also getting information of where that answer was from so in a case where let's say for documentation where you have a question you want to ask and you get the answer but you want to read more about it now you know what document contains that answer and what you can go and learn more about it actually for this type of scenario then even you can even imagine how not having the answer but the relevant documents become very compelling too right yeah yeah yeah it's super interesting the way that you can uh see it of course and yeah like i think they try to do things like uh they might do like some kind of signature in the training data to try to see what produced the generative language model's output like i think there's research like that where they try something like that but surely it can't be as straightforward as this is with the retrieve than read kind of thing yeah and so then the third thing he had said was that it's i think something that we've talked about a few times with distillation and sort of is just that it's more like economical when you decompose a task you don't need to store all the data and the parameters of the big language model and and then yeah you can have like 100 million parameter retriever 300 million parameter reader rather than gigantic model that does everything so i think that was really great conversation and topics around question answering and kind of the next topic i really want to ask you about is the developer advocate role and creating content and things like that is something that i really care about a lot and i'm really curious to see hear your opinions on on this whole process as well of kind of communicating what you're doing the different mediums of doing so oh man that's such a big topic um so i started uh developer advocacy in 2020 actually so unfortunately when i started for the first year in a bit we were in um pandemic and we had no live events etc so from moving from now on moving forward um one of my main things is really going to be like doing this maybe doing it live as well um doing community calls and events but my take on developer advocacy is that there there are certain levels of it there's there's one there's one thing of being um trying to like educate developers on the tool you're a developer advocate for and the way i've been doing that is like i said i'm not an nlp engineer i come from a deep learning background this is more in computer vision maybe but um it means that i have now entered a realm where i'm learning a lot every day so we may listen back to this podcast and there's something wrong i said and i learned what's what was supposed to be the right answer for example so my aim is going to be and i've started doing that i think i should be doing more is teach the community or like tell of the community about what i've learned along the way so there's an aspect of educating people inspiring people to to start using certain tools or getting into certain areas um and there's also the other aspect of making the tool that they're using more compelling for them to use for example um i feel like there's so many layers of developer advocacy you can do so much with it it's a social um role it's a it's a technical role um but it's also because of that extremely flexible and very prone to to to be a role where you can use your creativity a lot yeah there's so many different ways of packaging up the lessons you learn about how to use the software and trying to make it the mo in the most digestible format for your users or developers people interested in seeing what you have so i've been kind of thinking about like the flow of how these things connect like uh one thing i really like is the keras code examples the deep learning framework and they have like computer vision natural language processing and that kind of organization of the examples and and they implement things and it's very like i think it's a very great way to get started so i've been curious about how like maybe we've ate examples we have our github repository how we could kind of get into that kind of organization where we have uh use cases like uh e-commerce or uh archive like scientific literature mining or legal or like the different kinds of like real world things and then maybe the applications like uh search question answering fact verification like kind of sorting starting to look into the nuance of the tasks and that kind of thing so i've been kind of thinking about how you can maybe go from like github to youtube to medium sub stack like how these different kinds of like platforms flow together and i think hugging face spaces is also emerging as one of these core content platforms for developer advocates do you like do you think you'll view hug or maybe even already view hugging face spaces as similar to github like in terms of the content platform for showing the code like what would you put more emphasis on i think i i view hugging face as a very important medium for developer advocacy especially in the world of ai and machine learning and nlp for us for example at deep set hugging faces really the the medium where we share our models we've trained with the nlp community and from now on also give them examples of how these models can be used with haystack github is where we have our framework host our source code for uh the haystack framework hosted um but i feel like hugging face yes and it is going to be more and more for me uh very like it's it's such an opportunity to to get to the people who are every day interested in this particular branch of research who are interested in question answering or summarization or anything to do with nlp who would benefit from using haystack or deep set models but don't necessarily know of their existence yet but now with hugging face and the whole community vibe let's say around it um it's it's going to it is already a very important um domain for me to get to those people and one other question i want to ask about the uh the game of thrones demo is so you have you have such a tangible here's how you do something useful with haystack and you mentioned the idea of maybe having a live audience how would you think about designing that kind of live presentation would it be very uh like what kind of data sets do you all have or or let's look at the game of thrones thing and really try to understand that actually it's a very good question and on time as well because in a week's time i'll be doing this uh in london at odsc alongside you actually but i think you're doing it um online aren't you yeah unfortunately so i'm actually going to be using two demos there um one is a it's a different application it's not the game of thrones one um and it's a live haystack demo question answering demo about world countries and cities and information about them for example um and i don't think i have to focus on one demo over the other or like focus on like the game of thrones data set specifically it's more about displaying how semantic search works and how that's different different to keyword search for example so any live demo application where i can sort of portray to people how i just asked the question to this application and clearly it's returned an answer but you can already see from the answer that it got an understanding of what the context was it had a semantic understanding of what the sentence the answer was in was um so it's more about that it's not much about the exact data yeah and it seems like it seems like such a generalizable uh pipeline the retrieve then read kind of thing too i think with with the we've a thing and what caused me to think a little differently about it is where we have this kind of like class property schema setup where you can use cross references to say achieve uh like maybe it's like uh scientific paper has author then you also have an embeddings for author like you have an embedding set for the papers and an embedding set for the authors and you can do like multimodal yeah like kind of like hierarchical like uh take advantage of the structure of documents with that kind of thing so i think that integration with structure is what makes it a little tough to just say uh like here's the template that will work for your data set i think it might require a little more like okay well what kind of relations what kind of like semantic ontology sort of that kind of thinking can we do in your data and then how can we use that to build better vector indexes i think that's maybe one thing that makes it hard to think about for me uh think about like a generalizable way to say here is we've and here's how to use it for everything kind of like so but yeah uh so one other topic i want to kind of come into is some of the things that are exciting you about nlp and the things that you're working on oh man um that's a that's a big question um i'm really mostly excited about to be honest with you a lot about what we've really already talked about is and it's a challenge for me so i'm yeah like i said i'm learning along the way um to me the interesting part right now is um explaining to people who are not like people like me maybe even who maybe i'm a bit further than that right now but who are not nlp savvy necessarily but have been a part of this whole nlp and ai hype and they are starting to want to understand and learn how this whole thing works so to me the most exciting part right now is taking a step back and giving people the big picture why do we have this retriever reader pipeline in the first place what is the qa model actually doing and it's not it's not that easy explaining this to explaining this to people who are uh interested um it's really fun but it's not an easy task um so right now that's that's the main thing i'm focusing on is it so explaining it from scra from so is it explaining the retriever reader versus the let's build a massive end-to-end model kind of angle of it or here's what a question answering it model is to begin with both of them so because here's what a question answering model to begin with is a nice leeway to explain why a retriever why a retriever step might be very useful the other thing by the way that is quite exciting is um long-form question answering which i'm still wrapping my head around which is a generative question question answering um a technique where you you get basically long-form answers and i haven't got my hands on it yet but we have it available in haystack so this is another thing i'll try soon yeah that the generative stuff is is really exciting i definitely think there's like going to be a lot of opportunity with it and uh quickly this is some you quickly mentioned that you came from more the computer vision background and i also kind of had that background so maybe i wanted to ask you about uh retrieved and reading computer vision does it make sense in that context where maybe for image classification we're going to do a k n search and then similarly kind of reason about the label given the additional context i'm now imagining find the a stack of images where you say find the i don't know red ball and you have a retriever that gives you the first five images that that a ball might exist in um yes so computer vision was um i did my computer science bachelors and masters at the university of bristol and my master's thesis was around computer vision um so that's how my background is in computer vision um super cool so do you um like are you excited about say clip multimodal embeddings with image text and kind of seeing if this retrieve then read thing also kind of works in the multimodal embedding problems like maybe visual question answering vision language navigation trying to remember the image text like image captioning those kinds of things i think we actually have a few people um working on stuff similar to what you're describing but don't quote me for it um yeah so maybe one day yeah yeah it's super cool seeing the um like all the different data domains and how i think language like language to me seems to be and i saw a really interesting paper where they were able to turn hyper parameter optimization into a language problem and i so i think lang in my view language is the most powerful interface maybe that's a hot take but i think like language you can specify most tasks in language or use language to uh specify the task you're trying to achieve and put it into the space somehow to get a better representation of things yes i do think like i i do i'm of quite a firm believer that we're going to start interacting with machines in general in a very different way to what we've been doing so far and i think language is going to be one of the top things it is already like even even now every every single interaction we have with our machines with our browsers has some language aspect natural language aspect to it um and on the on the topic of language and quest and images how cool is it now we can like describe in human language what's art we want and we have models that can do that how insane is that yeah the the the dolly thing has been like truly insane to me what do you think about dolly i mean i'm just having fun with it i'm just having fun with it honestly um but i i'm really curious as like how far we're gonna go with this like where where where's our peak when it comes to this sort of thing yeah i'm i'm actually a little afraid of the video models once you take dolly and then it does videos and it can like generate all sorts of videos with that super realistic nature to it that sounds to me like a little bit like well that's pretty crazy deep fake that kind of thing um do you think there's i saw this one criticism from gary marcus about like compositionality in dolly where uh that prompted something like a red cube stacked on top of a blue sphere next to a yellow cylinder is some prompt like that and it's unable to chain together these symbolic components like every time i think like i think because i've seen people also on twitter put together crazy prompts like that and show look it did do it so i think it's more so like well it just doesn't do it every time so i think maybe this also yes all right um i was just going to say is yeah i mean it's a it's a generative model isn't it so um but my view of it would be i'm now curious could you send this over to me after this podcast recording um about the criticism um but i'm i like looking at this sort of stuff a bit like a pollyanna view it's like this is just the beginning obviously not not everything's gonna work perfectly but isn't it insane and quite really cool that it can do you know these three things for example anyway you can imagine that in the future it's those three things are going to be those 30 things yeah that's my exact the thoughts on it too that it can do these three things and i think that's kind of like the state of deep learning generally right now and and i think extractive question answering to come back to that is more solidly correct compared to the generative models where the generative models and deep learning they're not it's not like correct like a symbolic algorithm like traditional kind of software is it's like you know it it might amaze you or it might like totally fail it seems to be the way to look at these things like yep and i've also kind of seen that as i'm exploring the retrieval models i've kind of seen some cases that look like that too where if you look at the distribution of whether it returns the correct rank it's either one it got the exact thing or it's like 10 plus like it completely missed it and then and then you'll have like low density and two three four five for like evaluating the rank order so i think that kind of thing if it's either super correct and amazes you or it's completely wrong is maybe one way to think about the failure of deep learning models so anyways i think we covered a really great set of topics is there anything maybe that we missed out on haystack and uh and question answering and any general topic ah no i don't think so but i was just gonna ask you the last thing you said was this about the retriever step yeah like um looking i'm i'm really interested in trying to you know get my hands up into the research of it and doing things like uh like the mean reciprocal rank recall these kind of metrics to look into the performance of the retrieval models and really i want to try to answer this question of off-the-shelf model versus fine-tuned model and explore that for scientific papers i think the beir benchmark is one of the best ones out there that has um i think it's like yeah different different information sources like wikipedia books maybe legal documents and then different downstream tasks as well that are attached to them sort of like wikipedia has fever and i know i'm going all over the place if people listen to this but there's also like um knowledge intensive tasks where it's a bunch of downstream tasks that are built on top of wikipedia so this is a way of testing these retrieved and read pipelines for different kinds of downstream tasks using the same kind of information source but yes um i'm really curious in just seeing the uh retrieval performance in different domains so the beir benchmark is like the dream thing that i would like to get to just a side note about the retriever step being super accurate like the top one working out or the you know it's all down the number 10 number 10 document that it retrieved in a question answering pipeline really if it was a top one life would be super easy because then you know the answer is in that one right um but the point of that retrieval step in question answering is that somewhere in the top x that's the question answering pipeline wanted you to retrieve model retrieve documents from somewhere in those top x the answer exists so it doesn't yeah doesn't really matter that it's the first one the third one the tenth one it's just how well is it in putting it in that top x d what do you think about re-ranking the list based on the uh the certainty of the answer classification so it might be ranked position seven but the question answering is like i'm 96 sure that this is the answer whereas with the one through six it was like 40 you know so like you can kind of correct the retrieval with the re-ranking of the um we actually have a i think either a tutorial or a demo or some some sort of um demonstration of re-ranking documents based on which one could be maybe the most relevant to the query that was asked and so that's definitely a use case um but uh in the context of um a question answering model um it's just received for for the question answering model the only thing it cares about isn't the rank it's totally about this is the amount of the or the amount of context i'm going to look through it might decide that the best answer to the question was at in the first document that was retrieved it might decide it's in the seventh document it retrieved it really doesn't care about the rank that it was given the context in i think that's bringing us to another interesting point of whether you want to stack all of the retrieved evidence as input so it's like one big input or if you want to run the question answering to each thing and i think there's like the uh the fusion in decoder is what facebook calls their model of um separately taking each of the retrieve facts and then putting it in that like encoder decoder transformer kind of architecture where uh you don't have the attention over all the facts at once that kind of difference where uh and by doing that you can have like a really massive list of facts compared to if you need to do the cross attention on the facts yeah this is where we get into a whole other a whole other topic based on um the capacity the input capacity that a certain model has yeah well thank you so much tuwana i really enjoyed this weeva podcast and i learned so much i think this is such an exciting topic to retrieve and read pipelines thinking about uh developer advocacy how to create kind of artifacts to demonstrate what we're learning the different mediums like hugging face github youtube podcasting and all these kinds of things so thank you so much for coming on the vba podcast thank you very much thank you for having me this was fun ", "type": "Video", "name": "tuana_celik_on_question_answering_with_haystack__weaviate_podcast_20", "path": "", "link": "https://www.youtube.com/watch?v=kJtixgqdlHQ", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}