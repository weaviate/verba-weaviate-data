{"text": "Please check out Composer from MosaicML! https://github.com/mosaicml/composer Jonathan Frankle is the Chief Scientist at ... \n[Music] thank you so much for checking out the wva podcast we've had a lot of great guests in the wv8 podcast but i'm super excited about our next guest we have jonathan frankel from mosaic ml jonathan is one of my favorite research scientists and deep learning to follow along with since the debut of his paper alongside michael carbon his advisor at mit the lottery ticket hypothesis this paper i won an iclr best paper and it was just an incredible finding about the ability to apply these sparse mass onto these sub-networks and this kind of idea of pruning and how you can kind of achieve this training of the sparse sub network from scratch is theoretically possible and now jonathan has continued this research developing all sorts of things related to efficiency and efficient deep learning algorithms exploring the benefits of things like masking with pruning and all these kinds of things that jonathan will explain to you better than i will in this quick intro but i'm so excited to learn more about jonathan's latest company uh mosaic ml and the product composer and for our weev8 community this is super interesting because you know they're making it more efficient faster cheaper to train models and then we get our vector embeddings that we can search through for semantic similarity and all sorts of things so jonathan thank you so much for coming on the wba podcast and uh can you tell us about mosaic and ml and maybe what i've watched in my quick intro no no this is great so the story of mosaic ml goes back almost two years now and really it started with an email from our now ceo naveen rao to me and my advisor saying hey nice lottery ticket paper i want to do a start up based on that um and my first reaction anybody's reaction who knows the lottery ticket paper well should be are you insane interesting science not useful um and you know i'm willing to admit that you know this work is not practically useful it was meant to be an exploration into understanding deep learning but in order to use lottery ticket practically you would need two things to be true number one is you need some magic way to find sparse networks early in training the work is all about showing that these sparse networks exist but it doesn't show an efficient way to find them and people have been working on this problem in the literature but in my mind there hasn't been a ton of progress to be completely honest and i can say more about my thoughts on that in a moment the other piece is that suppose you found one of these sub-networks you need to train efficiently when you have a sparse network and you know for those who aren't familiar just because a network has fewer parameters doesn't mean it trains faster the network will hypothetically require fewer compu fewer calculations fewer floating point operations to be able to train but that doesn't make it go fast because our hardware can't actually take advantage of the topology of the network gpus and modern tpus and hardware like that can't take advantage of sparsity very effectively it's used to processing you know contiguous matrices or tensors or vectors so i kind of looked at this email you know getting back to the topic i looked at this email from davina went is he nuts like this is a pretty smart guy um but we got to talking you know my advisor and i sat down and talked to him and you know hanlin tang his colleague from from his nirvana days also joined now our cto um and we came to the following conclusion there's a lot of opportunity to improve neural network training by making the training algorithm more efficient and what i mean by algorithm is the following because it's worth clarifying here so there are a lot of ways that you can make training more efficient one is you can build a faster chip like nvidia just released the h100 it's a lot faster than the a100 which is a lot faster than the v100 that makes training more efficient and cheaper um another thing you can do is write better compilers you can take the same set of operations and run them more efficiently you know this is these are things like cu dnn and kernels and you know triton and that sort of thing but i think the interesting observation here is all of this is taking the exact same math the exact same way of training every operation that you're doing and just finding ways to run it faster you can you know spin the gpu faster you can have you know faster tensor cores you can you know run pick the right operations to run it faster but it's still the same math you'll get exactly the same weights in the network assuming you know non-determinism doesn't kick in on the gpu or things like that and the lottery ticket work is interesting because it says actually we can change the math we can get you a different network than you would have gotten otherwise but it's just as good and hypothetically in our fantasy land where we can accelerate sparsity it would have been more efficient to get there we can change the algorithm to make it more efficient and that's what mosaic is built on is trying to look at ways that change the math that fundamentally change the training algorithm to make things more efficient and i'll give you two reasons why this you know makes a lot of sense to me intuitively one is that you know think about the math behind deep learning there's nothing correct about this it's not like we're doing the right thing and mosaic is doing the wrong thing but you know making it work we're using sgd which is a convex optimization strategy in a non-convex landscape if you want to get that network to train you need like all the right hyperparameters just the right way if you set the learning rate a little bit too high or low bad performance you need batch norm or layer norm or pick your norm you know you just like we're living in this tiny tiny space of networks that work well it's minuscule it's like you know you you step off of this tiny little island of stability and you fall into lava and your network doesn't work very well so you know there's nothing correct about what we're doing so we shouldn't be afraid of changing the math the other reason why i think i think this makes sense is look at this tiny space of networks we don't know how to describe this tiny space theoretically i can't give you a proof or a statement or a set of assumptions to describe it and i think of this kind of the analogy i like to use is biology so imagine we have the laws of physics think of that as all the neural networks that are out there we can describe them all think about biology in you know the real world this is one emergent property of physics we are just physics this is one outcome of the laws of physics if i gave you the laws of physics you would never predict the biology here on earth partially because it's one very small possibility in the range of a huge number of possibilities partially because it's so complex that from first principles it would be hard to predict i look at the neural networks we train in practice kind of metaphorically like this biology it's these are highly complex systems that have emerged from very simple dynamics like physics but it's one specific thing that might have emerged and we can imagine all sorts of possibilities in theory we have to contend with all these but in practice we only deal with a tiny portion of them and when we study biology um you know we we don't go down to the physics immediately we look at motifs and systems and behaviors and mechanisms and all these higher level abstractions things that you know we wouldn't necessarily think of from first principles in physics and so i think of this in the same way as deep learning i don't care what might happen i don't care what you can prove i care about what is happening and how to describe it even if the descriptions aren't perfect i don't need a proof i just need good empiricism and then you can think of what we're doing is almost like pharmaceuticals we're designing some kind of chemical or some kind of intervention that takes advantage of processes we understand to get a different behavior out of the system and so we design a lot of these interventions the lottery ticket is one of those your body chemistry is no longer functioning the way it was before but ideally you'll get to a similar place or you'll be you know just as healthy as you were before but you know ideally we can make that outcome happen in some desirable way and you know there i'll continue this metaphor for one more moment one of the big challenges is not just you know okay designing one pharmaceutical you need a lot of pharmaceuticals to treat different conditions to address different aspects and you know lottery ticket you can think of as one pharmaceutical at mosaic we have you know i think 25 and counting right now and that encountering is because we've got a lot more in the pipeline that you'll expect shortly um and the last thing i'll say here is if i had to do 25 pharmaceuticals you wouldn't take them all you wouldn't just pop all the pills into your mouth and say ah i will be better now these drugs can interact in really dangerous ways and so a lot of the magic of our work is trying to understand the science of how do these things interact what's going on under the hood in these networks that causes things to interact negatively some of this is systems optimization some of this is you know just two methods seem to over regularize or what have you but there's a science here that we're starting to uncover and that helps us build recipes so i'll stop there i'm rambling but really mosaic improving training algorithmically i think this is a new layer of the stack and i think this composes really nicely with what lots of other folks are doing at other layers of the stack hardware compilers all the way up to good you know ux and mlabs so you know that's where we are wow this is absolutely fascinating and firstly uh to preface this i hope that that story of the email ends up in these startup books one day i get that idea that you get an email and that's the start of mosaic ml that should be one of those books but um yeah i'd love to the 25 and counting the composer the interaction effects of things like uh uh ghost batch normalization stochastic depth and maybe the og mix ran data augmentation strategy so before coming back later on i think i'd love to dive more into the design of composer and it's kind of api and how it integrates into most of the existing workflows of people working with deep learning but i think it would be great to transition right now into some of the methods of composer and you mentioned the interaction effects of these methods and the new science is just absolutely mind-blowing such a fascinating thing but um so could we kind of come through this list a little bit and maybe could we start off with um so i kind of as i went through the early documentation on method on the methodology and the composer i kind of chunked it up into categories of data augmentation normalization what i call model augmentation and then label augmentation and optimization so maybe let me start off with model augmentation in which i grouped things like swa running average of the model weights stochastic depth replaces specific layer with a stochastic version that randomly drops a layer of samples during training can you tell me more about these kind of things like layer freezing uh squeeze excite and how these kind of things come together definitely so that i think you've actually done a really good job chunking these off i'm gonna have to steal this because we're we're struggling with this right now this is one of the challenges of building the science is kind of you know i look at something like let's start with stochastic depth because i think that's an interesting one so you can think of stochastic depth as kind of a fancy version of dropout what it does is in a residual network you can skip a layer and the network still works you can just kind of chop it out but you have those residual connections that go that route around it so in a normal feed-forward network if you cut out a layer like the network's disconnected and it makes no sense in a residual network you can drop layers and so the idea in stochastic depth is you can do this two ways one is that you just for your mini batch you drop some of the layers and you just skip them on every forward pass the other is that you do this example-wise so each example kind of takes a different journey through the network and on each layer you only run through some of the examples so stochastic depth has a complicated story on the one hand it should hypothetically make things more efficient because you're not using the whole network on any given pass so this should be you know beneficial to speed you might think of it as a model change in that way but it's a regularization technique as well so the reason why i'm picking on this one is it kind of some of these methods really defy categorization or fall into more than one category which is what makes the science so interesting and also so difficult and stochastic depth has an interesting story so there are two things i'll tell you about this one is that you know the paper that came out with it makes a very compelling case that you know this leads to speed up but we actually tried it using the same methods from the paper and we didn't see any speed up in fact we saw worse performance what it turns out happened was the paper had some pretty terrible bass lines you know they trained a c410 network for hundreds of epics usually you know 160 epics is enough i think they were training for 500 epics in the context of that 500 epics the regularization really does help you and you can get better accuracy and therefore the appearance of faster training time but you know when you look at normal training regimes this doesn't actually help at all and in fact we found it to be hurtful so this is a good chance for me to take a step back and ask what does it mean for a method to be good what does it mean to have a positive intervention and so here's the way we look at it at mosaic you know there are a lot of games you can play in the research literature to make something look good one is that you can you know just have a bad bass line you can like train your bass line for too long way past the point where it's not helping your accuracy and then show oh i made things faster by you know showing that it doesn't take as many iterations but the baseline didn't either you know another technique is that you can go through and say well hypothetically i reduced the number of steps it took for you to reach a certain accuracy what i'm not going to tell you is each step takes five times as long and i'm pointing at the sam paper here the sharpness aware minimization where this is a huge problem each step takes twice as long and you can train in fewer steps but it slows things down and we had to mitigate this in the case of stochastic depth there are two issues here one is the baselines weren't good in the paper um the other is that just because you can drop examples doesn't mean you can get speed up from that going back to that sparsity example with lottery ticket you know if you drop the examples on each layer you've got this weird shape tensor you have to reshape the tensor run it through the layer and then reshape it back and then ended up slowing things down to the point where the method didn't actually help from a speed-up perspective landon uh who were our research scientists who worked really hard to get this method up and running tried his darndest to find a way to get some speed out of that and struggled but it also has a positive regularization effect so especially for longer training runs it is still beneficial so these two failure modes here one is you know bad bass lines and we've seen this a lot the other is these hypothetical fantasy land speed ups like lottery ticket is a hypothetical fantasy land speed up at mosaic we care about time and cost those are very unforgiving metrics um you know well clock time doesn't care about flops and it doesn't care about number of steps it just cares how long it takes to get to a certain accuracy and so stochastic depth does help but you know i hope there are two teachable moments there for people who are thinking about this space so if i can unpack just uh a quick thing about the short and long training time say 160 epochs to uh to maybe test a new algorithm compared to maybe uh you're training the new gbtx on your data set and you basically want to run it for as long as you can right like how do you think about these two resumes of short and long training times so the way that i think about this is in terms of pareto curves the the entire view of the world at mosaic is about pareto curves so i don't there's no right answer to the amount of training time really it comes down to what is your budget and what do you care about and maybe you don't have enough money to train gpt3 forever maybe you know you do maybe you're a big tech company and you've got a bunch of new data you need to get a model deployed for ad recommendation as quickly as possible so you don't have time to train it for weeks even if you have all the gpus in the world and so the way that we think about this is we train the model for all different amounts of time for any given model our baseline is not one baseline we take our baseline and we train it for 10 fps and 20 epics and 30 epics or we actually double you know each time but we say we don't know the right amount of time to train so we will create a pareto curve where we look at the trade-off between number of seconds it took or dollars it took versus accuracy if you train for longer generally you get higher accuracy that's not true in general you know you may eventually start overfitting or something like that but by and large let's just take that assumption for the moment so you get this beautiful curve and a speedup method is valuable if it moves that curve up into the left so if it moves that curve you get a better trade-off at all these points or at least at many of these different number of training steps and you know so you can think about a couple scenarios let me give you a method that takes the exact same amount of time to train but gets you higher accuracy like you know a regularization method like mix up which takes two examples and interpolates between them and then runs that through the network mix-up doesn't really take any additional time but it gives you better accuracy on the one hand that means you know for the same amount of time or the same budget you were putting in you get better accuracy on the other hand you could just train for less time or fewer steps and get the same accuracy you had before accuracy can be traded off for speed these two quantities are interchangeable that's really the key idea that a pareto curve shows you and so things that regularize and improve accuracy are speed ups for the most part some of them like stochastic depth actually you know hurt you on short runs and benefit you on long runs so there is you know there's a little more nuance to it there but you know by and large we think about this in terms of pareto curse and that means that bad baseline is just a point that's way far out on the right but it's way past the point of diminishing returns and so you know it's kind of a silly point that isn't even it may not even be on the pareto frontier as far as we're concerned because it may be that you can train for a lot less time and get exactly the same accuracy which means it's not on the pareto frontier and so we would just ignore that point as you know non-relevant that's such an interesting way of thinking about it and yeah i saw a lot of fredo curves as i was looking around the mosaic website and getting caught up and so uh so yeah i love that way of thinking about how to think about the contribution of these methods but if we kind of step a little uh back into model augmentation and um you mentioned stochastic depth works for resnets but if you have a just a sequential network where you know say node a goes to b goes this c goes to d and you can't drop a path because then what are you going to do so it's kind of specific to the architectures and then so another thing is um you've implemented a squeeze excite and i love squeeze net all that research the squeeze bert when they generalized it from computer vision to nlp as well and uh there's also uh alibi replace attention with alibi and that those sparse transform all that attention stuff obviously yep so exciting so how does um and before we really get into composer and the interface and how to use it how does that um how do those layers integrate with the the way that you've defined your model code so the way that they integrate is we do this thing called model surgery which is a module that we've written it's part of our library which basically allows you to do find and replace within a model graph and this is really important for these kinds of model modifications i want you to be able to start with standard code i want you to start with torch vision or start with you know hugging face and then you can use our methods to replace the modules that you want in hugging face you can replace the attention blocks with alibi attention which you know doesn't you know doesn't cost you anything in the speed side and gets you much better you know short to long sequence generalization which means you can train on shorter sequences which is great for efficiency or squeeze excite which is kind of this interesting animal it gets you much better performance and resonance but it it makes the model bigger so it can lead to memory issues and it can and it slows down inference so it's actually a trade-off there in terms of squeeze excite we don't include squeeze excited on some of our top benchmarks because the one golden rule i've had at mosaic so far is don't make inference worse i don't want people to have to say well you made training more efficient but i have to trade off inference i want people to be able to say you know mosaic is just strictly better using composers strictly better these methods don't hurt me they only help me no side effects or at least no side effects and inference so squeeze excite is great it's kind of a basic form of attention between channels of a convolutional network that's the way that i think about this and so i you know those kinds of methods are tricky because if you modify the network you're getting into this tricky territory where you may change the cost of inference blur pool another example which which uses anti-aliasing on down sampling same idea thankfully slow down for blur pool is very small for squeeze excite there's an appreciable slowdown but it's so helpful training becomes faster inference that's a trade-off one other thing i'm curious about is um say you're trying to maximize how much model slash gradient you can fit on one gpu is that uh say is gradient checkpointing the best way of doing that right now um yeah i think so so we have deep speed integration so we can use deep speed zero which does great in offloading um so it'll actually you know or activation offloading so it'll send activations off of the gpu onto the cpu and then pull them back in so that you can basically cram an arbitrarily large model into your gpu memory if you'd like to um so we we have integration with deep speed i think zero one and zero two um so you know you can go ahead and use those today our friends at deep speed have been really helpful to us again this is the nice thing about being a new layer in the stack you can kind of collaborate with everyone i could pretend i'm still an academic because you know i like to think we don't really have any competitors just friends so you know deep speed is available and you can do this you know it does cost you something but that's something can be worth it if you're able to cram a bigger model on there yeah so also on the friends topic i really love when i was looking through the composer examples the integration with hugging face and how hugging face has done so much abstraction on the model code layer that it's like as much as auto model for language modeling auto model for sequence classification is how easily you can interface these pre-trained weights and all the complexity behind the model definition code is completely abstracted away and you just point at the task i can tell you a little more about the integration between composer and then hugging faces it's just like a tren does the model surgery super interesting and and then it just has like a new training layer to it yeah so what we're really doing is you can bring in your model pick your favorite model and just bring it in bring it into composer and you can use it with our methods in our trainer in fact there kind of there are two levels to this one is that for all of our methods or for most of our methods at least we have what we call the functional api which is where you can use these methods in any trainer you want in any training loop you want to take these supply torch lightning go ahead um now there's a reason we built composer we built composer because it's really hard to deal with some of these methods and get them to intervene at the right times you need a lot of if statements in your training loop if you want to do this right and we there wasn't really a framework out there that provided the faculties we needed by inventing a new level of the stack you know we needed hooks and tools that nobody had really conceptualized before because you know nobody had thought about this use case before so we needed to design a trainer for that reason and you know we started off in pi torch lighting but it really didn't have the capabilities we needed to actually integrate all these methods and you know getting back to that composition of methods question the reason why we call it composer getting all the methods to interleave properly and run at the right times if you want to use more than like two methods is incredibly difficult if you're doing it in your own code so composer takes care of all that work for you it you know each method can have we have events that happen at different points of the training loop each method you know can get triggered by events at different times it can modify the state of the training loop we make sure all the methods in or leave in the right order if you know there's one operation where all the methods go in you know we basically have a stack you know the first method is the last method out because you know methods need to clean up when they're done and those sorts of details are really really hard to replicate in a place like pytorch lightning um i don't say we took a lot of inspiration from our friends at fastai we really owe them an enormous debt of gratitude not only for the inspiration on a lot of the ideas that we're using but also on the inspiration for the trainer where we use two-way callbacks where the callbacks can modify the training state so our training loop is very clean and then you write these callback functions that can you know that can take root at any time during the training process based on these events and can modify the training state that allows you to inject arbitrary code into your training loop without getting this wildly complicated training loop full of if statements so again thank you to our friends at fast ai we really owe them in that respect but you can bring your own model to composer if you want to and you know hugging face has a bunch of fantastic models we're not going to redo that work that's you know great work that our friends at hugging face did and you know people already use those models we use those models so we had integration for that you know um our friend ross whiteman who produces the tim library of computer vision models we have integration there as well so you can bring in your model from tim and use that we're working on integration with several other model zoos that are out there because quite frankly we don't want to build these models from scratch that's not our core competency that's not what we should be doing anyway and even if we did you'll want to use the model for that's industrial strength already you wouldn't want to use our composer version so you know we have them there if you want to use them we have some basic models we built in or bring your own and you know our method should be compatible and if they're not it's pretty easy to modify the model surgery to fit the new model that you brought in if you know the layers have different names or what have you it that's such a fascinating engineering design and the two-way callbacks the way it lets you interface with these different things one other thing i'm still kind of trying to piece together with my own knowledge is how the data augmentation flows with it how you have say the data loader and then because usually i kind of think of it as like import your data set define how you're going to be sampling batches from it define your model code and then the optimization logic so how do you kind of tie together that optimization logic with the data loading part so the you know we use the the standard pi torch data loader right now um and we're working one of actually the big things that we've been working on a lot of streaming data loading because at the end of the day you know you don't want to have to keep your data locally especially for big data sets and so we want to make sure that experience is seamless we're using the web data set project um and we'll be moving to the new pi torch data loader that's coming out soon um but with respect to the data augmentations this is a part of the pipeline i'm less familiar with but my understanding is we have you know it's just another event is that you know on the data loading we can you know run an augmentation at that point just another place in the pipeline to intervene so it's just another place where callback can show up and modify the data can you tell me a little bit more about how you think about which operations happen on the cpu and which operations happen on the gpu because this is a messy messy question so this is this is a less of a problem for nlp where the data is smaller and you don't really augment the data typically this is actually a pretty big problem for computer vision where we do a lot of data augmentation especially in you know these mod these modern self-supervised models where we actually do a crazy amount of data augmentation much more than we would do in a standard you know supervised vision setting um the problem with data augmentation is you gotta run it somewhere and it takes compute to run these image transformations you have two choices you can either run them on the cpu using you know standard libraries you can run them on the gpu using nvidia dolly if you run them on the gpu you're giving up some of your gpu throughput and you know in our experiments you gave up 10 or 15 of your a100 to do data augmentation there shouldn't end up being worth it for us on the other hand if you run it on the cpu you can end up bottlenecked on the cpu it can take you so long to do the data augmentation on the cpu that you're actually not using your gpu fully your gpu is way more expensive than your cpu so you never want to be in that situation and what we found is that there are a lot of really interesting papers out there that use you know these kinds of crazy data augmentation methods um and advertise like you know you can train in fewer steps yes but you're bottlenecked on cpu even you know the a100s on aws are very close to cpu bottleneck on rest at 50 on imagenet as it stands because the a100s are so fast and quite frankly amazon selected a pretty crappy cpu to go with them all things considered on our own internal cluster you know which you know stay tuned on whether other people might be able to access that at some point soon we picked cpus specifically for this purpose to make sure that they were powerful enough to keep up with the augmentation so the problem here is that you know our best number resnet 101 at about 3.5 x speed up on those awsa 100s that is data loader bottlenecked that is actually bottlenecked on cpu data augmentation we could go faster we've broken aws's assumptions about the nature of the instance they should build in the workload by speeding things up so much on the gpu that now the cpu has become the bottleneck and you know now we're working with our friends at mit who wrote the ffcv library to get that integrated because that's a way of basically doing a lot lower level operations and the cpu to speed things up um and you know we'll have that integrated i think it's actually already integrated on cfr10 and should be up on imagenet soon which should alleviate that bottleneck and you'll see all our numbers on imagenet go up a bunch more by virtue of that so it's you really got to think whole system on this stuff that's so interesting and i see that as a lot of these methods i think they have to do some kind of offline style computation like maybe it's an exponential moving average of the weights and it has that kind of background computation of doing the exponential moving average or the stochastic weight averaging similarly i think the um the sam sharpness aware minimize i think it also has some kind of optimization computation it does what are you oh sorry oh yeah yeah for sam you have to basically do the backward propagation step twice um which you know can get pretty expensive so i'm also really curious if you if you think about these like pretty kind of ambitious meta-learning loops where you have sort of like a teacher student you have like a teacher inference so maybe things like uh you know meta pseudo or even maybe kind of coming more into the things that are already in composer we have og mix where augmix has what augments is for people who don't know is you take your original image you form three augmented versions of it by randomly sampling three configurations from rand augment then you do a pixel-wise averaging of that to get the final augmented image and the the next extension they did was augmax where they add an adversarial controller into the weighting of those pixel wise averages so you're starting to add more kind of optimization in these little things how do you see that kind of idea of adding optimization not just in the gradients of the classifier but also in say those kind of meta layers of these kind of algorithms playing into the composer design as well i think it starts to necessitate a different computational architecture at that point because the augmentation stuff has gotten so expensive so we don't use augmeix we have it implemented augmix we have found no scenario where we can get around that cpu bottleneck for aquavix you have to do nine image augmentations your cpu bottlenecked across the board randogman we found works decently if you have you know if you have a good enough cpu to gpu ratio that can either mean you slow things down on the gpu you use methods that you know make things slower but reduce the number of steps or you know again it's a balancing act we got to make sure that you know our gpu and cpu throughput are balanced or you just use a bigger model which you know takes longer to train or you have a better cpu like we have in our own internal cluster that you know that allows you to use rand augment productively another interesting scenario where this starts to help if you look at the aws t4s not the you know not the 100s but the t4s they're pretty wimpy gpus they're built for inference so they're about a third as fast as a v100 but aws way over provisioned the cpus so actually on a t4 you can use augmix and randognet just fine so in fact in scenarios where you want to do a ton of augmentation and you don't really care about how long the job takes just cost the t4s may actually be a much better bet than the v100s or a100s but you know augmix we haven't really found any other scenarios where it makes sense randogman it makes sense in certain scenarios especially for long training runs but by and large we you know matthew levitt who implemented these methods for us you know i think was pretty frustrated at the end of the day that you know they didn't make sense simply because they were slowing down each step and they were taking you know the bottleneck was no longer on our most expensive component on our gpu it was on the cpu so you know great methods very useful but when i look at some of these self-supervised learning papers and i see the amount of augmentation they're doing i kind of take a step back and go i wonder if they're fully utilizing their gpu um but in terms of how to do this going forward or you start to get into these adversarial scenarios you need an accelerator just to do your augmentation like and i think nvidia i haven't looked too much of the h100 set up if i recall correctly they have kind of this coprocessor h100 that does networking data loading and preparation to send it to the real a100s or the real h100s so you know i think nvidia's got in the memo that having these co-processors is necessary you need a gpu just to prepare your workload for your gpu then you get into questions of you know is that the best use of your resources or should you just drop it all together um you know that's a complex systems optimization problem but this really changes the architecture and i will say that makes a lot of sense for vision i think it makes a lot less sense for nlp where data is just smaller in terms of number of bytes and we don't do data augmentation so this does beg the question do you need two different hardware setups for vision and for nlp which complicates the hell out of everything yeah and i think maybe in nlp too you um you don't really like repeat through your training data as much like if you're going through uh yeah like web text you probably will never even make it through the whole data set with most of the training so it's like why augmented to begin with and maybe just quickly i wanted to see what you think about maybe like offline data augmentation where you kind of start storing so it's not stochastic anymore it's deterministic and you store the augmented data set there's a paper called the deep bootstrap framework where they sample from a ddpm generative model and they uh they store the data set it's cfar five million it's available on github so you can you know you have this offline uh in that case it's sampled from a generative model but you can imagine applying rand augment and then storing it do you think that would be a useful direction going forward i think the data sets end up getting too big you know for imagenet you're repeating through the data set let's say 90 times conservatively that used to be the standard back in 2015. in the past couple years i've seen this creep upward to 180 and then to 300 then to 600 times through the data set um imagenet is already uncompressed probably about you know 400 gigabytes i'm guessing now you have to do that 100 times over and you're talking you know what is that 40 terabytes of data um so then you get into questions of is it worth the storage costs and is it worth the network bandwidth costs to stream this data you know over your cluster that can become its own issue especially if you're streaming the data from you know one cloud region to another from one part of the cloud to another usually for data sets like imagenet you can cache them locally or at least they automatically get cached in memory if you have about a terabyte of memory you you obviously can't do that for a data set of this size so i think this can work for cfar once you get beyond cfar i don't think it makes a ton of sense and really at the end of the day you start thinking you know in terms of storage costs it's cheaper to have a co-processor that will do augmentation or to just buy some really big cpus than it is to pay for that extra storage so one more idea i want to run by you before coming back to the efficient machine learning is what if maybe we stored the vector representations of imagenet so we had something like some kind of api offering that produces vectors and then we saw those vectors and then we start having complex networks from vector representations or maybe it could be like the vq vae where it's still like a matrix but it's like you know how they map it into the discrete codes but there's some kind of compressed representation and maybe we could have storage and things like a vector database that would store those things no i think this is a this is a great idea and in some sense that's almost where we are as a community you can think of simclear and moco or you know whatever the latest greatest method is you can tell them a year out of date on this topic um you know that is what we're getting essentially is you know if we were to just train a network on top of those vector representations you know that's what you do when you fine tune sim clear moco assuming you're just fine-tuning the head and not the actual network if you fine-tune the network obviously it's more complicated um one of the biggest challenges there is again augmentation wise yeah you've got to actually perform all this augmentation at some point but i guess to your point if we have a smaller representation of the image then we can do all that augmentation offline and i've also always wondered i'm sure someone's written a paper about this can you find you know ways of basically doing an augmentation to a vector that that is equivalent to an augmentation you know of the image itself and you know so that you don't even have to store the image so yeah i think this is a yet another place where these vector style representations are incredibly valuable so you know i'm all ears on that the biggest challenge is still you've got to get that simclear moco style network and what i'm finding and chatting with a lot of folks in the community is they want to train their own network on their data because they have a huge amount of data self-supervised learning unlocks this gigantic amount of data that most companies and just many individuals have floating around and everybody nobody wants to use gpt they all want their own gpt nobody wants to use a pre-trained sim clear moco they all want their own and you know people often ask me if you know we only do fuchsia or zero shot learning what's the point of training you know what's the point of mosaic if people are using you know these vector representations and that's it um my response is you know so far i have seen no evidence that people are training less um the h100 would not be coming out if people were training less so as far as i'm concerned as long as there's training there's mosaic and people seem hungry to have their own bert model pre-trained on their own task or you know fine-tuned quote-unquote on their own data but they have more data than the burt was pre-trained on to begin with anyway so rumors of the death of training have been greatly exaggerated i'll say that much yeah i think that's one of the most fascinating topics in our current stage of deep learning is this can we just use prompting where we just have these maybe discrete prompts or predefined templates to get large language models to do whatever we want train on infinite data and but even even then you would have like a continuous prompt tuning usually where you put the prompt into a latent space and you still want to put gradients through it and those papers seem to be the ones that are more successful anyways and so kind of and then quickly before coming back to prompting but and then the idea of yeah like one can one model produce in vector search when we talk about embedding apis it's like can one embedding api produce embeddings for every kind of application whether it's e-commerce biomedical literature mining you know news reviews and it's like there's a paper called don't stop pre-training which shows that you know that's not really the case fine-tuning on the domain it's going to be useful so coming back to prompting and this idea of still putting gradients through continuous things that you put in the middle of the networks uh what do you think about and there's papers like vnl adapter where they only tune four percent of clip they don't fine tune the whole architecture what do you think about that kind of masking in the architectures to you know put these little layers that you update i think this is a great idea i've seen this you know i wrote this paper on training only batch norm where i started with the randomly initialized network um and thought it was cool that you could still get good performance just by training the batch norm parameters which you know for reference don't even they're not really real parameters they're kind of scaling factors and bias factors for random features in the network i thought it was cool that you can train even okay with this let alone train somewhat decently i won't claim you can train well that's a bridge too far for that paper but you know in writing that paper i discovered this is an old idea most people aren't starting with randomly initialized networks but there's this great paper called k for the price of one that shows okay you just you know create new sets of batch storm parameters for each downstream task and keep all the other weights fixed so i think this is a pretty profound idea and it's a nice spin on the idea that we've always had of transfer learning where you just replace the head of the network and keep everything else it looks like the right answer is actually you know having trainable features scattered throughout the network is the way to go so you know i think this is a deep profound idea the biggest challenge being as soon as you scatter these parameters it's not as efficient as just training the head because you have to you know forward prop and back prop through all these other layers in the network there's a cost to doing this but you know given how much better it works i'm sold you know i think this is a fantastic idea it's i won't give credit to any one person for this idea because you know i'm sure this idea goes back into the 80s and before if we really were to dig and find evidence of it just as all good ideas in deep learning do but it's an idea that seems to keep coming back and there's a reason it keeps coming back yeah that paper training bash norman only batch norm that just was like how can that work how it like similar to things like uh layer sharing i can't believe layer sharing works either these things that kind of defy sort of like my core like principles of how these kind of things work and so yeah mentioning that um you still have to kind of train this whole thing and so is that kind of still was bottlenecking bringing all the all the way of the spar sub network masking back to life kind of not really i think this is i'm guessing if you were to sit down with a couple folks from industry and i have no insider knowledge on this i'd guess that you'd hear that if folks have kind of a multitask setting they're using this especially i'm guessing on mobile devices things like this are in use it's a great way to you know to efficiently store many different networks in a small amount of storage so i would i would hypothesize to you that this may be running on your phone or your computer right now and you just don't know it and i don't know it right now so it's a good enough idea i'm sure it's in practical use do you like the vowels of google somewhere who's doing it do you like these like mixture of experts models do those kind of things excite you they do excite me i think they excite me for a couple reasons one is that you know it's a way to train bigger models without increasing the cost of inference you're kind of you know you get more parameters for free in some sense it's not quite free you have to manage all these experts and you have to use the right ones at the right times but assuming you know ideal costs and not thinking too much about how do you load the right experts at the right times and all that good stuff it's basically keeping the cost of inference fixed as you increase the parameter count that's that's a huge brilliant idea and the other piece is you know it's sparsity i like sparsity and you know that's what i'm known for sparsity shows up in all different forms of neural networks and as i mentioned weight sparsity which i've studied probably you know in i would say it's the most interesting part scientifically but one of the least useful parts of sparsity mixture of experts is really interesting to me scientifically but more importantly it's incredibly useful practically so i'm a believer i think the biggest challenge with mixture of experts is there's some threshold you cross where you need a sufficiently large network before mixture of expert starts to make sense um and the question we should ask ourselves is is that threshold going to increase over time or decrease over time will we get better at training non-mixture of expert models to the point where you know really you know mixture of experts aren't useful until 100 billion parameters aren't useful until a trillion parameters um or actually a mixture of experts going to scale down to the point where they start being useful all the way down i think this is a big question for us as a field i'll point to one trend as context for this i look at vision transformers this is something that you know i still train resnets i'm still a believer in convolutional networks vision transformers are popular and i think they're really exciting um there have been a bunch of papers in the past few months showing that actually if we just give all the resonance the same tricks we gave division transformers we can you know resonance work to larger and larger scales than we would expect otherwise and do better and better um the threshold where it makes sense to switch from a resnet to a vision transformer is getting higher um and you know if that trend continues i kind of wonder will vision transformers always be niche to some extent because we'll keep getting better and better at training our cnns i'm not entirely sure but you know it's always a question to watch where do those thresholds go do they go up in which case cnns may be the future do they go down which case vision transformers may take over same thing for mixture of experts does that threshold go down and do we you know do we have a small mixture of experts that are more useful than the equivalent small models or do we get better and better about training those you know quote unquote small models as in smaller than 10 billion parameters to the point where a mixture of experts you know only makes sense at the absolute largest scales you know open question but always worth keeping those trends in mind yeah that's fascinating the idea of bringing the mixture of experts down i yeah i mean it kind of it kind of seems like the like the spar sub network's idea it's very similar to it right in kind of the purest form of it and um so i kind of did want to ask you a little more about attention and convolution just quickly um i've really been liking attention for say uh like feature like latent space fusion and particularly with like multi-modal image text models where you you kind of compress it into bottleneck all the way such that you have a vector representation of the image you have a vector representation for the text and obviously we like to search through the vectors with our dot product but then when you want to fuse that you can have that attention do you like that kind of attention layer for that thing and when you think about generalizing it do you think convolutions maybe like an mlp layer to fuse them or you know you could just concatenate them along the i think with some computer vision architectures you just kind of concatenate a tensor of features along the channel axis and then just convolve away with it right so i think they you know i'll give you the boring answer which is what i like has nothing to do with it um at the end of the day it's what works and you know i'm a believer in the science and we should do the science and understand what works best for application we should look at the trade-offs in performance um you know i don't have enough experience with multimodal models to tell you what the right answer is i will say cnns are still pretty darn good for you know basic computer vision tasks but you know when it comes time to combine features from multiple different representations or multiple different you know kinds of features from different kinds of backbone networks it would surprise me if convolutions work well in that setting because really they are a great fit for dealing with image activation maps and not so much for dealing with you know arbitrary tensors although there were a lot of cnn based nlp models in the pre-transformer days so i would expect that you know attention or a feed-forward layer would work better as to which one and in what context um you know i leave that up to you to go and do the research and find out for yourself and report back to me and let me know i know there's been a lot of push back and forth of like mixer models and you know just kind of non-attention but attention looking not attention transformer style models versus attentive models at the end of the day we can do the science and find out for ourselves and you know i like whatever works best you do the work let me know that's a great answer transitioning into what i wanted to ask you next is um you have such a clear thinking about asking scientific questions i really admire that i think the way that you're able to pose these questions identify the interventions confounders that are in these experiments is is you know very profound skill and i want to ask you how your experience has been with your science and then starting a startup how has the science and the startup creation played together the startup will not succeed as a business unless we do good science and that is somewhat by design um you know i i don't want we have a big research team at mosaic it's about a third of the company right now and it will you know it may not always be that large because we got to build a business but it will still be a significant research team and it's going to grow the company will grow faster than the research team you know the research team may be growing by 50 or more by the end of the year also you know mosaicml.com jobs come you know come apply for a job we're growing but i don't want us to be a google brain or a fair where this is a fun this is a fun diversion for an executive who's already made all their money and just wants to do something interesting you know no pointing fingers but i want to make sure that we're doing science that is essential to the business to get these speed-up methods to work well we have to intervene into training very carefully and we have to understand what we're doing and why the why makes us more efficient in terms of developing these speed-up methods the why is good for the business we are certainly in this wonderful scenario right now mosaic where the science and the business are aligned we can't have one without the other and so we you know we have to we have to do great science we have to be rigorous we have to turn over every leaf we have to be even more rigorous i would argue than you have to be in academia because we're not just trying to sneak some graphs through the publication process and you know sneak some claims through this has to actually work and it's you know anybody who's written a paper knows going from getting a graph in a paper published to getting it to actually work is really hard and really scary and often we say we'll leave that to the practitioners well here we are we're the practitioners and also the scientists and we have to make this happen so you know i think the science we're doing at mosaic is as good if not better than the kind of science that would be done in academia and it's impacting the real world so that makes it all the more difficult i will also say when it comes to scientific communication as i mentioned before you know we need to basically put prescription labels on each of these methods and describe everything about them it looks like you've seen our method cards which our researchers worked really hard on and i spent most of the past month on i'm just revising and cleaning up all of them to tell you how does this work what does it do what are the trade-offs what are the warnings associated with this what side effects might it have because you need to know you won't trust me to say like i did something crazy to your model and i won't tell you what it is we have to earn your trust and so we have to communicate all this information a publication is not the most efficient way to do so but scientific communication is still paramount for our research team so as far as i'm concerned science and business completely aligned and i'm really grateful for that kind of opportunity because you know i think it gives the researchers so much more of a sense of meaning than you'd have if you were just writing papers all day yeah it's so great that's so interesting to hear and um yeah the the startup and the science the way it plays is so interesting and i didn't want to quickly come back to um the yeah i love the mention of the interaction effect between these different methods and you mentioned that at the beginning of the podcast it was just like mind blown for me thinking about that do you see that as the way that you're going to empirically collect these data these the experiments to see uh how does stochastic depth mix with augments and how does ghost batch normalization interact with some kind of other strong regularization such that your regularization is going to be you know ruining the training is this kind of similar to hyper parameter tuning where you might use things like maybe osha where you have uh different resource allocation these kind of strategies to test all these configurations how do you plan on exploring the combinatorics of that space it's sort of like hyper perimeter tuning except that you know i'm not sure the space has the nice smooth shapes that you want for hyper parameter tuning where you know as you increase the learning rate accuracy gets better and then worse for example i don't think the space necessarily has those dynamics because adding a method or not adding a method is a discrete transformation in a lot of cases you can't just kind of slowly phase in a method in a way that would make hyperparameter tuning work much better and you know these methods again have these weird crazy interactions that are you know not smooth in the way that hyperparameter tuning would expect so really i think the way that we're going to do this is a couple of ways one is we're getting intuitions for the way that these methods might interact for how you you know i mentioned before balancing the training process if i've got something that you know requires extra augmentation on the cpu i want something that will actually slow down each step on the gpu ideally that is traded off for training for fewer steps something again like sam like sharpness aware minimization but sam if it can reduce the number of steps and not slow things down too much that can make room for a better augmentation technique and you hit this virtuous cycle so it's it's a systems optimization question in that respect there are also things like the regularization budget where you know we have a lot of different regularization methods we could use we should use the ones that basically impose the least overhead for the best improvement in accuracy we developed this method in-house called call out it's a lot like cutout which is where you take an image and just cover up a square patch of it except instead of covering up a square patch we eliminate rows and columns from the image so that you can actually lower the resolution it's like cut out but faster because now you actually can take advantage of those missing pixels so these are the sorts of tricks that you know come in handy and you know these are the ways of thinking about it but there's a science here and by improving that science we'll get better about coming up with these recipes not only ourselves but ideally automatically for customers um you know and then on top of that once we have a good recipe for a particular vertical like say resnet50 on imagenet our hypothesis is this should work reasonably well for other related models like densenet or for other image classification tasks so there's a way of kind of taking advantage of all of this but it's right now it's kind of it is more alchemy than science and we're we're trying to develop a science in a completely new area for the record i think this is a great academic problem and i would encourage folks in the academic world this is one of several big new exciting academic problems i wish people were working on right now that i've discovered only through kind of being pressed into the real world and having to deal with some of these constraints that i can usually avoid as an academic and um so it's super interesting i mean the idea of lowering the resolution all these efficiency gains in general maybe can enable say video processing where we've been bottlenecked by having to have these absolutely massive data sets to try to have high resolution high frames per second video but really what i wanted to ask you about next is um have you been like how have you grasped your intuition with the deep double descent phenomenon and things like rocking where they show that you they leave it running i think they even say that their experiment they had just left it running accidentally and what happened is it was like overfitted then like two days later suddenly it like has learned generalizable representation somehow has how do you uh wrap your understanding around double descent and these kind of things double descent bends my mind a little bit you know i there's a paper that i'm working on with you know a student in in my old research group where we're looking at the relationship between sparsity and double descent to try to understand you know maybe you know is the double descent phenomenon really just you know having a lot of parameters means you find a small subset of parameters that actually work well it just gives you more options but you're still learning a low dimensional representation so this is something we're talking about and thinking about to some extent the double descent phenomenon doesn't really matter in practice and i'll go out on a limb and say that for a few reasons first of all we're often not training these models to the point where they can memorize the data often the models cannot memorize the data gpt3 can't memorize the training set it's nowhere near big enough resonant 50 can't memorize imagenet it's not big enough so you know on that hand we're you know we're not really in that regime necessarily double descend is maybe an exaggeration of a regime the other part is that in order to get those two descents in order to get like that sharp peak in between you have to work for it you have to like the deep double descent paper had to use random labels to get that to show up that wasn't something that just shows up naturally it looks almost like a smooth curve with kind of maybe a little bend in it but you wouldn't really tell this phenomenon is happening so it does i think beg the question coming back to where i started at the beginning of this conversation to what extent is this a natural phenomenon and to what extent is this an artificial phenomenon that you know we can we can suss out to use to understand what happens naturally but you know there's always that question of when you see an artificial phenomenon how much does it have to do with practice and how much does it have to do with the natural world um you know i i think deep double descent does have a lot to do with the natural world perhaps i don't understand why yet and we don't understand why yet but it's always worth asking that question how well does our model or how well does our setting of inquiry actually relate to you know the settings we care about and in deep double descent i i think we've oversold a little bit just how natural these settings are they're not that natural at the moment that's an eye-opening take away from me because yeah i've just come to accept deep double descent as something to be aware of but yeah i actually haven't really encountered it in the experiments i've been running it it hasn't really been something that has come up with me but so the next topic that i'm super excited to get into is i see mosaic ml as almost being like electric cars in the way that this is going to help with climate science and say the damage that is done by training things like gpd3 and if if we just continue this arms race of you know just run as much as you can to you know build your ai businesses compared to these kind of efficiency algorithms which are reducing that kind of uh footprint on this can you tell us about your mission on helping with climate science with this with these algorithms yeah so a big part of at least my personal motivation here is the energy usage aspect um you know just as these models cost a lot to train and they take a lot of time to train you know time is money and time is co2 now we haven't looked into co2 emissions that much at mosaic simply because i recognize it's easy to do a bad job of co2 emissions calculations these data centers are often using renewable energy or near energy sources that are less co2 intensive these data centers may use different mixes at different times of the day so the amount of nuance you need to be able to tell people yes doing using these methods reduced your co2 emissions is actually really difficult and i you know this is the scientist in me i don't want to do it unless i can do it right so if there's someone out there who has expertise in this topic or is excited about looking to work on this i want to hire someone to basically spearhead this but i know i'm not the expert and i'm not going to read a couple papers and become the expert someone has to have studied this for a long time to really know exactly what to do here so you know i don't want to do this until we can do it right but by that same token just if you look at sheer energy usage you know time is money and time is energy and we know that at least from an energy usage perspective if you can train more efficiently you can reduce the energy usage and i look at the h-100s i you know i had a rather uh cynical tweet yesterday that i will try to explain which is you know the h-100s are pretty remarkable they're 6x faster at least if you look at the sxm highest bin part one that we probably won't see for another year but um you know 2x of that is from floating point eight so kind of you know a little bit of cheating in some sense as you start working your way down that 2.5 x or 3x faster that you get is by doubling the power you know you're going from 350 or 400 watts to 700 watts on the sxm parts on the highest end um moore's law is dead um you know dinard scaling is dead the idea that every two years you get a new part that is the same cost as it was before but is twice as fast for the same energy that's dead um we got a new part for twice the energy that is about three times as fast and you know jury's out on the price but i expect it'll be pretty expensive at first so you know plot that trend out and you know maybe the next part we get will be 2x the power 2x the price and 2x of speed i don't know how much further we can keep pushing this without throwing more power at the problem and so we've just hit our limits the models are getting bigger what is it the stat is like 4x per year larger um in the past couple years um hardware has gotten let's see we've got two years um we doubled our power output um and we you know 2.5 x or 3x star compute so 50 improvement over two years um call it 25 per year uh 25 per year in terms of hardware efficiency per watt um and probably efficiency per dollar um 4x or 400 increase in model sizes and model you know model fluffs you can do the math and see there's a pretty huge gap there and we just need more compute and more power to keep up with these models it's not like the good old days of the 90s or the 2000s where you know what was the saying you know um you know intel intel makes and then microsoft takes you intel doubles the amount of compute for a dollar microsoft makes software twice as resource consuming we're we're not in those days anymore we're just consuming resources way faster than we can keep up with it and hardware is not going to do it all the fancy architectures in the world that folks are coming up with are very exciting and video is still the fastest most efficient ship in town despite all the interesting hardware architectures coming out from startups and you can see what's happening to nvidia so what's going to close that gap either it's just going to be out of reach incredibly expensive and incredibly bad for the world or we have to change the algorithm that's where i think mosaic comes in i think this is this is the future of how we're going to be able to take that 400 increase per year and make sure that it's at the same cost and i think that was one of the uh the comments on your tweet was uh if you have it people are just going to run twice as many uh hyper parameter configurations they might at the end of the day people will spend their budgets um but budgets are not going to increase 400 per year so we can talk about energy we can talk about you know time but let's focus on cost at the end of the day somebody's going to say you need to spend how much money to train this model you spent four times less last year what are you talking about um and if budgets don't scale that's the end of the day if people want to use their budgets to do more you know if people get their budgets 4x and want to do hyperparameter tuning you know to some extent more power to them i can't stop them and you know mosaic will do better as a business if they decide to do that but you know i like to hope at least you know at some point some executive's gonna say what the hell are you doing why are you spending all of our money training this one model and at that point you know we're going to have to do something different super cool jonathan thank you so much for uh for your time and for coming on the podcast and discussing all these just amazing things i think the work that you're doing mosaic ml everyone out there should really try the composer it's such an easy thing to add on to your existing workflows and testing out these methods is so incredible so thank you jonathan so much for telling the story thank you so much for having me i'll give two quick shout outs actually three quick shout outs at the very end one is i'm just the messenger this is the work of a huge team of researchers engineers um you know everybody at mosaic so it's really nice that i get to show up and brag about 30 people's worth of work um you know i really am the manager here i have not written a line of code since i got to mosaic so credit where credit is due to the incredible team um and i'll say you know if you want to join that incredible team we are hiring like crazy so mosaicuml.com jobs and the last thing i'll say is you know check out composer give it a star um you know i promised everyone that you know i try to get us to a thousand stars by the end of march and the end of march is coming up so you know a star would be great um composer.dev you can take a look thank you so much for having me thank you and uh the composer to give it a star i'll be linked in the description of the video first thing so go click that and do that right now thank you [Music] ", "type": "Video", "name": "weaviate_podcast_12__jonathan_frankle_research_scientist_in_deep_learning", "path": "", "link": "https://www.youtube.com/watch?v=ZiBkspwrICA", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}