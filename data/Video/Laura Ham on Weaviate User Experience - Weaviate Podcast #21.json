{"text": "Thank you for watching the 21st Weaviate Podcast with Laura Ham! Laura Ham has worked on Weaviate at SeMI Technologies ... \nhey everyone thank you so much forchecking out the webv8 podcast today isgoing to be truly one of the mostspecial episodes of the week a podcastwe've had so far laura hamm is makingher debut on the wva podcast uh laurahas created so much amazing content thathelped me personally learn how to usewev8 and all sorts of things from wv-8examples to say the design of thegraphql and blog post tutorials aroundthis graph like data model and all theseexciting things so uh laura thank you somuch for coming on the webva podcastthank you conor yeah it's great tofinally be in your podcastso could we start with um the theintroduction of kind of the origin storyof what led you to be working on vv8yeah that's a good start so um that goesback already like five years i thinkuh when i started working together withuh both the one of the founders of ofsammy and weaviateum yeah back then we were working bothas a freelancer or some other projectsand bob started working on on wev8andi think like we were both superinterested in natural languageprocessing and how like all thedevelopments in that world happened andthat's where yeah we startedcollaborating andumyeah started working on we fadebasicallyso i think bob tells us when i asked himhow he first got into this he says theking minus queen plus man that kind ofthing was that a similar inspiration foryou that led you into this uh vectorembedding kind of part of naturallanguage processingyeah yeah definitely so at the time youhad like these first word effect modelsfor exampleuh coming up and with word effect ofcourse words are represented as vectorsumand that's also where you have this kingminusumman plus female is queen thing uh iscoming from of coursesobasically with these algorithms likeword to fact we were like they allowedus touse neural networks to understandlanguageand word associationsand umyeah what's what's been reallyinteresting to me umspecifically is that you can calculatethen like how similaruh different words or different conceptsare uh to each other so you can findsynonyms or related conceptswhich is of course very relevant if wetalk abouta searchbecause i think likebeforewith natural language applicationapplicationspeople were working are still working alot with ontologies or taxonomiesandthink like ontologies and taxonomiesthey need to bedefined beforehand so before you startusing an applicationandi see the potential problem or challengethere is thatthere is also always one or more peopledefining this ontology or taxonomy andthey have to agree on somemeanings of words because in an ontologyyou have a word and you have the contextof the word or what it meansso people have to agree on this meaningand um that's really hard if you workwith a lot of peopleyeah basically if you then yeah then youdon't know how to deal with sign namesoryeah you basically have to deal withokay i want to give the best searchexperience to a user so you need to knowwhat kind of words he or she is usingumas an ontologist and i think this is oneof the the problems thatalgorithms like word to fact or like thebirth models today with neural networkscan solveyeah i'm really excited to dive into theontologies and and yeah i rememberanother story baba told about theycouldn't agree whether it's a sea or alake and that kind of idea where youhave the disagreement on the terms andand uh going to new york with you andseeing the ontologies the symbolicgraphs i had never really been exposedto that kind of thing before but iquickly want to dive a little deeperinto the into your five years at uh semitechnologies in wva can you tell meabout sort of the and then we'll kind ofget deeper into how we evade itself hasevolved but how has your experience withthe kind of startup company growing howhas that beenyeah yeah that's been great so um solike five years ago i started more in aresearch-like position so i was lookingat technologies like word effect and howcan we use that to build agood neural search experience basicallyum so i've been experimenting a lotbuilding a lot of prototypes to test umandwhat's good to mention i think is thati've been working with like real usersor the customers from the start so wereally learned from them what theirchallenges are and like how we can solvethem and test things outsoyeah i was really more like a researchpositionback then i was also stillstudying i was doing a master's degreein human computer interactionso that also like the lessons that ilearned in my studies i could directlyapply inthe research that i did with reviewbecause my goal was to makethe search experience or the yeah useror developer experience ofnatural language processingor search search engines as friendly aspossible so it was really good that icouldput it in practice directlysoand during that time of course thestartup started to grow so we've it gotdeveloped uhuntil like the versionuh 1.0 we had like two years ago i thinkand since then the team has also beengrowing and that's just really amazingto seesoyeah during that time i've also beendoing a lot of talks at conferencesi've been in close contact with the opensource community so always like tryingto grow the community uh learn fromtheir feedback that they haveand umyeah since i was involved inimproving the developer experience i forexample designed the whole graphql apithat we still have today and thedocumentation that is still out thereyeah those kind of thingsand then now that the team is likeway biggeri can focus more uh on likeuhyeah one thing again that's more likethe researchso it's really excitingyeah that's so interesting and i'mreally curious so could we dive a bitmore into the human computer reactionand the user experience the graphqldesigni love how it has this like we've eighthas this get and then you can chain onthese functions like if you want to dothe near text search the near vectorquestion asking and then accessing theproperties of your data i can take methrough the design of the graphql andmaybe principles from human computerinteraction what what that kind of fieldof study how does it help you thinkabout these kinds of thingsyeah yeah super interesting soyeah first i will take a step back sofirst i want to sharelike sharewhy we have graphql soum because we also have normal restfuland restful api endpointsand but those like restapi endpoints are more for accessing theresources so accessing the data souh updating datauploading data umdeleting data you can all do those kindof things stressful but then if you wantto start searching through itumyou need a yeah you cannotumdo it in oneyeah single url query basicallyum so that's i mean the user experienceof doing a search in a restful like wayit's really hardso that's why we started looking atrafcal basicallyumand yeahof course like when i started designingit my first goals wereyou know the the generaldesign principles that you should havewith api design like it should beintuitive to bereadable[Music]yeah simple as possible like consistencyin terms of naming etcum[Music]and then i started designing and then ithought okay we're we're building asearch engine which worksfor example with semantic search in textor images butso i wanted toalso give a nice experience when youwrite a query that it feels like you'rewriting it in natural languageand that's where thethe get and thennear textand then like the class and propertiescome from because you can really read itas okay i want to geta data that is of type articleand from that i want to see the namethe content the authors and so on and iwant to find articles that are nearthis piece of textso i want to yeah i try to integrate thehuman language as much as possible inthere yeah well that was really wellsaid i really like that flow of it andand and that also helped me understandsort of the difference between the restapi and the graphql api if you want tohave like arguments to a rest api youhad to have like this long url rightwith like the question mark and thatkind of thing yeah exactlycompared to like stacking it out withthe graphql rightyeah it's really interesting the idea ofanatural language interface with a searchengine onsay like multi-modal data arbitrary datawhere you could say uh give me a pictureof a dog playing with the ball i don'tknow but like that kind of naturallanguage uh interface experience uh socould we then kind of transition a bitto um so when you have the graphql andyou have the new ux how does that tieinto thevector database of webva and the all theindexing stuff and how does the wholeteam kind of coordinate that kind ofthingyeah so right now we have like a big v8core team so like possessive coreengineers who are really building uhabbreviateand next to that we have like theresearch team that you and i are inum and umyeah so the goal is to have like we doresearch like we find new technologiesthat might be a potential feature orlike an improvement or somethingand our current approach now is thatyou know we um we pick up these thingswe test if they are useful for us we dolike experiments um yeah to evaluate itand then the goal is to come up with agood design for which we can pass on tothe core team let's say and this designthat consists ofyeah what this technology is but alsohow it should look like from a userperspectiveso how it should be used by user and umyeah that's that's a really importantthing that we uhfind really important with building vvais that we always try to have thehighest ux as possible so[Music]that means the design is from an apiperspective because wav8 ends at apilevel soall the things that the user ordeveloper using vfit will see is an apilevelso this is the user experience and fromthere we start building a designbasicallyandyeah so once we have that we can pass itto their core teamand they can start building uh and do itlike umin the most efficient way of course andso onso when you wheni want to ask about your approach to theresearch process and when you see newpapers or new ideas coming out do youinstantly have this perspective of okaywhat would this look like in we v8 isthat your first kind ofapproach to thinking about itumyes and no so of course i'm alwaysthinking about like can we use this withrev8 yes or no but it's like um i kindof address a bigger question um and thatisdoes it add value ina search experienceso weaving is a effective search engineso the goal is that people can use thisto improve their yeah search experienceso if it's a really nice newtechnique or new machine learning modelor whateverthat is not related to this or would notimprove the search experience then imight use it for something else but notfor we've yet obviously so there'salways this bigger question in my headlike okay can we use this to improve theexperience basicallycan i ask about the like the developmentof search and the application of searchhas any particular say use cases thatyou use to reason about most uh likemost problems say like the question ofwho's the best basketball player of alltime searching through wikipedia like doyou use these kind of like examples tomo to like guide your thinkinguh yeah i think there's always use casesin mind when i umwhen i think about usability or like uhfor potentially new featureandyeah this of course can come from my ownimagination but it can also come fromwhat i get back from like open sourceusers or umyeah other people that use a searchenginewhen you find yourself designing thingsdo you ever like maybego too particular into one applicationand then have to think about how doesthis generalize out or is ityes that's a tricky thing right so it'sum yeah of course like of course you'reyou're sometimes i'm experimenting andthen you really dive into one subjectbut then it's really important to indeedtake that step back and then generalizeand think okayis this only useful for this particularproblem or can it beuseful in more situations and both isfine but you need to be aware of whatwhat case it is basicallyyou generally have like maybe like lessis more is that kind of a good way ofthinking abouthow you kind of design these likelike function calls almost like when youdesign like a graphql api it's kind oflike a function maybe the the moresuccinctly you can describe it is thatthe better way kind of generallyyeah that's agood way of looking at it i think ofcourse it should beit should be simple and intuitive but ifyou oversimplify it that's that's alsodangerous of course it should still beclear what it meansand umyeah now that we see that we umare adding more and more features toimprove the whole search pipelinebasically this also umyeah so another topic comes to my mindnow is that how do we scale this uhgraphql api to serve all the possibleuse casesbefore we had like umyou could do you could use rev8 for justa semantic search so it would be asimplequery where you have one[Music]piece of text or one image that youwould search bybut now with all like umyeah other methods to increase like makethe pipeline biggeruh to improve the search experience andimprove the results basicallyum the graphql should also be uhyeah so be able to support that ofcourse and then yeah it becomes reallydifficult toto come up with a good language thatsupports it basicallyyeah i'm so excited about this idea ofthe adding new features and building outthe pipelines uh quickly before we diveinto that can we talk about say uh whatgoes into the integrations with uh youdeveloped these coding tutorials of howto combine wev8 with gina and wev88 withhaystack how's your experience beenmaking these like open source tools fittogetheryeah yeah that's really exciting so uhhaystack by by deep set andgina as well are neural searchframeworks which basically means thatthey focus on the entire ecosystemaround neural search so that goes fromlike data pre-processing and fine-tuningum to the searchas wellumandyeah those frameworks are like should beused in in pythonand that's really nice if you want tobuild like a whole search experiencerightumbut if you want to scale this toproduction or to like big data then youalso need an efficient database thatlies underneath this applicationbasically and that's yeah rephrase oneof the possibleyeah databases that supports this sosince i think the beginning of this yearwe have these integrations with haystackand gina that if you use their frameworkyou can use review as adatabase needed basicallyandyeah that's just super cool to work withwith these people on this projectyeah i think that's a super interestinguh connection obviously being like partof we've had i like the vector databaseand where you have the h and swefficiently find the nearest neighborstructure with the persistence and thedatabase features then you cani put pipe stream pipeline that upstreamto things like question answeringre-ranking and overall thinking aboutthese pipelines so i'm super excited todive into this topic of search pipelinesso we have maybe retrieval we could havesentenced transformer retrieval bm25retrieval aggregate step and then crossencoder all these things can you take methrough your understanding of searchpipelinesyeah that's a good question soum how i see it is that like basicinformation retrievalum that comes like that interacts with adatabase so if we have we've ate asdatabasethena basic information retrieval is justlike making query and getting someresults back from the databasethis however so we usebi-encodersmodels so that'sour neural networks thatmake factor representations out of wordsor sentences umand thenlike data will be basically retrieved inwv8 by comparing the vector of a queryand comparingwith thevectors of the data objectssoum this is a really efficient method umbut it also comes with some drawbacksthat ismaybe not as accurate as you want to beumfor this like we also next to buyencoders that create vectors out ofwords we also have cross encodersso cross encoders do not produce vectorsbut theycompare a query directly with a sentenceor a word and give a value between zeroand onehow similar it isthisthis kind of methods if they are trainedon theyeahrepresentative data sethave a higher recall like higher higheraccuracy thana buy encoderbut it comes with a cost because it'scomputationally very expensiveso what's interesting now is if wecombine these cheap methods of buyingcoders with the more expensive but moreaccurate methods cross-encoders sothe idea is basically to have like use abuy encoder to retrieve initial set ofdata[Music]which is a subset of the of of courseall your datauh relevant to your search query andthen re-rank themuh reorder them based on a cross-encoderwhich can be executedbecause it's uh like a shorter listbasicallyum so that's one way to improve theinitial results of information retrievalumyeah and there's uh i can go on withlike more waysso we have umyou can all so this was one way ofre-ranking results basically you can goon with that so if you want topersonalize search based on some userinformation for exampleyou could use another re-ranker tore-rank those results again andhave the most relevant results to thisuser place on top so you can basicallyextend this pipelinewith many models uh that you wantuh tohave a like a good search experienceyou knowi love what uh so i think bob pitchedthis idea of having like um you're inthe ocean and you cast a big fish netand you get a bunch of fish on the boatwith retrieval and then say you have tohavelike workers who pick up the fish andit's more expensive to have the workerscompared to the fish net then they'remore accurate at looking at the fish andmaking sure it's the fish you're lookingforso one of the topics that really excitesme about this is thinking about thiskind of zero shot generalization thingand these models that have beenpre-trained and they come off the shelfandwith retrieval models say it's kind of isay i think like the deep learning endof we've eats vector representations arebuilt on these pre-trained sentencetransformers let's saythey have data sets like wikipedia whereyou use the heuristic of neighboringparagraphs make these representationssimilar or maybe you have like the msmarco data set where they'vepublished like a million uh querycorrect result or like what they clickedon pairs and so you use that to trainthese retrieval models and then it'sbeen trained on such a big data set thatyou say it willdo this zero shot generalization to yourproblemand i think this kind of way that wehave these pipelines you see it as a wayto say correct the error of zero shotretrieval so you know the you're tryingto search through say the we vvadocumentation and you're trying to useretrieval from wikipedia and of coursethere's a bit of a domain shift so maybeinstead of having to fine-tune ourretrieval model we could just fine-tunethe cross-encoder right or these kind ofthings how do you see this kind of likezero shot generalization of theretrieval and the cross encoder we couldhave zero shot cross encoders toodoes it make it like simpler to adapt itto your problem kind of oryeah i think yeah that's reallyinteresting so i think we need toexperiment with that what works bestbecause i think if you haveso if you have the review like a vpadocumentation which contains a lot of umlike specific words to alleviate whichof course do not appear in wikipediaum then if you just use this model trainon wikipedia it doesn't work right onyour technical documentationi thinkjust umjust fine-tuning a cross-encoder likeretraining a cross-encoderwouldn't necessarily solve the wholeproblem because if the results were notcaught in this big net of fish in thefirst place then the expensive fishmancannot find the right fish anyways so wealsoneed tomake sure thatthose right fishes are initially alreadycaught in that big net basicallyandyeah so that might mean we also need tofine-tune a bi-encoderor what's also really interesting isuh combine it with um with sparse searchso for example bm25 which just looks atumso it doesn't use neural networks oranything it just looks at uh the wordsitselfso bm25 is maybe like a like a cage likea lobster cage like it like we have likeour fish net and we have maybe like someother kind of fishing device how do yousee the interplay of uh of the bm 25 andthe sentence transform i think it's abig question butlike just kind of taking it from thebeginning what like what does bm25 addto sentence transformers or that theyeach miss out onyeah yeah that's super interesting soi think like before we had just sparsemethods so justtf idf bm25um which just used like exact words toretrieve uh dataum but now like of course these bm25 andtfidf methods they don't know anythingabout synonyms or glycemiso that's where dense methods so methodsthat aretrained with a neural network tounderstand the languagesolve that problembut if you just use dense methods likelike review is usingas we speakthenif you use that on a data set that isnotuhlike as out of domain of your tradingset basicallyyou might see problems um and that canso thenit becomes interesting to combinevmlike methods with dance encoders toum yeah bridge that gap basicallybecause i think likeif you are if you have a search querythat contains some very specific wordsumlike umwhat does near text do in wv88 if youwant to search by thatthen we know that you really need tohave the word near text in your resultso if we use bm25 on that queryumwe will seefor sureresults with near text but if we justuse that by encoder we might seerelevant results but maybe the near textisn't really on top or likeumso yeah if we combine those methods ithink they like can solve a lot of outof domains problem problemsyeah it it definitely seems like theycomplement each other well and i thinkthis comes back into the user experiencedesign andhow you're going to interface with thesetwo ways of search so i guessmaybe the most straightforward way to doit would be to have say an aggregationscore right where you have like 0.5times the bm25 score plus 0.5 times thesentence transformerscoreuh rank maybe might be a way to likeheavily emphasize so so how is how wouldthe user interface design then take aconcept like bm25 send this transformerand then create an experience in whichyou can combine them easily for searchpipelinesyeah yeah that's super interesting sofirst we need to find out like what areall the possible ways that you cancombine them because the the way youjust described is like a linear way soyou have on you doboth searches so the dance search andsparse searchin parallel parallel and then you umdo like umalpha times the dance better times thesparse and you combine them so that's alinear waywhat you could also do is userf method and what that means isyou take both result listsand you don't look at the individualscores but you look at the order sothe results that appear on top on bothresults will appear on top in the finalresult as wellsothese are already two methods ofcombining the results basically maybethere are moreum so we need to make sure wefirst look at okay which methods tocombine them arerelevant um and then secondly tosupport them in a graphql design or likeanother way ofin an api it doesn't have to be graphqlmaybe umyeah that also supports if there arecoming more methods coming in the futurethat also supports likeuh yeah to be able to scale to thatbasicallythis is one idea that uh that i think wetalked about a little bit recently thati think might be kind of overdoing itbut do you think maybe when you have aquery you kind of classify whether thisis a query that should go to bm25 andyou should kind of add one more part inthe pipeline whereuh the query what is near text in wvaand then thisquery intent classifier or pipelineinference thing goes this is prettyentity centric we should probably sendthis to bm25 and weight it according tothisfunction do you think that would maybeadd like too much to it or do you thinkthat maybe couldjusti guess coming back to the userexperience it could compress it a lotbecause you just have this thing thatcreates the pipeline and then it's likeuhlike pipeline wizard or somethinglike thatyeah yeah that's super interesting so umi think we're definitely moving in thatway that we like umso we've it itself is already modularand i think like the whole graphql orlike the whole api maybe it is notrascal uh whole api design should alsobe modular so i picked this element inmy pipeline this element i want tocombine them like this umand thenum yeah and then like you said likelooking at the queries or really queryparsing and doing maybe even somemanipulation to the create to makeuh the list of results betterin the endum yeah that i think that's that's supercool so if you like what's what i saw inresearch is thatbm25 queries which are very specific sobasically long queries with a lot ofentities or named entities in theretheyum achieve a higher score in the resultlistthan um than shorter queries or like oneword queries so if we could somehowlearn from a query if it is veryspecific or not we can give a higherscore to the bm25 compared to the densemethodsand this leaves a lot a little bit ofexperiment but i think this isdefinitely what the way we're movingyeah i really want to dive back into themodular design but you brought upsomething that i think is pretty isreally interesting in that um you knowit doesn't always have to be socomplicated right like something likejust the length of the query could beenough of a heuristic to tell you whichone you probably want to use i can alsoask about the design of theuh the scalar filters how you can say doa vector search but then you can alsokind of label it symbolically and filterthe search that way how how did thatkind of come to be in theinspiration of combining it that waywithuh simple kind of filters as well asthis kind ofvector alignment rightyeah yeah um so that's really around thestrong points of rehab i believe soif youjust have a vector index or vectoryeah vector index where you can storevectors and retrieve vectorsumthat are basically algorithms like um ithink like files and scan you can justput vectors in there but not the dataobject itselfso if you doso you can do just a vector search whichmeans youput a vector as inputs you get vectorsas outputbut that's not really usable if you talkabout real data because you want to seeum the actual data objectum so we've a doesn't only store thevectors but it also store the actualdata objectwhich meansyeah if you search by one you can if yousearch by vector you canget actual results backthat belong to two vectors ofcoursebut then we've it also combines thisvector search with like you said the thestructured filtering um we call this awear filter so like in sql you can putthe air filterand that umso in that you can say some restrictionsto your search some filters so let's sayyou want like the color of your productalways to be read in the results thenthis is a really strict filter what thenhappens in a search if you combine thiswith a near text search we will firstperform thisstructured filteringand it uses like inverted indexes forthatwhich then creates an allow list for aneffective search so it's a two-stepapproach basically and yeah i think thatthat's reallythat'syeah really cool from a technicalperspective but it's just reallyumyeah it's just something you need tohave from a ux perspective if you talkabout the database i thinkyeah i think that's huge for the the uxof deep learning search generally if theif you are getting a nearby uhshoe image and you can filter it withthe red uh color tag you you already geta much better kind of uh space of thatandso i see kind of a couple ways of takingthis conversation but quickly i wantedto talk about the idea ofi just quit just kind of the comparisonof face and weaviate and uh how you canstore the symbolic things somaybe to give like a quick example ofsomething i've been doing is studying uhyou have archive title abstracts and youvectorize the abstracts and you storethe vectors in weaviate but then youalso store the indexand so what that lets you do is then youwant to take the title and see if youcan find the matching abstract and youdon't just have the vector because thenyou don't know if it matched it or notyou also have the index so that's likekind of one example that i've been doingthat shows how storing the additionalthings in addition to thevectorhelps you like study these things andthen also i'd say if the topic isweaviate versus face having thatpersistence of the database layer issuper useful for this because it takes asuper long time to like upload a millionvectors right and and that kind of thingso having it like it's going to stay inthere as a databasei've also found that to be incrediblyuseful so uh and then soi know i'm kind of jumping around theconversation topic but so what so thisinverted index thing is like um youinvert the category so it's red and thenthe things that have the property readhow does an inverted index and the h windex how do those two things coexistoh that's a good question that's reallytechnicalum soi think inverted index is something thatwe know already from relationaldatabasesumbasically what yeah like you said whatyou do is you invertthe yeah the properties of a data objectinin the tableso instead of like having each dataobject with a property and a value youwould have a value first and then eachdata object that has this valueum whichthat basically just improves theefficiency like it has alower latencyi think that the whole h and sw is likeumthat's an yeah in a n algorithm so uhbrookside's nearest neighbor algorithmthat allows for very efficientum vector search so that's on the otherhand basicallyumyeahyeah i think the that approximatenearest neighbor thing is definitely oneof my uh my favorite topics around we'vegate and i'd i love to get back into themodular thing but can you tell me aboutyour experience with the approximatenearest neighbors kind of how you firstbecame introduced it introduced to itand then sort ofwhat that unlocks sort of when you'rethinking about these search pipelinesyeah so a num so let me take a step back so if youhave likeum a database full of vectors which arelike basically points in a highdimensional spaceand you want to search through it youbasically want to do a k n search sonearest neighbor search from a certainpoint and this point is your searchqueryumso with k n you basically retrieve allthe elements or all the vectors withdata objects that are closest to thissearch queryumand it retrieves yeah all of them tolike how big you want to have itbut if you want to do this on a largescale and with large i mean i don't evenmean billions or millions but eventhousands it's really expensive so thecomputation time to retrieve thosenearest neighbors is really expensiveso that's where youthat's when that's whya and m so approximate nearest neighboralgorithms were introducedum becauseyeah if you have like a realuhreal-time application with a lot of datathen k n1 skill and basicallyh and sw so using graphs basically todo this kind of uhyeah sureretrieving the nearest neighbors butlike approximatelyis one way of doing ityeah i think it's amazing i mean doingthe like i've been doing some testswhere i'm doing the exact nearestneighbor and it really takes a crazylong amount of timeso i yes i think that's such a huge partof it and so we could then come intolike the modular design and userexperience and so like one module of themodular design would be the these vectorindexes the inverted uh object indexesfor say the color is red the hospitalname is etc the state is etc whateverthese symbolic filters and then kind ofgoing upstream to modules likenear textpipelinesquestion answering and then also it'skind of this um how this enables theintegrations with uh gina and haystackis this kind of modular design right andum so i wanted to ask about the you'vemade videos on how to add custom modulesand i want it kind of asked this topicabout the extensibility of webv8 andadding custom parts to ityeah so first like talk a bit about howthis modular design worksso basically aviate itself is is avector search engine vector databaseso without any modules it's a purevectordatabaseso that means you need to upload yourown vectors with your data and query itwith your own vectorsthen you can use for example abuy encoder or likea retriever model like a bird modelsentence bird model or resnet50 forimages you can add that as module on topof wev8 tocalculate the vectorsof your data and of your search queriesif you don't have them yetsoumyeah we've yet offered some of thesemodules out of the box so we have ourown trained moduleswhich are very lightweight but there isalsomodules like you can use everymodel that is published on hugging facefor exampleor open ai which have pre-trained modelsavailable to do this kind of inferenceand um yeah so you can use these modulesfor these models as an alleviate modulewith vvasumyeah then to your question you can alsolike not choose not to use any of thesepre-trained models butuse a model that you trained on your ownorwhateverumyeah so basically what you then do isyouuse basically design of a[Music]yeah if you have text you use design ofa text factorization module in viveviateand instead ofliketalking to this module model fromhugging face you talk to your own modelvia[Music]api callsyeah i think that the graphql design isso intuitive for how you can chain thesethings together and i definitely thinkthat whole thing is really interestingand so i'm sorry to be uh sort ofjumping around the topics a little bitbut i think umso we've talked about kind of like we'veeaten i think something that makes it sounique we've we have the um the vectorindex and how that plays with theinverted object index for adding thesesymbolic filters but then one other bigpart of like the we va data model thatthat i really think is interesting iwant to talk about is the graph likedata model and having relations betweenyour data and how you can say uhuh what's the hackernoon or it's a winewas the wine was made in and then like abrewery ororiginally from say france and placelike that so can you tell me about thegraph like data model and how you canlink things each thing can have its ownh and sw anduh inverted index right with each of theclasses that you'd link together withthese um cross referencesyeah yeah that yeah um sowe we then review like the the datamodel works like you have clusterclasses so data classeswhich have a name for example a wine umand like this data class can have someproperties with some value so propertycan be as you said or like it can be thename of the wineso you have class wine and then like avalue name of the wine you have uhum whether it's ummaybe like the type of grapes orwhatever and then but you can alsodefine like where it's coming from umwhere it's like who is the producerwhich color has itumso but yeah these uh properties can be aclass on itself so if you have a lot ofwines and you see thatthe countries for example is a limitedlist of course so you can also make aclass out of this country you probablywant to do thatso then from the wine you can refer towhich country it's coming fromand um yeah this is basicallya graphlike modelum because you can make close referencesbetween one and the other in aone-to-many umrelationship kind ofyeah wayyou think maybe we could sort of try topitch how we could put the we vadocumentation into this kind ofum this kind of data model soone way that i like it the most is withthis kind of multi-modal sense where umsaylike using the twitter thing for examplewhere you have tweet has imageand then so you can separate out thelike text vector index with the imagevector index and i also see you could dothat with like code also maybe like soif we have like a we've yet uhin the vva documentation say there is atutorial and we want to say has code hastext and then you havea code vector index so if you want tosearch just for a code snippet you cankind of isolate it that wayis this making sense how i'm trying todescribe this like could you take methrough how you might decompose thatdata and then organize it in this kindof modelyeah yeah multi-modal models are reallyinteresting becausewhatso if a machinery model is trained onboth text and images they can representit inthe same vector spacethis means you can search by onemodality for example text and findresults in the other for example imageor codesoumyeah this is really interesting becausethe way we need we need to vectorizetext is is like uh different from likehow we look at images so what iscaptured in the image so combining theseor in code so combining these into onemodelallows us to search through themby whichever modality we want and thisis also something that we can use in thedocumentation later documentation searchbecause the way you you need to indexlike encodetext as vector is different fromencodingcode or images as vectorsyeahi'm curious if you think this idea aswell say we have these um so we have thewe vva general slack where we see a lotof these questions about specific thingsandsay we want to have like a class that isum slack conversations and then we haveanother class is the um the vvadocumentation or maybe like a codetutorial that would like walk youthrough an example of how to use it andthen you maybe link link it that way soit's likeslot conversationuh most relevant todocumentation that kind of way ofbreaking down the data and theni guess like then thinking about how thehow you can kind of apply the relationalfilters through itdoes that kind of thing make sense as away to use the data modelumyeah definitely so you could use thequestions that are posted in slackchannel or on stack overflowasuh yeah aspart of your data model of course so youcan like link those questions with theactual content where the answer iscan be found inyeahyes yeah i think that whole thing isreally interesting the all the differentparts of the vba model and then the userexperience design how the graphql letsyou access these things i think all thatis just really interesting so kind of umstepping out into more of a meta topicyou've been an avid conference speakercan can you tell me about like what goesintouh sort ofyourpreparation uh the the whole the wholekind of experience i guess is somethingi'm curious about is someone who's likekind of interested in doing this kind ofthing as well and like you've traveledall over europe and in new york and solike what is that experience beenyeah yeah it's been really great so i'mreally happy that we can travel againlike after all the the go withrestrictions so actually i just startedto travel since this year before that idid like a lot ofmeet-ups and conference presentationsand like workshops online but i thinkthis wholething of being in person and being ableto actually talk with people see theirresponses like answer their questionsand learn about like their interest andmotivationthat adds so much value to um towhatever you're doing i thinkso i think like um yeah i get reallyexcited to to speak in front of thesepeopleand explain them a really new concept umand thenlike seeing their reaction okay thiscould actually be something that i canbe usinguh and then talking about like possibleuse cases and so onumyeah in terms of preparation soum usually i'm doing like introductiontalks to vector search so what is vectorsearchum because this is a relatively newconcept for most of the peopleso um yeah in that case i have like umsome slice ready and then i adjust likejust like the slides to to theconference i'm speaking atum to make it like fitting with theaudience etcand umyeah i also sometimes give workshops andthen i have likea bison jupiter notebookready to to share with the audience thatthey can actually build their ownvector search application within thehour basically soyeah that's just super exciting to doyeahlet me ask a quick question on which onedo you prefer betweenthe overview or the hands-on workshopkind of thingoh that's a hard questionumyeah i think the the workshop because ithere i also start with uh like a smallpresentation because i want first peopleto know what they're actually gonna doum but thento see the excitement of people thatthey like do their firstneural search basically their firstsemantic searchand then like seeing the possibilitiesthat it opens for them it's just yeahthat's really coolyeah and i think that it comes back tothe kind of hopefully there's aconsistent theme i know we're goingaround the ebay topics but that userexperience and i think there wasi remember reading a techcrunch articleuh with bob where it's like um it's likebob put their articles into techcrunchand then showed them how to do thequeries right and because that graphqldesign it's so like you get it rightaway when you start like using the neartext and i think that really ties thewhole thing together with theuser experience to putting on theworkshop to people saying like oh i getwhat this is right and that whole kindof flow of ityeah yeah i think that's a reallyimportant uh topic so umif you are using especially a newtechnology like some like newyeah new technology new databaseyou don't want to beheld back byum yeah a difficult uh way of queryingit byfor example sothe query language uh is like the api isthe main interface for peopleso that that is the first thing that weneed to like just like the documentationis we really need to have that uhintuitive and as simple as possibleandwe also so that also umso we v8 has client libraries like inpython java go[Music]javascriptumso that you can use vvate easily withinyour own applicationsand of course like the team like stefanand martin are working on that so that'salso part of the whole developerexperience basically that we want tohave as high as possibleyeah and i also think that um that semiconsole that lets you uh connect to thewebv8 instance and then do the graphqlqueries without even uh writing pythoncode or what and even in the python codei think it's you it's the same thingright where you just have like the threeuh quotations string bracket and thenyou just paste in that same graphql butyeah the the console thing i think thatis like a beautiful user interface foruh for getting started with it alsoyeah exactly yeah and also umso now that the community is growing wealso see that people work like peoplestart to contribute to eve but alsocontribute toexamples or like uh some demo projectsthat are made with vvaso we see people that made likea really simple user interface forqueryingimages for example in a multi-modal wayor a really intuitive user interface fordoing it in text orhaving a movie recommenderor search engine and i think like if youstart to abbreviate and you look at thisrepository of examples it's really easyto get started and that's what reallyexcites meyeah i mean i think that's such aninteresting uh part of what we're tryingto do with our next like you know fewmonths of doing this is like how quicklycan people learn webv8 how quickly canthey get their data and how quickly canthey understand how to query it and yeahso i think also just the whole thingabout content getting people to disputewith webview and i think this is likesuch a great uh coverage of topics and ithink from everything from theunderstanding of the graphql the userexperience hearing your story of thetime that we've ate and umdesign of modules extending it withother plugins like haystack gina huggingface open ai and how this whole thingfits together i think this is a greatoverview so thank you so much laura foruh for coming on the wii va podcast andexplaining these thingsthank you connor for having me that'sgreat", "type": "Video", "name": "Laura Ham on Weaviate User Experience - Weaviate Podcast #21", "path": "", "link": "https://www.youtube.com/watch?v=gjJBYcYMB-o", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}