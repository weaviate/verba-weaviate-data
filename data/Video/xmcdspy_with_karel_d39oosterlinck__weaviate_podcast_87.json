{"text": "Hey everyone! Thank you so much for watching the 87th episode of the Weaviate Podcast! I am SUPER excited to welcome Karel ... \nhey everyone thank you so much for watching another episode of the weeva podcast I'm super excited to welcome Carl doer link to the weeva podcast uh Carl's a PhD student at NLP at the University of gent and a visiting researcher at Stanford University where he's put together currently the most advanced dsie tutorial out there it's a super cool use of this uh inferring the labels to then retrieve and then rank this extreme classification taken down with language models Carl I'm so excited to be welcoming you to the wva podcast thank you so much for joining thanks Conor yeah I'm I'm very excited to be here thanks awesome so could we kick it off with the the problem that we're solving with dspi and extreme classification yeah so extreme multi-label classification it's a problem where you're trying to you know apply a fixed set of classes to maybe an input document but there's an extreme amount like over 10,000 so you know as an example one of the tasks we tackle is um kind of encoding job vacancies so a a lot of people put job vacancies online and the European Union actually developed an antology of like skills competences occupations and that is you know it's very nice if everybody can speak that same language um of course people don't always write vacancies with this antology in mind so looking at vacancies and figuring out how to map it to that antology is like an important problem or for example in biomedical NLP um there are huge ontologies you know enumerating all types of medical reactions all types of drug drug interactions all types of like active substances in drugs but you know when doctors write papers or when they write clinical notes we're not expecting them to you know perfectly adhere to this ontology all of the time so um it's really important to get medical papers into this anology so that when we need to search over the existing literature or when we need to do like statistical anal Anis we kind of have those agreed upon labels um so yeah that is that is mainly the idea behind extreme multi-label classification um yeah yeah so I I love that the like in uh weeva we have filtered Vector search where you'd apply like a filter to the vector search and but so we have this kind of like tagging the scientific papers with these terms and something that we could use with Vector search but I'm just so fascinated with how you've uh like this idea of trying to prompt language models to do you know a thousand class classification it it really breaks my brain I understand like usually when we're talking about useing llm for filtered Vector search we're talking about like you know parse this query and and then add like who this like say the query is like you know uh information about dspi where the speaker is Omar katab it can like parse the query and then populate that class they only have like you know 50 speakers in my data set but so I'm I'm so curious how do you prompt llms to do a thousand class classification yeah the general idea is that llms probably get the gist of the task like an llm probably I don't want to anthropomorphize here but like an llm can probably be prompted to be very good at saying looking at a vacancy and saying like oh this is clearly a job application for a python programmer and the person needs to also be familiar with scrum agile developments those are all Concepts the LM has prior knowledge of the problem with these anies is that um maybe python programmer is not an exact an entry in anology but it might be python brackets computer programming right so the idea is that the language model probably can be prompted to do this but it's really grounding to the ontology that causes a lot of issues so maybe the naive first thing that you want to do when you have 50 classes is you just put those in the promt and you let the language model pick out of them like hey these are the options but how do you do this when there's like 10,000 antology items and maybe you know every antology item has a description how do you put 10,000 labels and descriptions in a prompt seems kind of excessive even with really long context models that would just be a huge prompt making you know making you pay a lot for inference so really our idea was let's just try to get the model to get the gist correct then zero shot map whatever the model predicted on the real space then kind of get a short list and then do that final you know hierarchy options rerank them filter them out so um there's basically two ideas there so one is indeed that you need this kind of pipeline or this program this orchestration of inference and retrieval to solve this task and another idea is we really want to use zero or few sha components we don't want to train a specific retriever for every type of extreme classification problem we kind of we want to rely on the flexibility of a language model to like adapt to any task based on a few demonstrations and then we want to rely on some general knowledge in a pre-trained retriever to you know make it scalable and match whatever the language model was outputting into this space so cool I I love the connection with the vector search and how it's used to map it into the ontology it's really novel I haven't I can't say I have another example I I guess something that inspires my thinking is I really like this project the gorilla llms from Berkeley and I mentioned this on the podcast all the time but I love this idea of like the LM formatting API requests and then there's this problem of like okay well which API do I use and you can imagine the API Zoo having thousands of apis and so you first just have the LM like you know give a sense of what kind of tool it wants and then you map that into what you do have with the embedding search it sounds like that's kind of like the the plan here H um that is exactly the idea and I love that you brought this up because I I was just this week talking to a student at Stanford that also was using llms to Aid programming but in like very esoteric apis like in a programming language the llm doesn't just do out of the box and indeed it's it's the kind of the same extreme multi-label classification Paradigm I'm not saying it's a full solution there but it's definitely definitely a very good first step right it's okay we're going to have the model just in natural language try to describe what kind of behavior is needed to solve this problem then we're going to do a search and we're going to offer this to the language model so I really my long-term hope for this um this work this infer retrieve rank or IRA work is that you know researchers put a lot of you know smart ideas and inductive biases into how do we solve this General problem and then you want to use it to you know formulate SQL queries you apply Ira there you want to use it to extract you know chemical substances from biomedical literature you use Ira there and behind the scenes a lot of the ideas the technical ideas of how do you learn to R what parameters do you optimize which teacher and student models do you use that should all be abstracted for you amazing and so I guess the the the rough part in the I re rounding out the program before we dive into the compiler is uh so you you've retrieved and now you have the ground truth like not ground truth but like you have chemical reactions that are in the ontology and now you're reranking them with the query is it just the I think because this is a vector search podcast so I think most of our listen listeners understand rankers so I just kind of confirming that that's what's happening is that correct so that is what H what's happening currently this is just being done with an LM so you know we have the inputs a first llm will predict queries queries result in retrieved items and then the input and the retrieved items go into a final prompt where you know you take the top 100 scoring items and you ask an llm to filter out anything that's not applicable and this is really important because the retriever is frozen so that means that the retriever is um let me give you an example in these job vacancies sometimes we see a job vacancy that says you need to be good at you know software architecture and then the the llm outputs a query surrounding software architecture and because of we're using a frozen retriever it'll just find a lot of architecture skills you know it'll match very strongly in architecture and then it'll be like oh for this vacancy you need to like understand Building architecture so then you know another llm is like no this clearly was was wrong filters that out yeah gotcha okay super cool so I I really am so curious about the uh the dspi compiler now so we have the so maybe we could start from the beginning you know work our I think so infer is parametric retrieve nonp parametric and then ranking parametric again so could we kind of describe like how that infer part is optimized how like I understand dsy has like a signature Optimizer as well as like the bootstrap examples could you kind of take us through like the whole how it works yeah yeah um this was this was very fun for me to learn and play with and um so I guess the idea that thepi tries to embody there is you can play with the prompt right that is what people are doing but people are doing it in a very messy very iterative prompt engineering way and if somebody says like oh this prompt is perfect on my task and I have a slightly different task there's kind of like no way for me to take that and just apply that so the DSP ID there is you start with like a minimal seat prompt or a signature right something that just defines what the task should be and then you layer optimization on top to figure out what is actually the best instantiation or the best like derivative of this prompt for performance so in the work that we've done uh this involves finding good demonstrations um so you might have some unlabeled inputs and what happens is a very strong like a teacher language model tries its best to you know solve the problem on these inputs and if it's successful we will consider like the Intermediate outputs in that pipeline as demonstrations for a student model um so that is one one way and that is what our current results reflect but the nice thing about the the modularity I would almost say the paradigm shift that thei offers there is this optimization logic is separate from all the implementation work you doing as a developer so I think the the the optimization space like the the space of these algorithms is so underexplored I honestly I think like for the next months it'll always be somebody finds just a better fistic for teaching a student for finding good demonstrations and the idea is you know I change nothing of my codebase I just point it towards a different DSP compiler and my performance goes up um in the same way that you you know program your language model in pytorch and you can point it towards the atom Optimizer or then you point it towards Adam with weight Decay and that ends up being better yeah I love that analogy the the whole yeah like Adam now SGD or you know I think like nesterov I don't remember all the optimizers that well but that kind of analogy is so exciting and I love how you named your repository XM c. dspi and it's like a general program for extreme multi-label classification amazing uh so sorry so before I really do want to you know we we've got about an hour podcast here so up and I I want to firstly just make sure I 100% understand that the infer task are you when you're trying to like your measure of performance should it try to put something on the ontology right away or should it try to that's a really good question um that's a really good question because ideally no ideally you want a system that can learn to write queries that are not part of your antology that at the end maximize your accuracy on the antology the reason why I'm I'm um for example saying this is um colleague of mine from G University they uh they have like a biomedical retriever that's actually trained to match descriptions of ontology items with ontology items and you can imagine that if you use that one in the middle you actually don't want your infer module to just say covid right you want your infer module to actually describe the symptoms the patients is having in the maybe in the biomedical paper because matching performance will be higher ideally um so but the bigger question there is how do you get Supervision in the middle of these pipelines like if you if you say that you want your infer module to just be correct directly well then you already have labels there and you can optimize the infer modules separately but if you're saying like hey the infer module should be able to Output any kind of query as long as they lead to good performance well then of course the you know I think your maximum performance of the system will be better but your search space is much more complicated so you need to it's it then becomes really interesting to think about what kind of heris will I use to you know teach or to like find good demonstrations for these steps where I don't actually have real labels for uh yeah I think there's I think there's just so much to that it but so my my general yeah my general understanding is that with dspi you kind of optimize the module sequentially like top down like whatever is closest to the input that's optimized first is that but but then it's the farthest from the supervision because I imagine you could supervise the I like what you said about you could just take the infer part out and optimize Maybe just that prompt with with the ground truth of the ground truth um reactions and yeah sorry the question is not super well formed I'm still trying to piece this together just yeah is it is it top down that the optimization works or is it sort I guess bottom up like yeah so um uh I guess you know the optimization can work any way we wanted to um the I think the current Paradigm of of how it is being done in thei but of course that is rapidly evolving right the current Paradigm is hey we take this entire logic and we give it to like a a teacher language model like gd4 and we run entire examples through the pipeline and you know the examples that GPD 4 was able to resolve zero shots we kind of know that the intermediate steps were good oh okay right so it is indeed like left to right in some sense where every module probably introduces some error and there is some like cascading error but you know if at the end of the day the model produces something good we kind of label that as a good trace and then all these intermediate steps can come on the students however that is only one way to do it you know you can imagine that if your pipeline becomes very very very very long the kind of you know the compounding of all these errors makes your signal really noisy so you could do um a type of different idea which is maybe more of a backward path through the pipeline where you know maybe you start with the labels and you know the task of the of the last step and you try to have a student a teacher language model kind of reverse engineer what the input would have been maybe you know so you tell the teacher like hey I just need a query that will resolve in Python Programming that would be compatible with this inputs and then you can walk you know right to left through this Pipeline and then you can go even further what if we want to optimize left to right and right to left at the same time what if we have really deep pipelines we could we could apply something like a skip connection right we could like we could maybe say something like hey in the middle of the pipeline do your best to produce the actual outputs and then you could like Short Circuit the optimization flow so that you also get some signal in the middle of the pipeline um those are all like things that are possible in the DSP Paradigm I think they definitely need to be rigorously studied U but currently we're more doing the forward thing where we're just pushing examples through the pipeline and if they resolve we consider them we consider the whole Trace as potential candidates for the prompt of the student amazing Carl thank you so much for that that that just really helped my understanding of it all at night I have to clip that one out and we'll put that one on social media as well nice the um yeah so oh wow it's so I guess that whole idea I mean what really made me made it click for me is just you know you run the zero shot through the full thing and that's a good example and that gives you supervision for the intermediate I think most people who are on my level of just kind of now wrapping their head around the DSP compiler I think that would be the one that stick with them the um the skip connection thing to me it did like you know really trigger my imagination but I it's like I'm still trying to like this whole like intermediate supervision thing is still hard for me to kind of connect the dots on it I guess like like if we maybe come into so I guess I it's I guess there kind of like two things we could look at we could be thinking about like six layers of llm programs or we could maybe uh maybe if we could first just kind of ground this this infer retrieve then rank a little further and then maybe dive further into a really deep program uh so so we have the you know the infer now we're ranking and and so so we would just Trace through at zero shot and then we have some examples is there any like how to now say like the bootstrap optuna where there's like a ban optimization part to it how does that then come into that picture yeah um so I've been experimenting with the optuna I haven't had the time to fully delve into it because uh you know the actual paper deadline for this I work is coming up so I'm focusing on making the results that we have more rigorous um so I think i' I'd let you know the authors there could give you a a better the people that actually wrote that code can give you a much better Insight but how I understand it is you know you're doing discret search in some sense you're searching over which demonstrations do I put in the prompts but you could also be searching over you know which words do I put in the instruction um all of this is kind of like discreete like you know pick words out of the vocabulary or pick and demonstrations out you know out of training data set um and the way the the the the aspy optimizer now works is based on a a random search right or the one that I'm using It'll like take 10 candidate programs this is a hyperparameter based on your computational budget and it'll like try 10 random samples of demonstrations and the one that works best on validation well that's your program but um of course it becomes much more complicated because maybe two demonstrations are both individually very informative but once you have one the other doesn't really add information anymore so you know this just really becomes this this deep kind of discret search problem and you can start modeling all of this with like beijan optimization and trying to like figure out what is the effect of you know this demonstration given these demonstrations um and I think that's what they're trying to achieve there um I'm yeah I'm really excited to get more time to actually get that working um because yeah that's fair it's it's just very exciting to really apply a lot of the you know a lot of the rigorous the speed optimization techniques here yeah amazing maybe a quick question for you is um would the examples be input dependent like maybe there you know or or is it just kind of one set of examples is going to be the best one for the pipeline um that is that is interesting I've been thinking about that as well I think it depends on what type of data regime you're in so um let's say you just have you know in our in our extreme classification tasks you you can have like 30,000 classes let's say you have 50 examples you're nowhere near nowhere near covering even a fraction of the label space so there it would make more sense to try to figure out which examples are just of high General quality and put the model in a state where it kind of produces the correct queries and then you have that prompt then at inference time it's fixed you can imagine that once more data comes online you actually enter a regime where the moment you get a quer the moment you get a vacancy about like programming you can actually go consult a database of examples where you have good labels for and you could actually like oh pick the 50 ones that are related to programming and then maybe then maybe either put those in the prompt or like do another search and turn that into like the programming prompt this is the prompt every time we use to cover like a tenth of the skill space when we think the skill space is about technical skills um and and and that kind of ties in with a larger idea around the whole DSP work that I'm really excited about it's really depending on how much data you have the the the set of strategies to solve a problem differs so much if you have a few data points the best thing that you can do is you can you know take pre written prompts offline and you know do that if you have maybe 50 data points the best thing to do is maybe what we do now hopefully is um kind of decide on a very good General prompt and you know more data it might be dynamically constructing prompts even more data it might be like H let's consider fine-tuning the retriever um and you know an excessive amount of data might be let's collapse this entire system into like one specialized retriever if that's desire terrible but um yeah so so I guess it depends I think I'm very eager to try this and to add this to the IR ra software um but I guess it's currently not the data regime where I I'll be able to measure if this works really well it's all like so new and exciting I'm I hope that like uh for our listeners we're trying to have like kind of a sequential Narrative of it and but I think there's just so many ideas to kind of wrestle with I there there's kind of two questions I like you mentioned earlier kind of like the interaction effect effect of examples like yeah you have you know two good examples but maybe together in the prompt they're not they're not as good as a combination of the good example and some third example and but the the question I I think that is something that we could definitely double click on but I really want to ask this question about um kind of like information and examples like if I'm doing um extreme classification of drug reactions I might imagine that instead of putting examples I might want to put like you know a paragraph about the drug reactions right so where would that kind of fit in of like putting information in the prompt as well yeah that so probably that would fit later in the pipeline because um you already want to have a short list of potential candidates and then maybe in a ranking step you want you know everything that's not obviously wrong based on the name of the label maybe you want to do an additional ranking step we like okay we're going to take the top 20 candidates and we're going to take you know a very strong language model and we're going to give the language model actual medical agreed upon definitions and now it needs to do in context reasoning to figure out whether it exactly aderes to this definition or not um so so that is very exciting the the thing there is you're also you know you're compounding these pipelines so you're adding new calls so it's also more expensive so something I care about here is um you know if we just want the best performance the the pipeline that's going to do the most GPD 4 calls per test input will kind of win um but you know if you're paying $5 per inference that's just completely unus usable for any real world application so I think another interesting thing about DSP is if we're all building these systems in the same framework we can start measuring you know more costs and get a better sense of the overall utility of a pipeline oh yeah that's absolutely one of the things about dsy that excites me the most is um I guess for me there's two things here there's like you could fine-tune a smaller model like you can fine-tune maybe and I I guess that question remains to be answered like how small of a ranker can we have and it's still effective but then I guess the other thing to me that's incredibly powerful especially with like olama making like mistal models really fast is like um you know could we set up like I feel like dsp's about optimizing this perfect prompt that could result in even like you know mistol 7 billion parameter model being able to complete the task yeah um yeah so so I guess my opinions of it as just somebody that uses thepi is um I think finding the perfect One Singular prompt is is nice and is very important um but like you've previously said I think it it could it could as well be that for some types of tasks and for some amounts of data it makes more sense to maybe have half of your examples in the prompt fixed and the other half dynamically based on retrieval similarity uh retrieved so that the model you know is exposed to similar examples in context um that's a trade-off because you're paying more retrieval costs uh at inference and you're probably increasing your latency a bit um but that might as well result in better performance so there's yeah yeah so interesting you say that CU actually think that could be a massive application of vector databases and Vector search is finding that similar example and having embeddings of examples um yeah all that is so cool um so maybe if we could also so I think there's sorry to be maybe backtracking too much but there's kind of like the signature Optimizer which is like how you uh you know present the task to the model to begin with as well as kind of the examples and could we maybe zoom in a little more on the signature Optimizer and how that works yeah so so um I wasn't in the loop on that signature optimization um work so I just want to say probably Omar and uh the students that were working on that are you know maybe you know candidates for for next episode um and I think there's a paper coming out very soon so um but yeah that I I I guess the idea is just that not all you know nod all word one wording of the task is not optimal for all models um and you know for example when Omar and I were starting to work on this um extreme multilabel classification problem just one of the first things we did this was actually really fun hackaton he was kind of um deying his way through bi the this biomedical data set and I was like manually prompt engineering my way through the data set um and you know one common pitfall for some of these smaller models is the output formats um sometimes you just really have to emphasize that you want a comma separated list um and you can imagine that you know signature optimizers are good at that kind of stuff um you know maybe you really want your maybe you really want to allocate your like demonstration budget towards really like deep rationals hard things about the task you know in you know kind of exposing the model to all this biomedical context and maybe it's more you know more compute efficient to just figure out where how to optimally in the instruction you know how many exclamation marks to put after give me a CSV it's kind of funny but also that is that is kind of how people are prompt engineering this it's like you know if I put CSV in all caps with like five exclamation marks then llama does it and then you know then I can parse my outputs um but it feels like we might not want to spend human Ingenuity doing that yeah I guess the kind of parallel about that one makes me laugh because it's like we compiled it all night and it came up with this really terrifying threat to give to the language model to force it to Output Jon it's like but I I do agree that you know in my experience of uh of of prompting the language models you know saying like please please are very important in all capitals this is like how I've achieved structured output parsing and I do want to give a hot tip to Jason L and instructor and I think what they've done with the scheme of validation is super cool but then I'm still question then I still think you do need all this like very important very important please follow the syntax to like retry like you would use the pantic thing to check if it passed the you know if the output passes the schema and if not now you need to retry it and the retry trying I think it's time to break out the yeah and and just just like a natural thought arising now is you know what is the optimal way to do the probably like if maybe even you know with how efficient fine tuning is also becoming and the fact that for if you want to focus on formatting um you can generate synthetic data um it might make sense that formatting maybe down the line that's something that we fine tune against like this model always needs to return a CSV or my program will you know explode uh and maybe the most efficient way is not begging it to do so but the most efficient way is like a very targeted update maybe with a low rank adaptation um that all remains to be seen but it's it's interesting that we were instead of just like fine-tuning we now have this whole like this whole spectrum of approaches uh towards getting what we want out of these models um so kind of studying that across tasks across like data regime um across models is is going to be very valuable I think oh yeah I love that you mentioned that that we have all these different tactics and I didn't want to give another hat tip to Derek kleek from weights and biases I had asked him I said like you know what are people find tuning large language models to do because weights and biases right they see all the what people are doing and and he had told me that it's really about like this following some kind of structured output or changing the St like the yeah like have controlling the style and I I've done some experiments with any skills fine-tuning apis with this like uh formatting we8 graphql queries and I've seen for myself that it is very good at following an output with fine-tuning and yeah maybe fine-tuning is the best way to achieve the structured output I guess I'm curious like um when you do a like so with DSP it would come up with examples of following the the output format like if you need a list of ins right and how how well does that work in your experience is that like a I know it's kind of maybe handwavy to say like you know have you run it in a hundred different yeah but I mean so I've only been working on like uh expecting the models to get like a comma separated um lists um typically zero shot gp4 is really good um a llama zero shot will fail sometimes it'll just it'll maybe give me you know uh more of an enumeration in bullet style and then the question is is it me as the program developer to account for all these things um or is it you know is it the optimization procedure that needs to rule this out and honestly I I Omar probably has some some like de philosoph uh like deep ideas on this I'm not really fully sure because I think I think you know if my Optimizer ends up picking a few examples that you know teach llama to follow the structure I'm kind of like okay I'm very glad this happened out of the box but I really want to use my examples for like deep information about the task and if I could have gotten the same types of performance increase by just writing making sure that my parser also handles you know um new line lists with bullets maybe you know the optimizer could have done more interesting things so um you know the best pipeline probably for a specific task is at this point always going going to be the one where like the most human engineering still went into it I think that's just how research in in general kind of works but really I guess with Ira the hope is really that you know I Supply the parsers for csvs but they can also handle enumerated lists and just all of the like common pitfalls are already guarded against in the pipeline and then the optimizer can like be really good at just finding examples that give good information for the task that's such an interesting entanglement the yeah the output in the so uh pivoting topics a little bit I really wanted to zoom in on the like the teacher student kind of model I know in the uh rank Zephyr paper what they do is they have like a two-stage fine-tuning where first you generate examples with gbt 3.5 and then gbt 4 because you know and then stage two is gbt 4 because it's cheaper to kind of do it that way how do you think about uh yeah that kind of like which teacher Mo because I know gbt 4 is probably always the you know or maybe Gemini Ultra is in that category too yeah um absolutely vital Paramount I'd say to get this to work um in the real world uh you've hinted at it you know taking the biggest baddest language model and you know paying most of your University uh funding towards the API um is going to give you the most state-of-the-art papers um but really we shouldn't keep pretending that accuracy is the only utility we want to do research for um these are all systems they all have latency they all have a number of calls they all have memory requirements and they all have costs associated with them um and honestly it is within that space that taking all of that into account where a lot of really interesting research um happens so how I experienced this uh during the you know the experiments I've run for the paper and the experiments I'm still running is that um inferring with llama as a student model so having a student llama 7 billion the the Llama two chat version predicting queries works really well having that llama do reranking is pretty bad um so there it's really crucial that the reranking happens by a gp4 model um and that is now a a a target for ablation if somebody else can actually figure out like oh we're going to train a rank llama and we're going to use this rank llama and now it it actually is good at ranking you know and it works across a broad sweep of these tasks well then their contribution really generalizes and provides a lot of utility across all of these information retrieval tasks so that's just to kind of reflect how I think about these model choices I'll give you another interesting tidbit that's not in the paper yet but we found that when we're optimizing the first llama module actually it does worse with a gbd4 teacher and it does better with a turbo teacher uh my current hypothesis is actually that gp4 gives a very complicated rationale and llama has trouble following that whereas the turbo rationale is actually pretty straightforward and llama kind of adopts that straightforward rationale so um so that's maybe not in intuitive to really show you that there is study that needs to be done as to which model can provide bootstraps for which other model maybe yeah maybe like the best thing in the end is like we'll have gbd4 do the task but then like hey write a rationale rationale and explain it like M five and then that gets mapped to llama to like account for the capabilities that student has so there's a lot of non-trivial interactions between all these pieces but you need to take them into account because otherwise you're just going to pay $5 or whatever the you know I'm just saying a number here but you're you want to cut costs and you want to do this uh also locally I've had um I've had medical experts you know pinging me on on Twitter being like I want to apply this on my data set we can't use gp4 because of you know legal reasons um so for them you know it's it's it's an essential boundary condition that you use local models for example so yeah amazing that was another just packed with information on that I I had seen a tweet today that was like um uh benchmarking like coheres language model with open Ai and anthropic on uh query rewriting right so like and and it was reporting that coer was the best language model for this and it's so fascinating that you know for each of these steps in the program like infer and rank and I I really want to just keep it in the two layer I know I can get like six layers but it's just so hard for me to quickly think about that but like with this infer rank the two LM Parts you know maybe coher is the best language model for inferring and then also for teaching and then maybe you know some other model is the best of that other test that that kind of like which LM for which task is super fascinating and yeah yeah the way I just want to want to layer one top one thing on top there we we we've kind of decided on the fact that which llm you you take for which task is an important parameter but it's also just a discrete choice just like the demonstrations so given enough data it might even make sense to have charistics for picking which language model for which module um so then we're going really deep into the optimization right we're we're even optimizing what the underlying API is uh yeah just wanted to throw that out there yeah I hope this isn't too off topic but it makes me think like um maybe you could argue that like say uh P torch lightning now lightning AI like maybe because they have seen so many workloads that they could kind of like uh bake in some momentum parameter on their Optimizer like this is kind of the analog it's making me think of like um you know M like I don't think that kind of thing has emerged I don't I I'm actually not sure like maybe open AI with their kind of like you know closed experiments they've they've seen so much data and they've trained so many models from scratch that they also have well actually now sorry I'm kind of talking myself into realizing that they do have this they have like Optimal like there's generally like learning rate is like a pretty consistent thing I think like 10 to the minus three or something like yeah so what I want to say there is let's say you want to do some conventional fine tuning on a sequence to sequence task you go to hugging face your architecture has already been decided for it's either an encoder decoder model or it's a decoder only model so now there's two training scripts you can consult at hugging face the hyper parameters have already been decided for like what the options are you know that learning rate is the important one to tune you know that batch size you should just care about what your memory can handle all that stuff then you go to a forum and you find that somebody said that oh this learning rate was the best for flan T5 XXL and you know in that whole pipeline so much community and wisdom of the crowd knowledge has happened Without You actively thinking about it um and and kind of that's what I hope for in the long term for this work is that there are just good inductive biases baked in into how do you solve such a problem with pre-trained components and somebody can just come along take my script say like oh um no gp4s um I have a llama running here on this URL um I want to maximally do like 10 calls of llama per input um go um that would be the dream I would I would very much like my work to to get at that point yeah yeah amazing I'm completely Sol on that now but I guess I have one more question where it's like um uh can I bootstrap ex say I bootstrap uh 50 examples and now I fine-tune a model on those 50 examples and then that model is the teacher going forward is that a pattern that you think would have value or that is interesting um that is interesting because typically I've been thinking about fine-tuning as uh more of a mechanism towards the student side where you get a smaller model to mimic the behavior of a bigger model efficiently um so the reason why yeah so let's say you have like your let's just think through this right this is interesting so let's say you have your teacher model generate the 50 best quality examples if you then find tuna model on that it it's it's probably never going to surpass the teacher so without some other ideas it might not you know it might not and if it already mimics a teacher perfectly well then that's just the ideal student um so I need to think about this more I've been thinking about fine-tuning mainly as a student type thing to improve latency and costs uh at entrance time yeah I guess that like on one end there I think there was like some meta knowledge distillation stuff from Jeff Hinton like I think I want to say it was like two years ago this is a the kind of like the meta learning I think meta pseudo labels is maybe the name of the paper where it's like um it's pretty meta where where you're you know optimizing the teacher to train the student and so the gradient from the student like comes back to the teacher yeah oh oh yeah that kind of stuff yeah and then you have like second order grading yeah um there definitely are applicable Concepts to the Spy that I've been thinking about um I haven't had time to figure them out out but um and I know some other people at least at Stanford that I've talked to are thinking about similar things in different contexts it's yeah you shouldn't necessarily just the current Paradigm we have is you look at with which examples are good for this teacher then you map those to the students um there are a few pitfalls there for example trivial examples will be trivially completed by both the teacher and the student so you should never waste optimization Cycles figuring out if adding a trivial example to the student improves performance so maybe you don't actually want to pick traces of a good teacher you maybe want to pick traces where the teacher is much better than the student so you take both of their performance into account and you're like okay actually the teacher did not fully solve this problem but but because we have like a granular metric it did solve it much better than the students so actually our heris tells us that this is an informative example even though the teacher didn't fully fully solve it um and so that is already taking a bit more like the the dense interaction you could get between like the teacher figuring it out and the student making like their unique errors based on what the teacher said so that kind can kind of embody that meta Vier going for um I don't want to spend time like passing gradients through like other models and then um but maybe who knows yeah it's very interesting idea yeah well I think you've already kind of uh like inspired my curiosity with this with this idea that um you know like the the the the most thorough rationale isn't necessarily the best training example for like T5 700 million parameters and I mean that that already kind of opens up the box so much yeah yeah but I kind of being mindful of the time I think we did I think that was an awesome coverage of the optimizers but I I have another question I really really want your thoughts on which is the kind of the design of these programs like you know you you've Design This the solution to extreme multi-label classification as infer retrieve rank did you maybe consider adding another component to it like what what led you to settle on those three yeah um a kind of um they seemed like the minimal set of solutions that would be conceptually very satisfying um and we have ablations in the paper where it's like what if you don't compile this module or what if you delete this module so like going towards simpler pipelines we have like the full set of results um but then going towards more complicated pipelines well it's just all open-ended like it's the wild west right now so um one thing that I that I know or I feel very strongly will improve performance is some kind of chunking or ensembling logic um so for example when I'm dealing with full medical papers as I am in one of my tasks um that just doesn't fit in the context window so we might need to run Ira five times and then we need to figure out what is now the op optimal way to merge the ranking do we merge the queries and then retrieve once and rank at the end I've actually tried something like this and the initial results were quite bad because all the gains you get from ensembling In the initial step get filtered out because the ranking is like I don't see any evidence for this and it throws it out um because it can't view the entire evidence everything came from so another idea is yeah applying Ira to each to these chunks and then maybe having a kind of macro level rank Step at the end which is either a new language model but it could also be a kind of mixing components like a kind of Weights that learn how to mix the rankings or something like this so definitely in terms of just extending the context window you know you could add summarization you could add this kind of like macro ranking um I think that's definitely the first successful iteration on top of Ira um or at least the highest chance of like success or at least I can conceptually motivate it yeah that's amazing I guess there kind of two things I want to break apart of that um Let Me Maybe start with the summarization thing like one example when I was playing with prompts is uh you know summarizing these podcasts and so the first thing I would do is go by it chunk by chunk to come up with the topics and then I would use the topics to then again go through the chunks to be fleshing out the topics and I I found value in in it seeing the full scope of the podcast as it was writing it and so that I yeah and I think like the work that Jerry I think Jerry from llama index deserves the credit for like I think the paper is now called Raptor with this kind of like recursive summarization but in my opinion I think Jerry deserves the credit for like evangelizing this idea of like summarizing the chunks and having this like local like you will sum you will write a summary of these clips you'll receive the clips one at a time as well as a summary so far and then you know and so yeah I think that's so powerful but let me all let's if we could discuss further this like ensembling the ranker I find that to be really fascinating especially in the context of weeva in Vector search because um with our rer rankers it's like I could uh parallelize the calls to the LMS so I I don't think I'm suffering I mean I I so I don't think the latency would be hit too I mean now let's say I'm making five calls so I will pay five times as much but if I fine-tune the rankers you know maybe that maybe that's not so expensive but this kind of ensembling the llm components yeah we maybe just kind of get a little more perspectives on that like what would make uh each LM have a unique prediction to begin with that it would be worth yeah yeah so for um for full papers like for this task of like finding reactions in papers we know that there can just be information throughout the entire body of the paper that's really relevant for this task so we want to process all of it so we just kind of know from experiments there that you know getting more context into the model as long as a model can learn um when we were doing fine tuning initially in a previous work helps um so that's already intuition that we actually want to like you know scan the full paper um but then you know the the idea is do I program how does needs to be ensembled because um like you've said ensembling incurs costs um so we also don't want to like waste time by just running IRA and cost like 10 times right um so like I've already said it might for some tasks it might make sense to like first on maybe just Ensemble the um the Llama the cheap llama in the beginning we'll run the Llama 10 times we'll get you know 10 times as much credit queries we'll group those queries we'll put push them through the rece um Retriever and then instead of having a rank module that's like filter out everything that's not true maybe we should have a rank model that's filter out you know everything that could not be applicable giving like the abstract of the paper like we do a more soft filtering um and maybe that's the the most efficient performance budget tradeoff um I'm kind of still thinking about how do I program these things so that people can just decide this for themselves so people can just be like hey I want this entire pipeline run across chunks and I want to learn weights to like group the final rankings or I just want to run the first module 10 times and then maybe the second module twice um how what is a good Paradigm for doing this and how can you like learn to do this across a bunch of tasks I I guess that's what I'm um interested in uh when I hear this question yeah that's exactly where my thinking on this is going to is like um like if I want uh let's say I'm having each of these LMS write like a tweet based on the same let's say they all take the same input and I want diverse outputs and that's sort of the purpose of the emble then then I feel like with dspi syntax I can't just have uh you know one uh signature for all five of these tweet writers right because then they they're all going to have an identical prompt yeah but there is so there is there is temperature right so I guess I guess the idea is that your prompt really embodies what you want the language model to do um and if you want variability because you're exploiting l assemble then you know every time you send it through with the exact same prompts you give it a different temperature yeah wow so maybe that's like so with the DSP programming model maybe that would be because I think so right now there's like the dspi do predict and you know Chain of Thought is interfaced that way maybe Ensemble is already kind of interfaced like that and I'm just not aware of it because that I think that's kind of the abstra action that might achieve that yeah and I I sorry if that's already in there no no no well so what is in there now is you can when you're running a Chain of Thought or when you're running a predict you can give a number of predictions value just as when you ping something to opening eye you can say give me 10 continuations so that's that that thing but then you get like you get 10 sets of queries for example for the infr module that doesn't really fully solve what you're trying to do because how do you then resolve those 10 sets of queries so actually I've noticed that you know um the Precision of my ranking will actually drop when I Ensemble because um because there there is more variability so the chance that there's like one distractor query that like ruins the top two or the top three of my final ranking um that increases however if I look at like the recall a 100 of my query that increases when I Ensemble right because I am also finding more esoteric terms that are correct so even just onbling is just not always better depending on what you want to do um so there need to be like smart ways of depending on the metric figuring out somewhere you in interpolating between not ensembling and like full embling um that definitely as far as I understand is not an outof the boox feature of DSP oh yeah well yeah I think generally just kind of us mapping through the philosophy regardless of what's implemented in there it like it does sound pretty trick I guess because it also inspires me thinking about like multi-agent Frameworks that I might want to use DSP for and um I guess that kind of has the look of an ensemble like if I want to have uh like a society of uh coders and reviewers and so I want them to each have like slightly different training data or slightly different prompts such that there's some diversity in my population yeah yeah interesting yeah yeah so yeah the whole thing is just so interesting there's so much to dsy and I'm just like so excited with exploring all these things and yeah so I guess I think that was a really good coverage of of all these things I get one one other thing I wanted to ask you about was um recommendation as extreme clust because yeah we're super excited about trying to see how Vector search can be used in recommendation I think this IR program would be absolutely useful for recommendation yeah um so recommendation is also very interesting because I feel like there's already a huge body of literature there I feel like first com like some of those first commercially viable deep Learning Systems seem to have been like or or machine learning at scale systems seem to have like these recommender things um you can definitely frame recommendation as extreme multi- label classification right given an input given an Amazon product and the label space is every other product on Amazon you know find the ones that are relevant as well for the user um and there is actually repository out there that frames a lot of these recommendation systems actually as extreme multi classification um can you do will you be able to do it with an IRA type program I think definitely yes the question is will it be viable at the scale that these recommenders operate on millions of movies you know gazillions of tweets every day maybe even one llama call per tweet is just too much you know um so maybe there it really makes sense to go for these like scalable collaborative filtering or some kind of like fine-tuned retriever um that is just initially what I would worry about when when you know when I were tasked to do a recommendation with this type of thing I think you know Vector databases for sure really important for recommendation um just you know embed everything and collapse your search space from you know A Million by million Matrix to like you know 100 candidates per item um that's already really important but if it's like one or two llama calls per item per user in one of these like billion user systems um I don't have experience at that scale um and I don't have that amount of Compu I don't know so I'd really focus on efficiency there I guess but um the nice thing is indeed like some of the cold start issues maybe are solved right because when you're fine-tuning a retriever you know new items always come online it's very you know you got to like keep the retriever rolling so that you can recommend the new things the nice thing about this in context thing is that you know it's it will very flexibly adapt itself to like new new movie maybe you can give like the description of the new movie and it can like make some initial judgments about what types of people will like this movie um yeah there's definitely ideas there but I think I worry a bit about costs yeah yeah I think that's absolutely fair I guess like well yeah I think if you're worried about latency I don't know if the I don't know if the LM ranker has a place in that and that if that's your main problem but I do think there's a big opportunity for like more offline recommendations that this can satisfy like particularly something that reading your paper inspired me to think about was uh say I grabbed the last five tweets from from Carl and then I um you know I retrieve from we v8's database of blog posts and then I you know now I'm ranking them for blog posts that I'm going to show you to try to you know get you on the we train and so I so then my question to you for extending that would be uh I'm very interested in these sequential recommendation systems that kind of model the sequential problem of like you know I I showed Carl this article of uh let's say weat refc and you viewed it for two seconds then I showed you another recommendation you view that for one second but but then I showed you something about hnsw and you read that for you know three minutes now it's like oh let's more Vector indexing stuff right so so I really like kind of like yeah so yeah that is really interesting and and I think um definitely having language models in the loop there is really important um for indeed for the kind of like conversational or sequential nature of this process um kind of understanding in context the user has already been exposed to this therefore their prior you know they have different beliefs about the world now or something like this so therefore um I mean beliefs about the world in like the the technical sense not in like the political sense or something but it's like oh you know given that the user understands this or given that the user is not interested in this that makes more sense um yeah I'm I need to think about how such an agent would look like like how do you continually keep injecting new states of the user and how does that influence the whole like retrieval and ranking Downstream um but that does seem to require some type of flexibility that I feel that you know language models are Adept towards um I say that as somebody that has no experience in recommend their systems but you know that's my two cents yeah well I think it it kind of reminds me of like modelbased reinforcement learning kind of like how you have this model of what you predict the next state transitions to be and um I think the tree of thoughts prompt is kind of similar where you're kind of like where you kind of roll out potential Futures and yeah so yeah just something I think that I think DS I think that would be a huge application for I I think it already is super powerful for the idea I think of taking the last five tweets is the initial query and then map that into the retrieval and then rerank it and yeah and the recommendation thing is super interesting so uh Carl as an an anchoring question I really want to ask about your interest in biomedical NLP and what kind of LED you to pursue that it's so interesting well the um well it's interesting to me the the PHD is um it's such a roller coaster you never know where you're ending up or what you're doing um but but that's the fun thing that's why I absolutely love it um I got into this via VIA and we were talking um with my advisers and this this company that actually you know tries to solve some of these problems um specifically of like drug safety monitoring so you know we cannot check as a society we cannot check everything about a drug a medicine during the clinical trial nothing would just get released so a compromise we make is that whenever doctors write about something like hey Connor took this medicine and actually he ended up having an adverse reaction uh and maybe this reaction wasn't even known to science so doctors will write this in papers in clinical studies and and all that information will just be out on the internet but like unstructured and Pharma companies that produce drugs uh at least in the United States are actually required by regulation to to look at a certain query over all the biomedical literature in order to read it and like figure out did something happen with our medicines um so every time you and I you know every time we're taking a medicine we're kind of relying on the fact that the latest and best scientific study has been like viewed and like people have extracted this information from it it's all in a centralized database and a ton of different researchers are using that database to like study whether or not there are like statistical anomalies with drugs um so there's like a whole real world system running perpetually um and and I was fortunate enough through collaborators to talk with some of these people doing that work and this is just you know experts a pipeline of experts looking at these papers Consulting databases doing searches Consulting even the authors of Publications to to to do this right um and so we it felt so inspiring to us you're dealing with long context you're dealing with an immensely high stake situation you are dealing with biomedical knowledge you're dealing with tasks that need like multiple humans in the loop to resolve how good are language models at this so then we we made like a big data set for this biodex um and we found that actually one of the things language models were really bad at at was like speaking the exact terminology for these medical reactions um and that's kind of how I got into touch with Omar you know I was always like fascinated by his work um and then you know we just started thinking about it and one thing led to another then we were like well we cannot only do this on biomedical things let's also get job vacancies in there and then it kind of grew into this let's generally try to solve this problem as best as we can across the board um but yeah that's been the that's been the arc here yeah it's amazing Carl thank you so much for joining the weeva podcast it's so cool to hear about your story of uh getting into biomedical NLP the problem you just describe and all these things with d SP I mean I think there's so much to this podcast I'm personally just so excited to rewatch it and digest it and I'm so happy to be publishing this and presenting it to our audience thank you so much Carl awesome thank you Conor ", "type": "Video", "name": "xmcdspy_with_karel_d39oosterlinck__weaviate_podcast_87", "path": "", "link": "https://www.youtube.com/watch?v=_ye26_8XPcs", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}