{"text": "Hey everyone! Thank you so much for watching the 64th Weaviate Podcast with Shishir Patil and Tianjun Zhang, co-authors of ... \nhey everyone thank you so much for watching the wva podcast I'm super excited to welcome the co-authors of the gorilla large language model shashir and Tian JN this is one of the most exciting large language models out there I think it's so cool how this enables large language models for Tool use and all sorts of exciting topics so before diving into it shashir and tanen thank you so much for joining the weba podcast yeah thank you so much for having us Conor appreciate it and looking forward to it awesome so could we kick this off with kind of like this uh LOL LS for Tool use kind of like treating tool use and properly formatting API calls is kind of like a new task in deep learning this vision of gorilla being the a the API app store for llms I think it's such a compelling Vision could you kind of you know tell the story of gorilla yeah for sure uh so I think the start off sometime around the end of last year uh maybe November December 2022 when there was like you know Char was the rage and lot of people were trying it including us uh and as some early adopters what we soon realized was that like a lot of uh these tools at least from a chatting modality was good to maybe as a demonstration of the technology of what LMS can do but we felt that probably this is not what the future would look like right in our minds an llm is a powerful tool powerful nonetheless but still a tool right and then now you need to have tools need to interact with other tools to get more stuff done and in computer science uh the way different tools interact with each other is through API calls and that's that was The Germ of the idea where we were like hey you know this is a tool this needs to talk to other apis um and that's how we got started so this was like the original uh the story of the origins of corilla yeah amazing I love that um like the the video demo you have where you show like how it can use a CLI like you you have this example where in natural language you ask I want to download the gorilla data set from an S3 bucket and all those API requests all that intermediate orchestration is handled by this model and uh T and I remember when we met at Berkeley you you showed me this like GitHub CLI and how it can format the GitHub CLI request could maybe talk a little more about the particular kind of apis that you you two have started to explore uh using Gorilla for yeah so we have been initially because as a PhD and also machine learning researchers for the paper side we choose hugging phase models and also tensorflow and also torch up so those models are pretty particular for um machine learning audience and usually the case is like they know they want to know actually some model they use for image classification but not exactly what model they want they may also have some uh specific requirements for especially the model size I oh I want a smaller model under 10 million parameters or I want specific the resent family or what so on so forth so that's also some constraint uh that's for the paper side and the afterwards we're kind of moving towards uh think about how people in general are having a pretty uh it's just having not very good particular sense of what the exact um API to use the kind sort of remembers something as also we discussed a little bit with shishir and the especially for this GitHub stuff right you want to know if I want to check out some of the project base last Friday you sort of know the command basically roughly but not the exact command also something like kubernetes AWS so that has been our Focus for the second wave especially in terms of the gorilla SI we have been working on the Linux command line uh GitHub and the AWS Ager um also the gcp cetes is uh kind of the supports we are having from there yeah that's really I mean I love that so much I think all the time I have like you know I might have like a picture of this podcast thumbnail and I'd be like could you make a poll request to the we v. website to add this to the podcast and it's like that little like the little bit of doing the new branch and then putting it in that part all it's like a bit of overhead that this gorilla model I think can just simplify that uh dramatically and I think you know I really want to dive into kind of using Gorilla for the we8 apis but I guess kind of I want to take apart a little like in the gorilla paper you have the torch Hub tensor Hub hugging face as you mentioned you specify kind of like what model you want to use and it has constraints like you know I need a 85% Plus image net accuracy res net but please make it like less than 20 million parameters so I don't have to like break my bank serving this and then there's kind of like the apis like you know GitHub kubernetes you mentioned like get can you tell me how you see like apis for you know kubernetes GitHub cloud storage downloads and then kind of model inference like picking the right model from hugging face yeah so I think like it's It's Tricky and it's uh it's similar in different ways right so one example is if you look at the machine learning space of apis that we serve through gorilla a lot of times like for one task you have so many different models that they are doing almost the same thing but slightly different all right so it's like oh I have an optic detection model one could be for animals and the other could be for plants say for example or you know it could be like different models like you know almost the same thing one uses a res 18 backbone the other us a res 50 backbone as another example so the fact that they're so similar and we still able to like discern between the different apis is actually pretty interesting right because that's a challenging part if I ask you to differentiate between a stripe payment API and say a gcp API it's easy to do in some sense like very different different domains diverse functionalities different modalities but to say hey look from the two very similar apis can you now differentiate and satisfy the constraints that I give you and that's quite tricky and challenging right so that's why uh that's also what makes the initial API bench that we have uh quite tricky um because you're trying to like tease apart these differences among the different apis that exist now on the other side of like T mentioned it's not it's not every day that you go and ask oh how do I do a text to speech all right but on the other hand you know as researchers and a lot of people very often tend to use uh all the apis that uh T mentioned so in those scenarios it's like very very different use cases like one is more practical more useful and like looking ahead like this was more like from a daily uni developer or like a common user perspective uh but the good thing is we have like this Discord Community where we get a lot of feedback and a lot of users uh using it and so now the new thing that we learned is that oh like people are interested in um Salesforce and like service now and like all of these apis and like the open API spec and stuff like that so now we like trying to you know like train models with these new apis and add that into the gorilla ecosystem as well yeah I love what you've done with that this like API Zoo this I you know I see a lot of the thumbnails you know like YouTube video you there's quite a lot of reaction to gorilla and I think that kind of API app store for llms is the thing that like the headline that people are loving um so yeah so people you're starting to so starting from the model apis and I think there's quite a lot of interesting things that you've done with like the abstract syntax tree for how you evaluate if you formatted that particular request correctly but I guess they just want to kind of maybe even stay on this just for our listeners to say like this API Zoo you're building up a big data set of of different apis whether it's you know kubernetes or GitHub CLI how do you see that kind of API Zoo evolving yeah so soak the other the other interesting point is that a paper is a snapshot in time whereas the open source project evolves almost daily right uh so so and and we have this at the back of our mind uh even when you started off which was like look but at the same time you want something that's set in stone to Benchmark against because if you have a moving Target then you know how do you know how well you're performing Etc so this was a thought process where you would have like the API bench which released as a part of the paper this would be like one yard stick that you can use to measure you know all the different uh molds that come out and that's why we have tried to make it as easy as possible we put out the evaluation scripts and we have all the Train the eval split neat and clean uh so people can try try the different models bench Market Etc and this would evolve much slowly right uh on the other hand what we also learned is that there's way more apis than what you and me or the three of us could do or even you know like a small subset of people can do so the idea was that you know people can contribute their own apis uh and then we can now use this train the model and then serve many other people and one thing that's critical is that unlike you know there's been like a lot of oh legal questions around can you scrape this scrape that Etc is it okay to like for an llm to learn this data and serve it apis are meant to be distributed right like the more people use a particular API the more in Bound it is for the people whose apis people are being used right uh people whose API are being used right so in some sense like this is the incentives are purely aligned right like in fact people actually put out uh documentation tutorials Swagger documents that other people can use to actually use their apis and build on top of it so this was the idea and uh and there's been like a lot of interest in like oh how can we contribute our apis into the API Zoo because then we can train the model and then many other people can use this model uh to then you know um get get their work done with a very small or like very quick ramper period yeah add a little bit also another Point like this uh apis like people are building apis every day there are new apis or or maybe even changing the API versions and then like this this is not um one time shot uh time stamp like static data set it's like meant to be dynamic and people can building their new apis people can update their versions of apis or even the usage of the apis so that's also an another interesting question we want to study by empowering this API Zoo because people are now freely to update and then we want to also we can also study how the language model can the to the best support this Dynamic change of the apis yeah so it's super interesting I think kind of quickly before we go further into the training of the llms the gorilla LMS let's let's kind of differentiate between this gorilla LMS and like the open AI funks with the Json where you kind of give it like this Json dictionary of how to format the arguments like obviously it's like zero shot versus fine tune but maybe like do you have any thoughts on like what the limitations of that kind of open AI funks approach might be compared to this kind of train a llm specifically for uh data for the AP yeah that's that's a great question and I think the way I think of open functions is that you know you know what API to call you just want to fill in the arguments into that API and you and you get the response right and con there's also very similar to what they did with plugins in the chat GPT scenario where you go and pick a few plugins that you want and uh then you know to give a question it would answer what we're trying to address is the Step Zero which is API Discovery right uh like we have a friend who's trying to build an app and you know now he wants to integrate Payment Processing into that while stripe has a bunch of apis and there's like a bunch of other people with different apis which one do you even call right like how do you do the discovery and in many scenarios you don't even know what's a good Discovery mechanism right like if you want a good database on your for a react app that you have on your iPhone what do you use uh if you're in the domain then yes probably you know what your colleagues are using but otherwise how do you know so it's like API Discovery is something I think that's I think is an an open problem that we're trying to solve with Gorilla which is that you know you go with a you go with an intent and then can you get the right API and it's even better where if you can also like fill in the right documents itself and then you just get that as in a single attempt kind of thing yeah that vision is incredibly uh compelling to me and especially like as we step into the details of how you train these gorilla llms this kind of retrieval aware training you say API discover Discovery how do I find the right API I take a natural command like hey I want to add a you know secure payment processing in JavaScript in interface something like this and then you retrieve from this database to get the API can we kind of Step into the details like how you're seeing this retrieval aware training yeah so we see this retrieval aware training is uh actually slightly different from just a f so in the paper we have two versions We compare to one is purely zero shot basically we don't do any information retrieving but one of the bottom neck for this is as most of the language models are still they are also having today is like they cannot keep their knowledge up to date if I ask anything that is late happen later than their training data set end date they have no clue what's going on here although there are a lot of the uh other studies like retriever aware generation or um different varus of retrievers basically enabling Google search and other various sours of informations to connect between this upto-date information and also the language model itself but what we think the retriever World Training is like the beauty of that lies mostly in you can actually read your documentations like you the documentations can be updated like daily or even week daily or weekly right it's much lower the frequency you will want to train your large 70 billion language model or 170 billion language model so we can actually we want to actually train the model to read these documentations and actually figuring out how you can from these documentations write the corresponding apis and then the users or the developers can update their documentations every day or giving new function names or even update their versions and uh this kind of is what we see also sort of missing in the current um language model scenarios because we have actually identified a couple papers saying that if you put a bunch of the retrieving results and some of them are incorrect or some of them are Irrelevant this would definitely dramatically hurt the current language model's performance even in gbd4 so that's the part we actually want to dive into and try to solve using the retriever World training that part of the con yeah and the and you can think of the key inside with a with an example right so for example you say look I want to classify uh an image uh and then if I and you use a retriever and then argument your prompt with what the retriever gave you so the retriever told you hey why don't you use this uh you know like a V Vision Transformer backbone then you're like well okay this is relevant information I'm going to use this this if the retriever is correct but if the retriever is wrong and it says oh you want to detect you want to classify this image excuse me uh why don't you use this text to speech API then in some sense what we train the model to do is to actually ignore that to say hey look recognize that the retriever can be wrong so I'm first going to determine if the retriever is correct or wrong if it's correct then I'm going to use that API if it's wrong then I'm going to discard that retrieved API and instead rely upon my knowledge base to then generate the right call for this particular task so there's like this high level one bit information should I use this information or should I discard this information is what we're trying to teach the uh llm which I think is pretty critical because retrievers are never accurate right we we all know that uh yeah I mean like kind of especially in our world of weeva we think a lot about like rag pipelines and what that like you might um ret like your retrieval might be like bm25 and Vector search Hybrid search some waiting especially if it's like a you you know how do I use the pipe API like how do I use the hybrid search API that those keyword matches I imagine especially for this application but then you have say maybe you want to rerank you know rerank with a cross encoder the top results or as you mentioned like you could have something where you pass in the query in a candidate document to the llm and just ask it is this relevant please say yes or no and if no throw it out and there's definitely so many interesting ideas there which which is exactly what I want one thing I wanted to ask you is this kind of like there was this paper called Lost in the middle which is about you can't really just fix it by retrieving a lot to put into the language model right so like are you thinking about trying to just put one API in the you know hopefully the perfect API reference right the one API or do you maybe have some K API to put in that input window right so I think uh so in our at least in in our evaluations and numbers where so this is like basically recall at K right and we were like hey let's let's look at recall at K where k equals one because we're not trying to like in some sense rank to retriever Etc and that's also why like we do comparison with bm25 which is a popular Retriever and then one that uses like some GPT embeddings to compare which today most people consider to be pretty good if not the state of the art and we also have a Oracle retriever right so like if God gave you the correct API then how can you do and the and the idea is because with this we're trying to just demonstrate that there's going to be some noise in the process what we want is to so there is a community that's trying to like reduce noise right I'm sure uh for uh for VV this would be come in terms of how well can you improve your recall at K uh but in those scenarios where you have this long tail where you still cannot fit this because you know the queries are UND spec Etc in those scenarios can we like make the model itself robust go ahead and Tackle this so if you make a better retriever you benefit from it and in those scenarios where the retriever falls short you can still determine what's the right analysis is and get the result of the user yeah and also um if this is like for one of the if if you determine your task where you know for sure your task only requires one API but ofing the case your task requires um like stacking together words a mixture of multiple apis let's say like five 10 apis and then in this case you would definitely need more information in your retri process there are I think definitely a lot of other VAR of retrieving process you can um you can add to make this process more accurate but often the times it's like um is it's is going to be a little bit tricky to or or even harder to deal with when you're actually your your context of API is requiring multiple documents or multiple apis that's amazing I mean I I this mixing apis thing is what that's what blows my mind more than anything else is to have a natural language command where I'd say could you build me like a an index of archive papers using we8 and llama index and gorilla does every single thing in the middle of that calling the we8 apis as well as the L index apis as well as maybe these archive data downloader apis yeah I love that and something else is kind of like there's there's this thing about like late like with search you typically try to optimize throughput and you know optimize latency to make it as fast as possible because most of the time you're thinking about like web search or like e-commerce but I think with applications like this I think that latency constraint kind of eases off and it's way more important to rerank and make sure you have that top you know that top one is really great do you kind of agree with that opinion that that latency maybe that this is a good example of one of those tasks where you would be willing to tolerate a slower response if more accurate yeah Conor this is critical for two reasons right one is what you just mentioned which is lost in the middle kind of stuff where it's been shown that if you have too many contexts uh then you tend to lose a lot of information that's in between um and retain the ones that is at the front of the back so in our case in terms of apis if you were to like give uh 10 different apis then you know a lot of things that's in between gets lost uh so that's one and the second is and this is informed from the downstream task right like when you do search today and if the search result shows you 10 different uh links then the burden of proof is on the user to pick the right link and follow it Downstream whereas in the case of like a lot of these llms where if you're want to like change these apis together then in some sense like you are going to pick the best API that comes out so basically the top one is roughly what matters and then you're going to like uh pursue that and then proceed with the rest of your steps right now suppose the top one is wrong and you have like an fail early kind of mechanism that's good because at least you get a signal and then you can like go cycle through the remaining but what if it's like it's not wrong but it's inaccurate right or what if it doesn't fail but then it continues on like some long Loop that keeps uh regressing so that's bad right because now you're just wasting time and resources without really getting close any closer to your uh result SL answer than you were before so I think for these two scenarios especially as we try to automate more and more stuff uh it becomes critical that you get it right uh and in in the first yeah and I so I think just using one IPI by itself can be extremely powerful like just you know the GitHub CI kubernetes you mentioned but this kind of like combined archive with weate with llama index is super compelling and so I wonder about the the kind of the system architecture like does GPT 4 or like you know the most powerful cut capable like diversely reasoning ability that LM does that kind of orchestrate the intermediate calls to say like archive gorilla we8 gorilla llama gorilla or even just the same gorilla model for all three all you know train on everything in the API Zoo so you know this kind of like would you still use some other language model to kind of orchestrate the steps and have almost like a you know Chain of Thought like to kind of do this then this then this yeah I think I think uh we are actually not tied to um tied to either one of this let mean we could support we could imagine in one case is if you want a really complex task the task involves a lot of reasoning and involves a lot of uh to like basically like breaking down St tasks into a lot of sa subtasks and uh this involves a lot of reasoning then we could imagine like at least I'm not aware of any of the current open source model that can really achieve very good performance as well as gbd4 or even gbd 3.5 like Che chat gbt so this is like one of the unique benefits of even you do Chain of Thought like this your unique benefit of reasoning capabilities in there so I would say imagine like a task is really hard and you need a lot of reading steps CH of thought steps then probably the best usage is you take the most powerful model up to date like gbd4 breaking this down into some sub small steps and then you could call and under internal like gorilla for example that can give you very reliable and accurate tool usage or apis for these small steps and then you could chain them together afterwards but I would imagine also for some relatively easier tasks that involves only maybe two or three or a couple more steps of uh training apis then actually smaller model maybe can handle this well I mean we don't have any results for today up to now but I believe we could uh as also the open source models progress uh we could do better and better in this domains as well yeah I'd love to hop in on that uh so so let's get into it I mean I think like the 15 billion parameter is currently the stateof the r lolm at text tosql and SQL I think is a pretty tough API for this right like that might be like the the goalpost for all this but so let's talk about um kind of self the self- instruct curation of training data like how do can can llama 7B be trained to uh write we8 queries better than gp4 and how does this work yes the answer to your second question is yes 100% And that's also what we show in the paper uh and obviously like having a specific domain uh gives us the benefit but the pipeline is the following right so you have the documentation and a lot of the times um this might come in pretty diverse uh formats and how these documentations are right so what we have is a pipeline for training where we take all of these documentations and the first thing that we do is like we create a pretty dense representation of all of this information and this is human readable I think of it as a Json format but Json are pretty hard especially if you have code involved New Alliance and you know I mean it's it's doable but it it might be hard to make in consistently so we have like uh small unique uh flags that we use and by the way Al is open sourced in our repository so then we create like these dense representations of all the data we have and this includes stuff like oh you know uh here's how the performance metrics are or like here's latency numbers here's throughput here's the QC etc for each of these apis um in terms of V8 or like any of Lama index it would be like oh here's the authentication that's required etc etc and once we have this this serves us two purpose now one is I can use this to now generate some self- instruct instruct API pairs uh and for that you would use like any Jal purpose llm that doesn't necessarily have to be good at apis but it has some uh language understanding that it can like you know read some apis and come up with uh potential US based questions and we found that it's pretty good if we see this with some uh in context examples ourselves so we create a six I mean there's no reason to be six but we have like six instruction API pairs from which we sample three stochastically for every generation so we give these three as you know in context learning and say Hey you have seen these three instruction API pairs now given this new API can you please come up with an instruction that would be a good PA to this uh so that's so this is how like you can generate self instruct uh API PS but also this dense API representation now goes into our uh API Zoo which we can then use in the retrieval phase uh so in the inference phas for the retrieval argument retrieval uh aware Generations right so that's that's how we use this and once you have the self instruct then you would then go ahead and you would train the model and for training the model we use uh what I said is R the retrieval a training uh which is pretty critical in you know getting getting accurate numbers even when the retrievers me or may not give you the exact value so this is the training phase in the in phase that two there are two ways one is where the user asks a question a prompt then you find the most uh relevant top K documents uh which would be apis in this case you find the top K most relevant apis give this together to the gorilla llm and then gorilla will come up with a response that you can just execute the code and get the response but there's also another way and this is the most popular uh among our open source Community which is a zero shot approach you ask a question directly to the L M and it comes up with an API from what it knows that can help you answer the question yeah I think even just kind of the retrieval database of apis for the zero shot is useful as well but yeah so kind of to talk a little bit about my perspective on adding this to we8 and building we8 Guerilla what I've been doing is you know I have a data set of each of the we8 search apis and then I Loop through toy schemas and I you know generate it for each of the schemas and then that becomes the training data and I guess the big question that I'm so curious with your perspective on is like you know so so I am kind of validating these queries and so on and I'm I'm curious if you think like if gorilla will be one model or if like you know we8 llama index say rise AI unstructured Lang chain like all these companies maintain their own gorilla and then they kind of and then gorill there's like gorilla Hub where I put my gorilla a weeva gorilla into to then when you say hey uh could you you know get archive put it into we8 put it into a llama index query router maybe visualize it with a rise AI or like you know like like do you think everyone will maintain their own gorilla yeah uh for real okay before that con I I want to really appreciate how quickly you guys were able to turn around and build your own gorilla uh we because I remember we just ched uh I think less than a month ago and then you were like oh what's this going on and and the next time you ping me it was like oh we have all of this ready so it was pretty yeah top job with that well if I can butt in quickly I mean this has completely changed my perspective on how companies can train llms for their custom tasks like um yeah and yeah I mean thanks for that I because I just I love this idea it's so cool yeah yeah thanks Conor uh and yeah that's a good question and so when we started off out right initially our idea was that like we're going to have maybe one or like a few models that's going to serve most apis and then when the user comes uh we get them the apis but increasingly I think our at least have started to think uh believe what you mentioned which is they going to have a with multiple baby gorillas and came about yeah this came out from uh two interactions we had one is we're part of a PhD lab where we like you know like we have a retreat where we like give presentations and a lot of sponsors and people are interested come in and they here and in one such Retreat where we were discussing gorilla someone some institutions came up to us and said hey can we have a gorilla train for our own private apis and so initially I was like well why would you do that that's slightly counterintuitive but then we realized that the way you know they were like look we have multiple divisions within our Enterprise and each of them have their some API producers some API consumers but they don't talk very much or like there's no free flow of information right even within an Enterprise uh it's quite possible that your like your customer success Engineers are not aware of all the apis that your engineering team is coming up with so then in this scenario like you would have Gorilla just have internal apis to different teams within the same institution right that also just changed my because you could do this with um you know all of the functions and all the classes in a code base right like how you want to like if I'm saying um you know with we8 if I'm like hey I want to implement scan quantization or whatever and it's like you have to now you have to go interface with like how we8 is going to do that how it combines the database with the quantization scheduling and all this so yeah I mean it's it's funny to think about but yeah yeah and the and the second one is what you mentioned which is like you know atate uh there's a new person who comes in and they're like they know how to use all the other tools very well but not the new apis in that scenario then you can be like hey you want to use vb8 here's a smooth onboarding experience for you where you can just talk to Goa natural language it's going to give you an API call and you can get the job done so in the scenario you're getting the job done ASAP but at the same time you're also like learning through the process right so for both learning and like a smooth onboarding experience especially when you have new apis uh this would be a good experience right yeah I think M maybe I stepped into it too like I think still just at that high level of uh you use there's different like like levels to the apis and yeah it's so interesting I mean that Integrations of like we8 and llama index is different from like then within we like how does the LSM store interact with the hsw so yeah yeah it's it's really fascinating stuff um see so I guess my next question then well yeah so you mentioned the baby gorillas and kind of it sounds like you also kind of share this thing Gorilla by the way is just the best name for a paper you got the Emoji and all but um this kind of like yeah like would there be a maybe a hierarchy to gorillas like like some like the there's a gorilla that knows that some of these baby gorillas fit together like particular like to stay on this we8 maybe like let's add like coher like you want to use coher models we8 Vector database and then index client framework so like there was maybe a gorilla that knows that these things fit together as well as the baby gorilla specific to each thing does that maybe sound like how the gorilla how the gorilla hierarchy could play with each other yeah great question and this is the kind of research questions that we also trying to understand this like we don't have an answer yet uh but you know it's like yeah like you know what do the interactions look like right because if you're using like you mentioned like just CER for models llama index for all the uh connectors to different data sources or like vv8 individually may make sense but if you want to use all three of them then what would you need and oh what if I want to like use this information to then use some other set of apis so then who's the orchestration engine here it's uh it's all yeah it's it's open research we're thinking into that uh and I think we touched upon a little initially where you could use a third party or like even own llm which is more general purpose it just needs to know how to route it but not necessarily how what needs to be rout it and it's yeah open open research yeah I guess like another question I wanted to ask so like this evolution of Guerilla what like I forecast it could become is kind of seems similar to the promise of hugging face sort of can you maybe tell me your perspective on Guerilla versus hugging face how they're different from each other or if you yeah or or cuz like I could imagine hugg and face how they have like image classification uh you know like all the segment all the tasks like API usage could become another task but then I also think as you mentioned this kind of like hierarch like there there's and then there's probably more Nuance to this particular task so how do you see that kind of like gorilla and hugging face yeah I like it's also my personal view of things con is like huging face is like more like developers are mind sharing they're sharing what model they have trained they're sharing their um like basically the how the model is performing on different vars of data sets but gorilla is more like in my view a communication between the API itself like also another example is uh I think the Stanford AI tal paper they also recently released open source code so imagine if you have a ton of different I don't know language model agents and then the sort of will have happy will be having some ways to talking with each Char or to communicating and to eventually to solve a task like but Hing face in my point of view is like great tool for sharing your capabilities for sharing your results but the ultimately the people or the different models haven't been or it might be hugging face are trying to do something wrong but different models hasn't been grouped together in order to solve a task that's my opinion I I feel like in order to solve a task you may need some for example managers engineers and you may need some salesp person and then you know this is uh communication between different levels of uh future agents or future apis like have a sales API have a engineer API where um various of like apis they're trying to communicate between each other and eventually it's it's about task solving yeah I've Loved this kind of like I think GPT team is one of these like viral GitHub projects that describes this kind of idea of like role playing gpts I'm the marketer I'm the engineer I'm the product manager and yeah like the the way you've abstracted this into thinking about apis has made this idea so much clearer for me some like more tangible kind of uh yeah it's really compelling I love how you connected it with the like westw World paper I never would have made that kind of connection but you can think of like I could think of shashir as an API that I ask questions to and yeah so um so sorry if I'm pivoting the topics kind of hard you know I wanted to ask about the kind of structured output parsing I know this is a pivot in topics like you know people do things like look for like back tick back tick back tick Json and I'm curious like what kind of stuff goes into the gorilla uh for doing that kind of making sure it follows the uh this the syntax yeah so I think this is actually a Qui question Conor um what we think well at least what we are currently thinking gorilla is like it's definitely not compatible with gbd4 because gb4 is um we comparing to gorilla is like a super agent or it's more like a general intelligence bot that can do everything but this has good things and bad things good things is like uh it can do everything basically whatever you ask chat GPT or gbd4 it gets you some kind of pretty good response but one of the um this advantage of that is um it's its output is very diverse like for example if you ask it's um if if you just chat with them every time usually if you don't adding a specific deterministic sampling uh to G4 every time output is kind of is from different structure different content but sort of answering your same question it's like like more like natural language but this thing can be a little bit tricky for actually if you want to generate Json files for example or if you want to generate API costs because you need to follow the exact syntax and also you need to get every domain correct so what we think is like uh for the first thing is like fine tuning or a special model for this structure may help a little bit because now the model knows I need to follow specific structure and I don't uh um like also like gorilla is doing right now it doesn't care about maybe chatting capability right now um so it it knows like here is a structure output and uh I need to get the structure correct also the second point is with this retriever aware training you're providing some description of your output like the model basically reads documentation knows what the structure should be like in your outputs so the model should have a little bit more context about how output is structured and getting these things more reliably and clearly better than in my point of view like a general chatbot although General chatbot are kind of can they do this task in a pretty good manner I think well that's kind of the that's something about the general chatbot and then like okay so there's this idea of reflection prompting where I uh you know like I had it right at we query it didn't execute and I show it the error message and it sees the error message and goes oh sorry and here's the correct one and I I worry that if you fine-tune the model in and compress it you might lose the ability of it to do that kind of like reflect on the error message unless you had another model that's been specialized on looking at the error messages yeah I think I think that's a great question honestly but I um I think okay my actually to my study of the up toate models uh even GB even chat gbt is not good at this reflection like GPT 4 is very good it's to some extent much better than GPT Chad GPT 3.5 because I I did actually did some of the studies on the big bench tasks from the chat gbt point of view the accuracy improves from I say 20% to 40% after you output the arrow message but for gbd4 it actually improves from 20% to 60 or 70% so I would say this is a very open like a um research question here on how to on even how to get models to reflect itself reflect its answers and the getting the correction part based on the error message at least for the open source models because we don't know how open I did that but this is actually a huge research question so we are we one way is to imagine you have a separate model that can do this task another way is how you can also in the meanwhile not only teach the model to do this structure output but also in adding other capabilities of to the model that is crucial to your task solving it's also something like we're kind of exploring as well yeah I think it's kind of like well that last thing you said about like the adding new functionality and kind of like the maintenance the continual learning I think it all becomes so interest I think it becomes like uh novel in this particular kind of task compared to most deep learning tasks because you have like kind of like test cases like past fail whereas usually the generalization testing for continual learning is so abstract kind of but yeah I think all that is just yeah really interesting how it will be able to reflect I know there's like papers where they like have a they train like a code repair model where they purposely break the code and it learn learns those kind of differences and yeah all all that kind of discussion is really fascinating um so awesome so could we maybe conclude with kind of like some like what's on the horizon like what kind of things are you guys just thinking about yeah I think uh one thing that we want to like Explore More uh is this thing that we touched upon in our paper which is uh hallucination which we know that it's a big problem for llms and I think with you know like if you ask today oh how D before hallucinate there's no good answer right uh there's no number as such it's mostly people have objectives so we with abstract syntax Tre we have like a clear PL based approach that you can vure hallucination so that's something that we want to like double down and explore more uh and see you know what can we learn from it how well can we extend it and then retrieval a training is another approach uh which I think has shown a lot of promise for us uh with apis and just doubling down on that and trying and seeing how generalizable it is you know does it hold as we scale beyond what we have so far and then besides that I think there's just like a lot of inbound interest on you know what apis do we include uh and then the different modalities to like surve it like we initially had a collab notebook that you can talk to the model and then a lot of people were like hey can I try it uh into my workflow so we gave came up with the CLI tool and there's a spotlight that we just doing some finishing t on that we want to get into people's hands yeah so different modalities how people can you know if you think of it all these are just different ways to use apis and once you have the pipeline behind then you can like build this but otherwise yeah we're committed to the open source project uh so if people want to contribute uh we'd love to have people on board and it's yeah it's like a lot of it we're just listening to the community from our GitHub from our Discord and then just uh taking it one step out a time yeah I hope uh I hope we we are first WEA but I do also hope this podcast inspires more people to add their apis to this because yeah I'm so excited to see how this kind of yeah the API Zoo develops and all the research on what happens as you continue to scale it yeah those are all really interesting questions t let me let me get your response as well on the same question of um like kind of the future directions that inspire you the most yeah I think a Bea down top a little bit adding sh's point is like uh we have already we have also received or a lot of the great advice from the I think either GitHub issues or the Discord communities are coming to us so that's also kind of uh shapes what we are heading to to some extent but in in general I feel like this U RM like agencing is very interesting but we have a lot problem to solve to make it fully autonomous and also fully reliable successful uh agent for RMS and a lot of them including like long contacts also memory uh sorry the memory of the previous stuff and also tool usage and chaining together apis these are like also great research and open open research questions and we kind of want to also explore how gorilla can better support Al not only uh the research Direction into this area but also some open source Community like whether they adopt gorilla as um a back a backend the model or it gorilla as one component as you see moving forward in the future to building this RM agents to for task solving yeah I mean I I I hope this isn't too redundant but yeah like that kind of long context reminds me again of that like retrieve enough API references for how to import data into WEA and then retrieve enough enough API references for how to set up the query engine and llama index and then combine that into just generating doing the whole entire task and one kind of forward pass and yeah and then that kind of I love that kind of thinking of like what how how will it fit into the existing ecosystem because you know like I I imagine like what the I think llama index calls a data agent or Lang chain is really famous for this kind of agent orchestration I think it could either just be like the future of Integrations is all these things have like a language model in front of them that translates natural language into the thing but then you also kind of have to have some description of like what you could do with you know like you have to be like UA could do this these kind of things before you even would think to formulate the question like how do I bm25 search and then rerank with Co here like it would need to have some description of that but yeah amazing shashir and T genen thank you so much for joining the WEA podcast I think you have such an amazing project gorilla one of the coolest projects I've ever seen especially in the space of agents and large language models so much fun building this for the we8 apis and I hope people will find this interesting and yeah I can't wait to see where this you know how this evolves yeah thank you Conor it's a great yeah and thank you so much Conor for doing this it's always a thrill chatting with you so looking forward to chatting again ", "type": "Video", "name": "shishir_patil_and_tianjun_zhang_on_gorilla__weaviate_podcast_64", "path": "", "link": "https://www.youtube.com/watch?v=HUtYOLX7HZ4", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}