{"text": "Hey everyone! Thank you so much for watching the Generative Feedback Loops Podcast! We have also created a blog post and ... \nokay so I need to get into character here we go here we go here we go I'm beyond excitement to introduce Dr Connor Matthew shorten to his own podcast today but we're gonna talk about something very exciting but before we do that congratulations because as people heard from the introduction you're now officially a doctor so well done and we're going to talk about something super exciting and normally of course you interview people on the podcast but you've been driving this the work behind this so it would be makes more sense of course to interview you about what you've been working on and and and by way of introduction uh before I'll hand it to you is we of course seen a lot of opportunity with Factor databases and large language models where people generate something and they want to store that back into the database the search room and um we've kind of you know called it like generative feedback loops and I would love to get a little bit of your understanding uh Conor as an introduction to everybody watching what are our generative feedback loops yeah thanks so much Bob for the introduction and for having me on the podcast for having you on your own podcast Yeah well yeah I guess um I kind of so we have for everyone listening we have a great uh tour of Concepts to explain this generative feedback loop thing and the general I think kind of a great tour would be to kind of do like how you know they have that like levels of abstraction explain it to me like I'm five then I'm in high school college technical expert and so I'd say kind of the the introduction thing is about how we are prompting language models by putting your custom data into the prompt so you ask it a question like uh you know what kind of what's Bob's favorite flavor of ice cream and then instead of just having the language model rely on the knowledge it's learned from language modeling the internet you retrieve information about Bob and then this is the prompt that goes to the language model so that's been kind of the foundation of this this kind of like retrieval augmented generation has been huge and so now what we're looking at is firstly just continuing to explore that all sorts of data sources that you can prompt and like how are we going to be mixing these prompts and having these local memories there's so much Innovation happening in that we've seen all these tools like Lang chain llama index Auto gbt baby AGI just go on and on about all these things and so yeah so uh yeah so maybe let me pass it back to you but like uh doing kind of the highest level description of generative feedback loops uh like how would you kind of summarize it quickly yeah so um it's very much aligned also with with what you're saying so how I see it is that the um the first thing that we saw the opportunity with Factor databases if you purely look at text now so let's park other media types purely text that we saw that we could actually change something that we had not stored in the database we could use it on query time so the example we've always been using or at least I've always been using was if I store the data object um the Eiffel Tower is in Paris we could retrieve that by searching for landmarks in France and that was always so if we have like the document had the database itself or the data object is stored then we had like on query time this this new opportunity with tractor databases but with generative search I see it as like we're now also doing something with the output so we can say show me landmarks in France and represent them as if they were a Facebook ad so it takes out the data object and um and it and it then generates something from that and that can be interesting in itself but I think it even becomes more interesting and that's also something of course you're gonna also show us in a bit that we Loop that back into the database because the exciting thing about this is in my opinion is that we now can store uh uh information back in the database which that we can factorize again that was not stored um that was not in the original um a data set and I have a question uh for you so one thing that people often um you know what one critique that people sometimes have from these generative models is of course the hallucination um uh so I mean if it's correct it's amazing but sometimes if it's wrong then it's like well less than amazing and sometimes people argue that this from the injection basically so that we get the data object from the from the factor database and we eject it injected in the model um that adds a way to overcoming hallucination or a way for people um to inject data in the model where um you know on data that it can't be trained on or that it wasn't trained on and what I mean with can't we train on I mean because of course technically speaking you could train on everything but maybe like legal documents or something that you have it might be difficult to just throw that in the open to be trained on I'm I'm curious how you see that would it be fair to say that this is a way in helping to overcome uh hallucination or would that be a stretch yeah that are such an interesting topic is the hallucination when you do the prompt injection is it going to hallucinate and so I would adversely Point people to a couple of resources that I'm then going to summarize but um so uh John Shulman did a talk at UC Berkeley on The Late that's the latest update on reinforcing learning from Human feedback about uh you know making reducing hallucination with that optimization strategy can't recommend that enough I was also at uh Haystack last week and Colin Nash did this awesome overview of all these generative feedback uh sorry of uh hallucination the cases of how it occurs and yeah it's so interesting I mean I think uh yeah you can there's obviously the prompting thing where you just kind of tell it please don't make anything up right it is explicitly say that yeah and then I I really like kind of ideas around like the natural language inference thing where you have the context and the generated and you're using these models that classify entailment or contradiction I think that's a potentially promising way to you know filter it out but um it does I from the start I I might be going a bit off but I I had this conversation with Nathan Lampert from uh hug and face about about this kind of thing where I was like how great would it be if the rohf objective was don't hallucinate directly not not just be useful but you could just make it make it like that and so so I think it'll be I think though I think the optimization task will will figure that out yeah can you talk yeah can you talk a little bit more about that so how how would that what would that look like how would that how might it potentially work but what are the what are the ideas there how's that yeah so right now the the general thing is uh just um you know like did you enjoy this answer right like that's the signal and you know it's reinforcing learning train I think yeah and did maybe do the eoi eoi 5 thing like um these aren't quite language models anymore they they're pre-trained by predicting the mass out tokens but now they're fine-tuned with reinforcement learning and the key thing to the key difference there is that in reinforcement learning you make several decisions and then receive a sparse reward so you know the language model each token it generates is like a decision and it with the mass language modeling it receives feedback for each decision because there's like truth to it but whereas with the reinforcement learning you generate like you know 400 tokens and then receive the rewards so if you just change that objective to did you generally enjoy this answer to did you hallucinate anything from the context and you annotate a large enough data set and then it's extremely likely that that helps a lot and and yeah there's I mean it's such an interesting thing how I love those pictures where it's like uh is explaining the language model where it's like this giant monster and then there's like a mask on the front of it yeah exactly exactly I I think that's actually that's interesting you're saying that because the um um to stay with that metaphor it's the of the of the of the monster right so the um I something that I how I personally think about that sometimes is like when we look at like doing prompting injection through the database is that we basically can say we want to use the generative LM for its language understanding um but we don't want it for its knowledge and I have a practical example and and to stay with the with the um the monster examples like we want so the monster is probably very strong and very big so we want it for its muscles and we want it for its strength what we just wanted to do what we want basically so um uh so the example that I that I sometimes give is that for a simple so to stick with that Eiffel Towers in Paris example that you might store a document in the database that you factorize that says something like the Eiffel Tower always has been in Paris but it's moved by truck to Barcelona making making this up right and so you could factorize that and you can store that so now if you have the the the the the the query have um where's the Eiffel Tower it will return that document first that says that it's moved to Barcelona you can feedback you can sorry you can Loop that into the llm and say like okay where's the Eiffel Tower we're trying to get the question where's the Eiffel Tower and so but you must base it on the information we're giving you right now and then the llm responds like okay it you know maybe something like hey it was always in Paris but now it's moved to Barcelona by truck something like that and that is kind of where we want to get so that we use it for its uh for its language understanding but not per se for its for the for the knowledge and it's interesting and so the experiments we see with like you know the the role of of the uh the temperature and those kind of things in the um those kind of settings in in figuring that out but I think it's it's super super interesting but but before we before we dive into an example so so because now we've went from the query to like the database to like output what so what would the can you explain like to this is what he what the feedback loop would be so how would the feedback loop be closed basically what what's happening after it got the the information comes out of the generative llm yeah amazing so I think that was a good a quick introduction to the retrieval augmented generation so now now we're taking the next step where we're talking about the title of the podcast the generative feedback loops which is where so we're going to take the data and we're going to generate something and we're going to save it to the database that because then we can access that for future Generations we can search through that it you know we want to save it it's like our data that we're using so so yeah I think I hopefully think that the example probably I kind of like this example so why don't we Dive In yeah let's go hey everyone welcome to the ABA generative feedback loop tutorial using a data set of Airbnb listings we're going to be illustrating the concept of generative feedback loops where we take data from the vector database to inject into the prompt of a large language model and then we'll save the resulting generation back into the database to use for future prompt injections or generally whatever we might want to do with this generated data say it's a summary of a podcast or in this case a description of an Airbnb listing so let's dive right into it so firstly all the code for this can be found in weavate generative feedback loops in addition to this python notebook we also have typescript examples of how to do this so firstly we'll install the wevia client with Pip install we via client and then as a bonus part of this we're going to be showing weeviate embedded so we've had as recently released we get embedded where you no longer need to stand up a server to connect to wevia you can now run this right inside of your jupyter notebook which is super convenient for you know python developers looking to quickly get a sense of of how to use wevia before necessarily say putting a database into production and all that kind of stuff so one other quick detail is in this tutorial we'll be using the open AI uh the openai large language models you can also use the cohere language models and this repository will also contain a notebook with the cohere generations as well uh so if at any point you want to see which modules are running on your weeviate instance you can just do uh the client.getmeta and you'll see the enabled modules and links to the documentation of where to learn more about them so the embedded module is going to come with default these uh these kind of generative search open AI uh the text Evac hug and face and all these kind of things so first we're going to be loading our Airbnb data so this is a CSV file of Airbnb listings that we found on kaggle and so you know pretty standard python stuff to you know use the pandas library to read the CSV replace um you know null or infinite or nand values and then just convert it to a list of dictionaries so with this list of dictionaries the first entry you know we have these keys for different values the ID the property the name the quick description of it is his name and then uh you know you got the host name who's hosting this Airbnb you know what neighborhood is more particularly so Kensington is a particular neighborhood of Brooklyn and then uh you know other things about airbnbs like what type of room it is you know the price you can imagine all these features like you know does it have a pool all that bedrooms bathrooms square feet all these kind of features that describe an Airbnb so here's the first thing that is so interesting to me is that as of right now it would be very hard to just take these kind of tabular symbolic features and pipeline that into semantic search I used to have this recommendation that the best way to do this would be to do some kind of tabular to text translation where you have like feature name equals feature value you know in the next feature so it'd be like ID equals 2539 name equals client clean and quiet sorry apartment home by the park and so that would be the text string that you would embed and try to search with but what you're going to see with generative feedback loops is we're going to sync this up into a written description that contains all this information and then by putting it back into the vector search database you can search through the different airbnbs in a much more natural way using semantic search so let's continue on the kind of how do you do this with leviate so so this is how you create schemas in weave using the python client particularly so you open up the classes because weeviate has a a class schema where you know classes are like tables in a database where you you can Define more than one class basically and you can link the the classes to each other with cross references which is you know similar to like how most database systems have like a key adjoining system so we Define the classes listing we give a short description of the class and then we're going to be configurating the modules to use on this class so the module configuration particularly is that you set in the schema is the vectorizer so say this is going to be vectorized with ref to VEC or you know that kind of thing but this is going to be vectorized with the textivec open AI embedding model so then you just kind of tell it uh you know which which properties you want to vectorize with the skip parameter on each individual property but we'll get into that so then it's a vector index type we're going to be using H and SW for this and then the other vectorizer is what we Define in the module config uh so then what we have is we Define a list of properties with each property being you know like a attribute of the database table so this is our description this is what we're going to be generating and saving into this is the you know the description of the apartment listing that will be written by the large language model the product of our generative feedback loop uh so then we have all these other features that we use to describe the Airbnb so you can also see how we have a mix of uh text features as well as number features and please ignore that this is written as string because this should this should say text but anyways uh cool so so then at the end of this we you know we create the schema like this and then we're going to be importing the data into weviate so we're just going to be uh you know looping through that uh that data list that we had imported with our you know our pandas read CSV then DF to records save that in this data list so you know we just we configure the batch Imports uh then we just Loop through this we're only going to do the first hundred anyway we just copy the keys get the valid uuid because you know this is a database and then we just add it to the batch and it you know puts it into Weeknd all right so now that our data is in weba we can do a simple git query this is this is the client syntax so uh maybe you know listeners out there have seen the weba graphql console and you've seen these um graphql queries but each of the clients they also have kind of a querying language like this so you do client.query.getlisting and then you know the property that you want to have and then you can append on these you know filters like dot with near text or like dot with hybrid dot with limit dot with where so so that's kind of like what this syntax looks like with the python client uh cool so then we're going to add another class which is going to be our advertisements so we're going to be from the descriptions writing advertisements and then generative feedback loop saving that back in the database so we also create so this is how you create uh another uh schema to add to the database and then we're going to add another property with this cross-reference from uh Airbnb listing to advertisement and the named references has ADD so we just use this to create that that new property from listing to the advertisement so now the most exciting part of our tutorial we're going to be generating the description the Airbnb description so what we do is we have this syntax where we have these curly brackets which is where we put in the property value so you know with each of these prompts that go into the language model you'll replace the curly brackets with whatever the feature value is and this is how you prompt it to the language model so so this is kind of what I was talking about earlier where that you would represent your data with some way like this to search through instead of doing this generated thing which is I think a really exciting way to unify the most of the data which is tabular organized data with this new semantic search technology but so here's more into the syntax so we we have these generate properties that we pipe in by doing you know client.query docket listing generate properties dot with generate we give it our prompt and then we uh and then we um also give it oh sorry so we already gave it the properties within here because it's already a list so usually it'll be like a list with the with the properties so cool so we also get the ID out because we're going to need to take this ID in order to to add this new property back to its source so by taking the ID we can then key value the ID to the new description so then we look through these descriptions we have the new property which is the dictionary of what we're going to be updating so then we take that ID out from the underscore additional ID which we got from here and then we do client a data object at update the new property the class name and then this ID that the description was sourced from so now we're going to see our resulting generated description so first we're going to insert description back into this generate properties as we use the syntax of you know class name and then the properties you want to see so we'll just see the first one so we take all these properties from the available availability of the room the neighborhood it's in the short description of it the price of it and we write this uh in the large language model Rice's description this private room is located in the Flatbush neighborhood of Brooklyn offering a unique country space in the heart of the city with a price of 150 per night so on so I think I think you get the idea but like so this is the generated description that we've saved back into the database so now let's say we want to query the description so we dot get listing description is what we're searching what we want to return and then dot with near text so this is how we do our Vector search with Concepts being the name of the argument and then Airbnb near a place to walk my dog so what this means is we're going to take this this phrase Airbnb near a place to walk my dog transform that into a vector that captures the semantics of this sentence or this phrase and then we're going to search for the semantic similarity between this and then all of the generated descriptions we've done for all of the Airbnb listings to try to find you the best place to uh to walk your dog so you see the results of that and how that lets you search through these airbnbs with queries like you know two bedrooms hot tub or just whatever kind of casual language you want to use to describe the ideal Airbnb for you and in the future we can imagine all sorts of things from images and videos and just kind of the mind-blowing implications of this kind of technology that it continues to develop so now let's let's keep generating stuff now let's generate an advertisement so the prompt is now going to be please write an engaging advertisement for the following Airbnb listing description and then we're going to pipe again generative feedback loop we're going to take that data that we saved in the database and we're going to send it again as input to the large language model and then we're going to again save that result back to the database so again this is how we do our generate query visualize the first ad and then again we're going to Loop through the ads and we're going to take that out in this case we're creating novel add objects so we don't need the referencing uh ID we need the referencing ID for the cross reference so we you know we'll generate a random uuid for the advertisement object but then we'll keep the uuid which was the source of the description that generated this ad to use to link to this ad with the cross reference between the classes or tables if if you want to think about it that way so now we'll be looking at the at a quick thing of how to use this cross-reference syntax when you're querying weeviate so first we're going to set this where filter on the path so we're searching through the Airbnb listing things and we're going to add this filter where where the listing has greater than or equal to one advertisement and then we're going to see the description of the listing and then has ADD and then now the cross reference takes us to the referenced ad and then now that we're in the ad uh class similarly to this we take the content property out of it so we see how the description uh this private room is located in the Flatbush neighborhood of Brooklyn and then we have the advertisement and the advertisement is escape the hustle and bustle of the city and step into your own private country Oasis in the heart of Brooklyn's Flatbush neighborhood so we see how we use this description that we originally had generated and saved in our database and now we're going to put that into the language model and then feed it back into the database now with this advertisement so now let's go into targeting advertisements so first we can imagine some generic ad targeting we add a text property Target and we add that property to our database on the ads and now we have these targets say young couples elderly couples single Travelers and the generate prompt then becomes please write an engaging advertisement for the following Airbnb listing description and then we prompt inject the description and then please write the advertisement for this listing to Target and then you know the target which is another property that we now have but in this case I just put it in this list but you you could retrieve it however you want to do it uh so so similarly we generate it you know we we use the uuid to then link the new uh the new advertisement to the referencing uh the reference C or the referencer listing uh so then we're going to look at this um you know as a single traveler so so this ad writes attention all solo Travelers are you looking for a cozy and affordable place to stay in the heart of Brooklyn and then you know make your solo trip to Brooklyn Unforgettable things like this so so now let's kind of think a little more about all the different data sources that we can grab from to prompt these language models to write more more targeted advertisements for these listings or maybe even advertisement because advertisements sounds so kind of like spammy in the past but now what we're seeing is customized descriptions for you so it's it's not written just to be salesy but it's written to tailor to your interests and hopefully this will be more clear as we look at it a bit more so now we're going to add another class to the database user and then in the user we're going to have a biography for the user and the person's name so we have two users in our database Connor Connor often travels with a golden doodle named Bowen and Bob Bob is a prolific weightlifter who will get upset if he doesn't have a good gym to work out in so now what we're going to be doing is we uh so we import our users into the database and we see that they're in there we're going to add this uh we're going to add a user Target for the advertisement so each advertisement that we write a user Target ad for we're going to add a cross reference for that and the interesting thing about that is that we might not want to write one for all of the users you know we might want to be clever with the way we do it and things like that so cool so now we change the prompt to similarly to how do we how we had that generic Target now we're going to have the target the following user and then we're going to pass in the user biography that we had used by doing users equals client.query.getuser biography with additional ID because we're going to need this to do the cross reference later and then we get the data this way so now after we've added all the data we query it and we'll see some of our our resulting advertisements attention all weight lifters are you planning a trip to New York City and need a place to stay that won't compromise your workout routine if that's an advertisement for Bob uh and then for Connor you have attention all pet lovers are you looking for a pet friendly accommodation in the heart of New York City look no further than the village of Harlem so you can see how you know I hope that from this you can start to get a sense of all the things that we can do with with this kind of idea of generative feedback loops all the different data sources we can use the kind of compounding effect of saving this data back into the database and so with that said let's jump back into the podcast oh it's a very that was very interesting to see that Airbnb example um uh Conor and that makes a lot of sense so just to quickly recap what we're basically seeing here is that we're adding a data set without any descriptions to the database but because we generate the description and feedback loop that in it creates a vector representation and now we can do semantic search over these um uh these descriptions that is that is correct right yeah and so yeah I would love to start on that example of now you have something you can search through and so to tell a quick story when I was um when I gave a talk at odsc London last year I was um you know I was trying to think about presenting Vector search for data scientists people who are doing analytical queries with most mostly like tabular symbolic data and you know they're thinking how do I uh use semantic search on this kind of data and my advice for that used to be maybe you could translate it to text by just prompting it for with like feature name equals value feature name equals value and so on but now what we just saw with the Airbnb is it took all these properties from Price you know minimum nights you have to stay at the Airbnb what neighborhood it's in and and and it's synced it up into a description then now you have something that you can search with text and so I think that angle of turning tabular data unlocking that with semantic search by putting it into the generative model and producing this kind of thing that is one of my favorite artifacts so the example but something I really want to dive into next Bob is this um just this concept of you know Bob is a gym rat and he's always got to be in the gym right because we're getting to a we're getting to a um a situation where the um where we basically we're just getting to real time uh um uh personalization of data which is just if you think about it's like every I mean with the first generation of like um for example reading newspapers we had uh uh you know the newspaper was day old that's now through the internet became real time but we're not going to a situation where it gets optimized something that I'm in in line with is what I'm what I'm excited about is the um you could even go let's say that you have a news article right and there might be uh you gave earlier during the podcast you gave this you said like you know explain to me like I'm five years old of like explain it to me like I'm an expert so a a journalist could write something only focusing on Experts only focusing on experts but then real time if somebody says like hey I'm an expert on the topic of Finance but I'm a Noob on the on the uh um on the topic of um yeah um uh uh politics for example than a certain thing that's in the political uh in the sorry in the financial section of the newspaper could be specific for this person very into depth but when this person goes to the political section you could just say okay we just made it significantly easier and we only you know we so we simplified it and that's all happening real time and that is something that is I think extremely extremely exciting can you are there are there so besides the um uh the example of air maybe are there other examples that come to mind that you can think of yeah well for use cases yeah I think well I love what you just said that idea of the education personalization like last night I was playing with the uh typescript client and I'm you know I I don't even know the difference between like npm and yarn so it's like you know I have this particular kind of knowledge but I'm missing this plug and you have all this data about me imagine you know having just like every medium article I've written maybe every slack message all this thing to kind of build up this embedding of my knowledge and then pass it in and customize it I love that angle to how you present things but um yeah so I guess diving into another example I my yeah to stay on the Airbnb one more thing I just I just love this idea of like a per of the personalized travel experience and just how you know like when I travel I generally travel with my dog and so like I need to plan my trip to New York accordingly and the things I want to do to say if I'm staying at weekend are going to be different uh so yeah I think if we could even go into this the Sci-Fi almost and you shared this a tweet this morning about something that is this generate TV shows personalized to you how are you thinking about this yeah so I like so actually there was actually just before we recorded this podcast I I was discussing and maybe we can just cut a little piece of that video in the uh in right now so what we just seen in this in this in this video is that um a video was rendered based on just 100 coming from these generative models and everything we're talking about right now the use cases are based on text um so I had the Airbnb example it's very practical that is something people can start you know using today but I think what this will grow into is like a lot we see a lot of models evolve not only for text but also multi-model models and and images and those kind of things so um what we're seeing here is this and this is also why I'm so excited to work on effective database right so where we store information for example about a movie and interest about people um what you know how people like to to see movies and those kind of things so now you can see okay render this part for me as if it's well as we've seen a Wes Anderson movie or around this part is like it's part of the Matrix like we said with the news articles you can say like hey if you are very much into um I don't know costume dramas that you can say like okay just make this longer or really go more into depth on the on the um in the storyline or if you if you hate costume dramas like just just make this a short thing right and the uh if that's in a movie so um the point that I'm trying to excite with all with everything that's happening right now with how the technology is evolving I I would not be surprised that in um I don't know a few years from now we will watch the first uh uh um a movie or series or whatever and it might start with just five minutes or just 10 minutes while we're watching something that is completely really generated and that the real-time generation is good enough or that it can just pre-process something for somebody like in half an hour and that you know that Netflix says like okay your your next episode is ready right we pre-process it for you I would be absolutely I I I would could see this happening and think about like um if you think about also things like AutoCAD and those kind of things where it generates drawings where it helps you with those kind of things you could feel it information about like if you have a certain building about like a history of the country about the or the city that you're building in maybe the type of soil you're building on and it just takes all the information in so yes uh uh um uh text is the first iteration over this but with everything that's happening and I think this this video is a nice example of that it's just this is just going to be a matter of time um and and it's amazing that of course besides the machine learning models I think that core infrastructure um uh um you know database like we've yet plays a role in that so that's that's something I'm super excited about so so yeah so that that's how I that's how I look at it yeah I love how you're telling the story of kind of the evolution of search and I think for our listeners it like can't be emphasized enough sort of how novel this image search is this multimodal search because I think kind of in this context with text search you had you know you have these things or you had keyword matching and you have an inverted index and you had all these tools built around that you had symbolic features like you had the page rank algorithm for Google where you use hyperlinks to get features about web pages and kind of search with that but this kind of way of representing images videos audios or you know the cad drawings that Bob's explaining with you know when you have 3D structures of building designs like being able to put these all in vectors and then unify all of this kind of data from different modalities into one kind of space where you can put an image to query for a song potentially like all these kind of things and yes the the AI movie thing is so interesting to me I've been listening to a lot of AI uh hip-hop music lately even though I don't think we can play this if we if we cut and then play that I think I think we'll be in trouble because I think our next podcast will need a lawyer yeah exactly exactly exactly yeah so it's like let's keep it up but people people can imagine people can hallucinate uh what yeah what that might be like yeah so nobody's like it's funny it's like actually a um there's a um not not to go too far off topic but it is related there is a um there's an author that I'm that I'm a big fan of um his name is Bruce Sterling and um I believe he calls himself a fictional fictional design writer or something like that that is based on a concept that is called speculative design so it's like a big problem that we have with thinking about these concepts of where things going is that um as the late clay Christensen always uh um sets on he's the the Harvard Professor clay Christmas attack and the future is beautiful it just has one problem it has no data so it's like we can't predict anything right based because on the future so what speculative design does it that it's a it's a method it's a methodology to try to determine potential futures and um one of the things that Bruce Sterling says is that often you see these Innovations these Technical Innovations and the impact that they have first in music so um the hip-hop example that you gave is a good example right so it's um there's a lot of abstraction in music so I mean I I know of people using uh you know generative AI in music for a long time already and of course it was way more the capabilities were way simpler so those very abstract music but you could already start to see where this was going and very early on so um he makes a beautiful argument that he says like hey if you want to know what's happening what technology is causing just look at music first because if it's changing in the music industry then it will probably trickle into other Industries or um uh in this case art forms early on as well so it's a it's actually it's an interesting uh example we should I'll we should make sure to link to these articles in the uh in podcasts as well yeah I mean in the description of the podcast yeah yeah the whole the speculative design all that kind of uh like mental models of thinking that all that is so interesting and I think it I think even just one more thing on the music is it it really um really explains the vector database kind of part like let's imagine we have these brain computer interfaces that know so much yeah the kind of music that I like compared to the kind of music that Bob likes and these kind of like brain signals they'll be you can represent them with vectors with models that that you know to vectors it to audio and sync it all up and yeah it's a pretty exciting feature and um yeah maybe maybe one more thing as you mentioned speculative design Theory and I always find it personally very interesting like learning these things from you you tell me uh like the jobs to be done framework and the most recent one you taught me in the context of this uh General feedback loops project has been aggregation Theory can you explain what that's about yeah so so aggregation theory is a is a concept from a um a guy named Ben Thompson and he has this um this nice um block which is um uh is called strategory so it's not strategy but Strat Tech and that's his uh um uh that's that's basically that's his job and he just tries to capture what's happening in um or the impact that that that the Tech Industries have having on the world basically right especially from business perspective and of course in my role at the company at weave yet I'm very interested in this right so what we're talking about today is super interesting and that is super um exciting however we um it's important that we you know keep in mind like how do how does this add value into people's lives and then out of that comes like how people use it and those kind of things and one of the things that he describes in this um in this framework is that before it's it's based on the internet so and the role of that distribution channels have um and the control that they have so for example think about a traditional newspaper the distribution was very important because to get a newspaper it needed to be distributed so there was a whole role for the distribution Channel um so the first iteration of that was like because the distribution channel was gone because it was just on the internet there were like other outlets and other way to get information to people and that is what aggregation Theory describes and what the impact on this on business but I believe that what we now see with these at these generative search Okay cases is that we just going into like a next step into this right so they were basically saying like it now it's becoming for lack of a better term it's becoming hyper personalized it's like it I mean I can I can currently cannot think of a way to make it more personalized than this because it we have if you have the right information so the example that you gave that you like to travel with your dog that is if you then can say hey I need to go to New York and we know what kind of Apartments you like what kind of how much money you like to spend that you travel with your dog that is that is so hyper personalized that it's like a based on that first observation of in in aggregation Theory how how um uh basically um um the the internet shook up these distribution channels we have like this next Generation where we're basically saying like we're now basically shooking up these these these these databases or these Outlets where uh where we have just this generic content for everybody that we can say no no now the next iteration is like hyper personalize this so I'm I am uh to use your terminology beyond excitement about that I'm super excited but um exactly I I have I have one more question for you as well so I'm I'm I'm curious to see the um how how are you observing ever so the the first big thing that we saw where the opportunity for like the feedback loops and those kind of things had that became um an an area of interest for us and and bear in mind for those listening the it is an area of interest for us because we try to figure out how can defective database we've hit play a role in this um I think the first thing that we saw was the chat GPT plugins right or or am I am I missing something there I'm just curious what your thoughts are on that you know how that's evolving and what you're seeing there yeah that would be really great to talk about how this impacts with chat gbt and that generally the kind of idea of you know the interfaces the chat interfaces and when you save the conversation history and then using that history because you know every conversation you have it comes back and I mean I I let me ask you this question how many conversations do you think you've had with Chad GBC so far I I would say for me it's been at least you know at least 500 so far and it's just like been what like four months old something like that so it's like yeah it's so I'm so this is a great question and I have to have to say not only if I if you don't mind me making the um uh uh the the question a little bit broader I'm I'm I'm willing to go on record to admit that uh I'm still there my brain is still processing the fact that I'm using natural language to get something from a data store so regardless if it's if it's judgypt that I ask questions so the other day I wanted to have something sorted uh but I I then I had to go through information and I was like you know let's throw that into chegebt and see how it searches through it um uh um or the other day I had something where there was something stored in weaviate and that I used a very abstract a query to get the information out then it gave me actually what I needed and I've still I my mind is still blown it's like it's like still in this in in state of being blown right it's like still happening so um I'm starting to have more and more interactions uh uh uh also with with jgpt but I think what's more important here is just that change in the in the uh in the world that we're just going to a natural language way of interacting with our systems that is for me just I'm still I'm still Aller I'm still parsing that basically yeah yeah I am obsessed with this kind of text to SQL research and then thinking about how that translates to alleviate and yeah from like the and I'll give a quick shout out to Jerry Lewis llama indexes he's done some awesome content on this kind of stuff but let me like this idea that when I interact with the database I just say uh you know show me an example of ref that you searches through e-commerce images and then the database like then it can parse out the query and know what filters to add to the database because I I love this kind of filtered Vector search thing and I it's saying like uh when we had uh Sam Bean on from you.com and he described the kind of the idea of like a vertically integrated search engine if you seem like perplexity AI you've seen how you can uh you know do you want to search through Reddit do you want to search through Twitter Wikipedia so so then you have this filter you know where equals Wikipedia and and it's so exciting because in terms of like the technical Innovation behind gain Vector databases there's so much exciting stuff with like you know modifying hnsw and routing these things to have the filtered connectivity so yeah that natural language interaction where you just ask a query and then it parses it all that is so interesting but um yeah so so pivoting topics a bit coming back to the um the generative feedback loop thing I think um for me the first time I started actually using this in my life for my work is I was you know I take these podcasts and I transcribe them with whisper like yeah kick off a summarization of them and so then I saved this summarization of the podcast so not so to me that's like that was the first thing when I when I just started like using it for my actual like productivity and my job so yeah let me ask you maybe like um what what do you think like uh how how are you using kind of generative feedback loops yeah so one of so there are two two answers to the um uh to the well they're probably multiple answers oh let me think that's a great question so uh I think on the most holistic level so the first one is uh what you've shown with the Airbnb example to generate data that is not in the original data set so where we always have been inferring insights that we needed to have a human in the loop to infer some insight from a data set um that we don't have to that we don't have to do that anymore that we can automate that away that is something that I I'm thinking about um uh we're talking here about these um uh um uh like I did these these descriptions for for Airbnb but this this can be anything right this can be the houses that's that's for sale online it can be movie reviews it can be and then we're just talking about text right and so that can be anything um it can be cyber security threats right so that if we have learned something in the past from complex cyber security threats we can store that in the database feed that into model combined in the prompt it's like what do you think of X Y and Z and then it gives us something and we store it back and now we can search through it no etc etc so that is that is the first one the second one goes back to what was said earlier that said hyper uh personalization so we could basically say like hey if we know that um Conor is going to New York uh with his dog and he's going to stay near Central Park we can already start to generate content for you know um maybe I don't know what you do with the dog but take it or something I don't I don't know what you do with it but it's like as you can say hey if that's something of out of interest you can go there and if you say yes that sort of Interest then we can feedback with the back end that you were actually interested in that so in the future then if you go somewhere else if you go to LA with your dog then we know like no etc etc so I would on the on the on the highest level I would say um a combination of um um head to so generating insights from data that that normally humans needed to infer from the data and secondly just that hyper personalization do in your opinion did I miss something or do you think do you also see something oh yeah I think that hyper personalization I mean just like thinking about like business Executives like you like I can imagine like you know an llm is like a virtual assistant for your scheduling yeah and and you know the traveling thing just you know keeping you organized by knowing about the particular things you like and all that is so interesting um yeah it really brings me to kind of the the headliner that I was really excited to at the end of the pot because I wanted to hopefully we kept the concrete enough in this beginning of well I and it's as crazy as it is this actually is pretty concrete it's not something that's just purely hypothetical but um I don't know from chat gbt which I think captured everyone's imagination it was like a whoa what is this now we have Auto GPT which is another like oh my what is this too this is so much potential so Auto gbt with Wii V8 you know it comes up with a plan to perform a task it breaks it down into steps and it saves the intermediate steps back to the database it has this you know generative feedback it you know it could maybe be prompted to know that it can save its its intermediate Generations what do you think Auto gbt with Wi-Fi where is this taking this yeah so exactly um uh exactly as we as uh I think as we mentioned right so what you're basically doing is that you're saying um the data the information that we're rendering or pre-storing that information because there is a Wi-Fi integration in Auto GPT but that always starts empty so it starts with research and then stores that and vectorizes it Etc but what if you would say hey we have a bind we have data we're going to store that in in viviate and then you use Auto GPT to find stuff just imagine what that means and bear in mind I always like to say 89 of data is behind closed doors so um let me give you let me give you an example I um I spoke once to a lawyer and this lawyer said you know what the most used and then I don't mean from a database perspective but from a consumer perspective but the most used tool is that lawyers use to find stuff when they're writing you know documents or whatever you know what the most useful is Google search so but the thing is the power is that there's the human in the loop that this the human lawyer looks at the documents that they have internally so the word documents the PDFs whatever they have for whatever they're working on so they have that context in their head and then they use public Google or something to find something and then adjust the documents accordingly to what they found so what are basically saying like if we take these documents and we saw them in weave yet and something like Auto GPT um um Can can basically just create those feedback loops based on that information hey I'm looking at this part of the legal document but I need to know x y and z let's find that online we found something vectorized it store it back into leviate so long story short so the the that was the long answer the short answer is that um with these kind of systems that what Auto GPD is and what it's going towards that we're basically going to say hey if we pre-fill we create with information about something that cannot be accessed to the public internet then all activity starts to function as a knowledge worker in which the database plays a tremendously important role because that's where the information is sits and is vectorized so that's how I see that yeah it's amazing I remember when uh you know when the uh child gbt API or the GB The Da Vinci 3 API was when we were playing around with it initially and um we were exploring the kind of question decomposition where you have a question that has two did Thomas Edison use a laptop and then you need to break that into okay when did Thomas Edison live when were laptops invented and is that kind of like uh you know that kind of multi-hop question answer multi-hop is kind of the academic term for that kind of idea so I remember when we were exploring that you showed me this idea and I have to always give you credit for this because this was such like a woe to me that you were you were you were prompting it to populate a Json dictionary with if it wanted to ask another question what were the results of the intermediate question and I think that's the biggest idea in this kind of uh large language models that use tools is that they can they write it in the API compatible way they can output the Json they can see what you want to see it and yeah so it's it's so interesting because like if it knows how to write the data to alleviate all this kind of stuff exactly exactly and that is something I I am very curious six months from now if they would revisit this conversation where we are in uh um and the role that these these generative feedback loops play it's it's going to be exciting because we already see it but we're now basically going to support it extra from the perspective of the database so that's just that's that's something I'm I'm super excited about so um I think I think we're there don't you think because I think it's for everybody listening they need to check out your blog post and like the the the the GitHub repoters to start playing around with this because this is not fiction this is just people can do it right now and we um I believe the Airbnb data set will be available as well so that people can just play around with it is that in Python and typescript I believe am I am I correct yeah Python and typescript typescript was a new thing for me to learn but it's really cool uh yeah like um having the hover over it and see the types it can definitely help you uh navigate a new JavaScript library yeah fantastic fantastic well thanks so much uh Conor for letting me interview you I hope how did I do did I did I do a decent job yeah also you got to ask everyone to like thanks so much for watching please like And subscribe oh ", "type": "Video", "name": "generative_feedback_loops_with_bob_van_luijt__weaviate_podcast_45", "path": "", "link": "https://www.youtube.com/watch?v=1RALju6ZJz0", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}