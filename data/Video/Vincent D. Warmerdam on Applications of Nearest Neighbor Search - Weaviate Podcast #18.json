{"text": "Thank you for watching the 18th Weaviate Podcast with Vincent D. Warmerdam! Vincent is an engineer at Spacy working on ... \nhey everyone thank you so much forchecking out the wva podcast today i'mhere with vincent warmerdam an engineerat explosion ai which is a part ofspacey and vincent is working on somereally exciting things with nearestneighbor search and the things thatwe're interested in at we b8 so vincentthank you so much for coming on thepodcast and can you tell us about thethings that you're doing with nearestneighbor searchsure thanks for the introduction um sothere's a couple of experiments that i'mdoing uh part of those are related tothis tool that we make so prodigy youmight have heard of it it's a labelingtool um and i have recently found outthat for uh like some of my experimentsit really helps to have a goodapproximate nearest neighbor uh look uparound um there's two areas that i werethere were i was pleasantly surprisedwhere it worked quite well uh so onearea is in the case of datadeduplication turns out you can do acouple of really neat tricks there umbut i also noticed that if you are likestarting with labeling let's say an nlpdata set and there's also a couple oftricks that you could do within theapproximate nearest neighbor look up soum part of my work what i do is uh i ido a little bit of research a little bitof customer support um kind of likedoing lots of things over the company umbut usually whenever i do a little bitof research that also results in like aplug-in or a blog post and recently i'vealso been making some youtube videos onthis uh stuffumso yeah i've i've been surprised at howflexible approximate nearest neighborlook up tables are for like a reallybroad set of applications but i but themain thing that i thought wasinteresting is data duplication and uhlabeling weren't the most naturalcandidates that came up in my mind um soyeah that was like a pleasant surpriseyeah i think this is super interestingat looking at how we can do this nearestneighbor search from just the obviousmatch the query with the documents thingand how we can look at these we'll diveinto these two ideas of datadeduplication and then bulk datalabeling or sayweak supervision and we'll have thatlet's maybe start with datadeduplication uh can you tell me aboutthe general kind of setup of thisproblemyeah so uh let's say that you have aquestionnaireand you know there's a name and there'san address and zip codesome information let's sayand let's say that in this questionnairei may have gotten interviewed twice butin one situation my name got misspelledum okay so that's a very clear exampleof a misspelling should not indicate aseparate entity in a database soprobably this has to get mergedand you say this you kind of go likeokay questionnaires are kind of cute butdoes it have business applications umand then you learn very quickly thatlike when banks merge for example theircustomer profiles also need to merge andit would be a shame if let's say thesame mortgage appears in the data setfive timesum so this is like a genuinely importantbusiness caseumbut then you kind of think like okaywhat are the standard ways of solvingthisand you can go with the rule-basedsystemdirection you can kind of say well uhlet's only look at people in the samezip code and then if the strings aresimilar enough of the name you know wecan make a little candidate listand that's actually like a fair approachthat will work for a large chunk of thedata uh but probably not all because youhave high precision but maybe not thehighest recall when you do thatbut then if you start thinking wellmaybe we can do like a little encodingtrick where we say let's take a name ofa personand let's take three grams like thecharacters so to say so vin and then i ne and likeyou basically encode it that way you getthis little sparse arrayum and then even if it's spelledslightly differently like the sparsearrays will still be somewhat similarbecause there still will be a bit ofoverlapwith all thesecharacter engramsso that can get you thinking like heymaybe we can encode some of thesecategorical variables as text with justa sparse encoding and then if we justindex all of that and we have a newcandidate and we just say look this newcandidate is there a duplicate of itjust fetch the 10 nearest items and uhif there's a duplicate that's probablyin that set of ten itemsum and of course you can do fancierthings if you're dealing with text umsimilarly you can also use embeddingslet's saybut i've noticed just with some of thesesparse tricks like the sparse embeddingones the results were quite reasonableum whichyou can do some fancy things like youcan also do thresholding right so youcan look at the distance of the itemsthat you get back and you can kind ofsay well only look at the items wherethe distance is this big or this smallet cetera uh and i was actuallypleasantly surprised how easy to setsuch a thing up and also that it kind ofjust works umthe the way that you encode thingsespecially if it's like names and you'remainly concerned about spelling um likethe standard encodings for sparse stuffreally lends itself well for these kindsof lookups um so it was like a verypleasant surprise umif folks are interested uh this issomething i'm also exploring on theyoutube channel using prodigy but justthe whole idea of maybe doing dataduplication this way i think it's verypowerfulyeah so it's um so when i hear datadeduplication i think of it maybeusually from the perspective of sayyou're training a language model and youget internet scale text and you want tode-duplicate repeat instances ori guess generally looking at data setswhere you don't want to havethe same instance repeated twice becausemaybe with the kind of bias of gradientdescent it's going to be overfit to thatparticular kind of thing but it soundslike you're coming at it from theperspective of say uh knowledge graphconstruction or uh this kind ofcustomer databases yeah like a littlebit more in that wrong yeahyeah really interesting it's reallyinteresting way to kind of summarize thedata set too by merging similar vectorshow does this differ from say like ak-means kind of clustering sort ofcompared to just kind of like merging itdown with the semantic categories i meanso the downloadk means is not an unreasonable idearight but you do immediately hit theissue of how big should k be like that'skind of like okayyou solve one problem but you alsointroduce another one and you simplydon't have that if you just have asensible way of encoding things um andagain it depends a bit on what yourconcern is right so if spelling errorsare your concern then sparse encoding islike the obvious way to go about itif sentiment is more your concern andit's like a bit of written text andmaybe you're a bit more interested inbot behavior let's say like hey is theresomeone who wrote nearly the same thinguh then at some point you know languagemodels and text embeddings those thingsbecome a bit more interestingumbut one thing that uh that i realized asi was sort of doing this work is um ifyou have a tabular data set let's saysome names and addresses or a graphright and different nodes you can chooseto encode uh each column differently soyou are free to say like okay but thisis a user with a name okay that's goingto be sparsely encoded and what the useris sayingright that can be with dense models andyou just concatenate the two um maybe dosomething more fancy with likedimensionality reduction as apost-processing thing but that also canjust still be indexed like that's alsolike a an aspect of like deduplicationwith approximate errors neighbors that ithink is pretty interesting too there'syou can use the same column in twodifferent encodings you can encode eachcolumn differently and you can actuallyuse a fair amount of domain knowledge tosort of steer the algorithm the way thatyou likeso i think there's a lot to that in thissowe're getting on the idea of how weencode the data how we produce therepresentations and this idea of saysparse representations which are saylike bm25 tf the idf or say thecounterpart just ones and zeros yeahyeahand and um infusing that with the vectorrepresentations that come fromusually sentence transformers i think iswhat we're thinking about with text anddense passage retrieval these kinds ofideas and fusing those two so so maybewe could go a little further on thisidea of how are you thinking about thecombination of sparse representationsthat are usually based on some kind ofkeyword counting what like how and ithink they vary mostly in how thevocabulary is constructedand uh and then fusing it with thesedense retrieval methods yeah that's agreat question because a lot of whatyou're mentioning there is a bit of anunsolved problem because you you can goahead and just concatenate the two rightyou can say i'll just concatenate thespar stuff with the dense stuffand you can wonder well what kind ofeffect will that have for like distancemeasuresis the distance measure that i'minterested in for like the text is thatgoing to be different than the distancemeasure for like the users uhit might start getting a bit trickyone thing you can do though and this islikenot step one per se but maybe step twoso let's say um step one is the thesimple trick was just sparse vectors fornow but by doing this we have actuallybeen labeling tworight so okay this was an actualduplicate this wasn't and this was anactual duplicate and this wasn't okaythe moment that we have a couple ofthese examples then we can say okay partof my input is sparse part of my inputis dense i'm going to put a dense layerbehind thatand then i'm going to have after thatlike an actual label that i'm going totry to predictum like i think this might be theneatest way to sort of merge the twotogether because then at least there'ssome sort of asignal from your label determining whatthe next layer should be and then youcan use that hidden layer as your nextembedding for your uh lookup so to sayyeahsorry so so that that's an approach thati think uh is quite reasonable um but itdoes of course always depend on yourdata how many columns you have etcbut yeah maybe we could stay on that alittle more the idea of you have yoursay bm25 representation and then youhave the representation that comes fromone of these sentence transformersso we have two vector representationsand then say we want to put that intoanother dense layer that is going to dothe say we could call this multimodalfusion if we'd likeyeah not sure some phrase like that yeahsome phrase right yeahso what would that architecture looklike we have the two vectors should wemaybeuh would it be like an attention layerin thatlike because we could have the mlp wherewe just concatenate the vectors like ifyou're asking mineso i would start simple really right imean especially because in the beginningyou're not going to have millions ofdata points right um so then keeping itlightweight is probably the mostreasonable way to go about itumand also the thing like if you want todo stuff of attention typically you alsoneed an extra tokenizer involved and theorder of the tokens might start tomatter and stuff soit's not just the it's not just morevectors it's also some of that stuff yougot to keep in mind and again like myvantage point at the moment is a littlebit more that i have a table with manycolumns uh and then for one column isuddenly need a tokenizer and for theother one i don'tto keep things simple i would just keepit fast forward and dense umand also like a thing that i've noticedwith this deduplication stuff um atleast on the data sets that i've beendemoing onumimagine that we're actually doing thisfor like a bank let's say we're tryingto merge customers together and then atsome point it turns out that uh for somepeople there's like a typo in theirsocial security number let's saybut for other people just a completelydifferent number like obviously adifferent numberumokay yeah maybe that's a duplicate butthat also should be flagged for fraudrightso the business case also wheneveryou're doing something with duplicationthere'sa lot of the time i would say maybe alsolike a fraud angle involved and becauseyou're then also in the realm ofmultiple targets that you might beinterested in right umthen you kind of want to have a neuralnetwork maybe with two output nodes andthen again keeping it simple just thedense layer i would argue is probablythe first thing you want to doyeah i'm really happy you brought thebanking data set idea into the picturebecause i think our conversation isgetting into that realm where having adata set really would help communicatethis idea um so the data set to me thatjumps out the most and i'd love to getyour comments on this is the core ofquestion pairs of duplicate questionsbut maybe it's not maybe it's not longenough i know those questions areusually only like 10 words long andyou know as i've been studying vectorsearch with we v8 i've seen like big andmaybe not like massive inputs but only10 words long is not great for this ideaif we're going to combine bm 25 and dprand have a another density on that it'snot really the best data set for thatso maybe another example and again thebanking example is is great although i ithink i'd like to first start withsomething that's purely text for drawinghow we do this so maybe we could use thethe same heuristic they use in wikipediawhere to train the sentence transformersthey use the heuristic of neighboringparagraphs are going to be positive[Music]soand it gives us a good sequence lengthof say like 200 tokens so we cancombine bm 25 and dprso so say we take the wikipedia datatrain test split and then we have ourbm25 of each paragraph and our sentencetransformer embedding and then we trainour you know the dense fusion layer ontop of that to have the agreement do youthink that would be uha good example oftying this together yeah soi my background's not really in searchso much so what i am going to attemptnow is a little bit of a guessing gameand someone who's more knowledgeablemight be able totell me that i should read another paperbutum just for just pragmatically wheneveryou're going to be doing some mergingrightumcan we guarantee that the labels are ofhigh quality because we are probablygoing to have to say like okay we haveto be super sure that this is an actualanswer to that question or we have to besuper sure that this definitely isn't anactual answer to that questionand because we're dealing with languagethis is not there can be a whole lot ofnuance rightum and my impression is that there's avery similar problem in the realm oflike queries and sothis one query that a user typed andthen the user then clicked this oneother thing while would the user havealso clicked something elseyou don't always knowso i think from my perspective the issueis maybe not so much the architecturethat i would be primarily concerned withbut maybe more just a data quality likeam i really really surethat i have meaningful labels that canactually propagate some sort ofknowledge into these layersumhaving said that the moment you are sureabout your layers then it's much easierto iterate because then you can trustyour metrics a bit more as well um butespecially if you're just gettingstarted with this deduplication stuff iwould be more worried about that likeare we sure it's a fraud case are wereally really sure it's an actual umduplication or is this or is it actuallya person who lives in the same streetwho just has a slightly different namebecause that also happensyeah the the part of encoding it is sointeresting and maybe quickly before wego into the bulk data labeling as wellcould we could you expand on the idea ofthe index or half of this one of themost interesting parts of we've eight isthat hmsw built in the approximatenearest neighbor benchmarking in thatwhole world uh could you tell me howlike that kind of approximate nearestneighbor search forsay large scale deduplication what thatkind of picture looks likewell so it usually gets started on asmall set right and it's also somethingi've noticed with the annoy library theannoy library is quite good but themoment the data sets get uh fairlybigger um you're going to want to startadding more trees quickly likeuh i used to be a bit more positiveabout this but especially like i'venoticed that with word embeddings aswell you might want to have 10 000trees which feels a bit much but thatmight actually be quite reasonableumso the main thing from my perspective atleast because i'm more interested indata quality uh whatevera n solution i have ikind of don't want to think about it toomuch um so i do think that the stuffthat i've been using so far is in therealm of medium data uh and for thosesorts of things like in memory stufflike a n pine and descent those thingskind of work but i do know if i'm goingto go bigger then need like something ofa more formal like proper databasesolution uh like and i can definitelyimagine we've yet being a tool toconsider there yeahyeah quick quickly before uh could westay on the the 10 000 trees and maybeexplain to our listeners a little moreabout how the annoy algorithm works fordividing up the space so so to me annoyis very similar to like binary searchtrees it's just instead of a scalargreater than or less than it's like ahyperplane that divides thisn-dimensional space um yes and there'sno stochastic algorithm too so you haveten thousandso could you could you explain furtherhow you get to twelve thousand umi could be somewhat wrong but myimpression of the algorithm at least isuh you just pick two random points andyou to make a plane between those tworandom points and boom that's your firstcutand then from each leaf there you repeatand basically you do this a whole bunchof times and then you have one treeand then you know a binary tree so thelookup in the space can be quite fastandokay you can do that with one tree butthen the odds of you being in a leafand then missing a couple of interestingneighbors is quite bigso you know what we do we make a secondone of these trees or maybe a third andthen we put that in kind of an ensembleto figure out who the nearest neighborshould beum and on smaller data sets you might beable to get away with 10 of these treesbut for larger ones i've noticed uh youyou definitely might want to increasethe number of trees just becauseotherwise it gets quite approximatei will say like uh i annoy is prettyquick when it comes to indexing likethat's something i did notice it wasactually you know it's written i thinkit's written in c plus as well so it'sactually quite fastumbut uh you do want to keep an eye onthis one hyper parameter that's onething i've noticed you should not uh youshould not blindly assume that 10 treesis enough becauseonce it gets bigger it just isn'tso you ensemble the nearest neighborsfrom 10 000 trees to get all of thepotential duplicates in the data ibelieve i believe that's what they dobut i uh but i could be mistaken becausei didn't i didn't check the c plussource code to know for sure but ibelieve this is the approach that theydo yes yesyeah it's really fascinating i lovethinking about this idea of how thededuplication finding nearest neighborsand then how you do it at a very largescale i think that's really interestingandand um maybe quickly we could kind ofjust entertain this idea arounddeduplication for language modeling datado you have you thought about that kindof thing and do you think that'sinteresting yeah so before i used towork at raza this was actually kind of apretty big topic there like hey when canwe understand when someone is sayinggibberish like something we've reallynever seen beforebut the very simple conclusion i justkeep hitting my head on is just thatlanguage is hard and there's a book fromemily bender which is a little bit moreabout linguistics for people who aredoing nlp and the first chapter justnails it on the head on this topic um alanguage like language is more than justa bag of words if i take the sentence alion eats a man and i take anothersentence a man eats a lionone of those two sentences feels a bitmore probable than the other one but howam i going to teach an algorithm thatrightand and then language is also like amoving target and then we've gotinternet slang and oh can we also detectsarcasm and emotion andit it's really just a hard problem soagain the way that i would approach thisis to just remind yourself that thelearning is really in the labels if youwant your algorithm to have certainbehavior just make sure that thatbehavior is really encoded nicelyinthe labels itselfand and that's where language modelsalthough they're quite usefulyou're usually predicting like a maskedtoken or something like that and you canwonder well is that really the patternthat you're interested in in yourapplication most of the time it isn'tand that's why especially with explosionwith a tool like prodigy we reallyadvocate for like okay like focusing onthe labels that's probably where you cansteer the algorithm the mostyeah and language is hard by the way andlanguage is really really hard that'salso the reminder that i keep getting uhwhen working in this fieldyeah that notion of language is hard andi love that you brought up that uh workfrom emily bender and it reminds yourpaper from emily bender and alexanderkohler i think it's titled uh somethinglike on meaning form and understandingin nlusounds like an uh emily bender paper yesit's a great paper it really helped methink clearly about this and and let mequickly tell you a story from the paperthat i that i think really does thiswellthe anecdote is this there are two sayme and vincent we're stranded on islandsand we've got a underwater cable thattakes from my island to vincent's islandand i'm saying you know and i'mdescribing the island of vincent youknow there's palm trees sandand we'repassing language through this cable andas the anecdote in the paper goesthere's an octopus that is oh yeah ithink yes i i think do go on but ifakely remember this now yeah yeahso the octopus is intercepting whatwe're saying and so the octopus sees ourmessages and it learns how to you knowcut the cable talk to connor talk tovincent and vincent and i don't know ifit'seach other or it's the octopus but nowyou get in i like to think of this as anof distribution event or a domain shiftevent where suddenly there's a bear onmy island i'm like vincent help methere's a there's a bearand how is the octopus supposed to knowwhat a bear isin general emily bender has a lot ofthese arguments that are like umso so i'm also that's also the thing idon't know what your background is buti'm not a linguist like i was dyslexic iused to hate languages when i was inhigh school and now i've gotten more ofan appreciation for it but i've alwaysi've always noticed like also with someof my colleagues if they have alinguistic background uh there's just somuch more of a radar for these sorts ofhey language is actually really hardthat this can also be happeningumbut yeah now the the i do remember thatpaper emily bender is worth uh uh we'rekeeping an eye on uh because of thesekinds of papers he also did thestochastic was he was part of thesarcastic parrots one as well i thinkrightuh i'm i know i should be but i'm notlike terribly familiar with that one nobutthere's a bunch of super influentialpapers and papers i'll say that yeah anduh we've been working on but yeahum yeah there's definitely lots oflanguage in it and it does make itthings like uh adding symbolic structurewith like syntax trees all i think allthat kind of that stuff could be reallyinteresting and maybe i think a nicetransition from here to kind of sometopics in wva for our listeners is um isi really like the graph like data modeland things like semantic relations andontologiesso kind of maybe to me one way to bridgethe linguist knowledge and then make itsort of amenable to deep learningtechniques would be to do theseontologies and have semantic relationsbetween data uh have you thought aboutthat kind of thing of say um maybe todraw an example if if i'm modeling mytwitter datai might have tweets and then it has thecontent of the tweet and then it has hasarticle and then this article is thisother kind of object that has thecontent of an article andthat's maybe that's an okay relationshipuh idea of this another probably betterexamplewould be biomedical knowledge graphswhereuh you start off with the knowledgegraph where you have likeuh drug has targetuh as bonding target and then so youcreate the ontology that way and thenyou put uh text information thatdescribes the genes describes thetargetsuh describes cities pathways things likethatso i guess the question in theconversation topic would behave you thought about semantic webontologies named relations as a way ofaddinglinguistic stylesymbolic structure into the way thatwe're doing nlp so i've dabbled withthis but very very briefly and i havecolleagues who are way moreknowledgeable about this than i am theonly thing like from the what i willargue that i'm the novice here but frommy novice perspective one thing that i'mreally charmed by when i hear thesesorts of things is i can imagine that ifyou have a graph structure you can't puta lot of domain knowledge in and that'sactually quite useful like just ingeneralthe computer is the octopus who doesn'tknow what a bear is but at least you cantell the graph that the bears are mammaland dangerous and and hungry uh likestuff like that for example right andthat's already nudging the algorithmkind of it feels like you might beadding a nice little constraint to yoursystemand now graphs on the other hand arealso still graphs so they're notnecessarilylike uh they can be complex too right soyou're also introducing maybe somecomplexity but just that aspect of likeyeah but we could put domain knowledgein that seems very useful to to me umfor sureandso that domain knowledge interface couldbe useful fordeduplication because with similaritylabeling right you end up with a graphwhere you're saying this is similar tothis thing it's also similar to thisthingso so maybe we could talk about the ideaand coming back to labeling too i thinkit ties it together like how do we labelsimilarityahthat's the eternal question that we'venot solved yet i think butum like the way to think the way i liketo think about it at least is again sortof coming back to the whole idea of thelearning is in the labelssouh before and uh i was talking to aretailer the other day what a veryinteresting one soumwhen we are out of normal spaghetti whatis a good replacement productwell to some people uh it is going to beuh another pasta like any other kind ofpasta but to other people it has to bespaghetti so i'll just get like uh thethe whole oats like the super fibrousspaghetti insteadokay who's rightwell the similarities in the eye of thebeholder in a way right like it dependson the use case too and you can alsoimagine that umlike if you have kids for example theshape little shape of the pasta alsomatters because kids are stubborn theywill only eat the one pasta in a certaintype of shapeand the food analogy but like butsimilarity is really also kind of anopinion if you think about itso the way i just really like to thinkabout that is just okay we have somesort of business case we try to defineit as clearly as possibleand then we try to label with like in aconsistent way and we try to achievesome sort of a consensusumthere's a very there's a lot ofinteresting work also happening in thisspace um if you want to have a fun lookuh there is the google emotions data setand what's the data set has lots ofinteresting label errors at least frommy perspective but the interesting thingis they also have the annotators inthere so you can actually track like heywhen do annotators disagreeum a ton turns outlike uh justand especially in the realm of emotiondetection right um when is it sarcasmwhen is it humor uh when is it actualjoy uh when is it actual anger a lot ofthat is dependent on culture which isreally dependent on where you liveum so it's not necessarily a surprisethateven if you are a fluent english speakerbut you live in the other side of theworld and you're 40 that you're notgoing to understand what 20 year olds inthe u.s are posting on redditor not perfectly at least no one shouldbe able to blame you for itand i think the best example i had whileresearching this um folks who areinterested there's a youtube video imade for prodigy on the techniques thati use but there was oneoh my god those tiny shoes i want toboop it snoot like that was like thesentence and then the question is isthis about excitementi would argue it is something booping asnoot sounds like there's a photo of petsomewhere and someone wants to gentlytouch that nose so that sounds likeexcitement to mebut i can't blame anyone for notunderstanding what booping a snoot isright that's kind of like internet slangin a wayso there you so there you already kindof go like what is similarity well iwould argue it's similar if a label saysit is umbut if again but hopefully the way thatwe encode it correlates with it andthat's kind of useful when we have anearest neighbor look up um so let's sayi have a whole bunch of reddit data umi'm just gonnause whatever featurizer uh whatevertransformer just sort of index it andodds are if i can just have one sentenceabout a topic that i'm interested inkind of zero shot i am going to getexamples from my actual data set thatare at least to some extent similarso if i'm let's say doing social mediaanalysis and i haveum i don't know a support desk for alocal retailer um then if my delivery islate then i can just type my deliverieslate encode that and hopefully what iget backum are things that are in some sensesimilarum but that then you label and then youmight get a better embedding so then youindex it again and then you get intothis little kind of weird activelearning loop where you have a modelthat makes better embeddings which youcan actually use to re-index and uhhopefully by just iterating on this uhyou know you you're not just iteratingon your model you're also iterating onyour data and that hopefully is going tolead to like a very tight definition ofsimilaritybut that's at least how i approach thelike to think about it at leastyeah and i'm so excited to get into thatloop of active learning week supervisionand how uh say we've yet and vectorembeddings can help with that quickly iwant to just tell a little more abouttheannotation disagreement i i was verycurious in this idea of semanticrelations when i first saw we've eightand i was because i to me the idea ofvector embeddings and also adding thekind of named relation thing wasn't anobviousconnection i asked bob about it and bobtold me the story that uh he was at asemantic web conference and they werebuilding ontologies and they couldn'tagree on what a c was or a lake likejustsee this is a lake to usand it's that kind of disagreement inthe relations that i think you can thatthese embeddings let you really capturebecause i because you can have twolabels wherecoming back to your example it's sarcasmor it's excitement you can't really readit but you can have the two labels andit can kind of optimize for both thelabels and and capture the semanticsthat could be either thing in thatvector representationand then yeah this general idea ofannotation disagreement with humanlabeling i think that's i think there'sa lot to that especially when you'reuh labeling really complex things likesay you're annotating scientific papersandso there's a there's a paper uhi hope i pronounced his name correctlythere's a bunch of authors but uh yo ofgoldman goldberg i forget his last nameterribly sorry but uh the the paper'stitle is something along the lines of uhare we annotating the task sorry are wepredicting the task or are we predictingthe annotatorumwhich is a really good question if youthink about it turns out a lot of theannotated data sets out therenot at all all of them but like a goodchunk of them you can kind of wonderwell how's the annotation in balance isit like a 2080 rule where maybe 20percent of the annotators annotated like1280 of the yeah okay turns out for abunch of data sets this is kind ofhappeningso then you can ask the question uh okayum can we maybe predict who annotatedthis thing or can we can we make aprediction per annotator so then themodel output becomes not just hey whatlabel is this but what would thisannotator give for this labelsothe paper makes a point here to say likehey the label is not perfect that's agenuine concern but another way that ilike to think about it uh it would bekind of nice in production if we couldsay hey the model's just kind of notsureso like won't predict it's kind of likea flag that we can do we're only goingto be automating when we're actuallysure about our prediction and nowsuddenly if you have let's say 20annotators that you're kind ofpredicting and let's assume for allintents and use cases that you're goingto do that correctly pretty accuratelythen the moment that you know not allpredictions agree that would imply thatmaybe you're able to model thatannotators wouldn't agree and that mightmean that in production you should raisethe won't predict flagyeah that's brilliant that sounds like areally exciting idea to me and maybelike a well annotated data set is one inwhich you can't predict theannotator well so annotation's hardrightbut genuinely like umit's it's much harder than i thought uhand also likeback when i gave trainings in datascience one of my favorite things to dowas let's first annotate the data setwith sentiment because half the timewe're actually going to not agree on thelabeluh and then what what i also recommendpeople do is even if you're doing like atoy data set thing trying out the newtransformer library just check the datafirst because label errors arepersistent um there's a websitelabelers.com i don't know if you've beenno but like um the quick draw data setlike according to their estimates has aten percent label error or ten rightuh i i believe umthe amazon sentiment one has like twopercent label there and that's well likeyou know within the margin of state ofthe artsoit's uh it is just a hard problem againthe the only cure that i know or remedyi should say it's just uh the name ofthe game is iterating just every weekyou try to just have a meeting and youmake sure that the data scientist alsolabels and the business person and thenyou kind of go like okay let's look atthe examples we got wrongokay do we agree with the label that itwas predicting yes no and you knowthat's uh that's the the remedybut it's not easy like the modeling isthe easy that's just tensethis is an extremely interesting topicand i love exploring this applicationfor we've eaten and how we cantie it into these active learninguh loops and so so here's how i see thepicture of how we can help with this andmaybe could uh you know add to what i'mmissing but so i imagine that we're saywe have some uhyou knowwe're looking through our predictionsand we see oh this one has beenmislabeled then we find the nearestneighbors to that one and that's goingto guide our where we're going to golook for more mislabeled points sothat's so that's a tactic and that'sdefinitely not a bad one um one thing tokeep in the back of the mind like thesituation always kind of depends likeyou might be interested in just samplingmore from the rarest class right likethat can also be a really good reason soyou just say look there's this one classthat i'm really interested in but it'skind of rare maybe sample more of thoseand you can still use again you can usethe nearest neighbor lookup trick we'rejust gonna use it slightly differentlyanother thing you can also definitely dois like hey let's just grab some of theexamples where we have the mostuncertainty and let's grab more of thoseumthere can be more than just one validtechnique um the one thing that i do umthe one the one thing i'm trying to sortof figure out and i'm not there yet umis just what's the most pragmatic way togo about it because maybe sometimesyou'll just want to do randomumthat's a because i can imagine forexample right umlet's say we're doing a social mediathing i'm interested in finding textsthat are about a failed delivery likethe customer complaints use caseokay i might have lots of examples wherei started out by just looking for theword deliver if the word deliver appearsin the sentence it's probably about adelivery and if someone is goingreaching out to customer service it'sprobably a complaint um okay so that canbe a very reasonable place to start butthen if you're going to look forneighbors that are very close to it youare probably going to get lots ofsentences that just have the worddelivered in it those aren't badexamples but it might take a whilebefore you have a sentence with the wordorder in itsoso okay how can you protect yourselfagainst that well random is a thing thatyou could do and justdo it a bit randomly make sure thesearch space is kind of covered uh thereare also like some fancier techniqueswhere you kind of do a k means thingfirst over your entire data set suchthat you make sure that at least there'sa bit of coverage so sayum but this is also kind of the trickything with active learning umyou do want to construct a test setthat's not just because we were samplingthis way this week and we're going tosample a different way next week sobut but but it is still a game ofiteration that is something i do thinkis the pragmatic way to look at it andone thing that i was pleased by is thatthere are many tricks of going aboutthis but having a nearest neighbor lookup is actually like a legitimate umthat worked quite well for a couple ofthe tricks that i was doing which uhwhich is nice because not every trickworks it's nice that you find a trickthat doesn't work once in a while umyeah yeah it depends how big your dataset is as well like i mean if you onlyhave 2 000 examples to start with thenyou can probably brute force it on yourlaptop but definitely the moment whereyou start hitting like a 10 000 andsquaring that becomes a bit expensive sothat that's kind of the time where youmight want to start investing in thenearest stable lookupi think it's interesting also kind ofthecombination of the unstructured textlike our data like text or images withthis metadata as well andit kind of like what i'm imagining is ii really what you said to me about theidea that i'd want to do the nearestneighbor and then but look in the rarestclasses go hit the tail end of my dataand so that really stood out to me asbeing an interesting way to come combinethe nearest neighbor search with saysymbolic filters aboutyou know which class is the rarest classyou can add and you can and i don't meanto be too heavy on the features of wbabut i i love that part that you cancombine nearest neighbor search withsymbolic filtersto say likewhich sub which clusters are your dataand you could use k means toadd the symbolic values of which clusterit's into do that kind of thingand yes i thinkso i definitely so the general comment ihave here is in general post processingon the query is a good idea and thereare definitely many ways of doing it umanother likeanother simple way like in the case ofdeliveries you can also say give meeverything back that doesn't have theword deliver in itbut but but stuff that is actually kindof close like it's another way i'm notaware of the super symbolic feature youjust mentioned in we've eight but uh buti do like in general i agree doing postprocessing afterwards can be quitemeaningful that's definitely also atrick that's uh should be appreciatedmore because a lot of people just thinkoh i'm doing the nearest neighbor lookup thing i got 100 items back then youcan also still wonder well you don'thave to go through all 100 of them maybemaybe just try to find the most 10 mostinteresting ones in that and then moveon to the next uh itemum of course depends on your datadepends your task etc but uh but giveyourself the creative freedom likethat's the main point i'm trying toemphasize i guessyeah really interesting and i eddie anddidlocker has made something about preand post processing in the symbolic uhfiltering and i'm not quite an expert onit but if people aren't interested ini'll leave it in the link in thedescription butyeah this combining of nearest neighborsearch and symbolic filtersvery interesting for this kind of datalabelingfinding problems in data labeling soyeah it's really interesting so let mekind of pivot out a bit and ask you moreof a broad question about this wholespace of nlp and i'm very curious andsomething that guides my thinking isdo you have a particular application andi do think that chatbot thing isextremely interesting but do you have aparticular application that you'reusually focusing on developing and youkind of generalize these abstractionsinto developer tools likeuh like spacey and so on umso you mean uh personally or it's spaceyprofessionally because i maintain acouple of personal open sourceprojects as wellbut my role at explosion is a bit broadat the moment so like one day i'm doingsupport tickets the other day i'm doingopen source stuff the other day i'mdoing a couple of pr's to fix a coupleof bugs the other day i'm making contentso it's exposure my role is quite broadat the momentumbut but you know we we also offer aconsultancy service these days umand they are typically like uh towhatever problem a client hassomeone says hey vincent can we just uhyou knowzuma for a bit and like exchange ideasum but i also think in that realm of nlpthat we typically have clientsum it is a bit on the classification andentity detection like those kinds oftasks we we don't do generativealgorithmsi think at all um it's mainly like heycan we detect interesting substrings uhand can we maybe say something generalabout this uh this bit of text aboutthis document and those are the typicalthings that we do those that's likedefinitely our specialty so to say andthat's also wherelike our annotation tools can also kindof make a difference because we have uielements that are like really reallygood for those specific tasksumso thethat's the most typical thing i wouldargue in the spacey realm that you see alot of but then againbecause i also do a lot of stuff withprodigy like a lot of my work is alsomaking interesting demos um the coolthing about prodigy is in the end youcan script it it's just a python backendso you can make your own custom labelinginterface and use whatever your favoritepython tool is so part of my jobin that realm that is actuallyinteresting i suppose also like in thebroader sense is you can actuallyexplore like what arecool annotation tricks that you might beable to do and finding bad labels is onefor example like hey are there sometricks where we can automatically make alist of candidates that this might be abad label and again you can use thenearest neighbor lookup thing where youcan say heythe label of this one point is let's sayclass a but all of its nearest neighborsis b okay there you go there's probablysomething fishy happening there wechecked it outum but but that's kind of if i can pointout anything that's really fun about thejob that i have is a lot of the time ican actually make really cool annotationdemos and because i can use whatevertools python can use inside of prodigy ican just sort of do neural likebasically anything that python can doand that opens up a lot of the doors umthat said umthe hardest part is usually what makes agood label like that's a qualitativething as well that's definitely stillthe hardest part i thinkyeah whatand then another questions and maybe alittle scattered now but i'm sure sowhat do you think about maybe like acollaborative data labeling interface solike i you know i share it with my teamand we all like so we it's like a ui butall of us are together it's kind of likesay like notion for data labeling thatkind of thingi mean so umdepends on how you want to set upprodigy but the way a lot of people likeback in the day when i was doing demoswhen i wasn't working at explosion theway i would just do it is hey datascience class we are about to dolabeling uh on my own local machine i'vegot the server running ngrok and it'snow public and people can sort of uhjoin inum so it's just a server you can open itup on your mobile phone uh so that'sactually something uh like there arethese labeling workshops that is myimpression this is something that peopleactually do like hey umteam of engineers get together andthey're gonna say like hey we'reactually trying to maybe build a new usecase um we're going to do a sprint of aweek and day one is just going to belabelingand then usually after labeling for aday you kind of have you've seen enoughdata so you also know some of theexceptions um and like it's notnecessarily notion because it is savedin a flat file most of the time but uhbut that like i hear stories that peopleare actually doing this and one thingthat i have her one story that i haveheard that was actually kind of funny uhthere are lots of these examples whereyou start out with the use case we'retrying to do a customer sentiment orsomething like that but then afterlabeling you find out that actuallythere's fraud in here toobecause you're actually looking at likethe raw logs and you're kind of goingthat's not a customer that's a botwhere doing the labeling the use casecan actually kind of shift because youare actually confronted with the data uhthat is something we've also seen thoseare stories that i have heard let me putit that waysuper cool so the next question wellquickly let me ask you a quick questionjust to fill in my knowledge can youtell me a little more about ngrok isthat what makes it easy to create theseuh you know like web services frompython or it's more like a tunnel soit's it's basically like if i havesomething running on my local host on mylaptop that's great but then you can'tgo to my local host but then i can tellngrok hey just open up a little tunneland then ngrok will then have a publicuh url basically and that's gonnaforward to my local machine so for demosit's really greatnot for production by the way don't doit for production but for demos and likeshowing stuff to the people locallyright it's great um they have a freesurface they also have a paid one uh butfor especially for trainings and likejust quickly sharing the labelinginterface without having to worry toomuch about security and grocery i canhighly recommend it and that plugsnicely with say google collab rightwhere you canis that how like i know um like radiothey give you like a temporary webaddress for your app when you run it ingoogle collab is that how it's doneoh i'm not aware so i i've never reallyused gradio uh and i'm also not thebiggest collab user i have my own littlelinux server at home that i prefer touse but uh it could be i'm not a socan't answer that properly because i'venot used radio yetoh yeah yeah i'm just curious i was umwhen i was looking through uh simcityone of your projects that you'reinvolved in i saw the the way that youhave that one line of code to set up thehosting and i was and i've generallybeen seeing these thisthat's a fast api serverso that's just a fast api server um sothe for folks are listening uh some ofthe ideas that we are mentioning here imade an open source project a while agocalled simcity it's misspelled sos-i-m-s-i-t-yuh it's the joke that i had was it's allabout building a neighborhood so that'swhy i call it simcity umit's a bit of a hacky project like it'sit'sit's a brain fart that's pip installablebut it's not like i'm activelymaintaining it so feel free to playaround with it but don't expect supportsorryum but one of the things that you'reable to do the the goals that i hadthere just very quickly was that i wasable to sort of do exactly what we justtalked about here's a data frame here'sa couple of columns here's how i wantedto encode here's i wanted to index goand then you could use all sorts of uiwidgets from jupyter notebook or justrun it as a fast api server and you'dhave your rest endpoint or you can justsort of say hey here's a json objectwith like a name an address etceteragiving back to the 10 most closestneighbors uh that like that old partit's the thing that tries to automateand sorry do you mind telling me alittle more about fast api yeah so fastapi is a it's it's basically like flaskbut has a couple of more modern featuresthat are quite nice now i want to saymodern here and i do want to acknowledgelike the flash project has alsogotten a couple of new features in soboth projects have their own pros andcons but uh fast api is relatively fastas the name impliesbut it's just a very likable pythonserver that's really easy to set upum there's it is a bit newer so the oneof the main reasons to maybe stillconsider flask is that the pluginecosystem is just a little bit more richumbut for example fast api did have like async support right out of the box um ithink flask also has it now but theyintroduced it last year if i'm notmistaken so for all intents and purposesit's like flask it's a little bit morelightweight and you get a bunch of stufffor free so like the docs automaticallygenerate for example for all your apiendpoints if you add the docs string butfast api is nice if you want to make aquick web server such that other peoplecan communicate with your machinelearning app fast api is a prettyconvenient way to just quickly setsomething upfascinating thank you so much for thatreally interested in the trending thesekinds of tools that are making it so youknow you if you're just like kind of amachine learning scientist it's gettingeasier and easier to share your thingswith people andand yeah these kinds of tools um so letme ask you another uhsorry well i'm gonna make a little pitchthen so uh i do also host this thingcalled calm code uh so which isbasically like a learning center forlike stuff that i made uh it'sthe premise is it's like educationalcontent but not on youtubeso i don't want you to get distracted uhbut if you're interested in ngrok orfast api i mean there's free tutorialsthere uh if people are interested inlearning it umit's two to five minute videos afterwatching for half an hour you know howit works uh so ngrok fast api if youwant to learn it tons of places whereyou can learn it but comcode i've made acouple of tutorials so if people areinterested those are for free uh pleasewatchthey're great tools they really arein addition to the quality of thecontent i also have to complement thequality of the styling on that websiteit's not a little cool black and whiteicon thingum so okay i'll make another pitchthere's this project called the nounprojectand i can and i'm a paying member it isthe greatest service effort for everysingle noun they have a black and whiteicon both in svg and in black and whiteit's amazing for presentations the nounproject is an amazing amazing uh bit ofsoftware that i'm proud to pay money forit is the bestand yes every icon that i have on thecalm code comes from that website um andi kind of just love how it has like ayou know portal the video game it kindof has like that kind of a vibe to it aswell yeah but but yeah i don't want totake credit for those icons folks overat the noun project should butnon-project's greatso on the topic of making content can iask you about kind of i don't know ifit's too into like yours your you knowcore strategy but could you tell meabout content strategy and how you thinkabout that kind of thing andyeah well i meanso the story is a little bit funny umbasically uh the way i got into thiswhole content thing i mean i wasdefinitely like a teacher before so itaught calculus and university and youknow there's definitely like a teachingthing i did that for clients back in theday um but the way i got started in thissort of content developer advocacy kindof stuff was i went to this conferencecalled spacey in real lifeand when i went there i had never donenlp beforeuh but i figured you know a spaceycool project if i go there i might learnsomething about nlpum so i you knowyou uh you hang out with people and iactually met matt and enos like thefounder of the explosion company and youknow you go to a barandat some point i make a really bad joke igo up to matt enis i mean i say theprevious bar is really busy but this oneis nice and spaceyyou know really really corny joke uh butapparentlyfrom my perspective yeahi didn't get hired then but what theytold mewhat they told me was vincent we thinkhe would youtube well you know a littlebit about data science you're kind ofyou have a good voice and you're youknow you're kind of funny would you beinterested in maybe making some spaceycontent and i figured you know i canlearn from like the two peoplespearheading this project which ismaking a huge impact uh if i say yes i'mgonna be able to learn from them that'sa pretty good dealand um i'll i can give this wholeyoutube thing a trythat's how i got rolling um butthe deal that i did made because ididn't really want to become a youtuberin a sense of click and subscribe likethat felt kind of lame i genuinelywanted to just make umlike a nice example nice and tangiblelike if you want to apply spacey let'sjust show how you can use it to solve aproblemand they were super on board with thatas well so it was liketwo hands on one belly as they say inthe netherlands um but the funny thingthen was i was making some of the spaceycontent and then right around that timepeople over this company called razathey were interested in someone whocould explain algorithmsand then they said hey vincent we likethe style we see on the spacey thingthat you've made could you do somethinglike that but for chatbotsso that's kind of how the ball gotrolling butthe main thing i really care about isthat i just explain things relativelyclearly and i think that's enoughi'm sure i don't mind if people followme on twitter and i also speak of piedatas and i like it if the room is fullsurebut i get a little bit annoyed sometimeswhereyou're looking at like a youtuber tryingto explain something in data sciencethere are plenty of really good ones buti wanna but it has to be about thecontent like i really wanna see like areally nice explainer i love seeing whatpeople have like i found an interestingdata set and this one trick works inthat data set i don't like it whenpeople regurgitate this i could learndocsumthat's the main thing and it's also whyon the comcode website i don't want myface in it because i really wanted to beabout the contentnow of course i also make videos forexplosion and we put that on youtube andwe don't have a coherent style so thereit makes much more sense that my face isalso perhaps in the video uh but mypersonal uh preferred style is to justhave very calm content that's the thingthat i like uh most and i'm trying to dothat on youtube as well the onlydownside of the platform is that everyturn it does try to distract you withits recommendations so that's kind of abit of a downside i suppose butbut i don't really have much of acontent strategy besides i try to makegood interesting content by trying tofind interesting examples uh and sureyou know we we have a content planningthing whereif i've made a 40 minute long video wedo want to announce it properly right soyeah we do thatbut i really just try to findinteresting content first that's that'sthe strategyand one of those and one of the firstthings i made for explosion wasdeduplication by the way like how canyou do deduplication using prodigy uhbecause one thing i one thing i uh ingeneral that i have observed is i meanprodigy is great for labeling nlp tasksbut there's way more tasks just likewith these nearest neighbor lookups umthat you can apply it for and reallygoes well beyond lp uh i mean i can alsoimagine a m being useful for somecomputer vision tasks like that wouldn'tsurprise me at all um but i can but ibut i get why there's lots of nlpcontent out there because i also seethose applications uh but uh a n stuffand also like the labeling stuff goeswell beyond just nlp this alsokind of the point i want to make withsome of the prodigy videos that i'mmaking there nowso yeah long story that's our strengththat's my strategythe last thing you saidi do kind of want to talk about that alittle a little more the very last thingyou said about the nlp computer visionthingand uh well butsorry but i the story i found reallyinteresting and i actually kind of wantto get into that a little bit more so itseems to me like you know you'reengineering by day and then you you knowyou the problems you solve you turn intocontent and that's kind of the way ofthinking about it rather than likethe i'm seeking the purpose of this isto make oh yeahso for comfor calm code for sure like i've notmade content in months because i've notseen it so uh but i will say like forprodigy it's a little bit differentuh because for prodigy i am actuallysort of actively looking for what wouldmake a really cool demo to show off alittle trickumso it wouldn't surprise me if maybe atsome point i i show off an approximatenearest neighbor trick let's say becausethat's something i'm actively sort oftrying to figure out and similarly ialso i'm also doing some annotatordisagreement because i'm pretty surelike if i just have a couple of datasets with google really cool examples ofannotated disagreement then i couldprobably make like a good blog post or avideo uh soum like the one thing uh also like calmcode and my personal blog those are likemy personal projects and i really likehaving an off switchbut from prodigy that'sso that means that you know a good chunkof my week i can think about like whatwould make a really compelling exampleand what's like a really uh cool featurethat we might be able to show off sothat i would argue like there's a bit ofa difference there in that senseyeah for me this part of showing areally cool example is so much aboutfinding a really cool data set do youagree with that yeahyou know for sure if you don't have acool data setit's kind of like uh oh we can makemaybe we can do a benchmark on uniformdata you kind of go yeah i know i see icould see why you but yes technicallyyou have a benchmark it's just anothervery interesting oneright um so for sure but finding thedata set for sure is like one of theharder parts thoughyeah yeah i love working at a databasecompany because i to me it's like thealgorithms they're awesome but it's likelet's find more data sets to hit thisstuff with you know likethat kind of task of i always findmyself drawn to let's go find a data setor let's build a data set you know andthatkind of thing always appeals to meyeah and umit's also kind of the thing where youwant to have a data set that's also nottoo small not too big right so thisinteresting data set is also notnecessarily a well-defined definitionbut you what you're hoping for is thatyou have a data set where you can sortof tell a story where at some pointthere's a like a luke skywalker and adarth vader in your data set and at somepoint you're trying to figure out likewho wins let's say uh could be and thatwas like umi i would like to do a little bit morewith fraud data that's like access logsor something like that i think thatcould be interesting too um also becausethere there's gonna be some sort ofencoding you're gonna wanna have maybewith some urls and timestamps let's saybut it's also not really nlp it'sstructured textbut definitely for sure you can do someinteresting stuff there i think umyeah maybe quickly we'vei had some conversations about maybeturning structured data into text likewhere you parse the table to try to turnit into text and and this is likehorribly off topic butwhat do you think about it yes i i i imight have heard someone tryingsomething like that um never done itmyself to be honest like one likeespecially when i talk about tables likeone thing i have alsoumso a use case that i'm aware of is thatthey thought like oh we are going to bescanning documents and then we're goingto do ocrand then we're going to try to see whatthe values are in a table let's sayand then you can kind of wonder well isthat an nlp problem or is that maybemore of a you know a computer visionproblem let's say because maybe youdon't have to scan the entire documentmaybe you just need a pre-processingstep that says hey where's the tableright yeahso umso i do know a couple of people who areworking in that realm uh and also that'snot necessarily a solved problem becausewith tables you gotta imagine yeah yougot englishbut you also have tables written inchinese soyou also need to be able to handle thoseandthat's substantially different typicalocr stuff doesn't handle both chineseand english at the same timebut again a lot depends on what's theright label and what's the use case thatremains the sameyou have like document structureunderstanding thing that seems likebooming application as well you got thelegal documents maybe you got likeinvoices ormedical stuff yeahyeahyeahwell thank you so much for uh coming onthe eva podcast i love talking aboutdeduplication bulk labeling and just allthese topics it's so much fun to chatwith you and thanks againhappy to hear it", "type": "Video", "name": "Vincent D. Warmerdam on Applications of Nearest Neighbor Search - Weaviate Podcast #18", "path": "", "link": "https://www.youtube.com/watch?v=Nwh7L4Do0Fg", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}