{"text": "Product from Juraj: https://www.labelator.io Spark connector project: ... \nforeign [Music] episode of viviate air I'm really excited to have you all here like you see we are already waving to you because like everyone is super excited about this new episode and it's it's a nice one because like we're kicking off in the new year so we are all super excited about it uh so you should be really nice and fun so thank you for watching and thank you for listening whether you live or you're watching the video later uh I'm absolutely pumped to see every single one of you here um so because this is live please ask us questions as we go right like I will if if I see any questions popping up like I will go and share them with uh like the audience here and then potentially we'll elect the best person to answer the question hopefully it won't be me but hey you never know um so for today I thought that maybe we could give you a quick uh update in terms of what should you expect from this episode right so first up we'll have our special guest URI who will talk to you about uh label later which is a product that he built with weaviate uh so I'm really excited about actually learning that what he's built how does it work and uh euri promised me that there will be a demo so how cool is that uh then later on we'll have Etienne uh our our CTO who will come and talk to you about the Outlook of vv8 for 2023 so basically what you may expect of like what kind of things can you expect to happen around vv8 it's not necessarily A roadmap itself but it's close enough like this is kind of like the direction the general idea and now let him talk about it more uh then Zen we'll talk about the spark connector for vv8 we've had like a couple of blog posts around it uh but um but Zen really wanted to also do like a quick demo and then show you off uh how everything works so this is super exciting uh Conor will talk about the beer benchmarks in viviate and I'm so proud of myself that I spelled beer correctly but I think um you know for those of you for for for you that beer is normal you probably cringe at me but hey I'll take that uh and then Erica will talk about uh some Community questions because uh we do receive a whole bunch of questions uh on on the community slack on and we tend to always help and and answer them and then some of them they're worth highlighting uh so Erica will highlight three questions three uh and then she'll go over them and that'll be cool and then at the end if you have enough patience to stick with it and then wait until the end I like to actually show you a preview of our new new website so like there will be a huge update to the new document to the documentation for the review documentation and in general will be changing quite a bit around the Vivid IO website um so yeah this is basically the plan for uh this session uh so without any further Ado I want to pass on the button to your our special guest who talked to us about the label later hey you're right welcome to uh Vivid air I'm so happy you're here hi thank you so much for having me yeah and thank you for the opportunity to show what I've built with uh vv8 uh I I must say like uh it was a really long journey uh until I got there but uh Pleasant one so thank you so much uh for helping me even like with with uh building in and solving my problems so it was cool yeah absolutely to be honest this is like a two-way street because like uh I always loved like this was one of my first interactions when I joined the we data when I joined the company and the community and like I could see you asking a lot of questions but you also answer a lot of questions so I'm super pumped to have you here and then now showing you like hey where is this all coming from yeah yeah uh yeah why not help when you know the answer you know always are happy to help okay so maybe you can share my screen and I'll describe let's do that I'm building on on the fly so this is laborator uh basically what is laborator the laboratory is a tool where you can uh fine tune and build your own uh NLP models so usually how how you work as a data scientist you have a data set uh that you probably you didn't vibrate because it's kind of at work and then you you build a model and deploy it and do all the the engineering stuff and yeah it's cool but if you want to build something that is uh kind of unique you need to have a unique data set uh and to label these data it is problematic and uh I was facing this problem I was trying to build something uh something special which I couldn't find a proper data set for and like using the uh common uh annotating tools uh wasn't really uh working for me because I was getting crazy after after a while so uh I bought this uh it evolved from something else but um it's it's kind of cool so I'll show it to you um what we have here is uh like projects projects uh uh basically like imagine like a data set but combined with models and other stuff so uh if you have a project then you can just pick the project you wanna you wanna open and let's see this one for example and you are immediately like through into like documents uh but let's start from beginning I will do it if you'd be like uh trying to this product for first time you just create a new project let's say we want to do some I don't know comment moderation that's a use case that uh our customers are are using too so let's call it like comments first um and we might have some data or we might not that's also a common problem it's kind of a chicken problem that if you uh want to build a product that is uh um that is really unique and you don't have map you might not have a a demo data to train the model and you won't get it and you'll have it of your first customer so let's imagine that you have a problem you that you feel that you can build it with you can solve it with dimensional learning with AI uh but you don't have the data the real data that you you could change them all along but let's imagine that we have some data but they are not labeled so I'll just choose this this CSV file with with some comments downloaded from the internet and yeah I can choose like the mesh learning task that I want to solve so let's go this time for this classification and this is where the vb8 uh is coming in uh so when we import the data they will be imported with the vectors and you can choose like which model you want to start with so if you have English data you can choose one of these and you have if you have a different language you can pick uh this multilingual model that is pretty good but we have English this time you can choose any model from a hugging face Hub that's that's cool so you can just find your own uh domain specific model that you want to like fine tune and and evaluate further here we can Define our labels so let's say comments can be positive negative and I don't know maybe toxic uh toxic that we want to hide and now if I save it uh the data will start to import into vb8 yeah it might take a while but we don't need to wait for it so I just hope the two projects that this already has been labeled uh so actually I I to be honest I labeled this uh just part of the project that just half ago half an hour ago before this this presentation just to test it if everything is working so I labeled in half an hour like uh 600 uh it's almost 700 comments and what is really cool and what really really it is enabling me uh as I'm scrolling I'm immediately being shown like okay there is a comment there is like document and how many similar documents are already in the database and if this document has some some labels some some class uh it's very likely that the most similar ones will have the same label so in this way maybe not this one but uh in this this way we can like label all the documents at once so with one click I let's label 12 documents if I will go uh here I can see like more documents uh this is like more than an exploratory mode but if I'll go to backlog I'll be showing just the documents that are not labeled yet and for example for example here is like 60 similar documents so that's like low Henry for hanging fruit so I can label lots of documents or Advance yes they are similar but like even this can help uh to have the model more similar examples to strengthen the the um the learning so I just labeled 60 documents with with one click so this is the first part where we really we read is how pink uh me to to uh label more data more quickly uh uh then I can skip to this micro topics part which is more like an exploratory mode where uh as the data were imported uh there was also like topic modeling uh going uh underneath it and like the bubbles are basically a clusters of very similar uh documents that are similar by topic so it doesn't necessarily mean that if you are similar by topic they have the same label but it also can help me if even if they have different label uh to label them because I will have like different versions for the same label uh and again this will help the the training okay uh if I label like enough data I don't need to label everything I'll just go here models and I can train a new model so this is like really easy easy UI I can set up some Advanced features and uh Advanced uh settings but I don't need to I just click train a new model it will take few minutes to to train so we don't need to wait for it but yeah when it's trained you can see like all the evaluation parameters confusion metrics and so on uh I could even compare like this version of the model to previous model for example so I can see like okay this version is five percent uh was on this a particular uh part of the data set uh but here we are better because uh he was like two percent were were mispredicted uh and now it's now with zero so I can even like click uh if I see like here are some like here are some documents that are not correctly predicted I can click into in this box and in a new tab I will see like okay uh there is this the document and it was labeled negative but it's predicted positive so that's probably not good and like we can evaluate by it so uh and this seems to be like mislabel document so I just might fix it and uh retrain the model or not that doesn't really matter uh yeah and if I will go through all of these I just want to deploy the model into production so that's another thing that you can do right here just create a new node that is basically spun UPS a virtual machine in the cloud or you can select like self-hosted machine that means that you will deploy a Docker container and the container you don't need to do almost anything it's just like single Docker run command and every setup is uh being done here so when the node is is created I can just uh Define which model I want to deploy the pro here so I can just choose from which project and which model I want to deploy I already have this ready so this is my my latest model and yeah if I do this I can just test it uh cool so my first thing that I will usually do is like write best and the thing is that uh lots of times when I when I tried this with uh models that were trained on on devil data that data set I got this like prediction that was completely off this time it was probably right because I have this class natural but if I try this with for example agnews data set where you have this word science and technology then the prediction will be I don't know maybe science or world but just like not true because there is not such a class for it but these models are very often uh like inclined to give you an answer and that's a problem that I was also trying to solve uh and there's another another very cool feature that was enabled by bb8 so you can Define these routing rules and for every incoming like prompt you can Define uh it that it will be compared with all the data that the model was trained on and you can Define if the similarity to the data that the model was trained on is up to 80 percent 80 to 100 then like okay we can be confident enough that uh the AI the model will give you the right answer but but if it's off if it's for example in the range of uh 50 to 100 percent then I'm not that sure and I want to review the data so actually if you prompt the model the node it will create a new document in in the project so it will import data into vb8 and you can review the predictions and you can improve uh with time your model on the data that were not previously present in the data set so you don't need to collect all the data and like then figure out like which we should uh handle it's going on the Fly and you can basically continuously learn and improve your model uh and there is this last thing that is called manual you can Define like okay manual means the model and the AI doesn't know so if it doesn't meet any of these routing rules it means that it's so D so uh different than anything that the model uh have seen before that we don't want to give you an answer and that's then you can connect it with webhooks and when you actually uh correct the data or label the data in in the project uh it will be sent to your core product and like uh it will give you the answer like asynchronously uh there are also a bunch of other things you can compare it with your uh with your examples that is also very cool uh which you can help it can really help you to uh uh fix the data in the Productions sometimes if your model is uh is working is in a production and you will figure out for example I will give you a example what happened to us we deployed this into production for uh big news media company in Slovakia and uh first what we figured that the model was trained it had great metrics but we've we figured that uh when there were like three emojis concept one emojis it was giving us the same uh label which was like this is uh this is uh something that is toxic it was before we had lots of data with this throwing up emojis uh in the data set and so it was like predetermined to do this and if you would would like to fix it you would need to uh go over the data retrain the mobile but I could just like create a example uh with three throwing up emojis or I can just example like uh Pro up uh and then you can Define the similarity to this example or more examples whatever and then you can decide if the prediction is something you wanna go with manual so you don't under the model to determine because you know that the model is not working with this uh specific area of the embedding space uh pretty well yes so that was a quick quick uh demo of what I've built uh I'm happy to answer any of your questions if you have some that's really cool like and uh not even my words like Laura was like hey yeah this is pretty amazing uh to see that um I think maybe we could leave it for for like towards the end or something like hey everybody knows where to find you right uh conscious of time uh like but if you have questions or something you want to chat more about like some of the cool things how he did that like hey let's open up the conversation Let's uh let's talk on slack and that'll be pretty amazing but yeah you're right thank you for this demo was really amazing that uh it's kind of like an eye opener it's like oh the kind of cool things you can deal with it amazing thanks yeah it's really one of those those things where if you built such a versatile platform such as vv8 like you see some kind of use cases that that you expect but you also know that it's much more than a specific use case and then seeing something like that which is so far beyond of sort of what we do in on a daily basis that's so cool so inspiring I really really love this thing I I really like the sort of hybrid approach of fully automatic fully manual and something in between this reminds me of the software that I used to uh to do my taxes with which is kind of similar like you can't just trust an AI to do your taxes because if it messes up like the consequences are are massive but if it can suggest something for you and you can just say yes or no you still saved a lot of times I really like this this augmented kind of exactly and it really viviate is is uh something that we abbreviated this wouldn't be possible because if I would uh through if I should go for through all the data set and compare the incoming data with with any of the data that the model was trained on it wouldn't be possible but we deviate I can like do it and it's still I can work pretty fast super cool then let me let me give you an Outlook of what else to expect in mediate in the coming year and uh starting first with uh the word Outlook so we were thinking what should we call this is this a roadmap is this the the Outlook um what are we what are we um sort of showing here and we decided on Outlook because we do have a roadmap and the roadmap is very precise for what's coming next or the next step and it gets a bit less precise for one after that and if we're looking at entire year it gets pretty fuzzy and that is on purpose because we want to be uh fast to react to to customer demand user demand Community demand to see basically what is the features that you need but at the same time we want to give you an Outlook of what to expect to to come this year so um this is why we have the the 2023 outlook here and still of course we will have our roadmap so if you look at the roadmap you will see the next release that comes up has some things that basically overlap with this Outlook and the Outlook is kind of what to expect by the end of the Year this Outlook is made up of six specific pillars the first one is retrieval and ingestion pipelines so um an example for for this kind of category would be a request that now that we have the hybrid search API um one of the first requests that came in is like cool can I use this with q a and the answer is yes soon because right now you can't do it yet because traditionally basically the the Q a was sort of an implicit pipeline of a near-text search and then an answer extraction and we want to open this up we want to make this more flexible so in this this sort of Epic of pipelining the idea is that the user can just decide what to pipeline yes there may be predefined pipelines but you can also customize it and this could go further this could also go for ingestion for example so a recent feature request was for PDF extraction so we do have a PDF extract the text from the PDF then do a text to VEC so you have a two-step sort of ingestion pipeline anything being related to to bm25 and hybrid search would also be in this bucket then coming to the second bucket which is currently called Beyond billion scales so this is also super super important one to me uh last year was the first year that we had a billion scale data set in V8 and that was sort of the big milestone to to be able to achieve that that uh that scale but it doesn't stop there we want to continue and we want to make it better and especially a big portion of that is for example filters so filters is something that makes bb-8 unique and and powers and I guess in the use cases that we just saw from from Europe I'm sure there's lots of filtering going on there and um filtering gets more and more sort of expensive from a computational respect perspective as the data set grows and there we have we have big plans so we'll announce something soon to make filters way way faster at the billion scale and this is one of the examples but basically uh we want VBA to be even better at the very large scale um that brings me to the third pillar which also has scale in the name which is the improved Cloud operations and scaling so um big milestone reached at the end of the year in in 2022 which was the introduction of replication which is the the sort of biggest part of that cloud operations um roadmap and um it's it's sort of done in a way that it doesn't stop not because it's incomplete because but because we can always add more to it always improve it so for example one thing that we put out there um that that our architecture would enable us to do would be active active multi-dc kind of replication so you would replicate your data set across the world in different data centers if a whole data center goes down doesn't matter because you have the redundancy with the other Data Center and you have that sort of proximity to the user kind of thing um and and we put that out there and there was a lot of positive feedback already which is nice so this gives us gives us a good indication of what you as the users want and um gives us plenty of cool stuff to build in this bucket all also of course any other kind of kind of operations tooling so for example we introduced this node API which was a sort of relatively small internal request from our internal solution engineering team but that has helped a lot and helped in debugging and these kind of kind of tools that are cheap to build but have this big kind of impact they will all be in this bucket or in this pillar then the next one the fourth pillar is something that's basically completely new in this year because we've been researching in the background and research is something that that takes some time and this year we will release not one but two new Vector index types so you know sort of vv8 has been going hand in hand with hnsw for quite a while but um now you will get more options the first one is only so to speak an extension to hnsw where we introduce compression so you still use an hsw index but your vectors are compressed and that reduces your memory footprint so that's sort of a a very quick way to to lower your memory usage and then the next one which will be later this year is a fully disk based solution so we also teased that a bit um from our from our Italy trip last year in November um where we showed some some uh observability graphs with the memory uh going down quite a bit and this is um yeah this will be production ready within this year and of course there will be previews um and experimental releases so you can start playing around with it way way sooner than that the fifth bucket is clients and modules so this is um basically this is this could almost fill an entire BBN episode on its own this particular packet um but what we have in there is everything to do with the module system so for example re-ranking with cross encoders that's that's the part that also fits in the first bucket the um the search and ingestion pipelines um but if you have such a module or Sr if you have such a model in the pipeline to to re-rank it needs to come from somewhere and that's our module system um generative search that's sort of the working title for for I see Conor smiling and he can he can share way more about this than than I can but it's an area that we're looking into and that would have some some cool stuff about it um similar for for the client's ingestion pipelines um also something that we have in this episode even the the vb8 spark connector so plenty more in that category to to improve ingestion ingestion speed ingestion resilience these kind of topics which all also sort of fits in very nicely with the the replication Milestone so if the client side is resilient and now the server side is resolient then basically that means anything can happen and you will still succeed with your import which is super cool and the sixth one is probably the most important pillar which is the one that has the most open the most Greenfield kind of area is our community so you know our community roadmap which gives you um a chance to participate in our prioritization we have this this upvote feature which is so cool to see for us to to see sort of okay if if someone creates a feature request is this sort of an edge case that just a single user wants or is this something that there is a lot of demand for and then if something suddenly pops up with like 20 uploads then you know wow okay this is cool and this makes it very easy for us to to uh say like yeah let's focus our energy on that and it also makes it a bit easier to because there's only so many things that we can focus on at the same time it also makes it a bit easier to say this is why we're not focusing on this particular feature right now because look at this there's a lot of demand for for this one and um yeah super super happy that we have this roadmap and please keep submitting your your feature requests please keep upvoting your feature requests and please keep this as the third one that I find super cool keep advocating for the features that you want so we have Community users say like hey there's this feature request this is super important feature um if you think so too please upload for it and it led to it to a spiking like five upvotes which to me me is a super great sign that we have a really good sort of community involvement and really are building the kind of things that all of you out there actually need yeah thank you that's that's it for me from the the sort of sneak peek into the Outlook of course we will publish that um that's sort of why I'm not sharing anything other than just talking because it's currently uh under development will make it pretty um the content is already pretty but we also make the the presentation pretty and then we'll publish this very soon I like the promise that the the one of the big features about the roadmo is that it's going to be pretty uh no but this is really exciting like it's like um I've been with the company for like six months and so much changed and now you're talking about all these pillars uh and then like it shows us that like we have a very good idea of what we want to achieve and yet in all of this the P there is a big pillar for Community uh for great people like you right here right like say if there's something that we can do to make you more successful so that weave it can you know run faster or can perform a specific task let us know and then share it with us and talk to us because we do listen we do listen and we respond to that so I'm really glad that Ian that you made it like make a point about it it's yeah it's um it's community that makes it all tick I have to say yeah and it's really that that bi-directional kind of thing like we're we're helping users like like you're right you'll be the example for every Community you're here today because you're you're in the call um yeah we help the users like you but we also get so much back from you like you and that for for this you are actually a perfect example because so many times you've helped us debug some issues or maybe find something out that that um potentially could have been a bigger issue if we hadn't caught it soon enough thanks to you and of course we we do have our own uh QA engineering but sometimes sometimes something slips through and to get that kind of help and and um yeah really sort of helping each other not just saying like throwing it over the fence and saying okay this is there's a buck fix it but sort of helping each other going back and forth have these kind of kind of conversation that is such such a valuable thing about this this um yeah open source also for us not because it's it's not one-sided right like open source doesn't just mean that everyone gets to use it for free there's there's also plenty benefits that we get from yeah I must say that this is something that really distinguish viviate from from other tools that I've used like uh this this cooperation is really awesome with you guys thank you so much I will say really quick um when I was going to the community questions I saw and I believe also from the community called urai the Ruby encyclopedia you see it's just a lot of questions yeah events yeah nice speaking of those uh yeah sorry go ahead Sebastian no no go for it speaking about uh Community collaboration and us helping the community the community helping us back I guess that's a really good segue into the next topic because um as Etienne mentioned the the spark connector was developed by the community um shout out to Sam stalinga Sam Bean we had a podcast with him this is another example of uh of where the community the the friends at you.com we're using we V8 and they were also using spark uh to uh to develop their ETL pipelines and they wanted to be able to take that data and then populate uh deviate with it and um if he had mentioned that uh billion scale solution we actually used this was one of the ingredients that went into that uh that solution and I just wanted to talk a little bit about that give you a small demo and there's a blog posts and podcasts that we've also published around this as well um just imagine if you could share my screen here yeah absolutely so let's share your screen thank you um awesome so the Wi-Fi spark connector the concept behind this is uh pretty straightforward the idea is that if you have unstructured data uh natural language data image data and you're working with it in spark and this is critical if you're working with a a data set that's uh of any consequential size um when you're getting into the billions obviously but you're going to be working in spark whether it's to do ETL you can even train models within spark if you're just visualizing your data you're going to be working with a distributed framework like spark the main idea behind the spark the weeviate spark connector is that you can easily take your data data structures within spark and then use those to populate uh your weeviate database using using the data it's already imported into Spark and you can specify the batch size you can specify the URL for the weeviate host and if you have vectorized data you can even pass that in as a column that's in your that's in your data frame and this obviously helps with the uh with the speed of ingestion as well I wanted to show you a quick demo as well here so here I've got uh I've got this notebook let me let me see if I can zoom in a little bit here further so in order to get this running you'll need Pi spark you'll need the alleviate python client and then you can have your data ready either on Google Drive or for this example I've got it ready on disk and really a lot of this is just Pi spark functionality so you can import Pi spark and then you can start a a spark session I'm just showing this locally but usually this would be uh this would be on the cloud so I'm going to start a spark session here so that's getting started and then once that's up you can you can view it you can also use the spark UI to oops let me open it so you can use the spark UI to to look at what's going on in the background as well and then the the next step is to get your data I'm going to import it from a Json file but you can import it from a parquet file as well many other formats in which you can import it and I'm going to put it into a a panda's uh spark pandas data frame and this is very similar to the code that we actually used for the one billion import actually here I'm just showing you an example of a hundred thousand uh data points from the sphere data set but we actually use this very code for that for that job so once that data is in the spark data frame you can see what that looks like the cool thing about the sphere data set is that not only do you have the um the the Snippets for news articles but you also have the vectorized representation of those articles so we're going to use this actually to populate alleviate as well so the next step once your data is in spark you can then write it to vva and this is the novel component where the spark connector comes in right so I've got a alleviate instance up and running locally and then I'm going to create a schema that I'm going to use and then this is where the spark connector comes in we'll post a link that will that will that you can go to to look at the code behind this as well but the main idea here is that you can take your spark data frame your data structure and you can directly populate we V8 using that data structure and here you have the ability to control for batch size you can tell it where the host is located this one this option is interesting here so this actually specifies the name of the column that stores the vectorized representations for every row so if I scroll back up here you can see that the vector column here has the vectorized representations this is how we populate not just the uh not just the natural language data but also the vectorized representations into into eviate so we can run that and all of this by the way you can you can have a look at the spark UI all of this executes uh starts and executes a job on the back end there it is so all of this is uh starting and executing a job on the back end so you can monitor that as well and then once you have data in leviate then you can just you can you can query that data as you would so you can query the titles for example you can also look at the urls um before this there wasn't an easy way to get data from a spark data structure and to eviate and this uh this really helps with that process so I just wanted to talk a little bit about this if you're more interested in the details here we've got multiple blog posts around this and then we've also got the podcast with uh one of the creators of this Sam bean from you.com and then the other one being Sam's delinga amazing uh so we actually already shared some of the links uh of the things that we talk about in a description or that we will talk about and I think I'd be worth adding uh the links to the podcast that you mentioned then and I know Connor you did another one recently as well so I would definitely add those so like uh later on check out the description and the links will be there unfortunately YouTube doesn't allow us to share links directly as comments uh but we will definitely add that in the description hey Zen uh this this was pretty uh pretty amazing like I I like how simple it felt like you were like oh my data source here here's my structure just provide the options boom and then we had this separate interface to just you could watch as the data gets imported uh it's it's mind-blowing yeah exactly and anybody any any data scientist any application that you're working even on the million uh million scale 10 million scale you're going to be using a distributed framework like this and to have a connector that allows you to populate uh alleviate with that same data is just very convenient yeah amazing amazing cool thanks for this presentation so next up uh we have Connor can you share the screen as well oh yeah let me do that uh where are you here super cool so I'm so excited to be following Zane on this and talking about uh the spark connector and the Big Data uploads and how this relates to the Beer Benchmark so firstly here comes my Sebastian calls it Shameless plug uh we're just publishing our podcast with Nils rhymers uh nose rhymers is one of the creators of the beer benchmarks it's been incredible working with the cohere team on uh publishing this scheduling it with the podcast and everything with the integration with bb8 so please check out check that out it'll be super cool so I'm so excited to tell you about the we gave your benchmarks and our efforts in putting the beer benchmarks into Eva and sort of why we've done this and why it's important so oh sorry so let's start off with you know why is this important so we begin with sort of two questions for why do we want to add the beer benchmarks to eviate the first question is well how do we know that hybrid search is better than either dense Vector search or bm25 on their own and the beer benchmarks presents a set of tests where we can tell you that here's a performance of dense Vector search here's a performance of bm75 hybrid searches on the data set and then the second question that's very important is how do we VA users know that hybrid search has been implemented correctly in webiate so start off with what are the beer benchmarks beer benchmarks emphasize diversity so let's say You're Building A nutrition facts search app Financial question search app or you're searching through tweets it contains this kind of diversity of domain it's like esoteric style of query like the twit the tweets or things like this so you have this kind of diversity and that's sort of the core goal of what's captured in the beer data sets so another very important detail is the size of the data set so you see on the this 3D plot I think I've been kind of animating it in 3D adds a nice Dimension to the visualization but you see the um the MS Marco data set is the big data set with 10 million uh you know down to Natural questions is 2.6 and then you know you have Cora cqa dupe stack these are the ones say so NF Corpus is about three and a half thousand so you can you know experiment with it really quickly so a lot of these data sets are very interesting when you dig into the queries and the nuances of each of them but again the the core thing is capturing diversity so NF Corpus nutrition facts arguana are counter arguments where you're searching for uh disagreements to some claim which has all sorts of interesting implications to it FICA Financial questions try covid scientific papers about covid cqa dupe stack is like stack Overflow coding questions Cora you're looking for duplicate questions uh you know fever is fact verification Ms Marco is the Bing web search API so very interesting diversity and that's the goal of the beer benchmarks so now let's talk about adding it to Evie so there's five steps that we're going to look at how you download the data they've provided these excellent links that you can just you know W get and you have data vectorizing uploading to ev8 backups backups being the big thing here and then test so starting off you just uh they've met they've made these super easy to download so you just kind of go to this link and then you click the link and you have the data so now let's talk about vectorizing so what I did with vectorizing is I'm running this on the Google collab T4 GPU I thought the T4 might be a little more accessible for clocking the times in the a100 so you can see how long it takes to vectorize and I think kind of having this data table is is useful for people to understand like how long does it take to vectorize Big Data so I think maybe the most useful nugget is to see like 500 000 takes about an hour so you know whatever you're thinking about that's kind of a good way of thinking about it so then think about vectorizing larger scale data sets so we've done the natural questions and so what I've done is I have the Json file where you load the entire Json file and then you chunk it up once you have the Json file and so so you can do that and you vectorize each of the chunks but the key thing with parquet is parquet and Spark as Zane is mentioning that it has a better way of how you uh take just a slice of the data set to chunk it so when we're publishing the full blog post we're going to switch from the Json file to the parquet for the larger scale vectorizations so the next step is importing alleviate and so uh with the new uh batch enforce this is just insanely faster so the uh like I originally started doing this one by one and then the new python client with the uh batch Imports and dirt cool like what he's done with that is just so much faster and it's really exciting to do it so so these are kind of some of the times that we're seeing with importing to deviate you can see the the differences you see with like the difference between say Cora and then the touche 2020 data set the impact of the or uh yeah you can kind of stay with that one but with track covered you see like the impact of longer document likes and what that takes in the indexing and so I think there's a lot of this it's kind of more in the database indexing expertise rather than something I know a lot about but I think it's very interesting to just see the times two Imports we gate and see how much faster it's getting I think is incredibly exciting so then the thing that I think is adding so much value to this benchmarking these information retrieval data sets in weviate is the impact of the backups so you see this is this is this is like the 16 lines of code you would need to back up one of the beer data sets using like the command line to pass in the name of the data set and then back it up and then when we want to publish these data sets what you would need to do is you take the beer data set you put it in the backups folder the TMP slash backups is empty folder you know you have the queries for running the test and then it's just Docker compose up the restore.pi and then you have the data set sorry I accidentally put the dockercompose.yam while I'm seeing that now but I should just say Docker compose up D and then you have the backups and then you can see some of the file sizes for the backups so I think it's just super interesting the way that we can share these uh data sets that have been vectorized and loaded into alleviate and you just bam restore it and then you have this beer data set to test with then the thing with the beer benchmarks are the metrics so ndcg is an important metric because it accounts for multiple relevant documents with like multiple relevance scores so not just binary zero or one you have this relevant sub I so you know it could be like one two three four five so on so you know pretty much you just uh loop through the top ranks add the relevance and then you have the uh the log over the position that it was to penalize it for being lower in the uh ranked list and then you compare the dcg with the ideal dcg which is where you sort it based on the human labeled scores and then the normalized discounted cumulative game is just dcg over idcg so then we also looked at average hit at one where you say you know is the first result relevant document and then average recall at 100 which is uh recall is how many of the relevant documents are in the top 100 so say you have you know 10 labeled relevant documents and six of them are in the top 100 that would be sixty percent is you know in that top 100. so here's a preview of the results and the full release will be coming soon because there is one key detail that I left out which is um uh with the beer data says with the bm25 it's often to do bm25f in the Hybrid part where you also use the title so I missed this detail so it's just bm25 with the uh the content itself but we'll be adding this and updating the scores as well so these are just uh so so these are some early results of using Alpha equals 0.5 another thing we wanted to understand was the impact of the alpha so we varied it from zero being bm25 only to one being Vector search only and seeing sort of uh what is the impact as you see on the far right with the weevier query click how should you think about tuning that Alpha value so I think this benchmarking is really exciting for the continued search development of weviate as we're looking at things like the you know Transformer based cross encoders that we add as the re-ranking like what is the impact of that on these uh on this information retrieval performance how we might look at xgboost is one way to you know do this re-ranking and all sorts of things we're looking at other strategies of search like Colbert maybe having multiple dense Vector models the splayed sparse vectors and also something that's a little different that I think is extremely exciting is this sort of intent based re-ranking so now we've kind of answered these first two questions where we're trying to say you know is hybrid search better with by looking through the alphas and seeing where it's better and then uh knowing that it's been implemented correctly by checking our scores with the uh with the scientific literature around using the beer data sets another thing about some other ways that uh the beer benchmarks might be useful so the first question is can this help alleviate users choose a model so if you're working on a nutrition website your search results are probably similar to the NF Corpus search results similarly if you're answering Financial questions the fika data set might be able to tell you a lot about your app as well so we're looking at evaluating other sentence Transformers again these are vectorized with the all mini LM l6v2 and we can you know there's all sorts of text embedding models on uh hugging face and the sentence Transformers that we can Loop through we can use the open AI embeddings API the cohere embeddings API and you know give you a sense of the performance of different models when they're evaluated in this uh set of retrieval metrics so the next question I think this is super exciting and this is a super new thing like I'd say like as new as this week is how do I set up a benchmark like this for my data right so this is kind of the new idea with generative data augmentation the latest paper is called in pairs V2 there was an original in pairs and then promptegator so let's take this example so I've taken this passage from Sebastian's article on how to use the cohere uh embeddings model in weaviate so you see this passage you know multilingual model quick test and the information Sebastian's written so the prompt is please write five queries we would expect users to search for to find information contained in the following passage from the article put the article name with the subtitle you know subtitle name please make sure these queries are very specific to the information contained in the passage so you see how chat gbt can generate these queries I think the fifth one is is the most sodium one the probability is we look at say uh your eyes tool on how you can look through uh look through these labels we'll probably need to do some more parsing and some more manual cleaning these visualization tools for data set cleaning I think is a booming area and there'll be tons of use cases for these but this fifth one steps to set up a weeviate instance and import a data set with coherence multilingual model this would now be an example of a gold document for this query and we could construct the same kind of tests for anyone's data set you just rotate in your passage and it will generate these data sets like this so that you can understand the impact of bm25 and um and dense Vector search the different models the impact of the Cross encoder you know cobra when we develop that you can see the impact of all these things for yourself with your particular data and your particular application so just some final thoughts um I think this this is full stock search evaluation thing is evolving you've seen edian's a n benchmarks to show you how to tune the EF construction the hsw hyper parameters and now we're looking at the then one layer up you have the errors that are introduced by the retrieval and re-ranking kind of layer so you know you first you have the hsw errors with things so like as um mentioned the hnswpq the way you can press the vectors that might impact the recall a little bit depending on the how aggressively you can press them and things like that and so then we see these errors and matching the queries of the documents that might come from the embeddings and then further up the stack we have the question answering you know the dialogue as we've seen obviously chat gbt and summarization so we're seeing this kind of full stack search the next big topic is this in domain out of domain and zero shot generalization so beer was developed to evaluate out of domain meaning that uh the models that you're encoding the documents with in the queries they ideally they haven't been trained on the any of the data sets so I think that's kind of an interesting way to set up the benchmarks uh certainly you would imagine somebody building a building a search app as quickly as possible they want some kind of zero shot model but I think it would be nice if these data sets had some kind of time split to them and we can also evaluate this kind of impact of fine tuning and fine-tuning could be at the vectorization layer it could also be at the re-ranking layer and I think sort of in the real world the symbolic filters and the use of xgboost re-ranking is also something that's very impactful and maybe tough to capture in these benchmarks so that kind of is uh also related to the evolution of benchmarking in weba I think it's very exciting for us as we're developing new search features to be participating in the scientific benchmarking and the metrics and all that and then finally the last thing is the retrieval augmented language model training so you know as Eddie mentioned we're getting ready to make a huge push into generative search and we have some super exciting things coming out of that and the the zero thing the obvious application is retrieved and read where you retrieve context to give to chat gbt saying please ground your answer in this information and then you give it the information that came out of the vector database and I think what we're going to see is tons of people training retrieval augmented language models and I think this idea that you can restore backup from weviate and then you have that as your retrieval augmented component it has high research and these kind of things built into it I think that's incredibly exciting so thank you so much for listening I'm really excited to be publishing the full we Behavior benchmarks article and the more polished results as we have the have been 25f to the title as well and so thanks so much for watching excellent this was amazing hey Connor so when do we plan to publish the blog post is that the thing we're doing next week uh what two more weeks for that yeah okay okay I should be yeah so there's so much amazing content coming up but this yeah this is pretty amazing um thank you for this overview so uh next up we have Erica and uh Erica is going to go over some Community questions all right I had the honor of going to the community questions from the past month and I've collected three questions that I'm going to cover provide links with and then just a quick demo to answering the first question if you wouldn't mind sharing my screen I don't mind at all awesome all right so here we have the first question from Castle um they had asked if they can do exact phrase matching um because when they search for customer or beta with using the equal operator there it's returning either uh an object that has customer or beta but not exactly customer beta which is what they're looking for and um John and Martin had answered this question so I'm just going to provide a quick demo on how this can be fixed you get the exact phrase matching um so I would quickly go over to the documentation just uh so everyone can see where it is and what I'll be talking about um so how you can fix this um um oh my gosh I completely fine how you could fix this kind of uh problem with the exact phrase matching is with tokenizing on the field level rather than word um so I will go over the yes code sorry if that gave everyone a headache um all right so here I have my schema I might have done this when I was hungry because it is about pasta which is my favorite meal but as you can see here with my property meal I have tokenized it at the field bubble and what this is doing is it is taking cheese with pasta and tokenizing the sentence as one rather than breaking it out into cheese with and pasta into separate tokens um all right so I have already uploaded this and I will go over to the console which I prefer it's very nice all right you know what I also should have grabbed the query all right so I will use the like operator all right so here I have um my Valley string with cheese and I'm using the like operator and I want to find a dish that ha that has cheese in it right um but obviously since I tokenized at the field level it's expecting cheese with pasta or cheese without pasta right um but the cool thing about the like operator and it is right here under the filters documentation is you can use the asterisk so if you have one or more unknown characters it will find those objects that have keys um so if I if I run this now you have cheese with pasta cheese with rice and cheese without pasta I do not use cheese with rice um it was Sebastian who suggested maybe including hey I'm putting you out there it's weird I come to Copenhagen I'll convert you um but if I keep the asterisk because I'm I don't know if it if my object has within it but I'm going to include pasta um you can see that rice is now kicked out right because it is clear with the field token that it is pasta and then if I um just yeah obviously put rice in it is going to find that exact dish that has cheese with rice um so that is the first question on castles how to find exact matching um yeah so I hope that clears it up and also it was um I can provide a link to this demo if it helps all right the next uh question is are there any uh good performance benchmarks on wheat and the answer to that is yes and I will share with you how to find it in the documentation um so if you go to the documentation section on the website and you go to benchmarks and then a m you have WB A9 benchmarks and since we have ending here what is a better time than to ask um what should people look out for when they're reviewing this yeah yes I saw in the question that that was also about a comparative uh sort of Benchmark and uh this is something that we want to leave to to someone else because we're strong Believers and we know how to run bb8 and we're also strong Believers that competing Solutions are very good at running their solution but probably not at running bb8 and another competing solution the same goes for us so um for us it's super important to be transparent about these things and to give the user accurate information that really helps them and we can only make these accurate claims about ourselves and then not about it not about others wouldn't feel comfortable in in doing that so these benchmarks here uh Benchmark vv8 in isolation basically um but with real-life use cases so we have these these data sets I think we have three or four data sets where we chose different data sets um to have something representing a smaller data set something representing a larger data set um have a low dimensionality I think glove 25 is like the low closed captioning not available [Music] in general yeah exactly perfect in general there's some sort of a sweet spot and query performance and throughput is function of recalls so you can pick your desired recall so for example in in this thing that we have highlighted right here we picked a configuration that led to 96.56 recall and then with this specific machine you would get a QPS so queries per second of fifteen thousand which is again sort of a function of the number of CPUs but we also have the mean latency which is not dependent on the size of the the machine but that's basically just for for a single query so this gives you a number to expect and and this tells you sort of two things basically what what we can achieve with vb8 and therefore what you should be able to achieve with bb8 so um it answers the question is DV8 fast enough for my use case but it also answers the question of am I getting the kind of performance out of vb8 that I should be getting out of ev8 so if for whatever reason you would run that data set and you would not get that kind of performance they would have some sort of an indicator that maybe something can be tweaked maybe something can be optimized then maybe the kind of Hardware that you're running on is is not ideal so um the last part of the vector search is a disk lookup to to retrieve the objects maybe you're using a 20 year old Spinning Disk and one of the vector search itself is super fast that last last step all of a sudden now it takes 10 milliseconds where the entire search should only take two milliseconds or these kind of things so great great indicator um something that that Connor just mentioned right now the evolution of of benchmarks um this is a start I'm very proud of what we have here but it's only a start so some of the next steps that we we want to have is of course um the kind of yeah the beer benchmarks is an example of a benchmark that doesn't Benchmark performance or well not not performance in the sense of throughput and latency but performance in the sense of accuracy research quality that is something that we haven't published yet so that will be super cool once we have the the beer benchmarks to have something in that direction and then another thing that we'll definitely publish this year is benchmarks for filtered Vector search because right now um these are all unfiltered search which is sort of a good easy thing to compare especially also to to compare with other Solutions because unfiltered is is the easiest like if you pick another solution that also uses hnsw and you have unfiltered versus unfiltered gives you a very easy comparison as filters are introduced um they may have chosen a different way to do it the quality may be different these kind of things so that that's that's a bigger topic but it's just as interesting of course because when are you going to do an unfiltered search like most likely you're going to do some kind of filtering so that's the The Next Step that we want to have in the in the benchmarks and then um also maybe different scale so right now I think the biggest data set that we have in here is 10 million um but we were currently seeing users have data sets and hundreds of millions and in the billions so maybe also something in that direction so um yeah to to answer the question of how do the benchmarks evolve basically more benchmarks more use cases covered more more variety in the use cases covered to always answer that that question of can I use vv8 for x and am I getting the kind of performance out of V8 that I should be okay awesome thank you so much for sharing that very helpful and then all right quickly for the last question um Advent has a would like to contribute to Weeki and is curious on how to do that um let me go or switch over to my desktop um so how you can get to the contributor's guide on the website is if you go to developers and then you click on contributors guide and then what we have released in late December I believe is the good first issue on GitHub um so here you can go through these issues and just figure out what you like how you want to contribute to aviate and you can review it we also had a question like an hour ago from Andrew and he confirmed that he is watching live so Andrew I hope this answered your question and welcome to the webiate community and if you have any follow-up questions please let us know excellent excellent and I believe we shared a link to this like uh the GitHub uh first good issue or good first issue so if you want to find this link uh check in the descriptions and um yeah that's uh that's a good start thank you thank you and this is a good transition into seeing the new documentation ah yeah let's try to do that okay so I'm gonna go last uh because obviously you have to have priorities in the team uh but uh have you gone last does it doesn't necessarily mean that what I'm about to show is the less exciting and some of you will be really excited about this thing so this has been in the making for past few months and we kind of mostly kept it quiet uh and what we really were trying to do is uh uh as follows we're trying to migrate the like the Vivid IO site into like a new technology because when I joined the company it took like half an hour to 40 minutes to build and it was very difficult to kind of like make the documentation do what we wanted to achieve uh but but also like we realized that like you know it was time to refresh some of the things that we do so how about I share my screen and then give you a preview so this is at the moment like a private deal like on a deployed on Netflix uh but we are using uh docusaurus as part of our uh technology so um the whole video site is going to be basically built on docusarus which is react base which allows us to do quite a bunch of like amazing things so like our landing page will look like this you'll be able to actually sign up straight for uh our newsletter from here uh is not 100 ready ready yet but we should be uh going live with it next week um then we also have like the dartboard and the light mode I know that a lot of people are super excited about the dark mode uh and then like I'll just jump into like some examples of like our blog posts uh it's they're going to be redesigned and this is I'm something something I'm really excited about sometimes with some articles we have multiple authors so you can also share like the multiple authors that work on each of those uh and then you can see it actually looks pretty amazing uh obviously the podcasts have been redesigned slightly in terms of how it looks and by the way if you didn't know you could actually go and listen to all the podcasts on Spotify which is really really cool uh and then contributors guide like Erica mentioned are under developers contributor contributor guide and the documentation it's kind of like the busy boss here and docusaris gives us some interesting things so we could have this uh special I think they're called annotations or something where we could add like additional comments like hey this is like a note or an info uh the code examples are actually really like you could see them there's a different style so before it was like a bit of a harmonica like we're like jumping between different examples uh but in this case this is more like a different types of tops and then something that we did which you may appreciate or you might find a bit annoying but we actually decided to redesign the layout of navigation so let us know how you would you feel how you feel about it would you think but we did have like um some questions as well about it where people wanted to just like expand different bits of documentation uh so now you can do that um and then yeah we sort of like changed the whole navigation to kind of focus on how do I go about getting started how do I install what sort of various configuration references do I need to know about like Conor was showing an example of like how backups work uh like I mean using backups for deer benchmarks so you could actually read about backups here um the apis like the graphql all of it went here the rest API are kind of like bunched up together and this is not the end of the journey we will probably do some more uh changes to the documentation so any feedback would be super useful um but yeah that's kind of like the general idea of what you can expect from uh the new site uh and of course like with the VB cloud service you could go check out the pricing uh or just go straight up to try it out try it out so that's also available through here um and before this goes live if you want to play with it I probably share the link at this daily filing uh and if you want to just try to build it yourself uh it's on our GitHub site uh under the docusarus migration Branch so you could literally go and build it yourself uh this is a great thing I love about it is uh the fact that you can actually go and uh clone it and then set up the whole environment in like 10 minutes and I'm not kidding it's that easy and it's really really fast and then something else we will be adding it's uh is the well actually it's here already uh if you see anything in the documentation and you feel like oh something is wrong here or maybe something we could add if you click edit this page it actually takes you straight to GitHub where this page is so you could technically go help us through this edit this page and then create a PR uh which you know Community Based product like would love to see your PRS if you see any feedback comments or changes um so yeah that's basically all I all we have to to show and to share uh with you um I'm one thing I'm really curious about like uh give us any feedback and and ideas but I want to know like where are you all watching from there's like a whole bunch of people whether you live or are you watching later drop it in the comments because that's like that's kind of cool to see uh people coming from all over the world um but yeah I think uh that's most that we had for today and uh oh I see a question from URI about search in the docs um so it is something that we started working on uh so the idea is that we do want to add search to the documentation uh but it's not gonna go get released next week just yet um so Ken uh is working on it uh actively and that is that like once we have something that is like really ready in a good shape uh we'll be sharing search in the documentation because I agree like on one side good navigation will help you get to the right pages in a good way but sometimes you just want to run a quick search so um it's gonna come soon like it's uh it's not really just yet but it's gonna be sooner than than most of you expect I just don't want to throw a date yet uh like really close so I'm gonna be alleviate based it is yes duh you need to ask yes uh so it is going to be like a vivid instance um I mean some of the things that we try to figure out the search is uh how do you how often should we be indexing the whole documentation should we trigger it on every build uh or should we just do it periodically because like what if just somebody added like a new blog post do it and go and rebuild the whole thing Although seeing how it's not like we have five million pages and how efficient with it is I don't think it would be much of an issue to kind of like rebuild the whole index every time and it's like yeah it's like extra five seconds who cares right yeah we did it so fast could also make a very interesting case for for our users of sort of how do you deal with a bulk update without a downtime like how do you do um for example such a such a blue green kind of deployment where something like that updates in bulk but you want to have like a zero downtime switch so we could could dog food this and and create some nice content around this as well yeah totally agree totally agree uh yeah and there'll be a whole bunch of blog posts about it because I mean I think like not just blog posts but I like to have actual active tutorials so because blog posts kind of like give you a sneak peek give you an idea but you still have to figure out quite a few stats yourself uh so I'd like to actually create a full-blown tutorial kind of like hey you want to add search to your site uh be it on documentation or some other content on the app there here's how you do it like this is a promise from Sebastian like we're gonna make it happen and uh JP when you're watching it you know like I could committed you to do something fun foreign I don't think we have any more questions so we do the customary uh waving to everyone to say goodbye uh we have oh just like before that we have done watching us from San Francisco which is good uh we have Yusuf Potter from England UK and then we have Eric pullings from einhoven Netherlands so that's pretty cool it's uh it's good to see already some people so thank you all for watching and uh see you next time and I'll see you on the with its like and everywhere else Wherever You Are thank you everybody thank you thank you bye [Music] thanks for joining us you're right yes ", "type": "Video", "name": "Weaviate Air \u2013 Episode #4", "path": "", "link": "https://www.youtube.com/watch?v=wJqtzGi1FVM", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}