{"text": "A guided conversation about HNSW by Connor Shorten between Yury Malkov, Staff ML Engineer at Twitter and the co-inventor of ... \n[Music] today we have an incredible guest yuri malkov staff machine learning engineer at twitter and one of the inventors of the h sw algorithm which is one of the core drivers of the vector index that facilitates this massive information retrieval with the vva vector search engine and we also have eddie and dillocker co-founder and cto of we v8 so this should be such an incredible conversation between hearing about uh yuri's perspective as the scientist coming up with these things and building these things out and also eddien's experience of integrating these things in we've and i'm so excited to hear the trading of ideas between these two and get into all sorts of details between these approximate nearest neighbor algorithms and details of building up these vector index data structures so eddie and can you tell us about uh your experience with hsw how you came across it and the uh the the key questions that you're dying to ask yuri yeah yeah i i don't actually even remember exactly how i came across it but i do remember that when i saw it i was like wow this is this is this is exactly what we're what we need because we were in that phase of sort of uh just doing a proof of concept of vva in general like can we add value in that space is there a value for something such as a vector database which we didn't call that at that time yet because the term basically hadn't been invented yet um but yeah we were just playing around with with just brute force search just to to basically approve the concept and then we we looked into okay how can we do this at scale how can we do it more efficiently and i came across the hnsw paper and it's like wow this is this is kind of what we need and and the first i remember the first thing that i tried out was basically um can we can we incrementally update this so because for a database it's very important that you don't just build your your data set once and then you have it and then it's basically read-only um and yeah would this be something that was possible and to my surprise it actually was and not just was it possible but it seems like the entire design about how you insert data into an hnsw index revolves around the idea that you just basically use it to search it um which is really cool for for a database and then we we can get more about this later we added some more things to add filtering to it and uh deletes and these kind of things and that really made it a great uh base for for a database yeah so first of all yuri welcome and thank you very much for making that that kind of research open and so that we can build upon it and um yeah what i'd like to dive uh into is basically the first question how how did you get this idea where does it come from like what what did you do or or was there was there demand because it's i think if i see correctly the paper was first released in 2016 and even to this day we're seeing that new users are coming to vba there's a certain portion who's actively looking for vector search but there's also a person a portion of users who don't even know what that is who just see the benefits and want to get into this so thinking back six years this must have been such a novel idea was this driven by user demand for for a faster a n search or was this more of a research topic basically that interested you and how how did you get to to build this uh hello so uh yeah it has a much longer history so uh there was a predecessor of hmsw uh and uh well yeah maybe the release of hcw actually uh affected by the surge of interest so the the the the predecessor for uh hmsw was like we call it nsw or msw so it was started development developing i think in like something like 2005 so way before this time and uh yeah i should mention some people who are also working there so i work with like the idea there was that we should build the distributed search system that will scale to petabytes and it was a kind of a mix of uh of a symbolic search system and uh like a vector but mostly symbolic so the idea there was like we have objects uh so like there's of things we have lots of uh devices there so they have some properties like in the three form and uh we like make a three form queries and search there like using that and the the idea was to use small world networks uh to do that and so we had other papers uh based on that and uh like this word was pick up by leonid boitsov at some point and so he implemented it in the enema slip which is like one of the libraries that you can use for nearest neighbor research and uh when he implemented it he had also worked with some other people like nikita avril and and uh also alexander so uh there was a like it turned out there is an interest but because before that there was like very little interest so there is there was interesting from the search community that is uh like who did uh um we did algorithms for general uh metric search but uh there were i didn't see any like this much interest from uh community that did uh vector nearest neighbors church so we were targeting at the general metric spaces so and only at some point we switched to like this euclidean and because like a sign similarity spaces just because there was a demand for that but uh before that that was a complicated like graph space matrix and uh well essentially hnsw uh was a solution to uh scalability issues that we have so the previous algorithm had a poly logarithmic scalability so it was log n squared and uh so and we realized what was the reason for that so we made a fix and that was uh hmsw wow this is this is so cool hearing how it goes back even further and noticing this transition from yeah from metric space into into um cosine or dot which we basically use for for vector similarity comparison today and um that is actually something that now that you you say we are using in vva we're actually also using hmsw for um for geo search so this was because our poc that we built uh was built on um on elasticsearch at the time just for for general search and that was a feature there and then we trans when we transition is that okay we also need a solution to to do yeah just geo coordinates to basically just find what is uh what is related or not related but sort of what is the shortest distance in yeah in a circle around it and um i think at some point we just tried this out if well mention this w we already had it could we just plug in a different distance function and it worked and i kind of almost forgot about it again but it's super cool to hear that this was actually the history of it that is not just a coincidence but yeah this was was part of of um where it originally uh came from um yeah i've mentioned um the fact that we can build up an hmsw index incrementally and this is something that i i just have to ask this is this was this a design decision or was this more of a more of a coincidence um so so if you you say the history is um yeah within search already that that makes a lot of sense but did you actively try to achieve this or was this yeah basically a coincidence that that we can change it incrementally uh yeah that was a design decision so basically so so it also went from history so the previous algorithms uh that we use were kind of the same so they were based on incremental insertions but uh so like in terms of library and uh performance for there is a public library that i support hmsw leap so i didn't support features that would block uh like this this feature so the incremental insertions so and initially the idea was that we are kind of building like a peta scale databases and they're like it's a it's impossible not to build it incrementally and uh so they're also there like they were distributed there was uh like planted support like increasing the size of the uh storage like dynamically so we can add nodes to the the system so yeah that was a design decision so it should be dynamic really cool and especially that you say that you had databases in mind um because when we chose it well we knew that we wanted to build up a database but of course we didn't know that decision but we basically picked hmsw among other things because um it allowed for that incremental and as you say for a database there's there's really no other way um yeah so something that um to dive a bit deeper into into how h and sw work uh something that you have in there is this um i think in the code it's just called the heuristic two which is basically a way to to trim the connections and to keep those uh to yeah to the most valuable uh can you explain to our our viewers sort of um what it does and then also how you came up with that because i think when i read this the first time and and saw what it does i thought it was pretty genius and i wanted to to know yeah how does one come up with such a such a heuristic well uh yeah so there are also predecessors that help this come up coming up with this so basically uh so there is a a general no solution to the church in graphs in proximity graph so you have a dilenoid graph which is uh like a dual duo of uh varanoit isolation so you you like every greedy search in this graph will end up at the exact nearest neighbor so what i know it isolation is basically so you have a space you have points and uh so you have and you divide this space uh so that like each cell uh corresponds to the nearest neighbors of the uh element which is the center of the cell so it is pretty basic and uh so like if you look on theory so there are uh sub graphs of this dylanograph because well this graph has a huge problem like huge performance implications if you try to apply it to vector spaces because uh uh so well maybe yeah yeah so there is a uh like a theorem and we can prove it which was done and work on spatial approximation three 3 that if you have a general metric space so you cannot have access to all the points you don't know like what else points can be in the space uh there is no way that you can reconstruct the exact uh dyno graph from those points or or any sub graph like i mean like that the graph that has that has uh the graph as a sub graph so that will be like fully connected graph so you can come up with some examples uh with new points because that that can be arbitrary so you will have you will have to connect every point to each other which obviously doesn't scale uh you know there are sub graphs and one of the sub graphs uh well one is one near nearest neighbor graph so the the graph in which you connect to uh the first nearest neighbor so that is also always a sub graph of uh dylanograph but also there is a relative neighborhood graph uh which is also uh like a a sub graph but so there is a like a general way to extend this graph so add add few edges uh because there is like some ambiguity and how you define so if you just create a relative neighborhood graph it would not be uh it would not be searchable uh [Music] like for the points inside this graph so well basically basically you can think of the uh like this rng like extended graph or heuristic it builds the graph that's supposed to be uh searchable uh for the points inside the graph so if you if you like one of the points inside the graph uh and do greedy search it it should end up in this point so it will find it so that that that that is a reasonable heuristic and like a nice property of it is that in the one g space it uh it goes to just the list so so it translates to a simple list and and that way like hmsw was like like like converting into a skip list which is well known algorithm so this was known before actually so uh so i i i first saw like similar uh heuristics in a spatial approximation tree which also had this theorem which is a nice paper but uh uh like after i like first published the paper on archive i did the search and found that it actually was used before in 1992 in 1993 by area and mount though like i even though i had cited actually this paper in my paper i haven't noticed that they actually use it so that i was focused on something else but uh so this uh heuristic is like pretty obvious if you think about it and i know there are some extensions but the so it's they are all kind of going around it around the same solution yeah yeah i i would imagine that it becomes pretty obvious if you have that kind of a background on on what already exists and of course also if you have narrowed down the problems so to speak like if you if you um yeah if you're looking for a solution to um yeah basically keep only the most valuable edges and not keep keep so many so um yeah i think what's what's very obvious to you here um yeah i think this would have taken me or someone else just um looking at it without that that kind of research background wouldn't have been so so obvious um yeah maybe so like a good example where uh like it's one g so that was also well uh initially i saw when the rotation is w paper it didn't have this heuristic uh and uh so but it was performing also like rather well it was performing much better than the previous algorithm on some data sets but uh like it lost to uh vp3 on on a few data sets and uh so because those data sets were very low dimensional and we knew that if you just connect to nearest neighbors on one d so you will have like lots of connected components so you don't have a single connecting components it's like easy to visualize so you would need to have a list there and the so like if you target low dimensional data so that that that is kind of like uh also it's simple to understand why you should use it yeah cool um so next thing on my list something that we do with uh hnsw and this may also be something that that yeah i don't know if it was ever intended but it's something that that works great and that i was a bit surprised about that it worked so great um so one of the usps in vv8 is that you can do a filtered vector search so basically apply your your scalar filter um in the same way that you would do it in any kind of traditional search engine and then just have um yeah the vector search only on those loud items the way we do it is basically um just as this we have an inverted index inverted index basically gives you an allow list of ids and then uh when we traverse the h and sw graph basically um while we follow every connection which of course we need to do so that we we keep discovering the graph we only add to the result set those that are on the on the um on the allow list basically and that that works really well and um at first we yeah we just tried it out and and thought it was was yeah it led to the results that we expected um and the first time we actually measured it i was really surprised that the recall doesn't drop at all yes it varies a bit depending on how how more you increase the filter um but overall it it yeah it it it kept pretty constant like even if it went to to the very restrictive filters then of course it gets slower and slower because in the end it's basically a greedy search or a brute force search through the entire entire space um but the quality doesn't drop in and the point where it gets slow only comes very late so what we do in vivian is basically like if we know that the search will be this restrictive we actually just skip the index which is to approach for a search anyway and then only on those allowed ids and then of course it's faster again then then um yeah having the index search for everything and discard most of the things um yeah can you maybe also give a bit of background of why that is why the the integrity basically of the graph keeps so high or why this works or if if this is something that yeah you maybe you've thought about or used this as well or um yeah oh so yeah so it's very nice that you made it work so uh well it was kind of intended but uh we never tested it so the idea was uh that you can change your existence on the fly so the there there might be a difference between like the distance which you built the index and the which are used for querying and that is kind of the same thing so you are like changing the distance a bit uh well if there is an overlap between the like you can think of the of the if you have one metric and it has some optimal graph you have a second metric and it has somewhat different graph there is overlapping neighbors so that it should work so if there is an overlapping neighbor so you filter out those elements and the like uh like most of their this half of the neighbors are the same so you should have like it isn't performance but uh yeah so that is complicated thing so like we we discussed it and we had some previous algorithm but uh it never i don't think it ever worked properly so uh when we tried it we ended up in building like special structures to support uh the search so you have like one index match so you build the sub index for it so we try to build that and like that was pretty messy and i thought well that probably should be the optimal solution so when you like you have uh like your set of possible parameters and you partition them like group them and build separate index for them if you know that the user is likely to have like searching this set you you can speed up and build a separate index for that but that is very messy like i'm not a specialist in those kind of optim optimization so i never even tried it but it's nice to see that uh like you made it work yeah uh yeah yeah it's interesting that you mentioned the the partitioning because this is also something um that we that we have i i don't want to say we have it in yeah basically have it planned because the way that we partition so right now if you have in nvidia in a distributed setting um basically the entire index just gets charted across all the nodes that you have so basically that means each each subgraph so to speak or each graph basically just only contains a specific portion um right now this is random right now we use we use basically a hashing function based on the id and we are only the assumption that the id doesn't sort of follow any kind of pattern so it's basically the goal as of now is just to get it evenly distributed that i don't know if you have a memory limit on one of the notes that basically yeah you can distribute it among many nodes but something that we can do there is we can do that sharding based on one of those properties that we know that that would be filtered for so for example something that we see quite often is that people want to use multi-tenancy on vb8 and that the tend would then basically be the partitioning key and then if we have a filter where we said like tenant is five then all we'd have to do in this case is knowing okay ten and five lies on node three and node three basically has its own html graph and then from the perspective of h and sw on that node it's basically an unfiltered search because yeah we've already chosen the right graph which i think is the the same thing um basically a very similar thing to what you just mentioned um this is actually a perfect perfect segue to the next point um so i want to go bit into yeah right now we've talked about sort of why hmsw is the way it is and how certain things work and i'm also very interested in how where things can go from here so something that i have um on my and my list is just talking about memory in general so maybe for our viewers um the way that hmsw is designed and please correct me if i'm wrong there um it's basically that both the graph as well as the vectors are held in memory so that is just an assumption basically that's that's there which is for fast fast lookup basically both both for fast distance computation is for following the edges efficiently and this basically means that if you have massive data sets your memory requirements also grow that of course depends on the build parameters but as a very very rough estimate what we've seen is basically that if you if you roughly calculate with two times the amount of all your vectors it it always sort of sort of more or less matches a very very rough estimate exact of course yeah depends on the number of edges so if we think about billion scale vector search um that is a lot of memory that we could potentially use and something that we've seen pop up is basically to try and tackle this at what i would call the application site so something that that's been a popular release binary passage retrieval in the context of nlp so basically there the idea is that the vectors get compressed so to speak into binary vectors where each vector only holds a sub-portion of the information that the vector dimension did before in bpr it's basically just if the the vector if each element of the vector was positive then it's the one if it was negative then it's a zero so you really basically lose so if you go from a float32 vector to this binary vector you basically lose um uh 30 130 two parts or basically sorry you basically reduce it to 130 um yeah one over 32 of the original size lose that kind of information but of course also lose the the amount of memory that you need to to store and basically also increase the search speed um the downside of something uh like a binary passage retrieval is that the way that it's seen right now you also need to make your model aware of this so basically because you're outputting completely different vectors typically you would have a loss function that would optimize for both the the original both the cosine or dot product or whatever it is lost and also this this binary loss which should then typically be a hamming distance between the the two vectors so what i'm thinking is that the main topic here being compression this this kind of compression it's a in this case of course a loss full compression but might also be other ways to do this um if we do this at the application layer would there also be a chance to somehow get this or this or other uh compression techniques product quantization or something like this into h and sw itself so that basically we say the vectors as they are are just sort of they they stay the same your model is the same but at the index side we're trying to compress the vector somehow is this something that you could see compatible with hnsw or anything that maybe you've even thought about already um well yeah i i thought that that is already implemented in libraries so uh one thing like one one one solution is supported by phase so you can build the uh h and sw or i think it also now supports other graph indices or uh quantized data so there is an option for that so uh i also had a like i call first the paper uh in eccv 2018 uh which also did the quantization a and the hnsw but uh like from a different perspective that hnsw is used for uh like uh cluster selection uh in uh like in in ivf and uh like as far as i know that is still state of the art and uh like in billion scale search so you can use a much more clusters because you use like a graph index so that they're like we used one million clusters for one billion elements and then you do product quantization there and like with some tricks that like my co-authors introduced to save space and uh yeah that works pretty well so i think this solution is better than the one that is implemented in face or it was implemented i'm not sure maybe they support both because uh in graph intercessors there is a so you have an overhead on the links and uh like it's hard to compress links so and if you cluster the elements uh so that saves more space uh like if you like put them into a single giant cluster uh it loses a recall for sure but uh like it's a better way to save space than if you reduce the dimensionality of the element at least to some point but uh yeah so you can do both and well there are comparisons and like in the in the literature so people try that i think that is really great and that is cool because i didn't know about um face doing this this um yeah you know so if they got you correctly they're basically um doing the quantization on the data and then just using h and sw normally is that that yeah well yes so that that was published in 2018 i think on cbpr so since uh there there were other papers based on that uh like i think extension at least like i review papers uh for current uh ictv so i see that there are like follow-up papers coming on that so but i'm not sure like which of them were published but that that paper was definitely published and that is a part of uh like face open source nice this is definitely something that we have to check out both the implementation as well as the the papers of course um yeah on a potentially similar topic um if we're yeah another way maybe of tackling this problem is to also rely on the disk and um something that we we've seen pop-up is microsoft's disk a n for example and um there they also have the problem that it couldn't be updated so i think the the newest revision is a fresh disk a n um in general do you think hmsw the way that it is right now could suit itself to yeah storing parts of it on disk and how would that how would that look like oh yeah sure i think there were like previous papers though i'm not sure if they're like super incremental editions uh like i think also by microsoft that uh like used hybrid hardware uh so like hmsw on top and like quantization and storing and this is this in in the in the bottom so that makes sense but uh that is more complicated so you are adapting more to hardware that you need that you have so it's just like it's uh it's engineering it's hard engineering so you need to have some hardware set you need to optimize to work it properly so that totally makes sense so that can be ssg that can be like distributed storage like uh yeah but uh it's hard one cool yeah but that's it's good to know that in general because i mean hard problems to figure them out that's that's what we're here for and that's what our engineers are here for um but it's always good to know that yeah in general uh that's a direction that that um yeah that we can go um speaking of a hybrid approach the idea of building something something hybrid from a very high level perspective is also something that we've had before we're just thinking not so much going going into changing the algorithm itself but if we were to just take let's say two out of the box algorithms one that supports incremental building and one that that doesn't so for example uh if i'm now taking uh let's say google scan or something which which is not incrementally updatable and hnsw and if i try to sort of build a hybrid system um where the idea that i have in my head right now but maybe there's way better ideas would be that well we say everything that's that's continuously imported basically we want to keep in the in the fresh index so to speak in the h and sw index um and after a while we say like okay let's build a second index so assuming that the data hasn't changed anymore in that index um could we build it up into something like yeah in this case scan store that and reset basically the the fresh index and the idea of why we want to want to keep the fresh index at all in place is basically that stuff is immediately searchable so that that database you expect your import to to be immediately searchable um and then of course one of the challenges would be how do we handle deletes like what if in if we assume delete comes in in the fresh index then that would be easy because there we can still handle it before we sort of compress it to the static index um but yeah over time would probably be some sort of a some sort of a compromise of marking something as deleted and rebuilding something similar to like a compaction in um in a traditional um lsm tree database or something like this yeah i was wondering what do you think of this this idea is there something in general that you say um yeah there is a potential in this kind of things if we assume that we can use indices that have fewer memory requirements or something like this or would you say this is potentially especially as you already also mentioned a hybrid approach this is maybe the wrong approach to to combine these things maybe there's a better approach if you want to achieve the same goals well uh like as a person who worked on on the indices i think those can be yeah those can be like joined together so uh scan is using like trees as as uh as like divide and conquer uh so and it has like fast uh quantization uh scheme so i when i first saw skan i heard that was kind of cheating because uh like it uh it does not really uh [Music] decrease the computation whether it's kind of speeds them up and uh like there was n and benchmark and some of the data sets there were actually integer uh but uh like you can rewrite your uh metric function to be integer instead of fourth and everyone use fault and like you can have a speedup up so that oh they are kind of compressing the data but yeah scan actually works sometimes better uh on flow data as well so but yeah you can combine them uh there is also like for for the deletion point of view i think deletions are pretty uh costlier and graph indices so uh h9sw supports deletions so that was uh essentially it was like initially it was supported by flagging the elements which was contributed uh and then there was a contribution by adding full deletions so like that when the graph was rerouted but that is very expensive so uh one deletion is uh like equivalent to like many tens of up like insertions so when well incision is not deletion it's an update but uh if you plug an element as deleted and then updated to a new element that is like pretty much the same as deletion so a reasonable strategy it would be just to just flag the deleted elements and then when you delete like half of them you rebuild the index so your complexity does not change you have like a constant because like your you delete twice the elements and you build index once so like scaling does not change so uh i think that would be a better like from the library point of view that's like that's up to user so we cannot just rebuild the index uh like in background that should be controlled but uh like for a live like for for a service i think that makes sense so yeah yeah this is actually this is very interesting that you mentioned this because this is exactly what we're doing right now just we're not i i think we have a relatively short period that we used to to rebuilding basically so um we are doing the flagging as well and and basically then we just have an async process where the user can control when it runs but basically we do this based on time at the moment so we don't do it based on the size so probably as you say since it's basically rebuild it might be more efficient to basically wait longer or to maybe make the threshold not so much time based but yeah space we say i don't know if 25 or so of the index are deleted we do an entire rebuild so that's yeah like for from a point of writing a paper so you have like some degradation of performance like in terms of uh like uh so what would happen if if you delete the element so your performance will go up or if you add a new element uh instead so it will go down because of the new elements so there is a like if you set a threshold on performance it probably wouldn't would depend on how much what percentage of the data set you have deleted so so like for a comparison point of view so that would be like i think the optimal strategy yeah yeah that makes a lot of sense and definitely something that i think we don't have a lot of a lot of metrics yet on what the actual cost is over time because typically what we've seen in the use cases so far is the deletions are somewhat rarer than imports um but yeah there's a lot of room probably for for improving this um yeah coming up to me to my last question actually already um we've talked a bit about the the future of hmsw right now and you've also mentioned that there is of course future research um but maybe as a whole would you say hnsw is in a sense basically it's complete or would you say there are certain things that that yeah should still follow or yeah so sort of let's say five years from now ten years from now uh do you think the learnings from hsw would make it into something completely new or do you think maybe there's there's things that would be incorporated in it or maybe something completely different basically how do you see the long-term future of of hsw as a as a concept basically seriously i i hope that it will last for long so like the main idea of hnsw is that you can do like the main idea that i like that you can do a search with algorithmic complexity well at least low dimensional data uh with the graph structure so graph structure is known like to perform well and you can like have both best of both worlds so you can have like uh searching in graph and also algorithmic scalability of the trees and the like graph are probably better than trees because they don't need to backtrack so if you're using the search index like well not annoying but like similar with backtracking uh so you have you lose some performance because there is like a symmetry of the links in the graph so and well hmsw is a simple fix to get the algorithmic scalability uh well there are many ways how the like initial papers can be improved so as mentioned the heuristics might be updated in some papers uh there is also i think in some companies and some people use a different construction strategy so they add elements from the bottom like when you have a new element or they add to the bottom layer and then i convert to the higher level that ends in better performance uh also like there are works on like earlier terminating strategy uh works or like uh automatic tuning so you don't need to figure out the parameters so right now is like a big issue for like many of the libraries that users don't know how to tune the parameters and uh like there are i think only a few libraries that support automated parameter tuning and the yeah that is a nice thing so uh also there are i think there are there is a good direction on improving the graph uh with ml so right now it doesn't have any machine learning uh so it's assumed that uh every element in the graph can be like a query but there might be a query distribution which is like very different and so it is so there is a there is an application like in recommender systems uh like if you don't use inner product distance uh uh but like embed your items in some complex spaces there is a chance that some items are like they will never appear in search with inner product it can kind of tell it because it's like it's usually close to zero but if you are using something more complicated uh or you just like going better than uh l2 you cannot tell that but if you have a query set you can actually understand that uh some of them elements they are never returned you can just remove them from the graph and don't waste memory uh and like links uh on those elements uh so yeah there are yeah many ways to do that and one of the like future i thought that it was going to be distributed at some point so like when you're trying to do like trillion uh elements in the data sets so like you have to distribute it and uh it's not really clear how like what is the best practical way of doing that so initially i thought that it's going to be like you have an element and it resides on some node and you have links so it's mentioned in the paper so you can you can try to make it uh distributed because skip list was made distributed before and this is kind of like a skip list in some sense uh but that might not be the best strategy so you still need like a lot of hopes and each hope is a hope to a different node in the network so that is very costly you might also want to like aggregate those at some point yeah so everyone so i think we've just gotten quite a master class on these vector index algorithms and unless eddie has one more question i was going to try to kind of maybe parse this a little bit and give people maybe beginner people kind of a little bit because they might have gotten a little bit lost in some of the details of this but any more uh questions that ian or no no thanks i've learned a lot today um and this was super super interesting and i'm i'm definitely very interested in a couple of links to some of the papers that you've mentioned if you could provide those that would be that would be super great yeah sure yeah i've got quite the reading list too with um i think billion scale similarities search with gpus um yuri mentioned revisiting the inverted indices for billion scale approximate nearest neighbor search is a recent one building up on these ideas that we just learned about but to kind of step back a little bit i think um yeah that was definitely a master class and you two know so much about these things that i think um you did do like drill into this language and just to kind of maybe provide a little more of uh of like a introduction to people i i really liked um in the beginning eddie and mentioned the searching through geo coordinates and this was this is the first blog post i ever published on towards data science actually the first thing i ever even put out there before a youtube video is this idea of of doing a geospatial search and using say k-means clustering to have centroids that speed up compared to say doing a brute force distance calculation between your query latitude longitude and then every latitude longitude in america so you do these or wherever you're searching so you do these k-mean centroids and i think that's a great example for people out there listening to build up an intuition of what we're talking about is you uh you structure these centroids and then you can propagate them up and as we talk about these um different ways of say having a small world graph you have this property that small world graphs describes this idea that i'm say six people away from knowing barack obama through my relationships and that's this network science thing that that also behaves nicely with these um nearest neighbor distances and this idea of vector distances one topic that was talked about that i think is extremely interesting is this generalization from you know one dimensional distance we can use just a binary search tree then two-dimensional distance we get into that geo-coordinate search and then you know n-dimensionals we're talking about uh these different ideas like say binary passage retrieval that edian mentioned where we have the binary encoding to speed up having to put these massive vectors into the memory and then things like one other thing i wanted to talk about and ask about was uh the locality sensitive hashing algorithm where say you have a you know ten thousand dimensional vector and you just kind of uh hit like i think it's like fifty uh a slice of scientific the dimensions to do that kind of um distance calculation what do you think about uh locality sensitive hashing as a way of computing the distance uh well locality sensitive hashing is not learnable so it doesn't have any learnable component so uh that that that's why i i thought it was not practical so there are lots of like papers and theory so because you don't have any like dependence on data uh in terms of the projections so like you can build some theory and like have nice theoretical papers about it but uh like i never saw it like it to be like really practical and to outperform like optimized uh tribals so and i thought that uh like people who do it they also see like uh like quantization based algorithms uh like as better because they are learnable like they can adapt to the uh the the data set that you have so could we also take a little uh give a little more background about product quantization is product quantization basically the idea of you have your pre-trained embeddings and then you learn a linear mapping into a lower dimensional space from that pre-trained embedding or can you give me just more understanding of how product quantization works well i'm not a big specialist in product quantization but essentially what it does in the simplest form so you have a vector and you well you want to do quantization and uh well you can do like just quantization of each dimension uh but you will lose some information and what people do they uh well or on the other end you can do quantization uh in terms of k means so you have your data you build k means and now your quantize it but there is something in between and that is product quantization so you have your vector which is subdivided by parts and you do k means in each of the parts and now you have like a sub vector and uh like it is addressed by a k means cluster in it so and that works well pretty reasonable in many cases yeah i want to ask um one more uh uh definition what are uh voronoi regions uh what voronoi regions are so like you divide your space into cells uh which are closer to some point of your uh database like so like you have a vector space you have points and so you divide just the space around those points so that uh like every point in the cell is is has this element as the closest so basically you divide the space but by what are the nearest neighbors of those elements so as i was listening to the explanation of adding symbolic filters into h sw which to me is one of the most unique features that are that has been implemented in wev8 and i think it's such an interesting part of this could you tell me more about how this works i think i i'm sorry i got a little lost in the details of of when you were explaining it originally uh well you should ask 18 years old yeah so basically it's it it's really simple from from what we do um because essentially we still traverse the the entire graph so if you you think of the entire graph as being your your full data set um essentially what we do is just if you come across let's say there are no restrictions then you come across your your basically your exit conditions at some points you know this is my result set all we do is basically saying well if you have something that's on your result set that's not in our allow list we simply skip it and then that of course means you need to find something else so basically you keep traversing that that that graph space and and one of the reasons why hmsw is efficient in um in the first place is basically because you don't have to traverse the entire graph so by basically filtering something you're saying like yes i'm removing or not i'm removing them and you still have to traverse them but i'm basically ignoring i think is the better term some of those some of those elements and yeah the the payoff basically is that i need to traverse a couple of more edges and find other other verse hall sets but in the end basically you can still find them i think this is the part that that um yeah maybe i can't explain so much but that we just tried out and that we were surprised by that the recall was still there that that basically this this um yeah integrity is lost the way that we use the inverted index basically it's just a tool in front like like once we start traversing the h and sw index basically we just need to know which elements do we accept and which we don't and we happen to use an inverted index but basically this is this like the agent's w graph at that point it doesn't know where the the data came from basically it's just a list of of ids that we allow versus lists that we that we skip awesome so there's so much information behind this and i think it's extremely interesting and i wanted to kind of transition into this topic uh yuri i saw your paper um titled cnn with large memory layers and i was thinking about memory augmented agents and this idea that you can use these vector structures to have a really massive memory and i was first introduced to locality sensitive hashing in the paper the reformer that was one of the first papers presenting an efficient transformer model that could have a longer input window than 512 tokens so i was wondering if you had any thoughts on how we can use these kinds of ideas from vector indexing into say going beyond 512 tokens and i was also thinking that kind of generally even if you have a you know a slow inference time with say a memory that a model that's producing something like abstractive summarization over scientific literature that you know that kind of trade-off of slow inference but for the artifact of an abstract of summarization of say papers about vector indexes would be kind of worth it so i was thinking about if you had any ideas on how you could maybe put h and sw into say efficient transformers and that kind of idea uh well um so you can you use an indices and uh so therefore works i think there was a like lately deep mind retro and uh before that there were other papers uh on uh like supplementing the uh neural networks with databases so i think that is a very promising direction and uh well that is very similar to graph learning itself so you add some context and well you can think of that and an index can provide uh like uh like what what does it provide it provides the nearest neighbor graph so and uh like if you treat uh like you have you have a document you find nearest neighbors and you put some information and in the transformer there you can say that is the typical graph learning is just you have an additional input like from k n graph and uh like those approaches like h and s w they are also built on graphs so uh you can you can just uh use this graph as input and hopefully like what i like i would hope to see is that the transformers would learn to route on this graph so and uh they wouldn't be able to do a nearest neighbor's charge uh like what they actually need because when you just add nearest neighbors like uh like in retro and other there is no guarantee that those uh neighbors are actually the best information that you can supply uh for the network like for for the neural network there might be some elements around and if you allow the network to route like select like on which point of the graph you should go it it might it might perform better well it should perform better like my idea that like nearest neighbors are not very meaningful because sorry because we can treat the query the query value the query key value projections and attention as kind of like a vector search also right and we could so we could kind of blow up the size of the intermediate query key value matrices and then use these vector searches to it i think it would slow it down a lot but then you could have you know a lot of information like a mixture of experts okay okay so uh yeah i think i uh i know i i understood the question so uh yeah you can like uh do something like reformer so reformers you use lsh uh well you can use also other indices so so here lsa which is good because it is fast so you don't need to build an index for that uh but like maybe well uh like well you would probably want to build an index here because you have a different token distribution from sample to sample so the like an optimal index would be also different i thought there was a follow up uh on this uh reformer where there was like clustered so instead of lsh there were clusters to project which is like just a better version of lsh and there uh like i'm not very sure like where hmsw could fit in uh because uh so you will need to build it first so that's a like an engineering trade-off like whether you should do brutal force or like so it's no it's not it's not very clear so like you can so because it's an engineering uh it is an engineering problem like in many cases you can't just uh go away like with a go with lsh it will be slower uh yeah it will be less accurate but if you like increase the size so it might be fine uh so it is it is hard to like to really predict whether it should it it should be used there but uh for the documents i think like uh like if you have lots of documents and that those are like very different and uh they can reside so they don't change over time so you can pre-build an index for them like a graph so that that would make much more sense yeah i guess i was thinking about kind of uh like these contrastive learning methods where you have maybe an exponential moving average of one copy of the weights and maybe you would cache that and build up the index of those activations maybe some kind of pipeline like that and then i was also well uh yeah sorry so uh like when you do contrastive learning uh so usually the nearest neighbor search like unless you have like a really huge data like really huge data set is very fast because you do it on gpu and uh like you can easily search like within i don't know 10 million vectors like what what is the the closest uh elements because transformers or other networks that are run to produce the loader much slower so uh there i don't think like you should use them but uh if you want to go like to billion scale so like you have like a cache and uh that that that that would be a uh that would be meaningful have you ever thought about using this these vector search algorithms to search for similar activation pathways in very large neural networks oh well yes i did so it's a like very handy techniques to find like the problems with data sets so like like for images for instance if you have uh like if you if you have some like badly classified example so you can look for similar activations in the data set so and uh yeah i used that for cleaning data sets some some time ago and uh yeah but that that that usually works at least for images yeah i love that application of wev8 as a tool for uh doing de-duplication of say language modeling data and and uh they have the uh the merge variable as no eddie knows all about that will merge the nearby things and i love that application of it for um for d uh data deduplication so um one kind of meta question i wanted to ask is about um you know billion scale similarities surge just to build more intuition could you tell us about uh like the application of billion scale search that inspires you and you know keeps you working on these problems well uh so there are many applications like i now work in twitter so and twitter there are like billions of uh tweets that you can show to user uh like if you are doing out of network uh like candidates so the other there are papers so well there are like also uh like well there are also text so like uh if you want to build a really huge language model so right now you can use a lot of text and uh like also like i can uh mention the like they give my paper so uh like it depends on a n and uh like like i can easily imagine billion scale there so we have like a billion billion documents so that is like web scale well even like less than web scale i think last web scale is uh like 10 to 15 elements like or more okay cool cool eddie do you have anything to um wrap it up with no other than to to say thank you very much this is really insightful and i mean it's it's always nice to to hear the the minds and that's behind these kind of algorithms that you know but it's also so cool to yeah get this this additional insight of both sort of in both directions like both in the history of what happened before and i mean i knew that agents w was based on nsw but basically that's where where my history part ended and and yes you made as you told us there's there's it's building on so much more and of course always research is building up on other race version this is just very cool to see how this these things come together um that we are here at this point and and also with the same in the other direction to see what's still possible like uh potential integrations of something like like product quantization into into hmsw and yeah all these these other kind of things so um super cool very very insightful for us i hope it was also interesting for the for our listeners when we when we go very deep into those topics but i i enjoyed it very much so thank you for my side yeah thank you that was uh enjoy like and a nice like an enjoyable talk to me as well so yeah thank you thank you thank you so much eddie and yuri and to our listeners yeah such a master class in the depth of these topics and you know i'm really impressed with how much you guys have explored these vector indexing techniques and i'm excited to be learning more about it as well so thank you so much for watching the eighth episode of the we vva podcast more to come on you know developing our understanding of vector indexing and these things that are available in wevg8 which i think is such an exciting part of we've eight that you can't overlook is the implementation of these vector indexes that let you do very large scale similarity searches [Music] ", "type": "Video", "name": "weaviate_podcast_10__yury_malkov_and_etienne_dilocker_about_hnsw_in_vector_search_and_weaviate", "path": "", "link": "https://www.youtube.com/watch?v=WijYx9_Bpkw", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}