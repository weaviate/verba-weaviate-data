{"text": "Thank you so much for watching the 26th episode of the Weaviate Podcast! This is another really special episode! Jonathan ... \nhey everyone thank you so much forchecking out the weave podcast I'm superexcited to welcome Jonathan Franco backto the weba podcast Jonathan is one ofthe co-founders of Mosaic ml a companythat is just doing amazing work in thespace of making deep learning trainingmore efficient faster cheaper andthey've recently produced an estimate ofcosts for language GPT language modellanguage model costs across all thesedifferent sizes and price points andjust overall helping people get a betterunderstanding of what it takes to haveyour own large language model and I'vebeen so interested in large languagemodels and thinking particularly withweave and the role that they might havein search so I'm so excited to welcomeJonathan back to the podcast and talkabout what he's been up to at mosaic mlthank you so much for having me it'salways a pleasure to get to talk to youand I can't wait to talk about this newprojectawesome so yeah could we dive right intoit um so what what's the latest updatewith um the Mosaic ml cloud and traininglarge language modelsdefinitely so you know just to give youa sense of what we do at mosaic verybriefly you know our goal is to trainmodels efficiently just in generalthat's you know that's what we want todo for everybody and there are a lot ofways of making it possible to train amodel efficiently and obviously a lot ofmodels that people want to trainefficiently so I think the last time wechatted we had just released composerwhich is our our open source librarywith all of our efficiency methods builtin and I think we had shortly thereafterreleased our recipe where we gotten likea 7x speed up in training resonate 50 onimagenet over the standard Nvidiabaselines which you know I'm kind ofjealous I'm not seeing a bunch of peopledoing lottery ticket experiments Ihumanly couldn't do because it wastaking too long to train and now peopleare able to do it overnight and I'msuper jealous that people get to do thatwork nowum but our latest release is for largelanguage models for GPT 3 Type modelsspecifically and you know it's been alot of work on our part but we've puttogether our own software stack fortraining these models we've put togetherunderneath that in the orchestrationstack which is part of our Mosaic Cloudthat I'm sure we'll talk about a littlebit and the end result is we put outsome price you want to train gpt3 todayyou know you can call me today it's 450000 to get to gpt3 Qualityum we can talk more about where thatnumber comes from and how it compares toother numbers but I'll tell everyonethat's the starting pointum you know that's the Baseline to beatand first of all if you beat thatBaseline let me know we'd love to youknow obviously make it better but secondof all our goal at mosaic is going to beto drive that cost down as close to zeroas possible over the next while I've setthe goal informally to the team ofgetting to 100K sometime the next fewmonths I think it's eminently possiblegiven the kinds of speedups we've gottenelsewherewell so can we get into this stack uhsee things like the uh distributed dataparallelism uh the chinchilla theoptimal compute laws uh kind of likewhat goes into the package of uh thelanguage model training yeah so they'rereallythree big ingredientsum I mean the first ingredient to becompletely honest is we put a number outthere this is kind of ingredient zero Ithink a lot of the numbers you've seenpreviously for gpt3 have just beenoverblown to be completely Frank I thinkour number looks very impressive basedon the insane estimate some people haveput out but those estimates just don'thave really any basis in realityum you know this is a good number butit's it's an honest number and I thinkanybody else who sat down and tried todo this today would get to the samenumberso you know item zero is just trying tobe as direct and clear and honest aspossible with the numbersum you don't want to scare anybody offfrom trying this quick to the contrary Ithink everybody should be training theirown models here there's no reason whyyou should rent the model when you canbuy itum but you know there are really threebig technical ingredients that go intothis I think the first is the chinchillascaling laws so for those who aren'tfamiliar you know first of all we havethese neural network scaling laws whichare kind ofI feel like scaling law is a very strongword these are our beliefs about theright amount of compute to use to trainon a certain amount of data in order toget a model that you know is the bestpossible model that you could get thereif you have a or you know if you look ata different way if you have a certainbudget of computehow should you spend it how much data doyou need to really make the most of thatcompute in order to get the bestpossible language model whatarchitecture should you even trainum there's nothing fundamental ornatural about this this is probably ahuman description of a phenomenon thatmay be more sophisticated and morecomplicated and I could talk your hairoff about you know how much we shouldtrust scaling loss this is to be fairone of the only places in deep learningwhere we have a scaling law on this GPTstyle language modeling you'd never hearabout scaling loss for Bert for exampleum so you know we this may not workeverywhere but at least here we can kindof make these guesses and what happenedwas a couple years ago the folks atopenai um Jared Kaplan and collaboratorscame out with a paper proposing somescaling laws and it was awesome it youknow it was the basis for predictionsabout what size gpt3 should be theproblem was actually that it was alittle bit offum and as soon as you extrapolate tobigger and bigger models and this is youknow keep in mind we're dealing with yousee these nice linear plots of scale ona log log axis so being off by a littleis being off by a tonand so especially at these larger scaleslike gbt3we the predictions just weren't you knowweren't as accurate as they could bewhich is totally expected when you'retraining on small models and trying toextrapolate if you're off by a littleyou're off by a lot and the folks atdeepmind went through and basically fitthe scaling well a little bit better andfit it with more data with more computeI mean years later so they had many moreresources it's no flaw at the Kaplanwork it's really just you know it's likewe developed the James Webb Telescopeand now we can observe Stars better thanwe could with the Hubble telescopeand what they found was that for theamount of data gpt3 was trained onthe model was way too bigit just didn't have to be 175 billionparameters it could have been closer to33 billionand that dramatically reduces the costof doing thisit's a huge difference because the costincreases quadratically as you get tothese bigger models because biggermodels you need more data as well and soyou know you can get away with doingthis much cheaper that wayso that's why the um in the cloud Ithink it's like 60 billion parameters isthe 450 000 mark because 60 billion isjust as good as the 175 billion in termsof performance and that kind of thingit's actually 30 billion which is to mekind of mind-blowing like we're droppingthe size by you know to 20 of what itwas before like when it comes toefficiency that's about as good as itgets so you know I don't think that'sthe end of the story though to becompletely Frankwe're seeing so so much interesting workon data pruning and curricula and youknow questions of whether we you knowhow we should change what the attentionoperation looks like in the paper we sawthis interesting parallel attention thatcame out we've got flash attention likethere's just so many new ideas floatingin the space now about how to trainthese models effectively that you knowwe got 5x from just measuring somethinga little bit betterhow many other five X's are thereso there's probably a lot more hereBeyond this but you know for anyonewho's doing research right now andreally wants to get into this stuff Ithink the sky's the limit on makingthese large language models moreefficient this was just kind of the lowhanging fruitthat's super interesting can we diveinto what makes it more efficientobviously you're the king of sparsitywith the lottery ticket hypothesis andyou know there's things like sparseattention uh I learned about say the AliB attention when I was first goingthrough composer uh what are the youknow efficiency things that have goneinto the uh gbt3 training from Mosaic umnothingthat's the honest answer is that we'reusing flash attentionum from our colleagues at Stanford butotherwisewe haven't even put in our speed upmethods yet this is I mentioned beforethis is our Baselinethis is you know this is resnet 50 fromthe Nvidia examples this is the way thatI think about it this is the startingpoint for making things more efficientthe first thing you have to do is standup the stack and compute what yourBaseline costs and the answer iswow the Baseline is already a lotcheaper than we thought thanks to thethe work of the chinchilla folks andsome other things we'll mentionum but this is just our starting pointat mosaic and I expect you'll see in afew months from us that number droppedprecipitouslyum and keep in mind this is the numberthat allows us to you know have ahealthy business as wellum so there's a lot of room for us tokeep pushing down that numberwell and uh so I read another quote inthe article around something around umthe vision of training millions millionsof models compared to say the zero shotgeneralization kind of way of thinkinguh can you tell me more about yourvision for uh customized models say it'slike you know biomedical domainFinancial domain legal domain uh do yousee that being like how has yourexperience with Mosaic been on findingall these specific use cases and thenhelping people say uh you know convincethem what a language model is why it'sworth it and then how to say preparetheir data like what's that experiencebeen of getting people to train theirown language modeldefinitely so I think this is and thisis honestly something on the wavy sideI'd like to chat about as wellum people have a lot of data and youknow that sounds like a dumb thing tosay but honestly there wouldn't beweeviate if people didn't have a lot ofdata that they need to organize in someway and I think you can it's the sameargument at mosaic that I've yet to meeta potential customer who wasn't sittingon a treasure Trove of unlabeled data intheir particular domainand the question is really wellshouldn't you be able to take advantageof that or to put it a different wayright now we're asking or GPT to dotransfer learning essentially we takebirth we pre-train it on you know GoogleBooks and we pre-trained it on C4 andthen the hope is well you fine-tune iton your data and then it'll transferwhy should we even bother to transferlike don't you want to pre-train yourmodel on in domain data you may not haveit labeled but one big hypothesis wehave in Mosaic and we're seeing thisborne out in practice with customersright now is that you should pre-trainon your own data and in factpre-training on your own datamakes it much easier to get betterperformance because you're in domainyou don't have to force the model to goacross domains and across tasksso it sounds very obvious like why dotransfer learning if we don't have tobut I think in the academic literaturewe think of burnt we think it burnt twoways one is you know what do you do withall this unlabeled data you have but theother is that we somehow put theconstraint on ourselves that Burke hasto be genericthat you want to have given that it's soexpensive to free train your own bird wehave to make a birth that anyone can usefor anythingumwe're successful at mosaic and we pushdown the cost of Bert pre-training tothe point where it's not expensive andI'll argue to you it's actually not veryexpensive I think we've pushed down thecost of pre-training to the point whereit costs what image training used tocost so for those academics out therewho want to train on purp I think it'stoo expensive it's not you know reachout we haven't released a recipe yet butyou should expect that next month forbirthbut you know you don't need a genericbird if you want to build a bird foryour one Downstream task or your twoDownstream tasks and in that case youreally do want a free train on your owndata in fact you want to do even morethan that you want a tokenizer that wasbuilt using your data set because youknow the tokens that were used for C4are on Reddit or for Twitter or you knowthey have lots of emojis they have lotsof words or characters they may not showup in your data you may have charactersfrom a language that isn't included inyour data that are wasting yourvocabularyand so everything's in the tokenizerit's what you pre-train on you can builda domain specific or domain-specific TPTand I think that's pretty exciting likeif you have medical data you don't wanta model that's pre-trained on Reddit asyour basis you I don't think you want touse an API where you're just wearing amodel that was pre-trained on Redditwhat kind of medical data are yougettingum why not free train on PubMedum if you're working in a legal contextagain do you want legal advice fromReddit or do you want legal advice fromwhatever internal data you said you haveor from the actual laws from variouscountries if you've downloaded thoselaws for some reason in a particulardomain so I think over and over and overagain or you know even further field ifyou're working on code or proteinsyou probably don't want to start with C4it probably makes a lot more sense tostart on GitHubyeah that's super interesting and Ithought I thought a lot about in searchhow you know a lot of the the bm25 thekeyword scoring is so popular because ithelps for those specific words as say uhyou know say you're training it onMosaic ml slack and you want a languagemodel to talk to you about Mosaic ml youknow the word composer has never beenseen in Wikipedia so then these keywordscoring can help you adapt to thatcompared to this but um so kind of in mythinking around language models andlarge models I guess the reason I alwaysthought that Reddit and Wikipedia wereso popular is because it's it like helpsthe overfitting sort of with you know abillion parameter model is predictingthe mass token if you only have uh youknow 300 paragraphs I always thought itwould overfit to that is that kind oflike the regularization techniques helpovercome that or what you're thinkingaround oh I think this all comes down todata volumeso the assumption is made that peopledon't have enough datapeople have tons of data oh my god likepeople have hundreds of billions oftokens of language data on their domaininternallythink of a company that runs like a Idon't know a chat service for customerservice agents or something like thatcan you imagine how much in-domain datathey haveum everybody has tons of data it's beenshocking to me how much data is floatingaround so I think the important part torecognize is like C4s around a trilliontokensbut plenty of people have around atrillion tokens of data or more theamount of data that I've heard peoplehave it's just like it's astounding howmuch unlabeled data labeling labeling isexpensive you want to do it as best youcan you need it for the downstream tasksbut how do you leverage all that otherunlabeled datatraining gpt3 train a bird train youknow a Sinclair model on your data anduse that as your starting point not animagenet pre-trained SIM clear not a youknow a C4 pre-trained GPTlike I thinkwe have plenty of in domain data it maynot be in the public sphere if youwanted to go and find a data set of 200billion tokens of legal data that's hardbut a company that specializes in doinglegal work probably has that in spadesyeah that's incredible and I and likethe language modeling algorithm is sucha beautiful self-supervised you knowpredict the master token it's very easyto just drop your text in and I think alot of and you know simclear also withyou have images you can just augment theimages and then positive pair and thenwe have some good heuristics I wasrecently looking at this uh paper calledspider from Ori ROM and we had him onthe podcast a way to similarly do thepositive negative bootstrapping uh kindof one other thing I was curious aboutwith these laws like say the argumentfor Wikipedia for Reddit is thepre-training corpuses I always kind ofthought like this prompting uh like theway that you can sort of template theinput was a result of it of it's seeingthis kind of thing on the internet solike you know it's because it's modelingHTMLif you do like image source equals orlike I don't know how you prompted butlike like prompting it with like youknow how they'll like do like the titlefor an article by having title HTML tagsand then that's where it generates somaybe do you have any thoughts aboutkind of prompting and how that might bedifferent when you're doing it for aspecific data setdefinitely I think it again comes downto transfer learning in some senseprompting is not transfer learningyou're training the model to makepredictions on what comes next forspecific tasks or in this case you knowa lot of different things and thenprompting is just asking it to do moreof thatum if you want to prompt your model forsomething that is like what's in yourdata set if you're you knowdoing medical work and you have a dataset of medical facts and you want toprompt it for medical diagnosisinformation if it's in domain it's indomain and so you don't need toyou know you don't really need to worryabout thatum and I think you know the happy mediummay eventually be a mixone of the things we're doing a lot ofplaying with right now is mixing youknow open source data Plus customer dataand seeing whether like what the RightMix is should you like amplify thecustomer data more to kind of balanceout what's happening but you know maybeit is good to have a little bit ofopen-endedness like you know if you havea medical data set it's not going to beable to write your HTML for you you'recompletely right like that I would beshocked if that happens here in amedical context maybe you don't caremaybe you care a little bit about themodel's ability to just kind ofgeneralize a little bit and then it's amatter of mixing the data properly ordoing the pre-training at a specificorder I think these are things that likethere's paper don't stop pre-trainingthat like looks at this a little bit butI don't know if we have the tools inAcademia right now to really look atthis closely maybe something like thepile which does have a bunch of separatedomains would give us some ability butI'm not even sure we know how to measuresuccess like measuring crafting is hardand I you know their their harnesses outthere there are benchmarks out there butI don't know how good they are right nowso we're kind of in this tricky placewhereI'm not entirely surehow we measure and how we make progressin this front scientific we need thebenchmarksand you know this is kind of a call toaction for the academic world and peoplewho may be listening this is a greatproject to do you don't need to have amillion dollars in compute in fact youdon't need to have any compute in orderto play around with a little bit ofprompting on different kinds of datasets and building benchmarksum I you you mentioned one point that Ithought was incredibly interesting thisidea of mixing open source data within-domain dataum can you tell me a little more aboutthe approach that maybe you would useyour in domain data as a query and thenget like the top 10 nearest NeighborsFrom the pile and then you know thenblow it up 10x with that kind ofthinking what's been that kind ofstrategy maybe I mean as a scientist inme my reaction is always what's thedumbest thing you can do firstfor me the dumbest thing is I don't knowyou've got 100 million tokens if you'rea Dano we've got C4um C4 is a trillion token so if we wereto just sample from both the data setscombined your data would beunderrepresented let's you know oversample your data by 5 or 10x and kind ofmake it 50 50. that would be the dumbfirst thing I'd try and the other thingyou could try is doing stage freetraining like pre-train on C4 for awhile and then free train on your data Idon't know if catastrophic forgettingwould kick in this is all new scienceum again for any people listening whoare trying to write a paper right nowthere's so many questions oh my God Iwish I were an Academia at the moment tochase after these questions there's somuch great science to doum but then you can get into questionsof like how do you figure out similaritybetween things this is where honestlyhaving some kind of if only there were aservice that would let you find thesimilarity between different kinds ofobjects in a semantic wayum you know figuring out what thatsimilarity is and being able to use itso it's we have so many tools at ourdisposal right now there are a lot ofopportunities for us to to play aroundwith these things and really the livinglimiting factor there does become costbut you'll probably see a lot of thesame effects in small scale that youwill in large scale so you can studythis at 100 million parameters and thentest it at 100 billion parameters andsee what happenssuper cool and um so I want to kind ofget into the applications of largelanguage models I think uh so like kindof transfer learning for some other taskis a very popular one say few shotlearning and then you've recentlytouched on a really interesting ideawhich is language models as databasesyou you know query the language modeldirectly you can tell me more about yourthoughts on that idea yeah I think thisis kind ofI I don't think it's too early to callit an emerging consensus but I wouldcall it an emerging zeitgeistthat these language models are reallykind of shims or interfaces you can kindof honestly look at it two ways languagemodels are databases in and ofthemselves you can query a languagemodel for the facts that it hascontained in it um you know who was thefirst president of the United Stateslike you know the language model we'llsay that George Washington maybe youneed to prompt it a little bit withexamples of how to take question andreceive answer you've seen all this workon Chain of Thought kind of same idealike how do you get information out ofthe model how do you make the model domathin some sense this is all about themodel having information that it'sfiguring out how to relate or connect ininteresting emergent fuzzy ways thatwouldn't have been possible in arelational database the other way is tohave the model literally interface withdata or with some kind of data sourceand I mean it's an example of that inthat sense my other favorite example isthe folks in Adept AI who are buildingmodels that know how to interface withvarious web sourcesum you saw this paper from a few oppressrecently where he had a language modelset up such that you know it would ask aquestion it would try to deduce someinformation by asking question you knowit would provide a question and then youknow it would either provide the answeritself or a fear to this cool thingwhere he would just call out to Googleand put on the answer from Google andthen have it ask follow-up question andthen you know just keep using thatinformation but it was queering toGoogleit was using a data source and then Ithink there was an example I saw onTwitter today of having a model likewrite IPythonqueries that would use the Wikipediapackage to query Wikipedia forinformation and then use that as inputso we're getting to this reallyinteresting place where the model itselfknows things and the model can interactwith data sources in and of itself Ithink these are really two separatethings and I do Wonder from the futurewe'll train them differently maybe thisis what the folks at Adept are alreadydoing but if you want the model to doretrieval and interact with data sourcesyou'll train it completely differentlyfrom how you train it if you want it toknow things in and of itselfbut I do wonder whether one coolapplication may belet's suppose you have a big sequelrelational databaseyou just train a language model on thatcontentand have that alongside the database andyou know develop ways to query thatmodel for kinds of relationships orkinds of information that are in yourdatabase but that you might not be ableto suss out from just relations orwriting SQL queries it'll find newrelations I mean this is what wevia isin some sense and in many ways this isturbocharging it by literally trainingthe model with the datain addition to that so I think theapplications are endless in that respectI really want to go back to self ask andI you know seeing your endorsement forthe paper and then I checked it out andI just thought it was amazing and but Ido I want to stand this idea a littlemore so is this the idea of say you youknow you you take tabular data and youtranslate it into text like just by kindof parsing it like I read this papercalled language interface fine tuningwhere it's like you know if and then thefeature name equals the feature valuelike you have this template to parsetabular into text is that kind of whatthe thinking of maybe we move all ourdata into this text interface for thelanguage models yeah I mean that wouldbe again you you know me I only do dumbthingsum that would be the first dumb thing Itried what if you just literally takethe row of the databaseformat it basically treat it as asentence like put a little symbolbetween each of the items in thedatabase maybe give it the field namelike put it into a little record formatand then those are the sentences youtrain the language model on I wonderwhat would happenI wonder if that would be enough given asufficient amount of data for you to beable to query the model maybe you doneed to pre-train it first on somethingthat looks like C4and then you train this but really thequestion is just how do you shelve thedata into the modeland at that point maybe a new relationsshow upmaybe the model has some preconceivednotions if you pre-trained it on C4um but really at the end of the day thequestion becomes in my mind don't youwant your data and your database in somesenseI also wonder whether you could have alanguage model that learns to do SQLqueries on a database to extract dataout of that the same way we're doingwith Google or Wikipedia right now youcould kind of go either way but theimportant part is allowing the model tofind these new emergent softrelationships between data like you knowI'm totally convinced now of theweeviate mission in that sense like thisisyou know there's so much we can unlockfrom our data that we couldn't before byjust letting the model figure out howthings are connected in ways that arehard for us to write downyeah it's super interesting and um sowith this idea of things that are hardto write down I want to come back to theself-ask thing and um Chain of Thoughtprompting and you know this general ideaof language models with tool use I'veseen that phrase like tool you set Ithink that's a pretty nice one uh whereit's like say the language model uhqueries a calculator or it writes somecode and then it runs the code and saysokay what's the output from that and andwith these search engines I think isjust incredibly interesting because wehave all this kind of like searchpipeline stuff and query formulationthings where you might be like uhtemperature in Germany and then youquery it and then you get the resultsback and you're like oh actually I meantthis particular City or like you oraverage like you add keywords as youkind of iteratively search oh so wheredoes this self-ask thing this Chain ofThought is it in the training data is itsomething that is maybe learned from thelanguage how has it learned to be likeyou know have a prompt that breaks upthe question into like the compositionalfacts and then how how does it learn todo thatthat's a great question honestly youshould invite ophir to chat about thatand I I it's a great question I hadn'tthought of it that wayI don't know where it is in the data andit may simply be that the model ismashing up all this other data that ithas in an interesting way the same waythat you can get stable diffusion togive you like people sitting around acampfire on an airplane I think I saw goby or like salmon swimming in a streamwhere it was literal pieces of salmonlikeyou know that was never anywhere in thetraining except but it managed to mashit up and figure out what to do with itand I wonder if something similar ishappening here I what I what I loveabout all this work like self-askum Chain of Thoughtwe are developing new querying languagesthis is like us inventing SQL exceptthat we didn't design the database thedatabase came into being and we have tofigure out how to interact with it thatexample I mentioned about the IPythoninteraction like that's a again it's anew querying language and I honestlythought the most potent part of theself-ask wasn't even necessarily theself-ask partit was that ophir did such a phenomenaljob figuring out a way to measure thecomplexity of the knowledge that wasextracted from the modelhe gave us a benchmark a ladder to climba way to measure whether we couldretrieve certain kinds of informationfrom models and I think that's going toopen the door to a ton more benchmarksand you know what happens when there's abenchmarkum we optimize the hell out of thatBenchmark to edit coach science forwardwe develop new querying methods I don'tlove the idea that we'll go through andmanually come up with these queryingmethods because that just to me seems alittle unscientific in some sense maybeit's good enoughum maybe some of this prompt tuningstuff will give us ways to discoverbetter prompts or something like thatit's still pretty earlybut you know we've raised really two orthree huge questions hereone is you know how do you get theknowledge into the modelor where does the knowledge come from oreven is the knowledge in the model ordoes the model just know how to interactwith data sourcesnumber two is how do we query the modelnumber three is how do we measure theefficacy of that query like in SQL youknow if you got the right answer likeyou can go through and manually searchthe database or what have you do youknow if there's a bug in your likedatabase compiler herehow do we know how to measure what themodel knows is there a world in whichactually The Innovation that allows usto dramatically reduce the cost isn'ta bigger model or a better trainingalgorithm but a better querying languagethat allows us to find that actuallygpt2 had all the same knowledge or itcould do much of the things that we cando today with gpt3 we just didn't knowhow to ask it for it we didn't know howto get it out of the modelI don't know that I like that makes medream I kind of wonder if a billionparameters may be enough in two yearsbecause we'll get better at the queryingpart but to do that we need betterknowledge measurement and if I were agrad student right nowthose would be all the cool questionsI'd want to work on because you don'tneed a huge model to do itlike oh sure told me he used 900 worthof compute on that entire paperwhich is just insane given that I havegraphs that are 10 times worse that'snot in some of my papersyeah well that um so fear came up withthis like compositional celebritiesBenchmark whereum in the it's like umuh who I think it's things like who waspresident the year that Justin Bieberwas born uh so you you chained togetherthese facts and then it does that uhmulti-hop compositional question and uhJonathan we brought up the um the uh Ithink it's salmon swimming up a riverand um it's a diffusion picture and andit's like literally like uh literallybut it's like the food is like in ariver it makes no sense and I'm surethat future without the training set andpeople have pushed it enough it's notjust memorizinglaughsbut uh so I'm very curious about thislike compositional generalization Ithink like the Gary Marcus angle is it'slike um uh a cup of coffee with holes init it shouldn't be able to exist rightis because the coffee would come out ofthe cup or uh the ball on top of the thelike you chained together all thoseadjectives like a round green rectangleon top of uh uh purple cylinder what doyou think about this compositionalgeneralization is is this like kind ofthe ultimate generalization testI don't know I try to stay away from theGary Marcus conversation to becompletely honestum you know you can listen to old peopletalk a lot if you want to or you can doreal scienceum that's all I'll say about Gary Marcusright now on Twitterum but I do think it's an interestingquestion as to what kind ofcompositionality we wantum you knowwhen it comes to videoyou could get this kind of behavior Ithink really you know what may bemissing is that we're working onpictures and not on video then when yousee video you may say well there arestill things that can't really figureoutum okay fine you know we need to move tosome kind of better World model so it'sreallyI think the question of like the coffeecup with holes in it is a little bitseparate from the answer of kind ofcompositional reasoningon the compositional reasoning side Imean I think ophir may have pushed thesekind of compositional questions to theirlimit like once you get into three partquestions likewho was president in the year that theperson who wrote blah blah song was bornum like when you get to the third andfourth order questions it just gets likehumans wouldn't be able to answer thosequestionsso I think ophir has shown us a wayforward and there's going to need to bemore Innovation on how we measure thekind of knowledge that's in these modelshow we find other things that requirelogic to work their way through or itreally it is a throwback to some of the1970s and 1980s style AI where we mightneed these giant knowledge bases wherewe can come up with facts that we ordeductions that we know to be true basedon these you know these expert systemsorts of databasesand then you don't need to ask the modelto try to reason through and figurethose same things out deductivelyand I don't know I think that's a reallycool world to live in and a fierce paperlike it points the way toward a futureno we have benchmarks that look likethat where ophir doesn't have tomanually generate those questions but wehave you know maybe we dust off somescheme from the 1970s andthat may already be there for us to pullonyeah well I that kind of thinking abouthow we do this like aggregate query inthese language model databases veryinteresting like if we have a collectionof articles about hiking trails in youknow New England let's say and then wewant to ask how many hiking trails arein New Hampshire and then it needs tohave that kind of intermediate and thelanguage model is the interface of theaggregation that there's this otherpaper called like neural databases issomething that had like this um you knowa symbolic aggregate step explicitly setand I think all that is just incrediblyinteresting um I wanted to also ask youabout a kind of different way ofthinking about language models theretrieval augmented language models andyou know like the whole idea of uh youhave the you retrieve something and thenyou maybe it's like an encoder decoderwhere what you retrieve is you knowfusion and decoder style rather thanjust being say prepended to the inputand then you know decoder only gbt um doyou have any thoughts about things likeyou know rag Atlas these kind of modelsa retro yeah I likeI love where we are in the field rightnowum we have models that we can train tomemorize data we have models that canuse you know we have retrieval augmentedmodels we have the ability to fine-tunemodels we already have we can promptmodels we can do in context learning wecan have the models use external toolswe're and we can actually literallytrain the models to use the externaltools or we can prompt them to use theexternal toolsprobably one or two of these are goingto stand the test of timeI have no idea which and I love itI absolutely love it right now like thisis this must have been how it felt inlike 2015 for those of us the 99 of uswho aren't in this field in 2015 whenVision models were just exploding andevery week it was like a whole differentway of doing things Fiji resnetInception these were all such differentmodels that were trained in suchdifferent ways Adam was floating aroundat that time in different optimizationstrategies Bachelor and pops up layerNorm pops up it's just this whole sea ofideas and it was hard to know what thebest thing would be dancing at widedress net like it was just and here wehave even more diversity and justcompletely different paradigms forinteracting with these modelsit is a great time to be in the fieldlike I don't know people keepcomplaining like all the cool stuff wasdonethis is the cool stufflike this is why I'm here because wehave no idea what the right answer isand there's so much science to do andfor the people who sit down andsystematically develop ways to measurethis and systematically develop realworld scenarios and honestly that's alot of our work at mosaic unfortunatelyit uses a lot of customer dataum because and we can't release thatdataum although we try to find ways to sharethe findings and we can reproduce it onpublic data sets butyou know what a time to be in this fieldso much happeningyeah how much um like how much of thisdo you think is from the Transformer andjust text as being sort of the interfacewith multimodal domains I feel like alot of the well I mean yeah there's alot of breakthroughs to look at but likeI feel like the sort of text and NLP tome seems like sort of maybe the biggestI mean um images of course but I thinklike this text interface is just such aunifying way of doing it and then alsofor creating user interfaces for uhpeople to interact with these things uhso many pivoting topics um I heard youknow bachelor normalization laternormalization um can you tell me aboutthe state of uh composer and all theseregularizations I'm so curious about youknow the training recipes and making iteasier to train modelsyeah so I'll walk you through a bunch ofthings so Mosaic the way that we thinkabout this is at the end of the day wehave kind of converged around four orfive big models so much A lot's builtum you know those are things like resnetwhich is not only for imageclassification more academic but also asthe backbone for a lot of objectdetection segmentation models that areused in practice today you know we talka lot about the big Vision Transformersand everything but by and large whenwe've seen you know good old dressnet isstill the old standbythen you have you know your segmentationyour object detection models we've againconverged around a few different headsfor doing segmentation for doing objectdetection so we're kind of you knowthere are a few different pipelines NLPyou know it's Burton GPT that's the showyou know there are other sorts of thingskicking around there is retro you knowkind of a tweak on TPT there's you knowthings likeumshe was completely slipping my mindum but you know there are a bunch ofother paradigms out there by and largeit's broken GPT that run the showum and at the end of the day thosestandbys really are what works at mosaica lot of them are focusing on thescientific side is developing reallygreat recipes for training those modelsso you know we released our recipe backin June that was our proof of concept tobasically convince ourselves it would bepossible to speed up models this way andwe got a 700 speed up over the Nvidiabench bass lines which I'm so proud ofto be fair it's an old Benchmark um buteven over some of the newer stuff likethe Tim models we still were looking ata 2x or 3x Improvement and accuracylevels that had never been achieved onpress that 50 on image that without helpfrom outside data setsso we're really proud of that and youknow we're doing it for everything sohonestly I'll just you know I'll goahead and spoil it for everybody we'regoing to be releasing our segmentationmodel next week I believe that we'relooking at the team has told me anywherebetween a 5 and a 20x speed up dependingon your Baseline um it turns out eventhe baselines for this model weren'tvery good and we spent a little bit oftime improving those we had a blog postabout that over the summer and then oncewe've improved that Baseline you know wewere still getting between 5 and 20x andwe hope this will be really useful tothe computer vision Community those whoneed segmentation it's a smaller moreNiche groupum but it's a Workhorse and it's reallyvaluable and then on the Bert side we'llbe releasing our recipe probably in thenext month or so you should expect againabout a 4X speed up on pre-training onyou know things like C4 but alsoprobably on any data set for academicpurposes this is going to be hugewell you know wehonestly I can't talk about one otherthing that we did with it because youknow I'll just say we're still in the mlPerth quiet periodum but you know you'll see more aboutBert soon on that front as well andwe're really proud of that some of thisis even not even with the algorithmicspeed up so with really good systemsoptimizations as wellum GPT you saw our Baseline released youcan see this pattern Baseline speed upBaseline speed up Baseline speed upum did that for estimate in June thespeed up for Baseline was back I thinkin July for segmentation three monthslater speed upum you just saw our llm Baseline acouple weeks ago you know what to expecttoward the end of the year early nextyear you know that's what I'm hopingwe'll have TPT three for 100kand then you know Bert Baseline wedidn't really push the Baseline too hardthis summerum but you'll see the speed up out inthe next few weeks and so you know ifour track record keeps up we're justgoing to keep doing this objectdetection Baseline coming up in twoweeks you know the drillum I'll start working on self-supervisedVision you know the drill we may work onsome diffusion models you know the drilland it's just you know keep crankingaway at this honestly on llms I thinkthere's more than 5x thereI'd love to get that cost under 100kyou know I'm not going to promise itbecause it's hard but I'd love to get itthere and what that also means is that a1 billion parameter model gets down intoa couple hundred dollarsfor an academic researcher that's whatimage that used to cost that's hugeI'm still the second year PhD studentwho didn't have as much compute as hisfriend at least in my heartum I want all Academia all people inAcademia to be able to do realscientific research that the folks inIndustry aren't doing so you know youcan see what we're doing on that frontand thena lot of the big promises were buildingit on Miss Mosaic Cloud which is aplatform that is meant to make it reallyeasy to do this stuff you knowyou know multi-node training where youdon't have to deal with any of theissues good integration with our recipeswhere we can automatically profile yournetwork and figure out which speedups toput in depending on your data setdepending on your model those sorts ofthings we can catch you know lost spikesbasically it's you know trying toimagine what do I wish I had had that Iwas doing my PhD and hopefully you knowit's useful to more than just mebut you know the hope is that'll allowus to stay in business long enough tokeep giving cool things to the communityum and my hope is that if you train withus it'll be cheaper than training withany other Cloud because with our speedup methods you know our a100s aremagically 5x cheaper than everybodyelse'sso hopefully that'll you know that'll bea nice gift we can give to the communityin terms of dollars not just in terms ofusing the speed up methodsyeah well it's obviously you knowBaseline speed up is a nice Trend to befollowing um could we I mean um yeahlike you mentioned um you know detectMosaic will detect a lost Spike and thenI think it resets it and maybe ordersthe data differently um tell me a littlemore about how uh the the recipes are acomposition of like uh you know batchnormalization or different attentionlayers and and maybe like clever ways ofdoing Dropout like can you tell me howthe recipes are unique to each data setor or network or like how how is it notis it not just the same recipe or thatkind of thing it's a good question it'ssomething we're always studying andespecially right now we're doing a lotof probing for our old recipes figureout how broad they are like all rightlet's take less than 50 when theymention that because it's a benchmarkmost people are familiar withwhat happens if you run that recipe onthe places data setdoes it completely fall apart have weway overfit our recipe damage net havewe always fitted what if you changed theresolutionhave we way over fit what if you changefrom reset 50 to resident 101so we're working right now not just onthese great flashy recipes that give youa 7x speed up put on nice genericrecipes that give you a consistent 3x ona wide range of different things in anarea so you know for Burton gbt that's alot easier because language is languageto a large extent for vision that's alot more complicated because there aremany other angles to take so especiallyif my vision team were here right nowthey would tell you that's the biggestworry on their mind is how generic arethese recipes and how do you make themmore genericbut at the end of the day you know a lotof what we're doing if you start to lookinside it there's some stuff that's alittle bit bespokeum you'll see this with our recipe forimage segmentation where you know ifyou're measuring dice as your metric youknow we've added dice to the lossbecause you know that's a really goodway to improve your dice and it'ssomething that people have proposed andonce you do it it does help a lotum but there are other things we'redoing like exponential moving averagingto the model which is pretty generic itworks across all of our computer visiontasks so it's hugely helpful in MLP itdoesn't necessarily help but it doesn'thurt so there's no reason not to turn iton or something like sharpness awareminimization where there are some hyperparameters you have to set but it's beengenerally helpful for computer visionso a lot of this or mix up a lot of thisis actually pretty generic and I have noreason to believe it won't work onpretty much any computer vision taskit's up to us to prove that you know thethe burden of proof is on us butso far so goodand I'm really pissed like you know it'sI hope we'll have more results on thatin the next month or so on how genericthese recipes are and whether you'rereleased like you know the genericrecipe and the one built into our Cloudwill be the generic recipe that'll bewhat you get unless you really want toyou know go for it but you know if youcan get 3x instead of 5x but it worksacross the board I think you're prettyhappy tooyeah well and I it's a very interestingpoint uh from imagenet to places or youknow satellite images I don't knowmanufacturing images and the thinking oflike have all these data augmentationsjust been developed for say imagenet ordoes this really work for any kind ofcomputer vision a super interesting ideaI don't know yeahI mean um it definitely seems like maybelike some of the cropping uh yeah I meanthe whole like think about like thedifference between satellite imagescompared to image I think is veryinteresting personally yeah if you wouldask my vision team they would tell youthey're specifically worried about thatright nowum and so they're working on it andthey're getting a bunch of data sets andwe're going to find out what the genericrecipe is and I think we'll have youknow maybe that'll be the third blogpost in each series you know it's ourBaseline our best recipe then ourgeneric recipeand I think that would be funum there's one other thing I wanted tothrow in here which is reallyone of the big aspects of llms isactually this fully sharded dataparallel Libraryand I think it's important to discussfor a couple of reasonsumso when we train large language modelsthey're too big to fit on a single GPUin fact they're too big to fit on asingle node oftentimesand not only that but the activationsare too thickso you have to do what's typicallycalled 3D parallelismwhich is where not only do you split themodel across multiple gpusbut you also split the data acrossmultiple gpus which is what you usuallydo data parallel training you also dopipeline parallelism where you send eachexample through the model in a pipelinefashion you accumulate the gradients asyou kind of pipeline the examples backand then at the end you apply thegradients to model and then you repeatso you don't even do it all in one stepand you're constantly streamingactivations from you know first part ofthe layer to the next you might evenhave to do tensor parallelism and splita tensor across multiple gpus thesethings are hugeum and there's some fantastic Frameworksfor doing this so in the past one of thevery popular ones was deep speedis super popular for doing largelanguage model training we've workedclosely with the Deep speed team andthey're awesomeum they produce some awesome stuff infact they produce this other cool thingcalled xeroand it's a series of sharding operationsfor charting the state of things likeyour Optimizer and your model across thegpus and even in some of the fancierfloatations you know even doingactivation checkpointing to the CPU andeven doing activation checkpointing tonon-volatile storageattached to your model this is reallycool that's something I'll talk about ina minute because I think that's just thecoolest thing in how they do it becauseif you train a large language model youdo so much reading and writing butyou'll actually burn out your SSD in thecourse of one training runbut that's fine because the rent is soexpensive that's one SSDum but the nice thing about these isthat these sharding techniques actuallyreduce your need to do all these dataall these parallelism techniquesand so this fully shorted data parallelLibrary this is out of the folks at Fairum but what it does is it's just dataparallel but all it's doing is it'susing these zero techniques to Shard themodel stay at the optimizer State acrossall the gpus in the cluster and soinstead of needing a ton of GPU memoryon a single node to do data paralleltraining and again data paralleltraining is just where you have themodel you know you have multiple copiesof the model in htpu you run differentexamples through each through the modelsforward propagate backward propagate andthen merge the gradients so it's kind ofit's the dumb thing that you would do ifI gave you eight gpus and told you totrain that's what we're doing onliterally everything else if you use pietorch distributed data parallel forexample it's a naive thingfstp takes us back to a world wherewe're just doing data parallelismyou don't have to do model parallel wedon't have to do tensor parallel wedon't have to do pipeline parallelismwhich is the nastiest but we're doingsmart shardingso it requires a lot more Networkbandwidth in order to do thesynchronization but it's way simplerMegatron is notorious for being hard tocustomizethe models that we released for fsdpit's a one fileand you just have to wrap some things infstp but it's one file which means youcan customize it it's pure Pi torchso you can imagine for us at mosaic wewant to do speed up methods we need tobe able to modify the heck out of themodel in various waysthat's really hard to do in Megatronit's a piece of cake in the simplenotationand I think it's what I like it too iskind of a like a cafe versus tensorflowversus Pi torch sort of thinglike we wouldn't have gotten a pie torchif we didn't have some Frameworks thatpreceded it that got better about comingup with the right apisI think it's the same thing with fstp insome sense it's superseding Megatron ordeep speed at least for the moment whichare superseding hand rolled Frameworksthat you had to come up with yourselfum because we've learned how to get theinterfaces right we've learned what isand isn't necessary to make things runwelland so my argument to all of you is ifyou like to Pi torch over tensorflowyou're probably gonna like what we'veput together in fsdp over Megatronand it's customizable and easy to use ifyou want to tweak things and I thinkthat's resonated with peopleso you know maybe we'll have the Jacksthat will eventually replace this but atleast for now this is so much cleanerand so much easier and you don't have touse things like Megatron that are reallyunwieldy and difficult to work with andI've heard nothing but complaints thesame way that in the tensorflow dayskind of you know at least in tf1um it was the best game in town but itwas still something everybody complainedabout I think we still complain about Pitorch but a little bit less and nowpeople seem to love chapsI like to think we're getting to thatplace with llms and again they are foreverybody but we're still getting thesame utilization you would get withMegatron you're not giving up any speedyeah well that that's so fascinating andpersonally I don't have enoughexperience with these things to reallykeep the debate going but I I do likecompletely appreciate the abstractionsand the apis it it's just all incrediblehearing about all these things and umand yeah so maybe uh pivoting topicslike could you tell me about yourexperience with um building Mosaic as acompany you know it's growing like crazyit's so interesting to see this you knowcompany emerging can you tell me aboutyour experience building thisit's been hardI think our CEO Naveen Rao will alwayssay you know building a company is oneof the hardest things you could ever doum there are a lot of hard things inlife this has been the hardest thingthat I've ever had to doum definitely takes the cake on my PhDat a PhD you know you have to do goodscience that's really hard and you haveto create something that's never beencreated before that's really hardum not only do we have to createsomething that's ever been createdbefore you have to create somethingthat's actually useful to people anduseful enough that people will pay forit we have to understand what peoplewant we have to build one big thingtogether a PhD is very much anindividual Pursuit building somethingtogether is hardum you know trying to figure out youknow you had seen like to get to thebase we do the Baseline blog post threemonths later we do the speed up blogpostand to get those baselines it's probablyanother three months of work which meansthat we have to predict what ourcustomers are going to need six monthsin advance if we want to have it readythat's hardthis is all really hard in a spacethat's changing so quickly and in afinancial environment that's changing soquickly 2021 was a great year to beraising moneyum2022 is notit's been a tough year for startups it'sbeen a tough year for everybody in termsof you know tech companiesand so all of that is really difficultkeeping a team focused keeping a teamyou know with our eyes on the ball andwhat matters trying to serve ourcustomers appropriately it is so hardand I'm sure you can speak to the sameexperience it is so difficult and I lovethe challenge I hope they'll succeed whoknowsum but the experiences I've gotten andwhat I've learned have just beenincredible I really didn't plan on doinga startup that was I mean we've talkedabout this a little bit more privatelyyou know that that was not my plan inthe leastum I'm really glad that I had thisexperiencethat's super interesting and you'vealways amazed me with how many thingsyou're able to do in parallel I thinkwhen I first met you you were atGeorgetown you were at MIT you didFacebook how to like and now you're aprofessor at Harvard and Mosaic how doyou manage them like how was your kindoftime think like I'm very curious likehow you are managing your time sort ofnot in your new role in Mosaic and alsoobviously your role in scienceit's tough It's genuinely tough I meanI'll tell you two things one I haven'tstarted at Harvard officially yet I getone more year but Harvard has been veryeffective at finding ways to get freelabor out of me so advising studentswhich is a blastum and I'm also managing their computeinfrastructure because I'm you know I'mdoing that in my day job so it's reallyuseful there it's been a lot of work andthen you know between you and me I stillhaven't finished my dissertationI'm working on it but it turns outmosaic has been a huge distraction myadvisor will ping me periodically andyou know make fun of me like you knowyour students realize you don't have aPhD yet rightum it's the biggest running joke atmosaic that like you know oh Jonathan hedoesn't have a PhD yet yeah my friendsfind it so funny that I have a PhD andmy boss doesn'tumit's hard and I'm not sure I'm thatsuccessful I've also chosen toprioritize work-life balance latelywhich has been really nice but alsoum startups are demanding and they willtake all the time they can give them andI work really hard with my team to makesure they're taking vacation I yell atthem if I see them on slack on nextweekend there are times for that andthere are times when we have to push butit shouldn't be all the time and ifyou're in a place where you have to burnyourself out to be successfulI don't know if that success is worth itto be completely honestum there's much more to life than youknow suffering for your workand you know so yeah the answer is Idon't have the balance I should have butI have a hell of a lot more balance thanthe last time we talked to the timebefore that and I'm really glad for itbut it's hardum at the end of the day the people I'mmost responsible to are my teamthey're counting on me to make sure theystill have jobsthey're counting on me to make sure thatthey're doing the right work and youknow when push came to Sheldon it's amatter of taking the evening off ormaking sure that my team is in a goodplace and is safe and is happy you knowyou've got to be people firstyeah incredible Jonathan thank you somuch for doing the podcast it's soamazing getting to pick your brain aboutthese things there's so many interestingideas so caught up with it and you'rebuilding such an incredible company andand yeah it's it's really cool to hearhow you know the people first thebalance and all that good stuff insteadof some kind of like let's workourselves to death culture but but likeyeah overall want to do the samefeedback back to youum you know there's a life outside ofwork don't you forget that either youlead a pretty busy lifeyeah well thanks again Jonathan thanksso much for doing the review podcast andit's awesome I'm so excited about thesethings thank you so much for having me", "type": "Video", "name": "Jonathan Frankle on MosaicML Cloud - Weaviate Podcast #26", "path": "", "link": "https://www.youtube.com/watch?v=oFyYaZbRviY", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}