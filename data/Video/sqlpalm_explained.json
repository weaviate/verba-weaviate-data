{"text": "This video dives into the new SQL-PaLM paper, exploring how well the PaLM LLM can be prompted to convert natural language ... \nhey everyone thank you so much for watching weave it on YouTube today we're going to dive into SQL Palm improved large language model adaptation for text testql so before diving into it maybe let me give you three reasons why this paper is worth your time exploring so firstly this could really reduce the barrier of Entry to using databases if you just have to write a natural language question like you know what is the average age of country music singers instead of having to learn how to do the SQL syntax it makes more people able to access databases and by people we also mean large language models in this whole tool use kind of thing so the second thing I think this is a really interesting paper is just understanding further the zero shot few shot learning abilities of these large language models they compare their fuse shot prompted Palm large language model with say these models that have been fine-tuned on the spider text SQL data set and they're finding that basically they can prompt this model to just instantly outperform the previous models trained with supervised learning and that brings me to the third thing is I think especially if you're interested in deep learning research this spider data set we're where they collect 7 000 queries across 166 different database tables for training and then say like a thousand queries on 22 databases for testing it's a really interesting data set for how text SQL is measured they also have three variants of it spider synonym replacement spider realistic and spider domain knowledge that kind of tests this idea of you know how do we Benchmark how well these models can write SQL queries I think this is super interesting for wevia weavate also has an aggregate API built in as well so you can do these kind of queries with weave as well I've tested this kind of few shot prompt on wevia's aggregate syntax so we'll dive into that and all sorts of fun details so thank you so much for tuning in and I hope you enjoy this paper summary this video will explain a super exciting new paper on text to SQL translation SQL Palm improved large language model adaptation for text to SQL really quickly before getting into the paper if you like paper summary videos like this please leave a like And subscribe to the channel it really helps encourage us to make more content like this so let's dive into it with a two minute tldr overview so the high level task of text to SQL is to translate natural language questions into SQL structured query language questions or syntax so we take a question like what are the names and release years for all the songs of the youngest singer and then we translate that into the SQL query based on our attributes of the table select song name song release year from the table singer and then order the results by age in ascending order and limit the results to one for just this uh youngest singer so this is the high level idea being able to ask databases questions in just a very natural way and then under the hood the large language model is translating the natural questions into the structured query language so one of the key investigations of this paper is comparing the performance of few shot prompting the Palm model Palm is one of the giant pre-trained large language models one of the you know 500 billion parameter free trained models so we're going to be exploring how well you can just few shop prompt this compared to the performance with say fine tuning Palm on the spider data set and comparing with you know the current state of the art all the existing models that have been fine-tuned and deployed miscellaneous training tricks and inference tricks to perform this task of text SQL translation so quickly what that looks like is when you're doing few shot prompting in the input you give it examples of a database schema and a natural language question and then the corresponding SQL translation so you in this a diagram you're just seeing two examples of this in the input that's kind of one of the questions a few shot prompting is you know how many examples to give whether these examples should be tuned to be more in line with the downstream inference but then what you do is you then have this novel inference schema three plus question three and then it generates the new SQL so Infuse shot prompting it has these references of the task to you know to reference and sort of learn from in the input window which is you know one of the super interesting emergent abilities of large language models compared to the more standard inference setup where you just have the novel input and then you know you generate the output so a is exploring the few shot prompting B is exploring fine tuning Palm on the data set the spider data set of text SQL translations and then just seeing single inference single output kind of setup so this is a quick additional look at what this would look like when you're talking about giving it the schema you're talking about compressing representations like this into text to prompt the model so say you bolded singer as the table and then it has the keys singer ID name country song name and so on so interestingly you say have like foreign Keys when you when you're looking at concert and you need to join say concert ID and Stadium singer in concert so you have a lot of interesting ideas of how you could kind of extend this and it's also worth mentioning that this is mostly about kind of like a surface level SQL interface not like the underlying optimization of how the data is stored but you know offering this kind of interface in natural language so so this is the idea of taking these questions and then translating them into not into SQL based on giving it the table in as a part of the input so another really interesting detail this paper are the data sets that they use to evaluate this spider is a collection of about uh 7 000 trading queries I think a thousand testing queries across 166 database schemas for training in something like 22 for testing so they also have these variants of the spider data set as the text to SQL research Community has iterated and learned about this these things so spider's synonym is about manually modifying natural language questions with synonym substitutions so instead of how many singers do we have and singers might be like two kind of on the nose with the database schema you say how many vocalists do we have ideas like this to try to like test the robustness of how well the models perform when the natural language question starts to deviate from you know something that's closely aligned with the database schema so it was spider realistic that removes the mentioning of column names so compared to the question how many concerts are there in year 2014 or 2015 just more natural how many concerts are there in 2014 or 2015 and then spider domain knowledge is where you're adding domain knowledge so you know list all song names by singers above the average age and then hard to answer age-related question based on the you know if you only have the uh the birthday column so so you need to like derive birthday from the table and then answer the question by age and other examples like this so quickly before diving further into the details of the SQL Palm experiments I want to quickly remind you that you can do these symbolic aggregation queries directly in weviate using the aggregate API so in weavate if you have a class like maybe just to continue on this if you have say the singer class you similarly have these properties where you have you know Joe sham is a text property or say you have age which is a you know an in property or Ismail is a Boolean property in weeviate's aggregate syntax you can then access these kind of symbolic aggregations using this syntax so say you want to see which of your singers appears the most in your data set you would do name and then top occurrences so you want to see the average age you could similarly do this you know age mean to access that or the Boolean total true total false this kind of thing and then if you want to do this kind of joining as well you would do you know say singer and then say singer and then the relation is performed in concert concert you would do you know performed in concert dot dot on concert and then now you're in the concert class and you can keep using all this kind of syntax so that's how you do this in we gate we're going to look at a few shot examples showing that this exact kind of thing works in weavate which is super interesting so if you want to do this symbolic joining queries aggregations of your metadata you can also do that in weeviate there's another super super exciting connection of This research with weeviate we're going to dive into this more at the very end of the video on some key takeaways but Lang chain llama index they've recently been promoting this self querying retriever kind of idea which is really amazing and what it is is you take a query like what is a golden doodle and you add filters to make it filtered Vector search to step back a bit weavate Vector databases are about building these Vector index structures to allow approximate nearest neighbor search at absolutely massive scale in addition to just kind of traversing the proximity graphs to find the nearest neighbor's distance wise we also can connect them that match symbolic filters so say you want to build one index and then have a symbolic property like the source of the index so say it's you know text passages and then you have a filter on you know whether it's comes from Wikipedia it comes from archive or comes from Reddit you can add these filters onto the query and it will Traverse the vector index with that filter so it's really interesting the idea of enabling these you know query routers to either say which of these classes do you want to search through or what kind of filters do you want to apply on searching through the vector index so we'll talk about that at the end but this is another kind of connection to this idea of using these large language models or you know just generally the capabilities of language models to help facilitate the query interfaces so coming back to the SQL Palm paper let's look at some examples of natural language questions and their corresponding SQL translations as well as the database tables in reference so first of all imagine this question what is the average minimum and maximum age of all singers from France we have this database table with the tables Stadium singer concert singer in concert and then we need this to produce this query select average age Min age max age from singer where country equals France so the large language model needs to look at the table look at the keys infer this country property and then generate this query so already I think something that's really interesting about this and you know previewing the weeviate angle a little bit is how right now what we're doing with this SQL Palm paper is we're just giving it the names of the properties without any kind of description of what the properties are or say you know if it's a categorical property which of the values it can take on and maybe some kind of description of the categories as well or say it's like an integer the range that it can take on so a lot of interesting things for extending this further but right now it seems like just semantic Keys is working for this kind of translation but maybe that's a direction to take this further so here's another question show name country age for all singers ordered by age from the oldest to the youngest so again we take the exact same data schema as the the first example and we translate that this query into select name country age from singer order by age in descending order so again it needs to do an inference about the age ranking and then produce this order by age descending order so this is a great example of taking the natural language query inferring the intent that the user wants and then Translating that into this particularly order by age descending order thing so next up we have our first example of needing to join tables together so we have the question what are the names of Nations where both English and French are official languages we have this table country and country has the foreign key code which is How We join with country language country code so what we need to do is we have this um you know from country as T1 join country language as T2 on t1. code equals t2.country code where and then it learns to parse out the the English and French are the languages and then it needs to then get the names from the top of it it also needs to check this condition is official from the artificial languages so we're already starting to see an example of a query where you know doing this query out might require some knowledge of SQL some experience of doing it especially to you know like quickly slash instantly translate this natural language query into this kind of thing so this is a really example interesting example of introducing joining tables together in this picture of generating text SQL queries the next query is what are the number of concerts that occurred in the stadium with the largest capacity so now we're doing an aggregation we're selecting the count from concert where Stadium ID equals and then we have this nested query idea idea where we select Stadium ID from stadium and then we order by the capacity because we want the largest capacity and then we want to just limit it one because we just want the largest capacity not like a list of the you know top five largest capacities so now we're seeing this concept of an inner query super interesting concept again of translating this into the SQL next up we have a super interesting query find the first name of students who have both cat and dog pets and we have the database table student has pet and then pets so what we're going to do is we're going to first sub-sample from each of the tables so first We join the student has pet pets tables with the condition uh pet type equals cat pet type equals dog and then we're going to do the intersection of those two tables of students who have cats and students who have dogs to find the first name of of the students who have both cats and dogs or a cat and a dog as a pet so this is the first case I think where we're starting to wade into the Waters of query optimization their SQL is built on this idea of relational algebra where basically you do these uh like set pruning set operations that will filter the set so say you know you had a thousand a thousand objects in your database table and this one filter would potentially reduce it down to like 20 objects compared to whereas this other one would reduce it to you know 500 and then you're joining that with another table so there's a question about the order of operations for how you apply this kind of set filtering to reduce the cardinality of how many objects are in your databases hopefully that was a decent overview I definitely need a brush up on my relational algebra but to get the general sense of there's some optimization behind you know how you apply filtering operations and then the order the order in which you apply filtering and Joint operations the next query so this is a pretty complex one what are the IDS and names of all the countries that either have more than three card makers or produce Fiat model so in this case we have the union of these two conditions on the database tables so we're joining together uh you know the countries with the car makers so we have uh six different database tables in this so another case of the this example particularly is a great example where you know you would you would need to have written some SQL to have quickly translated this into this select t1.country ID from country so this I think is the perfect example of why we need these models and how this facilitates this kind of natural language question shown on the top down into this SQL syntax so quickly before moving on from the SQL query examples I wanted to run a little test of how well this would work with leviate's aggregate syntax so the way that I structured The Prompt is first I give it a description of the API syntax so the aggregate function is structured as follows then give it that same you know graphql all possible things you could do with it block as shown previously and then I give it the new schema so I you know I like to play with this webia podcast search example so I have podcast clip that has property speaker content podcast number and the duration of the podcast then another quick description of the task your task is to take a query and translate it to the appropriate aggregate syntax based on the syntax provided above and the data schema so then a training example who is the most frequent speaker in the podcast Clips you know and then this would be the how you do that you top occurrences value occurs and so now the test query of how long was the longest podcast clip so running this live hopefully works so yeah that's exactly correct and so we see how it's able to do this translation and maybe that's not the most confusing query as we saw some really confusing queries but just to get a general sense of how this might look with weaviate and the things we can do with this kind of large language model translating from natural language into structured query languages so let's get into the data sets that were used to evaluate Palm's ability to write SQL queries and a little more background on this field of research called text to SQL so to take a quote from the paper text SQL is a long-standing challenge crucial to enhanced database accessibility without requiring expertise of SQL so this angle is the you know the general human accessibility trying to help more humans use SQL systems you know as easily as possible get running as quickly as possible and then the other angle text SQL enables the development of conversational agents with Advanced data analytics abilities so there's a lot of talk about you know tool use large language model agents that use tools and this is about helping them helping the agents make SQL queries these symbolic aggregations to acquire information like as we mentioned like I don't know I think it was like how many concerts were performed in the largest stadium or things like this to enable large language model agents to acquire that kind of information from structured database tables so This research of text to SQL the flagship data set has been spider spider is the imagenet of text SQL or the beer like the you know the the big data set for this research category spider contains 7 000 training samples across 166 databases and 1034 development samples across 20 databases so having examples of a database schema and then corresponding natural language to query translations based on that database table so similar to say imagenet C and you know the the robustness corruption tests around imagenet spider has these variants spider synonym this describes manual replacement of synonym substitutions in natural language questions so you know sometimes the questions they they explicitly kind of match the key in the schema so you so they replace this to see how robust these models are when the question isn't so on the nose and talking about exactly the property in the table spider realistic is about removing any mentions of column names in the queries so similar idea instead of replacing it with a synonym you just outright destroy any uh mention of a column and then domain knowledge this one I think is a little more nuanced this is where you need to kind of derive a property from one of the attributes so it was I wasn't completely sure of that so with that said let's dive into some examples from the spider the original Spider data set which are categorized as easy medium hard and very hard so characterizing each you know train input output example is easy medium hard extra hard this is a pretty standard practice in deep learning for programming languages like say a lot of these data sets constructed on like code forces or leak code they also have this kind of categorization to it it'll be really interesting to do this for imagenet and all that stuff as well but let's take a look at the you know the training examples and how difficult they've been labeled as so an easy SQL question what is the number of cars with more than 4 cylinder cylinders just you know select count from Cars data where cylinder is greater than four then a medium question for each Stadium how many concerts are there and so I think this is graduates from easy to medium because now you need to join the concert table with the stadium table based on these IDs and then you need to you know select the count of the concerts and you also introduce the group by syntax and you know grouping the same IDs to you know aggregate the concerts anyways so then the hard question I think then you're joining together three tables country continents and car makers so you know extending the joining you have the condition having as well uh which is similar well you have a condition on the aggregation so maybe that's part of what makes it hard versus easy and then extra hard I think you have this nested query not in where you also have this kind of join in the nested queries so a little bit more maybe there's more to the difficulty levels with respect to queries that have that kind of opportunity to optimize with the relational algebra I'm not like a super an expert on this I'm just familiar with relational algebra but maybe that's something that could also factor into the categorization of the difficulty of the queries okay so now that we have a sense of what the data set looks like examples of these questions that are based on these schemas and then the corresponding natural language test skill question the key detail that we're going to be exploring in this paper is how well these this SQL this Palm model taken off the shelf and prompted with a task description and a few of these examples how well can that translate natural language into SQL and how does that compare with fine-tuning a palm model on these input output pairs so firstly let's dive a little deeper into the few shock prompt in the appendix of the paper they have the exact prompts for how they're going to compress tables with the questions and do this few shot prompting so we're at the end of the paper and we're looking at the exact prompt that was used to prompt the Palm model to translate natural questions into SQL so the first thing to note is they're going to ablate two details a concise prompt design compared to the verbose prompt design and this is kind of what I was mentioning earlier where a verbose prompt design is where you you know you give it the attribute and then you tell it the type of the attribute I think it could be interesting also to kind of say it's a category attribute to extend this with the different categories I think this would be sort of essential for that self querying retriever idea we mentioned at the beginning but anyways I think that's getting distracted from this but so basically verbose prompt is where you really tell it about you know these are primary Keys these are foreign keys and you know you give it a lot of information compared to concise where the syntax is a little more maybe let me zoom in to make it easier to see whereas the zoom in is just giving it the values directly so basically what you're doing is you start off by telling it the task this is a task converting text into SQL statement we will first give the database schema and then ask a question in text you're asked to generate SQL statements so then here's an example here's an example so these are the few shot examples so they're annotated as here is an example convert text to SQL so then you have the schema values so in this case it looks like the tables are separated with this um like Farm colon uh yeah so this is City okay so that's what that looks like uh then you have the column names associated with the schema oh so you do have the type so you just have a little more verbose way to say in the type so sorry if I get bad information uh so then you have the question example so what are the themes of farm competition sorted by urine ascending order and then you have the corresponding translation so so you give a couple examples of these and then when it comes to be test time you have here's a test question to be answered convert text SQL you have the new schema the new explanations of the types of the columns the primary Keys foreign keys and then the question how many singers do we have and then generate the SQL in addition to the comparison between concise and verbose prompting the authors are also going to explore consistency filtering and execution filtering so to give a quick background before we'll dive into the open AI playground and get an example of what this kind of Randomness in large language model decoding looks like consistency filtering is mostly used in question answering is where you sample diverse reasoning paths and then the ones that end up at the same answer sorry answering they end up at the same answer that's going to be the final question so if it's like mathematical reasoning and then you're like breaking down the steps of doing the math by decoding different paths in the language model output then you just aggregate the final answers it's the most consistent like say 40 if we're adding numbers or something and then that would be the answer that you give in the question answering execution filtering is unique to programming languages this is where you sample the diverse outputs but then you put the outputs through the code executor and then similarly how many ended up with the same output so you can use this for you know SQL python all this kind of stuff you can take the different code generated and then execute it and then whatever the most popular output is that's what will get sent as the answer okay so we're in the open AI playground and we're going to dive deeper into how consistency filtering works so the idea is that we're going to sample diverse candidates from large language models as we decode them with Randomness we have temperature set to one and we're asking the language model please write a one sentence summary of kubernetes for a five-year-old please be as creative as possible so the first time we ask it this it says kubernetes kubernetes is like playing with Legos to build a superhero City in the cloud okay so that would be you one potential generation then we sample another generation kubernetes is like a magical box that helps get your work done faster so that's an example of how we can sample a you know diverse outputs this is similar to the idea of like tree of thoughts where you you know go down all these reasoning paths so maybe another example would be uh you know how many people are in America please show each intermediate step as you you know determine this answer and then so this is kind of like something that maybe requires reasoning so you know it's sort of like a hallucination also to ask it to do this but okay so it came up with calculated people so it's saying you know 331 000 so this is like the final answer so then consistency filtering would be we just keep sampling from it and then see if it keeps coming to that same 330 Million number I don't know if this is the best example of that but you see how instead of doing the step two it just did it after step one you do this kind of thing but for writing python code SQL code you sample a bunch of different Pathways and then the final answer because in question answering consistency filtering is just about is this answer the same whereas with code execution filtering you take the green that it generated like you know write a Python program for bubble sort and it you know write this python code and then you would put that into the python executor and then consistency of the outputs so quickly before diving fully into the results the authors do find a significant benefit by doing this consistency and execution filtering you see you know no consistency down to 77 then the execution filtering 79 compared to you know up to 83 with the exact matching or the execution accuracy of the SQL queries so I think this is a super interesting idea for this was one of my favorite ideas and say the Lang chain chains is taking the output of the language model putting it into the say the python reple and then seeing the output and then chaining where the language model then sees the output and says okay is this the output the authors actually are going to conclude that they didn't find good results of putting the error messages for queries that didn't pass into the back into the prompt and saying hey here's the error that you got for this but I think generally that kind of that I mean like that's how I personally code as a human right is I see the error and I iterate on the error so I think there's probably something to continuing to mine these chains of execution filtering okay so let's dive into the results now that we have an understanding of what the spider data set is what it measures these text SQL translations given a schema as well as part of the input and we have a sense of few shot prompting versus say fine-tuning the model as well as say this consistency filtering execution filtering concise prompt verbose prompt ablation so firstly what we're seeing is the comparison of these few shot SQL palm versus the fine-tuned SQL Palm so the first interesting detail is that they don't find much benefit by fine-tuning the model this is actually an absolutely enormous detail because it it simplifies using this like crazy like if if you don't need to fine-tune the models and you don't need to think about like all this stuff around constructing the data sets the batching the you know the the evaluation or the model versioning all that kind of stuff that would come with if you really needed to fine tune these models to get it to do tasks like that so they do find good performance in the fuse shot I mean it's probably worth noting that say we're doing the weeviate trend we're writing weeviate aggregate queries or you know you're trying to customize this to any arbitrary API it might it might not have as much domain knowledge of that in the pre-training data because surely palm and its pre-training Corpus has SQL examples information about SQL in it so that might be slightly biasing the results as well but super interestingly so we're seeing 82.7 from the fuchsia model compared to 84.1 from the fine-tuned state of the art another kind of Reason though the fine-tune state of the art might be a little more interesting is because of this three billion parameter so it's gonna be a little cheaper to run this is another one of the big Topics in deep learning is can we use maybe these models to generate training data to then do knowledge distillation and then have a much smaller model that costs us less for inference so you know is it really worth it if every time we run these queries it costs us like two cents to translate it to the SQL especially when the large language models are doing them autonomously so it's an interesting detail these three billion parameter models that are you know pretty on par and we could probably use these models in a different way to distill them but nevertheless if you're a trans like again is using this example of the weeviate aggregate query translation I think that's the most interesting motivating case of the few shot because you know you're you're getting it to learn this new API as fast as possible without collecting train data or anything like that so here's another example breaking down the performance from easy medium hard extra hard so you know probably the biggest takeaway is that you do see this like monotonic performance decrease from easy to medium hard extra hard so that's a pretty interesting detail of this so especially if you're you know planning on if you're interested in designing systems like this if you should probably have some kind of maybe a classifier that says how hard the question is stuff like this or just generally logging the queries that fail and overall trying to work to get these models to perform better on the hard questions but just an interesting thing to see so then we have the variants of spider so again we have the synonym replacement uh the more realistic questions where you deleted any mention of the column name and then the domain knowledge I actually wasn't really able to figure out exactly what that is so uh so comparing with the chai gbt open AI default prompt this is another really interesting detail I'm sorry I forgot to show this in the video but open AI they have a recommended prompt for writing SQL queries so what they're showing is they're uh they're more verbose problems with the consistency filtering execution filtering improve the performance and then you see the robustness across the uh the different data sets you know it's still this three billion parameter model is doing a pretty good job which is exciting because this would be cheaper to run so then we have the ablation of the concise versus verbose prompting so interestingly the concise prompts perform better which they perform about the same but like the you know not really giving it these long descriptions of the properties I think it really depends on this and you know we're gonna I think the verbose prompting is more exciting going forward for the k for the case of filtered Vector search with that self querying retriever thing because we want to you know filter it based on property so I think it needs to have some sense of what the properties are like what the potential values that you could filter with so definitely an interesting topic as well so here's some discussion topics from the authors they say that you know once they fine-tuned the model so you know converting this to this the sampling diversity significantly reduced so that kind of consistency filtering execution filtering they had less diversity and thus that technique on top of the inference was less effective they also found that as mentioned earlier the self-correction giving it the error messages when it doesn't work that that didn't work that well which I thought was kind of interesting and then they find that you know that there are some problems in evaluation data set they give this comment that many of the errors are actually correct according to human experts so here are some of my personal Reflections and takeaways after reading the SQL Palm paper so first of which I think coming up with some kind of Auto API for weviate aggregate I think this is low hanging fruit it definitely seems like something we can achieve with using the openai large language models or the Palm that's just any of these large language model apis that we already have in place for say the Eva generate modules we could similarly put this after the query to translate it into these aggregate queries which then the next question is how exactly we want to design this pipeline so maybe it should First Take You know the auto API should be so open-ended that you just have a query and then the classifier or the language model first says is this aggregation query or a get query and then the third thing so so this is kind of just a little layer on like is this an aggregate query or is this a vector search query and but you can also apply Vector search within here to you know to say you want say you have tweets and you want to say like like you know similar to this tweet you like exactly paste the text for your new tweet and then you want to aggregate I don't know say like the likes or something from that I've given a talk on this at odsc London that's also on alleviate YouTube if you're interested in that kind of topic of the combination between Vector search and then symbolic aggregations I think it's a pretty interesting topic but anyway getting a little distracted but so then what I want to talk about is first of all this query routing idea so first of all imagine you have two different classes in weeviate you have all the podcast podcast transcriptions and then you have like the Wii V8 code base and you're asking a question like you know how is this particular thing implemented first asking that question the language model of which class should you search then this idea that I love which is which filters to add so this is the you know self querying retriever you ask what is a golden doodle and imagine your passages have this attribute about which animal they're talking about you can then filter the search space dramatically with that so say we have like I don't know we have like a 5 million scale passages about animals so then asking it what is a Goldendoodle if you can do that filter where animal equals dog then you dramatically simplify the search space so it's a pretty interesting thing thinking about these categories I think it's most straightforwardly manifested with you know like price filters so say you you know like take a picture from a shirt from like Louis Vuitton or one of those designer Brands then you just drop that into Amazon you say you know image search shirt like this for you know less than a hundred dollars and not having to know how to parse out the wear filtering the the large language model under the hood parts of that out for you the next really interesting topic I think is connecting surge primitive so you know we have these you know we have all sorts of different features for search and we V8 so say you have like you know you obviously have the wear filters we just talked about but you also have Vector search bm25 Hybrid search or say you want to re-rank the search results can we kind of automatically construct a search pipeline for doing this kind of thing and then sixth thing is this concept of generative feedback loops where we take data from the database you know send it to the large language model and then save the result back to eviate I think in a lot of cases these you know like when you're doing these kind of symbolic aggregations you might want to save the fact that you derived maybe as a natural language fact into some kind of Text corpus like I know generally like when I'm using SQL like back when I did that kind of thing I if I had a query that was useful I would save that query for reference later so I wouldn't always have to remember how to write it so maybe there's something to that in some kind of connection with generative feedback loops but overall my takeaway is I think this kind of idea of automatically writing these kind of aggregate queries exact same ideas the paper I think that's also something that we could achieve in Wi-Fi and it's quite exciting so thank you so much for watching this paper summary of SQL Palm again if you like content like this paper summary content please leave a like And subscribe to the channel it really helps encourage us to make more videos like this please check out wevv8 on weavate iO or the open source GitHub repository web and please follow us on Twitter at webiate IO thank you so much for watching ", "type": "Video", "name": "sqlpalm_explained", "path": "", "link": "https://www.youtube.com/watch?v=g3ocV0a_G2c", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}