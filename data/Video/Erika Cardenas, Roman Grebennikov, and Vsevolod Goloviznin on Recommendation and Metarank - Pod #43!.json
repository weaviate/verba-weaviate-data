{"text": "Thank you so much for watching the 43rd episode of the Weaviate Podcast with Roman Grebennikov and Vesvolod Goloviznin ... \nhey everyone thank you so much for watching another episode of the wevia podcast I'm super excited about this episode we're welcoming Erica Cardenas for the first time on the wevia podcast Erica has recently been accepted to speak at Haystack us 2023 about we v8's ref and recommendation and all these exciting topics so Erica firstly thank you so much for joining the podcast thank you for having me and we're I'm also super super excited to welcome Siva and Roman from meta rank meta rank is a super exciting uh ranking software and the watch their Haystack talk as well and there's so many exciting topics that I'm just so excited to get into I think this will be just an awesome search and recommendation uh podcast and that kind of topic so um maybe it kick things off Erica could you maybe talk a bit about kind of like the weevier perspective on recommendation and ref to back and kind of like sort of like where we're coming from in this topic yeah of course um yeah so we really struck to back um a few releases prior um so what this does is it really works well with recommendation um so what it does is it's factorizing the user along with its interactions with products or movies um so but it why it's called raft effect centroid is because it's taking the average of the uh interactions with the products or movies like I said um so what it does is it's creating my user digital profile shall I say by taking the movies that I'm interacting with so let's say I like sci-fi and rom-com it's going to average those two embeddings and then my digital profile is essentially just the average of those interactions that I have um so yeah this is great with recommendation and it it what's awesome about it is that it really works um quickly um so all it takes is a few interactions right for it to create this digital profile just to make sure that the recommendation is accurate um to the user and you're not kind of recommended recommending things that they don't like um so in addition to that the benefit of this approach is that um it can be characterized from its actions and relationships so the kind of um yeah just like I said the recommendations are clear and personalized to that specific user um so by aggregating their cross-references it allows me to be to immediately learn from the user's preferences and everything and how this it ties into using Vector searches that you can use the near Vector filter um or yeah use the near vector and type in my digital profile of my embedding and then it's going and then you can tie in symbolic filters of let's say I kind of want Sci-Fi movies that are uh were produced after 2015 so it's digitally appealing and it's not like the old stuff that no one kind of wants to see anymore um so this also ties into going to generative search um so let's say from my movies example if I want it to summarize like five new movies that I'm interested in um it can kind of do that right because it has my digital profile along with creating maybe um like along with um I don't know if you guys think like with gpt4 it will eventually create content so how do you really filter that out and make it like user specific and there's definitely ties into meta ranks with all of this content and like an abundance of information that users will eventually have how can we rank that and you know make it more user specific I have a question regarding recommendations because every time I hear embeddings and recommendations I'm triggered uh to ask a question how you compute this embedding so technically is it like a matrix factorization collaborative filtering or yeah I think if I'm hopping on this quickly um yeah I think right now we just have um like we vectorize all the products or the movies with clip and then we you know the user likes these movies you know like these three movies and so we just average these embeddings and send them back to the user yeah I think I yeah and I'd love to talk about how this extends to collaborative filtering I think that'll be a super interesting topic but I think um that was a really great background Eric and I think that really sets the stage for kind of our interest in recommendation I think also kind of ranking this topic is of course relevant to search in general where you know they will get into these kind of things but to pass in like uh Roman and Siva can you tell us about kind of like the founding story of meta rank generally how you're seeing the space yeah maybe I can uh I can chip in because probably later on Roman will do the main token because he's more technical uh than I am uh we've been working closely with Roman for more than uh seven years already I think um we've been working in an e-commerce startup before and uh the startup wasn't was in the search and recommendation space it was like a it was used in elasticsearch back in the day and you know the recommendations were based on collaborative filtering uh like really simple algorithms but they were working kind of really well and you know we've had quite a few customers uh it was a success story of a startup I was acquired uh back in the day uh and uh actually what Roman did in there is working on personalization how do you actually personalize search results and later on category and collection Pages for for e-commerce Stores um can I was kind of managing Roman a bit uh it's kind of that's how we came came up together uh with the idea of having an open source uh engine that can do personalization uh and do personalized ranking kind of for everyone that is not specific to e-commerce or attack or kind of any other vertical that you can easily can I just take uh drop some configuration drop your events and then kind of it runs the personalization you don't need to know uh a lot machine learning stuff for data science okay you don't need to set up complex uh data pipelines kind of use serious Stuff Etc uh just take the tool it works almost out of the box kind of that was kind of the idea behind metric and uh um yeah maybe Roman can kick in now because he's been working mostly on it from the code perspective yeah so as uh we you mentioned Haystack so when learn to rank was a hype thing like for in 2023 it's not anymore like uh like on a hype but more like a commodity but back in the days if you hear all the talks related to ranking they're kind of you only change the company names but the general like okay we did some sort of feature engineering this time of labelings and back and forth and then we throw everything to xgboost and got our Improvement in conversion you you can just see that the companies are doing the same thing but it usually takes quite some time to set up all this data processing pipeline because throwing things on exit boost is easy Computing the things especially in real time is not so we decided to make it like a commodity you know so how this how it was with Lucina originally because to do search back in the days you need to be you know like a Java specialist knowing how inverted indexes work and do this crazy things with the leucine API which is obscure enough and then solar and elastics search came and then no one even knows how Lucin works you just throw jsons to elastic and it works uh eventually this commoditization is happening for my own experience and my own opinion even with this Vector search because like hnsv is 2016 or something so can you do Vector Research in 2016 why not uh but did anyone build Vector search then no because you need to have you know PhD in different things to glue things together and now you throw jsons to maybe it and it works yeah so I love that um the you know the the whole design of meta rank I'm pretty familiar with it I started looking into it the kind of how you pass these Json dictionaries for the user features and the item features and maybe I mean there there's quite a few topics I want to hop into with this with because metal rank this whole like ml Ops stack around ranking could we maybe touch on that topic a little bit yeah so it's not about some basic features about items and users like this item has this price this user uh came from an advertisement there are usually some more stateful complicated features this item has a click-through rate of something over the last seven days and it quickly becomes quite complicated if you want to do back testing and training so you need to have some feature logging if there's two ways of computing features like offline online and if you want to do something more complicated uh I don't know embeddings like do some cross encoders it also complicates things but and at the end it the same thing so okay take mini LM from sentence Transformers and feed it with choir and with a title and compute the similarity so it's the same thing people are doing again and again so we wanted to make it also a commodity like you throw jsons to map Rank and it works listen on this topic so seven Roman you guys really touched on making metal rank user friendly and easy for people to use right it's like very abstract didn't really know what's going on under the hood it's just like as simple as just doing a Json file so um how do you guys like how do you guys prioritize the user experience in this case and what kind of approach do you take to that originally we took Canada um we tried to build the tool that we can use ourselves it's like me as a like I mean I have a development background uh although like I'm a manager now but can I still can can do some coding uh but I'm not versed in this all the metal stuff for data science whatever um it's gonna I was the test guy uh who could kind of go into my take match Rank and do something with it like we have this uh uh movie lens uh data set that you know we use in our demo and kind of used in several talks as well uh that will build specifically to Showcase personalized ranking and the idea is that me as a regular developer can take metric can run some comments can I can I read the docs can they follow the steps and it can they in the end get personalized ranking uh kind of we took originally this approach um it worked quite well although originally metric was kind of built with a quite some uh specific Technologies and in the back so you in order like if you want to run it to run it locally uh it's it's not hard kind of you just it's a Java app you run it but if you want to run it uh let's say in kubernetes uh it became one hell of a job because uh because of the technologies that we've used inside metrics kind of we've focused on simplifying that stuff we focused on simplifying like completely removing uh databases having like in-memory storages so that you can run it locally um but yeah like like and uh after that can I just get a feedback from the users uh the companies we have some several test pilots uh let's call them like that so companies from different verticals that try to use metric they have different use cases uh different uh amounts of data even like uh once like some companies have maybe like thousands of data points others have millions of data points like how do you optimize for that uh how to make it easy for them to use in different use cases kind of so test it on us and then uh getting feedback from early adopters but usually early adopters came and have some so obscure ways of using meta rank you never even thought that people will come with it but if you start thinking and just go back one step it's kind of reasonable and you just becoming a you get a better understanding on how it can be used so we're always focused on implicit feedback for the ranking like users clicks on items and we optimize based on that but in some cases you have explicit feedback so and for large companies that's kinda typical for example so you can have analysts who will just label the search results manually for top 1000 queries why don't you use it but with metal rank no way and so just speaking to people that's why we have slack and people usually go there and asks an absolutely weird questions can you give us an example of one of those or is it all covered uh there was a guy who came there and asked uh what about reinforcement learning and I was like what about reinforcement learning so can you do reinforcement learning it like I know in theory how to do reinforcement learning for a search but I never did it actually and this guy like what I did I will tell you and started just posting their like a long reads on how to deal with the different links and no so what do you think and click or below that's interesting experience yeah it's really interesting I mean maybe I think like recommendation on this topic a little bit I thought a bit about how it could be a reinforced and learning problem where you know maybe you make a sequence of recommendation decisions to ultimately achieve something like the guy I think about kind of like The Tick Tock the like video platform where maybe if I show you video a c d and e you watch eight for seven seconds I've already forgotten but what I just said the next one for three seconds and then for five seconds and then something about that leads you to watch the last one for like two minutes uh maybe a better example would be like kind of the education case like if I'm recommending you educational resources I like this example a lot because but let's say I have like five parts to my chapter and I also have a summary of the five things if I'm trying to recommend you something that'll probably lead to the average case best score I'll probably just out give you the summary but if I give you these five in sequence then so that kind of thing you have a sequence but to take a step back I think maybe if we could sort of explain I'm worried that we might have dived in a little too quickly for the podcast if we could just kind of explain the difference between retrieval and ranking I know it's a little it's like I know everyone here already knows it but if you guys could just describe how you're seeing the difference between these things oh I can I read re-read the splayed and kohlberg papers today so I can give you like a long lecture about an issues of retrieval but if he if we speak about matter rank it is a gestural ranking thing and it's not a silver bullet because it's heavily depends on the retrieval side uh so for some cases if your retrieval is very very focused on Precision like you search for pizza and it found single Pizza whatever maybe there are some margaritas and Quattro for mojis somewhere but it's not in this results it's focused on a Precision we got like a perfect Precision like one document and that's relevant documents so nothing wrong with the search results from the Precision perspective can you improve it with ranking probably now that's just because of one document and the search results so you need to balance between precision and recall so a little something which might not be 100 relevant but there will be just more relevant results overall but where is the balance that's the good question so I don't know for elasticsearch it's uh if you have a long queries you can combine them with or so it at the end it will allow you to have some better recall in the cost of uh noise in the search results uh semantic search and Theory helps like solves this particular problem because for example if you're searching for pizza it's just single term like you can't have pizza or or what or pizza uh so with with semantic similarity it will match also pepperoni and all that things can you explain more about the history of ranking with keyword features like exactly like a pizza and then like um so I didn't get the question like ranking with keywords is well yeah I guess my angle is like how do you combine like with the text features such as like with bn25 the BM 25 score along with like an n-gram matching like how do you rank with that ah okay so yeah Roman maybe we can also touch the point about multi-rich multi how do we call them retrievers yeah yeah but I will start with the usually how it how people rank when they don't have any feedback so for example you have n grams you have BM 25 scores you can't have some cosine distance between embeddings and so on these are just characteristics of your uh items so you can just throw it as a param as a ranking factor to the Lambda Mart model for this like literally just throw things and text to XG boost and hope that it will work and surprisingly it usually works so there are some approaches even to do it within elasticsearch like playing with boosts so there was a talk on Haystack a couple of years ago about learning to boost and there was like an almost analytical solution to the problem of how to optimize boosts not like randomly but you literally just compute which boosts her minimizing pairwise loss across your click-through history which sounds very smart but at the end it's just logistic regression and you got your numbers like put this boost there and you got your nice ranking but in some cases for case like multiple retrievals retrievers sorry when you have elasticsearch for Terror matching and I don't know via V8 for uh semantic matching and then you can intersect so these two sets of results in a single ranking so if document comes only from term matching then it gots only bm25 score if it comes from vaviate and elasticsearch then there are two ranking factors like your bm25 score and your cosine distance and you use it so if you have only two ranking factors you can go without all this complicated uh Lambda Mark methods but if you mix user Behavior I don't know like a click-through writer or maybe a click conversion rate or maybe length of a document or length of your query then a number of ranking factors goes up and you can't just mix it with logistic regression that easily because it doesn't become more stable when you have more ranking factors yeah and uh kind of when it comes to match rank uh from the metrics user perspective uh it's the same thing you just have some yaml configuration for those features so you have bm25 as a feature uh for for the Lambda Mark model you have uh let's say cosine similarity as as a fee as another feature maybe something else as another feature and uh metric does all the all the work for you for kind of combining those features together combining the scores of your items like if you have a set of results from multiple retrievers Metra and combines those uh kind of the the scores of each Retriever and uh kind of does all the smart things puts it into the model and can you you get the optimal uh ranking in the end uh that kind of increases your CTR or whatever uh whatever else it can increase can it depend depending on your kpis and your business goals um kind of that's kind of where we also kind of where we try to make the life easier for the people and you don't need to think how you match stuff together in case of multiple YouTubers for example but I got the impression that all this ranking optimization are kind of a complicated topic for people so if you are attending Haystack conference probably you know what's that but if you're trying to explain to your grandma what is the ranking and or just you know you pitch investors like we do re-ranking and they're like you do what uh so we're considering to make it a bit more a human friendly so we also do recommendations because it's also kind of a not completely similar but uh given the data model we have for the input events it's quite easy to implement recommendations on top so you you sell metric it's just a way to have a contract about type of different events like segment IO does for analytics for example uh net rank does does for ranking and recommendations so that's an interaction that's a metadata about item that's metadata about user you just throw it there and it works and you just tune the parameters of different models so from my experience doing machine learning is usually like five percent of all the time you do while building ranking or recommendations and 95 percent you just struggle and cry around different data pipelines how to compute these features is computed incorrectly okay that's a nice article released let's try it so um but it would be nice if the contract stays the same and you got all this improvements just for free yeah it's another kind of fun thing about recommendations is that can originally metric was only about uh re-ranking and kind of personalized through ranking and uh we did a hack and use lunch about a year ago and people were kind of really excited we were not expecting uh to have that much response and kind of people right in US joining slack Etc but everyone was asking about recommendations because whenever can you talk about ranking and personalization everyone thinks about recommendations and can I finally about like two months ago probably we've added the ability to calculate recommendations uh into metric as well and that's also kind of based on the user feedback that we got because um when it comes to recommendations uh there are a lot of different algorithms there are a lot of python libraries that you can that you can take to implement those recommendation widgets in your in your store or in your web app on your social network uh but in order to utilize them you need to be a python expert you need to know how you need to have data pipelines and all all the other things uh to hook that up and display the recommendation widgets with metric you already have all of that built in and thanks to like we've already have had that as well that's why it was kind of easy for us to to add those kind of recommendation Generation stuff into metric yeah so I think this topic is just super interesting this kind of like there's this kind of topic of like multi-vector representation of objects like if I have you know a book and I have a title I have a abstract let's say and and I have like content I would have a vector for the title a vector for the content Vector for the author on like multiple vectors that represent this object and then when I'm ranking them I would have like you know the vector similarity score for the title uh the BM 25 score for the title maybe also these engram features that Erica mentioned and combining all of this with an XG boost model my big question with this kind of approach is do you then kind of sacrifice out of domain generalization like by fitting it with this kind of model do you just hyper focus on this data you have like how are you thinking about the generalization of this kind of approach uh so in text search uh if we just speak about text search that you have only query and a document and that's it uh it's hard to generalize but usually you also have Behavior so okay this visitor interacted with this so was presented with these documents and you have some sort of a bias in this training data because people click more on the first items and then uh this item was displayed after that item so there is some sort of pairwise difference so it's just practically quite useful to do so and it still tries to optimize uh so on this interactions between the features like you described the 10 grams they can be non-uh not linear so it's hard to find some linear explanation for them like put them in a logistic regression and hope that it will rank properly um because for example BM 25 score is unbounded to the top so it can like one or five or 55 or 5000 that's possible values what you can't just easily normalize it and you so it quickly becomes quite complicated for normal normalizing different ranking factors and if you throw everything at exubus it just handles it automatically yeah super interesting I think um maybe staying on the XG boost style I'm super fascinated with the way that you've implemented the Kafka streaming and how you estimate the click-through rate within a window can you maybe talk about that kind of system design for how you get that feature with the streaming data because I think that's just a super powerful part of this so that's uh that's a long story so Sarah mentioned that we did some uh design decisions originally so when people try to run Metro rank somewhere they found that it's kind of a very complicated thing so we use the patch of Link before that for the streaming all the streaming stateful streaming things and it's nice when you're a large company with 1000 customers uh to use this type of streaming framework but if you are just a small to medium Edo Tech provider you don't need this type of heavy or artillery for data processing you don't you can process it on one two nodes you don't need a cluster of 100 nodes but being able to to scale you sacrifice the simplicity and some flexibility so we struggled quite a lot using a bunch of Link we try to solve this problem of complexity with documentation and when I wrote the deployment kubernetes deployment guide for meta rank which was like you know like a Bible so huge so long with all this you need to install this then you need to have kubernetes operator to control state of the Apache Flink jobs then you do a custom resource for the job that you deploy something there and you need to be like a professional devops to run it um at the end we just removed the Apache link and we wrote all this data processing pipeline into something more reasonable at the end it's just not really Java but scalab but whatever GBM app you sacrifice we sacrifice scalability but still it depends uh on performance so uh for this click-through rates uh one of the first complicated features in Metro rank but I remember that I in my previous couple of companies I spent quite some time implementing it properly because you don't only need don't only need click through rate uh right now for this period of time if you do back testing you need to have to be able to answer for question what was click-through rate for this item half a year ago for the seven days and you're like um that's you know that's possible to compute but uh imagine that you have a couple of millions or maybe billions of different search results and Computing it one billion times quickly becomes unreasonable so you do a lot of different tricks and hacks and all these hacks are part of Metro rank so it's maybe my fifth attempt to implement this rolling window click-through rates so technically uh this click-through rates are aggregated within a bucket so like one hour or maybe one day it doesn't really matter so um they are aggregated not like the CL the actual rate so you count number of uh for example interactions like clicks and number of impressions in a rolling buffer of uh of this where bucket is like a period of time and then uh from time to time eventually depending on the your load not every time like one once in an hour you divide one rolling buffer to another rolling buffer and get your rolling CTR and then you aggregate for the periods like one day and so on so we spent quite some time so originally we even had some something like a feature store implemented by yourself on top of a bunch of Link uh but it was quite complicated to do some stateful things on top of that and at the end we get rid of it and simplify that quite a lot but uh as a fan of test driven development it was like a pleasure factoring so you you have so many tests and you just make them all of them pass and at that moment you can release yeah sorry guys I need to plug in my computer everywhere yeah no problem oh some of that okay great I I do have a follow-on quest so let's go awesome so I think this kind of click-through rate this discussion of feature store is really interesting I'm really excited to have this chat and learn from you about this because I've been thinking a lot about how do we integrate with leviate with meta rank what would this kind of thing look like and maybe as a quick background so we we have symbolic properties but uh I don't I don't know how well that interfaces with this kind of CTR online estimation thing I listened to your talk where you talk about um you know cold data postgres hot data redis and then or does or does this just live in meta rank where in you know you as you mentioned like one thing that's so cool about this is you've done the the kubernetes like it's like an API that you know with weave yet I can just kind of send my data you know API git score back so like how how would we integrate sending you know integrating the properties because we do have metadata and we V8 how do we integrate it with meta rank so I think that's not the way we should be integrated with metal rank but uh the on the custom the integration should happen on the customer side so you send your it's like an analytical events you send to segment IO like user actions uh you got your inventory update and this item now is in stock or your title is changed or Price changed or you got a user new user registered and you know something about this user I don't know country or age it's also just bits of metadata and you throw these events into this into the API or into the Kafka stream but like with segment or Google analytics just some specific types of events you notify that this happened at that moment and meta rank stores this huge logo of all this metadata changes and also you send events for okay I tried to display this search results or maybe recommendations to this visitor and then visitor clicked on item number three so you sent rankings and clicks and then you have a very large click-through history with all the metadata for each item with the timestamps and then you can Replay in meta rank so you can configure different types of feature extractors like click-through rate and then you just replay all this history of events with your new features and you get your back tested results with all the features recomputed even if you change something so uh but uh we see a weight of integration between Metro Rank and vv8 for recommendations because how we do recommendations right now is that they are collaborated filtering based ones and in the case if your uh embedding is huge uh we're just storing everything in Ram in you know the station as we live nothing fancy and if you have a lot of items you can't really store this in Ram because it quickly becomes expensive so we have a way to integrate with different Vector search engines by offloading this embedding so how it looks like so you're not Computing these embeddings on the side so customer just sends analytical events okay that's a user that's a click that's a ranking meta rank periodically computes all these embeddings and dump them to vv8 for example and then if you ask for recommendation it just also like a proxy for the vv8 but also Computing the the embeddings by itself it can do clip embeddings for movies but it can do collaborative filtering embeddings for uh interactions it can do content embeddings for text so two weeks ago I took part in hackathon to build this type of functionality so it's not yet so it's in the master so but not there's no stable release for that but uh you can do any types of embedding so we can do sentence Transformers now and something custom if you just upload a CSV there with embeddings and then it will just serve them either from memory if it can fit memory or from some some bloggable Vector search engines if it can't yeah I also can in terms of kind of collaboration with the other companies we did um can we try to collaborate with open search because they've recently released the ability for external re-rankers to be plugged into open search itself um but it's it's still kind of uh it's only integrating only one part of of Metron uh only the retrieval part the ranking part but we still need to send analytical events anyway so kind of we don't see yet a big benefit of uh having some sort of like a plug-in that will do everything for you because you know because of the way metric is built kind of this kind of two there are two ways you need to use to to integrate with metric like the retrieval and the the analytical stuff um it's hard to build a connector for for the LA for external libraries uh kind of to integrate both of these parts um it's kind of with uh kind of we haven't invested invested yet into building a uh a plugin for open search um so that's why I kind of where we're trying to concentrate on the uh on like how you can utilize different tools together uh like a separate uh applications and kind of this hackathon that Roman mentioned and uh kind of the the stuff around embeddings that it built into metric it's not public yet can we haven't released it yet and there is no documentation yet uh but it's coming hopefully soon there is documentation if you go on our docs there is a like a drop down there and you need to choose like not not the stable one yeah the unstable docks yeah yeah but usually no one goes there it's good that you have the two separate attacks yeah because what we've noticed as well is that sometimes people uh kind of look uh in the old docks like they're using the old version of match rank they're using the old version of matrank and they're using the new Docs and that was the original problem of having like separate branches for the docks as well I guess on the topic of collaborative filtering and also like click-through rate and personalized recommendation um I guess how does this solve the like cold start problem like I'm new to a website but um maybe there's some metadata on my interactions on Google um how do you guys handle that I guess if my question was clear enough yeah but uh it depends on what you count as a information about you so when you just land it on the website we can already know are you on mobile or on desktop from which refer you came not like exactly refer but probably it's like organic Google search advertisement or maybe Instagram link on this type of granularity uh your location maybe time of date the time date and so on that's still content context and even with this amount of information oh and the page you landed to so if it's just the top page that's one thing if you land it on a specific product probably and you came from Google probably you search for this product on Google and clicked on a link so that's also an interesting bit of a con context about you so even without interactions uh any website can know quite a lot about you but not like you know privacy sensitive information but more like an aggregation so okay you came here we have no idea who you are we don't know the name your click history nothing but just some hints that your came from mobile from us and the middle of a Sunday night uh from Google and game on a specific product that's enough to adapt at least to see how other people like you interacted in the past and adapt yeah and uh yeah maybe also depend on top of that um in the company kind of previously worked in we we did quite a few a b tests that showed that uh kind of this approach really increases the conversion eventually because I'm gonna yeah of course kind of you're a unique person uh but when you aggregate you among like 100 000 of other people you have similar behaviors and uh the ability of uh of the personalization model to adapt quickly uh to what you do on the website uh yeah kind of you as Roman mentioned kind of we already know where you came from and kind of what's what's your location but as as long as you click a few items well with no kind of interests uh kind of what type of products you you want to buy or whatever it's gonna all of that can is directly ingested into the model and uh in almost real time you get a personalized ranking afterwards so you don't need to know like really a lot about a person uh to start personalizing as long as you have like the moral build as long as you use metric for example yeah I guess I always forget how much information is just um like how you can create like this digital profile so easily just like boom I'm on the website and now I've been I'm not as unique as you said right like I'm as similar as someone else's behavior and you know it is personalized in the products that you recommend are accurate amazing there is some sort of a balance between privacy intrusiveness and like business values so Metro rank tries to be as generic as possible so for example a long time ago we not not through it's not related to Metro rank but uh I've seen people doing it that you can integrate with the DMP type of Provider providers to get some more granular information about what you did before on other websites so advertisements as advertisers usually not only know this information about you like you know Facebook um they know your interests and you can purchase this type of segments like to which segments you belong and so but uh it's usually used for ads and there is some a couple of floors DMP platforms like from Oracle and so on and they on Oracle that was a link somewhere that you can go with your normal browser to unsubscribe like just opt out of tracking and there you can ask for the data you have for some reason this link is down for a year I'm just trying to do it again and again and it's still kind of temporary problems come again later and that's um but when it worked it uh shown you like a segments uh for like a type of third power party tracking and that's very privacy intrusive for me so I'm against using it and matter rank is about first uh for like a first party type of tracking so your we have zero known about you when you came on the on the website so okay you're on a mobile from UK that's kind of it but it's not that privacy intrusive as knowing your gender based on your searches on Facebook yeah I'm really curious well this kind of brings me into this topic of like the rank lens demo that you have and sort of this General topic of recommendation data sets I think um like what do you think about sort of the user features that are captured in these data sets are they realistic compared to the applications you work on I think uh probably like a small remark is that uh all the data sets uh that exists really suck because uh usually they have let's say like information about the items maybe some information about the searches and zero information about the the actual users that did the search so it's almost impossible to do uh like a ranking demo uh because there are no open data sets that have user Behavior embedded in them and that that was the background why we had to come up with our own data sets can I spend our money Etc um use crowdsourcing uh engine uh to Locker AI uh to to help us build the the necessary interactions uh for the for the data set as well Roman what do you think yeah so for search it's still problematic there are some data sets but they are usually very text focused or maybe in a very specific domain like a question answering in a medical search which is nice but not e-commerce usually where the money are um uh last year there was this Amazon EA es CI data set released which I'm really big fan of the problem of this data set is that there is not so it's also text to to focused on text you have like a search query product title product description that's it uh uh and uh you can't do much on this data set with meta rank even if it's wonderful on its coverage it's uh 150 000 queries referencing almost 1.6 million products so until this and explicit labels for all of them which is very very expensive to build and it's I'm glad Amazon open source this type of levels but you don't have metadata you don't have anything about users uh we can't do anything about users because it's Amazon but we can do about metadata because the product ID and the caci data set is Amazon product ID this Asin so you can scrape uh kind of scraping 2 million Amazon products is a tricky process but we actually did it we won't say that we've done it we won't say yeah we've done it so someone else did it and formally formally uh that's a good question from the legality so there was a court case LinkedIn versus someone I forgot the name IQ something who scraped LinkedIn and even they scrapped like a private date on LinkedIn your contact date on LinkedIn and still they won that it's kind of a still not really private data it's still open so we didn't scrape any private data we didn't uh use tiny tricks to uh sneak into the locked in section of the website it's just what Google Sees like Google can scrape Amazon why can't we scrape Amazon like like the same in the same way of Google so there is an extension for the csci data set from from me personally to with all this metadata like prices uh product popularity review score number of reviews and all the structured information about and categories which is also very important so the end goal is to have some sort of a search demo of meta ranks but there was no data set and then Amazon esci came but there is something missing but now it's now it's better yeah but still it kind of misses the the user metadata so it's kind of hard to do personalized ranking demos or uh like tests even um and that's maybe like if Amazon will collaborate with a Mechanical Turk for example which is an Amazon service they can produce a data set that is kind of labeled with users as well with user information um you know as I mentioned earlier that's why you know we had to build our own data sets and kind of we've made it open source as well for everyone to use uh but it's kind of it's hard for us to to make it popular among among people because we're small okay a small company and still it's not that big of a data set as well you know it's going to focused on the movies so not e-commerce so I don't call it a company it's just two folks doing open source technically so company assumes that you make money on that and oh yeah this is such an interesting topic um when we were prototyping ref I had you know gone to Twitter and sort of similarly used the Twitter API to kind of like reverse out their recommendation a little bit like you can get some tweets out that way and then you can see how well averaging the embeddings of like tweets like for the case of our ref thing uh so this last question I have for you is a pretty big one I'm very curious what your opinion is on these cross encoder models that are like take the query and document as input to a high capacity Transformer and uh particularly there's kind of well maybe actually let me just set it up just generally like what do you think about these kind of cross encoders compared to the XG boost style models that we've mostly been talking about this cross encoders are not again just supplementary theme like orthogonal so it's not only about cross encoders as is there are some other retrieval models like Colbert and all this modern stuff that's played and it's just extra information for the final ranking so you just combine different signals from different types of ranking algorithms uh together trying to push your ranking quality further so um I think cross encoders are wonderful uh but uh I got an impression that people uh consider cross and colors like some rocket science oh to train across and go to fine-tune cross encoder you need to have a team of data scientists to do which is surprisingly not the case uh so it's quite easy just you only need to have GPU what BT GPU but that's the only thing you need and but for example for for the sentence Transformers package everything is implemented for you and it's very nicely documented and different approaches but different pros and cons and even examples on how to fine tune on Ms marker for example just blog your own data set and leave it for for a night hmm yeah so that's what I'm actually doing right now you mentioned that everything was uh you were accepted on the haystack uh me too so we'll we'll see each other there so I'm trying and so that I'm going to speak about this particular type of uh particular problem of combining learn to rank and uh term search metadata and Vector search so it's usually not about choosing something like okay we need to use only Vector search instead of elastic no you shouldn't you really just throw everything together in a giant Ensemble model and it usually works better than single separate one also congratulations on getting accepted I'm looking forward to your talking yeah thank you now I need to prepare it that's the complete the most complicated part yeah uh yeah fantastic I think um yeah well I I love that perspective on Cross encoders as just another signal I I think it makes a ton of sense I guess kind of one more little thing I want to add to this is kind of the idea of the large language model ranking and whether you can you know like distill that into another feature or use that to train the cross encoder or the or this kind of thing but like the idea where I might have like you know let's say it's Conor watching movies and I have a description of Connor like I kind of translate these tabular features into text and then I use like query tabular to text translation and the current movie description like you know all that goes as input to the gpt4 fireplace to like get the ranking what do you think about that kind of like what role do large language models have in ranking uh I think the larger the model the more context it might handle not only about your query and your products but just some common sense understanding about things so in this hackathon we did a couple of examples of semantic uh recommendations for movies it was just movie lens data set with the different algorithms of recommendations like collaborative filtering semantic embeddings with mini LM like tiny model compared to Chad GPD just like you know minuscule and it was embeddings from cohere AI which are quite huge as far as I know it's 100 something billions of links and the embedding itself is 2 000 something the dimensionalities like 2000 48. so that's a huge one and if you start clicking on movies and see what was going to be recommended to you you notice that sometimes the small embedding and large embedding are generating almost the same thing uh in some simple cases so I don't know you go for a Terminator movie you might guess what you will get on all the algorithms because it's quite an easy task you don't need a lot of context you need to know what robot is and like robot killing people that's it um but a wonderful example was about aliens there aliens movie uh so collaborated filtering suggested something other people like yeah it was something about space and some some creepy movies uh and to this large embeddings from coheri also was about different types of aliens which are usually not very kind to people so a bit different types of movies than collaborative filtering but still in the same area and the small embedding which has a very limited amount of memory and the amount of context it can handle it found a couple of alien movies from the same Frenchies then it decided okay that's actually people escaping from something why don't you put movies about escaping from a jail the same thing and yeah there's a movie called a man called Ripley Ripley is so unusual name probably that's the same Ripley so semantically good match but uh with the 80 megabytes 80 millions of Connections in the network you can't just learn that there can be multiple ripleys actually and you can there are multiple types of jails and some jails are flying in space and there's a bit different thing than jail on Earth so I think the larger the model the more descriptive it is but it usually depends on Hardware so kohiria is like one dollar per 1000 embeddings and if you need to embed 1 million items like esei data set you need to to you need to spend two thousand dollars on that so quite expensive for for hackathon type of project quite expensive and we need to do it periodically because your items are changing in time so it's about resources yeah amazing well I thought this was a great podcast Roman Siva thank you so much for your time Erica for joining the podcast for the first time and um yeah it's such a great dive into these topics around ranking I think it can kind of the Cross encoder can seem so simple just this kind of like query document and there's distinction where retrieval is like this kind of coarse grain surge with the vector index or inverted index with the bm25 things like this and then we have this more fine-grained high capacity model and then kind of transitioning into the XG boost with the metadata features and all the things you're doing behind I I think it's so interesting like as we talked about the streaming data to estimate the click-through rate and and yeah just the whole thing is so interesting so thank you all so much for joining the podcast yeah thanks for having us thank you thank you for the invitation ", "type": "Video", "name": "Erika Cardenas, Roman Grebennikov, and Vsevolod Goloviznin on Recommendation and Metarank - Pod #43!", "path": "", "link": "https://www.youtube.com/watch?v=aLY0q6V01G4", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}