{"text": "Thank you so much for watching the Weaviate podcast! We are super excited to host Sam Bean from You.com! As well as ... \nhey everyone thank you so much forchecking out another episode of thewevia podcast this is going to be asuper fun discussion we're talking aboutyou.com the spark connector withweeviate and generally billion scale BigData Technologies and we V8 and allthese exciting things around searchinterfaces and so on so firstly I'm soexcited to welcome Zane and John for thefirst time on the web podcast thanks somuch for joining the podcast guyshey everyonethanks Tonyawesome so I'm also super excited towelcome our external guest uh Sam Beanuh Sam is working on you.com Sam thankyou so much for joining the podcast anduh can you tell us a little bit aboutwhat you.com is and um yeah maybe wecould just start from thereyeah thanks for having meum so you.com is a um a newer searchengine it is aum kind of started as like a a play onhitting the niche betweenpersonalization and privacyum we've since evolved the the missionconsiderably and just recently we kindof announced uh the latestum big kind of press that we're makingin the search space which is kind ofdemocratizing search and what we'recalling the open platform and so this iskind of like a mixture of some developertooling uh DSL and it allows basicallythird-party developers to createum hook up their data and create searchwidgets and applications as we call themum that can show up uh natively on onour search results page soum we kind of you just tell us like alittle bit about what keywords it shouldshow up for and we handle like theranking and the retrieval and andrendering all of that and so reallygreat discovery tool for third-partydevelopers who want to kind of plug intoa search engine so it seems like the thewhole kind of search interface is beingredesigned at u.com in addition to ofcourse like the retrieval technology canyou tell me about like more about thewidgets and sort of how you're thinkingabout how people interface with searchengines might want to obviously like youcan generate an image you you have thesethings for you know having the codeassistant tool you tell me about thethinking behind the interface yeah sureso I think that we've seen apps kind oftake a more prevalent role in the searchspace like you probably haven't calledthem that on on your your main playersin this first search space like Googleor Bing but you you've seen them likeyour side panel Wikipedia panesum those are kind of all things that wecall an app which is basically just away to take our search data and curateit and decorate it in such a way to kindof give it like a special presentationand enrich the user experience a littlebit more than kind of your your standardBlue Links are to give a betterexperience and and basically what we dois we operate like a a very partitionedum index and so basically we are able toum reach into specific slices of theinternetum you can think of them as likevertical slices of the internet insteadof being very vert or horizontallyum searching and then and then we know alot about the data because we crawled itwe've enriched it with a lot of likemachine learning systems that we haveinternallyum and then we can kind of like usingwhat we know about the data that is fromthat domain we can create like a muchmore curated experience for the end userand so things like our stack Overflowapp uh to your point some of our likeour code generation applicationsum are you right application are allkind of like very Innovative uh app sthat are in the search space yeah that'sextremely interesting the whole um justkind of the high high level idea of likea symbolic filter might be how we thinkabout it like in specific leviate landbut this idea of like I want to searchthrough stack Overflow I want to searchthrough Twitter Reddit like you thisinterface around picking the source ofyour information and searching andhaving that smooth integrationum so I I kind of first became familiarwith u.com with um I I first startedfollowing Richard Sasha's researcharound these kind of systems with theco-surge system it was this likecovid-19 information retrieval systemand it had this like brilliant diagramthat really outlined it uh can you tellus about kind of theoriginsstoryofyou.com yeah so I thinkthat umthe way the way that you hear is thatthere was there was Richard's had thisidea for this this kind of likeum this vertically integrated likesearch index uh for a while and I thinkthat he was working on it for a bet andthenum with his work at metamind UH beingacquired by Salesforce becoming uh uhentering the c-suite at Salesforce andthen kind of likeum doing a lot of like amazing workthere on on the TransformerTechnologies especially like Ctrl and Ithink Cove are two of The Shiningexamplesum meeting Brian McCann who's our CTO Ithink that they they realized that itwas a good time that Google's grip onthe search space was kind ofweakening a little bit because they'rethey are I think fundamentally theirattention is marred by what isdominantly uh more and more like amarketing companyum and so we thought that it was a goodtime you know thatum there's there's an opportunityum there's a lot of people around theglobe who are kind of wanting privacymore and more and sort of give peoplethat kind of ability to to have aprivate entry point into the internetum and also giving I think what what wefeel are some pretty fresh takes on onthe search experienceum it's a lot of responsibility beinglike the entry point into the internetfor a lot of peopleum and it felt like that was kind ofthat responsibility was falls on a fewselect players and the way thatumdata privacy was being handled for beingsuch a high responsibilityum that it was it was kind of like it'sit's one of our Central tenets that youknow trust facts and kindness are veryare very Central to to the company andso I think taking up the mantle oftrying to create a new entry point andGateway into the internetum with with some of those things at thecenter of it was I think largely the themotivation for for getting into thespace so that was kind of like a uhthat's a constellation of ideas of ofkind of how we got here but it was it'skind of a uh globetrottinguh tail yeah it's super interesting umand I really do want to come back intoprivacy and the focus on it and thatlike the particular details of how todeliver it and uh the new technologiesbut uh I think kind of a conversationwe're very curious about uh can you talkabout how you came across weeviate andthen we can dive into the sparkconnector yeah suresoumwe wein building kind of like our own indicesof the internet we knew that we wantedsemantic search capabilities we kind ofknew that the way thatum transfer learning and NLP wasadvancing and where had it had gotten towhen we had started the company wouldgive us a pretty unique advantage tobuild those capabilities as foundationalunitsum I think that obviously Google andBing have a lot of those capabilitiesum but their systems are largely kind ofbuilt off of lexical search and then alot of the semantic pieces are kind oflike retrofit onto their their retrievalsystems and so we knew that we wanted tohave these things as kind of like firstclass citizens from the ground upum that semantic search was going toplay a really important roleum and we were kind of like lookingaround forwhat semantics are databases we wantedto use and I think that very early wewere connected withum the people that we V8 and found thatkind of the the open source ethos was areally good match for this that it was areally good match for the company's kindof like background and being able tocontribute to things like this in thespace was really important to usum and so it made a lot of sense to tostart to invest in the ecosystem andlearn the Technologies I guess aseparate question I had before we getinto the spark connectorsum in the future do you see yourselfusing all the different capabilitiesinstead of just uh semantic text searchdo you also see yourself using semanticimage search audio search those sorts ofcapabilities yeah I think that I thinkthat we're trying to think ofespecially with where what we're tryingto do with our you imagine Suite whichis like a an image generator fromnatural text something that is veryum very popular as of this year thattrying to kind of create like this verymulti-modal search experience acrossimages and audio and text is going to beimportant and being able to handlehaving one system that kind of handlesvectorsin like a very Vector first way insteadof trying to like cluge a bunch ofsystems together which is I think howtraditionally you would have done thisum and I think thatespecially with what we V8 is doing withthe hybrid searchum that I think is going to be a areally big player for a lot of peopleeven if they don't even know it yetum because people who are like operatingthese search indices right now know thatyou basically need two different systemsright like there's no system that doeslexical search and semantic searchtogether you have to do that work andit's not simple code to write it is itishugely complex to kind of figure out howto like run those systems in paralleland how to how to stitch all of the datacoming back together and how to rank itall uh very holistically having a systemthat supports hybrid searchum at its core is is really importantand sojust something as far as likearchitecture Simplicityum in the search Space is reallyimportantespecially from a maintenanceperspectivemaybe we can talk a little bit about forpeople that are newum what uh can you talk a little bitabout what you use spark for at you.comyeah we use spark foralmost everythingum as far as like our our data ecosystemgoes it's all spark native so our arekind of like Eventing and logging systemon the back end is allum streaming Kafka with streaming sparkjobs which we use to kind ofum take all of our real-time uh datathat comes out of our system foralerting and machine learning use casesand then we flattened that out into likeum into a Delta Lake and so we we use welooked at like hoodie and Iceberg but weended up going with Delta because itfelt like what databricks was doing withspark was umwas just a little bit more fullyfeatured at the timeand sothat all integrates really nicely forthose who don't know Delta is basicallyjust parquet files which is a compressedcolumnar format that spark integratesreally well withum and Delta basically adds anotherlayer on top of that which is just thisbig transaction log that allows for acidtransactions on a data Lake which waspreviously impossibleum and then allows for things likeversioning and vacuuming and andoptimization and a lot of the thingsthat you can kind of do in spark etls tokind of createumyour your historical data Lake and thenwe also have a number of like for forour analytics for our a b tests for ourum machine learning data sets we have anumber of enrichment jobs that all runthrough Sparkand then even like model training we werun GPU accelerated spark clusters formodel trainingum for like data parallelism and modelparallelismum and then like I thinkpast thatumalso our entire like crawling system andindexing system and and web parsingsystem it's it's all spark soum a number of of cron jobs that areum just big scraping jobs that just dumpdata again to uh parquetand then a number of spark connectorsthat kind of read from the the data Lakeand then make uh then index all of thatdata making it kind of like searchablefor the end user and so from thatperspective when you're kind of likerunning in a very spark first ecosystemyou don't want to have like part of yoursystem writingum with kind of like spark connectorsand then you're running kind of like acluge of like maybe some python code orJava code that's also running off ofum running off of the same data butrunning in a different environment it'sjust going to double your maintenanceand double your your monitoring expensesum as far as like cognitive load andhaving to keep track of all that and soit made a lot of sense to with our eyeonum using we V8 for a lot of our oursemantic search and possibly hybridsearch use cases that we would need wewould need something like this to tofully integrate the the weva databaseinto our into our uh data ecosystem sowe kind of reached out and we we askedif there was anything we could do tohelp and uh as they say the rest ishistoryyou you mentioned um I guess this is agood segue into alleviate but what typesof data sets are you considering usinguh for Aviator I guess what's theroadmap for using wev8 with the with thestack that you have right nowyeah soumI think thatleviates data model makes a lot of sensefor from our perspectiveum the way that you can the way that youcanDefine different schemas and the waythat you can create uh the indices andand Define your indexes based on uh whatthe objects are meant that that graftedreally nicely onto our already our datamodel which was you know this this verylikeum vertically aligned search index wherewe already have these these um a datamodel per domain that we're crawlingum some of them are more General thanothers but for things likeum your data model for a stack Overflowpage is going to be wildly differentthan your data model for like a recipespageum some of the things are going to bethe same your title your descriptionyour url but a lot of the components anda lot of the what is going to be a partof that data model just quickly divergesand so that grafts really nicely to howuh leviate thinks about schemas and dataobjectsand meant that having a mapping whichwhich we did from a raw HTML page tothen a spark data frame and then beingable to map us that spark data framewhich we already had the code to kind oflike get us to that point and be able toMarshal that to alleviate data objectmeant that you could you we could thinkof very easily going from a web page toan object in deviate very seamlesslyusing Apache Spark great well it's it'sbeen interesting being kind of uh partof the channel and hearing a bit morearound the process and developing a spaconnector because I feel it's somethingthat not not many people do um a lot ofthe times uh you know they're createdone off and it's fairly infrequentso would you be able to talk a bit moreabout how you how you went acrossdeveloping the star connector and whatum what sort of learning resources youuse to create oneyeah obviouslyit it can't comes with umall of all the spark connectors aredifferentum because they each require a certainamount of domain expertise with you knowbe able to Marshal the data intothe end the sync as they call it inspark and obviously definitely was itwas not a uh one person's effort so Idefinitely want to shout out everyonefrom weviate who helped out andabsolutelyum Sam stalinga whoum was spent a bunch of time with mepair programming and did a bunch of thelift on developing the the sparkconnector with meum I think thatlargely you can you can start outbuilding a spark connector byfiguring out how to do it in a UDF or apandas UDF and then spark a UDF is likea user-defined function andit's just some arbitrary code that youtell spark you're going to Loop overthis data frameum we're going to tell you which columnsfrom the data frame are going to getpassed in and then you just execute thatcode per rowand once you can kind ofrun your youryou have your data integration workingin a UDF format it becomes a matter ofyou can do that forever if you want tolike at that point you're not going toget a ton of like the actual performanceimprovements because you can parallelizethat UDF in a massive way in the sameway that you could with like a like anative data frame writer but you'realways going to be kind of like makingchanges to that you're always going tobe kind of like cutting Against theGrain with what are the opinionated likespark idiomatic ways of doing things andso things like error handling thingslike checkpointing things likeum retrying a lot of the stuff that youwould use spark forum because you don't want to think aboutthat because it's hard and like you haveenough things to do on your plate thatyou don't want to think about like whatretryologic needs to look like in a datapipeline or or orerror recovery that you kind of delegateall of that complexity into Spark so itmakes sense at some point once you'redealing with a certain level ofcomplexity to try to graft all of thatcode into kind of like what what is theopinionated spark way of doing thingsand that usually means extending anumber of the the core kind of likeabstract classes that are available inspark so I like I said like the sparkdata frame writerum there's a number of examples that youcan look at it be it like neo4j orelasticsearchum I think that there's even someexamples that you can kind of look atbut there's just a number of of coreclasses that you need to overrideum but largely once you have that codeoperating in a UDF it just becomes amatter of grafting that into kind ofum the the objects that that spark kindof expectsum and then being figuring out how todeploy that code in a jar format into aspark cluster and then and thenaccessing it as a umas one of the writersbut it it really becomes a matter oflike you're either gonna have a UDF andyou're gonna run a a map you're going tomap that UDF over your data frame versusbeing able to say just like take thedata frame and I'm going to call it justdot write dot format wev8 dot save andthen you're you're doneum and so you kind of you take all thatlogic that you might have to maintain oror evolve inum in a way that is going to be moredifficult and thentry to once again like delegate all thatcomplexity into a central place which isthe the Wii V8 spark connectorand then as a community we can all kindof swarm on that and make sure that it'sthe the best in class for writing datainto eviate and then lots of people getto access it and it becomes very easy tokind of turn your brain off which iswhich is the point right make it reallyeasy to take your spark data frame pumpit into eviate and not have to thinktwice about it yeah what what I love uhwith the spa connector is it just makesit really easy to plug in different datasources so uh when we're doing thesphere test uh we could just read aparquet file and and load quite a lot ofsignificant amount of data reallyquicklyum you know other data setum other databases like reading fromCassandra becomes easyum so I'm interested to know you talkeda little bit a bit before about thepipeline we're using streaming in Kafkawith their data coming in are youplanning to add spark streaming supportto this connectorI think it's definitely we have an openissue for itum I think to to all of the people wholove open source there's an issue uh youcan open a pull request uh if you wantto go take a look at a really greatexample there is a um I think a neo4jtheir their library is set up so thatum you basically just need to ex it'svery similar toumto the batchum operation it's just like a differentuh a different API so you're using forbatch or using the smart SQL API or thetables API if you're in spark 3. forstreaming you're you're just you'reyou're operating on a streaming dataframeum and soit's a slightly it's it's not gonna be aton more work but I think thatumI think to answer your question yesthere's there's a plan to add it I don'tknow where we're gonna have time anyoneout there who wants to take a shot at itplease do because becauseum I think that being able to make dataaccessible in real time and we V8 wouldbe an awesome featuregreat thanks so I guess the other thingis Israeli the design Choice when you'reusing waviate to bring your own vectorsor to use one of theum Effect one of the modulespre-existing to create vectors uh andprobably when you're at Large Scale youwant to have more controls so you'releaning more towards creating your ownvectorsum do you have any insight around sortof how you're approaching creatingvectorizingumu.comyeah I can I can talk a bit about thatumso I think thatif you are going toif you're going to use the vectorizationmodules internally in vv8 I think thatis going to take a bit morethinking up front with how you're goingto do your resource planningI think that whenever you're talkingabout writing a ton of data and then youare simultaneously using the database onthe other side for a user facing featureyou're obviously going to have to thinkof read write isolation and then if yourdatabase is also doing yourvectorization you almost have threeoperations that you now have to isolatebecausewriting a ton of data and then trying tovectorize that data and then also tryingto read data andum for your users now those all have tobe isolated because your vectorizationcan take down your reads your rights cantake down your reads your rights cantake down your vectorization yourvectorization can take down your likelike all of these things now are kind oflike operated in the sameon the same computers and so I thinkthat when you're going to really largescaleunless you haveum if you ever if you have a bunch ofdevops resources then likego for it sounds like a lot of fun ifyou don't have a lot of devops resourcesum I think that the that error handlingand the the resource planning in sparkis probably going to be a bit easierum I think there's probably a bit moreum going wisdom on how to do that kindof resource planning that you'll find onthe internet just because I think sparkis used by more people less people areare using Vector databases right nowthan big data Technologies you'll beable to find a bit more on how to dothat resource planning it's going tomean thatumyour you you only have to think aboutthe read write isolation for what isgoing to affect your end users if thatif your vv8 database is hooked up toum some sort ofclient application that that people areusing which hopefully they areumand for us I think that we were able towe've spent a lot of time thinking aboutumthe best way to run these Transformermodels ourselves and so we've spent alot of time figuring out like how toquantize models how to run graphoptimizations how to like run like whatwhat is the most efficient way to runOnyx and in a sparkum in the spark environment because thatstuff isn't actuallyum doesn't work out of the box and so ifyou have that kind of domain expertisewhich we do then it makes a lot of senseto leverage that and and to figure andthen just use what you know about Onyxand what you know about modelquantization and stuff like thatum and get get the most out of that ifyou don't have a lot of that then thenthen maybe it does make more sense todelegate that complexity and to leviatebecause I know you folks know quite abit about what it means to to run largelanguage models as wellum so I think it it really depends onthe composition of your team and whereyour domain expertise lies I know that'sa non-answer and it's really along-winded way of saying it depends butfor us it makes a lot of sense to kindof we'll take the complexity of runningthe modelsum because of again what we know aboutOnyx and quantization in the huggingphase framework and using like thehugging phase Optimum librariesum and kind of write that code ourselvesand then we only have to worry about theread write isolation for our databasecan I ask a clarifying question is thatthe Onyx so it runs on the CPUs andthat's kind of what the spark frameworkis built around yeah soyou canso you can so there are actuallyFrameworks that you can run on GPU so Ithink cudf which comes out of uh Rapidsyou can actually run GPU acceleratedudfs at this point uh it's a bunch ofwork and uh it's not cheap and if youknow how to runum large language models on CPU withOnyx uh which I've done a talk on for acouple different NLPum talks uh there is a way to do it itrequiresum it requires a little bit of finessebut yeah the the short of it is is thatOnyx is not Onyx infant sessions are notpickleable and so because of the waythat spark works it in pi spark you runudfs by pickling your user code and thenyou broadcast that down to the workernodes and then those worker nodesunpickle the code and executed againstthe data it's the whole like pushingcode to data versus pushing data to codeinversionumbut you can't do that because you can'tpickle Onyx infant sessions and so youhave to come and see you have to come upwith like a file broadcasting system andyou basically have to broadcast the Onyxfiles down to the workers and then youhave to actually load in those Onyxinfant sessions on the Flyum and then you can get a if once youhave that working you can basicallyinfinitely parallelize that because youjust you just make a copy of the Onyxfiles on all of your workers and youjust load them on the Fly andum you pay a little bit of i o penaltyfor readingreading from the file from disk tocreate the the object in memory the Onyxinference session but with enoughparallelism if you're doing big enoughbatches that you're pushing through theOnyx infant session it ends up becomingum economical to do thatand then and then you can getin theory unlimited throughputum we can get likethat's how we've we've been able to getto likeum embedding like in indexing likewhatever billions of documents inumin short order I forget how how fast wedid it but it wasnot a ton of time maybe a few daysyeah so that that leads nicely to um Iguess performance questions around usingSparkle for Eva and caviar learningsaround what you what you found worksbestum so do you have any insights from thatand maybe I could add some some othercomments as well not as many as you doso I I would love to hear what you haveto say I know that I think we actuallyspent some time together uh working onsomething similar to thisbut I know that thethehyper parameters around creating yourgraph for for running hnsw is veryimportantum and I think that what I found is thatrunning therunning the vectorizers ended up beingwhat what consistently killed my jobsand eventually pushed me toum to to bring my own vectorsbut this this is something where I amstill a student if I'm being brutallyhonestyeah so you see I guess it does thelarger scale use casesum you just have more control if youbring your own vectorsum you also need to be careful aroundum GPU configuration if you're using oneof the modulesum as an alternative you can also usethe the cloud modules like to open Aiand click hereum but around performance just justwriting to Eva toI obviously it's number of nodes inshouting as well so by default defaultVBA will actually configure a class ofthe number of shads of your cluster sizeso if you have a cluster of size threeyou'll have them three shotsand and then when you're writing thedata the um in terms of the actual hmswif construction is the most importantparameter along with Max connectionsum where EF construction is basicallyhow far it searches in the graph andmatch connections is how many edges eachvertex hassoso those are definitelyum sort of parameters to optimize andthen batch size as well so we do have wedo have with the python client we haveDynamic batching and it's had a fewiterations because it it is umit it is quite hard to get right and tonot sort of push too much and create thebatches too too large and overload withyet uh so with spark it's if you've gota fixed size cluster it's just aroundoptimizing the batch size to beum what you expect and one thing toto help with that is really setting upPrometheus metrics and just monitoringthe the batch latenciesum one thing we have done recently whichwill be in the next releases optimizingbatch latencies and making thecompactions with Alice in trees fasterso there should be some performanceimprovements thereit's super similar advice wheneveranyone asks me how tohow to get the most performance out of aspark job it's usually like well youlook at the metrics and then you turnsome knobs and you see what the metricsdo and then you keep turning knobs andyou do that for a week and then you'vegot like a great and then it's greatum and there it really isn't like a tonof shortcuts to it rightyeah that's rightum and I guess the other thing is morethe ratio of the number of spots Spocknodes you need to alleviateum so generally if you're loading from averyum from a data Solace that's quite sortof basic or quite easy for spark to loadlike parquetum often a ratio of one to five or oneto four is is fine soum you will need more we the up notesand Spark nodesum generally if you're loading datayeah it's never bad practice todecompose and those in those situationsif you find yourselfwith a bunch of parsing logic and then abunch of like indexing logicjust pull those apart you parse you dumpto parquet and then you have another jobthat just reads parquet and indexesand you get a little bit moreintrospection because of the way thatsparkumthe uh the the dag execution works it'sall lazy and and it's it's super unclearwhen things actually happen and it canlook like some operations take a reallylong time when they shouldn't becausereally that that's what actuallytriggers the eager execution of all theoperations behind itumyeah yeah and there's uh there's oneinteresting piece of news is that um allthe hugging face data sets are Autoconverted to parquet files nowum so that came out I think last weekand that's yeah that just makes iteasier to load um all those heavy andplace data sets into read the edgeswhile using the Spy connector so sorrycan I ask a dumb question really quickso what so parquet Json CSV uh likesorry I'm still kind of catching upwhat's the difference between parquetlike what does it add to itparquet is the the the the company lineis his compressed column in our formatthat makes all of your dreams come trueumI think that it is is basically a way toif you're reading Json or Json lines orCSV the sheer amount of input dataum requires there's a few differentthingsif you want to read all the data in a uhand it's Json you have to you have toread every lineumit doesn't support any filtering rightbecause you don't know anything aboutthe internal structure of the dataum with parquet when you have certainthings likeum spark predicate push down filtersenabled you can do things like sayI want to read in only the data wherethe date is between a week ago and todayand if you were to read Jsonum unless you're you're doing your datapartitioning yourselves and then youlike only read in the subfolders thatactually have the data if you're ifyou're partitioning data by dateum you can do all that but parquet willdo all that for youum and basically you can do push downfiltering such that you only bring inif you're reading Json you would have toread that data into memory and then youwould perform your your filterbut you you end up having to bring allthat data into memory to do the filterwith parquet you can do things likeum you will only ever bring in memory orbring it data into memory that is uhmatches your your filtering if you haveyour your predicate push Downs turned onum and so that's really nice becauseit's compressed there's justfundamentally less data to read and thefile sizes are smaller spark is really apin like really finicky with havingum lots of of tiny files or too many bigfiles it likes its files between 250Megs in a gigum and so kind of like restartingum data to be of that size is a loteasier with parquetumso those are those are a few of thereasons so it's you can't read it it'sit looks like gobbledygookum to a human but to spark it can do abunch of fundamental out likeoptimizations against its reads andrights that makes it easier to get datainto spark and get data out of sparkyeah it's so interesting thanks so muchfor that I think I've like I'm just kindof starting to get in the water of thelarge-scale Imports and I've seen thisproblem where you're trying to Chunk Upthe Json but you kind of have to readthe whole thing and then parse throughit but yeah this sounds reallyinteresting and uh yeah let me kind ofstep out of the way becauseso uh learning so much from this butyeah John if you could take it overbecause yeah yeah another way to thinkabout it is um what what Spock sort offocused on doing was decoupling theHistorical dataway house we had storageand compute tied togetherand soum basically spark is the compute andyou are separating the storage onto it'sno longer a protocol just for thatdatabaseum to read it's actually sort of an openformat but it still has theum the properties that that make itefficient for reading so it's sort of abinary format of column now so you cancome just query The Columns you need andstuff like that yeah it's a good causethe there's the push down filteringwhich means you can get a subset of thedata row wise and then like John said ifyou want to select if you're saying onlyselecting two columnsit has you can do that by not having toread in the entire every single fileso that makes me think it's is itsimilar to hdf5 if you're familiar withthat yeah because I've I've used hdf5 uhpreviously and at the image formatuh no it's another data set format yeahso it is it is similar in some way toum one differences you can have multipledata sets in one hdf five file but inother ways it is quite similar to RKit'sumyeah it is it is similar but I guess thelarge-scale big data use cases more leantowards using parquet or can and sort offile formats like thatand so one last question I had was justaroundum so so using spark uh when using sparkwith a search engine I immediately thinkare using page rank the page rank umfeature in spark for generating featuresor potentially generating other sort ofum metrics uh that you're using as kindof uh ranking inputs so you're lookingto store those in webiate as wellso I'm not sureyeah if we were going to like if we'regoing to create features based on likesome graph formulation of our of our webcrawl dataI'm notI'm not surehowI'm not surehow I would probably combine combinethat with leviate and that could totallybe because I'm not an expert in thetechnology yet but I'd love to hearif I I think for me I'm always lookingfor Simplicity over anything else Ithink that like when you're working inBig Data like this when you're workingwith machine learning Technologies likethis the simpler you can make it foryourself when you're kind of likedrowning in all of this very complextechnology like the the worst thing youcan do is is to over complicate thingsbecause it's always just gonna be a kissof death for your product and soI think if I could yes because I'malways I in my at least in myinfrastructure if I can get out of onetool what's going to take threeotherwise be it like elastic for lexicalI don't know likeum using weeviate for semantic and thenusing like neo4j for graph if there's away to do all of that in weeviate it'salways going to be the preferablesolution for me so I'd love to hear kindof like how you would think throughcombining graph features or doing likegraph based queries in in weeviate andhow you would kind of complement thatwith the semantic capabilitiesyeah so I'm all thinking that um thesort of the end result of the graphcalculation you would still do the graphcalculation in spark but then you canrender to ev8 and yeah I see this moreof an extension of our hybrid searchcapability so once weum with the M25 combining with um denseretrieval then I think the roadmap thereis just adding more features aroundre-ranking and different ways ofcombining different signals togetherlearning to rank and stuff like thattrying to like combine features that aregoing to learn over time withthe the scores that you get from bm25and you know like your cosinesimilaritiesyeah that's a cool idea like where youhave the um Vector surge score as afeature in the bm25 score as a featureand the other symbolic features and thenyou just use like uh the gradientboosting machine I think is a verypopular machine learning model andyeah that's super interesting and I'mI'm so interested in this hybrid searchrank Fusion thing because I think thiskind of idea of like reciprocal brandFusion where you just kind of merge theranks in each list is a very powerfulkind of abstraction for sending thequeries to multiple things and I'm alsokind of I'm sorry if I'm going in toomany topics with this but like this ideaof in like we could have externalinformation sources as well so maybe weconnected to like the Bing API it couldbe like a part of hybrid search and thenwe fuse all this information togetherwith your specific leviate datayes I'm just I'm curious like yourexperience with hybrid search and you'rethinking around it I it's still a prettyNew Concept for me kind of just I'mstarted with just the bm25 vector searchbut I'm curious if you have any moreideas on this kind of ideayeah I think it it's always againthe cop out is it depends I think thatlikeyour use case is always going to Definewhere you want to go so like I thinkthat forpeople who have extremely high Precisionneeds and they're okay with being lowrecallumdoing things withumlike your keyword matching and yourelectrical search is always going to belike the easiest way to to get to a veryhigh Precision system I think that whatwe found historically is that usingsemantic search as a way to start tolike beef up your recall so if you havelike a pass of lexical search to getyour very high Precision items and thenmaybe you have a second pass of semanticsearch that can kind of like enrich thatwhat you're returning and improve yourrecall but doesn't really hurt yourprecisionum I think where it starts to get morechallenging is is if when you don't havethose as like two separate passes whereyou have either business logic orum you don't havelike something in between that's that'svery human legibleum dictating how you intersperse thoseresults I think that's whenyou're gonna be best off with some sortof like you need some sort of way tokind of like Benchmark and and be ableto like evaluate the system against somesort of data set that if you're in thesearch space you're probably collectingeither by human annotators orum by kind of reverse engineering anddata mining from your own likeinteractions data setsfrom the people on the site but eventhen likeit's it's always a challenge becausewith searchum you you are never going to have allof your counter factual data rightbecause like if someone searches andthey don't find anythingor if they did find it and they leavelike do you know why people just bouncedyou know are all clicks a good click ifpeople is like clicking it and then theythey like come back to the site and thenthey click on another thing if I clickon like the top five links like there'sthere's all these different in thesearch space all these different likeinteraction patterns that don't mapcleanly to your standard label data setyou could like just Benchmark a machinelearning algorithm against and then doyour hill climbing exerciseumand so I think that I think that it makelike what worked for us is starting veryhigh Precision with lexical searchusingum using semantic search to improverecall and then when moving to a morecomplicatedmixing algorithm trying to find somesort of way to to eitherumBenchmark against either hand annotatedorfinding finding ways to to data mindthat out of your system but like I saidlike that that step is really tricky inthe search space to get clean label dataI never made the connection to thecounterfactual in the relevance judgmentbefore that's super interesting um maybethis is uh too revealing of a questionbut can you tell us about how you'rethinking about collecting data u.comyeah so we we've announced this is thisis public you can go see like um theseannouncements I don't know I won't getin trouble butumbut we've done we do a number of likethird-party studies where we get kind oflike we do like blind tests of ofrelevance and we found thatum a lot of the work that we've donehave slowly gotten us to a point wherewe are pretty competitive with Google inin certain nichesand soI think we are always trying towe're always trying to ladder up rightso you have you have your top of theladder you've got your third party likelike studies that you doum that are very expensive you havehumans doing it you have to pay for itlike humans are slow so you don't have aton of labeled data coming back for youso you have like but it's also the therichest data that you're going to getyou get like human insights you getpeople explaining their thought processum and then you have kind of like goingdown a run you have things like you knowyou have your user metrics like thingslikeum like retention and people who arelike conversion metrics so things thatare still very high high signal to noisebut and our and you get a little bitfaster feedback than at the top whereyou have to do a whole studyum but still very slow feedback and thenyou kind of like you you start to godown the ladder towards like some ofyour click signals some of your hoversignalsum things like that that you can kind ofglean from your click stream data andyou're usually trying to as you go upthat ladder you're trying to find likehow strong the correlations are how toformulateum what is a good click versus a badclick and then doing like differentregression analysis to figure out likeuh if we if you hover for five secondsversus three seconds like is does thathave a better like correlation with withour retention metrics and then how dothose retention metrics then ladder upto our like our actual studies that wedo and at each step of the ladder you'retrying to figure out like how much youcan kind of you can glean about whatbecause what you're trying to do isyou're trying to like use the higher thefaster feedback signals like clicks andthen project into the future what youryour studies are going to show like whatdoes that actually mean for the overallrelevance of your search resultsumit's it's really hard uh we're stillfiguring it out I think that most peopleum now I think if you if you talk topeople who work at Google or Microsoftthey'll tell you that they still havehuman labelers they still they stillrequire humans to go and label thingsthey just probably have much morerefined ways of of laddering up andforecasting out what they're they'remore fast feedback signals are going tomean for those those studiesyeah that's interesting that that answermakes me think if we're thinking aboutit as a ladder are you trainingindividual models at each rung of theladder or is it more of an ensemblewhere you have the lower rungs that areinforming the models at the at the upperendit's it's more of the the latter and soyou've got a model that's going to tellyou you knowfor different and I think there's lotsof literature out there that'll tell youlike how towhat does it mean to to try to calculateuser satisfaction from raw click streamdata and you have different formulationsand then you try and for all of yourdifferent formulations you try andfigure out like what does that mean forum you're more user you're moreuser-facing dataso how do you take click data andcorrelate it with user data then how doyou take the user data and thencorrelate it withum your your your scores that you'regoing to get back from your morelong-term studiesyeah super difficult problem the data isprobably very messyvery sparse too so awesome that was sucha great coverage of these topics uh Samit was such a master class in these uhin these topics especially theengineering details the spark the datacollection at u.com the widgets thesearch interface it's so much learningto unpack and I'm so happy we recordedthis podcast so thank you so much forjoining the weba podcastyeah thank you so much for having meit's a pleasure and looking forward tobuilding much of cool stuff with youguys in the futureawesome thank you", "type": "Video", "name": "Sam Bean, Zain Hasan, and John Trengrove on You.com and Spark - Weaviate Podcast #32", "path": "", "link": "https://www.youtube.com/watch?v=pP8CZbDkUKU", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}