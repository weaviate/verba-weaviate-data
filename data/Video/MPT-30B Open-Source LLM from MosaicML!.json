{"text": "Hey everyone! This video dives into MosaicML's new MPT-30B open-source large language model... with a commercial license! \nhey everyone thank you so much forwatching weave it on YouTube I think wehave one of the most exciting recentbreakthroughs in large language modelsAi and deep learning technology Mosaicml is open sourcing with a commerciallicense the MPT 30 billion parameterlarge language model so we have threemodels that have been published the MBT30 billion that's the pre-trained modelthat's been trained by language modelingit has absolutely gigantic scale of uhText data and then we have two uh tunedmodels from that checkpoint MPT 30billion instruct and MPT 30 billion uhchat so two different ways of continuingthe training with label data and thatkind of thing for training from thecheckpoint so right off the bat aboutthe checkpoint obviously the opensourcing of that with a commerciallicense that's probably the thing thatjumps out the most when you read aheadline like this but there are allsorts of interesting details behind thiswhether you want to you know take thatcheckpoint and start sampling from it tolabel your own uh you know your ownfeedback preferences to then continuethe training on Mosaic ml's trainingCloud all sorts of interesting detailsof what you can do with with the opensourcing of the pre-trained base modeland then of course what you could do byyou know plugging in the 30 billioninstruct model into your application sothe 30 billion chat model has beenpublished with a demo so we can get aquick sense of the the intelligence ofthis thing by you know curing it on hugand face spaces so let's ask it pleasewrite a shortdescription of how approximate nearestneighbor search works so you know we canyou know test its intelligence on howyou can face faces and get a sense ofhow this works so it's hosted with theMosaic ml inference API so quickly theMosaic ml inference API is superinteresting development with Mosaic andmakes it super easy for us integratingit with weaviate is they also have thiskind of like inference as a service APIwhere if you're used to weavate we havelike a generative open AI generativecohere generative palm and so prettysoon we'll have generative Mosaic mlthat lets you do this kind of connect tothe infrared service using your API keyand you know running that and all thesearch results in paralleland so on so here's his answer pleasewrite a short description of howapproximate nearest neighbor searchworks approximately nearest neighborsearch is a technique used to find sosee I think you get the idea you canpause it and read it if you like but umso one more thing quickly before divinginto the blog post where Mosaic ml haswritten all sorts of details about howthis works it's just such an such adetailed comprehensive overview of whatit takes to train language models likethis and as well as the evaluation ofthem let's take a look atretrieval augmented generation with thisthing really quick so what were ourinterests with Vector databasesSomething That We're super excited aboutwith large language models is when youtake a query like you know let's say thequery is what is llama index we takethat query and we turn it into a vectorand then we use that to search ourVector index of text passages that havealso been vectorized by doing that umyou know Vector distance calculation tofind the nearest neighbor and then asthe solid the approximate nearestneighbor search the mean of that isbuilding up the data structures to letyou scale that to an absolutely enormousscale and not have to do the distancesbetween every single vectorized passagein your text collection but so we mightask a question like what is llama indexand then we would hit some data sourcelike say the Eva blog post and werecently have this new blog post fromJerry Lou Lama index and weviate and saythat Vector hits the vectorized thevector representation for this textChunk we grab that and then we to thelarge language model we would saysomething like you know please summarizethis text and then we give it the textRetreat from the vector database there'sa you know please answer this questionbased on the following text passage allthese kind of all these kind of thingsthat you can do with it but we see theanswer a lot of index is a dataframework for building llm applicationsit provides connectors for ingestingdata from various sources so you know itprovides a successful answer so let'sdive into the blog post get until thedetails behind how the MPT 30 billionmodel was trained and evaluated and thisjust super exciting development is theopen source large language models andjust generally the capabilities of thesemodels continues to evolve all rightlet's get into the details of it MPT 30billion open source large language modelcommercially licensed more powerful soone of the first things that jumps outwhen you read this is 8 000 contextlength on h100 gpus I believe h100s arethe most powerful gpus so it'simpressive to have set up that kind ofcloud infrastructure for all this sothey begin the story with uh wellfirstly you can check out the chat onthe hugging face which is what we justlooked at so so beginning the story soMPT 7 billion this model I think it cameout one or two months ago and theydescribed how in May alone these modelsthe seven billion parameter baseinstruction tune chat tune and storywriter which I think is um you knowanother specific data set that theyfine-tuned the 7 billion base on havebeen collectively downloaded over threemillion times so you know clearly peopleare interested in these open sourcelarge language models there's no youknow there's no dancing around that sothey to tell a little more about thestory of open sourcing the MPT 7 billionbase they describe how the community hasyou know extended MPT with lava MPTwhich adds Vision understanding MPT thisnew ggml inference library thatoptimizes MBT on Apple silicon and CPUsand then gbt for all which lets you runa gbt4 like uh chatbot interface on yourlaptop using MPC as a back-end model sothis is like this private GPT that comeswith the UI for your local setup and youcan use MPT as that model to run on yourlaptop so today we're excited to expandthe Mosaic ml Foundation series with MPT30 billion a new open source modellicensed for commercial use this is ahuge detail that is significantly morepowerful than MBT 7 billion andoutperforms the original gpt3 so prettyamazing they later describe 17 of theparameter account of gbt3 and they useabout 60 of the compute budget toachieve this so they're also releasingtwo fine-tuned variantsmbt30b instruct and mpt-30b chat we'llget into that and what that entails thatare built on top of MPT 30 billion theydescribe I think it's so interesting howthey're describing they publish the basemodel as well which has been pre-trainedwith the language modeling as well asthese models that have been fine-tunedwith that uh instruction tuning thewhole reinforcement learning from Humanfeedback where you have some kind ofinstruction and a long answer and thenthe human preference to do like this orthe chat conversation interestingly thechat conversation model actually isn'tuh commercially licensed but and I don'tthink there's detail about exactly whatdata set that is trained with but that'swhere you have multi-turn conversationsto fine-tune with so but they alsorelease the base model so if you want totake that 30 billion parameter basemodel and then you know I don't know youhave some kind of uh instruction tuneddata set for your particular thing youcould continue training with that basemodel and it's one of the mostinteresting trends that I expect todevelop further with this wholecontinued evolution of large languagemodels so so anyway so there all MPG 30billion models come with specialfeatures to differentiate them fromother large language models so the 8 000token context window that's pretty longI think you know 4096 is what I think iscurrently on the umon the open Ai apis and obviously that'slike 8192 but anyway so uh longercontext via Alibi attention so Alibiattention seems to be a big deal so Iwanted to just quickly highlight thispaper the title of it train short testlong so it's an interesting way thatthey can extend the context length atinference time if you're not interestedin the full details of like how exactlythey modify attention but that'ssomething they say um like if you lookat when you're configuring MPT 30billion you can you know turn it up to16 000 contacts window and that's aproperty of how this L by attentionworks I'm personally not an expert onthat but just taking out that nugget ofit quickly so um also you have the umthe the pre-training data mixture sothey're going to show a table that has acomposition of data sources anotherquite interesting detail and then againthe h100s are I think the most powerfulGPU so this kind of cloud infrastructurestuff has been set up so another reallyinteresting thing is that you can deployit on a single GPU so either you wouldneed one a100 with 80GB gigabytes in16-bit Precision or one a140 gigabytesand 8-bit Precision so the ability todeploy this on a single GPU that's alsoquite interesting because you know it'sit makes it easier to deal with the umthe Computing cluster if you only haveto have one GPU to deploy the model ontoso comparable models such as Falcon 40billion they have larger parametercounts and they can't be served on asingle GPU today so that necessatesnecessitates two plus gpus you knowwhich increases the not only you knowincreases the system cost as well askind of the overhead of how you're goingto think about doing this sosome additional interesting things youknow Mosaic ml training if you're goingto keep trading with the 30 billion withyour private data via fine-tuning domainspecific pre-training or training fromscratch I think that is such aninteresting detail and will be sointeresting to see how that plays outthey also have Mosaic ml inference andwe're going to dive into that later aswell so continuing further into the MPT30 billion uh family so they start offby saying customers can train MBT modelsefficiently with 40 to 60 percent uh mfuso I wasn't exactly sure what this wasso I checked this out uh super coolrepository llm Foundry uh you know I'vebeen familiar with Mosaic ml's composerLibrary composer is an open sourcing ofall sorts of efficient trainingtechniques for deep learning models solike different kinds of regularizationstrategies like particular kinds ofDropout or say particular kinds of likenormalization layers like ghost batchnormalization these kind of Innovationslike the owl by attention all this stuffhas been packaged in the composerlibrary and now LM Foundry looks likeanother library of open sourcing allthese details it's like huge detail tolove about mosaicbut um so this mfu metric uh model flopsutilization so you know like how much ofthe hardware is the model utilizing whenit's trading and I think you know againI'm not an expert on this myself butthere's all sorts of stuff behind um youknow like how they try to reduce idleGPU time with how they batch the dataand they describe like this fullysharded data parallel thing and allsorts of interesting details of tryingto you know make the most of all theresources for training these models soall sorts of things without diverging uhfrom Lost spikes lost spikes anotherthing you know as you're doing thegradient descent and these kind of thisis kind of training techniques that comeinto that composer Library I think totry to you know control the training ofgrading descent language models massivescale that kind of stuff so cool so MPT30 billion base the commercial Apache2.0 license that's a really interestingdetails competitive with other opensource models such as logo 30 billion orFalcon 40 billion they have thisinteresting chart moving into in asecond sookay so I think I don't think any ofthese details quickly uh so train of thelong context window of 8K tokens versus2000 for llama and Falcon uh can handlearbitrarily long context Windows viaAlibi or with fine tuning so again thisL by train short test along so they canextend the context window after trainingthe model they don't need to uh trainthem with the sixteen thousand thirtytwo thousand two then extend it so Ithink that's pretty interesting detailum okay so to build 8000 support intoMBT 30 billion we first pre-trained onone trillion tokens using sequences thatwere 2 000 tokens long and continuedtrading for an additional 50 billiontokens using sequences that were 8 000tokens long so interesting detail firstI think you know heating up the pot witha trillion tokens of 2000 token sequencelengths and then find like continuingthe pre-training still like Mighty muchmodeling predicting the next token but50 billion tokens on the 8 000 tokenlike so just you know details about howthis kind of thing is you know happeningwith the longer context Windows okay sohere's a super interesting table Mosaicml MPT 30 billion training data so youhave the C4 Corpus I think this is likethe you know the Colossal common crawlCorpus where you you know hit theinternet and get all sorts of data outof it uh red pajama I've heard thatphrase but I'm not an expert on thesekind of things but you know Wikipedia Ihave to imagine the stack maybe isum overflow semantic scholar so I'm notexactly sure oh stack exchange thatthat's probably stack over full likething and so books archived so you cansee basically the number of tokens thatthese um that these data sources make upso Wikipedia it looks like it's onlyfive billion tokens relative to this uhyou know C4 which is 2.4 trillion thesekind of details are so interesting Ithink umhis paper called duromine something likethat where it's about um optimizing thecomposition of data sources inpre-training so as a reminder this isused to pre-train the language model sothis is where it's predicting the thenext massed out token not where you'redoing that uh instruction tuning whereyou have it write a big long thing andthen you say you know how was that andthen you fine tune it that way this isjust like building that base languageunderstanding from pre-training onpredicting the index token on theinternet text basically uh cool so um soyou see this bar chart that shows thesize you see Wikipedia not as much asthe C4 and then you can see this is areally interesting detail as well thesequence length distribution so uh youknow where where it's greater than fourthousand ninety six tokens compared touh the Baseline and the Baseline is Ithink this 2000 context window so maybeinteresting like how the context isisolated I'm not an expert on thisdetail as well but say the um you knowWikipedia passage I imagine is only 3tokens long like um you know somewhat Idon't know this is like randomly likesome small City or something that madeit to Wikipedia but isn't like an 8 000token sequence so maybe that's whatthey're showing with this pot I'm notexactly sure okay so continuing on hereis my favorite part of the blog postthey show the evaluation of their modelwith this kind of diagram and so funnyenough I remember this from playingPokemon where your Pokemons would haveuh like different dimensions in this onekind of game but anyway so this kind ofchart I don't I don't remember exactlywhat this kind of chart is called that'swhy I reference the Pokemon thing butyou have the you know how well itperforms in symbolic problem solvingprogramming World Knowledge languageunderstanding reading comprehension andCommon Sense reasoning so you know 0.40.6 these this is all the um the singlemetric for this particular aspect oflanguage understanding so you see likeMPT 30 billion in red MPT 7 billion inBlackboard or purple shading so you seethat reading comprehension the new modelis much better maybe say symbolicproblem solving pretty similarly and soit's such an interesting chart becausethere are so many different dimensionsto evaluating these language models isprobably one of the most interestingtopics because you're trying to likeboil the ocean and something Zayn saidone time that really stuck with me butyou're trying to boil the ocean whereyou're trying to lay again you're tryingto language model you know all of thesedata sources soin the end it's quite interesting to seelike which particular types of thinglike how well can it write code or howwell can it write an essay on The GreatGatsby so this this kind of evaluation Ithink is really cool so in addition tolet me zoom in on this a bit so inaddition to the comparison of 7 billionversus 30 billion they also have um MPTversus Falcon 40 billion as well asllama 30 billion so they're probablyother I'm not an expert on falcon orLlama Or x-ray like knowledgeable of itthat much honestly so like of what kindof licenses they have is another detailpeople probably care a lot about likewhether it's commercially licensed ornot or you know like MPT 30 but they'veopen sourced the weights and they alsohave all this infrastructure behindfine-tuning it so I think it just thatit performs similarly is interestingmaybe it's another bitter lesson scaleis all you need kind of thing but soperforming about the same as Falcon andMom that's what I'm trying to get at andthen similarly with the 7 billion okayso the next Super interesting detail isthe Mosaic ml inference and trainingCloud so starting off with the most mlinference Cloud so I think it's sointeresting that Mosaic ml is you knowgetting into an inference Cloud thismakes it a lot easier for us tointegrate it with weaviate and all sortsof cool things so firstly right off thebat four times cost savings for the 30billion instruct versus The openaiDaVinci checkpoint also four times costsavings for the uh seven billioninstruct versus the openai carry modelso uh you know I don't want to get intoit Mosaic ML and opening I have friendswith both of them at wevate but this iswhat they're showing you on the cost ofthese two different models with theinference apis so they describe the youknow commitment to privacy and thenhere's a super interesting thing iscustomizability so Mosaic ml's trainingCloud they've always been you knowfocused on training and how you wouldhave a custom large language model foryour data and I you know I think it'sgoing to be super interesting to see howthis plays out and how much people wantthis it's you know it's very interestingfrom the perspective of like retrievalaugmented generation where you just youknow retrieve with the search modelsVector database to have to you know gpt4that has no knowledge that has just thisGeneral reasoning ability that justreasons over the retrieved context orwhether it makes sense to keep this isthe reasoning model to keep fine-tuningthe reader model large language model sothen it retrieves some information aboutsay we V8 and then this model has beenyou know rlhft with you know responsesabout we V8 and then does that lead tobetter answers is it worth the troublesuper interesting questions and I'mreally interested to see how this shakesout butwith Mosaic ml's cloud and also just theum you know the publishing of the modelso this headline customized MPC 30billion with Mosaic ml training so let'sconclude with uh the training costsbehind uh training these new models sothey describe with 512 a100 40 gigabytesthis takes 28 days and costs 871000so then with 512 h100s with 80 gigabytesthis takes 11.6 days and 714 000 so Ithink this is still you know quite abreakthrough from how much the originalgbt3 cost to train I mean 512 h100s thereason it makes me laugh is because youknow it is just an absolutely enormouscomputer if you think about it and thenyou know running it for 12 days doingall this I mean and it's it's justremarkable what it's able to do justthis whole state of this technology isreally remarkable butin a mosaic ml they've been really goodat getting these costs down so it'll bereally interesting to see how this youknow further plays outum so they also describe uh the cost offine-tuning on one billion tokens withMosaic ml so I think this is reallyinteresting so if you want to have a 16h100 80 gigabyte cluster to fine-tune onone billion tokens it'll take nine hoursand only costs 714 dollars so you knowthat's pretty reasonable to pay forsomething like that and all of it ismanaged on the Mosaic ml Cloud so youknow they're being super open aboutgiving you all these details behind howit all works open sourcing the LMFoundry Library composer writing blogposts like this that explain everythingso the whole thing is just extremelyinteresting if you wanted to you knowone billion tokens I think is quite alot so if you were to start annotating adata set of instruction tuning responsesyou know you have this MPT 30 billionmodel it's been open source so you canstart generating some responses fromthat labeling which ones you like whichones you don't and then only 714 dollarsto you know see how well that improvesthe model or not so super interestingdetail uh they describe um you knowsigning up for the demo with the uh theinference API and again it's our hug andface spaces so thank you so much forwatching this video on Mosaic ml's MPT30 billion Models Super excitingdevelopment for large language modelsand open sourcing them with commerciallylicenses all the details behind allthese things if you want to learn moreabout weaviate please check it out onweevier.io you can also check out theopen source GitHub repository of theweba vector database on github.comand also please follow us on Twitter atweeviate underscore IO thank you so muchfor watching this video please subscribeto this YouTube channel if you want tosee more videos about you know deeplearning technology Vector search Vectordatabases and all these exciting thingsthanks again", "type": "Video", "name": "MPT-30B Open-Source LLM from MosaicML!", "path": "", "link": "https://www.youtube.com/watch?v=CPIJAdAVMG0", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}