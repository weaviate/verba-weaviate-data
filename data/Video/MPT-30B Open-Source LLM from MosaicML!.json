{"text": "Hey everyone! This video dives into MosaicML's new MPT-30B open-source large language model... with a commercial license! \nhey everyone thank you so much for watching weave it on YouTube I think we have one of the most exciting recent breakthroughs in large language models Ai and deep learning technology Mosaic ml is open sourcing with a commercial license the MPT 30 billion parameter large language model so we have three models that have been published the MBT 30 billion that's the pre-trained model that's been trained by language modeling it has absolutely gigantic scale of uh Text data and then we have two uh tuned models from that checkpoint MPT 30 billion instruct and MPT 30 billion uh chat so two different ways of continuing the training with label data and that kind of thing for training from the checkpoint so right off the bat about the checkpoint obviously the open sourcing of that with a commercial license that's probably the thing that jumps out the most when you read a headline like this but there are all sorts of interesting details behind this whether you want to you know take that checkpoint and start sampling from it to label your own uh you know your own feedback preferences to then continue the training on Mosaic ml's training Cloud all sorts of interesting details of what you can do with with the open sourcing of the pre-trained base model and then of course what you could do by you know plugging in the 30 billion instruct model into your application so the 30 billion chat model has been published with a demo so we can get a quick sense of the the intelligence of this thing by you know curing it on hug and face spaces so let's ask it please write a short description of how approximate nearest neighbor search works so you know we can you know test its intelligence on how you can face faces and get a sense of how this works so it's hosted with the Mosaic ml inference API so quickly the Mosaic ml inference API is super interesting development with Mosaic and makes it super easy for us integrating it with weaviate is they also have this kind of like inference as a service API where if you're used to weavate we have like a generative open AI generative cohere generative palm and so pretty soon we'll have generative Mosaic ml that lets you do this kind of connect to the infrared service using your API key and you know running that and all the search results in parallel and so on so here's his answer please write a short description of how approximate nearest neighbor search works approximately nearest neighbor search is a technique used to find so see I think you get the idea you can pause it and read it if you like but um so one more thing quickly before diving into the blog post where Mosaic ml has written all sorts of details about how this works it's just such an such a detailed comprehensive overview of what it takes to train language models like this and as well as the evaluation of them let's take a look at retrieval augmented generation with this thing really quick so what were our interests with Vector databases Something That We're super excited about with large language models is when you take a query like you know let's say the query is what is llama index we take that query and we turn it into a vector and then we use that to search our Vector index of text passages that have also been vectorized by doing that um you know Vector distance calculation to find the nearest neighbor and then as the solid the approximate nearest neighbor search the mean of that is building up the data structures to let you scale that to an absolutely enormous scale and not have to do the distances between every single vectorized passage in your text collection but so we might ask a question like what is llama index and then we would hit some data source like say the Eva blog post and we recently have this new blog post from Jerry Lou Lama index and weviate and say that Vector hits the vectorized the vector representation for this text Chunk we grab that and then we to the large language model we would say something like you know please summarize this text and then we give it the text Retreat from the vector database there's a you know please answer this question based on the following text passage all these kind of all these kind of things that you can do with it but we see the answer a lot of index is a data framework for building llm applications it provides connectors for ingesting data from various sources so you know it provides a successful answer so let's dive into the blog post get until the details behind how the MPT 30 billion model was trained and evaluated and this just super exciting development is the open source large language models and just generally the capabilities of these models continues to evolve all right let's get into the details of it MPT 30 billion open source large language model commercially licensed more powerful so one of the first things that jumps out when you read this is 8 000 context length on h100 gpus I believe h100s are the most powerful gpus so it's impressive to have set up that kind of cloud infrastructure for all this so they begin the story with uh well firstly you can check out the chat on the hugging face which is what we just looked at so so beginning the story so MPT 7 billion this model I think it came out one or two months ago and they described how in May alone these models the seven billion parameter base instruction tune chat tune and story writer which I think is um you know another specific data set that they fine-tuned the 7 billion base on have been collectively downloaded over three million times so you know clearly people are interested in these open source large language models there's no you know there's no dancing around that so they to tell a little more about the story of open sourcing the MPT 7 billion base they describe how the community has you know extended MPT with lava MPT which adds Vision understanding MPT this new ggml inference library that optimizes MBT on Apple silicon and CPUs and then gbt for all which lets you run a gbt4 like uh chatbot interface on your laptop using MPC as a back-end model so this is like this private GPT that comes with the UI for your local setup and you can use MPT as that model to run on your laptop so today we're excited to expand the Mosaic ml Foundation series with MPT 30 billion a new open source model licensed for commercial use this is a huge detail that is significantly more powerful than MBT 7 billion and outperforms the original gpt3 so pretty amazing they later describe 17 of the parameter account of gbt3 and they use about 60 of the compute budget to achieve this so they're also releasing two fine-tuned variants mbt30b instruct and mpt-30b chat we'll get into that and what that entails that are built on top of MPT 30 billion they describe I think it's so interesting how they're describing they publish the base model as well which has been pre-trained with the language modeling as well as these models that have been fine-tuned with that uh instruction tuning the whole reinforcement learning from Human feedback where you have some kind of instruction and a long answer and then the human preference to do like this or the chat conversation interestingly the chat conversation model actually isn't uh commercially licensed but and I don't think there's detail about exactly what data set that is trained with but that's where you have multi-turn conversations to fine-tune with so but they also release the base model so if you want to take that 30 billion parameter base model and then you know I don't know you have some kind of uh instruction tuned data set for your particular thing you could continue training with that base model and it's one of the most interesting trends that I expect to develop further with this whole continued evolution of large language models so so anyway so there all MPG 30 billion models come with special features to differentiate them from other large language models so the 8 000 token context window that's pretty long I think you know 4096 is what I think is currently on the um on the open Ai apis and obviously that's like 8192 but anyway so uh longer context via Alibi attention so Alibi attention seems to be a big deal so I wanted to just quickly highlight this paper the title of it train short test long so it's an interesting way that they can extend the context length at inference time if you're not interested in the full details of like how exactly they modify attention but that's something they say um like if you look at when you're configuring MPT 30 billion you can you know turn it up to 16 000 contacts window and that's a property of how this L by attention works I'm personally not an expert on that but just taking out that nugget of it quickly so um also you have the um the the pre-training data mixture so they're going to show a table that has a composition of data sources another quite interesting detail and then again the h100s are I think the most powerful GPU so this kind of cloud infrastructure stuff has been set up so another really interesting thing is that you can deploy it on a single GPU so either you would need one a100 with 80GB gigabytes in 16-bit Precision or one a140 gigabytes and 8-bit Precision so the ability to deploy this on a single GPU that's also quite interesting because you know it's it makes it easier to deal with the um the Computing cluster if you only have to have one GPU to deploy the model onto so comparable models such as Falcon 40 billion they have larger parameter counts and they can't be served on a single GPU today so that necessates necessitates two plus gpus you know which increases the not only you know increases the system cost as well as kind of the overhead of how you're going to think about doing this so some additional interesting things you know Mosaic ml training if you're going to keep trading with the 30 billion with your private data via fine-tuning domain specific pre-training or training from scratch I think that is such an interesting detail and will be so interesting to see how that plays out they also have Mosaic ml inference and we're going to dive into that later as well so continuing further into the MPT 30 billion uh family so they start off by saying customers can train MBT models efficiently with 40 to 60 percent uh mfu so I wasn't exactly sure what this was so I checked this out uh super cool repository llm Foundry uh you know I've been familiar with Mosaic ml's composer Library composer is an open sourcing of all sorts of efficient training techniques for deep learning models so like different kinds of regularization strategies like particular kinds of Dropout or say particular kinds of like normalization layers like ghost batch normalization these kind of Innovations like the owl by attention all this stuff has been packaged in the composer library and now LM Foundry looks like another library of open sourcing all these details it's like huge detail to love about mosaic but um so this mfu metric uh model flops utilization so you know like how much of the hardware is the model utilizing when it's trading and I think you know again I'm not an expert on this myself but there's all sorts of stuff behind um you know like how they try to reduce idle GPU time with how they batch the data and they describe like this fully sharded data parallel thing and all sorts of interesting details of trying to you know make the most of all the resources for training these models so all sorts of things without diverging uh from Lost spikes lost spikes another thing you know as you're doing the gradient descent and these kind of this is kind of training techniques that come into that composer Library I think to try to you know control the training of grading descent language models massive scale that kind of stuff so cool so MPT 30 billion base the commercial Apache 2.0 license that's a really interesting details competitive with other open source models such as logo 30 billion or Falcon 40 billion they have this interesting chart moving into in a second so okay so I think I don't think any of these details quickly uh so train of the long context window of 8K tokens versus 2000 for llama and Falcon uh can handle arbitrarily long context Windows via Alibi or with fine tuning so again this L by train short test along so they can extend the context window after training the model they don't need to uh train them with the sixteen thousand thirty two thousand two then extend it so I think that's pretty interesting detail um okay so to build 8000 support into MBT 30 billion we first pre-trained on one trillion tokens using sequences that were 2 000 tokens long and continued trading for an additional 50 billion tokens using sequences that were 8 000 tokens long so interesting detail first I think you know heating up the pot with a trillion tokens of 2000 token sequence lengths and then find like continuing the pre-training still like Mighty much modeling predicting the next token but 50 billion tokens on the 8 000 token like so just you know details about how this kind of thing is you know happening with the longer context Windows okay so here's a super interesting table Mosaic ml MPT 30 billion training data so you have the C4 Corpus I think this is like the you know the Colossal common crawl Corpus where you you know hit the internet and get all sorts of data out of it uh red pajama I've heard that phrase but I'm not an expert on these kind of things but you know Wikipedia I have to imagine the stack maybe is um overflow semantic scholar so I'm not exactly sure oh stack exchange that that's probably stack over full like thing and so books archived so you can see basically the number of tokens that these um that these data sources make up so Wikipedia it looks like it's only five billion tokens relative to this uh you know C4 which is 2.4 trillion these kind of details are so interesting I think um his paper called duromine something like that where it's about um optimizing the composition of data sources in pre-training so as a reminder this is used to pre-train the language model so this is where it's predicting the the next massed out token not where you're doing that uh instruction tuning where you have it write a big long thing and then you say you know how was that and then you fine tune it that way this is just like building that base language understanding from pre-training on predicting the index token on the internet text basically uh cool so um so you see this bar chart that shows the size you see Wikipedia not as much as the C4 and then you can see this is a really interesting detail as well the sequence length distribution so uh you know where where it's greater than four thousand ninety six tokens compared to uh the Baseline and the Baseline is I think this 2000 context window so maybe interesting like how the context is isolated I'm not an expert on this detail as well but say the um you know Wikipedia passage I imagine is only 3 tokens long like um you know somewhat I don't know this is like randomly like some small City or something that made it to Wikipedia but isn't like an 8 000 token sequence so maybe that's what they're showing with this pot I'm not exactly sure okay so continuing on here is my favorite part of the blog post they show the evaluation of their model with this kind of diagram and so funny enough I remember this from playing Pokemon where your Pokemons would have uh like different dimensions in this one kind of game but anyway so this kind of chart I don't I don't remember exactly what this kind of chart is called that's why I reference the Pokemon thing but you have the you know how well it performs in symbolic problem solving programming World Knowledge language understanding reading comprehension and Common Sense reasoning so you know 0.4 0.6 these this is all the um the single metric for this particular aspect of language understanding so you see like MPT 30 billion in red MPT 7 billion in Blackboard or purple shading so you see that reading comprehension the new model is much better maybe say symbolic problem solving pretty similarly and so it's such an interesting chart because there are so many different dimensions to evaluating these language models is probably one of the most interesting topics because you're trying to like boil the ocean and something Zayn said one time that really stuck with me but you're trying to boil the ocean where you're trying to lay again you're trying to language model you know all of these data sources so in the end it's quite interesting to see like which particular types of thing like how well can it write code or how well can it write an essay on The Great Gatsby so this this kind of evaluation I think is really cool so in addition to let me zoom in on this a bit so in addition to the comparison of 7 billion versus 30 billion they also have um MPT versus Falcon 40 billion as well as llama 30 billion so they're probably other I'm not an expert on falcon or Llama Or x-ray like knowledgeable of it that much honestly so like of what kind of licenses they have is another detail people probably care a lot about like whether it's commercially licensed or not or you know like MPT 30 but they've open sourced the weights and they also have all this infrastructure behind fine-tuning it so I think it just that it performs similarly is interesting maybe it's another bitter lesson scale is all you need kind of thing but so performing about the same as Falcon and Mom that's what I'm trying to get at and then similarly with the 7 billion okay so the next Super interesting detail is the Mosaic ml inference and training Cloud so starting off with the most ml inference Cloud so I think it's so interesting that Mosaic ml is you know getting into an inference Cloud this makes it a lot easier for us to integrate it with weaviate and all sorts of cool things so firstly right off the bat four times cost savings for the 30 billion instruct versus The openai DaVinci checkpoint also four times cost savings for the uh seven billion instruct versus the openai carry model so uh you know I don't want to get into it Mosaic ML and opening I have friends with both of them at wevate but this is what they're showing you on the cost of these two different models with the inference apis so they describe the you know commitment to privacy and then here's a super interesting thing is customizability so Mosaic ml's training Cloud they've always been you know focused on training and how you would have a custom large language model for your data and I you know I think it's going to be super interesting to see how this plays out and how much people want this it's you know it's very interesting from the perspective of like retrieval augmented generation where you just you know retrieve with the search models Vector database to have to you know gpt4 that has no knowledge that has just this General reasoning ability that just reasons over the retrieved context or whether it makes sense to keep this is the reasoning model to keep fine-tuning the reader model large language model so then it retrieves some information about say we V8 and then this model has been you know rlhft with you know responses about we V8 and then does that lead to better answers is it worth the trouble super interesting questions and I'm really interested to see how this shakes out but with Mosaic ml's cloud and also just the um you know the publishing of the model so this headline customized MPC 30 billion with Mosaic ml training so let's conclude with uh the training costs behind uh training these new models so they describe with 512 a100 40 gigabytes this takes 28 days and costs 871 000 so then with 512 h100s with 80 gigabytes this takes 11.6 days and 714 000 so I think this is still you know quite a breakthrough from how much the original gbt3 cost to train I mean 512 h100s the reason it makes me laugh is because you know it is just an absolutely enormous computer if you think about it and then you know running it for 12 days doing all this I mean and it's it's just remarkable what it's able to do just this whole state of this technology is really remarkable but in a mosaic ml they've been really good at getting these costs down so it'll be really interesting to see how this you know further plays out um so they also describe uh the cost of fine-tuning on one billion tokens with Mosaic ml so I think this is really interesting so if you want to have a 16 h100 80 gigabyte cluster to fine-tune on one billion tokens it'll take nine hours and only costs 714 dollars so you know that's pretty reasonable to pay for something like that and all of it is managed on the Mosaic ml Cloud so you know they're being super open about giving you all these details behind how it all works open sourcing the LM Foundry Library composer writing blog posts like this that explain everything so the whole thing is just extremely interesting if you wanted to you know one billion tokens I think is quite a lot so if you were to start annotating a data set of instruction tuning responses you know you have this MPT 30 billion model it's been open source so you can start generating some responses from that labeling which ones you like which ones you don't and then only 714 dollars to you know see how well that improves the model or not so super interesting detail uh they describe um you know signing up for the demo with the uh the inference API and again it's our hug and face spaces so thank you so much for watching this video on Mosaic ml's MPT 30 billion Models Super exciting development for large language models and open sourcing them with commercially licenses all the details behind all these things if you want to learn more about weaviate please check it out on weevier.io you can also check out the open source GitHub repository of the weba vector database on github.com and also please follow us on Twitter at weeviate underscore IO thank you so much for watching this video please subscribe to this YouTube channel if you want to see more videos about you know deep learning technology Vector search Vector databases and all these exciting things thanks again ", "type": "Video", "name": "MPT-30B Open-Source LLM from MosaicML!", "path": "", "link": "https://www.youtube.com/watch?v=CPIJAdAVMG0", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}