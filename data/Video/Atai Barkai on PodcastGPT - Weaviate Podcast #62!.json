{"text": "Hey everyone! Thank you so much for watching the 62nd Weaviate Podcast with Atai Barkai! We are stepping into the meta with ... \nhey everyone thank you so much for watching the wevia podcast I'm super excited to host a Thai barkai co-founder of talk it AI which has created podcast gbt I you know personally for me the kind of dog fooding of using this we V8 podcast the transcriptions that I get from whisper and putting that into weviate and using that to prototype these retrieval augmented generation llm Vector database apps it's been so much fun for me so I'm you know so excited for this conversation I love diving into what we can do with podcasts kind of the future of podcasting with large language models and generally how you know I love like hearing like-minded people who are curious about this kind of application so firstly it's Ty thank you so much for joining the wevia podcast glad to be here Connor and yeah I checked out your uh weavia podcast GitHub and it's very cool yeah where we're at it's a great thing to dog food thank you that means the world to me I I mean the it may be first before diving into the GitHub and maybe telling you a bit about some of the projects that I've done sort of Lessons Learned let me like if you you could tell us about you know how you see the space and what motivated you to get into this yeah so talk at AI is building an llm native audio video database um so kind of a layer in between Vector databases and relational databases and the individual file analysis um so transcription the organization all that stuff um and are kind of uh breakthrough product um is a consumer product uh we kind of have our own design partner um for the uh database component it's called podcastgbt.ai um which just launched in Alpha about a month ago uh and it's basically um you know have AI listen to all your podcasts um find the most interesting parts for you uh and then send those to any podcast app um so obviously yeah like the big thing that shifted with language models is the computer can actually understand what people are talking about right it's no longer just keywords um and that just opens the possibility to to do search to do you know semantic search not keyword search but you're looking for type of content or content about something and you can actually find it and you can know how high quality is it how relevant how interesting um so how relevant to your other interests so so there's a lot you can do now that wasn't possible before yeah I mean and yeah I think that's perfectly it is it's kind of what I'm you know hoping we can hash out in this conversation is you know further exploring what's possible now that wasn't possible before when I came across podcast gbt and reached out to you because I was really curious to talk to you I one thing was um you know this kind of like interactive experience like chat with podcasts that you chat with your data chat with PDF chat with podcasts like can you tell me how you see that kind of like interactive experience with podcast listening yeah so actually it's interesting because the consumer product uh you know consumers don't care about the technology they care about the experience uh and podcasting is a very interesting medium um because so first of all I don't know if you know this worldwide there's roughly two billion hours of podcasts listening uh per month which is a lot right there's only eight billion people on the planet um and uh and it's generally it's very different kind of consumption from what people do on you know Tick Tock or Twitter it's long form uh you know roughly every session is on average 23 minutes long uh which makes sense to all of us right while you commute while we want um so people really kind of want to start pressing something and then not worry about it uh so the interactive component is very very interesting in outside of podcast consumption right to build tools uh for podcasters to build maybe like a just a plain search engine uh but when you think about just plain podcast consumption you actually want to go as far away from possible from uh interactivity and actually the direction we took this to um is we don't even have our own app we actually did start having our nap um but uh what we figured out is really kind of the intelligence is the product here and there's no reason to try to move um people over from their habit right people have habits formed over years they use this app the process app for years what's the minimum way that we can augment uh that experience without asking the user for anything so so the basically kind of the interface we came up with is uh all the podcasts are built on top of uh this open protocol called RSS um and yeah not many people know this but even you know apple and Spotify uh they don't actually host the content or anything they just you tell them here's where my podcast lives and then they kind of redisplit um so we can put up our own podcast for you um that is looks the same to that podcast application it's just one more RSS feed uh except the contents inside of it can now be um essentially Clips but long form Clips right not so not two minutes but you know 15 minutes 20 minutes um from maybe what was a three hour long episode uh and we can deliver that to you right away and also tell you what's inside of that clip why you should watch that clip right uh so it's very specific um so so yeah that's how we think about this from the perspective of plain consumer product for podcast consumption uh but of course when you start to think about tools for podcasters which is I think one of the obvious applications of having you know audio database then then the interactivity starts to become a lot more important um and then on our side that interactivity is mostly on the back end side all right we kind of figure out okay what are your interests um based on what you tell us right now actually we're going to add Twitter crawling soon so you can just you know see what your ideas are um and then surface things are really kind of hyper targeted uh for you uh in this long form world yeah well there a lot of really interesting ideas in there I love the kind of um I guess kind of the first one of the earlier things you were describing is that kind of syncing up of the clip recommendation clip extraction kind of with apple podcast with Spotify and how you can unify these interfaces I do think that's a really powerful topic like um you know with review like sometimes when I think about this weeva podcast and that kind of demo project I think about like maybe integrating this on the Wii v8.io website so like and you know I build my clip kind of recommendation but I do think what you're describing is probably the more likely uh future for this of that kind of personalization is managed by some kind of platform almost like sub stack how you have like an email list but I think kind of just generally it's like the like the evolution of these email lists they become more like a CRM for you where it like keeps information about you know why particularly you would want to watch this this podcast clip and I think yeah if we can maybe stay on that uh like hyper personalization of podcast recommendation yeah I think like just kind of like using the language models to take the podcast clip take some biography from a person or or even yeah I I loved what you said about the Twitter that kind of like data data connection is is a super powerful thing so yeah maybe if you could just tell me a little more about how you see like you know personalizing it uh you know how how exactly is that completely new with the language models and all this yeah yeah so I mean really before you even can get to personalization um on the if you think about the intelligence kind of pyramid uh that's kind of the personalization is the very tip of the of the pyramid um there's a lot of work you have to do beforehand um so obviously basically like the first steps is transcription getting the you know what the words that are being said and diarization which uh not as many people know the stone but it's who said what right so it's not you these these words were said but this person said that you know I'm saying these words you're gonna say words later um so that's kind of your basic fundamental building block um but then the really interesting part uh and by the way that you know and there's been massive um advancements uh in the last year you know uh open ai's whisper uh and Whisper two uh are just you know it's open source it's free you can run it on your own gpus and it's kind of significantly better than anything that existed um up until that point uh nowadays you know even more competition which is great but like we're really starting to get to the very very tip like transcription is almost solved not quite there's still you know um niches that they need to be uh solved and obviously it's not necessarily 100 perfect but overall you get very good quality um diarization is is still not quite at the same level but you can um get very very close to Perfection uh especially with the aid of kind of a language model to sanity check uh like does it make sense that all of a sudden this sentence was said by somebody else like maybe not okay yeah uh we can kind of uh use the intelligence there to straighten that out so but once you have this diarized transcript the really interesting part is um coming up with a hierarchy of segments um because you have essentially you know if you think about a podcast especially right because these things tend to be pretty long um you have uh you know maybe a 30-minute segment that is kind of an arch right that or an arc that gets covered and then inside of it it might be about five minute and seven minute and eight minutes that kind of are even more pointed to a specific Direction so you want to be able to find this uh hierarchy um of segments and that is really it's a very interesting problem um it's not trivial you can kind of do it in a recursive way a lot of the techniques that work um if you find the small segments if you kind of zoom out you can use them to find the bigger segments um so uh so that's very interesting and over there there's actually in our experience what works what tends to work best has to do with embeddings uh so kind of a good entry point to the um Vector database and we can talk about that but then once you've gotten all of this stuff so now we have a coherent segment that is you know 25 minutes long um now you gotta match the segment with a person uh and right so like if there's a three hour long podcast maybe I'm really into uh you know my background before uh Tech is physics right so if somebody's talking about uh interpretations of quantum mechanics that's really interesting to me maybe it's not very interesting to you so how do I map this segment to me and to you maybe a segment about um I don't know if any something else that you're uh into and and and there it's actually it's a very interesting one I think it's an open bar I think we're going to see a lot of advancements uh in that realm the most trivial thing you can do again has to do with embeddings right like what what type of things belong in your cluster of things um and obviously just traditional ranking personalized ranking like you see but I think what's really interesting with language models um is that this type of ranking can actually now rely on the content on what's inside of um the content rather than you know kind of proxies of user interests like who's clicking what um and if you think about you know what news feeds look like a lot of the times nowadays you can tell they're really optimizing for the clicks but it's it's both a conscious kind of choice maybe but also they really don't have much better to do like how are you going to know if this video is good or not if you don't know what's inside of it uh you kind of have to rely on clicks but if you can now uh look inside of the content that gives you a whole new dimension uh to uh rate content by uh which is especially important for you know long-form content where people you know they really want to be engaged for the duration of their Drive their walk uh it's not good enough to click bait them uh so uh yep yeah firstly I'm glad I you know you sometimes when I take notes when I'm doing a podcast I only have one piece of paper but I just did like half of my paper on what you just said so it was really great I you know I kind of beginning with that um diarization whisper having that integrated in the same platform that then does the llm personalization I yeah I believe strongly in that I you know right now I have like a collab notebook that my whisper goes into personally I've cut well I'm also kind of not necessarily working on this all the time but I've kind of given up on diarization because you know for me it's like um labeling I you know whisper doesn't give you the diarization and I haven't maybe someone wants a link to it and it'll just solve my problem right away but like I've just I've just decided to just chunk the text as you mentioned like the llm can sanity check like does it make sense for this cup that actually I think works pretty well and they just scale yeah and that scale is nicer for me to just get all these podcasts in there quickly and yeah so I believe strongly with that my other thing kind of with a platform that does this is like for me my whisper will translate like weediate to wv-8 you know yeah yeah yeah and so these kind of things are like kind of a problem for then you know like an embedding that is like you know uh you know like yeah I can imagine these little transcription errors are kind of problematic for these information specific podcasts like our podcast particularly is going to be about like you know we usually really dive into it on Vector databases and so it's like you know those little transcription errors but so then we so then talking about kind of personalization again is um this kind of like I love this separation as you mentioned like the popularity bias being too strong and we've yet we have this thing called reftubec where it's like this is like one of the Demos in that GitHub repository too where it's like Conor and then I interact with like four podcast clips and this is something like that you know is just going viral on Twitter is somebody discovering this that averaging these embeddings actually like produces a pretty nice Vector in the middle of it I'm just so surprised when I started playing with this but you know just averaging those four clips that the embedding of those four Clips I liked just using that then is the search Vector yeah and then yeah that kind of ranking stuff you know we just did like an integration with coheres re-ranking models and that takes his input like the you know the the query and the document and replacing the query with like a user biography and so maybe first search yeah yeah I think all that's so powerful but kind of the so the you know this in my notes that I'm replying but the the one thing that you said that I really want to get more thoughts on is that hierarchy of segments I've never heard that before so that like you know like a moment and I've you know I've been playing with summarization chains it is like this idea where it's like the prompt is like you will receive the clips one by one as well as the local summary so far or the local list of chapters so far and you just you know but that kind of idea of maybe having like a topic modeling where you you know you have the latency the embeddings of all the clips and then you cluster those clips and then you know you have like hierarchical topic modeling yeah can you just tell me more about how you see that perspective of yeah hierarchy of segments I think there's definitely something to that yeah yeah so I mean I think it's really interesting for like it's it's um well first actually I mean it's all kind of related the the and I would I would also love to hear we could do this later your theorization uh take because that's uh that's interesting for us actually the arization uh which surprised me uh significantly boosted performance of the rest of the chain um and I I suppose it might be a you know we did I didn't try to do just llm guest sterization maybe that would have been good enough um but if you just compare to the plain transcript um it really made a big difference to the rest of the chain um and you can kind of see it yourself if you if you yourself just look at a transcript with theirization without um it's much easier to understand what's actually happening um with this little piece of information um but on the on the um hierarchy of segments um so what's interesting actually with ref to Vector I don't know if you saw Yohe like of a baby AGI Fame like he he did he posted a tweet that is uh an averaging of all the embeddings of his tweets and then back to text uh and it's really interesting to see what the result of that was um and uh what you can tell is that the kind of highest order bit is style to some extent uh it looks like right you cannot mistake that text that came out for something other than a tweet uh it's a tweet by a techie right it's like kind of like so so that's the first kind of highest order bit to some extent that gets maintained there um so the way we think about um transcription and honestly uh sorry not your screenshot but segmentation um is you're trying to find the flow of topics right the thing that represents a flow of topics the best is an embedding but you want uh you want to kind of lose the stylistic parts um when you try to figure out the flow of topics so we the first thing we do before we we look at embeddings is we uh kind of normalize the segments that we're looking at to to be of a uniform style to some extent so we say okay let's summarize what's happening in this clip from kind of this third person perspective so hey this person talked about this they brought that up like rather than just have that plain embedding of what was said directly um and and what we found is that when you do that then when you look at embedding uh distances that tells you when actually topics have shifted so um a strategy that we that we use is we basically embed we take all these segments we chunk them up we normalize them we embed the normalized text and then we compare the embedding distance between a chunk and the preceding chunk um and and we actually do this for the two previous predecessors right so you see okay like if you have all this array of segments essentially um you look how essentially you're asking how different is this segment from the segment right before it and from the segment right the previous before and before before right and then um what you see is when you find uh when when you you look at all of these for the entire podcast the spikes in embedding distance correspond to topic changes to to essentially segmentations and and you can do this uh when you do this with uh segments that represent very short segments then you get the shorter segments but you can repeat the same exact process with a longer segments right so let's say now you've found okay here is one short segment now let's take which consisted of a lot of these kind of you know micro segments right so now let's take this we summarize this we normalize that uh and you can repeat the same process again and now you see okay these all these these four segments that are each you know five minutes long they make a coherent segment that is you know 25 minutes long uh because there's this Arc change that happens you know that um I don't know if that made sense it's kind of uh oh yeah that sounds I mean it's I think uh kind of firstly that kind of like normalized the Style by making it like kind of I read this one paper that's called uh like retrieving texts based on abstract descriptions and so you take like a podcast clip and you prompt the language model to yeah like just just make the content normalize the style that's yeah that's a really interesting kind of thing and you know as you mentioned just looking at the embedding distance to detect topic change that that could be a huge cost saver compared to having the language model go through it and and yeah obviously I love any kind of application of embeddings but like um yeah really my you know my curiosity as we were talking is going into this like multi-discourse topic this is like you know you you are trying to embed a chunk of text that talks about multiple things like I think kind of as the tie kicked off this podcast we had a lot of speaker turns that mentioned multiple things like it would be like you know whisper diurization ref devec and ranking like all in the same kind of clip and so what kind of happens with embeddings is like an embedding for just that you know multi-topic thing is doesn't really work all that well so yeah what you're saying I'm trying to I'm trying to come up with a good question to follow up with it but that kind of like you know separating out the topics I think that is definitely a powerful thing for just um probably like text chunking in general probably for anyone building any kind of application with Vector databases they should be thinking about something to do with that I mean maybe you yeah I don't know maybe I mean and then I really like what you're saying about how you would try to then tie all the topics that you decompose into one big coherent thing yeah is it this is kind of my first time hearing this so I'm I'm sorry if I'm not like quick enough to come up with like a good reflection to it but that kind of uh yeah it's um obviously this is uses use case dependent but for podcasts where it's people speaking right it's not you know a lot of text that is natively text is kind of by default very structured you know if you look at archive documents they all kind of look the same they have the same style um so you can kind of get away without doing any type of normalization because it's already normalized by whoever wrote the thing in the first place but when you look at just conversation free-flowing conversation um then uh yeah if you know the example you said initially we started talking about okay abiate and uh Lambda index and all these things if we just look at the embeddings for that that will now connect to some other time that we said lamendix um very strongly but if you first summarize this then the summary would be well a Thai encounter uh discussed at a high level like a few uh llm Technologies including you know we V8 and lime addicts and so on and so forth so so now that's a very different thing to embed um because Now it connects to summarization right so maybe we also in the beginning of the podcast we discussed at a high level some other things right we also discussed at a high level what talk at AI does so so now these two things connect whereas maybe they wouldn't have before we normalize it um so for me for you know for kind of free form content the step of embedding is almost necessarily tied to some type of normalization by the language model to begin with um in order again if you talk about kind of a longer chunk of content uh that you're trying to relate to other long types uh long chunks of content yeah I think that's absolutely brilliant that's the first time I've well I I've I've been studying this problem of like domain generalization and deep learning like you train an image classifier on clipart and it can't generalize to photorealistic images or sketches like understanding the confounding impact of style on deep learning models and yeah this just really helped a click for me I mean I love that summary index the idea that you use the LMS to summarize something in the embedding of the summary ends up creating better search um yeah so I thought that was really a really interesting conversation topic let's um if we could pivot I love also talking about how can we make podcasting more interactive kind of the long form make it more interactive I'm curious what your thoughts are on that yeah um it's it's a tough one honestly so we we actually um got to this world in a very convoluted way we started just with podcasts um not with AI actually more it's kind of social um podcasting and uh and you know there's a bunch of of companies that started out around the same time um and basically none of them made it and it's because like there's something when you start to like really look at podcasts this world of podcasting I mean what works really well in this world is obviously tons of consumption right two billion hours a month that's a lot um a lot of by a lot of people right so it's it's pretty Universal in the US something like a third of of adults listen to uh our weekly podcast as soon as they list on average to two to three hours a week so so that's quite a lot like it's it's pretty variable so the thing that is broken is distribution right so people generally have um four to five podcasts they kind of keep track of uh and then they It's All or Nothing consumption anything that's outside of these four or five Pockets it will never even get the chance to enter their sphere um and if you and it's also All or Nothing at the level of the episode because you have this you know two three hour long episodes oftentimes and unless you listen to the whole thing you will never hear the 30 minutes in the middle um and and that's just really hard to solve um because again it's kind of it's a multitasking medium uh right in its native form like these two billion hours of consumption people consume pockets in other places too right on Tick Tock that's a lot there's Autumn it's from podcasts that go viral but but that's a very different mode of consuming this content which is also important um for spread especially but um but if you look up just plain podcast consumption it's generally a multitasking medium uh so people consume it while they're walking their dog while they're exercising um and interactivity is really hard there because you're you know screen off it's kind of like a screen off medium um which is why it can be long form by the way that's in itself pretty interesting you know because every other medium in our lives is pulling to shorter and shorter and shorter right like so YouTube videos went to tick tocks right so if I'm like 10 minutes long to now a minute long right that's kind of the average uh text went from blog posts to tweets right so most media kind of pull too short for them but but if you but podcast because it's a multitasking thing you know if you imagine in your mind walking and listening to One Minute one minute one minute like that sounds like hell um so it's like the long form is what works and then that kind of comes hand in hand with not as much interactivity uh so the question is when you want to add interactivity what where are the entry points that you can do that um easily uh or that kind of that correspond to user habits and user preferences um and I think so so there's like a few places that I mean first of all like just podcast is just a wealth of knowledge uh that is really fascinating that in a form that is really easy to consume uh so better integration with search you know you search for something on Google you find text probably there is some five-minute click from a podcast if depending on your topic that will be like much much better for you to like get then um then then a blog post that's talking about it but you'll get the blog post you won't get this five minute segment so um so so that's one place to add interactivity um and then on the consumption side there's there's a lot of things uh two we can talk about but maybe I'll pause it there so but I I think yeah I love that everything you said about kind of how you consume podcasts like for me you know I think that two to three hours is probably about accurate and you just like for me personally I'm sure it's accurate for everyone but like you know I usually am kind of tired from work or coding and something so it's more like I'm yeah walking around or like playing a video game and that's more the experience and so when I think about that kind of like uh clip recommendation thing I think maybe one way it could work is like you listen to Connor and it's high talking about this and then maybe they are some llm with a voice narrator that says like you know and now here's a clip from machine learning Street Talk where they also talk about this topic and maybe that's one way to build the experience where you have some like intermediate narrator that's really like an llm with a voice clip thing but you know something that you ended the conversation with uh your speaking turn as it did the podcast thing about it like transitions but um this uh like the the information value of pod guest Clips like you mentioned whether you Google search for a topic whether you would want to be brought to you know a medium article a YouTube video an archive paper or a podcast clip and I think the podcast clip like it adds that it's kind of like when you're at maybe like a spelling bee when you're like could you use it in a sentence I don't know why that would help you spell it better but like if the if the task instead of spelling it was like what's the meaning of it right like I feel like that kind of like conversational context of something like you know like one of my favorite projects is building this like you know llama index has this like uh query router across different indexes so I have the blog post the podcast transcriptions and then I have the wevia code base and so I feel like if it's if you're talking about like what is reftubec there's there's definitely a cleaner in the blog post or the code documentation but there's something to that like how they used in conversation kind of right like it's kind of like a it has like a unique information value to it and that kind of brings me the question I wanted to ask about is it's it's like like these podcasts it's almost like a sport like you and I are right now like think about like what are we gonna say next how am I going to react to What a tie is going to say and so I to me it's kind of like this like post-game analysis almost like I'd like I love like the interview study kind of papers I think like that is also something powerful it's like how do you go from like a podcast into kind of a blog or something that's more curated and more like a reflection of these things yeah um sorry can you say that like just the last part again so so the like sometimes like just as an anecdote I remember when I started the weba podcast and I had this conversation with Han Zhao the CEO of uh Gina Ai and he like just gave me this massive tour of neural search and I was like this should be like a survey paper there was like this should be like an academic survey but there was so much to it so like I feel like they're even like our conversation now is like how do we go from like all these topics we talk about kind of like transcribing it and then I feel like also kind of turning it into the written artifact as well could be an opportunity for this stuff yeah yeah I know for sure it's it's uh there's definitely if you look at the content that is kind of right now hidden inside of audio and video and it is hidden right because for the most part these are opaque files they live in a hardware assignment where and uh when they do get surfaced for you they it's very non-granular they they kind of here is a video that you know is relevant for this but okay it's an hour long wait where right um and uh yeah so I think there's a lot of interesting things of just putting this uh a plain textual form uh and it seems like it's getting closer and closer to actual reality um and then uh on the other side it's also very interesting to kind of uh you know there's this trend that's happening outside of podcasts and of personalization of content right like you the you know there's the South Park episode that went viral that got generated by the LM so you know people you can have a South Park episode generated just for you that nobody else watches like I think things will probably start to get more and more to that place probably not literal personalization at first but more kind of the long tail of content so here's like a segment of people that you know before it wouldn't make sense to make content just for them um how do you start to do that now that it's much cheaper to create content um and uh I think one of the biggest challenges there and in general with llms is this kind of like uh Mirage of like we're almost at 100 right who almost can do the whole thing by itself and then what you can find is that pretty consistently you can if the LM can do it at all you can get to like 80 90 percent like pretty quickly like right you're not gonna get like 30 like if it can do it at all like you can get it to 80 90 but then the last 10 percent is really difficult um to get and you know we kind of saw this with self-driving cars where they kind of got really really good really really fast and it's like okay next year that's full self driving next year next year and it's like still not here obviously with progresses continue to be made maybe it will be here in five years um so but but that's something I'm thinking about in terms of plain you know if if you're trying to actually compete on articles right like somebody once you read your article for Real uh rather than something else then I think that the human augmentation is really the most uh interesting place where you can maybe take the podcast episode and not literally put a blog post up but you can do 90 of the work and then if this is interesting enough a human comes in the loop and like finishes it up basically um for consumption so that so that's something that could be uh very interesting yeah that I mean that's that's perfect I was like I was kind of disappointed when I was when I ran my summarization of these podcasts experiments because yeah like it what it'll do is it'll like especially that sequential summarization chain it'll collapse the topics into lists because it like doesn't really understand what I'm talking about with weavia and like maybe just like as if anyone's interested in like what I'm working on out right now I'm kind of I've gotten really into the LM fine tuning thing and you know particularly we're building this model called gorilla but like I'm starting to learn more about it you know Samsung has helped me a ton with sub strategies like we V8 listeners are curious about anyways but like I agree really strongly with this like um I think we need to fine tune the llms to get that 90 to 100 like you know for the for I don't think just the gbt4 API well I don't know though it's so interesting I hear so many perspectives on this and and it's like this has been I always this topic comes up on every single web podcast right whether you need to fine-tune models or especially now as retrieval augmented generation is emerging like you know when I as I talk about like sometimes my frustration with these summarizations of podcasts is that it'll just like collapse the thing into a list so so it'll have like multiple topics like it'll be like you know Connor and Alexa first discovered uh talked about this you know they described this thing like and so like it captures the keyword and then it has some some sentence that summarizes the topic that makes some sense and then like on the next step it just boom collapse all that into just a common separated list and the last topic sorry that wasn't too clear but but really so so basically you're asking it to go from the transcript to uh a text array like a comma separated array of uh so I'm trying to capture like this summer you know like I my goal kind of was like can I automate the task of publishing a podcast of which I always like write a description and then I cut up the chapters and as it was writing the description it would you know it would just collapse the because it's got the it goes you will receive the clips one at a time as well as the summary so far and then you inject the summary so far and then the next then you Loop through each of the clips and so the summary so far will be like you know they just they discussed this topic then they discuss this topic and with the description of the topics and then it will just boom comma separated list of of just the keywords that describe the topic but anyways I I think I'm getting a little distracted from our main conversation topics but basically my conclusion is like I'm I'm very curious about what fine-tuning LMS are going to look like even though I'm still on the fence like I am working on this but I think like it also could be the case that just retrieval augmented generation could retrieve more background on each of the keywords and avoid the collapse that way so I don't know what the what the conclusion is going to be but it's a lot of fun supporting it but let me kind of come back into one of the things we were just talking about which is this social component to podcasting I think it's very interesting like as an example I really like Ben morica's data exchange podcast and I think one thing that would be interesting is like you know we there every now and then we have the same guess like um you know he talked to Jerry Lou I also had Jerry Lou on this podcast and it's like uh like I would be very interested in like what did Ben ask Jerry that I I didn't or you know if I imagine any you could compare any two podcasts like this it's maybe more interesting for me personally because like I was in the podcast but like you know that kind of like comparison of podcast and maybe share a guess and it's like what did they uniquely talk about that's kind of where I see the social angle going yeah yeah for sure I mean what's interesting now is that uh although honestly I would also have to talk about the the fine-tuning versus uh rag like let's see I think that's super fascinating um but but on the topic of what you just brought up you know it's kind of interesting you see this pattern in podcasting where a guest would go on a different podcast and sometimes you get diff totally different things um but sometimes they kind of say the same thing on five different podcasts oh yeah and they kind of have to because unless you subscribe to that podcast you won't hear this episode even though it's about the same it's roughly the same content so I think it would be interesting you know if we kind of take for granted that it doesn't matter necessarily where you said something um because the right content gets to the right person the right time then um I think these things will naturally evolve to be more distinct every time so like he's like okay we already talked about this people who care about this they already saw this now let's talk about something else um and obviously there's value to kind of repeating the same conversation sometimes but but that can be up to the person they don't kind of have to do this to get their message out to some extent hmm yeah it makes me think about like um I guess just the future of social media generally like if I you know just like looked up Bob inlight and I just saw like um you know a summary of what he's been talking about for the last week on you know Twitter Hacker News uh LinkedIn maybe even like I think because there's also this interesting kind of interception between like your internal company meetings like and maybe I'm going on topic but like it's kind of this whole all this like podcasting stuff I think it's just extremely related to just recording your team meetings within your company like I'd imagine like you know that that there could be there's so much value to be extracted by just you know getting more of capturing the information shared in these kind of meetings but yeah it's all it's all really interesting I I maybe wanted to pitch an idea to you which was like um you know one of these machine learning conferences iclr they developed something called open review where they you know they open source the um the reviews for papers and I was and I'm and you know Twitter has like Community notes Now sort of x x it's called X now I'm not it's gonna take me a while to remember to say that but well I still call Facebook I can't do the meta thing yeah I actually have started calling him I think because like the the Llama 2 models the image like because now it's so I'm seeing the like you know all these exciting things and it's like meta and it could be more for me too it's starting to do fine yeah yeah but um so I was curious about what like an open review Community notes for podcasts might look like where like maybe the platform shows the transcription and like you like because sometimes I say things on the podcast and then later I'm like ah I actually don't really think of it that way you know you your podcast basically it's like oh this is not quite right yeah yeah and like people could kind of yeah yeah that's that's a really interesting thing um I I think this like in an ideal world like you should have always you know while you're listening like be able to open your podcast app and and kind of see okay where can I go from here so kind of like a combination of me note but also okay this he heard something this sounds really interesting what are you know Clips related to this topic that you're right now listening to uh from other podcasts potentially um and uh and obviously with llms you can combine this with like a different perspective like and and this starts to apply more and more if you get to like political topics right there's like somebody's talking says something with this that moment oh that's an interesting argument like did somebody respond to this argument on some other podcast I think that that could be very very interesting um and uh and obviously when you start talking about Clips then um you don't even have to interrupt anything you can kind of let people if you display a clip you can display the counter clip at the same time you can display related tweets about it I mean yeah I think there's a lot of potential there and I'm a big fan of like open review I think that you know on Academia the the existing peer review system adds a complete unreasonable amount of friction and you can see what happens you know with tweets the community like or seats I don't know what we call them now like it actually works right like this like people like you get really useful context um and if we can do this like we should do this everywhere rather than add friction you know you could either block the Tweet until they got community reviewed or you can let everything go and and put allow context to surface up uh with intelligence uh kind of as objectively as possible um yeah I love that that like um yeah like um they they used to call it like misinformation detection and then there's all like there could be bias but I think yeah that kind of like here's one perspective here's the counter perspective it reminds me like well it kind of coming back to thinking about podcasting it reminds me of there's this language modeling decoding algorithm called tree of thoughts where tree of thoughts is like you sample all sorts of continue like you know because language models are stochastic models and so they could sample many different generations and so say I sampled 10 and then you know I keep expanding the tree with 10 from 10 you know and so like I there's so many directions the podcast could go in and I think the other thing that's really brilliant about that idea with podcasting is like you alternate the language models that continue the tree and they impersonate Conor impersonate a tie by like either being and this is comes back to our fine-tuning versus rag Thing by either being the fine-tuned Conor llm or being the rag Conor LM that's hooked up to like because I I think I prefer the rag thing as being the directions huge for the future because I think it's going to be easier I don't know it's it's a really tricky thing because it's getting really cheap to fine-tune language models with uh sparse fine tuning and all that stuff so but maybe it's right now it's definitely easier to just curate the Corpus of information and things I've said to them impersonate me with a rag to the chat to the like gpt4 API yeah but yeah I do think that like how many different ways could we have continued our conversation we're having right now right like I feel like maybe like in a meta breaking like the fourth wall everything everywhere all at once like what what's the other versions of the conversation yeah and then having all those transcriptions and then um it will okay so this is kind of another idea I had written down that I wanted to pitch to you is I had you Shang Wu on the podcast who's building something called chat Arena and so chat arena is like where you simulate conversations between people and Personnel other people yeah so the Stanford or something else I think uh UCL uh yeah sorry but and this would be another benefit of the community notes is you can always correct because it's sorry I'm getting off topic but this idea of like you simulate all these conversations and then you look for Intel like look for new ideas in the simulated conversation like that one that's that's interesting um it's kind of like like I've gotten a ton of new ideas from talking to you now and but it's like um well yeah it's like could an llm because you know these like how you explain to me like a lot of these topics just comes from your real experience I don't know if an llm impersonating you would be able to react to come up with these new things but yeah there's like a lot of debate I think about like whether synthetic data can discover new knowledge broadly kind of yeah and no it's it's super interesting I think it's it's uh I mean it all kind of connects also the fine tuning versus you know uh rag because I think we're all still learning it sounds like a cliche but it's true that we're all still learning there's this new tool in the world and we don't really know what it's good at and what it's not and you can really see that clearly if you look at any blog posts about like gpt3 when it came out you know like two years ago or maybe a little longer at this point like they're extremely naive from our perspective right now like people are like saying oh what does it mean that it can like autocomplete the story like that but not like that like and then like what we are discovering over time is no there's these these types of things it's good at these types of things it's bad these are you know uh you know rlhf like here's a technique a pretty simple technique where like one percent extra compute all of a sudden this thing is much more capable at uh this thing and I think we will find you know what I I think there's almost certainly room for fine tuning I I find it really hard to believe that like in two years like oh forget fine-tuning just do rag maybe I don't know but I find it a little bit hard to believe uh with existing technology because it's just so much it it's so effective at um especially kind of selecting pre-existing capabilities from the base model because otherwise all you have to to talk all you have to available to explain to the language model what you want is inside of the context window and that only gives you so much opportunity and practically it's often just way way too verbose you know people do like few shot learning if you're trying to get a Json out of a language model like you might give it five examples ten examples like you're not going to give it 5 000 examples in your context Windows um and uh but with fine tuning you can do that and I mean that's what when openai added function calling like that's you know people try to solve that problem with chains for a month and you can get to some places like you know there's uh there's a bunch of pro there's like the guardrails project there's uh Json former but but the fine-tuning really really picks this up and on the other hand if you just care about plain facts then yeah probably it seems like that's a place where um rag is uh more helpful than you know fine-tuning new facts into the model um but uh sorry I'm forget I'm forgetting like how we we there was like the point here another good point to have that I'll test the yeah no I I got a ton it out of that I mean I think um yeah I can't reveal who said this yet but um a recent review podcast it separated it between if you want to teach the language model a new skill versus then fine tune versus knowledge then rag is one perspective but think about it like learn a skill versus learn knowledge like a lawyer you know has the knowledge of like here's the law says and then they have the skill of how to craft the argument from the laws kind of a and I can kind of relate to that thing and I guess kind of you know you mentioned the long context thing I mean and then there's kind of the idea of also like well instead of investing the energy and fine-tuning the language model maybe you fine-tune the embedding model or just tweak your search systems with like the rank you know ranking tuning and maybe there's just more to that giving it the perfect context but kind of like something and I hope I'm not about to get get us off topic too much but like this kind of thing of you mentioned like gbt3 to gbt4 I mean what a phase shift right like gbt3 you had to like you know few shop prompt it to do a task badly I mean like I almost set your opinion it would like come on you know and so now it's like talking to you fluently and it's it's like blown the world's mind like everyone you talk to if you talk to them about Chachi BT they're like they get it and it's crazy where whereas I don't think people really reacted to that with gbt3 but I think kind of like to me it's like this idea of like experience grounds language or on meaning and uh understanding it kind coming towards nlu paper from Emily Bender I I forget the title but like the the language models that are doing RL HF to like do customer support like we had this thing Kappa AI in our slack and it was you know it answers your questions about weaviate and I imagine having a conversation with that language model can maybe be interesting because you're like what are you learning like you answered 5 000 questions like what are people asking what are you learning and maybe there's something to that I'm not sure that's where we got it before right like the creativity part so I'm actually in my personal life forget like product development like I've I've had many interactions with a language model that felt like it evoked creativity right like it like you bounce ideas you know I mean even rubber ducking can be helpful but this was not a rubber duck right like it actually gave its own creativity now I I think probably it got I don't think it made things up it only got at them from pre-existing things that are related but you know I am not aware of these things that are out there um and B like one of the big advantages of language models is you know they don't get psychologically stuck I guess they can right you can get this Loop but every time every run is a fresh run um and um so that can help you get out of some you know you might see a problem in this kind of very you know laser focused blindfolds uh way uh and that language unlock you kind of open up a new direction that you maybe you would have gone to eventually but but it will take you a while so yeah I think there's definitely opportunity to actually make these things interesting uh and especially the more intelligent small intelligent models um yeah like you said gpd4 versus gpt3 is a big big jump um yeah absolutely I think that um you know some people it's only interpolating but it's like okay but it's interpolating between a super high dimensional space and it's like the inner you know the interpolations in this space are still going to be really interesting yeah like measure I love that topic of like how do you measure generalization broadly but yeah also I think this has been an incredible coverage of topics and so I yeah kind of wrapping this up I I think we did explore so many topics but is there maybe anything you want to conclude with kind of on like how you see the future of podcasting I mean I know we talked about a lot of potential things but like kind of maybe one thing that maybe you even had like maybe before we started talking we already kind of like what what you're convinced of yeah so I mean I think you know podcasting and generally audio video um you know like I said like talking we're building this audio video database is the first internal kind of design partner of this um product and I think what's obvious is if you look three years down the line one way or another um audio video are gonna become as searchable as text um where you can search and search and analyzable right so you can now do analysis at the level of semantics um you know like podcast is one application of this call centers I think that we brought this up at another Point that's another big application and right now you have to build all this custom code to answer pretty general questions like you know where are there calls where the customer got angry and in the end actually got resolved well um and in legal analysis people like do Legion assets over audio and video uh so so this this is I think what's really interesting is the trend of taking these unstructured you know speech and uh and video later too when you look at actual pixel on the screen and um make them as analyzable as we're used to with text uh so so I think that's going to be very interesting and because there's so much knowledge and high quality knowledge locked away in these formats I think that will that will have a a big impact uh yeah I I really turn into the exploding head emoji because the um like this post game analysis I you know I had a conversation with my brother-in-law this weekend who does uh stand up comedy and and we're talking about like stand-up comedy versus like giving an academic lecture and kind of like the way the crowd reacts to it and it's and it just makes me think like you know after you do uh comedy or a comedy podcast maybe you'd be like you know how did I do how many laughs did I get and that kind of visual information but I think it's also very interesting for like the academic thing because it's not as obvious like you know if you laugh like you laugh and it can probably easily detect that whereas like if you pitch like an idea in the other in the guest like likes it or you like you know like you're like oh you know like it's like more of a subtle thing and I think that kind of like post-game analysis of like you know how did I do in the podcast like was I interesting and I think there probably is a lot of like um audio visual like information that you would not get out of just the text transcript right but right now I'm I'm personally very by towards the text I you know that's where we're starting out like right now we're not really looking at visual stuff at all that that most of the information is I think in the text yeah but I mean certainly that audio video there's certainly there's going to be breakthroughs in deep learning in that I yeah yeah yeah really cool supposed to be in that's high thank you so much for joining the weba podcast this is such an engaging conversation for me I learned so much to change opened my eyes about this thank you very much for having me it was a really fun ", "type": "Video", "name": "Atai Barkai on PodcastGPT - Weaviate Podcast #62!", "path": "", "link": "https://www.youtube.com/watch?v=DgDbtuGugqA", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}