{"text": "A guided conversation about HNSW by Connor Shorten between Yury Malkov, Staff ML Engineer at Twitter and the co-inventor of ... \n[Music]today we have an incredible guest yurimalkov staff machine learning engineerat twitter and one of the inventors ofthe h sw algorithm which is one of thecore drivers of the vector index thatfacilitates this massive informationretrieval with the vva vector searchengine and we also have eddie anddillocker co-founder and cto of we v8 sothis should be such an incredibleconversation between hearing about uhyuri's perspective as the scientistcoming up with these things and buildingthese things out and also eddien'sexperience of integrating these thingsin we've and i'm so excited to hear thetrading of ideas between these two andget into all sorts of details betweenthese approximate nearest neighboralgorithms and details of building upthese vector index data structures soeddie and can you tell us about uh yourexperience with hsw how you came acrossit and the uh the the key questions thatyou're dying to ask yuriyeah yeah i i don't actually evenremember exactly how i came across itbut i do remember that when i saw it iwas like wow this is this is this isexactly what we're what we need becausewe were in that phase of sort of uh justdoing a proof of concept of vva ingeneral like can we add value in thatspace is there a value for somethingsuch as a vector database which wedidn't call that at that time yetbecause the term basically hadn't beeninvented yet um but yeah we were justplaying around with with just bruteforce search just to to basicallyapprove the concept and then we welooked into okay how can we do this atscale how can we do it more efficientlyand i came across the hnsw paper andit's like wow this is this is kind ofwhat we need and and the first iremember the first thing that i triedout was basicallyumcan we can we incrementally update thisso because for a database it's veryimportant that you don't just build youryour data set once and then you have itand then it's basically read-only um andyeahwould this be something that waspossible and to my surprise it actuallywas and not just was it possible but itseems like the entire design about howyou insert data into an hnsw indexrevolves around the idea that you justbasically use it to search it um whichis really cool for for a database andthen we we can get more about this laterwe added some more things to addfiltering to it and uh deletes and thesekind of things and that really made it agreat uh base for for a database yeah sofirst of all yuri welcome and thank youvery much for making that that kind ofresearch open and so that we can buildupon it and um yeah what i'd like todive uh into is basically the firstquestion how how did you get this ideawhere does it come from like what whatdid you do or or was there was theredemand because it's i think if i seecorrectly the paper was first releasedin 2016 and even to this day we'reseeing that new users are coming to vbathere's a certain portion who's activelylooking for vector search but there'salso a person a portion of users whodon't even know what that is who justsee the benefits and want to get intothis so thinking back six years thismust have been such a novel idea wasthis driven by user demand for for afaster a n search or was this more of aresearch topic basically that interestedyou and how how did you get to to buildthisuh hello so uh yeah it has a much longerhistoryso uh there was a predecessor of hmswuh and uh well yeah maybe the release ofhcw actually uhaffected by the surge of interestso the thethe the predecessor for uh hmsw was likewe call it nsw or msw so it wasstarted development developing i thinkin like something like 2005soway beforethis time and uh yeah i shouldmention some people who are also workingthere so i work with likethe idea there was that we should buildthedistributed search system that willscale to petabytesand it was a kind of a mix of uhof a symbolicsearch system and uh like a vector butmostly symbolic so the idea there waslike we have objectsuh so like there'sof things we have lots of uh devicesthere so they have some properties likein the three form and uh we like make athree form queriesand search there like using that and thethe idea was to use small world networksuh to do that andso we had other papersuh based on that and uh like this wordwas pick up by leonid boitsov at somepoint andso he implemented it in theenema slip which is like one of thelibrariesthat you can use for nearest neighborresearch and uh when he implemented ithe had also worked with some otherpeople like nikita avril andand uhalso alexanderso uh there was a like it turned outthere is an interest but because beforethat there was likevery little interest so there is therewas interesting from the searchcommunitythat is uh like who did uhumwe did algorithms for generaluh metric search but uh there were ididn't see any like this much interestfrom uh community that diduh vector nearest neighbors church so wewere targeting at the general metricspacesso andonly at some point we switched to likethis euclidean and because like a signsimilarity spaces just because there wasa demand for thatbut uh before that that was acomplicated like graphspace matrixand uh well essentiallyhnswuh was a solution to uhscalability issues that we have so theprevious algorithm had apoly logarithmic scalability so it waslog n squaredand uh so and werealizedwhat was the reason for that so we madea fix and that was uh hmswwow this is this is so cool hearing howit goes back even further andnoticing this transition from yeah frommetric space into into um cosine or dotwhich we basically use for for vectorsimilarity comparison todayand um that is actually something thatnow that you you say we are using in vvawe're actually also using hmsw for umfor geo search so this was because ourpoc that we built uh was built on umon elasticsearch at the time just forfor general search and that was afeature there and then we trans when wetransition is that okay we also need asolution to to do yeah just geocoordinates to basically just find whatis uh what is related or not related butsort of what is the shortest distance inyeah in a circle around it and um ithink at some point we just tried thisout if well mention this w we alreadyhad it could we just plug in a differentdistance function and it worked and ikind of almost forgot about it again butit's super cool to hear that this wasactually the history of it that is notjust a coincidence but yeah this was waspart of of umwhere it originally uh came fromum yeah i've mentioned umthe factthat we can build up an hmsw indexincrementally and this is something thati i just have to ask this is this wasthis a design decision or was this moreof a more of a coincidence um so so ifyou you say the history is um yeahwithinsearch already that that makes a lot ofsense but did you actively try toachieve this or was this yeah basicallya coincidence that that we can change itincrementallyuh yeah that was a design decision sobasically soso it also went from history so theprevious algorithms uh that we use werekind of the same so they were based onincremental insertions but uh so like interms of library and uhperformance for there is a publiclibrary that i support hmsw leap so ididn't support features that would blockuh like thisthis feature so the incrementalinsertions soand initially the idea was that we arekind of building like a peta scaledatabases and they're like it's a it'simpossible not tobuild it incrementallyand uh so they're also there likethey were distributed there was uh likeplanted support likeincreasing the size of theuh storage like dynamically so we canadd nodesto the the system so yeah that was adesign decision soit should be dynamicreally cool and especially that you saythat you had databases in mind umbecause when we chose it well we knewthat we wanted to build up a databasebut of course we didn't know thatdecision but we basically picked hmswamong other things because um it allowedfor that incremental and as you say fora database there's there's really noother wayumyeah so something that um to dive a bitdeeper into into how h and sw work uhsomething that you have in there is thisum i think in the code it's just calledthe heuristic two which is basically away to to trim the connections and tokeep those uhto yeah to the most valuable uh can youexplain to our our viewers sort of umwhat it does and then also how you cameup with that because i think when i readthis the first time and and saw what itdoes i thought it was pretty genius andi wanted to to know yeah how does onecome up with such a such a heuristicwell uhyeah so there are also predecessors thathelp this come up coming up with this sobasically uh so there is aa general no solution to thechurch ingraphs in proximity graph so you have adilenoid graphwhich is uh like a dual duo of uhvaranoit isolation so you you like everygreedy search in this graph will end upat the exact nearest neighbor so what iknow it isolation is basically so youhave a space you have points and uh soyou have and you divide this spaceuh so that like each cell uh correspondsto the nearest neighbors of theuh element which is the center of thecell so it is pretty basicand uhso like if you look on theory so thereare uh sub graphsof this dylanograph because well thisgraph has a huge problem like hugeperformance implications if you try toapply it to vector spaces because uh uhso well maybe yeahyeah so there is a uh like a theoremand we can prove it which was done andwork on spatial approximation three 3that if you have a general metric spaceso you cannot have access to all thepoints you don't know like what elsepoints can bein the space uhthere is no way that you canreconstruct the exact uhdyno graph from those points or or anysub graph likei meanlike that the graph that hasthat has uh the graph as a sub graph sothat will be like fully connected graphso you can come up with some examples uhwith new points because that that can bearbitrary soyou will have you will have to connectevery point to each other whichobviously doesn't scaleuhyou know there are sub graphs and one ofthesub graphs uh well one is onenear nearest neighbor graph so thethe graph in which you connect touh the first nearest neighborso that is also always a sub graph of uhdylanograph but also there is a relativeneighborhood graphuh which is alsouhlike a a sub graphbutso there is a like a generalway to extend this graph so add add fewedgesuh because there is like some ambiguityand how you define so if you just createa relative neighborhood graph it wouldnot beuh it would not be searchable uh[Music]like for the points inside this graphso well basically basically you canthink of the uhlike thisrng like extended graphor heuristic it builds the graph that'ssupposed to beuh searchable uh for the points insidethe graph so if you if you like one ofthe points inside the graph uh and dogreedy search it it should end up inthis point so it will find itso that that that that is a reasonableheuristic and like a nice property of itis that in the one g space it uhit goes tojust the listsoso it translates to a simple listand and that way like hmsw was likelikelike converting into a skip list whichis well known algorithmso this was known before actually so uhso i i i first saw like similaruh heuristics in a spatial approximationtree which also had this theorem whichis a nice paper but uh uh like after ilike first published the paper onarchive i did the search and found thatit actually was used before in1992in 1993 by area and mount though like ieven though i had cited actually thispaper in my paper i haven't noticed thatthey actually use itso that i was focused on something elsebut uh so this uh heuristic is likepretty obvious if youthink about itand i know there are some extensions butthe so it's they are all kind of goingaround itaround the same solutionyeah yeah i i would imagine that itbecomes pretty obvious if you have thatkind of a background on on what alreadyexists and of course also if you havenarrowed downthe problems so to speak like if you ifyou umyeah if you're looking for a solution toum yeah basically keep only the mostvaluable edges and not keep keep so manyso um yeah i think what'swhat's very obvious to you here um yeahi think this would have takenme or someone else just um looking at itwithout that that kind of researchbackground wouldn't have been so soobviousum yeah maybe so like a good examplewhere uhlike it's one g so that was also well uhinitially i saw when the rotation is wpaper it didn't have this heuristicuhand uh so but it was performing alsolike rather well it was performing muchbetter than the previous algorithm onsome data sets but uh like it lost to uhvp3 onon a few data sets and uh so becausethose data sets werevery low dimensional and we knew that ifyou just connect to nearest neighbors onone dso you will have like lots of connectedcomponents so you don't have a singleconnecting components it's like easy tovisualize so you would need to have alistthere and thesolike if you target low dimensional dataso that that that is kind of likeuh alsoit's simple to understand whyyou should use it yeah coolum sonext thing on my list something that wedo with uh hnsw and this may also besomething that that yeah i don't know ifit was ever intended but it's somethingthat that works great and that i was abit surprised about that it worked sogreat um so one of the usps in vv8 isthat you can do a filtered vector searchso basically apply your your scalarfilterumin the same way that you would do it inany kind of traditionalsearch engine and then just have um yeahthe vector search only on those louditems the way we do it is basically umjust as this we have an inverted indexinverted index basically gives you anallow list of ids and then uh when wetraverse the h and sw graph basically umwhile we follow every connection whichof course we need to do so that we wekeep discovering the graphwe only add to the result set those thatare on the on the umon the allow list basically and thatthat works really well and um at firstwe yeah we just tried it out and andthought it was was yeah it led to theresults that we expected um and thefirst time we actually measured it i wasreally surprised that the recall doesn'tdrop at all yes it varies a bitdepending on how how more you increasethe filter um but overall it it yeah itit it kept pretty constant like even ifit went to to the very restrictivefilters then of course it gets slowerand slower because in the end it'sbasically a greedy search or a bruteforce search through the entire entirespace um but the quality doesn't drop inand the point where it gets slow onlycomes very late so what we do in vivianis basically like if we know that thesearch will be this restrictive weactually just skip the index which is toapproach for a search anyway and thenonly on those allowed ids and then ofcourse it's faster again then then umyeah having the index search foreverything and discard most of thethingsum yeah can you maybe also give a bit ofbackground of why that is why the theintegrity basically of the graphkeeps so high or why this works or if ifthis is something that yeah you maybeyou've thought about or used this aswell orumyeah ohso yeah so it's very nice that you madeit work so uhwell it was kind of intended but uh wenever tested it so the idea was uh thatyou can change your existence on the flyso the there there might be a differencebetween like the distance which youbuilt the index and the which are usedfor querying and that is kind ofthe same thing so you are likechanging the distance a bituhwell if there is an overlap between thelike you can think of the of the if youhave one metric and it has some optimalgraph you have a second metric and ithas somewhat different graph there isoverlapping neighborsso that it should workso if there is an overlapping neighborso you filter out those elements andthelikeuh like most of their this half of theneighbors are the same so you shouldhave like it isn't performancebut uh yeah so that is complicated thingso like we we discussed it and we hadsome previous algorithm but uh it neveri don't think it ever worked properly souh when we tried it we ended up inbuilding like special structures tosupportuh the search so you have like one indexmatch so you build the sub index for itso we try to build that and like thatwas pretty messy and i thoughtwell that probably should be the optimalsolution so when you like you have uhlike your set of possible parameters andyou partition them like group them andbuild separate index for them if youknow that the user is likely to havelike searching this set you you canspeed up and build a separate index forthatbut that is very messy like i'm not aspecialist in those kind of optimoptimization so i never even tried itbut it's nice to see that uh likeyou made it workyeah uh yeah yeah it's interesting thatyou mentioned the the partitioningbecause this is also something um thatwethat we have i i don't want to say wehave it in yeah basically have itplanned because the way that wepartition so right now if you have innvidia in a distributed setting umbasically the entire index just getscharted acrossall the nodes that you have so basicallythat means each each subgraph so tospeak or each graph basically just onlycontains a specific portion umright now this is random right now weuse we use basically a hashing functionbased on the id and we are only theassumption that the id doesn't sort offollow any kind of pattern so it'sbasically the goal as of now is just toget it evenly distributed that i don'tknow if you have a memory limit on oneof the notesthat basically yeah you can distributeit among many nodes but something thatwe can do there is we can do thatsharding based on one of thoseproperties that we know that that wouldbe filtered for so for example somethingthat we see quite often is that peoplewant to use multi-tenancy on vb8 andthat the tend would then basically bethe partitioning key and then if we havea filter where we said like tenant isfive then all we'd have to do in thiscase is knowing okay ten and five lieson node three and node three basicallyhas its own html graph and then from theperspective of h and sw on that nodeit's basically an unfiltered searchbecause yeah we've already chosen theright graph which i think is the thesame thing umbasically a very similar thing to whatyou just mentionedum this is actually a perfect perfectsegue to the next point um so i want togo bit intoyeah right now we've talked about sortof why hmsw is the way it is and howcertain things work and i'm also veryinterested inhow where things can go from here sosomething that i have umon my and my list is just talking aboutmemory in general so maybe for ourviewers um the way that hmsw is designedand please correct me if i'm wrong thereum it's basically that both the graph aswell as the vectors are held in memoryso that is just an assumption basicallythat's that's there which is forfast fast lookup basically both both forfast distance computation is forfollowing the edges efficientlyand this basically means that if youhave massive data sets your memoryrequirements also grow that of coursedepends on the build parameters but as avery very rough estimate what we've seenis basically that if you if you roughlycalculate with two times the amount ofall your vectors it it always sort ofsort of more or less matches a very veryrough estimate exact of course yeahdepends on the number of edges so if wethink aboutbillion scale vector search um thatis a lot of memory that we couldpotentially use and something that we'veseen pop up is basically totry and tackle this at what i would callthe application site so something thatthat's been a popular release binarypassage retrieval in the context of nlpso basically there the ideais that the vectors get compressed so tospeak into binary vectors where eachvector only holds a sub-portion of theinformation that the vector dimensiondid beforein bpr it's basically just if the thevectorif each element of the vector waspositive then it's the one if it wasnegative then it's a zero so you reallybasically lose so if you go from afloat32 vector to this binary vector youbasically lose umuh30 130two parts or basically sorry youbasically reduce it to 130 um yeahone over 32 of the original size losethat kind of information but of coursealso lose the the amount of memory thatyou need to to store and basically alsoincrease the search speed um thedownside of something uh like a binarypassage retrieval is that the way thatit's seen right now you also need tomake your model aware of this sobasically because you're outputtingcompletely different vectors typicallyyou would have a loss function thatwould optimize for both the the originalboth the cosine or dot product orwhatever it is lost and also this thisbinary loss which should then typicallybe a hamming distance between the thetwo vectors so what i'm thinking isthat the main topic here beingcompressionthis this kind of compression it's a inthis case of course a loss fullcompression but might also be other waysto do this umif we do this at the application layerwould there also be a chance to somehowget thisor this or other uhcompression techniques productquantization or something like this intoh and sw itself so that basically we saythe vectors as they areare just sort of they they stay the sameyour model is the samebut at the index side we're trying tocompress the vector somehow is thissomething that you could see compatiblewith hnsw or anything that maybe you'veeven thought about alreadyum wellyeah i i thought that that is alreadyimplemented in libraries so uh one thinglike one one one solution is supportedby phase so you can build the uh h andsw or i think it also now supports othergraph indices or uh quantized data sothere is an option for that so uh i alsohad a like i call first the paper uh ineccv 2018 uh which also did thequantization a and the hnsw but uh likefrom a different perspective that hnswis used for uhlike uh cluster selectionuh in uh like in in ivfand uh like as far as i know that isstill state of the art and uh like inbillion scale search so you can use amuch more clusters because you use likea graph index so that they're like weused one million clusters for onebillion elements and then you doproduct quantization there and like withsome tricks that like my co-authorsintroduced to save spaceand uh yeah that works pretty well soi think this solution is better than theone that is implemented in face or itwas implemented i'm not sure maybe theysupport both because uhin graph intercessors there is aso you have an overhead on the linksand uh like it's hard to compress linksso and if you cluster the elements uh sothat saves more spaceuh like if you like put them into asingle giant clusteruh it loses arecall for sure but uh like it's abetter way to save space than if youreduce the dimensionality of the elementat least to some pointbut uh yeah so you can do bothand well there are comparisons and likein the in the literatureso people try that i thinkthat is really great and that is coolbecause i didn't know aboutum face doing this this um yeah you knowso if they got you correctly they'rebasicallyum doing the quantization on the dataand then just using h and sw normally isthatthat yeahwell yes so that that was published in2018i think on cbpr so since uh there therewere other papersbased on that uh like i think extensionat least like i review papers uh forcurrent uh ictv so i see that there arelike follow-up papers coming on that sobut i'm not sure like which of them werepublished but that that paper wasdefinitely published and that is a partof uh like face open sourcenice this is definitely something thatwe have to check out both theimplementation as well as the thepapers of courseumyeah on a potentially similar topic umif we'reyeah another way maybe oftackling this problem is to also rely onthe disk and um something that we we'veseen pop-up is microsoft's disk a n forexample and um there they also have theproblem that it couldn't be updated so ithink the the newest revision is a freshdisk a num in general do you think hmsw the waythat it is right now could suit itselfto yeah storing parts of it on disk andhow would that how would that look likeohyeah sure i think there were likeprevious papers though i'm not sure ifthey're like super incremental editionsuh like i think also by microsoft thatuh like used hybridhardwareuh so like hmsw on top and likequantization and storing and this isthis in in thein the bottom so that makes sensebut uh that is more complicated so youare adaptingmore to hardware that you need that youhave soit's just likeit's uh it's engineering it's hardengineering so you need to have somehardware set you need to optimize towork it properlyso that totally makes sense so that canbe ssg that can be like distributedstorage likeuhyeah but uh it's hard onecool yeah but that's it's good to knowthat in general because i mean hardproblems to figure them out that'sthat's what we're here for and that'swhat our engineers are here for um butit's always good to know that yeah ingeneral uh that's a direction that thatum yeah that we can goumspeaking of a hybrid approachthe idea of building something somethinghybrid from a very high levelperspective is also something that we'vehad before we're just thinkingnot so much going going into changingthe algorithm itself but if we were tojust take let's say two out of the boxalgorithms one that supports incrementalbuilding and one that that doesn't sofor example uh if i'm now taking uhlet's say google scan or something whichwhich is notincrementallyupdatable and hnsw and if i try to sortof build a hybrid system um where theidea that i have in my head right nowbut maybe there's way better ideas wouldbe that well we say everything that'sthat's continuously imported basicallywe want to keep in the in the freshindex so to speak in the h and sw indexum and after a while we say like okaylet's build a second index so assumingthat the data hasn't changed anymore inthat index umcould we build it up into something likeyeah in this case scanstore that and reset basically the thefresh index and the idea of why we wantto want to keep the fresh index at allin place is basically that stuff isimmediately searchable so that thatdatabase you expect your import to to beimmediately searchable umand then of course one of the challengeswould behow do we handle deletes like what if inif we assume delete comes in in thefresh index then that would be easybecause there we can still handle itbefore we sort of compress it to thestatic index umbut yeahover time would probably be some sort ofa some sort of a compromise of markingsomething as deleted and rebuildingsomething similar to like a compactionin um in a traditional um lsm treedatabase or something like this yeah iwas wondering what do you think of thisthis idea is there something in generalthat you sayum yeah there is a potential in thiskind of things if we assume that we canuse indices that havefewer memory requirements or somethinglike this or would you say this ispotentially especially as you alreadyalso mentioned a hybrid approach this ismaybe the wrong approach to to combinethese things maybe there's a betterapproach if you want to achieve the samegoalswell uh like as aperson who worked on on the indices ithink those can be yeah those can belike joined togetherso uh scan is using like trees as as uhas like divide and conquer uhso and it has like fast uhquantization uh scheme so i when i firstsaw skan i heard that was kind ofcheatingbecause uhlike it uh it does not reallyuh[Music]decrease the computation whether it'skind of speeds them up and uh like therewas n and benchmark and some of the datasets there were actually integeruh but uh like you can rewrite your uhmetric function to beinteger instead of fourth and everyoneuse fault and like you can have aspeedup up so that oh they are kind ofcompressing the databutyeah scan actually works sometimesbetter uh onflow data as well sobut yeah you can combine them uh thereis also like for for the deletion pointof view i think deletions are prettyuh costlier and graph indices so uh h9swsupportsdeletions so that was uhessentially it waslike initially it was supported byflagging the elements which wascontributeduh and then there was a contribution byadding full deletions so like that whenthe graph was rerouted but that is veryexpensive so uh one deletion is uh likeequivalent to like many tens of up likeinsertionsso when well incision is not deletionit's an update but uhif you plug an element as deleted andthen updated to a new element that islike pretty much the same asdeletion soa reasonable strategy it would be justto just flag the deleted elementsand then when youdelete like half of them you rebuild theindex so your complexity does not changeyou have like a constant because likeyour you delete twice the elements andyou build index once solike scaling does not changeso uh i think that would be a betterlike from the library point of viewthat's like that's up to user so wecannot just rebuild the indexuhlike in background that should becontrolledbut uh like for a live like for for aservice i think that makes senseso yeah yeah this is actually this isvery interesting that you mentioned thisbecause this is exactly what we're doingright now just we're not i i think wehave a relatively short period that weused to to rebuilding basically so um weare doing the flagging as well and andbasically then we just have an asyncprocess where the user can control whenit runs but basically we do this basedon time at the moment so we don't do itbased on the size so probably as you saysince it's basically rebuildit might be more efficient to basicallywait longer or to maybe make thethreshold not so much time based butyeah space we say i don't know if 25 orso of the index are deleted we do anentire rebuild so that'syeah like forfrom a point of writing a paper so youhave like some degradation ofperformance like in terms of uh like uhso what would happen if if you deletethe element so your performance will goup or if you add a new element uhinstead so it willgo down because of the new elements sothere is a like if you set a thresholdon performance it probably wouldn'twould depend on how much what percentageof the data set you have deletedsosolike for a comparison point of view sothat would be like i think the optimalstrategy yeah yeah that makes a lot ofsense and definitely something that ithink we don't have a lot of a lot ofmetrics yet on what the actual cost isover time because typically what we'veseen in the use cases so far is thedeletions are somewhat rarer thanimports um but yeah there's a lot ofroom probably for for improving thisumyeah coming up to me to my last questionactually already umwe've talked a bit about the the futureof hmsw right now and you've alsomentioned that there is of course futureresearch um but maybe as a whole wouldyou say hnsw is in a sense basicallyit's complete or would you say there arecertain things that that yeah shouldstill follow oryeah so sort oflet's say five years from now ten yearsfrom now uh do you think the learningsfrom hsw would make it into somethingcompletely new or do you think maybethere's there's things that would beincorporated in it or maybe somethingcompletely different basically how doyou see the long-term future of of hswas a as a concept basicallyseriouslyi i hope that it will last for long solike the main idea of hnsw is that youcando like the main idea that i like thatyou can doa search with algorithmic complexitywell at least low dimensional datauh with the graph structure so graphstructure is known like to perform welland you can like havebothbest of both worlds so you can have likeuh searching in graph and alsoalgorithmic scalability of the treesand the like graph are probably betterthan trees because they don't need tobacktrack so if you're using the searchindex likewell not annoying but like similar withbacktrackinguh so you have you lose some performancebecause there is like a symmetry of thelinks in the graphso and well hmsw is a simple fixtoget the algorithmic scalabilityuh well there are many ways how the likeinitial papers can be improved so asmentioned the heuristics might beupdated in some papers uh there is alsoi think in some companies and somepeople use a different constructionstrategy so they add elements from thebottomlike when you have a new element or theyadd to the bottom layer and then iconvert to thehigher level thatends in better performanceuhalso likethere are works on like earlierterminating strategyuhworks or like uh automatic tuning so youdon't need to figure out the parametersso right now is like a big issue forlike many of the libraries that usersdon't know how to tune the parametersand uh like there are i think only a fewlibraries that support automatedparameter tuningand the yeah thatis a nice thing so uh also there arei think there are there is a gooddirection on improving the graph uh withml so right now it doesn't have anymachine learninguh so it's assumed that uh every elementin the graph can be like a query butthere might be a query distributionwhich is like very differentandsoit isso there is a there is an applicationlike in recommender systemsuh like if you don't use inner productdistance uh uh but like embed your itemsin some complex spaces there is a chancethat some items are like they will neverappearin search with inner product it can kindof tell it because it's like it'susually close to zerobut if you are using something morecomplicateduhor you just like going better than uh l2you cannot tell thatbut if you have a query set you canactually understandthat uhsome of them elements they are neverreturned you can just remove them fromthe graph and don't waste memoryuh and like links uh on those elementsuhsoyeah there areyeah many ways to do that and one of thelike future i thought that it was goingto be distributed at some pointsolike when you're trying to do liketrillionuh elements in the data sets so like youhave to distribute it and uh it's notreally clear how like what is the bestpractical way of doing that so initiallyi thought that it's going to be like youhave an element and it resides on somenode and you have links so it'smentioned in the paper so you can youcan try to make ituh distributed because skip list wasmade distributed before and this is kindof like a skip listin some senseuh but that might not be the beststrategy so you still need like a lot ofhopes and each hope is a hope to adifferent node in the networkso that is very costly you might alsowant tolikeaggregate those at some pointyeah so everyone so i think we've justgotten quite a master class on thesevector index algorithms and unless eddiehas one more question i was going to tryto kind of maybe parse this a little bitand give people maybe beginner peoplekind of a little bit because they mighthave gotten a little bit lost in some ofthe details of this but any more uhquestions that ian or no no thanks i'velearned a lot today um and this wassuper super interesting and i'm i'mdefinitely very interested in a coupleof links to some of the papers thatyou've mentioned if you could providethose that would be that would be supergreatyeah sureyeah i've got quite the reading list toowith um i think billion scalesimilarities search with gpus um yurimentioned revisiting the invertedindices for billion scale approximatenearest neighbor search is a recent onebuilding up on these ideas that we justlearned about but to kind of step back alittle bit i think um yeah that wasdefinitely a master class and you twoknow so much about these things that ithink um you did do like drill into thislanguage and just to kind of maybeprovide a little more of uh of like aintroduction to people i i really likedum in the beginning eddie and mentionedthe searching through geo coordinatesand this was this is the first blog posti ever published on towards data scienceactually the first thing i ever even putout there before a youtube video is thisidea of of doing a geospatial search andusing say k-means clustering to havecentroids that speed up compared to saydoing a brute force distance calculationbetween your query latitude longitudeand then every latitude longitude inamerica so you do these or whereveryou're searching so you do these k-meancentroids and i think that's a greatexample for people out there listeningto build up an intuition of what we'retalking about is you uh you structurethese centroids and then you canpropagate them up and as we talk aboutthese umdifferent ways of say having a smallworld graph you have this property thatsmall world graphs describes this ideathat i'm say six people away fromknowing barack obamathrough my relationships and that's thisnetwork science thing that that alsobehaves nicely with these umnearest neighbor distances and this ideaof vector distances one topic that wastalked about that i think is extremelyinteresting is this generalization fromyou know one dimensional distance we canuse just a binary search tree thentwo-dimensional distance we get intothat geo-coordinate search and then youknow n-dimensionals we're talking aboutuh these different ideas like say binarypassage retrieval that edian mentionedwhere we have the binary encoding tospeed up having to put these massivevectors into the memory and then thingslike one other thing i wanted to talkabout and ask about was uh the localitysensitive hashing algorithm wheresay you have a you know ten thousanddimensional vector and you just kind ofuh hit like i think it's like fifty uh aslice of scientific the dimensions to dothat kind of um distance calculationwhat do you think about uh localitysensitive hashing as a way of computingthe distanceuh welllocality sensitive hashing is notlearnable so it doesn't have anylearnable component so uh that thatthat's why i i thought it was notpracticalso there are lots of like papers andtheory so because you don't have anylike dependence on datauh in terms ofthe projections so like you can buildsome theory and like have nicetheoretical papers about itbut uh like i never saw it like it to belike really practical and to outperformlike optimizeduh tribalsso and i thought that uh like people whodo it they also see like uh likequantizationbased algorithms uh like asbetter because they are learnable likethey can adapt to theuh the thedata set that you haveso could we also take a little uh give alittle more background about productquantization is product quantizationbasically the idea of you have yourpre-trained embeddings and then youlearn a linear mapping into a lowerdimensional space from that pre-trainedembedding or can you give me just moreunderstanding of how productquantization workswell i'm not a big specialist in productquantization butessentially what it does in the simplestform so you have a vectorand youwell you want to doquantization and uh well you can do likejust quantization of each dimensionuh but you will lose some informationand what people do they uh well or onthe other end you can do quantization uhin terms of k means so you have yourdata you build k meansand now your quantize it but there issomething in between and that is productquantization so you have your vectorwhich is subdivided by parts and you dok means in each of the partsand now you have like a sub vector anduh likeit isaddressed by a k means cluster in itso and that works well pretty reasonablein many cases yeah i want to ask um onemore uh uh definition what are uhvoronoi regionsuh what voronoi regions are so like youdivide your space into cellsuh which are closer to some point ofyour uh database likeso like you have a vector space you havepoints andso you divide just the space aroundthose points so that uh likeevery point in the cellisishas this element as the closestsobasically you divide the space but bywhatare the nearest neighbors of thoseelements so as i was listening to theexplanation of adding symbolic filtersinto h sw which to me is one of the mostunique features that are that has beenimplemented in wev8 and i think it'ssuch an interesting part of this couldyou tell me more about how this works ithink i i'm sorry i got a little lost inthe details of of when you wereexplaining it originally uh well youshould ask 18 years oldyeah so basically it's itit's really simple from from what we doum because essentially we still traversethe the entire graph so if you you thinkof the entire graph as being your yourfull data set um essentially what we dois just if you come acrosslet's say there are no restrictions thenyou come across your your basically yourexit conditions at some points you knowthis is my result setall we do is basically saying well ifyou have something that's on your resultset that's not in our allow list wesimply skip it and then that of coursemeans you need to find something else sobasically you keep traversing that thatthat graph space and and one of thereasons why hmsw is efficient in um inthe first place is basically because youdon't have to traverse the entire graphso by basically filtering somethingyou're saying like yes i'm removing ornot i'm removing them and you still haveto traverse them but i'm basicallyignoring i think is the better termsome of those some of those elements andyeah the the payoff basically is that ineed to traverse a couple of more edgesand find other other verse hall setsbut in the end basically you can stillfind them i think this is the part thatthat umyeah maybe i can't explain so much butthat we just tried out and that we weresurprised by that the recall was stillthere that that basically this this umyeah integrity is lostthe way that we use the inverted indexbasicallyit's just a tool in front like like oncewe start traversing the h and sw indexbasically we just need to know whichelements do we accept and which we don'tand we happen to use an inverted indexbut basically this is this like theagent's w graph at that point it doesn'tknow where the the data came frombasically it's just alist of of ids that we allow versuslists that we that we skip awesome sothere's so much information behind thisand i think it's extremely interestingand i wanted to kind of transition intothis topic uh yuri i saw your paper umtitledcnn with large memory layers and i wasthinking about memory augmented agentsand this idea that you can use thesevector structures to have a reallymassive memory and i was firstintroduced to locality sensitive hashingin the paper the reformer that was oneof the first papers presenting anefficient transformer model that couldhave a longer input window than 512tokens so i was wondering if you had anythoughts on how we can use these kindsof ideas fromvector indexing into say going beyond512 tokens and i was also thinking thatkind of generally even if you have ayou know a slow inference timewith say a memory that a model that'sproducing something like abstractivesummarization over scientific literaturethat you know that kind of trade-off ofslow inference but for the artifact ofan abstract of summarization of saypapers aboutvector indexes would be kind of worth itso i was thinking about if you had anyideas on how you could maybe put h andsw into say efficient transformers andthat kind of ideauh wellumso you can you usean indices and uh so therefore works ithink there was a like latelydeep mindretroand uh before that there were otherpapers uh on uh like supplementing theuh neural networks withdatabases so i think that is a verypromising directionand uhwell that is very similar to graphlearning itself so you add some contextandwell you can think ofthat and an index can provide uh like uhlike what what does it provide itprovides the nearest neighbor graphso and uh like if youtreat uhlike you have you have a document youfind nearest neighborsand you put some information andin the transformer there you can saythat is the typical graph learning isjust you have an additional input likefrom k n graphand uh like those approaches like h ands wthey are also built on graphsso uhyou canyou can just uh use this graph as inputand hopefully like what i like i wouldhope to see is that the transformerswould learn to route on this graphso and uh they wouldn't be able to doa nearest neighbor's chargeuh like what they actually need becausewhen you just add nearest neighbors likeuh like in retro and other there is noguarantee that thoseuhneighbors are actually the bestinformation that you can supply uh forthe network like for for the neuralnetwork there might be some elementsaround and if you allow the network toroute like select like on which point ofthe graph you should goit it might it might perform better wellit should perform better likemy idea thatlike nearest neighbors are not verymeaningfulbecausesorrybecause we can treat the query the queryvalue the query key value projectionsand attention as kind of like a vectorsearch also right and we could so wecould kind of blow up the size of theintermediate query key value matricesand then use these vector searches toit i think it would slow it down a lotbut then you could haveyou know a lot of information like amixture of expertsokay okay so uhyeah i think i uhi know i i understood the question so uhyeah you can like uhdo something like reformer so reformersyou use lshuh well you can use alsoother indices so so here lsa which isgood because it is fast so you don'tneed to build an indexfor that uhbut like maybewell uhlikewell you would probably want to build anindex here because you have a differenttoken distribution from sample to sampleso the like an optimal index would bealso different i thought there was afollow up uh on this uhreformer where there was like clusteredso instead of lsh there were clusters toprojectwhich is like just a better version oflshand there uhlike i'm not very sure like where hmswcould fit inuhbecause uh so you will need to build itfirstso that's a like an engineeringtrade-off like whether you should dobrutal forceorlikeso it's no it's not it's not very clearso like you cansobecause it's an engineeringuhit is an engineering problem like inmany cases you can't just uhgo away like with a go with lsh it willbe sloweruhyeah it will be less accurate but if youlike increase the size so it might befine uhso it isit is hard to like to really predictwhether it should it it should be usedthere but uh for the documents i thinklike uh like if you have lots ofdocuments and that those are like verydifferentand uh they can reside so they don'tchange over time so you can pre-build anindex for them likea graph so that that would make muchmore senseyeah i guess i was thinking about kindof uh like these contrastive learningmethods where you have maybe anexponential moving average of one copyof the weights and maybe you would cachethat and build up the index of thoseactivations maybe some kind of pipelinelike that and then i was also well uhyeahsorry so uhlike when you do contrastive learning uhso usually the nearest neighbor searchlike unless you have like a really hugedatalike really huge data set is very fastbecause you do it on gpuand uh like you can easily search likewithini don't know 10 million vectors likewhat what is the the closest uhelements because transformers or othernetworks that are run to produce theloader much slowerso uh there i don't think likeyou should use them butuh if you want to go like to billionscaleso like you have like a cache and uhthat that that that would be auh that would be meaningfulhave you ever thought about using thisthese vector search algorithms to searchfor similar activation pathways in verylarge neural networksoh well yes i did so it's a like veryhandy techniques to findlike the problems with data sets so likelike for images for instance if you haveuhlikeif you if you have some like badlyclassified example so you can look forsimilar activations in the data setso and uhyeah i used that for cleaning data setssome some time ago and uh yeah but thatthat that usually worksat least for imagesyeah i love that application of wev8 asa tool for uh doing de-duplication ofsay language modeling data and and uhthey have the uh the merge variable asno eddie knows all about that will mergethe nearby things and i love thatapplication of it for umfor d uh data deduplication so um onekind of meta question i wanted to ask isabout um you know billion scalesimilarities surge just to build moreintuition could you tell us about uhlike the application of billion scalesearch that inspires you andyou know keeps you working on theseproblems well uhso there are many applications like inow work in twitterso and twitter there arelike billions of uh tweets that you canshow to useruh like if you are doing out of networkuhlike candidatessothe otherthere are papers so well there are likealsouh likewellthere are also textso like uhif you want to build a really hugelanguage model so right now you can usea lot of text and uh like also like icanuh mentionthe like they give my paperso uh like it depends on a n and uh likelikei can easily imagine billion scale thereso we have like a billionbillion documents so that is like webscalewell even like less than web scale ithink last web scale is uh like10 to 15elements like or moreokay cool cool eddie do you haveanything to umwrap it up with no other than to to saythank you very much this is reallyinsightful and i mean it's it's alwaysnice to to hear the the minds and that'sbehind these kind of algorithms that youknow but it's also so cool to yeah getthis this additional insight of bothsort of in both directions like both inthe history of what happened before andi mean i knew that agents w was based onnsw but basically that's where where myhistory part ended and and yes you madeas you told us there's there's it'sbuilding on so much more and of coursealways research is building up on otherrace version this is just very cool tosee how this these things come togetherum that we are here at this point andand also with the same in the otherdirection to see what's still possiblelike uh potential integrations ofsomething like like product quantizationinto into hmsw and yeah all these theseother kind of things so umsuper cool very very insightful for us ihope it was also interesting for the forour listeners when we when we go verydeep into those topics but i i enjoyedit very much so thank you for my sideyeah thank you that was uhenjoy like and a nice like an enjoyabletalk to me as wellsoyeah thank you thank youthank you so much eddie and yuri and toour listeners yeah such a master classin the depth of these topics and youknow i'm really impressed with how muchyou guys have explored these vectorindexing techniques and i'm excited tobe learning more about it as well sothank you so much for watching theeighth episode of the we vva podcastmore to come on you know developing ourunderstanding of vector indexing andthese things that are available in wevg8which i think is such an exciting partof we've eight that you can't overlookis the implementation of these vectorindexes that let you do very large scalesimilarity searches[Music]", "type": "Video", "name": "Weaviate Podcast #10 - Yury Malkov and Etienne Dilocker about HNSW in Vector Search and Weaviate", "path": "", "link": "https://www.youtube.com/watch?v=WijYx9_Bpkw", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}