{"text": "This video explores a new paper exploring the use of summarization chains to represent long texts and use (original text, ... \nhey everyone thank you so much for watching weave on YouTube this video will dive into a new paper titled retrieving texts based on abstract descriptions quickly before getting into it if you like these kind of paper summary videos please let us know by hitting the like button and subscribing to the channel here's a quick overview of the paper in case you're in a hurry and don't want to hear the full experimental details so with this concept of retrieving text based on abstract descriptions abstract descriptions what drew me to this paper is it's similarity to this concept of summary indexing so summary indexing describes maybe at the chunk level so for each text Chunk say you're you know you have full podcast Clips or paragraphs of text instead of just vectorizing that full podcast clip you'd instead use say a large language model to write a summary of of the text and then vectorize that summary of the text we'll talk about how economical it is to use a large language model for this task but that's kind of the general setup is you know you summarize the full thing into some whether you know how you write the summaries whether it's an abstract description that kind of blur out the specific details or just a summary of the you know the info information content then vectorizing and indexing this content is quite an interesting idea it's also very interesting for this concept of a top level index so say your data is you know podcasts or books something that has like a high level structure and then it has is made up of these Atomic chunks so you you use like a summarization chain to summarize the chunks and then you vectorize this summary so now you can search through the you know at the podcast level and then search within the chunks once you have the matching podcast another interesting idea so this kind of concept of summary indexing and then abstract descriptions that's kind of like the the semantic match that Drew me to this paper but they have this really interesting new connection where you use these summaries to train embedding models and they publish these new embedding models they make some big claims about the quality of them we've integrated them in weviate and they do look like they're pretty interesting models so it's quite an interesting paper that actually ends up being related to these works like in pairs promptigator you know this kind of idea of use large language models to generate say queries or in this case descriptions that you can then use to you know benchmark the performance of say you know different zero shot embedding models or train your own embedding models so this concept of summary indexing I have to give a quick shout out to Jerry Lew Alum index he's reading a lot of great content about you know this kind of idea of using these summarization chains to then represent you know high level objects based on their summaries there's also really uh relevant for what we've been doing with generative feedback loops generative feedback loops generally describing taking data from wevia sending it to an llm and then saving that generated data back to alleviate so you can semantic search through the generated results so you know you write the summary and then save the summary back to Evie and then you index the summary to search through the summaries so here's a quick explanation of what they do in the paper they use this prompt let's write abstract descriptions of sentences you know the few shot prompting idea where you give it a sentence and an example of an abstract description so in this case an abstract description it doesn't mean a summary of long text it means a transformation of the text into the abstract thing it represents and so we'll see an example of that in a second also in the prompt you have this kind of like note you know some more meta information about the tasks this kind of prompting strategy then one other interesting thing about the prompting strategy is that you have this write five good and then five bad descriptions and then output a Json file with the keys good bad this is a really interesting prompting trick that you can use to kind of get like you know 10 generation out of one prompt and then you know you index the keys and give things like say the Lang chain output parser camel case this like one example of how you structure these outputs to make it obey the Json file I mean these languages models are really surprisingly good at respecting this kind of Json file structure and then you can you know parse it and send it along the chain so here's an example of what they mean by abstract descriptions say you have a sentence like dopamine constitutes about 80 of the catecholamine content in the brain you then transform that specific sentence into a neurotransmitter found in the brain in high concentrations because this abstract description is probably more likely to match the query so like imagine a user and their information needs they come and they're like curious about neurotransmitters in the brain and so this kind of abstract description it probably like vectorizes and indexes better than this specific sentence and that's kind of the key Insight but the other key Insight is that they also generate bad descriptions and then they use this to train embedding models so it's a really interesting like generative data augmentation and then self-supervised way to train these next generation of embedding models and you know they've publish new embedding models so staying on this concept quickly of how embedding models are trained this is something at weavier we're always trying to stay on top of and give people the best advice of where the best text embedding models are which you know vectorization models use so the general idea is that you want to have good negatives when you're training so in your contrastive learning objective you're trying to make the vector representation of this meerkat similar to the vector representation of this alternate view of a meerkat but then not a similar Vector to this picture of New York or this golden retriever so the idea of good negatives versus bad negatives is that you know contrasting the the vectors of these two meerkat pictures with a dog is is better for learning a semantic embedding space than contrasting it with this picture of a building so we previously had looked at this paper from Ori ROM and his collaborators about uh the spider algorithm so the spider algorithm is a way to look at overlapping engrams between passages in Wikipedia articles and so you know like if two passages of text you have the same engram like the priesthood for himself and his male descendants that'd be like a good example of a positive and then you just you know use the other Snippets of text in the Wikipedia page as negatives so so we're always on this hunt for how can we self-supervise how can we you know automatically annotate positives and negatives as scale with web scale data and that's kind of like where the zero shot embedding models come from so this paper is presenting a really interesting new way to you know instead of looking at these overlapping engrams from Wikipedia Pages you're contrasting the good descriptions with the bad descriptions so this is the anchor this is the positive and this is the negative and that helps with you know the representation learning task and you know they generate quite a large scale amount of these roughly I think like 169 000 that they use to do this so you can imagine you know scaling this up like crazy and it's pretty interesting so so in the end they're gonna you know combine two loss functions the triplet loss triplet losses where you have you know the distance between the anchor and the positive and the distance between the anchor and the negative this kind of thing and then you also have that in batch negative thing and then you know you weight the two loss functions with this Alpha then in the end the um the way that they evaluate this you know the the state of evaluating embedding models is you know you have the beer Benchmark which is a pretty solid thing but generally it's sort of hand wavy you know this this is sort of the Pinnacle of that but that you know they're gonna they have a human study survey and they you know they do show that the humans rate the humans in the study rate this model to perform better than these other bass lines like all mpnet based two that's a really strong you know model available in hugging face sentence Transformers or say bm25 which is the keyword matching so you know five is so you know we'll get more into the details but basically they show the uh the human annotators five search results uh they a mix of five from one of the models and five from another of the models and then they rate the top five results and so you know they're saying that they chose this one five most of the time and then say bm25 was chosen like zero out of the times at most times so this thing that all these models perform better than bm25 really in that you know this model is the best quickly before diving further into the details of the paper quick thank you to March Nantes for adding these models into Eva's text effect Transformers images so if you want to run these yourself here is the image for adding these new models so you know if you want to just stop the video now and just see what these models can do with your weeviate example then this is the image to do so so thanks Martin so following that quick description of the paper let's kind of walk through it a little slower so the background is that this paper is marrying these ideas of summary indexing with this General concept of large language model generated training data for self-supervising embedding model trainings so this idea of summary indexing has been one of the most powerful kind of ideas around Lang chain llama index and these kind of llm tools so this article from Erica Cardenas published February 21st is you know when Lang chain first came out one of the most powerful things was were these summarization chains we also you know we've been building on this General concept of sequential chains you know like where the name Lang chain comes from where you chain together other language model calls such that the output of the first call is the input to the second call and so on summarization chains have been the most interesting kind of uh example of this so so probably my favorite one is this refine idea this is probably the best way to say summarize a podcast so you take each of the of the clips of the podcast and you say write a summary you know you'll receive these clips one at a time so it writes this intermediate summary and then it keeps taking the clips updating that intermediate summary until eventually it's looped through all the podcast clips and ends up with a final summary there's also like this map re-rank and mapreduce I see these as really interesting for say question answering and I think this kind of refine ideas really interesting for summarization there's also an idea that I think originated in Lama index that was like this um tree summarized so you say line up all the text chunks you know in a list and then you kind of couple them in twos and build up the tree with the summary that way and you know aggregate it to the top so but generally this concept of you know applying These Chains to summarize content and and then you have this summary of content that you could then say vectorize and build indexes with or use and you know that's kind of the question is okay now what do we do with the summaries so this is also highly related to the concept of generative feedback loops that we're really heavily exploring at weviate this is the general concept of where we do these retrieval augmented Generations we send specific data from weviate to these generative models and then we save the resulting generation back in weeviate to then maybe you know access for a future retrieval augmented generation or in this case you know semantic search through the generated results so when we first published our blog post using Airbnb listing examples we were taking the symbolic properties of the Airbnb like you know how much it costs how many bedrooms it has and we took all that and we synthesized a text description of it that we would then index and search through so it's kind of a similar concept to this summarization index you know this idea of transforming data to facilitate searching through it the next really interesting idea and that's what this paper is that's why I think this paper is so interesting is is kind of marrying two different schools of thought and you know I love this when you had there's some connection that you hadn't originally thought of is this idea of large language model generated data for training search models so this is a picture from the in pairs paper you take a document like we don't know a lot about the effects of caffeine during pregnancy on you and your baby so it's best to limit the amount you get each day so you know imagine you have this big collection of text chunks it's like if you're building a tech search app with weave that's probably like the situation you find yourself in you probably don't have these questions already so you know having a query log already is more of like a big company thing to already have this kind of established thing so you can do is you can take the large language models and generate questions you know what are the effects of caffeine during pregnancy and this document you know would say be the best answer you have I don't know it's like specific enough for this question but like this this would be like a way to pass this into the language model prompt in some way to generate queries now you have like query document pairs to you know either say compare the embedding models if you want to say you know I don't want to use the open embedding models because it's 15 36 Dimensions per you know vector and then I can't afford it's more expensive so I want to use say the 384 dimensional embeddings from say the sentence Transformer DPR models you know and then so like you you want to ask these questions of which embedding model should I use but you don't have data to Benchmark this is a really interesting solution to that to give you data to then you know understand what the difference in the models are going to be for your search app and then also it's interesting for training custom embedding models for your data which would also probably give you better results so two extremely interesting things that you can do with this kind of llm generated search data so here's the motivation behind the novel Insight instead of generating specific queries that match specific information we generate these abstract representations so for example a lot of these information retrieval data sets and then the embedding models that are trained on them like say they're trained on the squad you know question answering data set it asks questions like what is the atomic number of oxygen the answer is eight or like what year did this King rule over you know France for and then it's like some specific date so most of these data sets that have been created and you know published are like query and then like specific thing that comes out of the query rather than this kind of like abstract information need and the authors motivate this by saying the user is not interested in a definition or a single answer but for sentences whose content is a specific instantiation of their query I think that this is a great quote because it's it's more in line with like this kind of semantic search concept you're looking for like you know semantic matches not like specific matches to questions in the thing so you know it's interesting that if you're interested in this kind of thing there's also the set of embedding models called the instructor models where they're really digging into the different information needs and it's quite an interesting topic but they give this example of say you have this query substance abuse in animals right like just you know this kind of abstract query that would be a better match for the Studies have shown that a subpopulation of primates chronically consume intoxicating amounts of alcohol so it's like this kind of query compared to something like which primate cons it has been found to show like and then it would be like some specific kind of you know species of it that kind of difference in what these data sets look like so another quote from the paper just sort of laying that out more explicitly systems that are trained to retrieve passages that contain answers to questions for example train mostly on Squad and data sets like this Beyond being focused on questions rather than assertions are also focused on specific answers rather than abstract situations so it's a you know it's a bit meta digging into thinking about information needs and information retrieval but I think it makes a lot of sense you you know usually with these kind of queries you're like fuzzly looking for some kind of uh similar concept rather than the exact match to your question with the embedding model so digging further into the paper this is the prompt that's being used to generate these valid and invalid descriptions per sentence so we start off with the main prompt of let's write abstract descriptions of sentences example so so this is kind of the new the structure of problems these days is you start off with some description of the task then you have a few shot examples of what input output should look like for the task so sometimes you can omit this but generally you know I've done a few interviews with people about you know who work on like agents in production this kind of thing and generally the consensus seems to be right now that keeping these few shot examples of you know task descriptions or say how to use a tool is still quite a productive thing to improve the performance of the language model so you have the sentence you know Pilates role in events leading to the crucifixion lent themselves to melodrama even tragedy and plotty often has a role in medieval mystery plays so then you generate this abstract description a description of a historical religious figure's involvement in a significant event and it's later portrayal and art so you know you see how you blur out the specific uh you know the specific details of the of the text and then you instead have this abstract description of what the text is about so then in addition to the few shot exam apples you have this kind of note to the language model and how to complete the task descriptions can differ in the level of abstraction granularity and the part of the sentence they focus on some descriptions need to be abstract While others should be concrete and detailed so then you have kind of the output prompt so you're saying for the following sentence right up to five good and Standalone independent descriptions and five bad descriptions which may be related but are clearly wrong and then here's another really interesting detail I'll put a Json file with keys good and bad so or get you know good comma bad because it like might even help a little bit more with the Json file but so this detail is quite interesting because this is this concept of say you know Langston's recently released output parser output parser's camel case like this concept of how do we make language models behave obey the kind of API syntax to then kind of chain out the outputs and you know parse them with say you know just like just a basic kind of python parser for how you would take this kind of output and then save each of the generations and then this kind of idea of using one prompt to generate 10 Generations this way you can also imagine that there's some kind of cross attention across the 10 examples it's generating so it's kind of like listing out things it's a super interesting prompt and then you pass along the Chain by having it follow this structured output so here's the next detailed paper that I think is extremely interesting they're going to take the output of the main prompt and then feed it into the make more abstract prompt so in my own experiments of generating queries for the podcast I've also thought of doing this kind of thing where say you generate you know five queries and then you have to ask it you know to take those queries as input and then say you know is this query specific to the document because you know it'll generate some queries that like aren't specific to the document so that's kind of what I'd found in my little like how I've been investigating this a little bit so I think it's really interesting that they're formalizing this make more abstracts prompt in this kind of prompt chaining with degenerative feedback loop for the llm generated training data so what they do is out of the five valid descriptions they're going to take each one of them and they're going to generate three more from them by feeding them into this next prompt so remember again you have the Json file so you index the Json file with the good keys and then you know you have so this new prompt so you have the sentence description pair and then you have a very abstract description so again we have this few shot example thing where we're giving the large language model an example of what we want this to look like and then you know we roll with that where we plug in the sentence we plug in the we plug in the original sentence we plug in the description the valid description that was generated in the you know first thing and then we get a very abstract description so it was a really interesting way of kind of chaining this along and refining the output of the um you know curating these descriptions or summaries so here's another really important detail of this they do this at pretty large scale so if if you were skeptical about whether these models would be any good you know we're checking them out with say the sentence Transformers Library I think this is a reason to be convinced to check them out so you know for each of 165 960 Wikipedia sentences they generate five dollar descriptions and five misleading descriptions so you know doing the math that's like roughly like 800 000 of each and then they are also going to do that and make more abstract prompt from you know three of seventy thousand so so for each of these you know 165 000 roughly Wikipedia sentences they're gonna have you know five to eight additional Pairs and then five misleading pairs so is the train is um you you have this kind of like local comparison where that's the way they did the metrics but so anyway so so it's interesting to see the scale of it really you know doing the million of each thing and you know doing that kind of scale it it's it's a large enough scale to think that these models would be like quality zero shot embedding models of course then you you know could imagine scaling this up further further with more Wikipedia sentences but it's pretty interesting so quickly before we even dive further into the human study evaluation which is quite interesting in its own way let's quickly touch on how embedding models are trained just quickly in case you're not familiar with it basically you have this kind of triplet loss or you have this info nce loss they're going to combine both of them in multitask learning the way that you do the triplet loss is you take the so this is the vector encoding of the anchor this is the vector encoding of the positive Vector encoding of the negative whereas with the batch negative you Loop through the encodings of all the other examples in the mini batch so you know the way deep learning models are trained is you sample a batch of data points and so with contrastive learning info nce you you have your anchor and positive and maybe also you would have a negative in there that you could kind of mix of this but you could in so you have the anchor the positive and then just every other anchor positive pair that's in that mini batch is used as a negative and you know you Loop through and you take the you know e to the you know the distance and this is how you do this kind of like differentiable loss function and scale it by doing this stuff like taking the log of it and that kind of idea so with the triplet loss you kind of like explicitly compare two points you have the margin hyper parameter that is you know then plus the vector distance the so the L2 distance between the anchor and the positive vectors subtract that with the distance between the anchor and the negative and then you normalize it also by having this large batch so this kind of law I think just the quick commentary is like this kind of triplet loss thing this is pretty friendly this you know not too ugly to train with whereas this kind of info nce thing I think this is a little more prohibitive with people training embedding models because this is a bit more of a chaotic thing to orchestrate with how you have to like you know have this like diagonal matrix to get the labels so this is where I think a lot of the the skill and the expertise in training deep learning models comes in and yeah just sort of an interesting detail if you're interested in you know exactly how the embedding models are optimized so here's another pretty interesting detail of this paper instead of reporting the performance on of their model on Ms Marco beer benchmarks they're using a human study with Mechanical Turk workers so you begin by instructing the human the workers about what the task is and So you you're shown a description in 10 sentences you're asked to choose the five most relevant descriptions so then what that ends up looking like is you know a period of difficulty and sorrow for an individual and then you choose you know you rank the top five by choosing five from this list to submit is which you think are the retrieve sentences that best fit this description so in the end you see that they chose the new abstract Sim models you know on the average they would choose 3.78 out of the five selected sentences whenever they did a head-to-head comparison ablating each of these two models against each other in like you know tournament style combinatorics so you know you had pairs of abstract Sim 5 versus all MP net based V2 5 and you know that made up to ten so each of these two have been compared with each other five you know in this kind of amount of times and then you see kind of the bar chart the performance how many times it was chosen out of the 10 and showing that you know these models performed pretty well in the study so here's some examples you know if you want to go through it you see kind of the difference in how how it captures the specifics but you know how it captures this kind of abstract concept with this new generation the kind of better results that leads to some specific examples of that if you want to dive into it so here are some final takeaways I had after reading this paper retrieving text based on abstract description so firstly I'm thinking heavily about this kind of summary index concept and I think that what we're seeing is the cost of large language model inferences getting cheaper and cheaper such that this will be something that's you know more common to do as we you know we're seeing all these open source models I think Falcon being the latest one at the time of this recording but this we're probably going to see cheaper and cheaper large language model inference such that summarizing all of your documents is more of a you know economically sensible thing to do based on your app or whatever you're building so it's very interesting with this respect to custom data for your document so I imagine a lot of weviate users have you know just the documents and rather than the queries because you're building kind of like a new app you don't already have all this like user data in this kind of log of query so so now I see there's kind of three things that you get out of doing this llm generated data first you get the summary index you can use to kind of index the data to search through you can also use then the original text and then these abstract descriptions and then how well you're able to retrieve the full text based on the abstract descriptions to compare the performance of zero shot models and you can also use that as data to train custom search models so kind of three use cases you get out of this llm generated data you build a summary index you can ablate the different embedding models and then you can maybe train your own custom models if you want to go for that so then I think this General concept of summary indexes is super interesting for this concept of top level indexing so you know if I have a whole podcast I want to summarize the whole podcast by doing that create and refine summarization chain through each of the clips and then once I find the podcast that matches my query then I will search within the podcast so I'm just generally interested in that kind of phenomenon I didn't I don't think it the paper particularly made me think more I believe about this but I just think it's something that's very interesting so then there was another paper that came out recently which was Voyager this Minecraft paper where you learn skills and then you interestingly what they did also is that they had an abstract description of the skill programs and that's what got indexed when they were retrieving so it's just an interesting thing of how you would summarize some kind of really abstract thing to then index the summary of the thing and Vector search cross summaries I think is really interesting so maybe something that's just like an idea is maybe we could retrieve with abstract descriptions and then apply these re-rankers like cross encoders to the original full text as an idea and then another thing is I just think this kind of how do we judge the quality of these llm generated descriptions I think this is quite an interesting topic so with respect to this last point of how do we judge description quality we VA Podcast 48 I had the pleasure of interviewing Professor Laura Dietz and I love this paper perspectives and large language models for relevance judgments and I think it's really interesting to relate to this so you know what we previously had you know human judgment this you know like with say like the Trek benchmarks for information retrieval like humans would just you know annotate the relevance of queries or say like you know we would divide up our team so everyone on our team writes 10 queries for each of our like documentation or something like this in order to Benchmark our search systems now we say have ai assistance where we use the you know the llm generated summary of the document then we have like a human filtering layer where the humans like look at the output and kind of rate them or maybe we do just like different tasks generally I think another interesting idea is two llms each generated judgment and a human selects the better one so this is kind of similar to me to the make a more abstract prompt where you say take the original five valid descriptions and then you generate three more you could also Imagine an intermediate layer there where the LM takes the five generated descriptions and says you know you can only take three which of the three you know best abstract summaries of this content that kind of idea of like chaining away with the language model and so you know it's just like a really interesting you know the times and thing this whole spectrum of how we're thinking about using large language models to you know annotate say query document pairs relevance or say also judge the quality of generated text this is like a problem as old as generative modeling is how do you judge the quality of generated images of generated text and so with images we had things like the freshay score Inception distance for say like Gans and now with text it's like when you write a summary of uh you know some text how are you gonna you can't like what they have now is like Rouge scores blue spelled b-l-e-u scores which is like engram overlap to some human annotated gold summary of the text so it's like how are we going to be annotating this kind of abstract description so I really enjoyed this paper and I think this is this is another interesting perspective on llm generated data that we can use for search so one more thing before wrapping up I'm working on a new demo of abstract description summarization indexes using the dog fooding of the Wii VA podcast transcription so if you haven't seen all this project already please check it out and stay tuned for generative feedback loops part two exploring generated summaries for each of these podcast clips and the summaries of the podcast themselves exploring writing personalized summaries of the podcast for different people writing biographies of people based on the podcast content and also say automatically finding chapters from the podcast by prompting it so exploring further how we can use large language models to transform our data and then save that transform data back into our database for some kind of future purpose thank you so much for watching this paper summary video of retrieving texts based on abstract descriptions please subscribe to the channel for more content around you know Vector surge retrieval augmented generation approximate nearest neighbor search and all these cool topics please check out check out weaviate at weeviate IO or open source on GitHub at wegate weeviate and follow us on Twitter at weeviate IO thanks again so much for watching ", "type": "Video", "name": "retrieving_texts_based_on_abstract_descriptions_explained", "path": "", "link": "https://www.youtube.com/watch?v=mn5P79n541Y", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}