{"text": "Hey everyone! Thank you so much for watching the 67th Weaviate Podcast, announcing Weaviate on the AWS Marketplace! \nhey everyone thank you so much forwatching the weba podcast I amincredibly honored and grateful to hostthese guests on the podcast this topicwill be absolutely epic firstly wewelcome back we VA Chief technologyofficer and co-founder Eddie anddillocker Eddie and thank you so muchfor joining the podcast again thanks forhaving me and of course I am beyondexcited to welcome farshad farabakshiana principal Solutions architect atAmazon web services farshad thank you somuch for joining the podcast hey thankyou so much and by the way my My TitleHere is actually gen AI specialist but alot of times people think I'm aSolutions architect so it's all goodnice to meet you allso could we uh kick it off with like umI think maybe you know some for ournewer listeners could we maybe evenstart from the beginning Eddie and ifyou could kind of just you know what isweeviate even just kind of like you knowalleviating the evolution of it that'sled us to this point of you know talkingabout with AWS about running it on thecloud and kind of how far the journeyhas comegotcha yeah sure let me try to keep thisbrief because I don't fill the entirepodcast with this and and hopefully as Isaw on the the self-hosting part we candefinitely dive deeper into that laterin the security privacy Etc as box uhbut basically vb8 is a vector databaseor for those of you who don't know whatthat is it's essentially the long-termmemory for for a large language model sofor retrieve login to generation forexample definitely something that we'regoing to dive into I guess uh and bv8 isan open source Vector database you canuse vv8 as a SAS offering from from usso that's our BB account service but youdon't have to if you don't want to ifyou don't want to give your data awayyou can self-host it or you can help ushelp you host it in your environment inyour Cloud tenant in your VPC that's oneof the USPS of vb8 because that allowsyou to to stay in control over all ofyour data and then if you have that youcan query bb8 with Vector search sodense embeddings use dens embeddings forfor um for the modern day retrieval youcan do the classic stuff so bm25 you canmix it with hybrid you get all kinds ofother database operations and all thatat Great scale and uh yeah that was thatwas my elevator pitch yeah I thinkthat's awesome I think just you knowthis core of this hsw Vector index doingyou know fast nearest neighbor search isyou know and then especially with thisretrieval augmented generation I thinkthis has caused just a boom and you knowthe explosion of how often people areusing these kind of things so uh farshadcan you maybe tell us about kind of youknow how you see the state of vectordatabases and retrieval augmentedgenerationyeah I mean at a high level mostcustomers that I talk to they're like ohwe just want to fine tune everythingright and you know fine-tuning is greatit's it's how youteach your model new skills but it's notnecessarily the best way to have it noknowledgeum and so Vector databases is the mostreliable way to do that and soone of the most common use cases that wesee is hey uh here's a really simple oneright like we want our customer serviceagents to be way more efficient rightand we want them to very quickly gothrough a very relatively largeknowledge base and retrieve preciseinformation there's honestly not muchreason to do fine tuning there unlessyou want your language model to sounddifferent in how it interfaces withthose customers but if you want toretrieve specific info like something ina data sheet Vector databases are verypowerful that so pretty commonarchitecture that we see come up as astrong llm plus that could be eightVector database combined to offer thatsolutionyeah it's amazing the chat Bots thinglike the whole like chat with yourdocuments you know chat with yourspecific customer tickets I think isjust you know an incredible evangelistfor all this and yes I think we've kindof set the stage of what the technologyis you know we V8 Vector database helpsyou retrieve relevant information to putin the context of llms let's get intothe Super exciting topic that you knowwe have ourspecial guests the technical experts onyou know what does it take to run thisin the cloudso a couple things rightthere's so many ways of doing this inthe cloud right now right so I'm gonnago top to bottom rightum and let's kind of view it asI view the LM as kind of like the centerthat everything goes aroundum if you want to work backwards from aproprietary proprietary llm like sayfrom like an anthropicum uh or a cohereumyou generally are limited to however thecloud provider lets you access that inthe case of Amazon we have a servicecalled bedrock what Bedrock is is an APIservice that lets you do prompting anddepending on the model provider actuallyfine-tuning as well right so you haveyour API portion of that what'sinteresting about Bedrock is that wenever share the data or the prompting tothe model providers so what's kind ofinteresting is that even though themodel providers say that they will neveruse the data our customers are like yeahbut we still want someone in betweenright so that's why Bedrock is one ofthe value propositions right so you haveyour API call goes toum the the model hosted on AWS but youcan't access the weights it's all opaqueand then around that is when you have tostart thinking about like well what Iwant to do on top of that right but inparallel to that whole thing is you haveopen source modelsllama 2 just came out right I think itwas last week kilometer two is doingreally well right and it's also opensource with something like llama two youcan use services like Bedrockum but you can also use services likeAmazon sagemaker jumpstart which thedifference between sagemaker jumpstartand Bedrock is that it's not serverlessusers have to actually manageinfrastructure as wellum but some people like that some peoplelike having more knobs to controlum you can also fine-tune llama2 rightso now once you start getting to some ofthese models that fine tuning is a partof it some people like to have dataengines that they incorporate RL HF intoitright sohere's a really simple example a lot offolks that build diffusion models theway they uh get human feedback isthey'll give you like four lowresolution models and then they pick theuser picks they wanted to make want tomake high resolutionand then now the model provider knowswhich one we've got the RL HF rightum so then you have a data engineinvolved right and then this is allusing managed Services we also have aton of customers that want to pre-traintheir own models uh a lot of folks tendto actually do this on bare metal ec2right so we'll get your a100 to look atyour h100s we have our own proprietarychips called traniums as well uh whichsupport things like llms and diffusionmodels and they'll basically take likeno joke likeup to like 4 000 gpus in one cluster totrain one large job right so they gethugeso there's so many different ways to doit and we're not even going to theinference side of it but this is kind oflike the model piece of it right andthen depending on how you're going to beusing that model you're going to beinterfacing with a vector database likelike from leviate and you're obviouslygoing to be using services like S3 tostore your data Maybe maybe you have adata Lake involved as wellum the interesting thing that is alittle bit different on training versusinference is thatwhen you're doing training all youreally care about is like your time totrain right it's just like train themodel as fast as possible uh when itcomes to inference all of a sudden youcare about how everything plays witheach other right it's like how tightdoes your vectorated database interfacewith your llm how tight does thateventually lead to your user experienceright and that's probably it's going tobe dependent on use case right so likefor example if you have a live customeragent that's having real-timeconversations with customers you'regoing to really care about latency atthat point right but if something islike oh we have a week to get thisresponse it's a complete completelydifferent architecture so now you startgetting into use case dependentarchitectures that decide whether or notthings are going to be good and thenonce you start really camera latencymodel distillation becomes important aswell right you want to get the model tobe as small as possible some people wantto get these models to work on the edgeum you know I I can think of companiestrying to build you know uh metaverse Imean whatever whatever you want to callit where latency becomes so importantwhere I can see a world where you'regoing to have custom ml chips forinference to host these models locallyto just super reduce that latency forthose use cases but anyway I just talkeda lot so I'll let you ask me anyquestions about that but I just wantedto give you like an overview of likewhat is this whole ecosystem look likefrom an infrastructure perspectiveyeah I'd say quickly with yeah I meanfirst I thought it was a really greatoverview I loved hearing all I mean I Ithink I first heard about like sagemakerwith some of the stuff that hugging facewas doing like kind of for me personlike you know coming out of my PhD inmachine learning and starting to likelearn about what the products are andhow this really works in the real worldand yeah hearing I mean the Bedrock APIjust so many interesting topics I thinkum yeah maybe it would be great to kindof just talk about that integrationbetween llms and Vector databases alittle more you know just like um maybeEddie you could explain kind of how wedesign our generative modules and sortof how we think about that kind of thingyeah yeah specifically I want to want tostart with something on that part youmentioned the training objective oftraining this potentially as fast as itas you can train it and I assume thatthe cycles in which you would retrainthey're relatively slow right orrelatively long Cycles so I'm curioussort of how does the the ability tochange your retrieval algor or industryalgorithm for your retrieval system onthe Fly Like does that sort of be beforeif you wouldn't do retrieval augmentedgeneration and you would potentiallyhave to retrain your model every time ithas to get new knowledge in so how dothese sort of how do these Cycles differand and uh when do I use what basicallyyeah that's a super cool way tocategorize it right because what we'restarting to get into is like now we wantto havea categorization and intuition for whatdo you want to train for and what do youwant to use a knowledge base forumI wasn't a call of anthropic last weekand I really like the way they thinkabout it they split it into skill versusknowledge right so if you start to thinkabout likewhat is a skill versus knowledge rightlet's let's use the lawyer example askill that a lawyer has is understandingthe law the knowledge that they have isrecalling different laws right so you'redefinitely going to have use cases whereyou want to build your model such thatit builds an intuition for what it'sdoing but you don't want to train it tounderstand every single law out thereit's pretty inefficient to do that andit's probably not reliable right soif you feel that and if you if youcategorize it into those two differentbucketsmymy thought process my intuition tells methat you can do way less training rightbecause if you try to use training to behave something be knowledgeableI mean you're all you can do is likeapproach what you can do with the vectordatabase even then you could never befully as reliable as a vector databaseright so I would say if you're buildingan llmum categorize it into those two thingsonce you feel that your Alum has theright skill base to soundlike what it needs to sound like whetherthat's a customer service agent orlawyer or programmerum or uh you know a musician whateverthat may be then it's time to startfocusing on the vector database and I'mwilling to bet it's like a 90 10 ratiodifference right if you start to startusing knowledge for Vector databases youcould probably do way less training thatyou thought you need toleftist differentiation skill versusversus knowledge this is great so I'mthinking in my head when you brought upthe lawyer I'm thinking that the lawyersitting in front of this shelf of booksand the lawyer doesn't have to memorizeevery single book they don't have tolearn it by heart because they can theycan just look it up and essentiallythat's the retrieval part in in thisthis awesome yeah and I I really lovejust like the separation I think likekind of like operationally like if youneed to keep fine-tuning the llm this iskind of one of the first things thatDrew me to retrieve augmented generationis you know if every time you have newknowledge you need to retrain the modeland then rebuild the apis and all thatkind of stuff compared to this thingwhere you know the vector database iseasier to update kind of like well howdo you think about that kind of aspectof it of how just the retrievalaugmented setup you know it lets youupdate it just with way less overheadthan kind of maybe a more traditionalmlops thinking of like keep trainingkeep training yeah I mean I would eventake it a step further I would bewilling to bet that for a lot of usecases if you use a vector data base andyou take advantage of a relatively largecontacts window you may never even haveto do any fine tuning at allright so the vector database is going toeat up a lot of the fine-tuning that youtraditionally think you would have to doin the pastum and we're seeing these models havevery large context Windows right liketodayanthropics context window is a hundredthousand tokens which means you couldinput an entire Harry Potter book asyour promptright so that in combination with thevector databaseI start wondering well you're probablyyou're still going to need folks that dofine tuningrightskills limited things Etcum but I think you're going to besurprised how much you can get away withwith a vector database plus some goodpromptingokaythat kind of brings us the opportunityto put that uh that training effortpotentially into the vector databasebecause the vector database also uses aretrieval model that commonly like likewith the um the latest models fromcohere and open Ai and all these thesemodels I thinkso if this has been put in in the shadea bit like people don't talk aboutfine-tuning it as much anymore becausethey become sort of more general purposeEtcum but before that I think that was alsoa very very common topic aboutfine-tuning those those models and I'mthinkingif you have that that skill set in yourteam to to train a model likepotentially put that Focus instead ofputting it on the the large languagemodel just put it on the embedding modeland and potentially get better domainsfor your uh domain thereyeah like the um you know the searchrelevance improving the search all theknobs you can turn is so fascinating onething I really love farshad is you bringup that like long context Windows isthis idea of like searching acrossmultiple information sources like youknow you're you have the You're Buildinglike a customer support chat bot and soit might search through one index oflike previous you know customer supportthreads as well as searching through saythe weba blog posts or maybe the youknow the code base if needed but likecombining all these differentinformation sources they call that likeFederated search and yeah I think justthis idea of how you combine uh maybelike multiple indexes together forretrieval augmented generation I thinkthat's one of the like emergingfrontiers of just you know packing thatprompt with as much information aspossible to complete the taskyeah no I think uh you're right I justactually learned about this recently Isaw some like you know block diagrams ofhow this is done on the llm sideum the llms uh look like donuttrajectory to know which database to goto to do different things right sothey're gonna like have an understandingof the question and then have like aprobability of what each database islikely to answer this question and thengo to that database right which uh isinteresting because I think that'sactually a little bit differentarchitecture than before rightpreviously a lot of folks would likejust go directly to the data and thengive that to the LM to generate ananswer but now it's kind of the oppositenow you have an llm accepting the promptunderstanding it deciding which databasewhich Vector database for example to goto and then joining in response right sothat's really cool like we're startingto enter like oh this thing feels veryintelligent right so I really like thatum and I think in generalI the analogies that I like to thinkabout that we're going to eventually gotowards is like the human brain where Ihave different parts of my brain to dodifferent tasks right there's a part ofmy brain that does math which isprobably not the part of my brain thatfuels emotions and that's probably notthe part of my brain that recallsinformation right and so what you startto look like is you you're gonnaprobably want like something that haslogic to know what part of the brain togo to or which which LM to go to andthen this example we're talking aboutfeels like it starts to approach thatbecause while it's not different llmsthat it's going to it's differentknowledge parts of What's called thebrain or the you know whatever and itstarts to go to but if you can havesomething know where to go for differenttypes of questions there's nothingpreventing us from doing the same thingwith L alums in the long term right heythis is a math question go to the mathelement the reason why that's importantis becauseyour brain would have to be so big if itdidn't have efficient smaller partsyou're going to do things and to makethat analogy with llms it's like we cancreate much more efficient models overtime and it create real products rightthere's going to be limitations of costand compute for specific use casesunless we get these distilled smallermodels to do very specific tasks rightso I think it's really exciting to hearabout this new architecture typethis this reminds me a bit of thatquestion of whether a hybrid surge ispotentially zero shot or or few shotbecause in in hybrid surgeon in our casewe have one parameter that can be tunedand that's essentially this Alphaparameter that tells you give moreweight to either the embedding basedsearch or give more weight to the the uha keyword based search and if the themodel has some reasoning likepotentially it could narrow this downalready it could say like hey this lookslike a brand name I'm probably going towant more of a of a keyword match orsomething or just like like a productdescription or something so this couldbe a move in that direction as well toto sort of give the model as you saymore intelligence more more domainknowledge about the back ends to justuse them better and and do betterretrieval from them yeah super excitingyeah that's brilliant Eddie and I lovethat like um you know the llm that knowshow to search you know friends I've beentalking to know that I don't shut upthese days about the gorilla largelanguage models which are like you knowyou've instead of using a 200 billionparameter gbt4 model or whatever to tomake weeviate queries you train a sevenbillion parameter and then it's muchmore efficient to run and exactly whatyou were saying far shot I agreestrongly with this that they're going tobe kind of llms over all of the toolsthat that the LMS know how to conf howto control the apis like edian'sexplaining with taking a query andsaying should I wait bm25 more heavilythan Vector search for this one oradding the symbolic wear filters toqueries and generally also I think justum there's kind of two things to it likelarge language model tool use the toolshave apis and so the language modelneeds to like you know behave exactlyformat the you know they call this likeconstrained output sampling orstructured parsing where it's like thellm needs to format the requestperfectly so this kind of tuning I thinkis perfect for that and I think yeah Ilove the parts of my brain kind ofanalogy and how each one is smallerbecause if it's everything is a largemodel then it's like kind of expensiveto tune but it comes into Vectordatabases too because like in additionto having specialized LMS to controleach tool you also have specializedVector indexes that contain differenttypes of knowledge maybe editing youcould talk about this is kind ofsomething that topic that I always lovethinking about is like when to separatemy data into multiple classes in weavemaybe also for new listeners we couldquickly do what a class is and like whento separate it into different classeshow to think about adding filters versusuh classesyeah the the original motivation in offof the sort of class-based schema in vv8was to reflect the real world so in anearlier version of V8 we even had adistinction between something a thing oran action that would sort of you knowdescribe either either things or actionthat's long gone because that led tomore confusion in the API than itactually helped but it kind of it's It'swhere the history of where the classescome from like they were meant todescribe real world things so if youhave books put your books in the bookclass if you haveum I don't know chapters or somethingput them in the chapter class and thatthat kind of like I'm already sort ofgoing to the the next use case of thiswhich is chunking up stuff soum uh for for these retrieval tasks veryvery common question or very commonoptimization problems basically howsmall like what what do I obtain avector embedding for do I just createone vector embedding for my entire bookgreat if you're just trying to identifythe right book If you're trying tosearch through your book that's notgranular enough do I obtain a vectorembedding for every word in my book wellgreat if you're trying to find words notvery good if you're trying to findcontext because you're losing all thecontext you're sort of going back to theoriginal steps of of vert to VEC so umthey're most likely something like theparagraph or maybe if it's a long bookand you're just trying to get a highlevel search and maybe a chapter in thebook something like that and that issomething that you can use the the uhvva class model for to retain thatAssociation for example between a classand a chapter sorry between a book and achapter in that book and that allows youto for example search through chaptersbut then make a relation to the book andthen maybe do something without so ifyou could potentially sort of make thesegraph like connections and even jump tothe author then authored these booksand as for your questions like whenshould I use these links between objectsof a class as opposed to a primitivefilter this is mainly a performanceoptimization so A Primitive filter ischeaper to run VBA it has a bitmap indexfor uh for primitive filtering basicallyfor for building these allows forfilters so if you want to get the mostperformance out of it it's probablyfaster to to do it sort of on the objectitself so essentially like adenormalized schema in in sort of SQLslash nosql termsum and and if you use these links it'smaybe sometimes easier to reason aboutso that's kind of the the trade-off doyou want something that's more easierthat's easier to understand versussomething that's more performantyeah amazing and yeah just understandingthat kind of filtered agents like howfiltered search works with the vectorindex and all that I think you kind ofquickly mentioned desk SQL with adifferent perspective but like this Ithink also kind of in our broaderconversation of just llms that use toolsand like selecting which class book bookchapter and that's like one kind of llmquery routing task but there's also kindof like is this a vector search or isthis an SQL search like if I say what isthe average age of my podcast listenersnot that I have a databasethat I have that but like that would bea SQL kind of query not a vector searchquery so yeah I think just all this kindof routing andyou know it kind of yeah like thissystem architecture kind of thing oflike yeah like you know I don't thinkit's quite the same topic asmulti-tenancy but it's like kind of likehow you have all these different classesthat you run in the cloud then and likeyou know maybe we could talk a littlemore about like you know what does ittake to and we could actually kind ofpivot the topics kind of broadly intojust like what kind of requirements arewe seeing to be running Vector indexesin the cloudcan I actually is it okay if I ask aquestionso it's related to the the questionyou're asking to Connor I'm reallycurious like what has been likethe one or two or three use cases thatrelatively novel and we're reallysurprised to see how well it workedwhile using a vector database to getthere like wow this works really wellfor this use case right have their bandsome of those come up and what werethose use cases what was surprisingabout it was there anything interestingabout the architecture to get thereyeah I think initially sort of if we ifwe look at the pre-lm era the main usebecause of course Vector databasesexisted before before chat GPT basicallyso the the whole sort of using them as abuilding block for retrieval augmentedgeneration is relatively new so beforethat I think it was mainly just searchwith contextual understanding so theselike very small differences in in theinput query like using the word not as anegation or something or for examplesearching for something that is that haswings but is not an airplane orsomething traditional keyword basedbased matching it would match forairplane and then it would also matchfor not which sort of completely mess upthe the um the the result quality so Ithink this initially this was my my sirfor me one of those moments where likeokay wow there is such potential in thisso so basically the classical semanticsearch in a sense uh and then post llmor post a Chachi PTum yeah it's gotten more into into allthese kind ofchat bot and andumthere was this moment we're like wowthis is this is so much bigger than justsemantic search there there's all theseall these systems thatI think the first time someone talked tome about generative AI in general it wasthat was way back basically for me itwas a bit hard to make the connectionlike okay yeah that's that's an awesomefield but I don't think that's a relatedfeeling at some point oh actually it isand that was that was such aum I think that's not not an exactanswer to your question of what are thegreat use case but this sort of in mypersonal Journey that was one of the thethings we're like oh wow okay there'sjust so much more to it and uh yeahseeing the first sort of chatbotapplications uh that that users do uh umspecifically serve the chat with yourdocuments kind of use case that's that'ssuch a great one this giving this sortof more interactive kind of kind of uhfeel to it and then especially if thisis like the typical demo cases that youwould see they're they're always liketake something from Wikipedia and thislike general purpose kind of kind ofGeneral general knowledge kind of thingsbut then seeing our customers implementthis on like super domain specific datalike in the financial industry orsomething thing that is so cool to mebecause it's it'syeah it's it's sort of this this verythese very boring documents where Iimagine people spend so much time goingthrough them like extracting all theinformation and now you can automate itand you can automate it with somethingas simple as a single query that whereyou tell the large language model heyretrieve the right stuff and then dosomething with it so that I think waswas for me uh definitely one of the eyeopener cases maybe Conor maybe you havea couple moreyeah I think probably the biggesteye-opener for me I think was you knowBob was teaching me aboutum oh it's called Uh like I don't thinkit's called disruption Theory sorry BobI forgot the name of this thing but it'slikebut it's like about uh you know it'sabout like this virtuous cycle of howyou kind of like have um distributionand customer experience and so kind offor me just like that personalizationcomponent that's achieved by llms thatto me I think it like just how I can youknow go through the Twitter you knowlike I like we all manage a ton ofrelationships right like in thetechnology industry and it's like it'simpossible for me to keep up with whatall everyone is doing every week forexample and so just having llms that canlike summarize what you know like sayMichael going neural magic have donethis week and maybe like connect it withwhat I'm doing and like automaticallygenerate these reports for me just thiskind of like you know the personalexperience that you're able to give topotential users of your product as wellas kind of like partners andIntegrations by having the llms likecombine knowledge sources that'sprobably that's probably my favorite oneyeah super cool and by the way that wastotally a selfish question on my part Ijust wanted to expose myself to what youall are saying being in this space umand yeah that's super exciting and Ithink you mentioned magic did I hearthat correctlyoh yeah well that's a friend of minewho's um you know the company he worksat this company where they're working onLM inference acceleration it's just likean anecdote of saying like you know theythey have they post these new updateslike you know I don't know what theirCadence of how many blog posts they doper week but let's say they do like fourper week and let's say I have 20 suchtechnology companies I want to keep upwith and now it's like 80 in a month andyou know I can't be expected to read allthis so like that kind of thing it thatI think that just helps us connect witheach other also like this kind of likesocialyeah I think that I mean I guess likeone more thing I really enjoy just thatI've seen is exactly what we weretalking about earlier where we'retalking about searching across multipleindexes uh Eric also weave she's beenexploring like llama index has builtthese really great tools on top ofweavate as the vector database and thenthey're exploring like the sub questionquery engine so I asked a question likea weeviate feature what is ref to VECand then it routes the question to oneindex of blog posts one index of podcasttranscriptions and then one index whichis the leviate code base and I mean thecode base we could talk about all sortsof things how you can use the levers ofweviate to search through code bases butjust seeing that in action the routingqueries to different indexes that is amind-blowing thing of retrieve augmentedgeneration for mesuper coolawesome so okay so I I would love tokind of pivot our topic into a littlemore aboutum you know we we Eddie and I'd actuallylike to kick this off with kind of likeum you know one question I've alwayskind of found really interesting is likeso with AWS cloud services you have likeall these different machines and I'veseen some things like of maybe likebenchmarking weeviate with likedifferent you know different particularmachines how do you think about thatkind of that kind of thingso I've seen me talking about likeinstance types yeah yeah yeahum soI think what's helpful in this contextis talk about like how do good how dohow do customers go about doing this andand why do it this wayum the best way to do this is whenyou're trying to work backwards fromlike what's the best instance type forme to do what I'm trying to do with Evaum it's just to do a proof of conceptright so this is something I do prettycommonly with a lot of customers that Iwork withum we get an understanding of whatthey're trying to do like what are someperformance kpis they're looking for andwe'll do some testing across instancesright so there's so many differentinstances at AWS right whoever you'reworking with whether you have an accountmanager or or like a specialist likemyselfumthey'll be able to guide you on likewhat are the options to test maybenarrow down to like three or fourdifferent instance types and then dosome testingum and and see where see where it goesright like sometimes you have you knoware you focused on and whenI say thisyour use case will determinewhat instance type you use and there'sreally common patterns for specific usecases right you have all these buildingblocks from AWS and you'd be surprisedhow often customers end up selecting thesame exact instance for a given use caseright and what that means is thatthere's a lot of tribe knowledge withinAWS like when you're talking withsomeone and you're like hey this one usecase it's like we're only going to haveintuition for what's going to be us butwe don't want to tell you we want you toget that conclusion on your own right sowe're going to set up an environment foryou to test it right because this is howthe tech World works right like we wantyou guys to make the best decision foryou and will help guide you get therebut ultimately you need to get that datafor yourself right so you know versustaking my word for it then you've builtthe best solution for your company rightso that's generally how we do it uh askyour account manager for credits alongthe way so you're not paying for itright so usually we can do that as wellthere's obviously limitations right likeyou can't have like a one year long POCfor example but yeah like we're morethan happy to help and that's generallya good protocol to do it data data datagreat to hear this this use case-centricnature of it because this is exactlywhat we're seeing as well likebenchmarks are always lying in a sensethat they're optimized for one specificthing whatever you want to show in thatthat particular Benchmark and I thinkfor especially for for users who are newto all of this if all they have is abenchmark they're just going to pickwhatever in that particular Benchmark ison top even if that's not at all whatthey're neat what they need so forexample often benchmarks are optimizedfor for either throughput or latencywhich is kind of a similar thing if youignore that that doesn't necessarilyhave to scale like often they're singlethreaded and then you'd havemulti-threaded queries and these kind ofthingsum but something that that comes up waymore often in in the discussions that wehave with our clients is actually costand none of these benchmarks optimizefor for cost so the question is likewhat is your actual query load what isyour throughput and then you see thatthat all these Curves in thesebenchmarks their actual throughput is somuch lower and really what we have tooptimize for is cost and then in bb8that could for example mean turn oncompression which is sort of anotherlatency versus versus accuracy versus umuh versus a memory footprint a trade-offum but also of course machine sizinglike do you really need the machine thathas that allows you to to I don't know100 have two thousand as opposed to 1000 queries per second if you have onequery per second and then you get thesethese machines that are optimized maybefor for more memory relative to CPU Etcand they're similar as as you saidum their patterns we also see thesepatterns like either your your uh verysort of on the on the throughput sidethen on AWS I think this is typicallywhat we see is like m6i or r6i machinesuh or maybe you're more on the yeah onthe operating cost side then we see morefor for example uh the the I think m7gor the the AWS graviton I think they'rethey're called so um long story short Ireally like this it's it's so like yesof course we would like to to tell ourusers yeah just do it like that or itCompares like that but really if youwant the best possible infrastructuresetup you just need to adjust to youruse case because every use case isdifferent yeah I totally agree it's sucha like honestly a distraction to be likeoh this is the best thing for this andumthe thebest customers that I've worked with andwhen I say best customers what I reallymean iswhat are the customers where I'm likewow I would join that company I reallylike the way they're doing things rightso the customers that I'm consideringbest in that contextum they don't follow benchmarks likethey go down to what matters for me andnot going back to like the AWSperspective nine times out of ten priceperformance is the name of the game sonot just performance not just price butprice performance which really meansgoing down through a use case right oram I getting the best bang for my buckthe only time price performance is likebeen kind of de-prioritized a little bitis if security gets involved like asecurity standard where we definitelycan't use it this way even though it'sthe best price performance so thereforewe're willing to play pay more to dothis other way right but most of thetime you're right and it's it's priceperformance right which was really usecase dependentI guess kind of for me I'm curious aboutum you know price performance and howyou tune it and then I'm like I'vealways been curious of me I I hope I Ihope this doesn't go too into the kindof the details of like vector indexingand Vector search but this kind of likeyou know you can tune hsw by adjustingthese type of parameters like EF youknow Max connections basically like howdensely connected that proximity graphis for how you route these queries andand you know I I think it's reallyinteresting with you know we've eatennow has like product quantization whichis reducing that memory by compressingthe vectors so maybeum yeah I hope this isn't too boring ofa topic like for people who aren't superexcited about kind of nerding out abouthow exactly you tune the the index forthe different computers but I'm surethat our listeners are you know areinterested in that kind of conversationlike so how do you think about uh tuningwith this question for Eddie and who'sreally been you know in the weeds ofthis kind of thing now I'll try to makeit excitingum so so in in a nutshell these are allparameters that you can tune for aspecific trade-off and typically thattrade-off is either index time querytime or memory footprint so youmentioned Max connections for examplethat's a parameter that literally leadsto to so agents W is a graph based indexhaving more Connections in the indexmeans more memory because eachconnection is essentially stored as yeahas a a number you can think of it sortof like the outgoing edges to othernodes are just un60 forms or somethinglike that so this is one of the knobsthat you can turn that make the indexbigger but then because it's it's biggerit's better connected it's it's moresort of it's easier to go sort of thisthis uh small world example I think thatwe've used in another uh in in previouspodcast so if you're if you're trying togo from from a person in Europe to aperson in let's say Asia then you justneed to know someone I don't knowsomewhere in the middle or so let's sayhave these like I think there's thistheory that that with five or six orseven hop officer so you can basicallyget to any person in the world and andthat's that's uh the nature of that thatproximity graph so the more connectionsyou have the easier it is to reach yourgoal with with fewer Hops and that's oneof those those uh turning knobs and thenum especially in in combination withproduct quantization that you justmentioned so product quantizationreduces the memory footprint of thevector embeddings but not of the graphso if you want to get your your totalmemory footprint down and the fact thatyou're using a product quantizationcould be an indicator that you've maybealready said on that trade-off of queryspeed versus memory footprint andtherefore operating cost you're alreadyleaning towards reducing operating costsso then one way to potentially reduce itfurther is you could say in my index Iwant to take that that same or do thatsame trade-off and I want to trade off abit of index memory footprint for higherlatencies and we say like okay fewerconnections which means the the graphfootprint is smaller and then in turnmaybe you have to search for it a bitlonger so this would be the queryparameter that you would have to to uhtune so this gives youum the effect on the infrastructure inyour cloud is essential deck if you tunethis well you could end up with the sameuse case either needing 16 gigabytes ofmemory or maybe 32 gigabytes of memoryandum of coursethey're not they're not identical likeyou would have for example higher careerperformance on the one with 32 gigs asopposed to a 16. but if you take thatinto consideration thenum it's it's easier to to plan yourinfrastructures they have lots of waysthat said vv8 has reasonable defaults soum there is a certain size where all ofthis tuning doesn't matter and we try topick defaults where it can just getstarted and probably you'll know whenyou have to tune them because you'lleither reach out to us or you'll startrunning into like memory limitationswhere you look at your AWS Bill and youthink like hey maybe maybe I can getthat memory footprint down of itum so this is for for mainly for theheavy production focus in large usecasesum but yeah it's good to know that youcan tune these things if you want toyeah and so I get kind of like this iskind of the question that sort of youknow selfishly I was hoping to getanswered in this podcast is I've alwaysbeen you know from when I first waslearning about weave and I learned abouthorizontal scalability kind of you knowand I've I think I have a roughunderstanding of how you Shard each ofthese kind of uh classes and then howyou scale out shards across machines andso yeah I'm just so curious to hear youknow hear your perspectives on how uhscaling out machines works in the cloudparticularly you know in this settingwhere you're running like a weeviatecloud service in AWS and so wev8 hassomething like you know again I Iunderstand this a little bit sometimesit's my best to set up the question butlike I know you kind of have things likekubernetes where you have like herehere's kind of like the code for how youwould scale up to more machines so yeahmaybe you guys can take the Baton fromhere like you know how are you how areyou thinking about like how we V8 canhave a service that then within AWSwould say okay we need you know 8 30 twogigabyte RAM that kind of thingso I'm gonna set up egn for thisquestion I'll give like the high levelanswer and then I have a feeling you'regonna know the very detailed answer howthis actually works in in your use casesthat you've seen so at a high level AWSoffers something called Auto scalingwhich is basically like horizontal likevertical sailing is when you basicallylike increase the horsepower of yourinstance type whether that's like biggerGPU or more RAM whatever horizontal isbasically you add more instances to dowhatever you're trying to do and I if Irecall from my ASA days this is like theassociate Solutions Architects coursethat you can take to learn more aboutAWS you can do things like monitor likeyour CPU usage or your memory usage astriggers to know when to do somehorizontal scalingum so now I'm going to pass the mic toeating to talk about like well whatthings do you monitor do horizontalscaling what are things you look for howdo you actually architect it usingorchestration like you touched somekubernetes right kubernetes is one wayof container rising and then doingscalingum but yeah I'd love to hear like whatare the specifics of what UC customersdoyeah kubernetes and vv8 are kind ofintertwined in the way but also they'renot so there's nothing in VBA that thatwill make it kubernetes specific butit's just one of the the operationsdecisions where we said like you get somuch for free with kubernetes serviceDiscovery scaling rolling updates likeall these these kind of things that wesaid like this is just how we want torun bv8um technically you don't have to youcould just spin a manual ec2 machines orso and just just uh build it that wayum on the the sort of clusterarchitecture slash design partum what's sometimes a bit difficult inVector search is that um indexes arehard to split up once you once you havethem so sometimes you're actually forcedto do vertical scaling which you kind ofwant to avoid because well if you have areplication then at least you can do itwith with zero downtime but it's wayeasier of course to do horizontalscaling and one way that you can achievethat with vv8 is the multi-tenancyfeature because the multi-tenancyfeature schedules each tendon so eachtenant is essentially like a lightweightchart in in vb8 and each of those thosetenants is scheduled on exactly one orif you spread it out on multiple nodesbut that means the next time you add atenant it can be scheduled on a new noteso if you would say your load is let'ssay you have a multi-tenancy applicationum someone can sign up for your let'ssay personalized chat Bots like a usercan sign up can upload documents and canchat with their documents and then youhave 10 000 users then each of thoseyour users would be represented by onetenant in in BBA and of course these 10000 users like the moment you startusing V8 you would have some of them butyour goal is to sign up more of them soideally you add more users every day andthis is where it gets really interestingbecause at some point you're going torun out of space and then you can veryeasily scale horizontally you can justsay Okay add more nodes to the clusterand then all your new tenants that yousign up from from that point on theywill be scheduled to those new notes andthat gives you basically infinite linearfailing for the number of of notes onething that's that's not released yet butthat's currently on a roadmaps then alsoto rebalance this and redistribute thisso right now it's like once it'sscheduled on a specific node it's fixedbut that is an upcoming uh feature aswell to then give you more control overmoving that data around and this is thisis one of the common cases that we seewhere people basically use horizontalscaling to um to increase the capacityof their their Vector search costs yeahI mean that's that's perfect you can askfor a better way to scale than aligningyour horizontal scaling with number ofusers right so that's that's perfectyeah that was super interesting I I lovethat kind of yeah the the way that youfirstly separate you know horizontalscaling and tie that with multi-tenancyand then compare it to Vertical scalingwhen you need it because of the vectorindex I think you know the the the cloudservice that orchestrates managing thatfor large apps I I find that all to beso fascinating and kind of like oneother topic I'm very curious about it'sum kind of related to this idea ofhaving like a product that manages cloudservices is kind of what becomesdifferent when you're running it likeprivately like you know we see this withlike you know just as a random examplelast night I attended a generative AIHealthcare Meetup in Boston and they'reall you know we need to run thisprivately so like what does that meanfrom the perspective of aw you know AWSunderstand it's like public Cloud I canjust kind of like go to uc2 and getrunning how does Amazon think of our AWSthink about uh private cloudum when you say private Cloud I justwant to make sure we're talking the samepage here are you referring to likesomeone owning like a data center withina Colo or are you talking about likesomething where heyum this needs to be all happening withinmy VPC within my own cloud singletenancy for this one company everythingdoes not leave this this thing right sowhich one of those two you're referringtoum I'm certainly interested in both Ithink um I think like kind of as Imentioned that like Healthcare in Bostonthing the latter Probably sounds likethat case where you're you know reallylike nothing can leave here and I Ithink the other thing that's really aninteresting part of this topic is kindof like having the llm the largelanguage model as well as the vectordatabase both be in that kind of setuptogether I I think yeah yeah we can talkabout both of those soum let me let me talk about the latterfirst because this is where I do talk tomy customers about and I'm certainlyhappy to share like a perspective ofwhat I'm hearing about like colos anddata centersum so as far as AWS and keeping thingsextremely privateum let's kind of walk through like alittle mental model of what anarchitecture could look like so whenyou're using AWS you can use Virtual uhyou can use vpcs right and within a VPClike you can be totally isolated fromthe rest of the internet or even otherAWS users rightum in that context you can easily haveyour vector database be in there rightso everything's within your VPC rightwhatever instance you want to host webeat on for example can be there andthen the question becomes well okaylet's say that I want toum make sure that my prompting neverleaves my bpc either I don't want anyoneto see my prompting and I don't wantanyone to see the responses from thatprompting rightum the the first level of like StandardSecurity that you can do is by usinglike a service called like I mentionedearlier Amazon bedrock with bedrock theway that it works is you effectively getan API within your VPC and it can be setup in a waywhere um basically via private link itconnects to where the model is hostedwhich means you can't see the weights ofthe model but because it's private linkand it's single tenancy this is allthings you can forget with AWS uh whatthat means is nobody else gets to accessthat stuff right so you can almost thinkof it as like you have your VPC bubbleand it kind of extends out to anothercluster somewhere but it's all singletendency no one else gets to see it andit comes back to you so in that scenarioeverything's completely private and it'sonly for youum and we have customers that like forexample have fedrampum security standards that they need toreach right so I just talked to acustomer this last week that's using adifferent service called Amazonsagemaker jumpstart where you can hostmodels uh in a similar fashion withinyour VPCum and let's say for example you wantedto use llama 2 rightwith jumpstart you would provision someinstancesum for your sagemaker endpoints thatyour vector database can interface withand that endpoint you get to decide whattype of instance that is right so forexample you can have llama 2 hosted ona100s you can have llama 2 hosted oninferential two and you manage that butwhat we do for you as a user is makesure that that instance is singletenancy and you're the only persontouching that and no one else gets tosee that so you're nowcompletely containing how you'reinterfacing with these llms whether ornottheir proprietary or open source rightand this does come up quite a bit andwhen I was making the comment earlierabout like price performance andsometimes security this is basicallywhat I was alluding to right like so ifif for example a customer most ninetimes out of ten customers like I'mgonna as long as everythinguh is done via bedrock and I know thatit doesn't go to the model provider I'mhappy security wise and then let's justfocus on price performancebut like what if a customer says no Ineed additional layer of security thenmaybe for some reason they have to usesagemaker jumpstart because they there'smore knobs there right and at that pointit goes back to price performance and inthe case a lot of times what I see islike a lot of times you know these thesemodels are pretty large and that meansyou have to use pretty beefy prettylarge instances to hold them right so Idon't know how muchum off the top of my head like how muchmemory llama 2 takes for the 70 billionbut there's a good chance that youeither have to run it on a100s orinferential tworight so if that's the case like theseare beefy instances they get expensiveso you you better and this is where youstart to think about well maybe weshould use a 13 billion or the 7 billionparameter version right so this gets inthe whole price performance thing rightum you're always working back for someuse case butbasically everything worked back Worksbackwards from from a privacyperspective keep everything within theVPC nothing leavesum that seems to nine times out of tenaddress security concerns from customersum there are other use cases allowancego into like the whole KOLO data centerpart of the question there are customersthat have data that they just cannot putin the cloud period they cannot leavetheir own their own data centers rightso in those use casesum obviously I don't have as muchpurview into that given that I work atAWS but in those use casesthese customers tend to buy likepre-made clusters from other companieswhich you may or may not be elasticum that part I'm not sure about but I'mpretty sure that you know if you're ifyou're a super large company that ownsyour own data center you're probablygoing to find a way for it to be elasticand I have no reason to suspect thatthey couldn't do horizontal scaling inthat type of environment as well but Ithink that most companies are able touse LMS in the cloud with the exceptionof companies that have data that cannotleave their own on-premises data centersyeah no that's really amazing andfirstly thanks so much because I I hearthat question a lot too and now I feellike I have a good answer right I thinkit's also interesting whether you needthe vector database private or the llmprivate or both and and kind of maybesome nuances around that and yeah Ithink this kind of like single tenantall that kind of thinking around how youdo this thing yeah all that's reallyinteresting uh Eddie and I'm curious ifmaybe you have some thoughts on like umyeah that kind of like tenancy and theand the vpcs as farshad describesyeah yeah so with tenancy because Italked about multi-tenancy before but ina very different context so it's kind ofa a difficult term because beforemulti-tenancy referred to our usershaving multiple users in this case nowtenancy refers to sort of the isolationboundaries within AWS soum so so let's say our chat botapplication from before is an AWScustomer they don't want their data tobe mixed up with competing chatbotapplication but still within sort ofthis single tenancy setup they mighthave multiple users so it's a a termused in multiple settings neverthelessthe concepts are exactly the same for usso data that shouldn't leave a certainrealm and this is this is somethingwhereum the the sort of again those using thesame term the single tendency nature ofyour own bv-8 deployment in your privateVPC in your Cloud tenantum that that gives you the capability tostay completely in control so I'mtouching upon this at the very beginningif if you want just something fast upand running just use your VBA cloudservice or you deviate cloud service inthat definition of tenancy ismulti-tenant so that means othercustomers that use the vv8 cloud servicerun in the same AWS project for exampleon our cloud service if you have theserequirements for for strict isolationyou can run it in your own um yeah wesay in your own VPC and the VPC is inyour Cloud tenant so it's basicallywhatever kind of boundaries you you wantto Define and you can do this this issomething that we're we're launchingright now as we're recording this it'snot public yet maybe it will be whenwe'redepends on it depends on how long ittakes to edit this or we're just aboutto launch on the bvl not on the on theAWS Marketplace we're going to deviateon the AWS Marketplaceum and it's a one-click deployment kindof setup where we can do exactly that soif you have an existing VPC you candeploy into that VPC if you're startingfrom scratch you can create a new VPCand this is the the kind of settingwhere you can make sure that becauseyour data that's contained in vv8 isthen wherever V8 is deployed so exactlythe same concept it can never beaccessed by anyone else it can never bemixed up with someone else no one elsehas access you are completely in controlif you tomorrow you decide you want totear it down you want to delete thedisks you are in in that kind of kind ofcontrol which is great for for all kindsof yeah security compliance Etc settingsuh the the downside basically is thatit's it's potentially a bit moreoperations effort because now you'rerunning vb8 and this is where our hybridSAS idea comes in so with hybrid SASbasically the vva team helps you runthis so depending on how much data careyou're willing to give away for exampleone thing that you could opt into issaying like hey I want to push mymetrics to the vva team then what we cando is monitor the metrics so you can saylike hey you're about to run out of XYZor we see a pattern here there or maybeif something happensum then then and you're reaching out forfor support to us and we can say likeokay we we have the kind of Diagnosticsuh tools that we need or another optionis like even if you can't even sharemetrics then we can also in the in thesort of uh least common denominatorwould be we can jump on a zoom call andmaybe there's maybe a sort of figure outwhat's going on together so you get likevarious levels of of support contractsand support uh settings in that thatkind of hybrid SAS setting where um yeahthe the sort of underlying thing is youown your data and we help you with theoperations part as much as you wantyeah I could I could see could see a lotof customers wanting to use themarketplace option you're talking aboutwhere it's within the customer's VPCumI would love to do a Blog with you wherewe show customers how easy it is to doand maybe like a use case that it worksreally well forum because I think a lot of customerswould appreciate thatawesome let's do that yeah I'm sure alllisteners the podcast are going to beexcited for that blog post as well andum so it kind of one you know Eddie andyou mentioned you know we get on AWSMarketplace I think that's one of thesebig headlines with our podcast and likesomething we're super excited toannounce and uh far sure I'm just reallycurious like um you know like we V8 youknow the startup and now being on theAWS Marketplace if you can maybe talk usthrough like um you know the what doesit take to get your cloud service on theAWS Marketplacewhat does it take to get your cloudservice on the AWS Marketplaceum well technically Eden knows more thanme now so since he's since he's actuallygone through it but at a high levelum AWS has so many different ways topartner with um basically companies orstartupsum and what we do is there's two methodsmaybe moreone method is through AWS MarketplaceMarketplace basically makes it easierfor you to consumeum partner products and services throughAWS Marketplace and what's kind ofinteresting is that if you likehave you know contracts with AWSMarketplace can actually count towardsthat right so in other words there'sincentive for companies to go out thereand to use Marketplace to consume thingsum rather than going directly to forexample BBA right so in other wordsthere's incentive for me to use consumemean to consume VA through databaseMarketplace and go directly to themright and it makes things easier forbilling reasons right you go through oneplace to do everything and the processfor doing it is basically you applyum I if I'm not mistaken you can also inparallel be a part of the AWS partnernetwork if you choose to and that hastwo categories one is consultant and oneas a technology partner you can think ofConsulting as like hey you can hire ourdevelopers to help you build whateverwhether that's infrastructure or applevel and we can do that and then at thetechnology level it's basically like SASthings right so we be for example be atechnology partner and be on AWSMarketplaceum it's not necessarily a difficultprocess but there is like a the processof going through like architecturereviews making sure things are set upand meaning standards with AWSum and the most important thing I wouldsay is like if you're a company thinkingabout APN or MarketplaceumI've I've never I'm gonna speakanecdotally here right I have to becareful hereum it's not a you do this and you getadditional Revenue it's not the way tothink about it the way to think about itis this thing uh you've already got afire you've got you've kindled your fireit's going and this thing is fuel itwill scale you it will add much moreopportunities to your existingopportunities right and the way a lot ofEnterprises think about this isum you make it frictionless for me toconsume your SAS product you're makingit frictionless right so if if no one'sever heard of you before it's going tobe very difficult for a Marketplace tomagically get new customers but if youalready have a presence in the world anEnterprise can say oh and you're inMarketplace so this is going to be easyfor me to consume you that's how youshould be thinking about it right so itdoes work but there's like a levelexpectation of how it works it'simportant to consider we have a lot ofcompanies that make a ton of money offof marketplace and APS very successfulfor them but what I don't want to do ismislead someone and say you do this andyou magically get Revenue through AWSrightyeah that perfectly matches ourexperience I think the the motivation toonboard into the marketplace was becausewe had customers asking like hey I wouldlove to buy this through the marketplacebut you're not on it can you get on themarketplaceyeah exactlyawesome well Eddie and far should I Ilove this podcast I think it was such agreat tour you know beginning with kindof Trends and Vector databases andretrieve vlogments generation steppinginto this you know machine provisioninghorizontal scalability multi-tenancy youknow all these Cloud stuff I just it'sso amazing so both thank you so much forjoining the podcast and sharing thisknowledge I learned so muchthank you both super funyeah same here thank you", "type": "Video", "name": "Farshad Farahbakhshian and Etienne Dilocker on Weaviate and AWS - Weaviate Podcast #67!", "path": "", "link": "https://www.youtube.com/watch?v=uHFM4sVHNxk", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}