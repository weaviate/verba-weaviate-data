{"text": "Welcome to Weaviate Air - Weaviate's live discussion and knowledge panel! Thanks for tuning in to this episode featuring a ... \noh I think we're live yeah welcome welcome to this viviate air uh it's episode number 11. uh double one this is super exciting um we kind of had a little confusion because I thought the Zen will kick it out but I can do it you know like you all used to this face you know and making a lot of noise um so today is going to be really awesome we have loads of great people on the call uh we have a special guest Ahmed from B1 who's going to do a really cool demo of something that he built with with it but also um his the whole business that he's building is super exciting so I don't want to spoil it all just yet but it's going to be super exciting um of course we have two new starters people that just joined with it recently so we have Mario and Duda they will say hi to you very soon and tell you what's what and of course uh Abdel comes to us to talk to us about some new indexing uh ideas and optimizations in viviate this is really exciting writing stuff you will get quite a bit technical but it will be inclusive for everyone and of course the usual suspect Zen and JP well and me um so this is this should be good so uh Zen how do we run it what's up yeah so I want to start off with uh our our main guest today Ahmed so welcome to the show um thanks for having me yeah it's gonna be awesome I'm looking forward at your demo maybe we can start off by uh introducing you uh to the audience uh here at weviate yeah sure I'm I'm the founder and CEO of B1 so it's a French startup that helps organizations navigate through large amounts of unstructured data and I remember last time with uh Sebastian we had a great uh demo during the the Intel Dev Summit uh and today is gonna be uh are like different but uh you know maybe I will talk about that later okay yeah awesome thank you I think so one of the great things that I liked about your um your platform was that you showed the demo in the uh in the Intel Summit but outside of that you've got a lot of other functionality as well right you you showed me the entire pipeline um that you use can you talk a little bit more about that before we get into the demo new and improved yeah sure um um I think when you uh approach AI uh uh in the Enterprise world uh you can just like talk about text only uh you need to take into accounts like images uh videos uh audio and other modalities and most of the hype right now is around text and I do believe that we need to include other modalities as well to have like a truly functional and useful product and this is why we have a full workflow to just automate the the building of a an own AI for Enterprises which means that an Enterprise can come with its data and then get out with like a fully functional AI that is um specialized uh in its domain and for obvious reasons uh for competitiveness for Innovation and the this AI that we built for our Enterprises keep learning from new data and that was like the the the the main issue that we are solving trying to solve yeah it's really interesting because there's this whole debate going on right now in the AI Community where people are saying that language is enough to get us to AGI and they believe like GPT 56789 eventually will get to GPT AGI um but then there's this other school of thought which you probably agree with that you can't get to AGI without multimodal reasoning right it's not um enough to just be a master of language you have to understand other modalities like humans do um to be able to to be able to reach this kind of barrier of AGI yeah absolutely I mean uh here we are talking but uh your face expressions are important uh you know whatever happens outside of like language is really important so I mean uh visual is a I think is much more important than text only so we tend to combine that with language understanding to have a kind of like um a very good understanding of our words because I I really believe that many others believe that text only is is is not enough we are making progress toward the AGI but still I mean I've seen some some research around like military modalities where you have like depth for example this is something that we um we are like working on but it's it's really tough uh to uh to to just so uh make machines learn about that uh and there are a lot of other like uh modalities that could be really useful to achieve that that AGI but text only is of course just a the beginning and you know we are all grateful to GPT to yeah um to to to have made that you know possible and accessible to everyone so this is just the beginning because I think even even those companies so if you look at Google Palm uh openai they're adding multi-modality to the regenerative uh setups as well um the other interesting thing um is that there's this Paradox and morvix Paradox which says that a lot of the things that we think are very difficult are actually easy to compute so um like identifying or picking out images uh like segmenting a radiology image we think it's very difficult computers can do it um like labeling thousands of images in a second we think it's very difficult but computers can do it but a lot of the things that we take for granted like when a when a child picks up a ball and then throws it we can't really automate that so um that's where multi-modality I think comes in right because in order to a simple Act of throwing a ball you have to be able to control a lot of other modalities to to action that yeah if we uh I mean if we make advancements towards um uh understanding complex things like you know um uh touch and you know feelings and other other modalities that we are living with like on a daily basis that that that could be a great achievement but from our perspective if we manage to deal with text images videos and probably audio that that that could be like really yeah good advancements and of course I mean not someday um when we see the the rapid uh race towards like AGI I think in less than five five years we'll see like systems that could understand the world as as we humans do and you know probably uh help us be more like efficient and amplify our intelligence this is this is at least our goal that we want this to to enable humans with the uh like uh more intelligence and more uh analytics and you know be able to act like an assistant greater than replacing you know domain expertise which I believe it's not yet there yeah so you were last time at you you showed me a great demo and I it got me all hyped and then I shared that demo um in some of my other talks and then you mentioned that you have a new and improved demo so you you haven't told me anything else and I tried to get some details but it's been on the hush hush so let's let's go into that demo um if you're ready so let me share your screen on the on the main screen here okay cool yeah so as the light just see it's a simple interface uh like a kind of GPT interface and uh today I'll be switching to two different domains which are financial services and real estate uh Financial Services because I I used to work uh in that domain like for 15 16 years so um I've seen a lot of Enterprises struggle with um you know financial analysis and all the legal documents all the disclaimers you know all that kind of reports and easier reports so I I thought that having a system that could like specialize in financial services that that could be cool and also I wanted to show like a demo about real estate because this is a kind of like domain that hasn't been disrupted yet um by AI and so yeah let me start so this is a guy that I like very very much Warren Buffett and he's really like um gives like with wisdom advices so um probably sometimes when you have like a two or three hours video you don't want like to to watch the whole whole video you just want to get answers for specific like questions so this is uh something we are working on and let me show you what what we get in terms of results so I just copy the video URL I just push it here and I process that video and once it's processed I can start asking questions so I I'm gonna ask questions to uh warrant with it so should we go to a college to be successful because he was talking about success so I guess uh this could be related and so the system will understand what has been said during like the the interview so the answer is according to Warren Buffett going to college is not necessary for everyone this is probably not an advice to follow uh for most of the bill so he believes that the best investment one can make is on oneself and you know a lot of a lot of wisdom and probably I can ask it you know what are the uh biggest mistakes uh in investing and so it goes on like the biggest mistakes people make when investing according to Warren Buffett are trying to trade too frequently and not realizing etc etc so this was like uh the kind of videos that you you want to to get answers for but probably not willing to spend like one hour or two hours listening to the full uh the full video another use case for financial analysts is to be able to follow like earning calls or uh like profit warnings and all the candidate kind of like uh events that happens in the financial markets so similarly I mean I'm gonna uh add a new video and process it and probably start asking some some questions uh like how is interest rates uh impacting stocks and it's it's gonna just like understand what has been said in the in the video and give me uh answers this is by the way running on Intel uh for Generation CPUs which are really powerful and suitable for that kind of of use cases so um yeah one thing to mention is that outside of the context of the video the the the solution doesn't answer so if you ask it for example what do you want to have for dinner or for lunch or should I go out tonight it's it's not gonna answer it needs to be in the context of of uh of the video I'm not going to read like all the of the examples but I I see in video for example here and I'd like to know more uh so what does the video uh says about India so again I mean you don't need to watch the whole video so the video suggest that in video expected to have a good quarter this is the answer that I was looking for um so uh another use case is like a full interview of CEOs and very often uh you have some tips for financial analysts that comes out of these interviews and um a lot of people are watching those interviews especially for like CNBC or other other big Brands and um so I'm gonna take this one and start another processing so how did I envidia uh Grew From uh gaming to AI so again I don't want to watch the full video I just want to answer because I mean how come like in video is known for gaming and now it's like uh the the leader in in providing like hardware for for for AI and this is like the full story so 2006 and you know Nvidia released the software toolkit uh called Huda and you just uh have like the answers to uh to your question so now comes in the most interesting piece which is the multimodalities so um um I was wondering if Nvidia is uh uh billionaire chips in in the US or somewhere else so uh what's the relationship between Jensen and gsmc I'm sorry for the J and so it's gonna answer like my question so Jensen the founder of uh NCO of Nvidia relies on Taiwan jcmc to manufacture nearly all Nvidia chips but um yeah I want to hear it so I just showed the videos and I don't know if you hear the sound of the video but here the video starts where uh Jensen's is still the founder of tcmc that it was his his hero uh so again mixing modalities makes it like more fun and more efficient to understand um I said I was talking about like I was going to talk about financial services but also um real estate and I here we are going to visit like uh mega mansion in Dubai uh it's actually a 60 million so I'm expecting a lot from this mention and uh here we go uh so I'm gonna process this video so this video is like 39 minutes um full of images and you know some interesting stuff is that a tour of your house Amit I wish so um the video is processed and let's like uh let's say I'm a real estate agent and I I'd like to look for a beautiful house for my clients and I don't want to see all of them and I just want to see uh you know the good answers for uh for some of like the the things that matters to my clients and so I'm gonna ask first to summarize the video so the video is a tour of luxurious mega mansion in Dubai and you know it describes like the dimension but I can go further and say uh lists all uh amenities and so here we get like a summary of all amenities but I like to cook and the kitchen is really important to me so um I'd like to of my clients so I'd like to see uh the kitchen and so it's going to describe a little bit like the kitchen but also uh so yes I can show you the kitchen the kitchen is in this property super modern has stunning marble control yeah that's that's really nice so I'd like to see it so uh here so this is a very very nice it's okay equipped yeah uh kitchen so this is validated but in Dubai you know that people don't work a lot they use their car and they have like very luxurious cars so I'm wondering if there is any uh luxurious car or probably a Bugatti in the garage for 60 million I expect to have like a Bugatti so let's see what the system uh answers for that oh yeah this is a Bugatti diesel but you know I know the Bugatti Chiron that's DeVos something that I don't really know so let's see it here I can see where we meet later on carbon fiber and all that kind of stuff in our full description and I don't lose time I just get right where my answer is can you ask it like how many sports car sports cars you saw I think there are a few Ferraris there uh uh Sports no problem yeah we can see it's like in the timeline but I've already seen like this this video I think there are a bunch of like Ferraris and the Bugatti and you know uh so yeah Porsche 918 uh post GT2 a Ferrari Monza uh Ferrari for five eight yeah so a lot of luxurious cars actually as you can just you know so this this was like the mustacherous car amount of excellent this is an amazing demo I love it I like I like how there was this transition from like you are talking to this like um yeah to to the AI kind of like hey go analyze this video and I'll ask you a bunch of questions and then you go like oh this is interesting I want to actually listen to that part for myself um that's like that's so super cool because then yeah you could it could be a long video right like you're only interesting that one bit but you don't remember what you want yeah I mean in in 20 years ago I I don't think I had the camera in my in my phone now I have three cameras in single phone so you can imagine that a lot of people are shooting camera like videos and those videos end up probably uh in the internet and from what I have read um like 83 percent of all uh internet traffic is is coming from videos so uh we thought it could be like cool to have a tool to to be able like to to to fetch those videos and you know understand what's what's being said and uh and get extract some value from those videos without spending too much time on it totally I mean actually the the great use case is we be there we could um at the end actually drop a link to we be there and start asking questions right yeah and I I can I can add something I mean with the performance like combined uh like the the combining like the the power of the interval uh CPUs and gpus and also the performance of the uh our favorite database which is very weird uh I I think we can achieve real-time analysis on videos nice hey so we've got a question from uh from the audience actually from our CTO um so lovediff join late forgive me if you explained this initially are the videos frames uh transcribed stored as embeddings how does it find the right time in the video yes that's right I mean we we store every modality as embeddings uh in wheel Gates and we have uh like some techniques and algorithms that's combine those like modalities to be able to uh find the right moment uh find the right information to show and you know it's all about uh relevance we don't want to have multiple results but have like the mass meaningful result and yeah everything is stored as embeddings and we view it is is kind of like Central to what we do because uh we need performance we don't want to wait too much for queries and uh when you deal with videos you deal with a lot of uh information so uh one there is only I think one way to do it is is using a vector database and with it is is our knowledge that the best one thank you yeah one one follow-up question to this so one so a great demo but one part of your demo um kind of piqued my interest so and this is something different um when you search for uh how is Jensen related to tsmc and like I have an understanding of how it can generate an answer but the timestamp also went to the part of the video where Jensen was talking about tsmc um the video doesn't show that he's talking about tsmc it's in the audio so do you guys have audio in there as well now yeah as I said I mean we we work with a lot of modalities we just store them all and we have like our system just finding out what's what's what's the best uh thing to show uh there are let me give you an example like you are driving and uh you hear music like uh I don't know your favorite music uh if we translate that in a video you would see yourself like driving and on the background having like some music this is not really related and so if we question that video uh then we need to ensure that what we get as an answer wouldn't include like the background music even if it's nice so there are a lot of like playing with modalities just uh is it's not just like about audio but uh but it's it's more than that it's understanding what's what's meaningful for the users at the end and not trying to replicate 100 how the brain works um but just trying to find like useful use cases that could could amplify like the human intelligence and and let us like uh interact with those videos in a similar swine that's so interesting so you've got you've actually got multiple modalities of search that are going in in the background I guess the the next question and perhaps this this will be the last question for me is um you're doing image search in the background text search audio search can you tell us how you decide which one is of the most important to answer any one question so when when you search about tsmc the audio search is important but when you search about Bugatti the um obviously the image search that's showing the Bugatti is important so how do you say that audio is important now but now image is important or something else is important yeah yeah I mean you ask the uh or probably Sebastian I don't know I don't remember you asked you know uh how how we deal with with such such you know uh modalities uh we have like a data platform that we use to provide human feedback and this human feedback is used to be able to retrain the model and have like an active learning Loop to be able to to try to figure out which like uh modality is best suited for for the answers so it's it's probably working the same way as uh error.chef and you know the more people will use the system the more the system gets better in defining what's important and what's not uh while getting their their answers so the answer is not straightforward but you know it's kind of like uh reinforcement learning you know and probably from a country to another things might like change and so we're just trying to figure out how best we can do that especially that we need to we have like some AI safety concerns uh because the prompt is is free so uh you can you can ask whatever question uh you you'd like but we need to ensure that the answers are not biased and they don't contain like harmful content and all that kind of stuff so there's a lot of work to do in that in that regard to ensure that these systems are safe for uh like a larger audience for now we are focusing on the Enterprise use but probably later we want this system to be like useful for for everyone and for that we need to we have some additional steps to go through super interesting thank you for the demo I'm confusing of modalities is super amazing yeah go ahead JP sorry I'm just sorry I'm sorry to button um if I were to learn more about you know B1 then uh and whatnot like what's the best place to go obviously this is locally hosted um demo so they go to the website uh you know give your phone call that I don't know what do they do yeah for now we are targeting like um Enterprises uh because we we believe that this kind of systems multi-modal systems are are more useful to to Enterprises for now and so yeah you can go to the website or I can leave my my details uh you know and we of course we are happy to to collaborate on any projects that is related to to this kind of like research and of course I didn't show one interesting part is when you mix actually videos with documents uh and this is really interesting because you get especially Financial Services you get a lot of documents so you want to send ESG reports and have an interview at the same time and see if there is any difference if like the the values of the company is really translated into this these reports so there are a lot of use cases but I see those use cases for now in the Enterprise world and probably we'll find out if if there are other use cases it could be applied to like larger audience awesome thank you for that that was that was great all right so it's it's going to be tough to top that honestly and it's going to be very difficult is that what you're saying yeah yeah the best for the beginning and then there's nothing better than that no just okay bye [Laughter] all right so uh next up um probably noticed um Sebastian introduced Marion and Duda so new new members to the team and yeah so let's hear a little bit about you let's introduce you to the wevia community and yeah yeah thank you thank you then hi welcome everyone um um I'm Marion I newly joined the gate as a community manager and I must say uh Ahmed I'm I was really Amazed by that um by that um demo and I probably can't top it but I just saw throughout the whole demo oh my gosh how would that make my life more easier because we just conducted user Community interviews and I actually um you know recorded some of the feedback um our community members had and it would be so awesome right to and and I also took a lot of notes on paper and it would be so awesome to kind of combine these documents together with um what has been said and just ask the question you know did they find um did they find our documentation helpful right and then get it from the context that would be amazing so yeah and that is but that is a great bridge to what I wanted to say um I joined recently as I said and um for us Community matters that's why I'm here and I want to spread you know joy and happiness and really kindness into our community and I also want our community members to be valued and to get the value out of viviate right so that's why we are starting with initiative where we go out to our community members and ask them about you know our documentation new features different uh use cases in Vivid and just ask them how they are you know how they are finding their Wows and their challenges and learn from their feedback and just get better and make it make make it more more awesome maybe a little bit more about myself I live close to Berlin so um I'm going to be around in a couple of events um around here in Berlin um starting from now on so keep your eyes open and um I'm also going to be at a lot of meetups so you can meet me in person and hopefully the rest of the team as well and um I will also be in this Community hanging hanging out with the vv8 fans and everyone else and that the perfect time for Duda introduce himself hello everybody I'm dude I'm based here in Brazil and just as Marion but a little bit early I also joined the review gate and it's been a very fun ride so far uh learning a lot of course as a a very full of potential uh uh tool that can be used to amazing stuff just like Ahmed showed us and I'll be I am the Community Technical manager uh support uh so I'll be always there on the on the forums and on also on the slack and on the documentation also with the help of GP and Zayn and so whatever thing that you need in if I am able to solve it and if I if I don't I also will bring data to our team so it's a pleasure to be here and I hope to see you around there on the on the forums or hours like so please feel uh more than invited to join us there thank you awesome thank you Mary absolutely so great to have you all here you know it's nice to grow the family the team is growing the team is growing so now we set the stage for the next present next fixing right yeah all right so next up got Sebastian talking about what are you talking about Sebastian oh yeah I wanted to talk to you all about online workshops so it's it's pretty cool because both use and njt have been running workers for uh for a little while already um but in order to find them it was a little bit tricky in the past because you kind of have to find them like you would announce them on Twitter or like put it on LinkedIn or different places and it was all about like um it's like yeah can you discover those workshops so actually the two of you have been doing a pretty awesome job so far with running literally we're running those workshops weekly so for anyone that's watching us right now if you're new to viviate it's literally an intro to within Workshop every week that is Run online and one week is like more like a European slash Asian time zones where the other one is around the uh time zones that are training for the Americas um so those are great workshops to join and how do you find out about those workers really straightforward you could go under developers and then into workshops and then you can see like they're always like the the future workshops so for example if you're interested in a workshop with JP tomorrow that shows in my time so I can click here and then you can go and register yourself or for the workshop so just drop us your name email and then we'll send you uh send you a link to the workshop which will be pretty great and uh you know like then you'll get a smart people like Zen or JP for a full hour or sometimes more and a good thing about this is as well that is a place for you to ask questions on how you do certain stuff right um it's definitely recommended this one to for anyone that's a beginner so if you have somebody in your team that's learning with it send them over to this one um we're also thinking and already like somewhat in in the Midway planning all the types of workshops or more advanced topics um so if you have any opinions on what kind of Workshop you'd love to see for yourself um drop us a note either here during this session or maybe uh reach out to us on slack so yeah that's me and the workshops awesome thank you if you want the smart workshops you go to JP's workshops if you want the fun ones you come to mind okay that's that's how we do it so it's almost exactly what I was going to say Zen but um I was gonna say for you know actually learn stuff go to Zen's just to listen to someone just go on on and on that's me and then I do love to chat and and to Sebastian's point about coming in and asking questions honestly one of the best things about this Workshop in Boston and I have said this is just the fact that we get to engage with users but like how they're using wavier and we've like actually ended up you know talking to a number of people about and answering a bunch of questions like not drink just during the workshop but afterwards as well because that particular use cases um and then they you know that wasn't covered in in our examples whatever because it was they were unique so yeah it's really it's a good time I'm honestly both of us so you know we're not we're not horrible people we think so good luck to have you the fun thing is that we sort of have a template for those work shoes but we don't uh and then each of you run those Works in a very different way and I know that you you keep trying new stuff so it wouldn't even hurt someone to kind of join both uh or something right um but yeah it's pretty cool yeah it's like a Pearl Jam concert every set's different so yeah it's uh I I want to keep it the same template and then I get bored the first time I deliver it and then I was like okay how do I how do I make it better so every time you come you'll see a different demo or you'll see some something that's changed so um that's why we need those Advanced workshops because if you get bored you know you have to you know show it with more advanced features and stuff right yeah so we we were planning on um seeing what other Concepts we can go into so for example just an hour workshop on uh document chunking and have a workshop on retrieval augmented generation like all of these interesting Concepts that um another one could be like fine-tuning embedding models like there's questions that we get from the community uh over slacking over the Forum and it would be interesting to just pick one and then do a div type make it I think that would be pretty interesting so those are coming as well all right awesome so this next topic is uh it's kind of near and dear to my heart because I've been um learning from Abdel about this and further for I don't know how many Ouija errors you've been kind of hearing his thoughts through my voice but today I wanted to welcome Abdel onto EVA Air for the first time and so now you don't get the second hand experience you get the first hand experience from the professor himself so Abdel welcome and it's great to have you here uh thanks and and thanks for for being the voice I think you've done very well I think probably it has heard better on your voice so let's see uh so I I I don't want to repeat everything you have said because again you you have made a terrific work think of the address very much uh I just want to to talk about the last changes we did so we we released um product quantization some time ago in version 118 it was on their experimental flag because we knew it was not a complete many things were still so many pieces had to to to come into into their place but uh but we wanted to test it a bit and to to have some feedback uh so back in version 118 we did some testing and we saw okay performance wise it's working fine but we are losing a lot of recall so the the the the the answers we are getting is not well they are not as accurate as we would expect and uh and then we analyze these and and we knew we could rescore all the data once we we have it but but we still had a couple of policies to follow like how however Mana does it how Asians wpq does it like rescoring at the end and we wanted to to test everything before before we a ship a complete solution and and we take the experimental flag out of it so I have some charts maybe to to show where we were back in the 118 and where we are now so this is this green curve so this is a queries per second against recall so this is how fast we we give an answer like we we can we can serve 400 answers in the second or 100 answers in a queries in a second and this is how accurate our our answers are like a like one is like everything is what should be like the the closest points that that we are trying to collect and this was our results no compression no PQ and this is what we get like with the relatively low compression but but high enough so as to to compress that so this this orange curve is Six Dimensions per segment meaning we had like a 1 to 24 compression rate so if you had like 24 gigabytes of vectors suddenly you only need one gigabyte for storing them um but you see there was a huge price in in Precision of the of the results and then we explore the the the different ways like we we could be rescoring like you know I'll use your example like you have the address you don't have the street numbers you only have the street uh okay it gives you an idea of the vectors you want to fetch it's it's even more than that if you live in a given Street let's say Let's uh come back to this example when you apply the product quantization it's not like you are taking the street and leaving the rest it's a vector and you have to apply some mathematics patients so you could also uh add some noise in the street at your story it could be not the real street but the street next to it and then you don't want to rely a hundred percent on the street you you want to give some room for for for the noise so ideally you should say well is it is everyone is like really silent I think the thing where like I'm looking at other people's screens to certain interrupt AB developers still talking I think Abdel went to into like the compress from 118 and that's their low recall I think I think he enabled PQ well while speaking Yeah the old version there we go did you just run into the old version of PQ of low Rico be nope nope he's still on the old version you have enough points and then you can you can collect the data the the so you can check all the enough points to to to have a good uh result set but how how do we do then we we take all those vectors and we go to disk take the real representation of the vector recompute the distances and take the final so this this made us jump from here to this range on Rico like it's it's now really close to to the to the uh on compress and then we also made some improvements from from 118 to 121. now it's a bit hard to compare so this is 118 no compression 121 no compression the green curve is a 118 no compression the blue curve is a 121 no compression so you see there's no PQ there and still there's a huge difference and that's because of the different improvements that we have introduced in in vv8 like grcp grp year uh PC support like the CMD optimizations on the distances calculations which by the way are not a supported for the compress case so we still have some more research to do and and then if you see the red curve now that's the the compress on 121 is is in a good it's really close to the to the blue so we we believe it's it's really out of experimental we have fixed enough box and and I think it's well we believe it's safe to use uh when we made all those changes in 119 we then realized okay this is going good but the indexing time was still like almost twice when you compress and we debug it a bit profile it and we realized okay there's a lot of distance calculations between compressed vectors and I don't want to get too technical now but but we have like this lookup table where we put all the distances for compressed vectors to to the query vector and we did something similar for the indexing like we have this Global lookup table where you compute distances once and you don't do it again ever during indexing you reuse it and now when you index uh let me see if I shrunk to the next yeah when you index so we try with sphere data set and with Wikipedia so we have dbpti collection of one million vectors vectorized with the open AI Ada 002 so those are big vectors in 118 if you would uh index your data compress and uncompressed you have like twice the time to index the data but in 121 it's more or less the same actually compress is a little bit faster but but not significantly faster so it's more or less the same and the Improvement of course is so the benefits are in the memory footprint where where you barely use mid 14 15 10 of the memory you would have needed for the for the normal use cases so for example with sphere 1 billion objects we went from 2.7 terabytes to uh 0.7 terabytes like if maybe five times less memory uh yeah that's uh one of the one of the amazing things about this so if can you go back to the first graph that you had there this is a what's the compression ratio between the blue and the red light that you're showing it's like uh the the red curve needs a 14 of the memory we need for the blue 14 it was like six and a half gigabytes of memory against 900 megabytes something like this yeah so like the fact that you're only paying this much recall for that amount of compression and then depending on what your application is of course if you look at this this is so awesome yeah exactly 85 reduction in memory footprint um and the other thing that I was talking to Abel about this is that we're not only making improvements on that red line that blue line the uncompressed version is also being improved day in day out right so really for the for Abdel who's working on the PQ stuff the red line is chasing the blue line and the blue line is getting better and better as well so uh it's uh it's quite a tall order so before it was closer and then we made the the simdi improvements uh like Abdul mentioned in the blue line improved even more right so um like for for me if like this is another thing I guess we're going to be releasing a blog post that has all these details uh in the in the next couple of weeks so look out for that if you want to if you want to then learn all these details from Abdel um but what's interesting is now you you really have a um a valid choice and going full PQ um and you can you can kind of look and see what your data is equivalent to in terms of the tests that we've done and see how much kind of recall you're uh paying for the up to 80 90 reduction in in memory footprint and yeah I I also wanted to say here about the price you pay in recall that so when when we compare we restore down so during the search or we can rescore at the end we saw okay the the curve is slightly higher I mean the the recall was exact exactly the same as uncompressed if we would have a risk or during the search but then the performance penalty was higher and with this we you have the flexibility to increase your parameters your search parameters like your EF parameter and you can still get the recall you want at a lower price in performance this is another way also to to read it you you don't have to pay in recall if you want a better recall you still so you need to use a higher EF parameter compare to the uncompressed version okay and then you might might lose a few microseconds or or maybe one millisecond but but that's it that you still have a an accurate solution I see and and by the way these charts uh so I I will also talk about how to Benchmark your data in case you you have your own data and you want to see how how it goes we are using scripts that are in the chaos pipeline currently in vv8 so I I will put all the information you would need to to replicate these experiments with your own data or with whatever data you you need to to run the experiments yeah yeah that's going to be awesome I think so a lot of the questions that we get especially after we ran the sphere experiment where people trying to replicate the sphere experiment and now like we're gonna we're gonna release the code that you can use to kind of plug and play your own data set and try with PQ without PQ and then you can and then if you're modifying the parameters of hnsw to to get some of the recall back um that's where I think it'll be super useful right because then now you can fine tune some of the parameters of PQ as well as hnsw and then you can see what amount of compression works for your data set that'll be awesome yeah one thing that we didn't cover yet um sorry JP um it's because we always talk like we talk about performance and like recall and everything but like I think um and it's great that we have a met here because to people that build with we made that also means that financially you can build bigger Solutions maybe like maybe before we say like hey 100 million that was a wild thing now with 85 savings on the same setup um I could throw 700 million vectors right um and then so you could kind of go like okay not only can I handle like way bigger data set my speeds are even going up but I'm not spending more on Hardware like this is this is so incredible um so there's definitely a great great opportunity uh for you to play because like if you're like on a small data set like you're like half a million objects yes it is a bit of an improvement but when you deal with like huge cases like this is like a wow thing and then you could either go like I could slash my costs a lot because I don't need so much Hardware or I could go like with my course I could go for bigger cases and this is I think this is great and I'm not I'm pretty sure I met you already messaging your engineers going like yep get on the latest version yeah so for Ahmed I think there's two interesting things there's the multi-tenancy update and then the PQ update and so yeah lots of lots of interesting things which is a great segue to JP who's going to be talking about some of these interesting updates JP what do you what are you bringing us today fabulous I want uh I'll be pretty quick but yeah that video really reminded uh so the the talk really reminded me of you know keep your vb8 versions up to date right because a lot of people ask us about like oh I'm having problem XYZ and quite often the solution is oh have you tried the latest version because we've made improvements and what have you and this is like an exam and those curves with the uncompressed data is a big example of that because even if you weren't using PQ just by updating the latest version you get huge performance gains like the two uncompressed curves are like just wildly different places to each other um so that's that's really cool um but then if you can share the screen that I've got open I can talk to 121 oh that's interesting did that sit around your screen as well like what's happening is yeah I jittered a little bit I don't know why it vibrated uh just just very excited Zen just very excited um so 121 came out last week it is now available on WCS as well and you'd have seen some social messages so if you really you know if you like this cute leviate guy um check that out as well but there are a few um features here that are really really interesting so I'll go through them one by one fairly quickly so we've got new contains any contains all operators this is syntactic sugar but what it really does is I actually scroll down to the right section before I talk about it um but what it does is instead of having to have a bunch of logic combinations of logical operators like whether it's ands or ORS this allows you to stream like that and it might when I describe it not like sound like a big deal because you know all this is doing the same thing but if you're writing code or if you're reading it later it will just like you know it would be easier to ride more likely debug free and easier to understand the logic internally what's going on so um and it doesn't really change anything else other than the fact that I'll use the graphql example here you're just using a different operator right here and in terms of contains any this will replace a series of um or statements which we've all written long boring involved um logical combinations States before you know whether from Excel or in actual programming languages and whatnot so this will make that a lot easier so that's contains any and contains all right and it's now I believe supported in all of our client libraries as well so please check that out cool so that's that one and the other one and you touched on it earlier then in terms of really irrelevance time and I'm sure you'll you know I see you sitting up on it about the multi-tenancy improvements and this is a cool one so when we launch multi-tenancy we talked about scaling aspects of it right how many you know as a rule of thumb how many tenants can we support per node and whatnot now what we've done with this update or uh not me I didn't do any of it but the team has done with this update has been to introduce the ability to activate or deactivate tenant shards each tenant is a Shard and what this allows you to do is to say if you have a million users and half of them are inactive users you can just switch them not necessarily off in terms of deleting them but you can turn them into the deactivated users and they'll take up no memory footprint basically right so you'll store all the data you can activate them if they do come back online but again it's a really convenient way to scale without kind of wasting Hardware resources on data and memory footprint that you don't need um so I believe it's somewhat experimental so in the sense that the API might change a little bit I believe there are more States coming and so on but this is really really cool so again if you're you know building something with a bunch of tenants and you know this is like going to be a really good cost saving and useful feature for you I think I'm going to scroll through all the pros and there's been a performance Improvement in terms of like how the system manages kind of coordinating all these different operations going on for multi-tenancy because there's you know large scale less attendance lots of things needs to be done so basically that's an under the hood improvement though so again if you upgrade from 120 to 121 you get this for free right and who doesn't like free stuff Okay so we've got new modules and then I'll throw this back to you uh when I'm done rambling and the first one is a text to back GPT for all module we've got the texture back contextreme module as a local vectorizer and that works well but it's kind of an older model so perhaps it's not as performant as some of the more modern models and there was one solution of to to that is to use the text-to-back Transformers module with which you can use any of the modern Transformer architectures now um obviously then menu views use that or know about it know that you need a GPU you kind of do really data GPU if you want to do any kind of massive scale vectorization on it but this module is optimized for CPU use so it's got one of the mini LM modules available for it and with this you can get really really fast vectorization and or inference whichever way you want to call it performance out of just the CPU so encourage you to check that out together and the next one of course we're really excited about is the multi-teve fine module I can see Zen just vibrating or tutoring with excitement there and that allows us to use the bind model to process multi-model data and um and then we'll you know show a cool demo after this um then gets just all the fun stuff but here's a little preview so if you look at the blog post um there's a little demo here of like using uh is it sound to retrieve images and a video to retrieve another image is that right yeah so this one I played the you can play the uh file at the top there um yeah there's a little there so that's the sound of a car honking and then this is the image that comes back um and I'll show in the demo what what other things you can do um the I I guess I'll go into it when I when I go into the demo otherwise I'm going to start rambling and I won't stop exactly it's my time now it's that go away um it probably came up with this car because sports cars you know they just run you over so uh yeah listen to Sebastian you need to pay us more for us to put Pitch the Bugattis in here man I'm getting a playlist just just for a picture yeah well I tried um so there's that and then there's a couple other things I'll just go through quickly so there's been a suite of performance improvements under the hood um so you can see the little waviate mascot being extremely happy um one of them that is not entirely under the hood is the virtual memory access so mmap is the existing method or the existing function that's being used to map virtual memory and use um it can have some potential challenges if you start to have like basically significant load so we've introduced this option under the hood to use period um so default doesn't change so if you don't do anything nothing changes but if this is a potential challenge for you you can try out the solution um so we mentioned simply operations before and for those of you like me who might not necessarily know what CMD is it's a basically parallels parallelization technique in processing um so if you've you know if you're as old as me and and seeing things like MMX on stickers it's kind of like the same thing on the old Intel chips now this is already available on um Intel processors but we've added this for um 64 processors so that's really cool and again upgrade you get free performance improvements I feel like a salesperson um and I've dealt with no this is uh yeah you know I'll wait there um and we're gonna get we get best team death improvements in terms of how the vector indexes um basically built and and how it's used so we've got these two key improvements in terms of How It's encoded and in terms of How It's accessed and what's available when to different processes so these are under the hood improvements um and it'll just basically make wavy8 go even faster which is really really neat um one change that you probably want to be mindful of is the backups there's a couple of things here one highlight that you should know is that 121 backups cannot be used in 120 so if my sales pitch is successful and you do upgrade before you upgrade it might be a good idea to produce a 120 backup in case you do think you might want to roll back you know you know production system or what have you because there are a whole bunch of improvements like compression that'll make the backups they think cheaper certainly for cloud providers and potentially faster I have to check but it's not backwards compatible so backup your data and upgrade if you think you might be rolling back the most important thing about backups is them yeah yeah make sure you back up to a medium that you can use um uh so yeah and and one small thing which is also but important is a hybrid search algorithm refinement so with the new hybrid ranking algorithm that we introduced in 120 I believe and that was to do it the way it was scored um there was a use case where if you retrieved on your small set of data the results might fluctuate probably more than you might like so we've introduced this little thing where even if you ask the like three objects we've yet will grab a hundred right and then calculate the scores and just give you back the the three just because that stabilizes the way the scores are calculated um so you won't see it um you'll probably just notice that if you're using the um relative score Fusion I think is the algorithm name if you're using that if the score is a little bit more stable than before um and so it'll be perhaps a little bit more intuitive when you look at the scores so yeah that's kind of it for me um it's a bit of a lightning round but again it's available in WCS it's available open source um it's really cool it's fast it's got all these new bells and whistles on it so again um yeah please try it out and let us know awesome thank you I'm sold I'm actually using it on this computer right now yeah I can prove it what are you doing with it Zen yeah let me show you let's see oh wow that's really cool so what I wanted to talk about was um one specific module so I've been playing around with this um for a while let me show you guys here so this demo is not going to be as cool as the demo that we saw earlier uh from Ahmed but you can you can take the multi-direct bind module and you can kind of incrementally uh use it to to get something um that that could be as cool you're gonna have to put a lot of work in but um let me show you a little bit of the capability of the multi-davecbide before I go into the demo so the model itself has the ability to encode seven different modalities and that means that you can perform any to any modality search right so you can pass in text and search for text audio video images and then there's a bunch of other modalities but these are the exciting ones and then you can do that with anything so your query can be any of these modalities as well and the output can be at any of the uh modalities so that's what I mean by any to any search so the two modalities and if I if I was going to show you all the combinations we'd be here all day so let me just show you some of the most exciting combinations um so what I did here is I put together a notebook uh to show you how easy it is right so uh setup is pretty straightforward I've got this running locally on my computer um I've created a schema this one has two classes one where I'm storing images and another one where I'm storing audio uh depending on your use case you can have one class that stores uh all the modalities or multiple classes that store different ones um but for my use case I'm going to show you two different types of searches so one over image and one over audio so I'm adding the I'm adding the data separately and then I've got a bunch of helper functions here to kind of make the process a little easier so then you go in you add all the images so I've got I think 20 25 images so it's a toy example maybe next time I'm preparing a talk so for that one I'll have a real like a scalable example um and then I add the audio file so I've got a bunch of audio files that I scrape from the internet and then now we can perform first I'll show you uh text image audio video to image search so here you can go in and let's say we search for the concept A sparse Market this comes back so this is nothing new you could do this with clip um and then this works just as well actually the image bind module is built on top of the clip module so the the image and the text modalities that it uses are clip so um this works as well as the clip module itself right so there's all of these and then we can go to image to image search which you can also do with the clip module so here the formatting is a bit wonky but this is the query image and then I can show you here this is the query image and then you get I took three response images and I and I squished them together so you can see relevant things here if this is your query image you get the response image over here if this is a query image then you get response images um no this is this is all old stuff right so let me show you new stuff so now you can play an audio sound let's say you wanted to query with this audio and I don't know if this will play for you but let me try it no did you guys hear that nope okay so delete I'll release it yeah so it's it's I'm not I'm not gonna so this is The Voice so this is a car uh honking and you have to trust me and then I'll and I'll open and then so this will this is what you get back when you do that which is interesting because you get the car but then you also get the city which is where now this demo is not as heavy hitting as if you if you could have heard it but this is a lion roaring and you get all this stuff back and then you can keep playing around with this so I've had a lot of fun with this um I'll release this uh notebook afterwards so you can play around with this as well you can kind of swap in and out uh the um the files that you're putting in and then you can also do near video search so here this one you'll be able to see so I've got this uh video that's playing uh no audio on this one and then when you your query is a video it goes in and this is what you get back as response images um now keep in mind in this class I only have images so it can only give me back images if I had multiple modalities stored in this class then I would be able to get back other things as well um yeah so if it's your query video you can go through this we get this response if your query video is a bunch of lions you get spec and then it's pretty amazing yeah so it's kind of the first time I saw it I was wowed and then I played a played around with it for like two weeks the other thing that you can do so this is just images right you're getting back images um the other thing you can do which is really cool is uh one second let me scroll here your video search so then the other class I put in a bunch of audio files so let me scroll down to this okay um so here the the query is this uh text so animals on a farm and it retrieves the audio sound that's most related to the animals on a farm okay and this is um you won't be able to hear this but this is a um a a rooster in the morning okay um so you can play around with that as well and then if you can query with an image this is yeah I'm not going to make the sound oh come on maybe next time you're on the friends here so if this is your query image you get the audio of uh so in this case actually it didn't return the audio of a badminton that's because I had audio of badminton in the database as well but apparently in this image the man is more relevant than the fact that he's playing a badminton so it it sent me uh the uh audio of a of a man sneezing I had both things in my database and apparently that was closer in Vector space so I got a true here I I made that sound that counts um and then you can do query to query right so something like a Shazam app where if I play a tune it will be able to retrieve a song for me this one I played a tune of a dog barking and then it it barks back at me so it plays uh another uh another um audio sound here and then the last one so we could finally uh get an answer to what did the fox say yeah but it'll be in Fox speak so so I these these two dogs are kind of conversing with each other in multi-dimensional space I don't know what they're talking about but it's pretty interesting and then the last one here if you play um if I play a video so if this is my query the entire video I can get back um the a an audio sound here which is just a school belt ringing right which was the closest thing to to a classroom so yeah JPM yeah um and each input here is being embedded into like it's been converted into one embedding right yes so there's um there's multiple models there's six different models the one for image and video is the same so that's why you have seven modality six models um so the interesting part here is that you have six different embedding spaces but each one of them is kind of binded together that's why it's called image bind by the representation of clip so clip understands words and images really well and it and the paper details this they use Clips understanding of images and words to pull in the representations of audio video all of these other modalities to be close to clip and so that's why even if you even though you have all these multiple modalities if I send in a video it goes through a separate model if I send an audio it goes through a separate model but the embedding space of all these modalities um it kind of talks to each other in the training uh in the training Loop so that's why you can kind of use these six six models together at query time yeah and I guess from a user perspective like how you decide to chunk if you have a movie or if you have like you know like I never thought about you have an earnings call right how you chunk that would be really important because you basically couldn't you know or average Vector for that segment so if you have two very just separate scenes after a scene transition that might not be what you want you want to have some way of intelligently chunking that information somewhere which would be kind of you know very important depending on whether it's audio or video or combination of both um it'll be like a fairly significant decision as to how you decide to do that yeah so if you wanted to so this is where chunking gets really interesting so if I play this um video here this uh this video is entirely about the classroom right but if for some reason part way through this video you had a car or something like this um then it wouldn't work right because then you're averaging you're generating one vector which is the average over 21 seconds which is a lot and you can read this in the papers as well but per second you generate two frames so you get two image vectors per uh per uh Second and then you do that over 21 seconds and you take the average over all of them so you kind of Squish together all these representations if you have very uh diverse frames in a video it's going to be very difficult um and this is where uh ahmed's um presentation comes in right so you have to do a lot more to to deal with that you have to do very interesting kind of checking strategies and and other algorithms to deal with that but out of the box the the model itself just kind of squishes the entire thing together if you have a big video you probably do want it um you want to kind of semantically Chunk so that you have one concept per vector that's awesome um I mean the cool thing is that sorry um because I run I've been playing the the demo as well with the model and at one point I use as an input a picture that we took from like uh basically ujp Marion and like uh Derek and Tommy like were together and we took a picture together and we also I think Doug was holding his dog and the interesting thing was that like when I presented this uh picture for search the output that I got back were pictures of people like people Gatherings and there's also pictures of dogs um so the model was able to kind of like recognize like oh yeah this content is important yeah we didn't have any meerkats in the data set but like uh Vasco is here with me and he's sad you know he's depressed now he's like what do you mean no meerkats yeah we're gonna change that demo for next time the other interesting thing about this is that um this has only been trained on kind of publicly available data so you can I think this was trained on nine different data sets that were stitched together um but if you want to use it for like very specific applications um then it won't know about those of course and this is goes into what Emmett was talking about for that purpose you have to fine-tune the model to to learn about A New Concept and then it'll be able to associate that New Concept with um other Concepts that it already has in its embedding space and that's where I think the potential for this can go through the roof right it's really usable out of the box it knows about a lot of general concepts but if you have a proprietary thing that that only your Enterprise data has then you can actually fine-tune the model and then use it before you feed data into your vector database and then now you have kind of customized multimodal multimodal search which I think the potential for that is uh is really great all right that's everything for me folks and then do you think that to yourself as a host no sorry yeah I'm gonna I'm gonna throw it back to my I'm gonna throw it back to Sebastian to round us off wait me again okay um so thank you all for listening for watching us if you enjoyed this video give us a like um definitely come back for the next episode in a month we'll do it again um if you have an exciting demo to show off definitely would like to see you um a big big thank you to Ahmed for uh for his super nice demo our super exciting uh to see what you built and um yeah definitely looking forward to see more of your work and what we want can do I definitely recommend to anyone watching here check uh check B1 because that's an amazing company um super exciting about that finally getting the real real update not to Zen proxy um so it's nice always nice to have you here my spider brother um you know and uh big big welcome for dinner and money in the new starters so um yeah and JP and then keep keep kicking and screaming you know maybe one day uh you wear me off and I'll get you that pay rate so thank you for watching and see you next month and yeah once more thank you for Ahmed for your special guest as a special guest bye thank you folks bye ", "type": "Video", "name": "Weaviate Air Episode 11", "path": "", "link": "https://www.youtube.com/watch?v=ZbwjxJuo1eg", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}