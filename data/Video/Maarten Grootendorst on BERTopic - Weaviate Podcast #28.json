{"text": "Thank you so much for watching the 28th Weaviate Podcast! This episode features Maarten Grootendorst, developer of the ... \nthank youhey everyone thank you so much forchecking out another episode of thewevia podcast I'm super excited towelcome Martin gruchendorf to thepodcast uh Martin has created thisincredible uh Library educationalcontent around Bert topic so this ideaof topic modeling using an analysis inVector space to extract high-leveltopics to understand what is in theseVector clusters and what makes themdistinct from each other so firstlyMartin thank you so much for joining thepodcast oh thank you thank you forhaving me here I'm really lookingforward to thisawesome man so could we dive into the Ithink it's about four steps to producingthe bird topic analysis could we kind ofwalk through the algorithm yeah yeah ofcourse uh uh so generally four steps uhdepending on your view a little bit moreand extend it to 10 or so but let's keepit straight forwardum so so what often is happening is ifyou get a bunch of documents right thereare maybe a few thousand hundreds ofthousands millions of them especially inthe context of we V8 for example therecan be many moreum we embed those we need to have somesort of numerical representationand I'll go into that a little bit laterbut it really doesn't matter necessarilywhich type of embedding technique youuse that kind of depends on the use caseso when we have that numericalrepresentation it's typically in highdimensional space right and uh that'sgenerally fine because it gives a lot ofinformation about the document but doingsomething in high dimensional space canbe trickyso before we go to the clustering stepwhich talk about a little bit later wefirst reduce the dimensionality of thesen dimensional factorsthat typically works works out well itit can be a time consuming step sothat's still something we need to takeinto accountum but it makes sure that some of thethe global information is you know isbeing kept in low dimensional spacewhere we don't have the curse ofdimensionality where we can use eveneuclidean distance measures with whichobviously we cannot use in highdimensional space so so we can dosomething with thatum after that we're doing the clusteringthe dimensionality reduction of courseis done with umap which is now currentlyyou know mostly state of the art thereand the clustering is being done withhdb scan it has the nice property of ofgenerating outliersbut that depends on the use case becausenot necessarily everyone agrees thatit's a nice feature to have rightum so so then you have clustering so Ihave these three initial steps we haveclustered our documents intosemantically uh uh into topics withsemantically similar documents andthat's nice and all but then we need todo some extraction of these topics toget some representation out of thatand there are quite a number oftechniques that you can use for that butfrom the context of per Topic at thevery leastum we wanted to have somethingthat doesn't necessarily make a lot ofassumption on what happens before thatnow if you take hdb scan for exampleit's in the name it's a density-basedalgorithm so these clusters decant theycan be circles they can be lines theycan be squares or anything you canimagineum and because they are so different instructure we wanted to have a topicextraction method that doesn't make anyassumptions on how those how thoseclusters might look like or might notlook likewith the ID of later introducingmodularity but again that's somethingfor later so then we resort tosomething that has been done quite forfor quite some time and it's just ageneric bag of words approach with thesmall Twist of not considering documentsbut considering uh topics insteadclusters so to say and same thingapplies with the the CTF IDF that's doneto score these words again we're notlooking at individual documents becausewe don't necessarily care about thosebut we're more interested in theirabstract Global representation so alsothere we focus on clusters instead ofdocuments and yet those four steps areessentially how birdtopic worksyeah and I'm sorry to distill it intojust four I think this is kind of howwe've been starting to wrap thefoundation of we being where we have youknow embed the embed the data intovectors using say a sentence Transformerthen uh compressed into mentality withumap or tsne hdb scan clustering andthen concatenate the Clusters intocluster documents and then extract andthen you say TF IDF to get the keywordsI just kind of been my Foundationguiding thing uh I attended to talk atcohere where you presented this reallyinteresting diagram of kind of thebuilding blocks of this and you touchedon this density based clusteringcompared to k-means you might quicklyjust touching on that a little more saythe difference between uh you know whatafter you do you know vectors then umapand then say hdb scan versus k-means howthey have different structures youmentioned say you know circles in yourdata could you explain that a littlemore the difference between those twoclustering algorithms yeah sure souman assumption of hdb scan or notnecessarily an assumption it takes intoaccount that certain clusters can havedifferent forms and shapes and sizes andum you know it works rather well whenyou have a piece of data that startshere and goes all the way up to acertain other point and it follows thatdensity along the line and using that itcan essentially make sure okay we stillhave that cluster because it's difficultto capture those clusters that aren'tnecessarily in in one side small densestructureokay means does something entirelydifferent and basically draws lines overall of these uh clusters and assumesthat everything within you know let'ssay square or certain form that the verycentroid of that is the mostrepresentative of that cluster and hdbscandoesn't necessarily have that assumptionso when you for example extract thecentroid from a cluster found by hdbscan it doesn't necessarily have to bethe most representative one because youknow if you have a circle so all allpoints around the circle and there's onesingle point in the middleand see this game most likely will findall of thosemaybe not the the middle one but if youtake the the centroid of that it's notreally representative of everything thathappens on the outside right so then acentroid based method for topicextraction doesn'twork it might work if you use gay meansbecause that assumption is more builtinto the algorithmbut we don't care about you know all ofthe assumptions that all of theseclustering algorithms have there's waytoo much taken to account so so weseparate themyeah that's a super interesting Insightthe uh you know say the circle shapewith the dot in the middle and this sortof describes this idea of Representativedocuments so what might be the problemwith that where where you also love theexample of the line and if you have Kmeans it's going to try to like cut thatup into four parts of the line with thecentroidsyeah wow so the the density thing isincredibly interesting uh can we maybetouch on the final product the keywordlisten and come back to the hierarchicalthe topic trees but um so so as we youknow concatenate each cluster into along cluster document and then we usetfid you have to find the uniquekeywords I've seen ideas like settingthe topic labels where you go throughthe list then you find the label throughit can you talk about sort of what youget by doing that when you when you gothrough a list of keywords like saymedicine Healthcare and you say okay letme call this the medicine cluster uhSports basketball baseball the sportscluster I can talk about that kind ofidea of setting the topic label and howthat helps search through the keywordlists yeah of course so so the thingwith topic modeling is that's a rathersubjective technique right it's uh firstof all it's unsupervised so it's reallyhard to have a ground truth andobjective evaluation metrics and thesecond thing is what you consider to bea topic I don't necessarily really agreewithnow the the entire idea of having all ofthese words in in a topic descriptionhelps you understand oh this is what thetopic really is about it already alreadyprovides you with some context aboutyou know what you can find in that topicbecause if you would give it a singletopic label like like sportsI still don't know what we're exactlytalking about in that topic so there's alot of human evaluationum a human in the loop is still verynecessary in these kind of techniquesbecause I can give you some sort ofdescription but you still have tointerpret it yourself and understandwhat the topic is aboutum and I think that's very importantwith these kind of techniques becausepeople tend to look for evaluationmetrics and then do some grid search onthat and find the so-called best methodthat that really doesn't work in topicmodeling you can try if you have a veryspecific use case with a very welldefined evaluation metricsbut even then it's it's so subjectivethat the person really is necessary todo something with these words and theselabels and you know ideally create themtheirselvesyeah is that human in the loop I've seenideas like use like a gbt3 to try toguess do you like that idea yeah yeahI've heard that one before of course Imean it makes sense it's it's textgeneration so if you provide it witheither some of these words or perhapssome of of the representative documentswhatever those might might be then youcan get some sort of descriptionfrom there the difficulty here in liesum you know gpg isn't always necessarilythat accurate in creating topicrepresentations it's not specificallytrained for thatum from an API and and open sourcedevelopment perspective you would need avery large gpt3 model in variouslanguages on top of that which you knowmakes it not really that easy to use outof the box so hopefully I can provide abasic pipeline where you can tweak andchange whatever it is you want and ifyou feel like you need gpt3 to do somefine tuninggo atyeah it's really I think maybe the ideais like with the few shot learning wecould give it a few examples and andmaybe that would work um but really I Ifind this idea of labeling the Clustersso interesting with with say uh with saythe way that we search through Wikipediaarticles we vectorize each of theparagraphs and then we still have thetitle of the article as like a symbolicfilter so you could ask say uh what yeardid Barack Obama become president andyou can filter that by having articleBarack Obama is the title and then thathelps reduce the search space a lot sodo you see maybe labeling these clusterswith uh with the summarization of thekeyword lists I guess as being a way tosimilarly kind of say just searchthrough this cluster and unsupervisedyeah you could definitely the the onething to note here is is that you hadneed to have some ID beforehand whatyou're looking for uh with topicmodeling that isn't necessarily alwaysthe case you know you have uhtickets for some software hundredthousands of tickets and you just wantto see what's in there without anyguidance okay then it's completelyunsupervisedum with few shot learning of course youcan say okay I have some ID I don'tnecessarily know the entire thingbecause otherwise it's a classificationtask but with few short learning you canthen say okay I have some ID let's seeif we can do some semi supervised viewshot learning on top of thatbut then again you have to define thoselabels very well beforehand because ifyou don't uh again then it becomesdifficultI've always been I haven't quite wrappedmy head around how semi-supervisedLearning Works in this sense do you mindexplaining that a little more and Ithink that would be a great topic to gointoso there there are roughly two ways todo semi-supervised uh topic modeling andthe thing is with your topic I'm tryingto keep it as simple as possible withoutany without making way too complex forthe user the first one is a dependencyon umap new map in itself does somesemi-supervised dimensionality reductionand by leveraging that uh you knowhdb scan finds those uh those thingsrather well the other thing you can dois essentially saying okay I have atopic that I know is in there it's aboutsports or whatever and then you createsome keywords or sentence yourself andyou embed that one and then it's simplya cosine similarity with all of thesedocuments and searching for the onesthat are you know that are above acertain threshold and simply say okay Iknow these labels uh those are these inthese topicsum and then you average them outby nudging it a little bit more to thetopic that you have to find yourself andby averaging out those uh documentstowards that specific topic most likelysomething like hdb scan will capture itso sorry for misunderstood so you try toembed a single word so sayum you know I I'm searching through mytweets and I want to have like deeplearning let's say a single phrase so Iembed that into the same Vector spacesthe sentence Transformer is going tovectorize the tweets and then I use thenearest neighbors of that to say this isa topic and then hdb scan like it cantake that signal to kind of structurethe whole Space around it sort of yeahso what you're essentially doing isumyou embed them in the same space andwhen you find documents using some sortof nearest neighbor approach that arevery similar to deep learning youaverage the embeddings between thatdocument and the Deep learning andbetting and every effort to them and usethat instead of the original you knowdocument embedding then you nudge itslightly towards the Deep learninguminput and if you do that with a certainamount of documents hdbs can of coursewill pick that up because you kind ofhave notched it towards the sameembeddinghmmyeah I think that's incrediblyinteresting and it kind of is inspiringmy interest further in this idea of saykind of recursive partitioning likewhere first we have all of our datacluster cluster and then we want to gowith it so say it's all of Wikipedia andwe have like sports history medicine andthen we want to take the medicinecluster and then do it do it again andnow it's like different kinds ofmedicine how do you think about thatkind of recursive verb topic like keepgoing into the Clusters when it's reallybig data so so so that's something thatyou can actually naturally do with ofcourse hdb scan but because within theirtopic we're not trying to be toodependent on a specific clustering modelit's a post-hoc analysis so what you cando is you can say I have 100 000documents I want eventually somefine-grained hierarchy in that so whatwe do is we say with okay means forexample we're going to generate athousand or or five thousand smalltopicsand after we have created those 5000topics we we simply do some sort ofhierarchical analysis on top of that thethe exact one that you want to do thatthat's up to you of courseum and build those 5 000 topics up untilyou get you know less and less and lessand less and then you can slice them atdifferent levels uh and from thereextract the hierarchy that's in thereand zoom in whenever necessary soinstead of going from one topic to verysmall ones we're going from very smallones to a single topicuh so so when you you're concatenatingeach cluster to make the clusterdocument that is where how you run thetfidf algorithm to get the keywords sowith the hierarchy is each small slicethe inverse document frequency iscompared against all the data we startedout with or sort of like its neighborsin the tree on the same level so to sayso so essentially what we're doing is wehave with the CTF IDF from those 5000topics we just have a a topic to ourMatrix with some scores and within thattopic term Matrix we're essentiallycomparing which ones are most similar toone another and we iterately iterativelymerge them and at each step of mergingwe get another layer and get anotherlayer again another layerum and because we've already done theclustering whenever you combine twotopics uh together the only thing youhave to do is recalculate the CTF IDFrepresentation you don't have to doanything with the clustering whatsoeverbecause the CTF IDF is based on thedocuments not necessarily on theClusters that's just splitand if we know which Blitz to combine wejust recalculate the CTF IDFrepresentation and you have your newhigher abstract clustercould we step one so the way the topicsare merged do you mind sorry I'm not cutthe way that it's merged just one moretime yeahyeah of course so so what we'reessentially doing is you have for eachtopic you have a vector right uh in thiscase it's a sparse Matrix uh for for theentire topic term Matrixum but for each topic you have somevaluesand we have 5 000 topicssome are bound to be similar to oneanother especially if you use justcosine similarity between two topics youget certain topics are very similar soif you do simple linkagesome hierarchical linkage what youbasically can say is okay I'm gonnasearch for the two topics that are mostsimilar to each otherand these two topics I'm going to findthe documents for all of for for both ofthem and I'm going to combine themtogether into one new long document andthen recalculate the CTF IDFrepresentationand if we do that plenty of times uhthen we suddenly get a little bit lessuh less topics and uh more abstract andmore General representationsuper interesting and and yeah that wayof the unsupervised kind ofsub-population Discovery is sointeresting uh can we talk also aboutgeneralizing this to image embeddings orsay we have a vector that describes theimage but then we also maybe have sometext for each of the image too likecaptions or maybe it's linked to anarticle uh can you describe how we coulduse this algorithm to explore otherkinds of vector spaces like yeah likeimage vectors Maybe audio vectors wouldhave an Associated textyeah so so what were topics essentiallydoing is saying okay look I'm gonna makethe clustering step in some wayindependent from the topic extractionstepand what that means is that we normallysay okay I'm gonna convert my documentsinto some sort of numericalrepresentation through language modelsbut what if we have other metadata thatwe can cluster instead well we cansimply throw those in instead of youknow the document embeddings and clusterwhatever else is interesting and whetherthat's metadata or images for exampledoesn't really matter again we don'tcare how we do the embedding extractionstep we just want data to Cluster andwhether that's the document embeddingsor imagesthat doesn't matter and then when wehave clustered all of thesedata whether that's images or text orstreaming data or what have you then wetake the documents for each of thoseclusters and do the topic extractionstep so so what you essentially can dois you can say beforehand that you wantyou knowimagesto be clustered and those can be from uhyou know booking.com or Amazon orwhatever you know what have you and youcluster dose instead of the the documentdescriptions and the great thing aboutthat is is thatyou know an image typically representsone sort of concept one sort of topicand with very long documents that's notnecessarily the case then we get a lotof different topics and we have to splitthem into sentences and if you do theimages you know you get one thing andthen you can use the documents and seewhich parts of that document mostlyrepresents that imageyeah I think that idea is super powerfulthe decoupling of clustering and topicextraction slash kind of metadataanalysis like if we have patients wecould uh put say their uh you know likeimages like medical images would be agood one to vectorize or I don't knowmaybe we have like clinical notes so wecan vectorize and then we'd also havethe metadata like age weight heightpre-existing conditions and so with alsothis cluster analysis we can you knowhave these sort of like histograms uhyeah all the violin all the kind ofthings for each of the metadata each ofthe Clusters as well do you think aboutthat kind of thing also like symbolicdata visualization achieved through theclusters of metadata on them oh that'sinteresting especially the the exampleyou mentioned with with clinical data soif you have a number of patients uh andyou cluster dose instead ofum the text related to those pages youcan say okay we have found certainsubgroups of patients and within thosepatients they typically talk aboutdecent decent these topicsand that might make it you know evenmore interesting to do things like thatit's more Advanced Techniques so to sayso I haven't actually seen it being usedlike thatbut yeah I mean when you do a fittransform you provide documents and somesort of data typically embeddings butthrow in your metadata I'm very curiousto see how something like that wouldlook likeyeah I sort of brought this upaccidentally but one of the questions Ihave written down to ask you about and Ithink we'll come back to the technicalquestions after this is uh so you'realso a psychologistdo you use this in your work how haveyou found like has this kind of beeninspired by that uh ofto certain extentso there's a lot of psychology thatindeed tensed up back tends to to getback into the package itself and itreally comes from making sure that it'sto use that you communicate thingsproperlyum API development things like thatwithout necessarily looking at what isstate of the art or what is best it'sreally practical approach so to sayand it really stems from my own needswhen constantly needing to installdifferent types of topic modelingtechniques for different use cases whichit can be difficult at some pointbecause if you install if you pipinstall a it has a different feature setthen you pip install Bespecially when you compare for examplea hierarchical topic modeling withDynamic topic modeling they typicallyend up with two entirely different topicshots and then I cannot combine them orcompare them or you knowand that makes it difficult andumI then start to focus okay what do userstypically want to be using for for thesekind of things what makes it easy whatmakes it simple what makes it fun maybeumso at some point I figured okay when wecreate a topic model I want to haveincluded Dynamic topic modelinghierarchical topic modelingsemi-supervised topic modeling uh onlinetopic modeling if it's possible uh justmake it if possible a One-Stop shop fortopic modelingso that you can create a baseline witheverything you you do and then if youwant there are more specialized topicmodeling techniques that you maybe canuse for very specific instances that'sfine but this should provide you withwith most things needed inside of atopic modelumso so yeah I've mostly focused on thingsfrom a user perspective what do usersneed uh what do they want if I hearabout a feature in in the issues pagetypically I write it somewhere down andsee if I can create itumso yeah it typically it often tends toget back to the psychological backgroundinto development and Communications anddocumentations and things like thatso is it like with the perspective oflooking through psychological literatureuh and trying to break it up to keep upwith it because I know like with tryingto keep up with machine learningliterature it's pretty exhausting so Iwould love something like this toyeah no that would be perfect butunfortunately I haven't found a way todo exactly that because I've I agreefully with you that it's exhausting tokeep up with literature I mean even in avery specific soft field like topicmodeling it's it's oh it's so difficultto keep up and we're of course in a verylarge natural language processing domainwhich gets even larger when you considermachine learning and the transition fromTransformers into computer fission allthese other types of fieldsuh no I mostly focused and that's my mypragmatic approach on things that I canuse right nowso state-of-the-art is awesome state ofthe art is great and at some point uhyou know it's necessary because that'sthose are the things that we're going toend up and using but I really like tofocus onthe things that I can use in practicethe things that are performantum that simply workbut to get back to your originalquestion I've mostly used topic modelingnot in psychological domain but nowmostly in uh in clinical researchyeah super interesting and I I want tocome back to you mentioned Dynamic andonline topic modeling which are twotopics I know you understand that I'dlike to ask you about but uh kind of onthis topic of applications also I Ithink this kind of sub populationidentification could be super useful forthe general thing of generalizationtesting with deep learning where we wantto know like what is in distributionwhat is out of distribution and uh saywe you know put our data through hdbscan and then we only train on data thatisn't defined as an outlier and thentested on these outlier points ordifferent kinds of ideas like that whereyou things like say you train in onehospital's medical images then you teston a new hospital and the Machinelearning model failsyou think about that kind ofgeneralization testing as a applicationfor thisa bit uh generally topic modelingis approached as in a very explorativeuh way of getting to understand yourdocumentsum it really helps that you understandokay this is my data these are potentialclusters we can of course use hdb scanto get more to the very core of thesetopics but it often throws out a lot ofdocumentsthat yeah could be or most likely are inum in the cluster so it's rather strictfrom uh from the default settings whichis fine because it really depends on onyour approach but there are more andmore packages that I've seen developedthat focus on on the explorativeapproach of essentially bird topic socreating a 2d representationand do the do labeling yourself and seeif some things make senseum so that's that's what we're topic forexample also gives back to you there arevisualizations where you can have a 2drepresentation of the documents and therelated topics uh and there are someother packages that go beyond even thatwhere they say okay now that we havethose potential topics let's see if theymake sense let's label them ourselvesum so that that human labeling becomesmore and more importantyeah that's extremely interesting I Ithink also I hope this isn't tootangential but like this exploratorydata analysis I think it's really usefulfor like data cleaning deduplicationwhen I first saw vert topic I put thechord 19 data set into it and then I sawI was like oh I didn't even realizethere were languages other than Englishin this data set until I did thisbecause I didn't really like lookthrough it because like when web scrapedata or are you talking about like goingto PubMed and getting 300 000 papers foryour data set it's like you don't reallyknow what's in it and so yeah exactlyand and I think that's that's alsosomething that has gained popularityover the last few years right thequality of the labels that you have andwhat what is it exactly in all of thosestandard data sets that everybody hasbeen using and nobody knows really ifthere's something in it that shouldn'tbe in there I think even the 20 NewsGroup State the set has some has someDutch articles or some Dutch languagesomewhere in there that I found recentlyyou know know these kind of things areimportant because we're using themconstantly in research and we don'talways know exactly what's in thereso to give to use such a method whetherit's per topic or something else to havean idea okay but we're missing thisinformation or these labelspotentially shouldn't be in this clusterbut or in this class what should be in aseparate cluster or are they connectedto one anotherthat kind of information is is way moreimportant than I think people realizemm-hmm yeah and with like thisself-supervised language modeling whereyeah you have unlabeled data but thequality of it yeah it's not like the UNlike the quality of your labels in thesame sense of like an ner model whereyou're like labeling each thing but youstill want to have good data for thelanguage modelyeah yeah exactly and uhspending time on labeling a little bitmore than you do on fine-tuning yourmodel I think that you will get the mainadvantage of going through your dataunderstanding what it means uhthose labels are exceedingly importantbecause they are often in othersubjective still in a way because youlook through a document and you give thelabel no I I mean that's that's myinterpretation you know and anotherlabeler might disagreeso spending time on these kind of thingsis just importantyeah that's do you see that a lot ofdisagreement within label it like youhave a list of keywords right and thento to psychologists would disagree withwhat the list of keywords that what itwould conclude to soyeah so so it's not necessarily that Isee psychologists a lot with topicmodeling uh I see that more in policyand politics but what you definitely seein Psychology quite a lot is uhnow if you have observers that observecertain Behavior if you're a certaindata or certain things that that humansdo that that need human interpretationyeah that then we need several peoplemaking that judgment to see if there'san overlap and if two or three don'tagree over the same instancesomething is going onat the very least we can say it's not avery clear label or something that isn'tuh clearly defined or cannot even beclearly labeled and as such we can sayokay it's an outlier or it's notsomething that represents the thing thatwe have in mindum and that's also something that indeedyou see more and morepopping up in Industry so so Prodigy isdoing amazing things with the with thelabeling toolsum and I think that's where it needs tostart looking through your data seeingwhat's what's happening thereideally if you have the time have twopeople look at the same date and see ifthey come to the same conclusions ifthey understand the use case well andunderstand what the label shouldrepresent well because you know if wetalk about certain labels I might havehave a certain Concept in mind becauseof my history and experience with thatconcept but your experience with thatconcept might be different so you knowyou have your own subjective experienceinvolved in in labelingand in a way we might need to throw thatout or you know average them acrosslabelers people who look at the dataum a jacketyeah I think the idea of across humanlabelers and then there's also thisinteresting idea of having multiplemodels embedding spaces so this topic oflike domain adaptation like if I embedmy core 19 data set with the all mini LML6 train on all sorts of data likecompare like the general model comparedto the one that's been trained directlyon PubMed papers and I look at the topicspaces of two different models have youexplored this kind of thing yeah sothat's also often what you see happeningwith something like the topic is thatthere are a lot of embedding techniquesthat you can useand I should benefit to that because youknow you can use it for your specificuse case and you can check what whatfits but there's also the disadvantageswhich that with that in thatyou know you can end up with quitedifferent topics and uh then thequestion Becomes of course which one iscorrectnow the definition of correct as wementioned is difficult because whodecides what is the correct topic andwho doesn't but the main thing here Ithink is that you consciously andpurposefully choose which embeddingtechnique you useandyou know some might represent it betterthan others but it's you who choose fora very specific method for very specificreasons and that can be because you havefine-tuned the model on your specificdata and you've tested it with apre-trained general model that doesn'tcapture that domain as well as youthought it wouldum you can also throw a lot of modelsyou know in in their topic or somethingelse and see what comes outbut then you run a little bit the riskof you know choice overloads you have somany choices you don't know what what todo and what to chooseand then you know I give a lot ofum responsibility to to the users it'sup to them to decide okay I'm gonna usethis model for this and these ReasonsI'myou know trying to motivate the users toreally think about Which models are yougoing to be using and why are you usingthat specific model do you understandthe model do you know that when you useyou know uh TF IDF as your inputdocuments that you're constrained withyour vocabulary but if you use fast taskfast text you have a little bit more youknow flexibility with respect to thosewords and if you use long former thenyou can use a little bit longerdocuments instead of you know theregular Transformer models that thathave small token limits and if you usecertain sentence Transformer models arerather fast and and others are very slowuh the ones are some are more accuratesome more are trained to more datathis is tricky and it's sucky that thereisn't a single solutionbut that's just the way it isum and as such it's up to the user toreally dive deep into okay this is mydata and for this data I think this typeof embedding more would work bestyeah it's extremely it's such aninteresting interpretability of themodels to see where how they'reclustering things what they think arethe nearest neighbors of things and sosomething that I want to pick your brainabout also is how do you evaluate thetopic models that there are benchmarksthis is the different thingsyeah so I've gotten this question quitea lot before that's all right becausewhenever you you know develop a modelpeople are gonna ask is it accurate isit performant is it doing what what wewant to be doingandmy answer is also it's always uh youknow maybe not the most fulfillinganswer but it depends becausewhat does it mean to have a a good topicmodel does it mean that the topiccoherence is as best as it could be sothat the topics you know are easilyinterpretable by by humans does it meanthatthe right documents are are clustered inthe right clusters does it mean thatit's performant you know I I we couldmaybe create a model that runs for twoweeks or create one that runs in a fewhoursum there's also new valuation metric wecan optimize for diversity for topicdiversity uh we can optimize foradd depends also on on the underlyingalgorithm right if you use somethinglike a means how many topics are goingto optimize for and what does it mean toto have 100 topics versus 50.because you know for some use cases youjust want 20 topics or so and with HTTPscan you kind of have to tweak to makesure you get roughly around that 20.and you can fine tune based on thatso so it really depends on the use casesbut what I would generally recommend todo is talk with your stakeholder stockwith your audience talk with the peoplewho are actually going to be you knowusing this model reading about thismodelumthey are the ones who need to dosomething with that so for my use casesin in clinical researcher and clinicalresearch I talk with medicalprofessionals I show themyou know something that I thought waslogical a number of topics a number ofgranularity also with respect to thosetopics that can also be evaluationmetrics and I often then tell me that'swrongand that's fine because that helps mereally iterate and see okay it's it'snot granular enough they have veryspecific demands that's fine sell 50topics we're gonna do 200 and see ifthat's makes a little bit more senseumso I I really want want to do it butthere isn't one topic evaluation metricuh that works best because also wellwhat is the definition of best in thecontext of topic modeling that reallydependsyeah super definitely like a human inthe loop a strong component to it and II think maybe like when we would presenta human a surge system and we'd say heydoes this query document pair match soit's like well relative to what else andit's like maybe you show a query in adocument but then so you want to showlike documents two three four as welland it's like these very long paragraphsattacks it's like I can't read throughthis whereas if you have the keywordlist so it's like query and then keywordlist comparative keyword lists maybethat's a better way to like get a humanjudgmentor something like that no no I Idefinitely uh agree with that becausethe thing is we're still we're stillhumans and we're lazy we're not going toread through hundreds and hundreds ofdocuments to see if everything matchesup so so things you can do of course isyou know you have a topic you havecertain keywords you read through thosekeywords you thinkyeah I'm not still I'm still not surewhat this is about and then you can showa few maybe randomly sampled documentsand not too many but just a few togetheran intuitive understanding of oh itmeans these in these kind of models andthese these uh types of topics can thenbe extracted from thoseum so so it's still you know like youmentioned a way of approaching it takinga perspective from the userhow can you make sure that they have theeasiest time making sure that everythingworks as intendedthat is the approach of the topic it ofcourse doesn't always succeed in that sothat's what development is forum but also that that iteration and thatfeedback process to make sure thatumthat makes it a little bit easier forthe user to understand what is happeningand for that I provide many manyfunctions maybe too many you knowvisualizations uh diving deep intocertain topicsbut it's always nice if you're justpresented with you know the keywords andjust a bunch of documents and say okayokay these make sense these do not makesense just from you know a single glanceat those topics and those keywords sothat is the intentionbut it doesn't always succeed in thatyeah it was I mean it's superinteresting I think there are so manyapplications for doing this Vectoranalysis this way of bubbling up thekeywords and then having that be kind oflike the layer above it that lets yousee the Clusters I think that's justbrilliant uh could we kind of for myunderstanding could we get into the twouh could you explain to me what dynamictopic modeling is yeah sure so what isessentially what you're doing is uh youhave your topic modeling and let's sayyou get a topic from that and the topicis about cars so you have a bunch oftweets uh the in those tweets you find atopic about cars and you think okaythat's interesting we have somethingabout car greatnow the thing with tweets is is thatthere's a timestamp attached to it andthat might be interesting because theway we talk about cars today and a fewyears ago might be entirely differentthe topic is still exactly the same it'sstill about cars but today Tesla is waymore popular than it was a few years agofor exampleso we're still talking about the samething about the same same concept thesame topic carsbut the way we talk about this that isdifferent and you know you can do thatin a dynamic way soum over time that you see if a certaintopic has increased popularity over timeand if it has it's the way we talk aboutthat topic different but you can alsosplit thin classes for example so if wehave a certain political topic uh Chinafor example then Obama would talk quitedifferently than Trump about that topicit's still the same topic they stilltalk about China in a certain way butthe way they approach is it might beentirely different and the entire IDwith Dynamic topic modeling and forexample cloud-based topic morning isthat we say okay we have a certain timestampfor our documentsand we take only the documents in thattimestamp and then do the CTF IDFanalysis on top of thatwe don't have to do clustering anymorewe don't have to do any of thoseembedding steps we can simply slice upour data a little bit further so westill have the same cars topic butinstead of this entire line we're takingthis small stepand from that we essentially say okaywe're going to recalculate the ct5dfrepresentation and maybe it's differentin order to not have to recalculate theClusters you would need to have say 2014to 2020 in the initial set and then youcan 2018 to 2020 slice rightyeah that's super interesting one of mythings was to ask about how the topicsevolve over time and see a dynamic topicbeing the phrase to describe that andyeah that is just amazing especiallylike as you think about continuallearning data Centric Ai and managingthese data sets how are they evolvingyou know if we're searching throughsoftware support tickets uh for Wii daylet's say and then we introduce a newfeature and then we see how thediscussion evolves around it over timehelps us to like search and understandthe just our data set of support ticketsopen uh so now can we dive into onlinetopic modeling and what that means yeahso that's very similar to to what you'vementioned before right so uh if you geta bunch of topics or sorry a bunch oftickets for for your software that youhave you can do Dynamic topic modelingon that and that's when you already haveall of that data and you just see ifthere's a trend but you know there is aTomorrow there's new data coming inthere and you might want to know alittle bit more about these new topicsor this new data whether the Clusterschange or whether they become more aboutsomething entirely differentwithin the same topic of course so whatonline learning is doing is basicallysidekits scikit-learns partial fitfunction it's when you train your dataset on a small part and then youcontinuously update it whenever new datacomes in to make sure the for examplethe Clusters get better aligned whetherit discovers new topics which which isalso possibleum and and the basic usage for that isagain rather straightforward becausewe have a pre-trained language model wedon't need to necessarily fine tune thatfor for new data that comes inum it generally captures the rep stationwell uh with a small side note of coursethat if you know there are modelstrained before covid so whenever kovitcomes in it can have difficultycapturing that but setting that aside wecan then use for example incremental PCAto continuously update how they shouldbe you know reducing dimensionality wecan use many batch k-means tocontinuously update the k-meansrepresentationsctf IDF is is rather straightforwardthat's just recalculating itand then there's a gun factorized stepbefore that to make sure that whenevernew words come in we simply add it tothe bag of wordsand that sets you know you don't have todo much to make sure it gets updatedand the one thing to note here is thatyou can introduce a Decay Factorbecause if we have trained for the last10 years on some dataand now suddenly a new topic comes inthen we still have 10 years data ofworth in our topic model so so whathappens if you have you know back andforth representation with 10 years ofdata it tends to focus on those 10 yearsof data because it's quite a lot andthat small new data that comes indoesn't really get that much attentionso you can introduce a Decay Factor byessentially saying so whenever we updatethe model all all datagets a little bit smaller because it'sback of words it's a count right wedecrease the count width one or twopercentso it doesn't get slowly smaller andsmaller and smaller all the older dataand newer data gets a little bit moreattention and becomes therefore also alittle bit more accuratethat's super yeah as we've been thinkingabout the design of bird topic and we V8and how interface this that really mademe think about like the symbolic filterlike adding them as like a wear filterfor year after 2018 that kind of thingwould be really important to the designand this whole conversation has beenreally enlightening to thinking aboutthe design of trying to build this kindof vector analysis in weeviate and whatpeople will need um so you mentioned youhave this user perspective and you'rebuilding the bird topic it's a pythonLibrary can you tell me about yourexperience building out that librarythat was a very interesting experienceit's it's the I think one of the firstpackages that I developed outside of myMaster's thesisum and it has been quite a ride for uhfor the last few years because there area lot of dependency issues uh because ofnumpy some other uh things that wereneed to taken into account but it wasreally really interesting really fun todo because there's so many people thatwant to contribute to the package thathave very interesting suggestionsum by focusing on those users and byfocusing on what their experience iswith the model I make it more than justsomethingfor myself right I don't I don't want tobring out a package that I'm only usingmyself I'm not doing it just for meum and because there are a lot of peopleusing that uh you know I need to listento them I need to make sure thatI take into account the featuresthat are looking for the experience thatthey look forum so for me that psychologicalbackground I think has helpedtremendously in not only listening tothe feedback that has been giving butalso taking their perspective and alsounderstanding okay if I gonna writedocumentation for example how should Ido that how should I make sure thatyou know when I communicate certainConcepts I do that clearly and in awell-explained manner and obviously Idon't always succeed in doing that and Isee that back in the issues which isgreat because then I see okay there areway too many issues about this eventhough I think it's clearly in thedocumentation uh that means I'm wrongit's not clearly in the documentation II should re-examine it you know and lookfor ways to improve it ask for feedbackthings like thatit's a very similar experience withkeyboard and polyfuss where you knowthere's a focus on transparencymodularity user experience these kind ofthingsall of these packages they're notnecessarily the most state-of-the-artthe most complexuh algorithms out there that nobodyunderstands now we're going back toeasily explainable topic modelingtechniques or or packages giving controlback to the developers having themgiving giving them the opportunity tobuild whatever it is that they want todo you know we might to say that youshould do your topic modeling and thisin this way I don't know every use caseI don't know what you're working on andwho am I to say what it should look likeso that has mostly been the the designphilosophy behind these packages andsometimes they work because it gives alot of options to uh to developerssometimes it gives a little bit too manyoptions as a choice overload which canbe difficult and balancing that outhaving a great out of debug experiencewhile still giving you know thousands ofoptions so to saythat has been that has been trickybecause it doesn't always align with oneanother and yeah you know you have tomake some hard choices here and thereyeah super cool well yeah and I I dothink kind of in the space of topicmodeling there are like these LDA likeMatrix factorization approaches and Ithink kind of these steps of uh you knowbird embeddings new map compression hdbscan tfid I think that stack is a littlemore approachable than having toconstruct a big topic term and then youknow singular value decomposition to getoutand I also really want to compliment youon your medium articles that explainbird topic for because I you know Ididn't really know that this kind ofthing existed and your articles are justsuper clear on helping me get up tospeed with this so I definitely want tothank you for that thank you and I thinkin general you've documented thisextremely well the python library is sointeresting there's so much to lookthrough so many interesting topics withthis I think it's a huge idea forunderstanding Vector spaces and all theapplications we've talked about soMartin thank you so much for joining theweba podcast I'm so happy with this oneI think this so much information with itand I couldn't be more excited about thedevelopment of bird topic and leviate ohthank you very nice being here talkingabout it to you about this a really niceconversation and as always if there'sany feedback that you have anything thatyou find okay this is stupid you shouldchange it tell me because I'm reallyopen to anything I cannot make anyguarantees of course but I'll do my bestThanks Martin", "type": "Video", "name": "Maarten Grootendorst on BERTopic - Weaviate Podcast #28", "path": "", "link": "https://www.youtube.com/watch?v=IwXOaHanfUU", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}