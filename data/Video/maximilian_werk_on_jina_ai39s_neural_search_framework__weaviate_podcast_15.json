{"text": "Weaviate is used as a database for Jina AI's Neural Search Framework. In this podcast Maximilian Werk, Engineering Director at ... \n[Music] everyone thank you so much for checking out the we va podcast today i'm here with maximilian work engineering director at gina ai we're going to talk about all sorts of things related to gina ai's full stack neural search solution it's such an interesting software we've been publishing blog posts and tutorials of how to integrate wev8 with gina ai and they're just so many interesting components of this so i'm super excited to get right into it so i think a great way to tip off uh our podcast would be to talk about uh the ecosystem of gina ai uh how you tackle neural search problems and the general philosophy behind it hey conor thanks for having me uh here and give me the chance to talk about jina and how we tackle neural search so as han already mentioned last time we have several components inside gina so i think the first thing that you will come and touch is the dog array which somehow is our tool for massaging data and loading your data into a format that gina understands and then all parts of the ecosystem can handle you can think of it a little bit like numpy array where you put your data and then you can do all the crazy stuff either with numpy or scipy and it's somewhat the same here we have the data format and the reason why we didn't talk numpy in the first place is because in search you have unstructured data and different data types and we needed some structure to know okay where is what um and also optimize it for network transport so um yeah this is the doc array the first part then i already talked about network transport so we have our gina we called internal gina core which is the main package that you also find usually and there we care about all the nasty stuff that you don't want to care about so network communication deploying docker images somewhere where your nice code is inside making sure that when you don't want to deploy docker images but run locally that you also can just run locally without always having the whole several docker images spin up so yeah this is what core takes about so all the infrastructure also coordinates how to deploy to kubernetes and things that you just want to as especially as an ml engineer you don't want to care about um then we have the gina hub and i already talked about the execute or briefly mentioned some of the executors and the executors some other working tools that we have and um so whenever you have your custom code you put it into an executor and these and we want people also to share the executors so for example we have a clip executor we can use a clip either for image or text as an encoder and we want this to be sharable because we believe in this social aspect that people can use other people's code gene itself is open source so we like the idea that you can share it by the gina hub but you don't have to so if you have your private custom executor for your application or potentially even business knowledge is inside you don't want to share it so you can also upload it to our hub without making it public and just having your somehow under your own supervision and in the long run this hopefully currently we are developing also more features for this hub which makes them really a communication platform where people can communicate and also use more services of that we will offer hopefully pretty soon yeah yeah the gina hub was so interesting i've been loving learning more about it and it reminds me of say like github but i love how it's also like hosted microservices and you can query these microservices on the gina hub but if we could kind of step into the executor and and just to increase my understanding of and hopefully people listening is understanding of how executors put operations on document arrays and how flows kind of orchestrate the connection between executors and so the first question i want to ask is about the kind of functions you're applying on your doc on your document array in the executor so are most of these functions already contained in the document array object so similar to numpy array arrays uh document arrays they have built-in functions so uh you know if you're saying documentary.embed or documentary.find or you know the neural information retrieval metrics is it mostly about orchestrating the calls to document array and putting them into an executor object and then wrapping that up sending it to the network layer yeah i would say so when you develop it locally then you just have to exactly this call to the document array and perps and you also load your model and then you call the model and the executors are exactly about okay putting these steps into something that is really runnable somewhere where you want to put it in either cloud or on your server or wherever you want to put it um and yeah you don't need to care about this wrapping yourself too much especially all this network communication this is then taken away from you um and the document array itself you somewhere you ask okay what functions do we have there so we have a lot of um i personally use a lot this helper functions so when i want to embed images they first need to have a certain format and so on and i always forget about how to do this and then there's just a functional case scale me this is the right thing do the right things here and then usually it just works underneath which i'm personally super happy because i never can remember how these functions are called in any other packages and which package how i do it and so we have a lot of hyper functions especially for image pre-processing but also quick data some more understanding functions them you might not put into an executor but all this data massaging for example as a preprocessor you can put in front of your model in order to have the right format when user just send your query as an image you need to transform it in the correct format and for example this you can put an executor but whatever you do on on your doc array you can somehow put in there and um we try to do it like the executor itself has not much or our our executor that we give you that you then use subclass from has no let's say business logic so there's no smartness whatever you want to do with an executor you need to do yourself but somehow it really focuses on keeping away the pain whereas when you want to do things there then you do it on the drug array this is exactly what you described before there you do your operations on the dockery yeah i love the pre-processing uh part of the gina hub i think it's so interesting the way that you take say a big image and then you either probably the most naive solution is to put a sliding window across it and then put each of those chunks into its own tensor in its own as you have that nested and hierarchical structure and people listening i love this nested structure of the gina doc array that lets you go define matches as well as define segments of these objects so say you're searching through a scientific paper you can segment it into abstract introduction related works and you can easily have this kind of hierarchical embeddings and that's one of the most my favorite parts about gina and the things that i've been learning from talking to han and going through the documentation is i love this hierarchical embedding then we just release a new feature let me just hook in there so besides hierarchical embedding we also use this internally to have this multi-modal document which allows you to store text and image somehow at the same time in a document which was not easily doable beforehand but someone then natively use both of them which is really let's say you have a pdf that you want to search on then you might want to sometimes scan the search on the image space and subtitle text page or this space are both together and so um or if you just think about any e-commerce application so usually when do you go to a shop you always have description of your product and the text and potentially you want to search on both and this is where we use also this hierarchical structure in a way that you can have both or multiple modalities at the same time in one document but you don't feel it it's hidden and you somehow you just use it as it would be one so that's pretty cool new feature we just released in a week ago or so yeah that's just so fascinating that adding the images and text search to searching through documents whether it's papers legal documents i don't know how many pictures are in legal documents but this kind of thing and and maybe like tables also that kind of thing for papers especially but um one thing i'm curious about is how well do these kind of pre-processing executors when they're uh you know wrapped up in that kind of executor logic how old are these generalized to the hub so uh maybe to give a couple examples if you have say an object detection model that's pre-trained and it's a part of the executor and the object detection model parses the images chunks them say we found a basketball we found shoes and it designs the uh the overall document object that way or maybe you have a pdf parser and that generalizes to you know it can chunk up any text and separate the images is that kind of the vision for how these pre-processing functions hosted on gina hub generalized to all sorts of applications yes i think yeah this is a very good point so i think this is what we want we want somehow to solve all the kind of application but i think it's naive to think we have this one segmenter that segments text and images and then boom we're done and everybody can just segment the pdfs so um what i see personally that this usually someone have this generalized chunker that does then all the magic is kind of hard but you rather have certain use cases like okay for pdfs i have perhaps one chunker that does this but this is just pdf we will have a hard time to do this at the same time for pdf then for i don't know what else could it be the office document where you do that so and my vision is that for different use cases um you have different executors that can be specialized and then you need this the social sharing because um developing one of these executors may take you several days and to really make it back free it takes you potentially weeks let's be honest that's the case right whenever you want to make a software really good it takes you quite some time and not everybody has the resources and i believe in new research the the problem currently is that in order to solve one research application you can do everything yourself but you will it will take a considerable amount of time to make a product out of it and when you look at traditional search there you have some more ecosystem which is has grown over years over one and a half decades or so in order to give you all the features that you need and the features are well approved and a lot of people worked on them and they tested them and know okay they work in a new research or any emerging technology you need this a lot of parts try them out a lot of the executors in the couple just fail and nobody will use them again that's just i mean life right but some of them will be good and they will somehow stay and i think the hub is some kind of also place where this um new ideas happen and sometimes something is really good a little bit like an open source repository right and sometimes they are good then you have another one which is a little bit better in usability and all of a sudden it raises much higher and i think in the hub people have similar behavior and um so i believe there won't be generalized executors to just do everything but rather a specialized ones and then when you have your use case um people wrote a tutorial somewhere else and then you know okay i can use this one because someone else really explained me how i can use this and then i can use this in a good way i don't have to write it myself so i perhaps need still half a day to understand really what happens but not four weeks to make a production ready so this is then already you save three weeks four days and some hours and i think this is the way how uh new research must grow that you build a lot of components and this is also what we do in general in general um that we try out okay let's solve a new problem and then see where do we fail and usually we always wear somewhere but it's not that so it gets better but still we failed somewhere and then we see oh this is what something that we need to solve or we see oh look we failed in implementing our own indexer uh not completely failed but not made that perfect with all the replication and the nasty stuff that comes when you do databases and oh but we wait is there let's use them let's somehow see where we can partner up and um somehow when it's a really hard problem um i think it doesn't make sense to try to solve everything yourself but find your partners and um yeah this is how we do it and i believe this is how new search can only grow yeah and i'm really excited to get more into partners and understand more of the integrations uh really quickly i just want to stay a little more on this general end-to-end framework and and yeah you mentioned uh there's there all these different components and they're very flexible modular and and you can rotate them in and out for your application case and so i've been thinking about generally this kind of theme of say fine-tuning adaptation with these uh parametric models and you know whether you have an embedding api that's been trained on internet text and now you want to fine-tune it on biomedical papers or you want to fine-tune it on e-commerce products and it was trained on you know just internet image text pairs do you think maybe in the pre-processing layer the segmenter could be should be a parametric function so some are doing natural language processing how they train say named entity recognition models and they try to like label this is the most salient or let's say like um in computer vision you famously have things like grad cam where they highlight like this is the head of the elephant this is the most salient part of the image do you think we should maybe fine-tune these kind of segmenters as well for particular applications to be honest i don't know so i haven't worked with an application where we had this somehow uh segmenting part but i think yes just at some at some point you will run exactly this problem i'm not sure how good so what i understand when you have the segmenting for example in self-driving cars this works already pretty good on images that you see okay this is this especially uh when you have uh self-driving tough cars have a quite kind of narrow domain so you need to identify people and other cars and static objects so it's a kind of um already predefined domain and there it's solved already but yes you're right it might be that for for other use cases we exactly also need to um train these parts in our pipeline and then um not uh currently we we have never i have never thought about fine tuning this but absolutely right um this makes sense and i'm really excited to get into the general topic of the gina ai fine tuner and i think it's such an important part of this ecosystem but uh one more question i wanted to ask you about uh gina hub and the executors is um so i actually have two the first of which is um so with with the executor logic if i'm if i want to implement my own custom kind of processing on a document array where would i be it would i be able to just have to have like a for loop where i'm looping through it how does it say vectorize it and parallelize it is that a kind of a native thing to define some operation on a document and then wrap it into an executor uh yes so i the the executor itself has the function i think it's called apply yeah so we actually have the new apply function which we also implemented some weeks ago which allows you to have a document array and just say okay do with this with every document here the following and then the supply function does the job for you and also as far as i understand it cares about paralyzing it out fanning it out and making it really fast and um yeah so that's possible out of the box yeah i'm really excited to hear that with my cut i've seen so many cases where i've switched like a for loop to a numpy vectorization and the difference could be like 40 minutes to 10 seconds when that kind of changed so so it's really interesting for me to hear about that apply function and add that to my understanding so one other question i have about the general philosophy of gina hub is are are the executor functions are they private if i contribute a gina hub and is this viewed as say my product i'm like a company and i'm i'm pushing this gina hub executor is this my product and my code is private or is it open sharing of the code of the executor that's pushed on ginahub however you want so uh you can decide and if it's openly passed pushed we only do this wire you push your code to github and tell us here's the github repository usually and so i think you can also just push the code but we encourage highly to also show this is the repository because this will also build trust with people right seeing being able to go to the code underneath and check okay is this doing something weird on my machines or is it actually doing what i expected to do um while not too many people might want to do this i think it's generally a good idea to have this because some people go in there and will find malicious stuff and this is always a problem right when you have a hub someone can put you something underneath so we encourage to um include the code in a as a gita because the repository whenever an executor is public but you can also just push it and get the private executor and then use it privately and but use the same infrastructure so you push your code still but you get docker images also in different genera versions which might be not so obvious feature but when we have a new gina version and if you want to use the same executor you just can say oh you use a new gina version you get another docker image with a new later gina version um which is seems like um yeah okay perhaps that makes sense or not but we had big problems with this version control of different versions was of the whole ecosystem different parts of the ecosystem and then also people having their individual executors this this can be quite a pain and so do we want to take away also this pain from you and why the hub looks if you look at it it's like yeah okay i download some code and package it the docker image which which is there there's a docker file so what is the magic here but there is quite some some things then in the moment you put things into production you would feel the pain and we take it away yeah it's fascinating i imagine yeah i remember han talking about the infrastructure layer and having things like push notifications for when you need to update one of your uh executors in the gina hub and overall kind of the flow of versioning and that under the hood stuff that makes a marketplace kind of uh business come to life and and yeah as as you're talking about say the security concerns i was thinking about how easy it would be to say add another layer where you have a uh maybe some kind of cyber security model that takes even like a text representation of the code and tries to classify if there are problems maybe something like that but but like the modularity of it is pretty extensible to add that kind of virus scanning layer onto the infrastructure of gina hub so yeah i don't see that as being something that would um be too problematic just kind of taking people's functions and just adding it to your database potentially right could be not good so yeah so so kind of the next question and i think it comes also this is about the network layer that wraps this and i think the design of the at requests uh tagging and then you have the paths i think it's pretty straightforward how you do that i you just kind of specify the past and then it has that kind of sequential flow where i think when it exits it'll it'll just call the next path is that correct do they share like a file system so to say or is it all about passing data from one executor or one wrapped executor to the next one yes it's all they don't share any file system so the idea is the executors can in principle live on completely different machines data even different data centers so it's possible that you have when you experiment for example you have your local executors which does the easy stuff but then you want to encode 10 000 images with a clip image encoder and you don't have a good gpu on your laptop then you might have an issue but you can still use uh external executor so um the whole um ad request logic that we have in our executors is purely for understanding uh when you glue them executes together in your flow definition um when you call which endpoint which other and which endpoint should be called at the executor so the easiest example here is the indexer so sometimes you write data and sometimes you search and usually when you already call the api at the very beginning of the flow of the gateway you know okay i call the um i want to index later or i want to search it but you rarely want to do both or never and so there i already specify i want to index now and then all executors in the flow know okay i need to do whatever i need to do doing indexing and it might be that you have for example um you could talked about index segmenters before and the segment i might also have two different uh segmenting functions one for indexing then you get pdfs so you need to do the pdf magic and get images and text out of it and the other one might be segmenting just for user input which might be just pure images and you want to search for only people in these images find them in pdf's random example and then the segmenter would get the faces or the whatever you need to identify people i honestly have never worked with image recognition or a people recognition but as an example somehow when you want this then the segment i must do something else but it might be that you can actually use the same segmenter because both is image segmentation so you just have the logic a little bit different for these two functions and then you can do this for all your executors you can define okay in this pathway you do this and then the other pathway you do the other thing and then they you don't need to build two executors to do this but just one executed that does both and this one executor also lives on one machine which for the segmenter is neglectable segment that doesn't care but the indexer when you index something and it saves it locally and the machine and the state and you want to search can't be another executor because it should be on the same machine because executors are um two instances of executors are always share no state with each other so and as such they uh when you have the search in the index they must happen in the same executor and you can't just say i have two of them so uh and i'm sorry if i'm understanding this incorrectly but so if you call an indexer you would have to put the other executor on the same machine as the indexer rather than say uh like writing the indexer to a database or like some kind of layer like that and then multiple executors access that memory is that uh so yeah when you have the indexers either it saves the state locally then somehow you need the index and the search function on the same local storage because saves it locally or they have a shared space or they have some database underneath which they call where the index the indexer is more or less just api layer then multiple of them can share the same database and is this kind of the general philosophy behind like microservice architecture i think like functional programming is that the correct kind of general uh design pattern of this yes yes so so having um so let me talk a little bit about the history how we came there it might be confusing but i think it's uh interesting to to see this example how things develop so this whole thing is called the flow because um originally we had something like a like a circular network so you send your traffic to one executor and then the next one the next one to the next one and at some point it came back to the gateway and then returned the answer this was our initial idea and there you somehow ah whatever i want to say what was your question again there was something very mentioned the micro service architecture the uh right right okay when you have this flow this is not really a micro service uh it's kind of micro service but it was connected by a zero mq so um not really what you see as okay if microservices apis but yeah yeah not really what how you traditionally see a microservice perhaps and but we found okay this is kind of nice because you have as few network hops as possible but at some point we figured out okay network hops might be that might be preliminary optimization and we rather want to have one central and then somehow ask the executors one after the other like like a star somehow and then every executor is like a microservice because you just ask a question and you get a response you ask the next one and get a response and exactly then you have them encapsulated as microservices which you call one after the other so is that um i think they call that the master slave architecture where there's the um the center node and it sends up the um the requests and yeah but master slave is somewhere when you have databases and they somehow all share a common state more let's say share some some knowledge and then you somehow have one that has the knowledge but here's more like uh um central somehow note it's not another master it's just a central note there it has different services so the first service is a segment segmenter okay get me segmented or a second service is um index or get me indexing back search services okay lock some information wherever you want get this back and then the last service is perhaps another indexer because you want to index the something else and so it's more like a central gateway it's also why it's called gateway that's our gateway which gets the traffic and then distributes it where it should go and so once it orchestrates the distribution it flows sequentially around the executors but always back to the gateway somehow it comes always back but without uh writing data to the center node so it sends its data that it's returned to the next executor node and the like if this is the center and then we get a circle around it no no it always sends back so what you described just was how it was before but this has several disadvantages if you have this uh how it was before then um and somewhere never happens you need to either propagate this arrow through all the consecutive executors or you directly need to return the arrow to the gateway and there are a lot of when you have this the circle flow a lot of features that you have in modern micro architecture microservices or microservice architecture that you can't use and which are just not possible scaling itself gets complicated because you need in front of every single executor you need somehow another mini gateway in order to scale or especially since we say that they can be anyhow anywhere in the world let's so if one executor sends the next one but the one is local at my machine and the other one is somewhere in the world then it might be scaled with five replicas then i locally need to understand where are these five logo replicas or i need a gateway in front of these five replicas and the whole network attention gets much more complicated when we don't have the central uh instance we somehow know everything about the architecture that we have here and then can um communicate this and at some point the central instance is the gateway i think to be honest i'm not even sure if we can already replicate it on if we can it but it's pretty uh soon planned that we also can scale this one because um when you have just one entry node and you can replicate everything else for failure but not the entry note you have the same problem again and so all this should be scalable and then all of these entry notes have somehow have an understanding of the whole architecture super cool and so gina has these six great examples right after you go through the intermediate boot camp where you uh go through the course of executor and flow and uh these tutorials for people listening uh in the text domain you have fuzzy string matching question answering chat bot and open domain question answering on long documents and images you have image to image search search images from text and search small images inside large images do you think we could walk through say the open domain question answering on long documents to get a sense of the end to end gina system so maybe we start off with our collection of documents we apply our segmenter to each document then we apply the embedding algorithm then we apply the h sw index and then we kind of maybe we have a re-ranking layer and then we have our question answering layer could you help me understand how how that then comes into the flow and the executor pattern sure let me walk you through this uh to the open the main question answering uh guide that we have first perhaps a small disclaimer i'm not sure if this is a perfect way to solve this problem but it should still show you how you can solve tackle such a problem and how to use the gina components so don't expect it to be the perfect answer to the uh or the latest state-of-the-art answer to how to do open the main question answering but rather yeah understand the components of gina so if you look at the diagrams that you see where you have the gateway then the question generator text encoder and simple indexer you this is our flow that we have in order to index our documents so as i said before when you build your search engine you always have usually have this two steps first you need to get your data that you want to search inside into the indexer and then afterwards you want to search it and um here in this case we built two flows which do the steps individually and so we want to answer questions on a big document so someone let me explain short what we want to do so someone gives has a big long document or perhaps even multiple long documents and afterwards you come with a question and want to directly jump in this long document to the right sentence and get the answer and for this how one way to do this is that you take this long document and break it down into paragraphs or sentences and then you for each of these sentences you formulate a question in order to which would be answered by the sentence and there are models in order to do exactly this somehow to formulate questions for a given answer and in the later when you search and you have a question you actually just look at all the questions that you generated where is my uh somehow where's the newest question to my question and then just return the answer um i think this is the progress uh or the way we do it here and um let me double check if this is really true yes okay it's exactly the right thing because we had so many uh approaches how to solve this problem i wasn't sure if we really do this approach in this example okay and so so the first thing that we do is we build our query flow and you see the gateway and this is um what why it's at the beginning and at the end because your user will interact with the gateway at the beginning once when you send the query and at the end the gateway will answer so the for you it feels like the query is going around what i explained before actually the query goes from the gateway to the questions generator then back to the gateway then to text decoder then back to the gateway then to symbol indexer and then back to the gateway but for you it doesn't matter so from from a conceptual thing is just okay it goes from one executed to the next one and so the flow is the the complete uh uh graphic that you see from left to right then a single executor is one of these teal boxes so for example the question generator is the is one executor and in this question generator is the model that takes a sentence and predicts a query a query that would be answered by a sentence and the question generator itself also um chunks down the document as far as i remember so it also cares about having when you have long document making it to shorter documents so this takes care of both these things and the next executor is the text encoder this then takes the questions that come in and encodes them into a vector space so this is the first model the question transformer somehow is just for transforming or generating queries and the second model is now for generating us the vectors that we need for vector search and then we use a simple indexa to store the vectors inside the database and the simple index are somehow local database which you can easily use which doesn't go anywhere in the cloud you don't need it just needs a file path and it just saves there source data and you can retrieve it later on so it's very convenient for local experiments and so so do you have any questions here uh directly connor about what what is what yeah this this this looks great and it's very clear to me how it flows like i guess like and i'm sorry that i'm getting hung up on this i bet maybe listeners also have already kind of understood this and i'm just a little behind but so the the gateway is the abstraction of the networking object so is that correct the gateway is the thing that's uh just kind of contains the information about the sequential order of how to run things and where they're hosted you know what as a user you don't care about the gateway you never see it as a user you just see the flow and you start something and then you have an api and you call the api and you don't care that there's a gateway but yes you're the gateway is somewhat the central piece that i talked about before that somehow where you can send your request to and then this takes care that all the executors are called in the right order and when they're scaled also that the traffic is distributed evenly between different replicas of one executor and takes care about a lot of these uh things and so um so are these executors so the flow or points to where they're hosted with their api endpoints and do you host these executors on different cloud services so say my pre-processing is hosted on my local machine but then my document embedder where i'm running queries through a sentence transformer that lives on gpu ec2 or something like that and then we come back to say some kind of host for the queries is is it like is so so you post the executors each on different machines yes however you wish yes you can host them all the same machine you can hear hosts in a different machine you can host them all in one kubernetes cluster you can even not host them at all and we have the sandbox feature that i think you mentioned earlier where you just say i want to use this executor from gina please just use it and then the moment you send traffic to us we spin it up if it's not already spun up and then you can just send queries and after 50 minutes or so or i'm not sure sometime it gets down if you don't use it but as long as you use it you can just use and use it and currently it's for free so you don't even have to host the executor itself so um yeah but in principle that's exactly right you can put them wherever you want yeah we have a similar service with the we va cloud service that helps you do like a free sandbox of these cloud services and i agree i think it's so important for say integrating up with uh like google collab and getting a quick sense of the networking layer and how to do that kind of thing so that was super interesting and i love this tour let me directly go in here so if you for example you have the simple indexer where i said okay this stores everything wherever the simple index lifts but you can also use um um somehow abbreviate connector vivian api and this isn't just an executor which whenever it gets then the would not index locally but would exactly use this vivid cloud service and then send to this uv cloud service and it's stored there and when you search it would somehow uh be also like a proxy for the vv8 database and then you can somehow when the query comes the query also goes to deviate and the result is returned back and then you somehow you use gina as an orchestration layer around vv8 and but use the the power of the database itself um in the viva cloud for example yeah that's so exciting for wii veda i love that you just with the network layer you just have the dot slash path and then the the vva cloud service gives you your end point and then you just plug it right back into the gina flow i love that part of it and and switching from the simple indexer to the vivid indexers just use the other thing with okay you need credentials to also connect the database obviously the executor needs to connect but that's it somehow so for uh when you develop locally you can just try locally without uh caring about ubv cloud and then in the moment you want to change you just change and when the indexer has the same interface it just works yeah it's amazing it's so exciting and um so so i thought that was a really great tour of the executor and flow and i also just for people listening i highly recommend the intermediate boot camp like that's just the latest thing that i was at with trying to get my own understanding and it was super useful the documentation is excellent for these kinds of things so uh the next thing i want to talk about is the gina ai fine tuner i really want to get into kind of maybe you could introduce it and then i could kind of get the particular things that i'm curious about okay so i said earlier we had this problem that at some point we um how we do it we try out to use gina and we see where we fail and then we improve there and originally we thought okay the actual problem in solving neural search is not that the model getting a good model but the problem is the old infrastructure and i think this is true but solving new research has a lot of open problems and we figured there are people that are able to fine-tune their model but a lot of people that have a business or problem or want to create a startup or also already have a search um they don't have the resources to do all the same time to spend researching a new ecosystem and having a machine learning team to build their or compute their own model and we tried several our customers somewhat to to sell them gina or some more explainer you please use them and not even sell just let them use it and we figured so much so often actually they are not able to get the quality under control themselves so because building up this knowledge takes time takes resources so they can't just do it and it works um even for big companies it's not trivial to do so um and so we thought okay that's obviously the next construction side where we need to go i need to do something and so we uh invented the fine tuner or created this fine tuner and um the idea was um initially to be uh somewhat agnostic to the systems use pytorch or tender flow and or pedal pedal and then you somehow have your problem you bring your data and then you get a better model so you also bring your model and you get a better model and i think the open source fine tuner is pretty cool and it's usable um you haven't seen too much progress in the past two months perhaps because um also doing developing it we also found okay it's nice it's a nice tool but also in order to use it you can need quite some knowledge how to use it because you need to understand how to configure it it gives you all the tools that you need but you still need to understand how to use it and it speeds up your progress but still not enough to allow people that are really um just have their business problem they can already use it so you still need quite some machine learning knowledge to use it well and somehow uh use it um in the right way which a lot of people have so for them it's also i think a really cool tool or also a time saver tool but um we figured okay actually we need to develop it further but we are not sure what is the exact right direction and so we first said okay let's first develop in-house a little bit more uh continue developing it a little bit more to one or two specific needs that we have and not keep it a general purpose tool it was in the open source version and try to solve really one or two very specific problems with it and then we're not yet sure what will be the next step perhaps you open source it again perhaps not um might happen that we open source it then when we go further but we didn't want to have this development in the open because if we would have said for example oh okay we throw away two of the three frameworks just one and someone would be unhappy and we need to argue and we need to make sure that um we meet the requirements we add a new feature and with gina we have added a lot of features in the past then at some point we discovered oh we need to throw it away because it's actually not good and it stops our path further down the road so um here we said okay let's inner source it let's uh for a moment uh keep it away from the community to not disturb ourselves and then let's see where we go it might be that we we definitely offer services around it and potentially we open source it again at some point in time perhaps even soon so this is not yet decided but yeah but the reason is not that we somehow want to inner source it for keeping our knowledge but for have some calmness in developing it further and really figuring out what we need there um yes and we i guess in the next two or three months you will see some quite exciting news around the fine dinner which then again makes it really really easy to use it and yeah that's super exciting and um so yeah it sounds like such a there are so many parts with the fine tuning thing i mean i think the tailor layer is extremely exciting the uh you know the the idea of masking out parameters so that you're only fine-tuning say five million parameters of a billion parameter model and then that way it reduces the cost of doing so and i recently spoke with jonathan frankel from mosaic ml and composer and how they're implementing all sorts of things with this kind of model augmentation to do that kind of efficient fine tuning and so then one other part of the puzzle that i see is say the data labeling and so i'm kind of curious like and then and then there's also for people to get a full picture of this fine-tuning uh ecosystem there's also the the hard negative mining there's particular loss functions like triplet losses or maybe you have some kind of contrast of learning objective that are different from like cross entropy y minus y hat that kind of thing so so there are a few different things with it and one thing i really wanted to talk to you more about and get your thoughts on is the data labeling thing how and han had mentioned that um say with um amazon mechanical turk people are get pretty good at labeling data but you still need to have a a good interface to to show to show them how to label the data to you know what what is this about really have you thought about the data labeling kind of software space and how that ties into this particular problem of similarity labeling yes so if you follow the progress in the fine tuner you have there was a labeler at some point and we took it out again and you might wonder why and the simple answer is it was not the quality was not good enough for us that we want to keep it so we didn't felt like good enough to keep it there and it was not yet the tool that we envisioned and so there are other labelers out there that are also more or less specific to fine-tuning search problems but none of them really convinces me so and so i still see the problem there so there is an open field so if you if you if someone in the community knows a good labeler where they think okay this nail search labeling please put it in the comments of the show because i want to know where i want to have the same synergy as this v8 because it would be great to get this off our shoulders but still but it's still not a good there is no good tool uh at least i have not found a good tool and um so this is definitely needed so if this there is no good tool in three to six months yes i guess we will take this challenge again and we'll try to build a really good tool because um this is needed yeah i'm gonna i'm gonna recommend perhaps let me explain why it is so hard for search because in solando actually so i worked before celando similar as han and there we had a labeling tool in-house and this was actually great but certainly not open source in this case and so because for search labeling um in the end you want to have when you have a query and you have let's say 10 000 documents in your catalog you want to say which of these 10 000 documents are good um so what you need you need to see for one query a lot of answers perhaps not all 10 000 but at least the top 100 from whatever is your search metric and then start labeling them but over time you when you have uh it might be that you missed some down the road somewhat that somebody labeled some documents in the long tail that's also totally fine but um when you go then and search and change your your search engine then it might be that some in the long tail turn up to be in the top results and then you actually need to label them again so i think labeling is for one a tool that you need but also to figure out the right process to do the labeling in a way that you can get to measure the right things because if you miss something in the long tail and another algorithm picks them up and puts them at the top righteously but you never label them you will never not measure it and actually you might see oh this algorithm does not perform so good because some things that you labeled initially but might be not as good it just went a little bit down and so labeling in search is an extensive task but also it is a task that way you need to understand how you do this um progressively and yeah one more thing there and once you label somehow when you have this long tail and once you start labeling again some things a long tail and then you evaluate again you also must evaluate your first algorithm again against the same data set so this is again a more operational problem than a labeling problem itself so you label your data set over time it gets better so your data set is also not something that you're just aesthetic which people from that come out of the science world they have the data set aesthetic you won't touch it again someone has done this data set that's perfect but no i think when you have a live problem a real problem that you want to solve um you will have had not have the chance to label it thoroughly but you need to iteratively label to really measure your algorithms and this is um yeah as i said more operational problem than a front-end tool problem could you um what the long tail thing could you help me understand that does that refer to maybe you've asked a very bizarre question and it's long tail in the distribution space of the queries that it sees or is it say long tail in that um you ask a question and most of say it's you ask a question where there's new information and most of the returned documents are that old information and then you know maybe on result like 73 it's found that new information and that's maybe the long tail or you help me just understand yeah okay we had before the um quest open the main question answering and so imagine you have now 10 documents which you want to search and in total they have each a thousand sentences so you have a thousand sentences that could be the right answer and now i have a question whatever it is think about it any questions do you have a good question that we can play around um i like thinking about like who won the nba championship because it requires updating the information and then it requires also that reasoning about understanding the year so you say who won the nba championship in 2022 particularly i love it i love it okay let's say who won the nba the last nba championship that might be the question actually because of the crisis context so and you label the first time and then with an and you can't go to uh tens of sentences so you need to somehow label just some sentences so what you usually do there is you have some search system it just gives you answers and you go through the top 50 answers and you label them and now you have this question and you may see oh uh in this year this is this uh okay this is kind of relevant perhaps a perfect relevancy so you usually do not relevant kind of relevant super relevant or even more fine-grained some more labels and yeah they might say yeah okay this is kind of relevant but not the perfect relevance and then you label this and um yeah you do this but you don't get the perfect so the last in the top 50 the last year was not in there and um so now you might tune your algorithm and then it's a and the right answer is somewhere at position 280. and now we tune your algorithm but out of a sudden the right answer is at position nine and now you need to go there and fight and somehow see again okay the right answer is actually bubbled up and somehow now i need to say okay nine is good so nine is much better than 280. or per that also heavily depends on your search problem so if google puts the right answer on place nine you might not be happy if your e-commerce search platform puts the right answer on the right product on place nine which is really the right fit that might be totally fine because you anyhow c9 product at this thing and you can immediate the image process much faster so um how much of this you have to re-evaluate from these answers to in order to also give credit to the new model that actually answered this question much better it very much also depends on your search system so in an e-commerce search plus you might always re-label all answers under place 9 or 18 or 15 or whatever it is for your platform but for google you might say oh no only the top three are relevant so i only need to re-label the top three because when my new search engine puts it at place four it's still bad and um yeah so but this is also again operational so you need to understand um this is your your system and and this is my problem and um how do i do the relabeling and so i think this for this labeling we need to better understand how to um first solve some very specific problems and then generalize it to provide a tool that actually different people can use and um so so i see kind of two layers of which you could do this fine tuning you could try to say you have some massive model trained with contrastive learning on a massive data set and that produces your initial embeddings you can maybe try to fine-tune that to adapt it to your data domain or you could have say a re-ranking layer after that in which you have a few options like you could have a pairwise encoder that takes two inputs at a time and ranks them like that or you could have maybe a mapping from the embedding vectors into a new embedding vector space or maybe you could stack them and predict the rank order with the softmax labeling yeah and now i ask you another question so how do i evaluate now the first model so i can evaluate both models together because i want to my final result is good or bad but now i just want to train my first model and develop fine-tune both models separately but i want to find you the first model um when does it get better it gets better when the second model can really digest it so uh it's not even clear if you want to label just be after the first model or to get better training data how to do it somehow how to make it better because it's not clear if if some uh relevant questions bubble up the second model will perform better or worse off it might be that it bubbles the right one up but also some really disturbing answers which disturbs the second model bubbles it also up and then the second model performs all of a sudden worse even saw the first model and average performed better and so then the evaluation again sure you want to evaluate the whole system but uh so if you just want to evaluate you can create your valuation data set and that's fine but if you want to train for the second system again you know what is your results what you want to train for but i have no clue how to or it's not trivial and obvious how to define the training paradigm for the first question but come back to your question yes fine-tuning the second model at some point is also really important because it's re-ranking especially in search questions where the first answer must be right is super important this is an absolutely fascinating point you bring up and i'm a huge fan of like michael bronstein and their work on graph neural networks and geometric deep learning and this idea that if you if you have a graph neural network architecture prior and your re-ranker it'll be invariant to the rank order that your first embedding model has produced and that way you don't have these propagating errors where uh your re-ranker might be used to this certain like because it has the prior of looking at it as these this stack of vector embeddings and so i think um at this holds only true if your first model provides you the same set of 500 answers the answer the all of the first 500 is irrelevant but if you're out of a sudden from the 500 250 go out of the set and 250 new come in then the graph neural network model will still give you different results or am i mistaken here uh i think it would be i think it would be it would treat it as a set right so it would um so if um so if you if you fine tune your embedding model and you get a different 250 i think the the general idea of the graph neural network would be to make it less fake less biased towards the initial ranked list so as the embedding model changes it doesn't propagate the errors into the re-rank or as heavily as if it has this prior of the initial rank order and sort of probably biasing itself to predicting it that way but i guess there are things like in transformers how they have that um position encoding like where you give it the signal of left to right in text sequences you could probably add that kind of prior and maybe so maybe if you cook in that kind of embedding in the input layer then you would still have that problem maybe something like that it's just kind of something i'm thinking about as you're telling me about this problem but to be honest i i have not looked into graphics too much so i i uh i have a hard time answering or giving better insights there so i'm sorry for this i think like the key thing is like the the bias in like a convolution is like the answer is locally structured sort of and then you bubble up the local structure and i think the transformers of the graph neural networks are more like global kind of prior but yeah it's a fascinating thing and definitely i think graph neural networks are on that like cutting edge of thing where i'm like do i want to use this is it mature enough where i am going to have a good understanding of how it fails but all that kind of stuff so it's a super interesting topic um fine tuning labeling all the different parts of fine tuning and search systems or fine-tuning your embedding model your re-ranking or maybe the downstream question answering functionality uh can you tell me a little more about particular experiences at zalondo and i'd love to hear about the story of how you and han met and came to be working together at gina as well yeah so and i worked sometime in the landlord then i was asked oh do you want to join the search team and was a new team founded somewhat they should restructure the search build it from an old monolith to microservice architecture which sounds great until you do it um and yeah then i joined the search team and actually the first day i came there han wasn't there so i took a place and just sat in his place because none other was free and but some days later he he somehow uh yeah i then left my hand spot and we said someone none and me said all in a row which is kind of funny so they are now the ceo and cto of gina and yeah life happened and now i'm also here and um there we um han had this vision of this new research already and in the beginning i i was pretty skeptical because um i have before worked in the pricing department and i have seen bring machine learning live needs a lot of knowledge of the problem so uh getting the model right okay yes the model must be good otherwise it will fail definitely but it's not the um perhaps not the hardest part because getting the model right once you have formulated your problem you can do it somehow this is uh there are people that are smart enough that can solve this problem but um understanding how to actually solve search turned out or generally how to solve a business problem in my opinion is often the harder part because there might be requirements which you don't see so there's always a cto with their most loved question and this should be right answered if you fail your model can be perfect if it fails this one question the cto will hate your model so and then you need to somehow build around perhaps around your model uh kind of a safety net which helps you to filter out the bad results or keep good results that you developed over time which is completely unrelated to machine learning so it's just an engineering task that you build around your model and um so but after some time hand and convinced me okay this might be somewhat doing this noodle search with vectors with embeddings might be a really cool thing to do and going away from the traditional search pipelines um and but then two other guys implemented it also put into production and i came to this team only later so i can only in retrospect say what they did and the problem is ham left salando and someone else took it over and it's always when someone completely leaves before having a proper handover which took some time and someone else really onboarded the code there's always the code is always dropped and then the other person also left so what they did in the search team is they said okay let's try it again and let's do it super simple we don't do any multi-layer we just do a one-hot encoding of all our products of all our queries and that worked pretty well to a certain degree and was in the end a very simple approach to the problem um just having this one hot encoding with which cylinder has a massive amount of data so we could just use a massive amount of data train it and that was good oh it was yeah well it was i would say it was kind of good um it also was used in production and we could see okay we actually improve on different kpis give me a second to gather my thoughts yeah what i figured out there um is that uh how we then put into production was in the end there was no model in production there was no no fancy there was a lookup table that was pre-generated which uh somehow um back in the days was the easiest way to integrate it in the running system and then to build some trust that actually we could provide value and then only after week and which is when when you think about how gina doesn't research is always full system for pipeline but back then there was nothing like gina that was just we had to build everything ourselves and then the easiest way it was just okay build the lookup table and provide answers for certain questions where we said okay we for these 300 000 queries we have pre-generated results which we can look up and give the users and um then again uh it was interesting to see okay how do we do training there so we trained purely on click data and now fashion is a really um seasonal thing so and in fashion you have every season new items and the season switches and so on which data do you train and we wanted to you could train on all the data in the past that was too much and then you also have the really old data in there which is kind of i may not give you too much insight because my maybe trends that don't exist anymore so um it's hard then you can't do just for last four weeks because when you just train in the last four weeks you have the problem that um when you have a season switch you don't have the old season your trading data so you can't answer any new questions about the new season so we came to okay a good thing is just take one year and you have a full season cycle somehow inside your training data which uh yeah and then but then the training data was quite big and it came to me ops problems which was not really machine learning but rather okay how can i put the data there and and um do several training runs and save my data um and again rather operational than just the the pure machine learning magic was not not so magic but getting it running and getting the engineering behind it done was often the the real challenge that's super interesting and um so i think so it was a couple things and uh so one thing i wanted to maybe unpack is um is maybe is it this idea of sort of like a test time calibration layer that you're describing earlier like uh like an ensembling and then you kind of have multiple ways of maybe when you have a test query you map it to a few different train queries so so like if you have a frequently asked question template the the problem is about taking a new query and trying to find the most similar query that you already have is that kind of the one of the architectures you're describing this was yeah this was exactly this long uh long document question answering exactly that exactly you do this lookup table in a prepared questions that's super cool and hearing all that sort all these uh different components of it is so interesting and i'm curious like currently do you have like an application that guides most of your development or is it about mostly just like kind of being a master of a bunch of things and going in and out and seeing how it changes the overall like holistic view of it yes we actually have some clients that somehow guides our development and some more i come always new challenges that helps us some uh understand where should we go um so that's very helpful because they don't have our uh our own goodwill somehow obviously they have their requirements and they're not just okay i can keep a blind eye on this floor so that that's obviously pretty good but um internally we have when you go to our documentation page you have to a bot at the bottom it's like a chatbot which uh where can i can put a question and then you get directly jump in the documentation to the right place and um this is something that we need to know or that we use in order to test our own deployment our own um also uh nlp skills somehow uh develop this further and um this might become a product at some point in time but for now it's mostly to in order to test ourselves so actually people just said we need to understand how how we do things and so this is one thing we have um one more um image related uh project which we may announce pretty soon but i don't want to talk about this oh too concrete but there we did a lot of research and how can we actually do it text to image and image image search with better quality and there we heavily use the fine tuner and that okay this is how we configure this tool correctly in order to get real or quality that that really impresses us that we like and that is better than the pure clip the default clip model um yeah and as i said we have some some other also with other modalities from from customers where i can't really talk about it i'm sorry is um in the multi-modal space and i love how gina has this kind of thing where you can add the text and images is very uh natively integrated how you in the document object you have dot text and dot tensor to put the image do you think about other kinds of multimodal fusions and generally is this one of the core motivations so what works surprisingly well is 3d models so when you have 3d models of any kind which you have i mean every computer game is full of 3d models every film that you watch is a movie that you watch is full of computer of 3d and models and in the moment you have some rich or at least a little bit of metadata to them or to some of them must even be for all but for some of them this is pretty easy to train because it is very restructured you have which models can pick up well so i would say 3d models is actually something where i don't see yet that people come up with a ton of problems but i believe in the moment people come up with problems we can actually solve this pretty well with your research so we have seen some and they're really promising and so if there are people in the audience that have a 3d model a search in mind where they say oh i have here this application then please use your research or try your new research because there you will have a lot of success really in short time yeah i guess for video you have the problem that videos are so rich that we need to refine our understanding of neural search so currently it's often one query vector to one result vector but if you have a video and you just have one vector for your video that obviously doesn't work because even the 10 second video they might be driving a car in the background there might be a dog barking there might be someone shouting there might be some some overlay so there's so much information in the video that you can't just say this is the one thing that this this video is about so there um i think that the research is also developing this direction that you have these um multi embeddings for one document where you somehow say okay this is my my video and now i get 10 embeddings out of it and then he's embedding someone focus on different parts of the video or somehow and this they're i'm not sure how they named but there are new models where you also don't say okay please this embedding focus on audio this on text but they automatically give you somehow the multiple embeddings and there i believe the research is not yet far enough that this will go into broad production might be that google uses it okay fair enough but in the broad production but it's also i see some development there and i found this this multi-embedding idea very fascinating because it is naturally the next step if you also if you have a sentence potentially the sentence is about two different things and sometimes when you search this one thing is really important and with something research the other thing is really important and perhaps it's a bad idea to have just one embedding for these two topics but rather you need a technique to have two embeddings but then also to combine them in the right way and um these techniques uh developing this um i see uh quite some potential in the future yeah even for for just so sorry for interrupting no even for just somehow uh fashion search because i come from salando i mean if you if you search for take my shirt you can say okay it's a floral shirt so the embeddings can can focus on so many different uh parts under the arms okay it's a little bit uh looser under the arms or it's as v-neck or whatever it has so it has so many things that you can look at detail at and when you search for it that you might care about that one embedding um i i'm curious if we see there at some development it comes back to your segmenting so uh yeah very segmenting this is just like a segment right you have your uh just a different perception you have your on your document and you have different segments inside this document that you then want in code or yeah yeah that's fascinating maybe like a generative model that can add some context to the query and and have a few different ways so you can have that kind of like ensemble of different ways of forming and embedding of a query i also really thought that was super interesting what you're saying about 3d objects because it has that like depth information that's so important about our visual world whereas uh videos they yeah and they're very noisy with all the things that could be in the background the audio and and they but they're still 2d slices even though they have that kind of motion and time prior that the that the the 3d images don't capture but uh could give me a couple examples of 3d is it like you know like point clouds from lidar scans is that maybe like mris i think are naturally 3d and in games these are kind of mostly the things that you're looking at or something yes yes it's really uh i think um so we haven't looked yet at laser scans but i think there is really promising because um i mean you need also some pre-processing you have laser scan you might have uh i don't know several million points but then you need to segment them again also the search query so there out of the sudden we have laser scan i want to identify okay which is which object here and but your catalog might actually be just uh whatever other objects let's say a house have a catalogue of shelves of desk of different chairs and now i have my d3 scan and i want to have a 3d model with exactly the right pieces then we change somehow the paradigm between what is query or somehow the size of information in a query and in the result so whereas an e-commerce or wherever you have a lot of information about the product but the query has three words out of a sudden you just have this one share but your query is huge and you need to first segment down your query and then answer like multiple of queries inside one query so um there i believe uh the identifying of the object itself once you segmented your 3d scan correctly it's kind of easy kind of easy but the segmenting itself might be perhaps also easy i don't know but yeah to these guys is one thing then the second thing is that we see is exactly 3d or models in games in whatever movies where you have just a database of 3d models and there it's also when you want to then build your game and you have a character you want a similar character because it's a browser of the other character they should look similar so then get another character which looks similar or six like this i think there it's um yeah it's uh it's just amazing the um the i see it kind of as we va and gina ai and addition to kind of looking at the search application where it's kind of building like the data structure layer as well for these multimodal objects and you think about say vision based robotics where what they're trying to do now is they're trying to ha they're trying to bring like the success of transfer learning to robotics where they want to have the success of gpt-3 kind of adapt to any domain they want to see that happen in robotics and what maximilian is describing this having these say lidar scans that create these 3d point clouds of the scenes and then i mean you'd also need a video of that right it'll be four dimensional even right you'd have the uh the time axis of the frames as you do these scans but the richness of that data and putting it into data structures like the gina ai doc array and how it makes it so clear how to structure multimodal data where you have your you know you could have the 3d data the video of 4d data with the video with the point clouds as well and you can also have text descriptions of what the robot's doing maybe you have meta tags like you know this video is collected inside of a lab in berkeley and the lighting was this whatever meta symbolic tags you want to put on but this is like the data structure for putting unstructured kind of multimodal data together and then you got the we've a database putting together these indexes for efficient search through absolutely massive collections of vectors as we see things like billion scale similarity searches and all this is just so exciting so i really enjoyed all this uh talking about all these topics maximilian it's super exciting this field of neural search and i think just so much exciting technology is being built so thank you so much for uh for being on the podcast if there's uh maybe anything you want to conclude with that i might have missed on no i think you you nailed pretty much so the purpose of this uh understanding how to really get the quality up and uh is some exciting topic for the future from the operational and from the uh tuning itself perspective yeah yeah definitely and i really think so much for having me here it was really a pleasure somehow talking about uh gina and my view on new research itself so yeah thanks a lot for having me and thank you so much and i really hope you'll be interested in coming back on the podcast to share further how your understanding is developing with all these things and just super exciting thanks again awesome [Music] you ", "type": "Video", "name": "maximilian_werk_on_jina_ai39s_neural_search_framework__weaviate_podcast_15", "path": "", "link": "https://www.youtube.com/watch?v=o6MD0tWl0SM", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}