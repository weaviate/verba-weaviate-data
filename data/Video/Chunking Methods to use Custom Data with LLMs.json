{"text": "Thank you so much for watching! Introduction 0:00 Split after number of tokens 0:52 Rolling window 1:30 Parse by PDF Header ... \nwhen you ask the large language model aquestion it is important that itretrieves the most relevant informationto answer it this is otherwise known asretrieval augmented generation forexample when you ask Chacha PT what thename of your dog is it has no clue butif you feed it the data that is instored in your vector database and maybeyou specify that Erica has a dog and hername is Bowen chat gbt is able toretrieve that relevant context andproperly answer your questionin this video I'll go over how you canchunk your text data so to ensure thatthe language model is one receiving themost relevant information maybe insteadof feeding it the whole PDF document youfeed it by Elements which I have avisual for and then two you don't wantto go over the context window and thisis dependent on which model that you areusing starting with the text splitter sowhat this does is it takes the PDFdocument which we have here on the lefthand side and it it takes the chunksdepending on the model or the chunk sizethat you defined so for example GPT 3.5turbo 16k has a 16,000 token window butother models have either a smaller orlarger window size um so just what thisis doing is just splitting up the textonce it hits that limit and then boom wehave chunk one which is the blue textand then it moves right on to the greenwindow which is the next chunk therolling window complements the textspitter extremely well and this isbecause once chunk one is finished chunktwo will begin with a few tokens orcharacters that is included in chunk oneso in this example you can see the bluetext from this document is the beginningor the start of Chunk 2 and this is veryimportant in documents where the secondsentence doesn't make sense because itdoesn't have the information fromsentence one llama index Lang chain andhstack they all three have thisimplemented in their Frameworks and atthe end of this video I'll share withyou the link to the documentation so youcan test this out for yourself moving onto how you would trunk PDF documents sopreviously shre and I partnered up oncreating a demo for unstructured so whatwe did was we ingested PDF documentswhich were two research papers and thenwe chunked it by the elements whichunstructured has and if you haven't usedunstructured before I highly recommendchecking it out along with llama indexum so what it is doing is it takes thePDF heading so in this example I havethe abstract introduction and relatedwork so each section will have its ownchunk and this is very important whenmaking queries that are specific totitle so this is that kind of semanticregion of searching and maybe you'd havelike the abstract property introductionproperty Etc and then this is one way tomake sure that you have extremelyrelevant information if you'reinterested in using unstructured withllama index so if you start typing inunstructured you can f uh find the fileloader here and this is where you caninjust txt files doc PowerPoint jpeg allof it then moving on to the Llama indexnode parser so this is where you wouldDefine that trunk size that I talkedabout and then also the chunk overlap soum with this chunk overlap of 20 tokenit is taking the 20 tokens from theprevious trunk into the following trunkand this is a lang chain documentationon where they have their text Splittersum so they have I believe code on how todo this and maybe if you want to splitthe text by the new lines or also thechunk overlap and the chunk size as welland then ending with Hast stack theyhave the chunk size and then again thesplit overlap so you can check that outand then I want to end this video withrecommending you join uh the ariseWorkshop that I will be hosting alongwith Aman from arise and Ronnie fromunstructured we will be exploring thetrunking techniques and reranking forenhancing your retrieval and the resultsin your frag application I hope you guysare able to join and I'll see you nexttime bye", "type": "Video", "name": "Chunking Methods to use Custom Data with LLMs", "path": "", "link": "https://www.youtube.com/watch?v=h5id4erwD4s", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}