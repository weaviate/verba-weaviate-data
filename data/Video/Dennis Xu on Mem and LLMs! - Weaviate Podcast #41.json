{"text": "Thank you so much for watching the 41st episode of the Weaviate Podcast with Mem Co-Founder Dennis Xu! Mem is one of the ... \nhey everyone thank you so much forchecking out another episode of theweeviate podcast I'm super excited towelcome Dennis Zhu co-founder of mem isone of the most exciting note-takingsoftwares out there and I think withthis latest boom in large languagemodels you know gbt4 and all this thiskind of Knowledge Management like thenext generation of note taking and justhow you organize your information is oneof the most exciting applications and soI'm so excited to hear Dennis's ideasand how you're thinking about theseideas so firstly Dennis thank you somuch for joining the weekday podcastthanks so much for having me Connorawesome so could we maybe dive in withkind of the the founding vision of memlike this what's new with yeahabsolutely yeah I mean so I I think youknow thethe story goes back pretty farum you know back in 2014 actually myco-founder and Iumwe uh we were both doing our freshmanyear and internships right and we weresaying we were sitting at a restaurantand uh you know I pulled out my phoneand I saidgiven all that you know given how largeyour digital Footprints aregiven all that our phones actually knowabout us and capture about usit's isn't it incredible how little thatwe can actually leverage thatinformation and that knowledge in orderto actually then help us do things inour day-to-day liveseven as simple things as you knowrememberingum remembering things that we wrote downin the past before which fundamentallyis what note-taking is about right andand that's kind of like what we startedwith which is you know what is thisUniversal behavior that every singleperson at some point in their life doeswhether they write it in a note or orthey do it by sending you know a text tothemselves or whatever that isum to Simply help remember things andthat was kind of like the you know thethe origin and the beginning of a lot ofthose ideas and one of the things weimagine is well what if you couldactually then uh create the structuredrepresentation we called it the me APIright where you know you could carrythis identity of you right anywhere thatyou would goum and you know essentially compilingnotes into into knowledge into thestructured Knowledge Graph thispersonalized Knowledge Graphum and and yeah that's that's kind ofhow a lot of the the idea began butum one of the things that we we've kindof always known is that you know noteswere would be where we startum but the vision has always kind ofbeen far bigger than that right it's umlike what we're building on them isbasically it's a personalized knowledgeof this thing for every individual forevery team every organizationum and it'snow obviously we live in a superinteresting time where you know it feelslike I've waited my whole life for thismoment in technology is our carum we've always had you know there'salways been things that machines havebeen better at humans at you know we'veknown that for a long timebut those have always you knowcomputation right or their processinformation fasterbut we've kind of always assumed thatthere were just you know these dumbobjects that required uh instructionsand very explicit instructions that youknow had to be programmed by by anindividual and now I think we've finallycrossed the threshold where machinesactually understand human language andyou know we can talk about the semanticsof whether it's just predicting the nexttoken how much of that is understandingbut you know I was having a conversationwith a friend recently and and you knowhe said something that was reallyinteresting which as wellif you think about the objectivefunction of a humanum it's simply to reproduce but thinkabout all of the different things thatwe have developed over time that havenothing to do with reproduction rightand all of these capabilities right andsoum right you know regardless of all thatthe the premise is now that we'vemachines that can understand humanlanguagethe the power of this personal KnowledgeGraph that you can build you know thispersonal assistant that you can buildfor anyoneum is uhthat's pretty coolyeah it's super fascinating I think Ifirst came across mem when I would I'dwrite these like Twitter threads of AIpapers and I'd see someone would commentuh at Mehmet to save it to the to themand I and that's how I first came backand wear this and seeing how it extendsfrom just a note taking out to how itconnects with other data sources withyou know Twitter being the example Ijust gave but you know I'm sure you'reacknowledged by all these different datasources that your you know informationcould be hooked into and I there'scertainly a lot of to unpack in thestory I really like that thing you saidabout the personalized uh kind ofKnowledge Graph I like this I like wehave this one idea kind of ineviate landcalled ref to VEC which is about how youyou know have a user and then youinteracted with these items and that'show you form an embedding for that userand some of these yes yeah yeah and Ican imagine these kind of like Pageswhere you know say I create like abiology page and I start saving somethings and then I have this embeddingfor biology and I hook that up into allthese external sources as well and thenit's like yeah analyzedyeah and you know I think what you'regetting at is like everything that weactually do nowadays is captured in oneway or another digitallyum and you can create really richembeddings that represents you know thecurrent context and context really endsup being in most and almost like everyassisted experience what what ends upbeing the the the key piece that'smissing right like if you know if youknew exactly what I was thinking at anygiven point in timeum it's actually not you knowridiculously hard to to surface therelevant informationum at that point to you but it's justyou know do we know what you're thinkingat any given point in time right so yeahthat that that idea is very interestingyeah so and so I really kind of maybefor the sake of the podcast and beingentertaining I want to hop right intothis gbt4 topic like how is this justkind of maybe the almost the clickbaitything like how is this going to changeeverythinguh yeah I think you know I so so here'shere's the big thing I think you knowI've heard a lot of people talk abouthowumyou know gpt4 versus gpt3 and gpt3 iskind of this you know very vague term atthis point because it's gone through youknow three different evolutions andpeople have always called the gpt3 butyou know the earliest versions of gpt3were nowhere near uh you know uh well Iguess like turbo 3.5 now or even DaVinci you know two before that right andI I think it's it's pretty interestingto meum I think there's there's a lot of usecases where you're not really going tonoticethat many differences between chat GPTand gpc4um and I think for a lot of use cases itfeels oh it's like slightly better it'sdefinitely better it's slightly betterbut it's slower blah blah blah right andthen there's there's one thing inparticular that that you know at leastwe've been experimenting with that wefoundum that I think is groundbreaking and isobviously being slept on is just theabilityum for this to write code uh and towrite incredible sound code that isbetter than extremely experiencedEngineers can write when you know it hasaccess tothe entire internet's worth of knowledgeum and all of that stuff right and sowhat becomes really interestingobviously when you have a system thatcan write code right is then all youneed to do is give it an environment intheory and I think this is why there's alot ofyou know conversation around safety andandall of this stuff right like AGIum uh opinions on on some of thosethings but I I think what's what'sreally interesting is if you can havesomething that can write amazing codeand then you can give it an environmentwhere it can actually execute that codenow you actually do have thisself-improving system that can in theoryum do anything and learn anythingautonomously right you just have to kindof give it the right tools right soobviously there's there's been a lot ofconversation a lot of thinking a lot ofum I would I would say it's still likein the early days most of thedefinitely the experiences and and theproducts and and a lot of theinfrastructure too is uh in its toyphase stillum like clearly people have have youknow awoken to the possibility so Iwould just say the main thing with gpt4for meum is the ability to write code and thenI think taking you know one step backum the the way that I I've describedkind of this day and ageto just you know somewhere because a lotof my friends are like heyI've been hearing about this like youknow GPT thing I'm seeing a lot of itum I don't get it like what I mean it'scool right like I you know there's likewhat is it andand and the thing that I would say isyou havewhen the internet first you know reallyreached scale it made the cost of movinginformationaffected the zero approach zero so thecost of distribution right and that'swhat enabled social media it's whatenabled you know blogs and like kind oflike you know what people would calllike web2 rightum a lot of that stuff and I think whathas happened is now the cost oftransforming information has gone tozero um and that is likethat's that's that's really interestingobviously you know there are so manyindustries that are purely just abouttransforming information so manyprofessions that are about you know justtransforming information I've I I'veheard there's already legal you knowum legal and like you know legalcompanies and lawyers tend to be thelatest to adopt new technologyum but a lot of like you knowparalegals Junior employees are actuallybeing replaced by this technology evenlike already todayumand you know you have anything thatinvolves moving you know taking takinglike all these ETL pipelines eltanything that involves you knowpreviously would have requiredarmies of Engineers and Engineeringteams to both build and maintain a lotof that is now effectively free right soI think we're still grappling with theimplications of a lot of what that meansum and what that will mean all I know isI think things are gonnayou know getweird and we'll see yeah I definitelywant to come back to that kind of likeETL data ingestion topic but I thinkthat idea to stay on that idea of how itcan write code a little more it's beenabsolutely mind-blowing the ability ofit like when we had Harrison Chase fromLang chain on we were talking a lotabout uh and also Bob and light at theCEO of weba we're talking about like thethe way that it could maintain softwarelike you could see the errors and kindof maintain the software and just thegeneral idea of you know writing somecode executing it seeing the output andthen chaining that to you know keepwriting code better yeah it is yeah soand and then um I guess kind of onetopic I want to come on in that writingcode sense is like so as I've beentrying to adopt it in my life I'm mostlydealing with these big code repositoriesso I'm thinking a lot about like howwould the like the leviate code basewhich is even bigger than 25 000 tokenslike how do I get get it to navigatethis kind of thing this one I think alot of these ideas yeah like blank chainllama index on how do youknow exactly and I think this is likewhere we're embedding uh you know reallyplays a role too right I think also thewhole world was kind of awoken too andsince embeddings over the past sixmonthsum you know we we've been buildingum we actually started building onembeddings pipelines before we didactually any better stuff and this wasback in December of 2021.um and this was you know back when thethe best available generative model wasDaVinci II uh and it was at that it wasat that point where it was likeinteresting but not actually usefulum you know reliably useful yet but theembeddings models were were obviouslyyou know fascinating right because it inmany ways it wiped away you know yearsof compounding advantages that manysearch companies had right that theydeveloped like these you knowproprietary technology just for searchand then you know I thinkin a lot of ways a lot of that isbecoming commoditized right withembeddings whether it's searchrecommendation systemsum yeah I mean there's obviously still aton of challenges building you know onembeddings with with each of those butyeah amazing and far easier than beforeyeah I think so obviously with the weavepodcast the topic of embeddings is likethe number one thing I want to talk toyou about and I'm so excited about likeI think I figuredyeah so so with the to come back intothe design of mem I'm very curious aboutlike how you think about kind of likechunking up all the units in that gointo the Knowledge Management and youknow embedding it yeahyeah I mean there's you know I thinkhere's the really interesting thingthere's there'sI think currently the primary school offunum where most people are is you justhave this what I call like this likeblob of vectorsumyou have you know Vector index you youvectorize any piece of information aslong as you can represent it in textalthough with multimodal models now youknowum I started to make an appearance Ithink that's going to be reallyinteresting as well but before before wego into thatum and then you just dump it all inthere and then you do a nearest neighborsearch right and then you find the youknow those thingsum and I think that works for a lot ofthe use cases that we can think aboutimmediately and can think about now anda lot of like the search use cases thatactually already existum in the worldumbut what happens is you quickly startlike for for any like realuse cases thatum are are deeper you quickly start torun into the limitations of basicallyjust this like one shot nearestneighbors like you know that's what Iget rightum and you run into that pretty quicklyand I think one of the things that werealized and this isthis is essentially like you know howhow we I think how we think aboutbuilding our infrastructure you knowdifferently is embeddings are a keypiece of the puzzleum but ultimately you also do needstructured knowledge underlying it soit's like you have embeddings thatoverlayin our case it's a structured KnowledgeGraph right that we also use languagemodels to buildum right so it's kind of like imaginethis you know database this actualdatabaseum like you know SQL database thatyou're reading and writing to rightreading from and writing to and then youlayer on embeddings on top of that andnow you can do things instead of justlike these one Shadow find you know thenearest neighbors you can actually dodeterministic you know multi-hop queriesthat require knowledge of you knowmultiple different things all at thesame time a lot of the things that youknow I thinkum things like Lang chain right alsolike thinks about rightum but if you think about you knowit and I think this is all like you knowdeveloping but if you think aboutsomething like Lang chainum obviously one of the issues with itis because everything runs sequentiallyyou know a it'sexpensive but I think that's you knowgoing to become less and less of anissueum but uh latency wise it's expensiverightum so but imagine if you could actuallyconvert a user input whatever that isright into a set of deterministic graphreads right or you know database readsthat we know are fastrightum and and then retrieve informationthat way and then youumyou know blend that actually with like ahybrid index using a netting I thinkthat's a lot of where uh where the worldwill actually head long termyeah that idea of like questiondecomposition like are follow-upquestions needed um I don't know if I'mgonna remember this off the top of myhead but it's like did Einstein use alaptoplike yeah yeah so you break it into thetwo questions to assemble it and yeahand that and there's kind of in thebeginning of as I was listening Ithought we're kind of getting into likethe adding structure to Vector searchand this topic I think is so fascinatingespecially when you're adding the largelanguage model controller on top of thisyeah a lot of yeah like a lot of kind ofleviating the vector databases havingsupport for the symbolic filters as wellas the hsw the vector index and you knowlike things like in our 118 edian talkedabout the bitmap index the speed of andall these kind of things and yeahdata structures right yeah that yeahthat involve embeddings but it's notjust this like single flat blob uh yeahyeah and so I'm so curious about thisactually there's nothing I'm talkingwith Harrison about privately is likehow do the large language modelsinterface with these symbolic filtershave you thought about this yeah yeah Imean so I think this is actually whythe fact that it can write code this isreally in right right accurate codeum really comes into play right becausenow you can actually imagine you definea data structure you define a certainKnowledge Graphum then you can you can have thelanguage models you can teach thelanguage models right so to speak toknow how to interface with it and toknow how to query from it and to knowhow to write to itum and you kind of takethat whole process that would have beenyou know teams of Engineers for each youknow each Pipeline and eachtransformation beforeum into just like heyyeah so when Chad gbt first came out uhBob had showed me this thing he's likehe shows me his prompt sequence and he'shaving it write Json like it populatesthe Json dictionary so it's compatibleto the next API call and all that was sofascinating um so yes I think that was aa really great top coverage of like thegbc4 and I I really want to kind oftransition to this next topic this is apretty big one which is the kind offine-tuning models versus a zero shotdebate and I'm curious especially withyeahum so so my my take on thisum has been prettyyeah so so here's my takeum I think it's clear we're at somepoint in the S curve of the capabilitiesof these language models I really don'tknow where we are on the s-curve are wenear the end are we you know towards thebeginning if I had to guess I would saythere's no reason to believe that we'reanywhere close to the to the top of itand that we're probably likely very muchstill in like the steepest part rightand I think when we are in that scenarioand and if we if we just play back whathas happened over the last you know 12to 18 months and we assume that you knowuh things will continue in in that waythen what's happening is every sixmonths literally every six monthsthere is a step change in capabilitiesand sodepending on what you're fine-tuning amodel for my my opinion is if you'refine-tuning a model to improve itsGeneral capabilitiesyou know with some piece you know ofyour own data or whatever that isI think that's totally not worth itright now because I think what's goingto happen is the next model is going tocome out it's not only going to becheaper it's going to be way fasterrightum and it's going to be way better andall of the time and effort you spent inactually figuring out how to fine-tunethat model we'll go to waste exceptexcept and I think you know it's animportant question to ask like what willnot changeum and this is really you know once wedo get to the top of the S curve inprogress you know slows when we nolonger have a new state-of-the-art everysix months you know it takes longerum the actualthen at that point I think fine-tuningsqueeze the you know the additional 20or 30 percent you know out of out ofthese models becomes actually reallyuseful and the question the importantquestion to ask right is like what doyou need by that point in time like whatis the thing you need to act what is theasset you need to build over the courseof that time in order to be able to dothat and obviously you know it comesdown to you know one way or another someform of data rightum and you know even that's obviouslyit's a loaded term in terms of like howyou you know what is actually usefuldata what it's not butum you know depending on how you defineit at the end of the day it's the datais useful the actual time spent in infiguring out how to tune these models oreven takenyou know off-the-shelf open source modelto you know try to you know do somethingwith that to squeeze whether it'slatency or performance out of it I thinka lot of the time doesn't actually makesenseumI think that the difference isumif you take something like alike a llama you know with alpaca rightI don't know if you saw that that cameout of Stanfordum obviously that's very interestingbecause now you can have things thatactually just purely run on the edgeright you know run exclusively on yourphone and that unlocks a whole new setof capabilitiesum that previously wasn't possible rightbut if what you're trying to do isreplicate a set of capabilities[Music]um at least wewe wish I away fromfrom fine-tuning for that purpose we dowe do actually do a lot of fine tuningum but that's primarily to makeumthe the reason we do it is touh basically get a smaller model to dothe job of a larger modelat lower latencies and that will justyou know obviously that data setcontinues to scaleum as you know as as new models uh asmodels improve and stuffyeah I thought you said a lot ofinteresting ideas um I think to quicklyI really like that s-curve thing youdescribe with this step function if it'sdoubling every six months it doesn'tmake sense to put the effort into it andBob started to be referencing you somuch in this podcast but he he wasrecently on uh coheres uh Twitter theyhad a talk where he said that basicallyhe thinks that like 80 of these casesthe zero shot model combined in a hybridsearch with like bm25 lexical searchthat will cover most of these searchcases totally yeah it's not completelyagree with you but I really want to comethere are two more topics that youmentioned that I think are sointeresting the Llama Alpac alpaca thingwhere it's like you know it is relatedto the next topic of distillation whereyou're trying to get the large modelcompressed into a smaller model but umit gets I I think the reason the alpacathing is so exciting is because peopleare like running it on their phonesright like uh or yeah yeah yeah yeahand it's like the idea of I think whenum when GP T when DaVinci 3 came out itwas something like two cents per thous Idon't know the exact price and like itwas it was more it was six cents perthousand yeah it was it was prettyexpensive to generate a lot of text sothe thinking was like uh you got to beso careful about how you do it but nowas it's trending towards zero it's likethese language models could have likethese massive conversations withthemselves and as I think about mem Ithink about this idea of you know Icreate a a workspace where I puteverything I know about contrast ofrepresentation learning and then I justhave the language models talk to eachother about like sample a paragraphsample paragraph and then build newknowledge over time and yeah exactlytotally yeah yeah yeah yeah yeahyeah and that idea I think is incredibleso so I I do want to come back to thedistillation thing um maybemaybe one topic to begin is I'm verycurious about these kind of like rankingmodels so like the large language modelcould of course rank these documentswith very high Precision like if youhave the query and then you have the top10 it could give you a really greatranking uh so it is maybe I don't Idon't know if it's like prying too muchinto exactly how the sausage is made butlike how what kind of tasks are youthinking of distilling from largelanguage modelsyeah umso there's there's that's one of them uhwhat you just mentionedum you know historically I think insearch it's been called like a crossencoderum and I think there's still a lot ofvalue and you know cross encoders butyou can actually spin up like a apurpose-built Crossing coder reallyeasily actually using you know kind oflike a smaller uh you know one of thesemodels rightum and then you know it returns a lot ofprobs you can do interesting kind ofmath on top of thatum a lot of interesting things there butthere's there's a lot of things whereyeah uh I'll give you one example youknow someone says somethingum to their assistant right to theirmental system like hey do this for me oryou know XYZum but it's it's kind of you know if wethink about the assistant like as aperson it's kind of unpredictable whatyou say to a personum and what you say might have differentintents sometimesum what you the even within you knowthis is a huge problem within searchobviously like search intent like is itis this a a keyword based intent rightor is this like hey is this a vectorintent and then yeah you like you knowrank different things differently uhmaybe you even uh and this is a lot ofour Explorations isum what if you could create Dynamic uisright because depending on what theperson is asking you depending on whatthe person is doingum there's actually totally a differentrepresentation of the information thatis useful and valuableum but you know being able to even justlike classify the intent of the messagelet's sayum obviously you use the the mostimportant use gpd4 on that and it'sit's going to get it right like almostyou know every time assuming you alsogive it enough contacts and all thatstuff but it's slow it's slow it'sexpensive and you're actually really runit on their bigquery right and so whenyou when you have atask like that that usually experienceokay you know fine tune is actuallyvaluable right particularly when latencyand or cost is the issue and you're okaywithcausing you know brain damage to thelanguage model and being okay with themfor getting everything else and onlybeing able to do this thing rightbecause then you can take a much muchsmaller modelum to you know much yeah yeah tobasically just do that one specific taskright and then there's also like youknow ways where you canuse those models to generate trainingdata for the smaller models and it getsreally gets really meta but yeahyeah that last one's my absolutefavorite idea I mean when you said theum the brain damage thing thatinterrupted my I love that topic so muchthat like because you when you fine tuneit this like catastrophic forgetting howthe robustness of it is gone now andyeah but it's very good at one thingyeah yeah yeahI kind of well I I do want to stay onthe ranking thing just a little there'sone question I have for you especiallyas you know product focus with the memthing is is this latency constraint andhow you think about because the crossencoder is really slow compared to youknow Vector yeah yeah yeah well I thinkthere's a lot of thingsum I think a lot of the Innovationthat's going to happen over the nextyear is going to be on the ux sideum you know internally we call this likeaiux which obviously you know what thatmeansum but it's you know to give you onenaive example you look at what Bing doesright where you type something in andit's showing you some of the work thatit's doing rightum I also saw this like in the stripellm like for their documentation thingthey did some really interesting kind oflike front-end animations where firstthey wouldum because well they actually mirroredthe steps of what happens right firstthey would retrieve a list of documentsthey would show you those documentsfirst and then when the answer wouldactually come in it would push thosedocuments down and as you know it kindof tricks you into thinking oh actuallythis is fast right because something ishappening all the timeum and I think there's like there's alot of those tricks you can play youknow kind of uh there and then there'sseparatelyum and this is where I think you knowreally having a structured KnowledgeGraph in our case uh comes into playumthere's a like if you think about mostof like what makes search Fast right insearch indexesuh it's just it's pre-processingum so if you actually havethe data in a structured way such thatyou can pre-process itum you know into useful indicesthat's another way to to make sure thatcertain things that you need to be fastare fast so you know that that littlethe the knowledge graph topic is a funnyone because with weavia it's not likerdf technology with these two full yeahI'd rather talk about hsw but that isvery interesting the knowledge graphlike does mem use things like neo4javaor like tiger graph those kind of thingsas wellno we we actually just we we build allof it on top of postgresyeah I also saw this thing with uh withllama index it's they call it like graphgbt where you could just kind of do likeone query it's just like a client-sideindex yeah yeah like take a thou take athousand search results and they'llbuild a knowledge graph in memory kindofyeah yeah yeah yeah yeahsuper cool so so maybe pivoting topics abit I'm very curious about the earlierwe mentioned this idea of ETL dataingestion to the kind of database partof it and um so yeah I'm really curiousabout like I I find this like memetTwitter thing to be such a fascinatingintegration with how you flow data fromTwitter into your knowledge managementand then yeah so I'm very curious likeyou know obviously open ai's whisper hasunlocked like a massive ETL now peoplecan take YouTube videos podcasts andjust you know do it like that uh andthen gbt4 I so I'm also talking withBrian from unstructured about hopefullygetting him on the podcast as well anduh like gbt4 the ability of it to do OCRright images yeah yeah yeah yeah well II wouldn't even call that OCRumbecauseit's umat least the way I think about OCR is itjust gives you kind of like a snapshotrepresentation back rightum this is kind of it's more likeactuallyit's it's as if like a human could readthe thing and uh you could you could askquestions based on it and they're justum Works kind of through this likelanguage model right it's it's um soit's it's it'sthe the idea of multimodal like theencoder being able to actuallycreate a representationum that is shared between texts andimages and audio and you know videobecause if you think about kind of likewhat people are doing today you knowthis is another thing that I think isgoing to become obsolete pretty quicklywith whisper is people are doing that onYouTube videos rightum and you know it makes sense but youend up losing so much informationbecause you have you capture none of thethe visuals rightum and so much of YouTube videos is thevisuals soum you know you couldtranscribe and then you could try tostitch together the frames right fromyou know the the actual visualsum but if you could actually just godirectly from an mp4 file format intothe embedding that actuallyis in the same space as you know yourtext embeddingsumthat's where things get really reallycooland obviously I think that's that's thefuture right soyeah I think we just saw uh runways isit called Gen 2 I'm sorry if I messed upthe name but the this video generationgenerally the idea of processing videosso I my kind of funny story with this iswhen I first started doing deep learningI was a basketball player coming to gradschool after basketball and after afailed basketball career and so I waslike okay how do I build a deep learningsoftware app for basketball and I hadthis idea yeah you could probablyextract all the highlights from thegames using like confidence yeah thatworks but the problem with that like onething one problem of quite a fewproblems but like the image data videodata it's so high dimensional the ideaof like compressing a minute video intouh let's say 1500 dimensional Vector doyou think that kind of like will need tobe more clever about how we're doing ityeahI think I think that's an issue I Iso one of the things I learned aboutmaybe maybe this was Llama Or maybe thisis something elseummaybe it was llama but I think one oflike the fundamental Innovations was itmade it uhit it turned it from you know 16-bitumlike values right per Vector toum to four bit values right and so itchanged kind of like the thegranularity of of what could berepresented to basically just like eightvalues right between negative one andoneum and that's really how they managed toget something so smallum and yet itkind of keeps a lot of the performanceso I thinkI think there's some really interestingyou know kind of compression techniquesthat were just at like the verybeginning of of discovering soumyeah I I'm I I have a feeling we'll beable to get to that uh pretty soonyeah that's fascinating I love that youbrought that up but one of the bigfeatures new features of weave in 1.18is adding product quantization wherelike you say we quantize the vectorvalues from 32 bits that or well exactlyyeah yeah there's there's like a coupleparts to it but I think something aboutquantization there's kind of like two atfirst there was this thing called binarypassage retrieval where you sort ofoptimize the contrastive models withthat tan H Activation so it's kind ofzero one naturally and then you havethis kind of binary Vector that's longerbut captures information I I think likequantization aware training is anotherthing that hasn't been realized as muchbecause if you put it in like instead ofproducing the vectors and now we'regoing to run k-means tile encoder on thevectors maybe if you put that into theoptimization it's like one of theseinteresting kind of yeah fine tuningmaybe there's something to like ahierarchical structure all these kind ofyeahthis kind of idea of how do we like oneof my favorite topics in weave Vectordatabase is like how do you have a likethere's we've done like a billion scalething with like you know like spherewhich is kind of like copying theinternet and doing that kind of thingbut yeah the videos and all that somaybe kind of evencontinuing on the kind of multimodal Iguess I don't know if this topic is tooout there for these kind of things butlike are you interested in these recentadvances in robotics and say things likesay can rt1 do you think that will havesome kind of role in this as welluh I've been Loosely following it buthonestly I haven't been closelyfollowing enough to be able to sayanything insightful yeah sorry I thinkthat was maybe too tangential the topicI actually one thing I think is reallyinteresting with the KnowledgeManagement thing is like the biomedicalthing let's actually dip into that topicso I think these kind of knowledgegraphs like this idea of havingstructure and unstructured like justthese like you know drug drug proteindrug like Gene protein interactions andthis kind of graph structure right Ithink it's like the one example of acitation is this paper called Prime kgfrom Harvard from the biomedicalinformatics and it's it like if peopleare out there looking for like anexample of what I'm talking about primekg is like the best one I know of andit's like this kind of way like I Iwonder if like with this knowledgemanagement software like mem do youthink you would have like your genomeyour proteome in a in like yournote-taking softwarethat I mean that's that's basically theum yeah that that's basically the visionright which is you know really westarted with notes and I think over thecourse of this year it's going to becomepretty clear to people as uh with someof the new releases that we have comingcoming out thatum like notes are just kind of onecomponent of the type of informationthat you can capture uh and so you knowwe're building a ton ofuh pipelines right from like manual datasyncing uh or sorry automatic datasyncing of you know your email for youknow your calendar like slack messagesall of that stuff right to the one timehey I have this PDF I have this video Ihave this thingum you know and kind of like drag anddrop in right uh into that so reallyyeah the idea is how do we create thisUniversal Knowledge Graph thatrepresents your entire world ofinformationum and then how do we give you the powerof actually being able to hold that inyour hands right because I think if youthink about it in the pastthat's been held in the hands ofadvertisers who then use that to Targetus for that right but if you could carryaround you know one thing that I thinkthis is on our blog too but that we'vethought about isyou know you can imagineyou sign into Netflix or new flicksright with menum you know in in 2024 andyour experience is automatically thisperson is for you right and today yousigned with you know whatever you saidwith Google or something with all thatstuffand you know nothing happens you justmanage to sign itum but if we could actually build thatpersonal Knowledge Graphumthen it becomes really interesting soyeah I think that's exactly what you'veunlocked with mem and in this kind oflike you have an embedding of you thatyou take everywhere that comes from allsorts of things like me from the YouTubevideos I like are now synced up with theAmazon things I'm buying and nothingyeah yeah but yeah I'm also very curiouslike how you think about like privacypreserving AI yeplike is do you use things like Federatedlearning do these kind of topics come upuh I mean I think it's it's one of thosethings that obviously we're we're superumaware of on on at least kind of theresearch sideum I thinkthe obviously the topic is superfascinating right like being able toactuallyyou know run inference onum on encrypted data right would bewould be kind of the dream unfortunatelyit's it's not really possible right nowum I think everything that uh uh it'sit's all you know basically that the theviability level of you know beingresearch projects right soum eventually I think that's that iswhere the world is is gonna head thoughum you know whether it happens in 20252030uh you know 2035 that's that's to bedetermined butyeah amazing well Dennis thank you somuch for this tour of these technicaltopics about you're so knowledgeableabout this and this was such a funconversationum maybe to kind of wrap things up couldyou talk about like kind of yourexperience founding this company likehow's it been it's super successful andall that andwell I I mean I feel like we're at justthe very beginning of the journeyum I mean especially you know given whatthe kind of the the the vision is foryou know the end products if there evenis is this idea of an end product rightbutum yeah it's I think it's beenyou know I would say one of the thingsthat's been really interesting for us isfrom the very beginning we kind of knewthatum this was the vision this idea of youknow how do I build this likepersonalized Knowledge Graph that I cantake with me anywhereum and I think when you have that youknowin mind at the very beginningum I think it's a lot of us to be a lotmoreum I think patient right and makecertain Investments That for example inthe knowledge graph right because that'sone of those things where actuallybuilding this structured Knowledge Graphthat you know and then teaching languagemodels to know how to interact with itum is is is quite the build right it'sit's very complexworking with a lot of evolvingTechnologies kind of all at the sametimeum but having you know just knowing thathey that's this is we have to do itbecause this is this is why we startedthe company this is why we're doing ituh I think has beenum you know one of those things uh thathave you know made us pretty unique interms of how I think most startupsoperate soum that's been you know it's been areally interesting part of the journeyobviously in the middleuh it's been you know around three yearsso far in the middle of the journey iswhen gpt3 really became a thingum and it totallyum I still remember this likethis happy hour that we had and this isduring covet and we had these virtualhappy hours and one of them was justlike when it was when the first you knowDa Vinci one first came out and we justlike sat on you know in the playgroundhe's playing with it uh you know becauseI got I got access to it like oh this isthis is kind of interesting this is youknow this is really coolum and just seeing this Evolution andthis you know this revolution happenedover the courseum you know like of this companydeveloping and realizing kind ofjust how much of an unlock it is for alot of the things that we were doingum as has truly been a sight to beholdright so you know Icouldn't be more excitingto to be building what you know whatwe're building right now soamazing yeah and I'm I'm also so excitedto see we're building and Dennis thankyou so much for joining the weba podcastand contributing all this knowledge hasbeen so interesting to be talking withyou so thank you so muchthank you", "type": "Video", "name": "Dennis Xu on Mem and LLMs! - Weaviate Podcast #41", "path": "", "link": "https://www.youtube.com/watch?v=RujNYB5ZE2c", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}