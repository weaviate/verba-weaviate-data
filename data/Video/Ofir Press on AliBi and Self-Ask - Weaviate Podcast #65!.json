{"text": "Hey everyone! Thank you so much for watching the Weaviate Podcast! I am SUPER excited to publish my conversation with Ofir ... \nhey everyone thank you so much forwatching the wevia podcast I'm superexcited to welcome ophir press oh fearjust graduated with a PhD from theUniversity of Washington uh studyingdeep learning and doing all sorts ofamazing stuff with long context largelanguage models as well as self-assprompting and all sorts of interestingdetails I first became aware of ophira'swork from a recommendation from JonathanFranco you know highlighting this sellfast prompting and ophir was really youknow one of the first people to discoverthis combination of language models andsearch engines and this kind of tool uselike thing so I I was just so impressedby this and I'm so excited to talk toyou about fear oh fear thank you so muchfor joining the weba podcast thank youso much Connor thanks for having meawesome so I think a super interestingtopic to start things off would be toget into this long context largelanguage models like what happened howhow come large language models cansuddenly take in like 32 000 tokens asinputyeah so fora long time we were kind of trainingthese 1000 or 2000 token language modelsand for most of his time language modelsweren't very good they weren't veryusefuland we were justour goal was to we were mainly judgingmodels by perplexityand we werekind of looking at them as like anacademic question they weren't veryusefulbutover the past two years they've suddenlybecomegood at some tasks and now they'rebecoming useful and that's leaving thelab and it's starting to enterproductionand one of the first constraints thatare one of the biggest you knowconstraints that people run into is thatthey want to generate a really longcodes file or they want to read a reallylong book chapteror they want to summarize like you knowa bunch of newspaper articles andwith most models you're gonna hit thattoken limitum pretty quickly and soumgetting models to handle longersequencesumbecame like a really important goal forthe entire communityum training on Long sequencesis really really toughbecause it requires a lot of memory andat some point you're gonna run out ofGPU memory andit also requires probably training onlogger documents but most of thedocuments that we train on are crawledfrom the web and most web documents arevery short they're like a few hundredtokens longand so for the most part even todaywe're doing most of training like 90 oftrainingand very shortyou know 100 tokeninternet or multi-hundred token but sub1000 token documents crawled from theinternetand then we fine-tune for just a tinybit of training maybe one percent fivepercent ten percent of training wefine-tune and longerdocumentsumand there's a few issues there thatum people should understanduhwhen thinking about building thesemodels that can handle long sequencessoI think this all begins by understandingposition embeddings so we can talk a bitabout thatumif you want to go thereumso yeah when the original Transformermodel was invented in 2017they kind of gave you two options forthe position of biddings which is kindof interesting because everything elsein the model it's like you know you gotto do this you gotta do that you gottado this but for the position embeddingsum you had two options you had absoluteposition embeddings and you hadsinusoidal position embeddingsandum so let's just briefly describe bothof them soumwhen you feed in words to a Transformermodelthey were like we don't want to justfeed in wordsbecause then it might not knowthe order of these wordsumif I tell it you knowum the table is blue how will it know todifferentiate between that actuallylet's think of umuha sentence where the word order mattersum what was like the disambiguation likeuh the boy was running outside he wentto get ice cream I think that's a greatone where it's like he is in referenceto the boy from earlier kind ofum no but but there's even betterexamples likeum uhof fear is teaching Conor so word orderreally matters there because I'm tryingto say a little fear is teaching Conorif we if we flip Conor and I feel that'sa different Conor's teaching of fear isa different perfect sentence so for thatso this we really need to know what wordis first or where the second third andfourthand so on and sothe uh people who invented theTransformer model were like when weinput were to this model we also wedon't want the model to just know whatwords are in the sense that I'm feedingin we also wanted to know what orderthey're coming which is a very naturalhuman uh very like intuitive thing whenyou're telling somebody words you're notjust giving them a set of words they'realso giving them in a specific orderclear that that matterslater like this was two years agowith a bunch of other people and me weshowed that you actually don't needposition on banks we could get into thatlater but at the time they thought thatposition embeddings were necessaryum and so they were like okay how do weum tell the model the order we alldidn't know how to tell it what wordswe're giving it we're just going to giveit word vectors but now we also need togive itpositioning informationand so they were like okaytwo here's the two options that wethought up of the first one is calledabsolute position embeddingsjust like we're learning word embeddingsfor every word right so we have a wordrepresentation for cat a wordrepresentation for Connor or whateverposition for teaching we're also goingto learn position representations soevery time we input a word we'reactually gonna it's actually we'reactually going to be inputting twothings we're going to be inputting aword representation and we're going toadd to that a position representation soin the sentence of fear is teachingConor we're going to first input theword of fear with a positionrepresentation for position one and thenis with the position or position twoteaching of the position representationfor position three and so onand just like our word embeddings startout with a random initialization rightso word embeddings when we're donetraining they kind of have meaning rightthe word embedding for teaching is closeto the word I'm putting for taught theword of betting for tables goes to theway I'm running for a chair but whenthey start out they're totally randomthey're just in random places in inspace so the same thing is true for uhposition embeddings absolute positionabundance they start out randomlyinitialized but as training goes on themodel learns certain properties forposition one it learns certainproperties for position two for positionthree and so on it's probably puttingposition one very close in space toposition two and three but very far awayfrom position 500 but 500 is very closeto 499 498.so that was their firstkind of thing that they invented to addpositionality and it totally workedum so this worksput in the paper in the original paperthey talk about this there's an issuewith thiswhat if I trainmy new Transformer model on a thousandtokens so I crawl the web I have lots ofdocumentsI concatenate all of them up into thisreally long you know billion tokenstring and now I'm when I train I can'ttrain on this entire billion uh did theysay billion dollar billion token uh umuh string I can't train the entirestring at once because that one fit intomemory so what I do is I chunk it intochunks of a thousand words I'm trainingon the first a thousand words I'm asecond a thousand words on thatumand so in the original paper they talkabout howthis position embedding system might notwork if I want to extrapolate to longersequences if I want to train on athousand tokensbut then process like you know write athousand one tokens of code orunderstand a New York Times articlethat's a thousand one tokens they werelike if we do these absolute embeddingsand thenwe try to input a thousand in one tokenswe're not going to have a positionembedding for word a thousand and oneandthey recognize that this is going to bean issueand so they were like okayuh let's have another thingand that's uh the sinusoidal position ofbangsso our issue with absolute position ofbettings was that we had a positionembedding for each wordthat was trained and then when we wantedto look at more words than we saw uhduring training we just didn't have aposition of anything and so they sawthis by having a non-parametric functionso that's just a fancy word for sayingthat we're going to have a function nowthat has no learnedum parametersit's a it's like a deterministic likeyou can write it up in Python there'snothing learned in there and thatfunctionit takes a number in int and it returnsa vector such thatumpositions into that are close byget mapped to vectors that areclose by in Vector spacethat's it so we're not going to get intoall the math behind how they wrote thisfunctionumbecause it's not super necessary tounderstand intuition here what's neededis just to understand thatthey have this function and you give itlike you say hey what's the vector forposition 15 and it's like here it iswhat's effective for position 16. andthe promise to you is that the vectorsthat they return are going to be closein space for positions that are closedup so those are the sinusoidal positionembeddingsand they also worked in the originalTransformer paperumandumyeah that's it so the Transformer paperin the original paper they actually didnot lookinto figuring out whether it thesinusoidal position embeddings enableextrapolation in practice they enable itin theory because before right we weretraining a thousand tokens and then wewould want to feed token a thousand andonebut we didn't have one because we didn'ttrain it with the absolute position ofweddings but now with sinusoidalumwe don't need to train position athousand one we just ask for it and weget it so they were like this mightenable extrapolation but they neveractuallytested it in practice yeah so that'sposition embeddings is thatis that does that make sense yeah superinteresting and I think um so maybejumping ahead to The Alibi attention andthe way I understand you know watchingyour video is that the the way is youjust kind of do you know okay the thequery times the key minus three minustwo minus one to kind of do it back butI guess kind of as we're talking aboutthis positional embedding stuff I'mstill kind of missing the piece of myknowledge that understandsum how the kind of quadratic attentionplays into this is like is this is Alibithis kind of position embedding coupledwith some kind of new uh attentionoperation like what happened to thatquadratics you know N squared attentionthat meant that you couldn't have longuh inputsright that's a great questionumsolet me piece that back into the theoriginal story sowhen the original Transformer wasintroduced they talked about howsinusoidal position embeddings mightenable extrapolation in Alibi we showthat they actuallydon't in practice enable extrapolationumandand now I'll talk about the quadraticthing so yeah for a long time a lot ofpeople were talking about how a majorproblem with attentionis thatumit takes quadratic time in the length ofthe input sequence but that'snot actually a problem in practiceif you look at like a seven billion invery small models like less than abillion parameters attention might betaking up a large chunk of yourfeeds forward computation but um if youbenchmarkany model that's larger than uh likefive billion parameters you'll see thatumthe attention computation is actually aninsignificant part of the total feedforward time and I think that in verylarge like 70 plus billion models theattention the total time spent onattentionis less than one percent of the totalfeeds forward so I know that it lookskind of scary because theoreticallyattention is a N squared operation butum that's like atheoretical thing but the most importantthing to do when building Transformersis to actually look at wall clock timeand to Benchmark the real modelsand um it's super important becausesomething that's theoretically o of onecould end up being a lot of time andsomething that's theoretically oh eventsquared could end up being very quickand that's what happens with uh largeTransformer language modelsum as I said in the 70 billion plusprogram range only one percent or lessdon't query of this but it's around onepercent of the total feeds forward timeit's spent on attention most of it isspent on the feed forward layers and sothat wasn't an issueumI know that there were a bunch of papersthat tried tackling this issue bybuilding like linearumuh Transformer models butum yeah they in my view these paperswe're trying to solve a problem thatdidn't really exist andum none of these models actuallyachieveduh good empirical uh performance and soI've neverneeded to to use any of those types ofstrategies and I've never felt thatthose wereum necessaryso yeah so Alibium uh is just a position embeddingmethod we still use theum normal attention mechanism the thesquared attention mechanismumyeah and you can ask some more questionsand then they can open my slides andshow you uh like a brief overview ofalibioh yeah sorry if this is a dumb questionbut so I thought the problem was whenyou're doing that query keymultiplication like you just can't fitthat on a GPU when it's like you know32k inputs with the embedding Dimensionand you end up just having this giganticMatrix I thought that was kind of whatstopped you fromyou know scaling the inputs as big asyou likeum it does become an issue at some pointso it's not this is definitely not adumb question but it does become anissue at some pointumespecially now when people are thinkingabout doing like you know looking at ahundred thousandumtokens at once it's going to become anissue so so now this might beuma research direction that will receiverenewed interest but all the way up tolike eight thousand or sixteen thousandtokens you can definitely fit thatumon a GPU what can umbut can you also just kind of likesimilar to the gradient checkpointingand like ragnets like can you just kindof like you know cut up the keys andthen multiply the query by this set ofkeys kind of exactlyit's it's it's gonna be like really it'sgonna make things slower obviously butyou don't have to look at all thekeys and queries at once you could justif you have 10 000 queries and tenthousand Keys you could uh break downthis computation you could compute everyquery by itselfumso yeah you could definitely do thatit's just going to be slower but yeah ifyou have a really if you need to attendover a really largeinput you could definitely break it downyeah really cool so maybe we could hopinto the slides now and um yeah let's doit as Opie are setting up with this I'dsay to our web podcast listeners we'retrying a new recording software and soif you like this kind of like halfpodcast half like show something visualyou know please let us know in thecomments and and then we'll stick withthis kind of format but yeah it's supercool I think this will be fun to kind ofyou know go between conversation andmaybe looking at something because I dothinka lot of conversation topics especiallylike as we're talking about positionalembeddings maybe kind of like seeingsomething might help help theconversation click maybe not uh greatfor like you know people who like tolisten to pot I think likehopefully the commentary over the slidesalso will still make this work even ifyou're like you know listening to thisat the gym or something where you don'treally like the visual component of thepodcast but yeah it's really interestingyeah I'll try toumdescribe as much of this uh in wordsum so that anybody's just listening youwon't miss out too much uh anyways uhhere's how you implement alibiumso it's one easy step but Step Zero isto comment out the sign of solarposition embedding so yeah as I saidAlibi it's just like a drop inreplacement right there's like uh theabsolute and Senator position ofBrothers there's now rotary positionembeddings so Alibi is just a drop inreplacement for any of theseum so you comment out let's noticeableposition embeddings and then the onestep that is required to implement alibiis to add a bias to the attention scoresum so I just have some intuition herebut this is very obvious too all thehumans that are listening to the podcastum if I have a sentence here like thequick brown fox jumped overum and the last word is over right andI'm trying to generate the next word soover is my query and all the other wordsare my keysand I'm attending to all of theseuh previous words to try and generatethe next wordand as humans we know that when we'rereading a bunch of textthe words that matter most to thecurrent context the words that mattermost to my prediction of what the nextword will be other words that are nearbyand as a as a word is further away itmatters less and less and less and sowhat Alba is going to try to do is tojust encodethis intuition that all of us have intoa Transformerlanguage model so instead of telling themodel hey this is word one this is wheretwo this is worthy we're not going to dothat anymore we're not telling it whichword is one which word is two which wereto see we're just for every contextwe're telling it hey this word matters alot this word matters just a tiny bitless this word matters uh even more orless but and then like you know thesuper further away words don't reallymatter so that's what we're going to dowith Alibi um and here's how we do itso as I said over is the last word sothat's a query and then all the otherwords are our keys right so we have allthese dots products that's how wecompute attention at this current timestepand this is normal attention right so wecompute the dot product between over andover over and jumped over in Fox over inBrownand Alibi is going to justgently change thisby adding or Sorry by subtractinga bias from each of these dot productsand the spias is going to linearly growas we get further away so over and overuh once as a key in which is a querythat's the same word so there's zerobias and then we're going to start goingfurther away over and jump that's oneword away we're going to subtract oneover in Fox we're going to subtract twoand so onand this is the intuition behind Alibibut in reality we actually multiply thisconstantumbyslope it's called M here and this is a anumber that umis not learnedit's just auma setfloatumyeah that's it so that's what happens inAlibi and like the only there's just onemore small technical detail when you doa tension we typically do multi-headedattention right you don't do attentionjust once at every time Step at everylayer you do it multiple times rightTransformers usually have dozens ofheads and so Alibi uses a different Mvalue for each headumuh so if you have eight heads these Mvalues are going to be half a fourth andso on it's just going to be thissequence for 16 heads it's these valuesthere's just a method in The Alibi repothat tells you which M values to use andwhat this doesis that when every head attendsbackwardsto the previous words it biases previouscontext inum varyingly uh decreasing weight sosome heads are going toum have large biases which is going tomean that they're going to only lookvery closely nearby and not reallyattend to words that are far away andsome hats have very very small M valueswhich means that they will look reallyfar away beforethe bias becomes really big and we stoplookingyeah so that's Albaoh it's so interesting I was going toask if this had something to do withthat kind of like lost in the middlepaper because it seems likeum you know when before you explain themulti-head attention how you'd havedifferent values for the M I wasthinking like well this is reallybiasing it towards the most recent uhtokens and then that would reallyprobably impact how you think about thekind of retrieval augmented generationand how you pack the prompt and sort oflike yeah yeah soum there's a bunch of things here sofirst of allum yeah at first this kind of looksweird because it seems like we'reforcing the model to only look reallyclosely nearby and not really look faraway but we've benchmarked this againstsinusoidal and you know rotary andabsolute position embeddings many manytimes over and every single time we findthat it beats or matches all these othermethods so even though it looks asthough we're kind of biasing to onlylook nearbywe're actuallywe're permitting we're letting it lookeverywhereand we're just using this biasas a way toinject positionalunderstanding into the modelkind of give these positionality hintswithoutreally forcing the model to just looknearby it can look wherever it wantsit just has some heads that are kind ofhave a harder time looking further awaybut some heads can they have this thespice is incredibly slow over there andso they can look far away and so whileat first it might seem like this modelcan't really look far away it canum I think that we should look at thispenalty of spies more of a kind of ahint of oh this word is further away youcan look at it but just FYI it's kind offar so yeah so there's noyou don't lose anything and I mean thebenchmarks speak for themselves uh itworks andyeah so that's for Alibi and then ofcourse language modelsas a whole no matter what positionembedding you usehave a very strong recency bias and thisis probably due to many many things butum one of the reasons that this happensis because language has a very verystrong recency bias right you'retraining on all these documents from theInternet or even if it's books orwhatever you're training onmost of the time in order to predict oneword into the future you only need tolook at the past five or six or sevenwords and so no matter what position ofbetting you're going to use yourlanguage modelis going to have an incredibly strongrecency biasright and that's going to make it a bittougherif during inference you're suddenlygiving it like you know five papers fromarchive and asking it a question thatinvolves kind of querying all five ofthem that's that's going to be toughbecause most of the time during trainingit was like you knowcompleting like you know 200 wordsum like news briefs from like Reuters orsomething it was doing a totallydifferent and much much simpler task sothat's an issueum that's a very big issue but thatdoesn't necessarily comefrom the position of betting that youuse it's mostly I think because of umof how the language isyeah that inspires me to ask you thisquestion so I'm very curious about thisidea of retrieval aware training I'veseen like you know Mosaic MPT 30 billiondescribes this staged training wherethey first train with one k length uhsequences and then you know 4K for likesay the you know the first 8 000 stepsof training are with 1K and then thenext 2K for that but so like this kindof retrieval aware training where youwhere you're training a model that takeslike 32k 100k length inputs so you kindof would retrieve from your you knowVector database hopefully and uh and uhand do this during training to have thismodel so it doesn't have this you knowI'm only completing 200 word uh Reuterstitles as you mentioned but it's seenthis kind of thing of having just anenormous context window right so so howimportant do you think retrieval isgoing to be kind of integrated into thetraining of large English models yeahthere's like a few different things thatyou touched on here that are kind ofdistinct so let me break this down pieceby piece soumso stage training is a name that weevent invented inum a paper we published three years agocalled short former for a process thatwe did not invent but we I think we werethe first to kind of uh exploreempirically and kind of show uh that itreally worksum so stage training to my knowledge wasfirst done by Bert and um when we usethe term stage training we mean that youtrain on short sequences for a bunch oftime and then you train on longersequencesum for a minor amount of the time so forexample Bert is a model that can handle512 token sequences but if you read thepaper you'll notice that they trained on128token sequences for 90 of training andthen they did the last 10 of trainingwith 512 tokensumandum they did that and then nobody elsereally did that and then when we wroteuh shorts former we noticed thattraining and long sequences at that timewith the data that we had then and thesmaller models that we had then wenoticed that like even though peoplewere training on like 2 000 or 3000token long sequences at the time wenoticed that it wasn't really beneficialand so we coined this term Statetraining and we really explored it andwe showed that you can actually train onuh very short sequences even shorterthan 128 tokens uh for a lot of time andthen just do the last bit of training onlike a thousand or two thousand tokensand you get results that are just asgoodum as training on really long sequencesfor all for the entire duration sothat's stage training so stage trainingumisn'tum necessarily A retrieval augmentedthing State training just means hey I'mgoing to train on really short sequencesand then I'll train on on longer onesum and that worked really well for Bertin I think it was 2018 okay or 2017. umit worked really well oh no but it was2018. it worked really well for us whenwe wrote shorts former uh in 2019 andthen with Zeke recently did this withone of their MPT models where if Iremember correctly they traded on like 2000 tokens for a really really long timeand they fine-tuned it or they did thethe last bit of training on 65000 token long sequences so you can fita 65 000 token long sequence in a gpu'smemory but it makes training reallyreally slow and so if they would havedone that on for like you know 100 oftraining that would just have beenprobably too expensive way too slowum and so uh because a lot of the thingsyou need to learn as a language modelare really in the short term a lot oflearning is just in the in a hundredTokens The 100 previous tokens sothere's no real reason to train on 65000 tokens for the uh entire duration oftraining so it was like this reallysmart thing where they just trained on Ithink it was 2000 uh token sequences ortwo thousand token long sequences for alife percentage of the time and thenthey trainedum on the super long you know 65 000token books or whatever it was and Ithink that's a super smart strategy soit's still stage training still workstodayum so that's stage training and nowseparate from that uh is the wholeretrieval retrieval augmented models andthen you can I guess combine with thembut let's talk separately aboutretrievalum augmented models yeah I think thatgoing forward a lot of the things thatwe want to do with language models willhave to use some kind of retrievalaugmentation if you want to do like youknow co-pilot style like you know codeassistum for really large repos you're notgoing to be able to fit an entire repointo a language models contextumif you want to do like medical searchyou're not gonna fitevery medical paper that's beenpublishedin your language modelsumcontext window not even in two or threeyears maybe in 10 years you will butright now it doesn't it's not cominganytime soonand soumretrieval augmentation is just a veryclear very simple uhway to kind of get the best of bothworlds both have access to you know lotsof content but also use the power oflanguage models so I definitely believein retrieval augmented systemsumand then the question arises of shouldwe do this during training or should wejust do this during inference and as yousaid if you just do it during inferenceyou have a model that was trained inrelatively shorter documents and thatduring inference you ask it to uh lookat like you know five archive papersit's going to have a tough time I thinkthe solution for that is to doretrieval augmented fine tuning or toumkind of do the last bit of training withsome kind ofum uh log documents uh that are put intothe language models context I I haven'treally seenI haven't been convinced that traininguh with retrieval for the entireduration of training okay somethingthat's necessary but umyeah I think that's something thatpeople are doing research on right nowand it's a really interesting subjectyeah that makes a ton of sense to methat like you you want to build up kindof representations with the shortersequences I imagine the throughput ofgetting each you know training bash intothe GPU with shorter samples is easieras you mentioned maybe like breaking upthe key multiplications so that bringsme into into your next really famouswork that I'm so excited to talk aboutis this self-ask prompting because it'ssuch an interesting part of retrievalwhere if I ask you a question like Ithink the canonical example is didAristotle use a laptop and like youbreak this question into the twoquestions to facilitate the retrieval soI'm just so so curious what your currentthinking on self-ass prompting as wellas kind of what inspired you tooriginally uh go down that routeyeahumI was just playing aroundwith gpt3 this was writtena bit more than a year oh actually ayear agoum so this is before Chachi BT beforegpt4I was playing around with itand I started trying to figure out if Icould trick itand so I started asking questions likeumwhen did Israel land on the Moonumwhich uhCanadianumwas the first person to cl to exploreEverest like all kinds of questions thatare incorrectwait what did I say oh first person yeahhe wasn't Canadian rightumand it would get them wrong I mean sincethen people have put these types ofquestions on Twitter it's like a funnymeme now I like this type of stuff butlike people have asked itum I can't remember who this was I'msorry but somebody asked it when was theGolden Gate Bridgemoved across Egypt for the second timeand gpt's three is like oh of coursethis happened on like you know March 171984 whatever I don't know it's likeit reallyumand yeah for all those questions that Igave like if I remember when I askedgpt3 like you know when did Israel firstsound of the Moon it would be like youknowJune 10th 1992 rightand then I was like okay how do I solvethis like there's no way thatthis can't be stopped like with justpurely the model no retrieval noGoogling no nothing like justjust the model and I pretty quicklycame up with a solution soin the world of gpt3 we weren't doing alot of zero shots prompting where youjust ask it a question we were doingum more like you know single shot or fewshots prompting where you would have afew questions you would write your ownanswers and then you would ask gpt3 likeyou know a new question that you wantedan answer todayumso what I did to toumto fix this issue with a false questionsisum I was like from now on whenever I askyou a question let's break it down intoTwo Steps step one try to understand ifthe question is truthfuland then only if it is truthfulum then answer it right so in theexample questions I would have like aquestion likeumwhen did Spain first acquire nuclearweapons so I would have a question whendid Spain acquire nuclear weapons andthen I would be like is this questioncorrect question mark and then colon ohand then I'd be like no you know Spainnever has acquired nuclear weaponsand then and then I would have answercolon no answer I'm not answering thisquestion like it doesn't make senseright so that's in the example that I'mgiving gpd3 and then I would ask it anew question when did Israel land on themoon and then it would ask itselfhas Israel ever landed on the moon notherefore I'm not answering thisquestionmm-hmmand yeah this work that managed to nowall these questions that it was you knowwhen was the Golden Gate Bridge movedacross Egypt it would ask yourself hasthe golden great bridge ever been movedacross Egypt no and thenumit fixed this this issue and I wasreally really cool to me that just bypromptingyou couldfix these significant issuesand I think my main lesson from there isthat I mean I've known this for a reallylong time and everybody should know thislanguage models are kind of stupid orall machine learning modelsare notthey're kind ofgood at doing what they were trained onand they're not greatat doing things that are super out ofdistribution and so whenever I'm promptengineeringthe thing that I'm thinking most aboutis how can I break this down if I have atask that a language law has never seenbeforethe question that I ask myself is howcan I break this down into subtaskssuch that each subtask is as close aspossible to something that it has seenbeforeso asking it questions with falsepresuppositionsis out of distribution because most ofthe time on the internet when people askquestions they're not asking theseabsurd silly questions they're askingtruthful questionsand so I think that's why it's kindawent crazy and started like giving datesto all these like fictional events thatnever happened it's because most of thetime when somebody asks a question theoutput distribution is a date likeyou're not going to start arguing likemost of the questions I mentioned aretotally legitimate and it's not verycommon that a model observes somebody'ssaying oh actually that question istotally wrong I'm not gonna act like andsothe thing that solved it was to to breakit down into these two steps of is thisquestion even uhreal and then answering it and so thattook me that got me to self ask so inself askthe first thing that we kind of show isthat language models struggle withum multi-hop questions multi-hopquestions are questions that requireretrieving multiplepieces of evidence to answer for exampleum when was the founder of Craigslistborn right you first have to figure outwho the founder of Craigslist was andthen you have toum figure out when they were bornand we notice that language modelsum really struggled with those types ofquestions uh who was president of theUnited States when Billy eilish was bornthey got that wrong and when you lookinto it you can ask language whileyou're so I asked gpt3 who was thepresident when Billy Ash was born and itgets it wrong and then I can ask itum when was Billy Alex born it gets thatright and then I I take that data andI'm like who was president on that dateand it gets that right too so eventhough it has the facts that are neededto answer that's compositional questionit has some kind ofreasoning bug that doesn't really allowittoum to understandcompositional questions sometimesand so what we did was we took thisthinking of um uh breaking downcomplex things into simpler things andsoum The Prompt for self-ask basicallysays hey when you get a compositionalquestion don't try to answer it just tryto break it down into what is the firstsub question that I needum to answer in order to get to thefinal answer so the first question inthe biliate case is um when was Billyeilish born and so I'm like okay justwrite the first sub question great nowwhat's the answer to that it answersthat and it's I'm like okay what'swhat's the second question that you needtoum yeah so who was president on thatdate and then what's the answer to thatGeorge Bush or whatever so the financesGeorge Bush so we just have uh a promptwith a few examples of me doing uh thisprocess the self-ass process with abunch ofumuh example questions and then you giveit your own question and itautomatically decomposes it into subquestions answers to them and thenanswers a full questionum and it does it better than Chain ofThought So yeah I'll let you ask somequestions but I have a bit more to sayafter thatwell yeah I'd love to just keep rollinginto theum Chain of Thought I mean I love it'skind of a really great example of kindof still making the case for a few shotexamples this kind of you know giving itexamples of when a question isunanswerable or you know and andgenerally I love this kind of questiondecomposition I think it makes a ton ofsense for you know all sorts ofquestions and and you know as people areI think most commonly people arebuilding like chat with yourdocumentation all this kind of stuff andyou can expect these kind ofcompositional questions wheredecomposing the facts is extremelyuseful I guess yeah I I mean the theChain of Thought it sounds like you gotyou have somethingyeah so soum we talk a bunch about Chain ofThought in our paper Chain of Thought issuper super coolbut umself-ask improves upon itum so in the paper we show that Chain ofThought sometimes kind of loses itstrain of thoughtum so we have this example in the paperand I'm not going to remember it exactlyright but uh it's like who was presidentof the United Statesum when super connectivity wasdiscovered and we'll get into retrievaland that type of stuff later but rightnow we're just talking about pure youknow there's just a question in alanguage model there's no retrieval herewhenever I say the word retrieval I meanthe language was kind of retrievingfacts from within its memory or whateverwe're not actually retrieving anythingfrom like a webso you have a language model and youhave a question and you ask it uh whowas president of the United States whensuper connectivity was discovered in inthe first uh exactly we're going to useChain of Thought right so we have anexample question with an example answerthat uses Chain of Thought and thatmeans that channel thought just meansthat you're like a human you're kind ofsaying well uh super connectivity wasdiscovered in 1905 the president of theUnited States then was Woodrow Wilson orwhatever so the financial ounces wouldyou Wilsonumbut what happens is that chain ofthoughts others forgets what it's tryingto do so for this specific example withgpd3it said something like superconnectivity was discovered in 1905which is correct and then it said thepresident of the United States in 1913was Woodrow Wilson so the final answerorit said some it said the correctpresident for 1913 but then it said thatwas the final answer do you understandso it knew that superconductivity was uhdiscovered 1905. it did tell us who thepresident of the answers was in 1930 butthat's wrong like it was the presidentof 1905 was a different person and so itgot that wrong it kind of it went offtrackand so what we've learned from self askis two things is first of all always tryto decompose complex queries it doesn'teven have to be a question If you'retrying to build a complex SQL queryusing gpd3 maybe try to First have itbuild sub queries and then combine it ifyou're trying to write a complex storyor marketing text or whatever maybe tryto have it first right like you knowthe the you know one perspective of it asecond perspective then combine them sobreaking complex things down into easiersubtask is super important but then thesecond thing that we learned is thatChain of Thought can sometimes beproblematic because it forgets whereit's going and in general when you'regenerating a lot of text with a languagemodelit can sometimes kind of lose track ofwhat it's trying to do and so the secondthing that umwe we think is really useful in self-askis that it's constantly reminding themodel of what its current sub goal is soself-ask gets this uh question rightbecause self-ass says okay when wassuper connectivity discovered andlamas's answer 1905 and then it says whowas the president of the United Statesin 1905 and the Moss is the president ofthe United States 1905 was blah blah sothe answer is and it gets it right so byconstantly reminding yourself of whatyour current sub goal isum we get the language model to be ontrack more and so we show in the paperthat our method umah does better than Chain of Thoughtyeah I think we all need that right likewhat am I doing again a reminder of mymajor goals and uh yeah I mean the wholething it inspires my interest I meanthis like Auto gbt this kind of moreautonomous AI it it you know it soundspretty natural but that's the next stepI think there are just so manyinteresting things you can kind ofextrapolate about autonomous AI like youcan imagine like uh an llm that wants totrain like a new LM and so it goes andit gives training data for this new llmor I think maybe just the idea of likehaving it make pull requests to code orlike you know research tasks that runovernight or running an experiment andjust kind of waiting for the experimentsto be done I think there's so much tothat kind of like autonomousyou know evolution of this and I guesskind of um I hope this isn't too offtopic but I wanted to get your opinionon these Guerrilla large language modelsas well which which is like a largelanguage model that's been uhspecifically trained for a particularset of apis and so kind of the thinkingwith this self-ass prompting and thisquery decomposer kind of concept is likeif I need to uh split my question intomaybe a complex query or if I need tosplit it into like you know spinning upor like using the GitHub CLI to format apull request these kind of specializedllms in the middle seem to make a lot ofsense and it's almost like uh API usageas a as a task like instead of that kindof like open AI functions thing whereyou output like a Json that has thearguments of the function you insteadhave these intermediate llms specializedto formatting API requests so I'm justcurious if you've seen this paper and ifyou you know if you think that this isan important Direction going forwardyeah I haven't seeing that specificthing but I cantalk about my view about integratinglanguage models with external tools soin self askumwe had this thing where we integrated itwith Google search and we weren't thefirst paper at all to integrate languagemodels with search Google had a paperwhere they integrated language models ofsearch buta lot or all the previous workhad to do a lot of very complicated likereinforcement learning fine-tuning forthe language models in order to gettheir language models to understand howto operate a search engine APIand in self-ask we showed this reallysimple thing I don't know how we werethe first todo this but we were the first to showthat you don't need to train yourlanguage model to follow a search API inorder to understand how to use websearch you can just kind of trick itinto using a search engine and soareprompt is very structured it's alwayslike you get an input question and thenit's like okay sub question onewho was president of the United Statesin 1905 sub answer one blah blah blahsub question twoand so what we did was we used the sameself-ask method and prompt but this timeuh so this system is called uhself-askplus search engine software'sGoogle search sowe have a questionlike who uh you know uhwhat was my question that I likedumwhat is the current time in the countrythat's hosting the World Cupright so if you askGPT 4 gpd3 this question they have nochance of knowing this because I thinkthey were trained before the World Cuphappened and they definitely don't knowso they don't know where the World Cupis happening and then they definitelydon't know what time it is right now inQatar or whatever right they don't haveany real-time dataand so we did this before gpd4 andplugins and web scrapingum and it's still simpler than thoseapproaches but what we did was uh we atwe gave this question to gpd3 with aself-asking prompt and it was like okaythe first question sub question I wantto ask isum what country is currently hosting theWorld Cup right and that's like answercolon and then we stop it so it's aboutto answer itself we stop it we go toGoogle we extract the first questionthat it asks because it's really easy toextract because it literally says youknow sub question one colon so supereasy to parse out we send it over toGoogle we used serp APIum and then Google gives you back ananswer and then we just take that and weput that into the language model promptand we're like here's the answer so thelanguage model thinks that it asksitself and it answered itself eventhough it was Google but it doesn't knowthat it doesn't need to know that andthat's like okay uh sub question twowhat is the time in Qatar and we're likeah wait a second we pause it we takethat question parse it out send it toGoogle Google tells us the answer we putthat back into the prompt and it saysokay the final answer is you know 7 pmumso yeah we did this like a year ago andit's super simple and it's really reallyuseful and it's super powerful and a lotof the previous approaches had thefine-tune language models I think one ofthe previous papers for some reason theyneeded like humans to like they had tokind ofum use like human annotators uh andrecord how they did Google searches inorder to use that to like very verycomplex stuff we did none of that it wasall super super easyumbut yeah uhuh I'mvery muchin favor of Simple SolutionsI've been usingum gpd4 to write python code for meandit's almost always executable and it italmost always runs and then wheneverit's not correct I'm like hey this itreturned this bug or whatever and thenit just rewrites it for me sometimes ittakes it like 10 tries butumat some point it will return somethingthat's or I think for like 70 of myquestions it ends up returning correctcode soI I'm not sure that right now I see aneed for specificlanguage models for specific apisum I would justinclude like the API in the prompt orthe relevance function calls in theprompt and ask gpt4 to program for mebut yeah if you want to automate thisit's it's not currentlyas I said only like 70 of the time doesit return code that runs so it's notsomething that wouldumyou couldn't put this in production andexpect it to work all the timebut I I'm notI don't currently seeuh the benefit of fine-tuning your ownmodels on your APIwell I think um to speak to a bit to theweeviate cases like um kind of I thinkone of the big things about these Vectordatabases is kind of adding likesymbolic filters like where do you wantto search so like you know if you say umyou know what what what's the latestopinion on Al by attention and whetheryou want to search Twitter or or XRyou're going to search X or if you wantto search archive and then you could youknow tailor the search accordingly maybeyou want to add a filter to that onelike date is greater than July 5th 2023or something like that and then I thinkone other important kind of lever forformulating these queries and the reasonwhy you would want an element that knowshow to control the API is um you know isthis a vector search query or is this anSQL query so I think there are cases Ilove that reflection prompting thinglike where you get the output from thecode executor and then you just keepchaining that and I think that couldalso work with search where um you couldget the search results and say okay do Ineed to re-rank this or should Iactually use like a hybrid searchbetween M25 and dense Vector search andI think they're yeah I think the wholetopic there's quite a lot of uhinteresting things with this but yeah soum kind of another topic is uh you knowat the end of your dissertation you'retalking about new directions for newtraining data and you just mentionedkind of you know I think we're all justwildly impressed by how well it canwrite code uh and I think kind of youknow it's extremely powerful like aswe're talking about this topic of likeautonomous AI I just love how with codeit can run it can come up with whateverexperiment it wants kind of so it's likethis kind of expiration exploitationlike the you know reinforce and learningall that kind of coming into it how doyou see that kind of language modelstrained on code and how that how thatcould change everythingum it seems like code is really criticalfor building strong language ones I meanwe still haven't really hada very good thorough large empiricalevaluation of this but it might be thatthe reason that gpd4 is soum good at tasks that aren't evenprogramming like it's really good atsummarizing like math papers like why isthat happening that might be because itwas trained on a lot of codes codesmight umuh make models smarter we don't know yetbut that's a hypothesis of people arethrowing around I'm justumI know that lots of people uh have beenusing LMS for like you know legal stuffpeople are trying to show that languagemodels are really good at answeringmedical questionsand I think that might be a verybig use case in the futurebut the current thing that I'm mostexcited aboutis code assistanceum because I mean almost everybody thatI know is using AI to help them programright now it's already made such a bigimpactbut there's stillum a lot more to do there so a lot ofthe time gpd4 will program somethingincorrect and I'll show it like you knowthe runtime errors and I'll tell it heyit's not this isn't good this isn't okaylike and we'll be like sorry what aboutthis sorry what about this and likewe'll be in like this infinite Loop ThatNever Ends of just incorrect proposalsum so there's still a lot to do thereit's notum it's not at the place where it canautomaticallyumdo things correctly we're we're far fromthat it I give it a function descriptionsometimesand it hallucinates thingsthat I never asked forumsometimes it's trying to besmart smarter than it needs to be andthen and ends up writing like silly codeumso yeah there's a lot to do thereumwe still have a lot of understanding uhthat needs to be achieved in in ingettinglanguage wants to hallucinate this to bemore consistent but a lot of times so inour paper we showed this in the self-asspaper we showed this silly example oflike you knowGPD free with chain of thoughts sayingsuper connectivity was discovered in1905 the president in 1913 was like thishuge inconsistency like we were askingabout like the president in 1905 why didyou go to 1913 right in a very shortcontext it's not like it had said 1905at the beginning of the page and thenwritten like you know 17 pages and thenit went to 1950 no it was inconsistentwithin like seven wordsand that was a problem with GPT througha year ago but it's still a problemright nowumand so yeah there's stilla lot to be done thereumyeah but I I really like the theprogramming announcement hmmyeah I I guess kind of just sending itin sorry to be kind of coming back tothat other topic but like with theability of the language models to writeapis yeah it's like if as you'refine-tuning it for the API it loses thatability to have like that reflectionlike see the output see why it failedand then kind of revise it accordinglythere's all sorts of stuff going on withlike structured output parsing and uhyou know trying to enforce the outputalmost like a compiler like to to followthe grammar such that it you know youwould only sample tokens that follow theyou know the acceptable Grabber and yeahI think it's all just such aninteresting Direction writing code andyou know the the code interpreter Ithink is just getting so many people'simaginations on what this kind of stuffcan do and you know I love using it tojust like I could just you know grab abunch of data and then I can ask it heycould you parse this out and turn itinto a CSV or visualize it and I thinkall that is just incredibly powerful uhso one other thing is I'm talking aboutthe the potential of video transcriptdata can you tell me a little more aboutuh how you think that's unique from theinternet Text data and the opportunitiesthereum yeah so there's another rumor goingaround that openai train gpd4 onum transcripts of YouTube videosumI think there's a bunch of benefitsthereum that you might not have in likeyour typical language model trainingdataum some YouTube videos like podcasts arereally long and there are conversationsabout the same topic that maybe makereferences to something that was said 30minutes or 40 minutes ago so that'sreally gold for language modeling for along time[Music]all the text we had was like reallylike had short was either incrediblyshort or was slightly longer but itwouldn't make like wouldn't have longterm dependencies and soas we're trying to build language modelsthat canhandle more complex tasks like analyzinga large legal document or taking likefive code files and figuring out how towrite a sixth file as we're trying tomake language models handle these morecomplex tasks we need training data thechallenges our models during training asmuch as possible and it could bethat long uh YouTube videosor long podcast uh transcriptsare a way to geta lot of that training dataum for cheapumyeah you know that's music to my earsbecause my favorite like project is totake these podcast transcriptions andlike you know build up the semanticsearch index with them and you know tryit through Rag and I mean I think it'sso interesting like if you took thesepodcasts like the Wii podcast and youmade that the data set you probably andthen you're retrieving from some otherinformation Source you probably want totag it with the date as well becausemaybe you know some new information somenew perspective is discovered from lateron and if you retrieve that but yeah Ilove it I think they're I love thatangle of where do we find this kind oflong context data and yeah this wholeconversation of fear thank you so much Ithis coverage of Alibi attention showingthe demo of How It's computed and howthis enables extrapolation to longersequence lengths I think this self-assprompting thing is just so powerful Iyou know I I think still kind of thejury's out on how to really maximize itin these kind of Auto gbt this moreautonomous kind of query decompositionand Tool use and then I think theseperspectives on what we can get by moretraining data with code and videotranscripts is also exciting so fearthank you so much for joining thank youso so much Connor Thank you so much", "type": "Video", "name": "Ofir Press on AliBi and Self-Ask - Weaviate Podcast #65!", "path": "", "link": "https://www.youtube.com/watch?v=pOsURBfTThc", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}