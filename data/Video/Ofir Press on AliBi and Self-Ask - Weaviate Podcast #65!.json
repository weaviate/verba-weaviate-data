{"text": "Hey everyone! Thank you so much for watching the Weaviate Podcast! I am SUPER excited to publish my conversation with Ofir ... \nhey everyone thank you so much for watching the wevia podcast I'm super excited to welcome ophir press oh fear just graduated with a PhD from the University of Washington uh studying deep learning and doing all sorts of amazing stuff with long context large language models as well as self-ass prompting and all sorts of interesting details I first became aware of ophira's work from a recommendation from Jonathan Franco you know highlighting this sell fast prompting and ophir was really you know one of the first people to discover this combination of language models and search engines and this kind of tool use like thing so I I was just so impressed by this and I'm so excited to talk to you about fear oh fear thank you so much for joining the weba podcast thank you so much Connor thanks for having me awesome so I think a super interesting topic to start things off would be to get into this long context large language models like what happened how how come large language models can suddenly take in like 32 000 tokens as input yeah so for a long time we were kind of training these 1000 or 2000 token language models and for most of his time language models weren't very good they weren't very useful and we were just our goal was to we were mainly judging models by perplexity and we were kind of looking at them as like an academic question they weren't very useful but over the past two years they've suddenly become good at some tasks and now they're becoming useful and that's leaving the lab and it's starting to enter production and one of the first constraints that are one of the biggest you know constraints that people run into is that they want to generate a really long codes file or they want to read a really long book chapter or they want to summarize like you know a bunch of newspaper articles and with most models you're gonna hit that token limit um pretty quickly and so um getting models to handle longer sequences um became like a really important goal for the entire community um training on Long sequences is really really tough because it requires a lot of memory and at some point you're gonna run out of GPU memory and it also requires probably training on logger documents but most of the documents that we train on are crawled from the web and most web documents are very short they're like a few hundred tokens long and so for the most part even today we're doing most of training like 90 of training and very short you know 100 token internet or multi-hundred token but sub 1000 token documents crawled from the internet and then we fine-tune for just a tiny bit of training maybe one percent five percent ten percent of training we fine-tune and longer documents um and there's a few issues there that um people should understand uh when thinking about building these models that can handle long sequences so I think this all begins by understanding position embeddings so we can talk a bit about that um if you want to go there um so yeah when the original Transformer model was invented in 2017 they kind of gave you two options for the position of biddings which is kind of interesting because everything else in the model it's like you know you got to do this you gotta do that you gotta do this but for the position embeddings um you had two options you had absolute position embeddings and you had sinusoidal position embeddings and um so let's just briefly describe both of them so um when you feed in words to a Transformer model they were like we don't want to just feed in words because then it might not know the order of these words um if I tell it you know um the table is blue how will it know to differentiate between that actually let's think of um uh a sentence where the word order matters um what was like the disambiguation like uh the boy was running outside he went to get ice cream I think that's a great one where it's like he is in reference to the boy from earlier kind of um no but but there's even better examples like um uh of fear is teaching Conor so word order really matters there because I'm trying to say a little fear is teaching Conor if we if we flip Conor and I feel that's a different Conor's teaching of fear is a different perfect sentence so for that so this we really need to know what word is first or where the second third and fourth and so on and so the uh people who invented the Transformer model were like when we input were to this model we also we don't want the model to just know what words are in the sense that I'm feeding in we also wanted to know what order they're coming which is a very natural human uh very like intuitive thing when you're telling somebody words you're not just giving them a set of words they're also giving them in a specific order clear that that matters later like this was two years ago with a bunch of other people and me we showed that you actually don't need position on banks we could get into that later but at the time they thought that position embeddings were necessary um and so they were like okay how do we um tell the model the order we all didn't know how to tell it what words we're giving it we're just going to give it word vectors but now we also need to give it positioning information and so they were like okay two here's the two options that we thought up of the first one is called absolute position embeddings just like we're learning word embeddings for every word right so we have a word representation for cat a word representation for Connor or whatever position for teaching we're also going to learn position representations so every time we input a word we're actually gonna it's actually we're actually going to be inputting two things we're going to be inputting a word representation and we're going to add to that a position representation so in the sentence of fear is teaching Conor we're going to first input the word of fear with a position representation for position one and then is with the position or position two teaching of the position representation for position three and so on and just like our word embeddings start out with a random initialization right so word embeddings when we're done training they kind of have meaning right the word embedding for teaching is close to the word I'm putting for taught the word of betting for tables goes to the way I'm running for a chair but when they start out they're totally random they're just in random places in in space so the same thing is true for uh position embeddings absolute position abundance they start out randomly initialized but as training goes on the model learns certain properties for position one it learns certain properties for position two for position three and so on it's probably putting position one very close in space to position two and three but very far away from position 500 but 500 is very close to 499 498. so that was their first kind of thing that they invented to add positionality and it totally worked um so this works put in the paper in the original paper they talk about this there's an issue with this what if I train my new Transformer model on a thousand tokens so I crawl the web I have lots of documents I concatenate all of them up into this really long you know billion token string and now I'm when I train I can't train on this entire billion uh did they say billion dollar billion token uh um uh string I can't train the entire string at once because that one fit into memory so what I do is I chunk it into chunks of a thousand words I'm training on the first a thousand words I'm a second a thousand words on that um and so in the original paper they talk about how this position embedding system might not work if I want to extrapolate to longer sequences if I want to train on a thousand tokens but then process like you know write a thousand one tokens of code or understand a New York Times article that's a thousand one tokens they were like if we do these absolute embeddings and then we try to input a thousand in one tokens we're not going to have a position embedding for word a thousand and one and they recognize that this is going to be an issue and so they were like okay uh let's have another thing and that's uh the sinusoidal position of bangs so our issue with absolute position of bettings was that we had a position embedding for each word that was trained and then when we wanted to look at more words than we saw uh during training we just didn't have a position of anything and so they saw this by having a non-parametric function so that's just a fancy word for saying that we're going to have a function now that has no learned um parameters it's a it's like a deterministic like you can write it up in Python there's nothing learned in there and that function it takes a number in int and it returns a vector such that um positions into that are close by get mapped to vectors that are close by in Vector space that's it so we're not going to get into all the math behind how they wrote this function um because it's not super necessary to understand intuition here what's needed is just to understand that they have this function and you give it like you say hey what's the vector for position 15 and it's like here it is what's effective for position 16. and the promise to you is that the vectors that they return are going to be close in space for positions that are closed up so those are the sinusoidal position embeddings and they also worked in the original Transformer paper um and um yeah that's it so the Transformer paper in the original paper they actually did not look into figuring out whether it the sinusoidal position embeddings enable extrapolation in practice they enable it in theory because before right we were training a thousand tokens and then we would want to feed token a thousand and one but we didn't have one because we didn't train it with the absolute position of weddings but now with sinusoidal um we don't need to train position a thousand one we just ask for it and we get it so they were like this might enable extrapolation but they never actually tested it in practice yeah so that's position embeddings is that is that does that make sense yeah super interesting and I think um so maybe jumping ahead to The Alibi attention and the way I understand you know watching your video is that the the way is you just kind of do you know okay the the query times the key minus three minus two minus one to kind of do it back but I guess kind of as we're talking about this positional embedding stuff I'm still kind of missing the piece of my knowledge that understands um how the kind of quadratic attention plays into this is like is this is Alibi this kind of position embedding coupled with some kind of new uh attention operation like what happened to that quadratics you know N squared attention that meant that you couldn't have long uh inputs right that's a great question um so let me piece that back into the the original story so when the original Transformer was introduced they talked about how sinusoidal position embeddings might enable extrapolation in Alibi we show that they actually don't in practice enable extrapolation um and and now I'll talk about the quadratic thing so yeah for a long time a lot of people were talking about how a major problem with attention is that um it takes quadratic time in the length of the input sequence but that's not actually a problem in practice if you look at like a seven billion in very small models like less than a billion parameters attention might be taking up a large chunk of your feeds forward computation but um if you benchmark any model that's larger than uh like five billion parameters you'll see that um the attention computation is actually an insignificant part of the total feed forward time and I think that in very large like 70 plus billion models the attention the total time spent on attention is less than one percent of the total feeds forward so I know that it looks kind of scary because theoretically attention is a N squared operation but um that's like a theoretical thing but the most important thing to do when building Transformers is to actually look at wall clock time and to Benchmark the real models and um it's super important because something that's theoretically o of one could end up being a lot of time and something that's theoretically oh event squared could end up being very quick and that's what happens with uh large Transformer language models um as I said in the 70 billion plus program range only one percent or less don't query of this but it's around one percent of the total feeds forward time it's spent on attention most of it is spent on the feed forward layers and so that wasn't an issue um I know that there were a bunch of papers that tried tackling this issue by building like linear um uh Transformer models but um yeah they in my view these papers we're trying to solve a problem that didn't really exist and um none of these models actually achieved uh good empirical uh performance and so I've never needed to to use any of those types of strategies and I've never felt that those were um necessary so yeah so Alibi um uh is just a position embedding method we still use the um normal attention mechanism the the squared attention mechanism um yeah and you can ask some more questions and then they can open my slides and show you uh like a brief overview of alibi oh yeah sorry if this is a dumb question but so I thought the problem was when you're doing that query key multiplication like you just can't fit that on a GPU when it's like you know 32k inputs with the embedding Dimension and you end up just having this gigantic Matrix I thought that was kind of what stopped you from you know scaling the inputs as big as you like um it does become an issue at some point so it's not this is definitely not a dumb question but it does become an issue at some point um especially now when people are thinking about doing like you know looking at a hundred thousand um tokens at once it's going to become an issue so so now this might be um a research direction that will receive renewed interest but all the way up to like eight thousand or sixteen thousand tokens you can definitely fit that um on a GPU what can um but can you also just kind of like similar to the gradient checkpointing and like ragnets like can you just kind of like you know cut up the keys and then multiply the query by this set of keys kind of exactly it's it's it's gonna be like really it's gonna make things slower obviously but you don't have to look at all the keys and queries at once you could just if you have 10 000 queries and ten thousand Keys you could uh break down this computation you could compute every query by itself um so yeah you could definitely do that it's just going to be slower but yeah if you have a really if you need to attend over a really large input you could definitely break it down yeah really cool so maybe we could hop into the slides now and um yeah let's do it as Opie are setting up with this I'd say to our web podcast listeners we're trying a new recording software and so if you like this kind of like half podcast half like show something visual you know please let us know in the comments and and then we'll stick with this kind of format but yeah it's super cool I think this will be fun to kind of you know go between conversation and maybe looking at something because I do think a lot of conversation topics especially like as we're talking about positional embeddings maybe kind of like seeing something might help help the conversation click maybe not uh great for like you know people who like to listen to pot I think like hopefully the commentary over the slides also will still make this work even if you're like you know listening to this at the gym or something where you don't really like the visual component of the podcast but yeah it's really interesting yeah I'll try to um describe as much of this uh in words um so that anybody's just listening you won't miss out too much uh anyways uh here's how you implement alibi um so it's one easy step but Step Zero is to comment out the sign of solar position embedding so yeah as I said Alibi it's just like a drop in replacement right there's like uh the absolute and Senator position of Brothers there's now rotary position embeddings so Alibi is just a drop in replacement for any of these um so you comment out let's noticeable position embeddings and then the one step that is required to implement alibi is to add a bias to the attention scores um so I just have some intuition here but this is very obvious too all the humans that are listening to the podcast um if I have a sentence here like the quick brown fox jumped over um and the last word is over right and I'm trying to generate the next word so over is my query and all the other words are my keys and I'm attending to all of these uh previous words to try and generate the next word and as humans we know that when we're reading a bunch of text the words that matter most to the current context the words that matter most to my prediction of what the next word will be other words that are nearby and as a as a word is further away it matters less and less and less and so what Alba is going to try to do is to just encode this intuition that all of us have into a Transformer language model so instead of telling the model hey this is word one this is where two this is worthy we're not going to do that anymore we're not telling it which word is one which word is two which were to see we're just for every context we're telling it hey this word matters a lot this word matters just a tiny bit less this word matters uh even more or less but and then like you know the super further away words don't really matter so that's what we're going to do with Alibi um and here's how we do it so as I said over is the last word so that's a query and then all the other words are our keys right so we have all these dots products that's how we compute attention at this current time step and this is normal attention right so we compute the dot product between over and over over and jumped over in Fox over in Brown and Alibi is going to just gently change this by adding or Sorry by subtracting a bias from each of these dot products and the spias is going to linearly grow as we get further away so over and over uh once as a key in which is a query that's the same word so there's zero bias and then we're going to start going further away over and jump that's one word away we're going to subtract one over in Fox we're going to subtract two and so on and this is the intuition behind Alibi but in reality we actually multiply this constant um by slope it's called M here and this is a a number that um is not learned it's just a um a set float um yeah that's it so that's what happens in Alibi and like the only there's just one more small technical detail when you do a tension we typically do multi-headed attention right you don't do attention just once at every time Step at every layer you do it multiple times right Transformers usually have dozens of heads and so Alibi uses a different M value for each head um uh so if you have eight heads these M values are going to be half a fourth and so on it's just going to be this sequence for 16 heads it's these values there's just a method in The Alibi repo that tells you which M values to use and what this does is that when every head attends backwards to the previous words it biases previous context in um varyingly uh decreasing weight so some heads are going to um have large biases which is going to mean that they're going to only look very closely nearby and not really attend to words that are far away and some hats have very very small M values which means that they will look really far away before the bias becomes really big and we stop looking yeah so that's Alba oh it's so interesting I was going to ask if this had something to do with that kind of like lost in the middle paper because it seems like um you know when before you explain the multi-head attention how you'd have different values for the M I was thinking like well this is really biasing it towards the most recent uh tokens and then that would really probably impact how you think about the kind of retrieval augmented generation and how you pack the prompt and sort of like yeah yeah so um there's a bunch of things here so first of all um yeah at first this kind of looks weird because it seems like we're forcing the model to only look really closely nearby and not really look far away but we've benchmarked this against sinusoidal and you know rotary and absolute position embeddings many many times over and every single time we find that it beats or matches all these other methods so even though it looks as though we're kind of biasing to only look nearby we're actually we're permitting we're letting it look everywhere and we're just using this bias as a way to inject positional understanding into the model kind of give these positionality hints without really forcing the model to just look nearby it can look wherever it wants it just has some heads that are kind of have a harder time looking further away but some heads can they have this the spice is incredibly slow over there and so they can look far away and so while at first it might seem like this model can't really look far away it can um I think that we should look at this penalty of spies more of a kind of a hint of oh this word is further away you can look at it but just FYI it's kind of far so yeah so there's no you don't lose anything and I mean the benchmarks speak for themselves uh it works and yeah so that's for Alibi and then of course language models as a whole no matter what position embedding you use have a very strong recency bias and this is probably due to many many things but um one of the reasons that this happens is because language has a very very strong recency bias right you're training on all these documents from the Internet or even if it's books or whatever you're training on most of the time in order to predict one word into the future you only need to look at the past five or six or seven words and so no matter what position of betting you're going to use your language model is going to have an incredibly strong recency bias right and that's going to make it a bit tougher if during inference you're suddenly giving it like you know five papers from archive and asking it a question that involves kind of querying all five of them that's that's going to be tough because most of the time during training it was like you know completing like you know 200 words um like news briefs from like Reuters or something it was doing a totally different and much much simpler task so that's an issue um that's a very big issue but that doesn't necessarily come from the position of betting that you use it's mostly I think because of um of how the language is yeah that inspires me to ask you this question so I'm very curious about this idea of retrieval aware training I've seen like you know Mosaic MPT 30 billion describes this staged training where they first train with one k length uh sequences and then you know 4K for like say the you know the first 8 000 steps of training are with 1K and then the next 2K for that but so like this kind of retrieval aware training where you where you're training a model that takes like 32k 100k length inputs so you kind of would retrieve from your you know Vector database hopefully and uh and uh and do this during training to have this model so it doesn't have this you know I'm only completing 200 word uh Reuters titles as you mentioned but it's seen this kind of thing of having just an enormous context window right so so how important do you think retrieval is going to be kind of integrated into the training of large English models yeah there's like a few different things that you touched on here that are kind of distinct so let me break this down piece by piece so um so stage training is a name that we event invented in um a paper we published three years ago called short former for a process that we did not invent but we I think we were the first to kind of uh explore empirically and kind of show uh that it really works um so stage training to my knowledge was first done by Bert and um when we use the term stage training we mean that you train on short sequences for a bunch of time and then you train on longer sequences um for a minor amount of the time so for example Bert is a model that can handle 512 token sequences but if you read the paper you'll notice that they trained on 128 token sequences for 90 of training and then they did the last 10 of training with 512 tokens um and um they did that and then nobody else really did that and then when we wrote uh shorts former we noticed that training and long sequences at that time with the data that we had then and the smaller models that we had then we noticed that like even though people were training on like 2 000 or 3000 token long sequences at the time we noticed that it wasn't really beneficial and so we coined this term State training and we really explored it and we showed that you can actually train on uh very short sequences even shorter than 128 tokens uh for a lot of time and then just do the last bit of training on like a thousand or two thousand tokens and you get results that are just as good um as training on really long sequences for all for the entire duration so that's stage training so stage training um isn't um necessarily A retrieval augmented thing State training just means hey I'm going to train on really short sequences and then I'll train on on longer ones um and that worked really well for Bert in I think it was 2018 okay or 2017. um it worked really well oh no but it was 2018. it worked really well for us when we wrote shorts former uh in 2019 and then with Zeke recently did this with one of their MPT models where if I remember correctly they traded on like 2 000 tokens for a really really long time and they fine-tuned it or they did the the last bit of training on 65 000 token long sequences so you can fit a 65 000 token long sequence in a gpu's memory but it makes training really really slow and so if they would have done that on for like you know 100 of training that would just have been probably too expensive way too slow um and so uh because a lot of the things you need to learn as a language model are really in the short term a lot of learning is just in the in a hundred Tokens The 100 previous tokens so there's no real reason to train on 65 000 tokens for the uh entire duration of training so it was like this really smart thing where they just trained on I think it was 2000 uh token sequences or two thousand token long sequences for a life percentage of the time and then they trained um on the super long you know 65 000 token books or whatever it was and I think that's a super smart strategy so it's still stage training still works today um so that's stage training and now separate from that uh is the whole retrieval retrieval augmented models and then you can I guess combine with them but let's talk separately about retrieval um augmented models yeah I think that going forward a lot of the things that we want to do with language models will have to use some kind of retrieval augmentation if you want to do like you know co-pilot style like you know code assist um for really large repos you're not going to be able to fit an entire repo into a language models context um if you want to do like medical search you're not gonna fit every medical paper that's been published in your language models um context window not even in two or three years maybe in 10 years you will but right now it doesn't it's not coming anytime soon and so um retrieval augmentation is just a very clear very simple uh way to kind of get the best of both worlds both have access to you know lots of content but also use the power of language models so I definitely believe in retrieval augmented systems um and then the question arises of should we do this during training or should we just do this during inference and as you said if you just do it during inference you have a model that was trained in relatively shorter documents and that during inference you ask it to uh look at like you know five archive papers it's going to have a tough time I think the solution for that is to do retrieval augmented fine tuning or to um kind of do the last bit of training with some kind of um uh log documents uh that are put into the language models context I I haven't really seen I haven't been convinced that training uh with retrieval for the entire duration of training okay something that's necessary but um yeah I think that's something that people are doing research on right now and it's a really interesting subject yeah that makes a ton of sense to me that like you you want to build up kind of representations with the shorter sequences I imagine the throughput of getting each you know training bash into the GPU with shorter samples is easier as you mentioned maybe like breaking up the key multiplications so that brings me into into your next really famous work that I'm so excited to talk about is this self-ask prompting because it's such an interesting part of retrieval where if I ask you a question like I think the canonical example is did Aristotle use a laptop and like you break this question into the two questions to facilitate the retrieval so I'm just so so curious what your current thinking on self-ass prompting as well as kind of what inspired you to originally uh go down that route yeah um I was just playing around with gpt3 this was written a bit more than a year oh actually a year ago um so this is before Chachi BT before gpt4 I was playing around with it and I started trying to figure out if I could trick it and so I started asking questions like um when did Israel land on the Moon um which uh Canadian um was the first person to cl to explore Everest like all kinds of questions that are incorrect wait what did I say oh first person yeah he wasn't Canadian right um and it would get them wrong I mean since then people have put these types of questions on Twitter it's like a funny meme now I like this type of stuff but like people have asked it um I can't remember who this was I'm sorry but somebody asked it when was the Golden Gate Bridge moved across Egypt for the second time and gpt's three is like oh of course this happened on like you know March 17 1984 whatever I don't know it's like it really um and yeah for all those questions that I gave like if I remember when I asked gpt3 like you know when did Israel first sound of the Moon it would be like you know June 10th 1992 right and then I was like okay how do I solve this like there's no way that this can't be stopped like with just purely the model no retrieval no Googling no nothing like just just the model and I pretty quickly came up with a solution so in the world of gpt3 we weren't doing a lot of zero shots prompting where you just ask it a question we were doing um more like you know single shot or few shots prompting where you would have a few questions you would write your own answers and then you would ask gpt3 like you know a new question that you wanted an answer today um so what I did to to um to fix this issue with a false questions is um I was like from now on whenever I ask you a question let's break it down into Two Steps step one try to understand if the question is truthful and then only if it is truthful um then answer it right so in the example questions I would have like a question like um when did Spain first acquire nuclear weapons so I would have a question when did Spain acquire nuclear weapons and then I would be like is this question correct question mark and then colon oh and then I'd be like no you know Spain never has acquired nuclear weapons and then and then I would have answer colon no answer I'm not answering this question like it doesn't make sense right so that's in the example that I'm giving gpd3 and then I would ask it a new question when did Israel land on the moon and then it would ask itself has Israel ever landed on the moon no therefore I'm not answering this question mm-hmm and yeah this work that managed to now all these questions that it was you know when was the Golden Gate Bridge moved across Egypt it would ask yourself has the golden great bridge ever been moved across Egypt no and then um it fixed this this issue and I was really really cool to me that just by prompting you could fix these significant issues and I think my main lesson from there is that I mean I've known this for a really long time and everybody should know this language models are kind of stupid or all machine learning models are not they're kind of good at doing what they were trained on and they're not great at doing things that are super out of distribution and so whenever I'm prompt engineering the thing that I'm thinking most about is how can I break this down if I have a task that a language law has never seen before the question that I ask myself is how can I break this down into subtasks such that each subtask is as close as possible to something that it has seen before so asking it questions with false presuppositions is out of distribution because most of the time on the internet when people ask questions they're not asking these absurd silly questions they're asking truthful questions and so I think that's why it's kinda went crazy and started like giving dates to all these like fictional events that never happened it's because most of the time when somebody asks a question the output distribution is a date like you're not going to start arguing like most of the questions I mentioned are totally legitimate and it's not very common that a model observes somebody's saying oh actually that question is totally wrong I'm not gonna act like and so the thing that solved it was to to break it down into these two steps of is this question even uh real and then answering it and so that took me that got me to self ask so in self ask the first thing that we kind of show is that language models struggle with um multi-hop questions multi-hop questions are questions that require retrieving multiple pieces of evidence to answer for example um when was the founder of Craigslist born right you first have to figure out who the founder of Craigslist was and then you have to um figure out when they were born and we notice that language models um really struggled with those types of questions uh who was president of the United States when Billy eilish was born they got that wrong and when you look into it you can ask language while you're so I asked gpt3 who was the president when Billy Ash was born and it gets it wrong and then I can ask it um when was Billy Alex born it gets that right and then I I take that data and I'm like who was president on that date and it gets that right too so even though it has the facts that are needed to answer that's compositional question it has some kind of reasoning bug that doesn't really allow it to um to understand compositional questions sometimes and so what we did was we took this thinking of um uh breaking down complex things into simpler things and so um The Prompt for self-ask basically says hey when you get a compositional question don't try to answer it just try to break it down into what is the first sub question that I need um to answer in order to get to the final answer so the first question in the biliate case is um when was Billy eilish born and so I'm like okay just write the first sub question great now what's the answer to that it answers that and it's I'm like okay what's what's the second question that you need to um yeah so who was president on that date and then what's the answer to that George Bush or whatever so the finances George Bush so we just have uh a prompt with a few examples of me doing uh this process the self-ass process with a bunch of um uh example questions and then you give it your own question and it automatically decomposes it into sub questions answers to them and then answers a full question um and it does it better than Chain of Thought So yeah I'll let you ask some questions but I have a bit more to say after that well yeah I'd love to just keep rolling into the um Chain of Thought I mean I love it's kind of a really great example of kind of still making the case for a few shot examples this kind of you know giving it examples of when a question is unanswerable or you know and and generally I love this kind of question decomposition I think it makes a ton of sense for you know all sorts of questions and and you know as people are I think most commonly people are building like chat with your documentation all this kind of stuff and you can expect these kind of compositional questions where decomposing the facts is extremely useful I guess yeah I I mean the the Chain of Thought it sounds like you got you have something yeah so so um we talk a bunch about Chain of Thought in our paper Chain of Thought is super super cool but um self-ask improves upon it um so in the paper we show that Chain of Thought sometimes kind of loses its train of thought um so we have this example in the paper and I'm not going to remember it exactly right but uh it's like who was president of the United States um when super connectivity was discovered and we'll get into retrieval and that type of stuff later but right now we're just talking about pure you know there's just a question in a language model there's no retrieval here whenever I say the word retrieval I mean the language was kind of retrieving facts from within its memory or whatever we're not actually retrieving anything from like a web so you have a language model and you have a question and you ask it uh who was president of the United States when super connectivity was discovered in in the first uh exactly we're going to use Chain of Thought right so we have an example question with an example answer that uses Chain of Thought and that means that channel thought just means that you're like a human you're kind of saying well uh super connectivity was discovered in 1905 the president of the United States then was Woodrow Wilson or whatever so the financial ounces would you Wilson um but what happens is that chain of thoughts others forgets what it's trying to do so for this specific example with gpd3 it said something like super connectivity was discovered in 1905 which is correct and then it said the president of the United States in 1913 was Woodrow Wilson so the final answer or it said some it said the correct president for 1913 but then it said that was the final answer do you understand so it knew that superconductivity was uh discovered 1905. it did tell us who the president of the answers was in 1930 but that's wrong like it was the president of 1905 was a different person and so it got that wrong it kind of it went off track and so what we've learned from self ask is two things is first of all always try to decompose complex queries it doesn't even have to be a question If you're trying to build a complex SQL query using gpd3 maybe try to First have it build sub queries and then combine it if you're trying to write a complex story or marketing text or whatever maybe try to have it first right like you know the the you know one perspective of it a second perspective then combine them so breaking complex things down into easier subtask is super important but then the second thing that we learned is that Chain of Thought can sometimes be problematic because it forgets where it's going and in general when you're generating a lot of text with a language model it can sometimes kind of lose track of what it's trying to do and so the second thing that um we we think is really useful in self-ask is that it's constantly reminding the model of what its current sub goal is so self-ask gets this uh question right because self-ass says okay when was super connectivity discovered and lamas's answer 1905 and then it says who was the president of the United States in 1905 and the Moss is the president of the United States 1905 was blah blah so the answer is and it gets it right so by constantly reminding yourself of what your current sub goal is um we get the language model to be on track more and so we show in the paper that our method um ah does better than Chain of Thought yeah I think we all need that right like what am I doing again a reminder of my major goals and uh yeah I mean the whole thing it inspires my interest I mean this like Auto gbt this kind of more autonomous AI it it you know it sounds pretty natural but that's the next step I think there are just so many interesting things you can kind of extrapolate about autonomous AI like you can imagine like uh an llm that wants to train like a new LM and so it goes and it gives training data for this new llm or I think maybe just the idea of like having it make pull requests to code or like you know research tasks that run overnight or running an experiment and just kind of waiting for the experiments to be done I think there's so much to that kind of like autonomous you know evolution of this and I guess kind of um I hope this isn't too off topic but I wanted to get your opinion on these Guerrilla large language models as well which which is like a large language model that's been uh specifically trained for a particular set of apis and so kind of the thinking with this self-ass prompting and this query decomposer kind of concept is like if I need to uh split my question into maybe a complex query or if I need to split it into like you know spinning up or like using the GitHub CLI to format a pull request these kind of specialized llms in the middle seem to make a lot of sense and it's almost like uh API usage as a as a task like instead of that kind of like open AI functions thing where you output like a Json that has the arguments of the function you instead have these intermediate llms specialized to formatting API requests so I'm just curious if you've seen this paper and if you you know if you think that this is an important Direction going forward yeah I haven't seeing that specific thing but I can talk about my view about integrating language models with external tools so in self ask um we had this thing where we integrated it with Google search and we weren't the first paper at all to integrate language models with search Google had a paper where they integrated language models of search but a lot or all the previous work had to do a lot of very complicated like reinforcement learning fine-tuning for the language models in order to get their language models to understand how to operate a search engine API and in self-ask we showed this really simple thing I don't know how we were the first to do this but we were the first to show that you don't need to train your language model to follow a search API in order to understand how to use web search you can just kind of trick it into using a search engine and so are prompt is very structured it's always like you get an input question and then it's like okay sub question one who was president of the United States in 1905 sub answer one blah blah blah sub question two and so what we did was we used the same self-ask method and prompt but this time uh so this system is called uh self-askplus search engine software's Google search so we have a question like who uh you know uh what was my question that I liked um what is the current time in the country that's hosting the World Cup right so if you ask GPT 4 gpd3 this question they have no chance of knowing this because I think they were trained before the World Cup happened and they definitely don't know so they don't know where the World Cup is happening and then they definitely don't know what time it is right now in Qatar or whatever right they don't have any real-time data and so we did this before gpd4 and plugins and web scraping um and it's still simpler than those approaches but what we did was uh we at we gave this question to gpd3 with a self-asking prompt and it was like okay the first question sub question I want to ask is um what country is currently hosting the World Cup right and that's like answer colon and then we stop it so it's about to answer itself we stop it we go to Google we extract the first question that it asks because it's really easy to extract because it literally says you know sub question one colon so super easy to parse out we send it over to Google we used serp API um and then Google gives you back an answer and then we just take that and we put that into the language model prompt and we're like here's the answer so the language model thinks that it asks itself and it answered itself even though it was Google but it doesn't know that it doesn't need to know that and that's like okay uh sub question two what is the time in Qatar and we're like ah wait a second we pause it we take that question parse it out send it to Google Google tells us the answer we put that back into the prompt and it says okay the final answer is you know 7 pm um so yeah we did this like a year ago and it's super simple and it's really really useful and it's super powerful and a lot of the previous approaches had the fine-tune language models I think one of the previous papers for some reason they needed like humans to like they had to kind of um use like human annotators uh and record how they did Google searches in order to use that to like very very complex stuff we did none of that it was all super super easy um but yeah uh uh I'm very much in favor of Simple Solutions I've been using um gpd4 to write python code for me and it's almost always executable and it it almost always runs and then whenever it's not correct I'm like hey this it returned this bug or whatever and then it just rewrites it for me sometimes it takes it like 10 tries but um at some point it will return something that's or I think for like 70 of my questions it ends up returning correct code so I I'm not sure that right now I see a need for specific language models for specific apis um I would just include like the API in the prompt or the relevance function calls in the prompt and ask gpt4 to program for me but yeah if you want to automate this it's it's not currently as I said only like 70 of the time does it return code that runs so it's not something that would um you couldn't put this in production and expect it to work all the time but I I'm not I don't currently see uh the benefit of fine-tuning your own models on your API well I think um to speak to a bit to the weeviate cases like um kind of I think one of the big things about these Vector databases is kind of adding like symbolic filters like where do you want to search so like you know if you say um you know what what what's the latest opinion on Al by attention and whether you want to search Twitter or or XR you're going to search X or if you want to search archive and then you could you know tailor the search accordingly maybe you want to add a filter to that one like date is greater than July 5th 2023 or something like that and then I think one other important kind of lever for formulating these queries and the reason why you would want an element that knows how to control the API is um you know is this a vector search query or is this an SQL query so I think there are cases I love that reflection prompting thing like where you get the output from the code executor and then you just keep chaining that and I think that could also work with search where um you could get the search results and say okay do I need to re-rank this or should I actually use like a hybrid search between M25 and dense Vector search and I think they're yeah I think the whole topic there's quite a lot of uh interesting things with this but yeah so um kind of another topic is uh you know at the end of your dissertation you're talking about new directions for new training data and you just mentioned kind of you know I think we're all just wildly impressed by how well it can write code uh and I think kind of you know it's extremely powerful like as we're talking about this topic of like autonomous AI I just love how with code it can run it can come up with whatever experiment it wants kind of so it's like this kind of expiration exploitation like the you know reinforce and learning all that kind of coming into it how do you see that kind of language models trained on code and how that how that could change everything um it seems like code is really critical for building strong language ones I mean we still haven't really had a very good thorough large empirical evaluation of this but it might be that the reason that gpd4 is so um good at tasks that aren't even programming like it's really good at summarizing like math papers like why is that happening that might be because it was trained on a lot of codes codes might um uh make models smarter we don't know yet but that's a hypothesis of people are throwing around I'm just um I know that lots of people uh have been using LMS for like you know legal stuff people are trying to show that language models are really good at answering medical questions and I think that might be a very big use case in the future but the current thing that I'm most excited about is code assistance um because I mean almost everybody that I know is using AI to help them program right now it's already made such a big impact but there's still um a lot more to do there so a lot of the time gpd4 will program something incorrect and I'll show it like you know the runtime errors and I'll tell it hey it's not this isn't good this isn't okay like and we'll be like sorry what about this sorry what about this and like we'll be in like this infinite Loop That Never Ends of just incorrect proposals um so there's still a lot to do there it's not um it's not at the place where it can automatically um do things correctly we're we're far from that it I give it a function description sometimes and it hallucinates things that I never asked for um sometimes it's trying to be smart smarter than it needs to be and then and ends up writing like silly code um so yeah there's a lot to do there um we still have a lot of understanding uh that needs to be achieved in in in getting language wants to hallucinate this to be more consistent but a lot of times so in our paper we showed this in the self-ass paper we showed this silly example of like you know GPD free with chain of thoughts saying super connectivity was discovered in 1905 the president in 1913 was like this huge inconsistency like we were asking about like the president in 1905 why did you go to 1913 right in a very short context it's not like it had said 1905 at the beginning of the page and then written like you know 17 pages and then it went to 1950 no it was inconsistent within like seven words and that was a problem with GPT through a year ago but it's still a problem right now um and so yeah there's still a lot to be done there um yeah but I I really like the the programming announcement hmm yeah I I guess kind of just sending it in sorry to be kind of coming back to that other topic but like with the ability of the language models to write apis yeah it's like if as you're fine-tuning it for the API it loses that ability to have like that reflection like see the output see why it failed and then kind of revise it accordingly there's all sorts of stuff going on with like structured output parsing and uh you know trying to enforce the output almost like a compiler like to to follow the grammar such that it you know you would only sample tokens that follow the you know the acceptable Grabber and yeah I think it's all just such an interesting Direction writing code and you know the the code interpreter I think is just getting so many people's imaginations on what this kind of stuff can do and you know I love using it to just like I could just you know grab a bunch of data and then I can ask it hey could you parse this out and turn it into a CSV or visualize it and I think all that is just incredibly powerful uh so one other thing is I'm talking about the the potential of video transcript data can you tell me a little more about uh how you think that's unique from the internet Text data and the opportunities there um yeah so there's another rumor going around that openai train gpd4 on um transcripts of YouTube videos um I think there's a bunch of benefits there um that you might not have in like your typical language model training data um some YouTube videos like podcasts are really long and there are conversations about the same topic that maybe make references to something that was said 30 minutes or 40 minutes ago so that's really gold for language modeling for a long time [Music] all the text we had was like really like had short was either incredibly short or was slightly longer but it wouldn't make like wouldn't have long term dependencies and so as we're trying to build language models that can handle more complex tasks like analyzing a large legal document or taking like five code files and figuring out how to write a sixth file as we're trying to make language models handle these more complex tasks we need training data the challenges our models during training as much as possible and it could be that long uh YouTube videos or long podcast uh transcripts are a way to get a lot of that training data um for cheap um yeah you know that's music to my ears because my favorite like project is to take these podcast transcriptions and like you know build up the semantic search index with them and you know try it through Rag and I mean I think it's so interesting like if you took these podcasts like the Wii podcast and you made that the data set you probably and then you're retrieving from some other information Source you probably want to tag it with the date as well because maybe you know some new information some new perspective is discovered from later on and if you retrieve that but yeah I love it I think they're I love that angle of where do we find this kind of long context data and yeah this whole conversation of fear thank you so much I this coverage of Alibi attention showing the demo of How It's computed and how this enables extrapolation to longer sequence lengths I think this self-ass prompting thing is just so powerful I you know I I think still kind of the jury's out on how to really maximize it in these kind of Auto gbt this more autonomous kind of query decomposition and Tool use and then I think these perspectives on what we can get by more training data with code and video transcripts is also exciting so fear thank you so much for joining thank you so so much Connor Thank you so much ", "type": "Video", "name": "Ofir Press on AliBi and Self-Ask - Weaviate Podcast #65!", "path": "", "link": "https://www.youtube.com/watch?v=pOsURBfTThc", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}