{"text": "Hey everyone! I'm super excited to present a paper summary of HNSW-FINGER! HNSW-FINGER presents a clever technique to ... \nhey everyone thank you so much forwatching this paper summary video of hswfinger finger is short for fastinference for graph based approximatenearest neighbor search the authorspresent a super clever technique thatextends the hnsw algorithm perfectly byapproximating distance calculations sowe're going to approximate thesedistance calculations by Computing a lotof values offline using these coolalgorithms like singular valuedecomposition all sorts of details thatwe'll explore in this paper summaryvideo but to give more of a tldr hnswfinger trades off additional memory forfaster Vector search so when using thisalgorithm that's the trade-off basicallyso we'll get into exactly how muchmemory and exactly how much of a speedup you get in terms of the evaluationyou're especially going to see thebenefit of hsw finger when you'redealing with you know higher dimensionalvectors so they find the biggestImprovement on just 960 dimensionalvectors but you know say just at thetime of recording this the open AI Ada 2embeddings are super commonly used andthey are 1500 dimensional vectors so youcan expect a huge speed up on usinghigher dimensional vectors like thatwith an algorithm like this and all thatwill be evident as we step into the mathso I had so much fun reading this paperand I hope this video helps you you knowunderstand the math as well it reallyhelped my intuition on approxine yearsneighbor search as we look into the likethe hsw stop condition and all this coolstuff so let's dive into it thanks somuch for watching okay so here's a quickoverview of the algorithm in two minutesjust in case you want to get the highlevel concept behind it so the corevalue out of vector databases isapproximate nearest neighbor search wetake a query we turn it into a vectorand we want to find the most similarVector to our query in our database ofvectorized documents we don't want tohave to compare the distance between ourquery and every single Vector in ourdatabase so we build up these datastructures to facilitate approximatenearest neighbor search so shown on theleft is the hnsw hierarchical proximitygraph to Route these distancecalculations such that instead of say wehave you know a billion vectors in ourdatabase instead of doing a billiondistance calculations we do say you knowten thousand or so or however many butso that's the core idea as we build upthese data structures So within hnsw wehave the search layer algorithm for howwe're going to Traverse the graph and wehave this upper bound between thedistance of the candidate and then theneighbor that we're exploring and so weuse those distances for how we you knowappend it into the queue or terminatethe search so the authors find that over80 percent of these distances here whenwe're exploring these neighbors towardsthe mid phase of the search they usuallydon't contribute anything to the actualend result so we can get away withapproximating these distances so the keyinsight for how to approximate thesedistances is we're going to project thequery and the neighboring node onto thecenter node so again the way that hswsearch works is we find some Center nodelike say this blue node shown in mymouse and you're exploring the neighborsof that blue note so this blue note isthe center node C so we project thequery onto that Center node as well asits neighbors now the cool thing is thatwe can then compute the projection ofthe neighbors onto the center nodeoffline and store these offline and thenwe you just look it up and quicklycalculate the distance so that's done bydecomposing L2 distance between thequery and the neighbor into theseparating into the projected andresidual components so you know this isthis is the key detail that we'll beexploring in this paper summary video isunderstanding how each of these fourterms are computed this last one the uhyou know the angle between the residualuh components that's the key meat behindit but you compute these most of it isstored offline and we'll get into thememory overhead but this is the keybehind how it works uh so there's alsothis clever technique of using adistribution matching where you look atthe you know the distribution of theangles between the real data and thenonce you have the low rank projectionwhat the angle then becomes and doingthis kind of normalization thing it's asuper common thing to see in deeplearning so I think it's just cool tosee this in general so I I maybe shouldmention that we measure this angle ofthe distances by uh with the datadistribution so we you know have the SVDa low rank projection by looking at theactual uh residuals in the distances allthis will be clear from the papersummary video but just to give anoverview we use the data distribution tohelp with this approximation so thenfinally with the results I think lookingat this just 960 is the most interestingthing you see a huge benefit in queriesper second at different recall levelscompared to just Baselineimplementations of hsw the benefit beingnot not as big on Lower dimensionalvectors 96 100 128 but these higherdimensional vectors I don't really knowtoo much about like the informationretrieval benchmarks and how those reactto higher dimensional vectors generallybut you know as I mentioned in thebeginning of the video the open AI ada2embeddings are 1536 so makes it seemsthat they would bet that would benefitfrom this enormously so let's dive intothe details further this video willexplain the technical details behind thehnsw finger algorithm to give a quickoverview of the presentation we're goingto start off by exploring the hsw searchalgorithm and particularly the stopcondition and how we're exploring theneighbors to our current Center node andcomparing that with the upper boundwhich is the furthest distance in ourworking queue so far so the key Insightfrom that is that over 80 percent ofthese distance calculations don'tcontribute anything to the final searchresult unless we can get away withapproximating these distances so that'swhere the devil is in the detail is inlooking at how exactly we approximatethese distances so they first show thedecom composition of the L2 distanceinto the projected and residualcomponents we'll get into how exactlywe're going to compute our basis byhaving the SVD of the residuals as webuild a matrix of all the edges in oursearch graph and then you know low rankapproximate that and then use that toproject the vectors into the space andthen have the projected and residualcomponents and so there's a lot ofdetails behind this but I think it's sointeresting once you finally wrap yourhead around it and I think in additionto understanding just what values needto be pre-computed you see the memoryoverhead as well which is the thirdpoint is understanding the detailsbehind the memory overhead that thiscreates and I think that further justhelps you understand the algorithm sothen we'll get into this distributionmatching technique it's you use the datadistribution in that SVD decompositionto project the vectors so it's prettyinteresting to you know then align thethe real angles with the projectedangles by using this kind ofdistribution matching technique whicha super common thing to do in machinelearning and so it's really interestingto see it in this algorithm as well sothen we'll get into the results I thinkthe most interesting takeaway ismentioned in the beginning is that theyou get the best performance from higherdimensional vectors which makes a lot ofsense as we look at the math becauseobviously you're not having to do youknow all the distances on a 960dimensional vector and so it makes a lotof sense for why that is the one thatperforms the best but I think it's justinteresting to see I think we'll talkabout some future directionsparticularly maybe comparing this withproduct quantization to understand thedifference between the two algorithms aswell as just maybe what we can dofurther with this kind of distributiondistribution of the data to speed updistances okay so once we've built up anhsw proximity graph we're going to beexploring it by using the algorithmshown here the search layer algorithm sosearch layer takes its input the querythe entry points from the previous layersay Layer Two down to layer one or layer1 down to layer 0 we start with theseentry points that come from searchingthe top layer and that's where thehierarchy of the approximators neighborsearch comes into this so then we havethe ef ef is one of the hyper parametersto be looking at as your say like tuningweeviate and looking at you know EF it'sthe size of the queue as you'researching so you know if you have asuper big Q then you're going to have alonger search and so on so and then youhave the layer number that you'researching through particularly becauseusually you'll store the proximity graphyou know as like a dictionary where youindex it with the layer number andthings like this so the output is goingto be the EF closest neighbors to thequery so we initialize the visited setthe candidates and the the dynamic listof found nearest Neighbors The CW The Vset so then while we still have elementsin the candidates we're going to pop thenearest element from C to Q from that uhcandidate queue then we're going to fwe're going to peek at the furthestelement from W to Qthen this is going to be the terminatethe search condition if the if thedistance of that nearest neighbor isgreater than the furthest element thenthat means that we've evaluated all theuh all the reasonable candidates to besearching through so now we're steppinginto uh probably the most importantthing to be looking at with particularlythis paper is so for each e in theneighborhood of C so we've popped up Cit's the nearest element to our queryand now we're going to be exploring theneighborhood of C so you know say thisred point is our query and um uh we'llbe an example let's say oh yeah so sothis is our query this red point andthen we bring this blue point with usinto the layer one so now we'reexploring the nearest neighbors uh tothe red point which this is our queryagain so now this node is the closestnode to this one so you know thisbecomes our new furthest element andthis ends up being the entry point as wecome here or I guess the green one issupposed to be our query but anyway so Ithink you get the idea the green one issupposed to be the query in the toplayer where we you know this is thenearest neighbor on layer two and thenthis is the nearest neighbor on layerone and then take this into layer zeroor so onanyway so that's basically the idea Idon't I don't know if that picture issuper clear but but basically so oncewe're there we're going to add thisneighbor to our visited set and thenwe're going to get that furthest elementfrom W to q and we're going to becomparing the distance between each ofthese neighbors with that upper boundand the furthest element from W to Q soonly if e is closer to our furthestelement in that Dynamic listed found inthe Earth's neighbor so you know say weentered the graph with three entrypoints from layer one and now we're inlayer zero and we're exploring theneighbors back closest one if it's notuh if the distance isn't less than thethird you know that third as if thethree that higher highest distance thenwe're not going to add it to the queueand so on so basically what they find isthat when you get to the mid phase ofthe search say you have 10 of thesenodes in your queue now and you'relooking for that 11th as you'reexploring or you're looking to you knowadd a tenth and then prune out one ofthe ten what they find is that over 80percent of these distances you aren't atis lower than the upper boundthe distances further I mean so you'renot adding it to the queue unless itdoesn't it's not even worth Computingthe distance so we approximate thedistance so in conclusion because mostof these distance calculations don'tcontribute to the search we can affordto approximate these calculations so nowit's time for the devil and details theI think the most important thing tounderstand about Labor so how are wegoing to be approximating distance sowe're going to be showing how toapproximate L2 distance with projectedand residual components so let's startoff by just a quick recap of that ideaof projected and residual components sothe core intuition is that we have thisquery vector and then we have theneighbor of the center node C that we'reexploring we always have this referencewhere we're looking at four e Neighborsfrom C so there's always that kind ofrelationship as you're exploring nodesso we're going to be projecting thequery and the uh my arms have been thatwell but like to the center node C so wehave the projected and residualcomponents so basically I think the keything to understand is that theprojected component is like how much ofthis Vector is this other vector so it'sgoing to be a scalar times the originalVector so Q sub proj is T times thevector C so you know and similarly D isB times the vector C so that one ispretty straightforward and then the theresidual ends up being the Q minus theprojected component is how you thenderive the residualso with the D projections we can computeall of that offline because we build abig Matrix of the um of all the edges soyou know we we already know what Dprojected onto C is going to be and wealready have these edges to compute thatwith so let's actually let's let's getinto the calculation and then we'll stepthrough each of the four key uh terms inthe equation and then you know see howit's computed soL2 distance Q minus d uh so we're goingto break that into the projected andresidual components and then you kind oflike multiply that out and you end upwith these other terms so the firstthing to note so if you'll see my mouseis we you know first we split apart justthe basic thing of you know Q becomesthe projected plus the residual and thenD similarly is the projected timesresidual and you multiply that minussign through it I don't know how much ofthe algebra I should walk through whenyou're separating it out but so you knowthen you continue separating it you endup with this term as you see this againcomes like here as you're breaking intothe LT distance between vectors you'lloften write it as like you know a minusB transpose times a minus B and then youmultiply that out and you end up withthe uh you know the the plus two atranspose t b sorry with the T again butso you end up with this term and becausethe projected components are orthogonalto the residual components this termjust you know exits out but I think theonly thing to be real to be seriouslyinterested in is what you end up withwhich is this final thing so we havethese four terms terms that we need tocomputeso similarly if you're approximating thedot product you don't need to have theseadditional Norms in the finalcalculation so you have these just kindof inner products so you know if you'reinterested in L2 distance versus cosineangular distance personally I in myexperiments with playing with you knowVector search for a while now I've neverfound that you know especially with deeplearning produced vectors that arealready kind of scale normalized for thesake of optimization I just use LTdistance maybe somebody has an opinionabout that and I'd be happy to hear itbut I think let's just stick with LTdistance for now all right so as welooked at the derivation of L2 distanceinto projected and residual componentswe're left with these four key terms andthis is the key detail to understandingthis paper is understanding how each ofthese four terms are computed offlineokay so the first term is going to bethe L2 distance between the queryprojected and then the neighboring nodeprojected onto the center node theprojection components of each of thesevectors so the first Insight is thatboth of these query projected and theneighbor projected are some scalar timesthe center don't see so we have t timesC and we have B times C where T and Bare scalars so from there we're going tobreak the L2 distance we're going totake out that c that's common to bothand then we just have T minus B squaredand then the L2 Norm of C so wepre-compute the the norm of each of thenodes so you know for every node in ourdatabase we've computed the note the thenorm of it so you just look this up thisis a part of the extra memory cost thatwe'll look into later so then T minus Bsquared is just a subtractionmultiplication to itself so it's notlike we don't do any computation butthis is you know as you see in the topof the memory reason the arithmetic iscounting how many um you know the lowlevel of how many additions and so onthat we need to compute so B can also beprecalculated by looking at the sameequation so you know B is going to bethat component of the neighboring node Dof C in that you know and you cancompute that offline so let's get alittle more into this soso this T equals query transpose times Cdivided by the norm of C that's going tobe like the coefficient when you projecta vector onto another Vector would justbe this then times the C again I thinkis yeah so so from there you can just uhbreak down so then remember we alreadyloaded in the norm of C from that wepre-computed so now we just need to dothis top part the query transpose timesC so that just ends up being the norm ofthe query again the norm of the centernode that we already computed and thenwe do need to do this L2 distancebetween the query and the center nodebut we only need to do that once toexplore the neighborhood of this Centernode so in these H and SW graphs theyoften would have like 32 64 edges pernode so you know we just compute thisonce to then explore the 64 neighbors ofthe center node and node to each ofthese D's as We're looping through theneighbors so that's how we get the firstterm and hopefully it's clear that justthe uh you know the decomposition intoscalars how we're able to pre-computethe nor form and then you know how howexactly we compute the T by having itit's the coefficient of the projectionkind of and you know then we break thetranspose into this so the next twoterms are pretty quick to uh derive sowith the residual Norm we just computethat offline we just look it up frommemory to add it to our four terms tocalculate the uh you know the Q restranspose D res so then we have the normof the residual component of the queryso the way that we do this is again thequery equals the projected plus theresidual components you've separated theoriginal Vector into the residual andprojected components just like post eachother and so so to get the residualcomponent we'll just subtract theproject the query from the projectedcomponent so the projected component uhso again we calculated that t value instep one so we're now going to square Tand multiply it by that Norm of C thatwe've pre-computed and loaded intomemory and so now we'll just um you knowwe just have the projected componentthat we calculated from t squared timesthe norm of c and then we just subtractback to query from thatof the the norm of the query sorry andagain the norm of the query is umsomething that we've already computed sothese are these are scalars as we'vecomputed the norm hopefully that's clearokay so now for the most interesting ofthe four terms the angular distancebetween the residual components of thequery and the neighboring node so firstwe're going to break out this transposedot product into the norm of the queryplus the norm of the uh the neighboringnodes residual component times thecosine distance between the residualcomponent of the query and the residualcomponent of the neighboring node sothis is where now we're going to lookinto the singular value decompositionthat is used to do this approximation sowhat we're going to do is first we'regoing to create a matrix this d d subres the residual Matrix where each entryin The Matrix is the difference betweenthe vectors so you know if there's anedge from A to Bthe first entry in The Matrix is goingto be a minus B and so and you know thena to F the next one is the differencebetween a to F so we construct this bigMatrix that wayand then we get the low rank basis bydoing singular value decomposition wherea singular value decomposition youdecompose The Matrix into theeigenvectors and the you know like thetop R singular values they capture mostof like the variance in the vector soit's like you know like about projectit's kind of like how tsne and PCA andlike low rate low dimensional algorithmswork it works like that where you saythese are the components that capturelike the variance in the data so thedata being the residual Matrix the uhthe yeah like the distance Matrixhopefully that's clear so like that isthe you know the the top components ofthat distance that you then use toproject the vectors into that space sookay so now let's look at how we get theother half of that term which is goingto be the projected residual componentof the query so first what we're goingto do is we're going to leverage thatwe've already computed a lot of thestuff that we can use to derive thiswithout having to do the multiplicationso or or having to compute the residualcomponent and so on so so we We Begin byknowing that the um that the residualcomponent is going to be the query minusthe projected component so so wedecompose That Into You Know acutetranspose B minus Q projected transposeB and then from there we expand the Qtranspose projected again remember is Ttimes the center node that's the that'swhat the projected Vector is this Ttimes C thing and T is the scalar oflike how much of the center node theprojected component of the query is soso then we break that up into again thet is calculated like this where QTC andagain we've already calculated that sowe already have that calculation andthis um so this qtb the the reason thatthat's interesting to put in the frontis because we can just compute this oncefor the query to then do our entiresearch withbecause um none of these are dependenton the center node C you know when we'reprojecting or the residual componentthere's always going to be thatreference C that we're doing this fromwhereas the qtb that's just Universal toevery single traversal we're going to doso then we get the Q res uh TB from thisthese subtractions and then we take thesign of it and then we just have theHamming distance between the residualtranspose B that we derived from thisand then again the Dres TB that we'vecomputed offline and saved as thisbinary representation okay so hopefullythat's a clear explanation of where weget each of these four terms from thenwe just combine them by adding themtogether and then we have the distanceof the query and this neighboring node Dso they show uh later on when theyablate the comparison between that SVDof the residual and compared to say ifyou know if you don't have theseadditional terms as well so you onlytook the the sign of the Hammingdistance between the um you know theresidual the query and the residual thedistance they'll show some ablations onwhy having all these terms helps withthe stability of this algorithm and theaccurate approximation of the distancecalculation so let's get more into thememory cost of doing this and I thinkthat'll really help clarify what'shappening here so for each node we'recalculating theuh the projection of it onto that lowrank basis so this is what we would thenuse as reference in in those CTB so thisis this is computed for each node ifwe're then going to be exploring fromthat node to its neighbor so this nodeis going to be the C in our referenceand then we're exploring the D neighborsof C so then we also compute the uh thenorm of each uh of each of these nodesthat would again look up several timesin the in the calculation so this is theextra memory for each node now thereally the memory comes for each Edge sowe have for each Edge we're going tohave the sine code so again we have thatD res TB so we're Computing that andstoring that offline so that's ourbinary uh representation of the residualprojected with the B then we have fourbytes of the pro projection coefficientB so again we use that that b to havethe T minus B squared in the um in thecalculation of the original distance ofthe projected query in the projectednode and then we also store the no thenorm of the residual component for eachof the nodes for in the edge so theresidual component with respect to thisSource uh so like if it's a to B andwe're looking at it from the perspectiveof B and sorry a is like our C Center nolowercase C so we're storing theresidual of that the norm of that foreach Edge that again we look up severaltimes with this calculation putting thattogether that results in the additionalmemory being the number of nodes we havethe Delta V for vertices graph beingmade of vertices and edges we have thenumber of vertices times four R plus oneand then the number of edges times Rover eight plus eight so what that endsup oh sorry what that ends up meaning isso let's say for example we have thedata set that just one million vectorswith 960 Dimensions per vector and saywe have a maximal 96 edges per node sowe end up having one million times andthen the four times say we use 64 as thelow rank for that projection so we keepthe top 64 singular values of thatprojection and then we and then we addthat with the 96 million edges of whichwe then have in this case 16 additionalbytes per each of these edges soso you can see how kind of therelationship between how many vectorsyou have and how many edges per node youhave and how that scales with this so Iguess kind of interestingly yeah so youknow you have this 96 times 1 millionfor the edges so maybe when you're usingthis you'd want to have I guess the moreedges you have the more computation youwould save when you're exploring theneighbors of that Center node but thenyou know that would lead to more memoryoverhead is you have 96 and then youknow going on into whatever number butso in the end we end up having 1.7gigabytes extra added with this fingerindex so the offline values for thefinger index and so with respect to thisjust example where the it wouldoriginally be 3.6 gigabytes for the datapoints and so you know you're talkingabout like adding like half so inaddition to the memory overhead there isthe time overhead of calculating allthose values they find their experimentsthat it adds about 10 extra time sothat's really not too bad if you lookfurther into the detail they have thederivation of you know all those memoryreads and arithmetics if you want tolook into the distance between justdoing full L2 distance and then you knowthe series of steps that we looked atwith calculating those four componentsso so another clever technique that theyuse is distribution matching between thereal distances between the neighboringnode and the center node and theapproximated distances so oh sorry sowhat this ends up looking like is shownin the green you have the real distancestend to be gaussian distributed whereasonce you approximate them with thisprojection they end up being a littleright skewed so what you end up doing isyou calculate the mean and the varianceof the real distances with that residualMatrix and then you also calculate themean and the variance of theapproximated distances and so then youwould normalize this approximateddistance score by having Tu to use theapproximate distance not the scalar thecoefficient from the projected thingearlier but so this approximateddistance becomes you know what itoriginally was minus the mean of theapproximated distances for all of thedistances and then you know the varianceover the mean of the very audience andthen you know plus the new means so thisis a way to try to you know align thedistribution and make it more like thereal and the real distribution of thedistances so now let's get into the funpart the results of the experiment sothey're going to be testing this withsix different data sets and twodifferent uh distance metrics euclideanas well as angular distance so just incase you were worried about that fromoriginally looking at the differencebetween the L2 distance derivation andsay you know cosine distance and so onso they also do test it with the angulardistance so I think the most interestingthing to see is the gist 960 shows thebiggest benefit shown in the red is thehsw finger what you're looking at isqueries per second versus recall so inthe a n benchmarks evaluation protocolyou look at different hyper parameterswhere you could you can get really goodrecall by trading it off speed by sayincreasing that EF and so on so so theythey Loop through the hyper parametersthen put a plot on each of these curveswith um the recall and then the speed soyou always are going to have that kindof like accuracy versus speed trade-offwhen configuring approximate nearestneighbor search algorithms so anyways Ithink the interesting thing is thatshown in the top right the higherdimensional vectors get much betterresult that's also consistent with the784 from the fashion mnistbut this one is you know the mostsignificant probably here as welllooking at you know you know like 1500versus uh you know 500 so you knowsomewhere like in this particular youknow measurement of it it's a prettymassive speed up but generally they saysomething like 20 to 50 faster but youknow these lower dimensional 96 100 128they don't seem to see as much of abenefit from this so I think it's alsojust worth mentioning that the textembedding uh 802 they're 1536dimensional so it makes sense to thinkthat they would the finger would looklike this with those particularembeddings so some additional results ofthe authors comparing hsw finger withtwo other techniques that I personallyhaven't looked into so I can't speak toyou too much but they also add some kindof uh distance approximation on top ofthe hsw proximity graph they also testthe importance of using the fingeralgorithm compared to say randomprojection locality sensitive hashing orsay you don't do the full derivation ofthe terms and you just measure that Qres t uh d-rez part at the end so that'swhat the Asian SW SVD thing is so theydo show that you're getting much betterrecall by using the um the finger as itis where you use the um you know theactual distribution of the data to dothe low rank approximation compared tojust like random projection where yourandomly sample vectors to project itinto the space into the lowerdimensional space and end ideas likethis so here are some of the reflectionsI had after reading this paper uhfirstly I think it's very interesting toconsider uh hsw finger and productquantization so product quantization iswhere you compress vectors by you knowyou have this vector and then you'llslide these windows and then you'llcluster each of the windows and then youwill represent the window with thecentroid ID so by doing this you knowsay you have like zero you know two64-bit values and then you've compressedit to like an 8-bit ID too that'sbasically the idea and so productquantization is about saving memory andtrying to reduce the memory overhead ofhnswbut also kind of what's interestingabout it is you end up using it for diskbased Solutions so like disc NN is aboutyou you load these pre-computed I'msorry you load the um the compressedrepresentations and this way you getaway with not needing to have so muchRAM for doing it and I think hnsw fingercould also have a similar kind ofapplication to disk where you don'treally need the real vectors anymore forthe actual traversal of the vector as wesaw you're just doing the distancesbetween these uh pre-computed valuesthat don't require the full Precision oryou know whatever the original Vectorwas rather you just have thesecomponents so you know even thoughyou're adding memory to the thing if youwant to preserve the original vectorsyou don't I don't think you need thoseoriginal vectors for the actualtraversal part so this is kind of somequick thinking about I haven't reallyput too much thought into thatparticularly but I think the nextinteresting thing is just measuring theonline maintenance of this kind of lowrank projection in that residual Matrixyou know what happens as you add moredata that's kind of one of the bigthings that separates Vector databasesfrom Vector libraries isyou know people continually add datainto the index so how often are we goingto need to recompute all these valuesbecause we would need to like reprojectit and so on that's also the problemwith product quantization is you'redoing that K means to Cluster thosecentroid IDs and then it's like well howmany of these do we need to load beforewe have a representative sample so Ithink also that kind of distributionshift could be a really powerful uhlever for that online maintenancebecause if you just know thedistribution you see the thedistribution starts to skew you can youknow just kind of align it with thatnormalization trick and then further Ijust think finally this like using thedata distribution in these a n indexesthey already kind of you know do thatwith how the edges are constructed but Ithink explicitly doing things likecompression with the data itself thankyou so much for watching this papersummary video of hnsw finger this paperis such an exciting way to speed upapproximate nearest neighbor search bypre-computing these values toapproximate that distance calculation uhthere are a lot of details to this papersome memory and I hope that I got a lotof them right if you find anything wrongplease don't hesitate to leave it in thecomments and you know help myunderstanding as well as everyonelistening hopefully so again thank youso much for watching if you want tolearn more about weediate you can checkit out on weevia i o Open Source onGitHub weekend and then on Twitter atwebiate underscore IO also join ourcommunity slack and discuss these ideasaround approximate nearest neighborsearch I think it's so exciting to seemore research with this and I it's justsuch an exciting part of Libya is how dowe you know compute these Vectordistances efficiently so thanks so muchfor watching", "type": "Video", "name": "HNSW-FINGER Explained!", "path": "", "link": "https://www.youtube.com/watch?v=OsxZG2XfcZA", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}