{"text": "Hey everyone! Thank you so much for watching the Weaviate Podcast! This is pretty novel episode featuring both Weaviate ... \nhey everyone thank you so much for watching the weba podcast I'm super excited to do two things we've never done on the weba podcast before firstly we're welcoming both we VA co-founders Bob Van light and Eddie and delocker and because we just need to bring everyone in on this because our guest is someone who has dominated the vector database market and so I'm so excited to welcome John dagdoll and John can you tell us about hyperdb and how you just figured it all out so quickly sure yeah so hyperdb it's a hyper fast local Vector database we're uh we call ourselves a BS database as in a better search database um yeah I'm happy to tell you more if you've got some questions for me it's just um you've you started you were so fast uh John so tell us a little bit about like if you say for example we are is that like your whole team or is like a royal Wii what are we if you say we what are we talking about because yeah yeah we're a wild worldwide community of developers it started with me but as soon as I put the project online we got PR's and contributions from all over um so yeah now we're a diverse team we got a great uh Discord we call it the hyper debit card so yeah um interesting and yeah go ahead have you been able to raise money so far yet we have a lot of interest from VCS they've reached out especially after the initial announcement um we've actually raised 600 million dollars so yeah wow congratulations and why because it's like a trend uh while raising money is affected to be to have your logo on the NASDAQ uh thingy why have you decided to stay silent and not do that well we're also stealth even though we have a lot of PR okay that's very very smart yeah that is very interesting um I was really inspired I saw a lot of companies you know putting their putting their logos out and raising money from VCS and you know that I looked into this approximate nearest neighbor database thing and I thought to myself well I could approximately make a nearest neighbor database and so that was where the idea for hyper DB was born um we're the fastest Vector database in many ways we're the fastest to a thousand GitHub stars um we're the fastest pip install and we had the fastest external contributor uh join wow and but I think you're you're actually you're even still downplaying a little bit because um I'm very curious to hear um the I mean what you've been creating in and and people can sit on GitHub right in um yeah actually this it all comes together in this nice file the Galaxy math rain shit.pi can you maybe share what's what what brought you to this yeah to this Insight in creating this yeah I just I I like to name files so that if you just read them you know what they are and we really believe that that's the key to hyper DB's success um we do things like Hardware accelerated math kernel blasts um so very fast cosine similarity and then also we have some some really interesting things so alignment is a is this problem that is is now really really trendy in AI that you've probably heard of so actually we're also an aligned Vector database uh and we try to be metaphysically aligned with the the nature of reality and so if you look at similarity metric you can see it brings in some of the the metaphysical Randomness that is actually inherent to life I I can really hear the the BS the better search and all of this so great yeah it is unbelievable it's it's it's fantastic congratulations it's just I I remember we sold and we were like whoa this is just sometimes you just have to you know you see something happening and you just said okay so much and that especially means a lot coming from you Bob yeah thank you for saying I think it would be interesting to learn a little bit more about you John and what you're doing and what you're actually working on so it's like the because um a lot is happening in the space and um it's actually it's just it was just I mean I think what you've done is you did it exactly at the right time and it was like the right thing to do and it is always good to keep everybody on this on their toes in this way so we we just you know we really appreciated that but we would love to learn more about the actual child and the actual thing that you're working on so maybe you could share a little bit Yeah so for anyone listening that hasn't come caught onto the joke yet uh hyperdubu was kind of just a joke that I put out on Twitter um there is a real GitHub repo it does actually work for everything it says it does um but I I noticed so I've been working in in natural language processing language Technologies for five six years now um I'm just finishing my PhD at UC Berkeley in the materials science department but I do scientific natural language processing so things like extracting data out of research papers and training language models to help us work on like figure out what we should work on next and I've been using hybrid search Vector database kind of stuff for a few years now and then I just exploded all over Twitter um people were raising money all over the place and it became kind of especially poignant because of the the language models uh using them as for retrieval augmented generation and I think it got a little out of hand in some some ways um especially just kind of the hype tweets all the time so I thought you know why don't I make my own um so on a Saturday night I kind of was just a little I don't know I don't know Wham I built this this repo and I'm I used uh like I generated a logo that I thought looked kind of hyper and then I put it on Twitter and then it went viral I didn't expect it to go like this big um yeah it got seen by half a million people um it hit the top spot on Hacker News uh I had dozens like I'm not kidding dozens of of VCS reaching out to me very big name VCS uh so it was it was kind of funny and they would say things like hey we checked out your project we think it's really cool we'd love to meet you and it was just kind of I got us I got a feeling or I got a A View into what the world must be like for some of these companies that have a lot of hype going on right now yeah yeah no that's any and that's interesting and I think what's so interesting about what you're saying is the um so what's interesting today I I I I I put something on on Twitter that was like I saw this nice and I forgot which uh repo uh it was on but we can add that to the the show notes Here you saw this nice tree right so where these how these models evolved and at the the bottom of this tree you see like glove fastx and those kind of things and I remember that that for me at least that seat was planted like hey wait a second you can actually do something um we can do something with these embeddings when it comes to to to search and and those kind of things I remember that the might actually be funny to share so the first the very very very very very very first prototype that I uh that I that I built for me was actually also in Python and basically what I did was that I had a few of these paragraphs and I took these individual words from the paragraphs and then I just calculated centroids based on the words that I got from that funnel that was like that you could download from the uh from the glove uh um repo um that's um that was actually built very fast and and it was interesting right but then the the Kudos are actually building a database an architecture in a database and what it means to build a database I think that that 100 goes to a good goes to hn but that insight and like hey wait a second these the defectors can be used to for for better indexing and better better better searching um um um that's an opportunity we believe that that's an opportunity for a new database in the market but I'm curious to hear from you John it's like what do you think happened in the in in the um I mean it's easy to say for example you know in November we got like little GPT thing and that accelerated that is true but I'm just curious what you think was that people all of a sudden what they saw that they didn't see before that yeah because we had gpd3 which has a lot of the capabilities of chat GPT I mean almost all of them like chat GPT is kind of just like a a tuned version of of that model um I think it really was kind of the UI changes that that being able to have that back and forth conversation um like when you when you work with gpt3 it's just trying to continue whatever you wrote and so it's not a natural kind of interaction you have to kind of abstract your thinking and understand what the model is doing in order to work with it but I think when when openai released chat gbt uh I think I recall like Sam Altman saying in some interview or something that they were surprised no one else had done it and so that's why they did it and then released it I don't think they expected it to go as big as it did um yeah but that's a much more natural way of like interacting with with AI and then I think a lot of people there are these moments where like a a switch flips in their mind which is like they can imagine themselves using something for the first time I think a lot of people like uber for example I don't know if you remember your first Uber ride but for me it was very like weird I'm like I'm gonna get into a stranger's car that and I like see them on this app like okay and then you do it and then you realize like it's not weird and it's natural and it's good and then it just is a new way of life and I think that that happened with with NLP and and language models and stuff with chat gbt that back and forth conversation opened people's eyes to what's possible and then they could ask it to do something rather than try to like create a beginning of a prompt that then completes into something that they wanted it to do um and then they realized with the true power of these models are um and then in terms of the the vector database uh part of this um I totally agree with you that it's like word devec was that moment for me when I saw the word to BEC paper and then I started playing around with it being able to do math with words was just like mind-blowing um for for analogies and things like that and so yeah I think that the the vector database piece is trying it's like the memory of a language model so how do you how do you get it to to use specific information um in its generations and so yeah and I think Lang chain is also part of this so so Lang chain is for people that haven't played around with it it's kind of a library of a bunch of scripts for chaining together calls to llms and different prompts and you can change the prompt based on what happened in the previous step um and it's got a lot of attention it's one of these super hot projects right now um but combining that with the the flip switch in people's minds uh really made Vector databases really important on the map I think uh at least in the like the public mindset right now yeah I think what might be interesting also a question for you hn of course we uh I I know the I kind of know the answer but it wouldn't be nice for the people that that that um listen to the podcast so what was for you the first time that you had this Insight from a database perspective from a fact database perspective you were like oh wait a second I'm just not chaining algorithms there's a bit more to actually building a production system what was the first what was the first time that you're like oh wait a second yeah I think that was uh it's hard to say in time but I I know the species uh VC back companies always thinking like uh financing faces and I know that this was around our our seed a phase because we we had we had just started yeah experimenting with isn't it exactly as Bob said before like these these kind of small paragraphs and seeing does this actually work and at that time deviate was because we also came from that direction of Knowledge Graph and was kind of intended as like almost like a stateless layer on top of on top of knowledge graphs and I mean building something stateless is relatively easy and that was kind of like what I did before is there a whole running 12 Factor app sort of stateless loads in the cloud making sure that they scale which is switches in hindsight super easy compared to making a database scale um but I know that that we were we were already sort of kind of experimenting with this okay do we really want to become our own database and I remember that we actually hid that information in some of the the the first talks that we had with we see companies reaching out because we're just like okay this is too crazy this is like we're we're not there we need to First prove that this actually works before we we tell others that we're building a database and yeah it did work and I mean the rest is history um and to me this is this is to this day something like when I read these tweets like hey if you have a hundred embeddings maybe you don't need a database maybe you can just run it in memory like for me there's always like it's not just about scale there's so much more like if you want to serve this in production if you want that to have that be highly available if you want backup solution if you want filters if you want like all these kind of features that if you want a migration path for people who come from uh from and I think this is also interesting in the context of Lang chain and stuff with which brings new users in but you also have users who already use a search solution if you want to give them migration path to sort of more modern search they're not going to drop all these hundreds of features that they already have and sort of database I think is the wrapper term for for everything that's going on here and um yeah so yeah making sure that that works at scale and at production quality is um unfortunately a bit more work than just um yeah putting yeah there's like multiple types of scale too there's like the scale of your data so what I recommend is if if you have like a like less than a hundred thousand vectors then you just want to like mess around with stuff on your computer using like an external database that you're actually like sending data out of your house into their servers and then back in just to do like little things like that probably is not the right you know tool for that job but if you need to have something that's got I mean availability all over the world and needs to have latency under like 10 milliseconds and uh like production applications that's a whole different set of needs right um and then also just so I I uh during my PhD we created a couple scientific search engines um that we serve to the world actually um one was on covid literature and one was on Material Science literature and the pain of getting that notification at two in the morning that something went down and that you have to fix it it's just yeah it's it's something that still I wake up in with nightmares and the the the solutions that are hosted or offer kind of more robustness um they really help with that kind of thing so if you're trying to build like a product that you're going to give to other people uh definitely that's something to consider um and then also I think a lot of people we're not really at least in the way that we're having public discussions like not here but you know in General on Twitter and stuff um people are not making the distinction between databases and search engines and I think that that's actually kind of something that we should be talking about as a community you know what what what are the needs of a database uh you like and what are the needs of a search engine uh maybe we want to get into that here but that's that's actually a very good point and and I I wanna I wanna add um uh two things to that right so we also of course deal with libraries so we have the yeah like like and we of course have to hypnotize on the podcast talk about face and those kind of things with with the libraries the databases the search engines um and of course uh embedded also starts to play a role right so I think that duckdb let the way in that and it's very interesting because it's it's a new way of of also working at a um um with data and then and in factors play role in that so I kind of see these kind of these four um you know the these these four things I'm actually curious um from from both of you to to um to hear the uh what you think about that right so how you how you how you look at these four things and how you because I notice I spend a lot of time of my time talking to people explaining these four Concepts and and how they're different I'm just curious how how you guys look at that hey you you want to start John oh sure yeah um so I I think that the user is is one important factor so like for libraries the user is kind of a developer right somebody's trying to achieve a goal and they they don't want to bring everything from maybe like they just need a little little part of the puzzle maybe they they want to add a semantic feature to their like a semantic similarity feature to some analysis that they're running um in that case you might not want to set up an entire database just to do like calculating similarities between maybe a thousand things or something like that um and then I think also there's maybe a fifth thing I would add which would be the data um because what you're actually starting with is so important uh even I mean that that's like the the biggest lesson I learned in all of the research I've done um or in this space at least in scientific language processing what data you put into this stuff has profound effects on on how it performs so for example if so one of our papers in the early days we trained were devec on a corpus of research papers and then we used the word tobacc um just it predicts the Sim the the the likelihood that two words are going to appear together in a corpus uh and you can actually ask it the probability of words that never occur in the original training set and then you can use that to make predictions about what should co-occur and like we actually discovered a bunch of new materials that way um but if you train the same thing on just if you just mix in a bunch of unrelated like non-non-material science text or even Material Science text us on a different type of materials uh domain the performance just disappears and so that's that's a really key piece especially when you're using semantic similarity and stuff like that um and then you know I'm not I wouldn't call myself an expert on on any of this stuff to be honest uh I use it a lot but I'm not developing these algorithms for like I'm not developing Vector similarities approximate years neighbor algorithms or anything like that but that's actually interesting it's interesting to hear it from the perspective of a a user who's knowledgeable in the NLP space right so that that's what makes it in my opinion extremely interesting yeah if I'm hopping in quickly I remember that um I think it was 2019 that paper you were part of that had the unsupervised latent space of Materials Science and I love that like that was one of the first papers that I saw these clusters of some domain like scientific literature mining we also had Kyle low on the weba podcast what maybe like yeah because what's the current state of scientific literature mine the covet systems one of the biggest like what are the frontiers of it yeah I think um what the court what Kyle and and the um that team did on the core data set was really interesting like that I think so the the pandemic accelerated a lot of trends that were sort of already happening and I think text mining of scientific literature was actually one of those um it was the first time that we had a large-scale specific data set of full full research papers made available to the research Community with no restrictions on how you could use them um that was a really interesting moment and that was a very it's a very still a very interesting data set because usually the licensing problems with the copyright of research papers can can really slow things down uh for example we have a bunch of full text in in for the work that we do but we can't share any of it with other researchers we can't even share the database of abstracts that we use to train our models so so nobody can they can use the models that we've trained but we can't actually just give them the original data that it came from um and so that that's that's one Trend that I think as now everybody kind of is just putting stuff on archive first or even just never putting it in a in a peer-reviewed journal that's copyrighted [Music] um that's definitely I think a trend that's gonna it's gonna be important um I think you also see a lot of like searches becoming very important because the quantity of papers coming out and quantity of data not just in in science but in the World At Large like people are publishing things in blog posts that would have been research papers a few years ago and so just having like a good sense of what's out there and how to find it is going to be really important um and I One Trend that has not caught on yet that I think is actually going to be really big in the near future at least in science um it hasn't caught on yet is like these AI assistant kind of chat gbt style things that allow you to there's like a lot of common data manipulations that we do like maybe go through 100 papers and then pull out a specific number that we want to make a plot of so that we can try to analyze a trend that's going to be the kind of thing that before you had to have I mean you had to be somebody like me that like spent years figuring out how to how to manipulate data like teach yourself how to code and then learn these these language model technology things and then put it together into one thing just to make a plot out of that or you can give it to what they do is they hire an undergrad intern and then give them this mind-numbing task of pulling these numbers out of papers or something but probably in a year or two it's going to be something that you ask like a chat gbt like research assistant if I could just I think this is a great transition into a recent research paper of yours on structured information extraction I I love this topic of just like you mentioned you know I remember back when I was wrestling with the chord 19 days at two and I've always mess with that data set also and partial through the schema now the language models can kind of like help with that can you tell me about what you're learning about that yeah um so this is something that we've always been this was the goal of our projects all along was to extract research so so I got into machine learning because I I started as a computational material scientist so what we do is we we simulate materials on supercomputers at the atomic level and then instead of doing experiments you can just kind of calculate what the properties of the material are going to be before it's ever made in a lab and we were generating a lot of data this is uh we and we make it available to the World on something called the materials project materialsproject.org if you want to check it out um and those data that data set size is in the order of like 100 000 maybe 200 000 materials now and then we have a lot of diverse data for those materials but uh it was a lot so I was like okay maybe we can do some machine learning on this so I started learning ml a bit and trying to train property prediction models and things like that and then I realized very quickly that we don't have enough data to do the things that like we really dream of doing here and if you look and just where is the data in the world on on scientific knowledge it exists mostly in the text and tables and figures of research papers and so that's like how I got into that um and so our original goal is why don't we just extract the data out and build data sets in the train machine learning models on those and then what we found is actually the the structured information extraction problem wasn't solved yet um and it still was it was it still is pretty hard but the new language models are really powerful and if you fine-tune them you can actually have them extract structure Json documents that you could then just insert straight into a database uh like something like VBA um and then get kind of this really powerful mix of structured data you have the the original text from whatever you're extracting from you can build your own features on top of that and then that could be the the layer that enables these like scientific research assistant models that I was talking about you know I think this is a great on the um Vector databases what do they add a great angle to it because once you have these filters you can do filtered Vector search it's not just like cosine similarities because you have these filters um so this is a great edian could you maybe quickly explain like how does filtered Vector search work sure uh filtered basically you've explained it right now [Laughter] without going too much into how it works under the hood the idea is that uh the vector and the object that the vector belongs to it they're not two separate entities they're not and and I think this is this also comes back a bit into like what is the separation between the database and a search engine like in V8 we make sure that those lift together they scale together and they're they're always together and why because we want you to be able to use those structured properties in combination with your your original uh or with your your vector search basically so you can kind of combine the best of Two Worlds the traditional angle that the filtering so again sort of if we're looking at the migration path if someone comes from the SQL world and says but I want to set where price equals lower than twenty dollars then they don't want to sort of lose that just because they're so e-commerce is a nice application and say they're they're looking for or a beautiful dress to wear in the summer within a certain price range they're not going to drop that that price range just because it's now now the NLP part of it is is better um but they still want to want to keep that and similarly that also allows for for hybrid search so the the bm25 and um and Vector embedding based search because depending on on what the model is trained of sometimes or not just because based on the the sort of in domain out of domain but also just in general they're not necessarily meant to match keywords but sometimes you do need to match keywords in your kind of application and having that in it and not just being like a vector being this abstract thing that maybe gives you an ID back but having the actual object and it allows you to to to filter on it to do keyword matching on it to to aggregate on it so so faceted search for example it's also for e-commerce is super important where you you return results and then you want to have these kind of ranges like it's it's lowest price highest price average price all these these kind of things and and these I call them database features was probably not the right term but it helps to to sort of paint the picture of um yeah what is it Beyond just pure Vector search yeah and I think that like aggregations and things like that are really important or groups um so you know you might want to say let's let's stick with the science example so high energy physics papers if you want to say you know what is the average number of authors on a high energy physics paper that's kind of a difficult task to to do manually and it's even difficult if you have even if you have just kind of a traditional table of all those papers uh because you need to get a sense of like what is it a higher what is a high energy physics paper within that you maybe previously you would have trained a supervised machine learning model to make like a binary label for just that label then you'd have to go and label that data is this a high energy physics paper no yes no yes no maybe you would use like a bag of words features or like TF IDF features or something to try to train a model on that maybe you could have just done like some some manual keywords to look for but with semantic search you could just kind of grab a couple high energy physics paper abstracts and say just give me a bunch of documents that are similar to this and then give me the average of the numbers of authors or something um and so that that's like a really powerful workflow I think um yeah and that these things enable yeah and to build on top of what you're saying which is just saying John so it's like uh it's not out yet but when this podcast will be out it will be out so because a economy maybe in a bit you can say something about as well because one thing that we're looking at is what we call these generative feedback loops and that is something that's very interesting as well so exactly based on the example that you just gave John is you get some information from the database and put it in the in the in the uh generative llm but then you feedback loop that back in and get a vector representation uh uh for it and I think that those kind of things also very exciting so I mean Connor you you know more about this and I but so maybe you can you can say something about this but for the for people who don't know yet what it is but what that exactly does uh well I think um well I think you did a great job of explaining the whole concept of it and I think it kind of it it's kind of like how we open the podcast talking about not like the height necessarily but things like Auto gbt laying chain this kind of like you know I think it started academically with the Chain of Thought reasoning showing that it could have like a task plan so there's that that's one angle that I like a lot about this save the generations back in the database because your database is like uh it's more of like a living entity maybe like the the data ingestion the continual things it can do with itself sort of and save its outputs so I mean I think like and I would love I would love to get your perspective on this with the scientific literature mining example because uh I had to give Jerry Lou from llama index a shout out because I've been learning so much of these little things you can do but I love this like compare contrast documents prompt so you know you upload your new paper about some material and then it can like sample passages to put into the input to form up this comparison maybe like automatic automated literature reviews that are kind of like sourced this way where you you know each comparison is two paragraphs save that back in the database with the cross reference in Wikia speak has you know has comparison so like you know that kind of automated analysis of your materials papers yeah and I think that that approach is really interesting because it leverages what so I think think uh for people that have worked with language models for a while um you kind of get an intuitive feeling for their strengths and weaknesses and uh those are different than I think what people are attributing to them today um so let me give an example um if you ask a language model to to just tell you some facts about stuff like really well-known facts that appear millions of times in the training set like the Eiffel Tower is in Paris or something those are going to be in the model's output but if it's only encountered something like a handful of times or once or twice it's not going to spit you it's not going to give that fact back to you and it doesn't make fact-based reasoning decisions that that not at least not in a structured strong way yet um but it is really good at manipulating uh content into formats that make sense within the context and so like what you're saying if you give it a couple abstracts and then say tell me what the where these abstracts agree and disagree those models will be very good at that kind of thing that's just their like really strong suit um and so I think that that you can get around the the hallucination problems and some of these these weeks weakness weak areas of llms right now by leveraging kind of what what the community's been building um and like when chat gbt came out that kind of chat ubt is sort of a gpt3 a little fine-tuned for for how it should output answers but combined with a essentially like a vector database right where it's like looking up past messages that it sent and you sent um to to give you better context when it needs it or to get better context when it needs it like in fact when I saw chat GPT and I got first access to it I I dm'd my friend Joe bergum uh who is uh he's he's the the head of Vespa AI uh which is like Yahoo's Vector database I'm sure you guys are familiar with Joe but of course yeah but I'm like hi Joe yeah thanks for watching by the way yeah um but I I DM them I said hey I think if we just connect an llm up to to a Vespa DB like I think that just creates such a GPD kind of experience um and yeah it it just this explosion of thoughts happen in my mind when I started seeing like all the possibilities here with the fact that people wanted to use them right and I think that's another thing like the user maybe there's a Sixth Element to that thing which is the user you know what are they trying to accomplish I think people's imaginations have been expanded recently and what they they think they can do exactly and that's also something so that's what he what he like how this whole conversation started right that question I'd like what that I was curious what you thought that happened and I think exactly what you just said that's the thing right we see that with Vivid as well the people like oh wait a second so now I can basically have my own data whatever that means or whatever that is right you can inject that and then get these results out I um funnily enough I always use the Eiffel Tower and is in Paris as an example um because one of the things that that you know also been experimenting with is like if you if you just basically over yeah for for lack of better terminology if you overwrite the knowledge that the model has with prompt injection so and the example that I often use is if you have something like uh the Eiffel Tower was in Paris but it's moved by truck to Barcelona and then basically if you now have the question where's the Eiffel Tower and from the vector database you can get out the um uh that that document that says something headed the Avatar has moved and you can use it as a prompt injection and you can basically uh uh in your in your prompt say okay you must base your answer on the following information we're giving you and that is I think that is in that is very exciting because then basically you're trying to use the model more for its language understanding rather than the knowledge that it has so I I have another question because a lot is dominated for obvious logical reasons a lot of is dominated in discussion by um by language models and we kind of can also see with like multi-model models and image models those kind of things I'm curious because you also been working in in other fields John what do you think um what what what different types of models or what kind of different types of embeddings will we see in your opinion and if there are not uh language models that are not image models will we see other things for I don't know yeah anything that can come to mind yeah I think that there's um so I think at some point we're gonna have it's beyond like an image model um I think that this is what some of the the AI like the quieter AI startups are doing like the big ones um which is you're going to be able to have like structured embeddings or latent representations of like a user's intent using something so like what was the sequence of of things that they just did on their computer that will like inform what we what they want to do next and like I think it's not hard to imagine a near future where it's just embedded in the operating system of the computer you're using and I mean it just like you notice that you want to do something and it'll just do it for you or like suggest hey do you it looks like you are you know you have a PDF open do you want me to just summarize it for you real quick um instead of you having to like write a prompt that says summarize this document or something like that I think there's also a lot of opportunities in like multimodal but understanding like how information is spatially laid out not just like an image embedding but like let's imagine we we all see lots of PowerPoints probably PowerPoint slides are laid out in a there's like semantic meaning and where stuff is and what size it is and what color it is and what images go along with it um Beyond just like a single embedding right there may be like contextual understanding there like what was the previous slide what was the train of thought that this has been going on through here so there's like right now we're just scratching the surface with how we how we represent information there's so many deep deep levels that we could get into um so that's actually where I'm interested in going in in my near future is is especially this kind of these like deeper ways of thinking about how do we represent information how do we leverage that to do useful stuff very interesting will that how do you think that embeddings is for the for the coming for the coming years that that's the way to go or do you do you first see something like something new something different I think we're going to see a lot of I think you guys have experimented with this a bit which is like the query expansion stuff using using these models where you you can basically like if you tell an LM hey there's like more ways you can interact with a search experience uh to get information you're interested in like you can tell them about the different fields that are in your DB and like what possibilities there are in there uh especially if you can then like fine tune it just a little bit to like work better with your database I think really interesting stuff is going to come out of that um I think multi-vector stuff is really interesting to me so you have like multiple documents you might have like an embedding for each sentence within a paragraph so like you might do one one embedding search to find the right paragraph and then another one to find the right sentence and then use that as evidence for something um how do you how do we uh so like right now embeddings when you just do a query embedding lookup they're just doing a cosine similarity or some sort of nearest neighbor similarity search for things that like seem to align together but that's not like the optimal I mean like a query looks different than a document right like big questions where is the Eiffel Tower maybe the document says the Eiffel Tower isn't where that might be similar but like another documents might say like where is the Eiffel Tower's Builders grave or something and then that might like also be similar because it has where is Eiffel Tower and stuff in it um especially if your your embeddings aren't like from a really deep model um and so I think that's one problem is that that query answer alignment thing that might be like solved soon uh where where you have another model that is just re-positioning the query vectors into the document space in a better way um you may also want to do things like contextual embedding of like there might be like Fast embed where you take embeddings as they are and then you re-morph those based on the context that you're looking for so like you might have a bunch of documents but you really only care about like one aspect of that that that space that you're searching in so if you have a bunch of um like fashion descriptions of like fashion items uh if someone is really just looking for like material based stuff like what material are they made out of like they don't care about like the cut of the thing um you might do like a if that isn't like an explicit column in your database you might not be able to do a lookup on that with like semantic search but probably there's a way with ML that you could realign those two things just to redistribute them based on more material information rather than like the overall information of the thing um yeah yeah things like those I think are really interesting areas that this is all going to go into yeah it's super interesting I I have the same thing with the interfaces right so we so for example what you now see with these models and then for example translate something to SQL or for that matter uh we've had career those kind of things that I would not be surprised that we're going to see Innovation where that just goes away that would just literally directly go from natural language to a form of doing a lookup in the database yeah I don't know how it will work yet but I I those kind of things I think will we will see um as well and I'm I have one more question that I'm very curious to get your thoughts on on John so one of the things that I this kind of related to how we started the the podcast right and how everything is like now you know that that everything is growing very fast and rapidly and and and I think it would be fair to say that a certain type of hype is happening one of the things that I I always say is that I think that the um um so so the the term that we've adopted to somehow share that with the outside world to to make sure where we believe where things are going it's like we've adopted this term AI native and one of the things I always say that I believe is that um it's very exciting of all the things that you can do now with purely with the models right so and people will build new products people will build new status but what I actually believe is that in in the not too distant future um it will just be sprinkled in everything right so you gave the example of the um of doing something on your laptop that you might open like PowerPoint and you you get some suggestion or something is happening why these models in combination with these embeddings and these database play play a role um would you think that do you think that's a fair assessment of of how you see things developing or or would I be stretching it when I said it yeah my so I'm not you know the deepest uh expert on all this like there are probably people out there that have an even better view into where things are going but I think definitely that's going to be the future we live in like AI is going to be so it's just going to be like part of everything um because it the way I the mental model I use for this is that there's like this set of problems that we know how to answer with step-by-step solution that we know all this all the steps so like that's just code right um so I know how to display a button on something that is rendered by a browser so that when your user clicks it does something like that is something I can like break down in the step-by-step solution but I would not be able to write a piece of step-by-step code to do image classification for example of like what is a dog and what is a cat that'd be incredibly hard to do almost impossible but the beauty of machine learning is that it lets us create programs like that we can run on computers that do things that we don't know how to write this explicit step-by-step solution for just by showing it enough data of the examples of what we want it'll do it and so before you imagine like we had like only this part of the world that we could build in because we knew how to write code for it but now like the rest of the world that we didn't know how to write code for before is available to us and so I think that's actually where a huge number of problems that people run into or like human desires and like the problems we want to solve required that that higher level of of thinking or or kind of ways we didn't know how to write code to solve it um I think that's going to be the the story of the next like 25 years yeah I have one more thing on that John do you like I love this kind of Step by Step where you know we could easily Define a task now the task is more abstract and I think the next step on that is the tasks that we don't even know our tasks already that's really interesting I think the answer to that is this open-ended open-endedness stuff like uh from like I remember like Jeff cloon and Kenneth Stanley had done a lot of this they have this like um the the initial experiment was this thing called poet where it was like a bipedal walking agent and um all sorts of environments for it to walk in and now it's they they are testing this in in Minecraft simulations so I think with the with the large language models and how we're simulating them like you're good to have like a little economy where they each have roles and you can simulate this and create these like open-ended worlds so every do you like this idea of open-endedness yeah that sounds awesome I uh I think that that's especially going to be interesting so uh I don't play a ton of video games anymore I don't have a lot of time uh that I spend doing that but I still think about them a lot for some reason and I think that like it's just kind of this this idea of this like Holo deck that you can just go into and experience a different world that has its own like story and life going on in its own things like that that is an enticing idea to me that we could do that in a more serious way too not just for entertainment but for you know understanding economics or like even even you know how how have we built this city this way like let's say we're going to redevelop a certain area what would the impact on people be like would it have any weird externalities that we're not anticipating right now like maybe you could simulate that with some agents uh in a like a long and and figure out like in the next 50 years how's this area going to look uh things like that but not only that so I so the um the the podcast that Connor and I recorded about um these degenerative feedback loops that were what can this lead to right so in in one example that I that I uh often give us like in the form of newspapers so that we could say like you can get to a situation where um you might be reading a uh a newspaper that just shows a completely different article like as in the text in the article based on my preferences and my knowledge um as opposed to what is shown to some somebody else so if there might be an article about specifically highlighting something about you know maybe machine learning then for John for you it might show more detail uh um or or agenda for you and for me it might be um you know more holistic view or it you know tell it to my interest but I could imagine that we're going to see platforms and I I would also not be surprised if that starts with um uh with for example newspapers or maybe music or movies those kind of things where and then building on what you just also said Connor is the um that that just is completely different at some point for for everybody else that is not even like a article that's different but just that you just get your completely owned space in in the news realm for example or in in entertainment for those kind of things but you just see things that are tailor-made to you and and where there might even be agents that help you navigate to that world that is there made for you so I I can definitely uh um I I can I can definitely see that I can also see the scaling uh uh issues that might come with that agent but that's yeah I love the I love the idea of of dynamically adjusting the the not just the content but also the modality like if if you maybe prefer reading a long article or you prefer watching a short video or something like that there's like so many different ways to consume anything really and I guess right now you just have to sort of either you pick the largest user group and you produce it for for that particular group or you produce multiple things but the idea of just having the content just be there just once and and have the user on the Fly turn this into a video or a text or a blog post or scientific article that that sounds really enticing as well yeah like I could imagine a future where I'm watching a video at home and then I need to drive somewhere and so like it seamlessly transitions into an audio only version version of it but like expands the description of stuff so that you don't need to see the visuals and then you know maybe I I get out my car and I go into a coffee shop and it swaps to like a text version that is easily readable on my phone like so that the information is the is consistent between all of those versions but the the like actual delivery mechanism has changed um I think also so like this this idea of everything is going to be custom tuned to us there's also this really interesting alternative uh like thing that will probably happen in parallel which is like what why do we consume art right now it's like movies or something like I like to use Wes Anderson as a good example like his movies are so different from other movies and like from what we would have created ourselves in our own imaginations like those images and the colors and stuff that like we we kind of it's delightful to like live in someone else's mind for a little bit like that's kind of just interesting to humans I think to people and I think that like the power of these generative models is going to be that like you can have are people with like you could go go see someone else's hyper-tuned version of the world and that would be probably extremely interesting and stimulating for people and so we're all going to be able to be our own like Wes Anderson like we'll have beautiful amazing stuff that we could share with others so they can kind of visit us in our worlds and stuff um I think we're going to see just a lot of really good art coming out in the next this interactive art things like that yeah yeah and it's nice and so also something purely by coincidence that we discussed yesterday also with Connor is that the you often see um there's this this uh this author I really like um or Bruce Sterling who writes about these ideas about these Concepts and then he says like if you want to see these Innovations then look at music it's like often it's first happened somewhere somebody does something with you know music so it can be generative music or those kind of things and then it slowly trickles into other art forms and then it trickles into into into business and so it's um exciting to see what will happen but I I can I can see this this is just a um I can just see this happen it's like not not very hard to imagine so yeah thanks for sharing this is great yeah I'm sorry I think so much about that I because I think a lot about weave yet we have ref to VEC which is our approach to personalized embedding where it's like we use the graph structure of with users and products and I love what you're saying like if the four of us were to watch a movie together we could maybe average our four embeddings and then search with that Vector so I just yeah but anyways this is super exciting yeah and Bob you at first I remember you'd pitched the idea with uh Marco and Chris of like an embedding that you take everywhere you know like your digital kind of embedding that you take all over the Internet and what that idea was so cool yeah exactly and and that that's that that's a little bit complex in the sense of then then that somehow needs to be aligned with other uh with all the other models but but the point of that was more like a a representation that you so the the example that I gave is that would be nice if you would have a representation so that if I to stay in in the the realm of art that if I go to Netflix right that I presented with my embedding or my identity and if I go to Disney plus that I again presented with my uh with my embedding or plural embedding whatever but conceptually that's the idea right so and as you can basically say okay this is this is what I'm interested in and then um that it generates stuff for you I I can I can see that happening we we we talked about you mentioned Wes Anderson the other day somebody made this this video I saw on Twitter uh with like this generative Star Wars movie with in the style of Wes Anderson um and I think what's very interesting I can now somebody made that right so somebody was rendering these frames and then you know probably somewhere in Final Cut just you know got the got these things together that is just that is just work that can be automated right so um the time that you're watching a show on Netflix and that's just like hey in 15 minutes from now your next show will be rendered for you based on which that's that's so gonna happen that's just I I bet a nice bottle of wine on that that that's gonna happen in the not too distant future yeah yeah I like this idea of having this embedding you can bring with you that kind of describes who you are and like what you enjoy and what you're looking for [Music] um it's sort of like it'd be really cool if you could you could uh decouple it from like ad targeting in a way um like it's not what age am I and you know what part of my life am I in like I'm looking for more this is what I'm interested in seeing right now like being able to provide that to to the recommendation systems no matter where you go on the web and have like a universal system that you could just give them permission to use your personal vector and then are your personal embeddings uh because like for example yeah like I like certain types of restaurants like atmosphere to me is really important like how did the restaurant feel architecturally inside and like being able to go to a new city and then just pop that into the search and be able to like have it recommend places specifically for me in the context of that City that'd be cool yeah or the I think the example that I gave on a podcast I if I remember correctly um uh is the uh is it a car right so let's say that you rent a car and what you already see now with a Tesla that you can adjust it for like how you like to drive when it when it drives for you those kind of things that you just you know you just rent a car you go in the car and the car just okay this is the type of driver Bob isn't what he's interested in and hey maybe at like a busy day so let's just you know relax a bit more whatever whatever the thing does right but I think that's an example of um uh uh where such an embedding does not have to have specific information like my age or those kind of things but that it just knows what type of driver um uh that I that I am so it's okay but I always like to do those like I like to decouple these things in the in the brainstorm process so basically we're now in this realm of brainstorming about the opportunities and I always like to decouple to risk the risk from it because the sometimes what happens with these um uh with these creative processes when thinking about these kind of things if you then try to reserve a little bit in your mind for like okay this might be a problem then you're basically you hit a roadblock and you're thinking so I always like to go through the whole process of thinking about it and then getting going end-to-end and I had like these uh these these these speculative design kind of methods for instance okay now we're like end to end in the opportunity let's write them down or those kind of things and then let's revisit what you know potential risks might be or something but that's just more a practical thing but I I like the to think about these kind of things and to share this with world because that's really nice and I like I love what this uh this podcast turned into because I think um more and more people are getting inspired by these ideas like hey you know I'm working in whatever they're doing right and I can align this idea with what I'm trying to build so that's that's exactly that's very cool thank you I've very recently had an exact example of the serves personalized in a bidding approach my my Spotify contract was tied to my phone contract which I had no idea but when I switched my phone contract all of a sudden I had no more Spotify subscription so I thought like okay I already already pay for YouTube premium so why not give me YouTube music a shot and it was interesting to for the first time and I don't know how many years to to retrain basically an algorithm recommendation algorithm to retrain it like what music do I do I like and it's kind of like on the one hand I wish that I could have just taken my Spotify embedding so to speak and just put it into into YouTube music which kind of like opens up that question of who should own what data like is is recommendation data that's that's kind of specific to you as a user like should you have like a like a a a a a request where you can just download this and personalize it but at the same time it was also an opportunity to really think about like what music do I listen to Because Spotify recommends it to me as opposed to because I actually want it so like how do I want to influence this retraining so not just have your personalized embedding but also think about how it evolves and how you maybe want to want to change it yeah I think it's it's that's really and that Sparks a lot of ideas for me too because um I've I I use YouTube more than I like I don't uh watch TV I just watch YouTube for the most part I have it on my Apple TV on my like and it's it's I'm really it gets hooked on these like cons like it shows you oh nothing but gardening videos for like weeks and you're like stop showing me gardening videos I don't want to watch these anymore but you watch one video and it just like oh like the recommendation system thinks oh now this is all you want to see um but it'd be really nice to have like more trainability into these things or be able to just write out like what you want like hey stop and then have it adjust the ranking algorithm to give you you know something that's more aligned with what you want yeah it's it's it's it's fascinating it's it's interesting I noticed that I recently was using a um it was like one of these speakers that you can buy and they have like their own apps but they were connected to the to the Spotify API but they didn't have the you know the the suggestions and the ranking of uh Spotify itself and then you notice already how good it is inside spot if you're in Spotify because I try to find an album I just couldn't and I knew that it was just that this app this other app that was connected to this to the Spotify uh API just had like very basic keyword matching and if you try to find an album that has like I don't I don't even remember what I was trying to find but I found some pretty weird albums with the same words in there so it's like um uh but it's a good point and that next generation of really using language to just describe that or but I think also conceptually that we just need to go to a situation where you're telling YouTube something like we're not talking to each other like hey YouTube appreciate the the gardening videos but I kind of had it my garden is done right so I want to see something else yeah exactly yeah I think just just the I remember the first time that I put something in a that I that I that I made a prompt that I wrote a prompt and that's something sensible came out that I remember like whoa I need to get used to this Paradigm Shift of writing a prompt and not doing something in a certain structure I really really really needed to get used to that so but I think the um uh um so even my point is even if these platforms get to the point where they accept that and I can actually properly do that just changing people's behavior that they start to interact with these machines um uh um uh in a way that is like basically another person I find it fascinating I once there's actually a I think it's like a paper maybe it's like a psychology paper or something but but it's like that somebody published about the question that if you interact with these machines if you if you should say please there was like a whole thing that they were and then it was like related to Children right so if you for example if you ask something from another human being you'd say please but um you know should you teach our children to also you know politely ask the machine to do something for you it's it's a it's it's interesting it's exciting stuff that's happening well who have you have used please before in chat GPT I'm definitely guilty I'm not decided on whether I think uh even AI like I think definitely not at this level is AI like actually conscious but uh some some people I you know I respect their opinions on things they're like yeah maybe we should have a more open mind about what Consciousness means and I'm like okay well it doesn't hurt to be polite at least you know like if some super race of aliens out there comes into Earth and they're like 10 000 times more smarter than us if they said please to me I would be like you know okay that'd be nice great yeah thank you I think about it saying thank you would be worth the extra request to the API though yeah so like you know so I think actually we could get to this kind of instruction fine-tuning database lookups sort of or I don't know what to call it that's a bad that's a bad way to putting it but like being able to create a like a model that you just say hey can you downrank gardening videos for me or like say it in a more natural human way and then it would translate that to uh things that could go into the call to the search back end that like downranks things with certain keywords like you might know it might know how to make the database do what you're asking it to do like maybe this is something that that you guys could pursue um being able to just take a user's like contextual prompt in and then translate that to database you know query features that that make the search more relevant to what they're actually looking for Beyond just the query yeah and that's actually that's that's a that's a very interesting point and and um bring something else uh um to mind is that something that is super exciting also for me to see is that what's kind of new in infrastructure is that or databases specifically is um no models no effects of database right that so it's like they they go hand in hand and um the people that are building these models and I work on these models we we're working with a lot of them and just just as you just did John and we're getting suggestions I was like yeah how can we do that from the perspective database and and how can we and then what's the what's the where does the database start why does it ends basically those kind of things and also with the models I think that is something that I'm super excited about because it just basically means more people working together on solving these problems and um in the end yes it's exciting uh but it's also just people working together right so that's yeah that's that's you know I'm I'm that's something I'm maybe even the most excited about so that's that's cool yeah and then we can like imagine a future where there is no distinction between your database and the model and it's just one thing like that you say here's a document insert it and it does and then you say hey get me documents that match this kind of Query profile and then it just pulls them out or or you can mix in like you have images and documents and audio and it's just all just like a blob but it's an intelligent blob that knows what's in it and knows how that stuff relates to each other like you won't even have to explicitly create indices yourself like it'll just do that based on the data that's in it um yeah that's actually why I'm why I'm the the um the term Vector database right so the term factor is and I I get it I understand why um it grew into being you know being cold like that and and why we call it that but actually I hope uh that in the in the note to this in future um our users don't even have to worry about the fact that it's an embedding that's being stored so the example that I often give is um if you use a traditional search engine do you know what kind of indexes starts when you search for something most users don't so and I hope that we get there that that we get there as well that's also why why I think it's important that the the accessibility to the models but also to the database so that you move as much to a natural language as possible um that that's important because that just gives a lot of people access to use these Technologies in their stack rather than um you know complicating it with the kind of language and if people need to learn what embedding is and those kind of things so it's a um uh so long story short I I agree with you John I hope that at some point this just becomes like one Big Blob of you know stuff that people can just oh I want to use this for my next project or whatever and then it just embed it regardless if that's like in in the cloud on their laptops in their in their phones in the cars I don't really care about this but that's the um I I really hope that there will be um that that will happen sooner than later I think to some degree we almost already have that if you depending on who you define as the user because for us from our perspective it's typically the one the the database operator or the one that would spin up bb8 but the end user that just sees the search interface what are the odds that they know that there's a vector search going on like if you use Google right now what what how much do you know about what's going on under the hood so and I think that as as more and more vector and and similar search solution make it into production that also means that more and more end users are experiencing many basically the benefits of vector search without even knowing that it's that's there yeah I like this like abstraction concept from computer science where you just kind of handle something and then that's no longer a consideration you need to make and and you can think of it as a different thing like like a key value store you're not thinking about how like those the data is being like laid out in memory and stuff you just think of it as I put a key and I get a value out and I think like we're going to keep abstracting stuff higher levels so that you know right now the abstraction level that that like software Engineers think uh about Vector databases like I need to embed vectors and put them into a certain place and then I need to you know do do lookups like whatever whatever but I think like at a certain point we're just going to have this again like the abstraction layer is going to increase and it's just going to be like a thing you can put things into and get things out and you're not going to think about anything about how it's actually works on the inside um and then then what's possible right so like now you don't need to think about those things in order to build a build an app and who's going to be able to like we're going to be able to think it higher and higher levels which is really I think that's where like really interesting things come out and you kind of see that starting to happen with the llm community uh like they they sort of treat the vector database that they're using or you know maybe a hyperdb like numpy array cosign lookup thing but they treat whatever that is like they don't actually care how it's implemented and that's why sort of they'll just pip install something and then use it even if it's sending their vectors out to a third-party like server just to do search on a 10 10 documents or something because they just want to think at the higher level of I'm instructing an AI to do something it needs to be able to like have some data and look it up um and so this is kind of like the trend we're going into yeah exactly I mean it exactly and that's like um I think this is gonna go for like at least the coming but you said 25 years probably that's gonna uh that's gonna happen so thank you for sharing this it's super exciting also what you're working on because the um we of course we besides of course hyperdp we didn't do with the other things uh where that you were working on the in-depth so that's nice thank you for sharing yeah thanks for having me on I really enjoyed talking to you yeah thanks so much thank you thank you so much John thank you so much to John dagdollen for joining the wevia podcast to talk about hyperdb and all this super exciting work on scientific literature Mining and Material Science please follow John daglin on Twitter at jmdagdollen thanks again John ", "type": "Video", "name": "hyperdb_with_john_dagdelen_bob_van_luijt_and_etienne_dilocker__weaviate_podcast_46", "path": "", "link": "https://www.youtube.com/watch?v=85VZFQw_7Oc", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}