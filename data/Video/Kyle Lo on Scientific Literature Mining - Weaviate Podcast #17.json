{"text": "Thank you for watching the 17th Weaviate Podcast with Kyle Lo! Vector Search enables us to find semantically similar items in ... \n[Music]hey everyone thank you so much forlistening to the vva podcast i'm superexcited to be welcoming kyle lowe aresearch scientist at the alleninstitute of artificial intelligencekyle's work on applying nlp tools theapplication of scientific literaturemining has captured my interest honestlymore than anything else in deep learningresearch to give a quick preview kylehas worked on papers such as cybert apre-trained language model forscientific text tldr extremesummarization of scientific documentsfact or fiction verifying scientificclaims and a data set of informationseeking questions and answers anchoredin research papers just to give you aquick sense of some of the things thatkyle has worked on in the space ofscientific literature mining science isbecoming more connected and open whichis good but exhausting for researcherstrying to keep up with it kyle andcollaborators are developing models fortasks such as summarization questionanswering and even fact verificationthat facilitate this problem of keepingup with the literature and in my viewwill greatly impact the efficiency ofscience kyle shared his insights in thepodcast on building these kinds of datasets and the lessons learned in applyingcutting edge deep learning algorithms tothese problems to return to wev8 andvector search a bit i first became awareof vector search when researching deeplearning applications for covid19 i sawthe co-search system from salesforceresearch which uses vector embeddings ofqueries to match them with vectorembeddings of scientific documents manyof the data sets kyle has worked on areavailable on hugging face data sets andcan quickly be loaded into we va toexplore vector search in scientificliterature mining so with that saidenough for me i really hope you enjoythe podcast and as a quick reminder ifyou enjoy this topic you may also likeour second we vva podcast with charlespierce who similarly discusses his workon scientific literature mining hey kylethank you so much for doing the we vvapodcast hey uh good to be here thanksfor inviting me so i think a reallygreat topic to start this off would beif you could open us up with what isscientific literature mining and justthis kind of problemyeah so scientific literature mining isum essentially like a class uhthe goal is to take the breadth ofscientific literature that scholars havedifficulty keeping up to date with andreading everything that's beingpublished and trying touh apply text mining techniques nlptechniques umthat people have been developing to tryto make scientists and scholars liveseasierso this can be throughextracting useful bits of informationfrom these papers i could be summarizingthese papers it could be building toolsthat help scientists discover um what'sthe right papers or discover new papersthat they wouldn't have read ordinarilyit's kind of this broad field of tryingto make sense of large amounts of usefultextyeah i think it's one of the mostinteresting applications out there and ii personally have definitely seen thisproblem of trying to keep up with theinformation overload of trying to keepup with all the cutting edge sciencesis such a daunting task can you tell meabout your kind of progression in yourcareer and what led you to scientificliterature mining and kind of how youuse your own intuition of kind of likethis meta thing of being a scientist andthen studying theinformation acquisition process ofleveling up your own sort of skills as ascientist yeah that's interesting so idid kind of stumble into this field umlike when i joined the the when i joinedai 2 um i joined right into this somaticscholar team and i had like no idea ihad no nlp experience i had no textmining experience umi was just interested like umi likedi liked reading papers um like uh i hadkept up kind of in my professionalcareer uh still reading stats papersmachine learning papers umanduh so i guess there was always still alittle bit of like attachment to like ohit would be nice if this was easierbecause reading papers and keeping withpapers is hard umand uh so thewhen i joined and they basically pitchedme this idea of like we'd be buildingtools that makes this thing make thislife easier i was like i could use thesethese things that we built this isamazing and that's kind of how i gotstarted in it because it's like an easysellumand uh i guess likeyeah in terms of like the relationshipthe the kind of the interestingcircumstance of like i'm also like auser of the tools that we're like tryingto develop umit definitely makesuhthere's some things that are definitelyeasier because you stay motivated a lotbecause it's like at the worst case umwhatever you build there's at least outat least i'll want to use it even ifnobody else wants to use it it'll atleast be useful for me probably and soumuh you know it's like i don't have tospeculate too much um and maybe there'slike you know like10 people out there who also have likethe similar uh way of consuming researchas i do so there's not as much worrythat i'm likehallucinating a task or a tool thatnobodywould possibly use umthere's always a little bit ofconfidence thereandthough there are times where it's likeam i designing systems or am i workingon problems that are really specific tomeand like nobody and like i reallytailored to like how i do research orhow i consume literature and like maybeit won't generalize to other people sothere's that kind of balanceandbefore kind of stepping back and gettinginto concrete works like you know cyborgbiobird and all sorts of the thingsyou've done that we're going to get intoi do want to kind of stay on this alittle more and ask this kind ofquestion aboutyou know you're describing say itreminds me of like developer tools whereyou're building something that youyourself would use and it really helpsyou guide the intuition and somethingi've always been curious about as i'vebeen reading and i've read quite a fewof your papers now about uh doing thescientific literature mining and i'mcurious about like the biomedical domaincompared to the deep learning domain andi know you have a paper uh q asperinformation seeking that is annotatednlp papers compared to a lot of thisbiomedical stuff so i'm curious fromyour perspective on developingscientific literature mining as you saylike i'm the user that helps me guide myintuition do you think that it's betterto make progress on likeyou know kind of the recursion again ofnlp for nlp papers or this kind of nlpfor uh biomedical or say physicschemistry like that kind of thingyeah so this is this is a really um wecould talk for a real long time aboutthis because i think this is also like asubject of debates on that team umthere's awhether we a lot of projects umthat i work on like i guess like my teamworks onwe try to work on techniques that arefairly general so it's like notnecessarily specific to biomedicalpapers or like you know calling itbiomedical papers is actually overlybroad it's like half of our half ofliterature is like biomedical so atany one sub subfield is bigger than allof nlp combined so it's like umuh working on just like nlp paper soworking on just like ai papers computerscience papers this only we all havelike two million of these um biomedicalwe have like 40 million of these papersumsoumworking on any onetargeted field um the more you focus ononetargeted fieldi think you startdiscovering really interesting phenomenathat are specific to that fieldand you start um andthings that are like really critical toresolvefor there to be adoption by people inthat fieldum so likefor example i i just was dealing withthis uh recently umeven the basic task of just likeextracting references from a paper likethe bibliography section extractingreferences from paper and then linkingthem to a database ofknown papersand that's how systems like smashscholar google scholar web of sciencelike all these pubmed even like all alot of theseaggregators that report citationstatistics either they get it directlyas metadata from publishers or they haveto extract it and link it themselvesbut physics papers a lot of them don'tinclude titlesin their bibliography entries and soif you develop a tool that's justlooking at biomedical papers or computerscience papers you would develop a toolto think actually title linking is likejust matching based on title gives youreally really good performance um andthen you would completely like not buildanything that's useful forthe physics domain and soum it's like these kind of like littledetails as soon as you start focusing onparticular disciplines umyou you realize that like this is likeit's likemake a break it's like table stakes ifyou don't resolve this thing nobody inthat community will use your tool umbutuhbut the problem with this is like isthis like a scalable way of doingresearch like you know like you couldimagine like discovering a lot more morewith these things um you can spendalmost all of your time uh trying tojust like deal with this deal with thisand kind of doing uh new tasks each timeyou try to adopt like break into a newdiscipline uhsothere's a little bit of a juggle of likeshould i focus on biomedical only um ifi focus on biomarker only you know it orlike computer science only it's like ait's a familiar domain i have a lot moreof experience i can build tools thatwork for thisbut if i spend too much time justfocusing on thiswhen i is it like like is it likeredoing most of it uh when i when i jumpto another when i jump to anotherdiscipline or are my techniques actuallyfairlyapplicable with minor minor tweaks um asi moved into some of this discipline soum[Music]yeah i guess different people on theteam have like different strategiesabout what they feel safe committing toum early on uhfor like i'm just gonna specialize onthis discipline and hopefully it'llgeneralize later and other people golike actually no i'm gonna start myprojectstrying to pick three diverse disciplinesand i'm just making sure my tools workacross all three even if that means likea lot more upfront investmentinteresting so it sounds like uh youknow like different disciplines ofscience have different challenges ofreproducibility and different challengesof communication as you mentioned thetitle linking for physics papers is uhdifferent from say computer sciencepapers and um and maybe we could step alittle back from the idea of saydesigning very specific tasks forparticular domains but if we could kindof say that this general framework ofsay question answering or summarizationis aperfect task set up let's say and it'sall about uhjust the particulars of the domains andso i kind of want to come into thistopic of data domain uh domainadaptations say maybe things likesemantic drift as like the meanings ofwords change as you go from saywikipedia and then into uhdeep learning papers where maybe someword has some kind of different meaningso i i kind of really want to ask youabout this origin of i think you wereone of the first authors that did uhlike bio bert cyber this kind of umlanguage where it's data domain bert tocommunicate this idea of it's the burtalgorithm but it's been trained on thisparticular data domainyeah yeah so um i think bio bert wasfrom jin hyuk lee and others at uh koreauniversity and uhyeah no no problem there they didexcellent work um we were working oncyber at roughly around the same timeand thenwhere we were kind of going across justlike everything that we had in spanishcolor umandi guess like both of our groups weresort of stumbling to like this idea oflikeumwhatit's it's like impractical to to liketrain a bird from scratch really umwhich is like a train retrain aperfection each time you have a newdiscipline you gotta train bird fromscratch again umbut someone had to like kind of gothrough that kind of mostly engineeringwork to toto prove it out um i think at the timeit was actually kind ofuh those decent amount of pushbackactually unlike going going down thatroute like now we know that i was likethat was a good idea but at the time itwas just likewhy do you need that like areumwhat'sit's it's a lot of effort to try to kindof wrangle google's like kind of earlytensor like like like kind of firstversion of vert code um this is like prehugging face eraand uhand it wasn't clearwhypeople needed umthese like kind of discipline specificor domain-specific birds umbecause bird was trained on like such awide crawl of of of different link likelanguage like documents umanduhmaybe i mean you could definitely makethe case at the time i think like oflike different language birds thatthey're so different that obviously youneeded but like scientific textsprobably some scientific texts leakedinto the bird training corpus like whywhy do we need this specialized thing sothat was kind of the motivation for thisit's likeare wedo we believe in this idea that likeactually no science is hard enoughwhere just training on like wikipediaand like reddit crawls uh isinsufficient for picking up kind of thethe type of language that's used insideof the text and at least people on ourteam and people at korea universitybelieved in this uh that it wasdifferent enough to invest in thatrebuilding of from from scratch or atleast like spending a lot of effortadapting a birdumyeah i think that wasmostly an engineering effort and thetypes of things we found was likesurprisingly um there's more similaritybetween these disciplines than wethought um and i think we know more andmore nowadays that actuallyumvocabulary is one of those things thatlikeyou can get away with like not worryingnot trying to retrain your vocabulary umwe like foundbiobert actually does really well inbiomedicaltasks umeven with the original burt like kind ofwiki trained vocabulary so it's likethat's fine um obviously there's beennewer bio births like from msr there'slike i think there's like pubmed burtand there's likeuh there's liketwo or three of these at this point idon't remember which there's so manylike kind of clinical bert i think umandsome of them have retained vocabularysome of them don't reaching vocabularyit's kind of unclear it's not as likeobviousto us like what's the impact of havinglike a specialized vocabulary um andseibert'suh results were also just like thevocabulary is like the performance boostwasso small that like we weren't even sureif it's significant or if it was likedidn't matter because it weren't likelike just like rebuilding upper fromscratch because those are reallyexpensive umbut so there's something else that'shappening that's not vocabulary likeit's some something but likethe performance there was substantialperformance boost on these scientifictasks these biomedical testsfrom adapting aregular burp to these uh to ourdisciplines butum it wasn't a vocabulary it's in likeit's in something else in in the weightsor something i don't know umsoyeah i think the we've had somefollow-up work in in this space too withlike domain adaptation butum i think there's still room for a lotmore study on likewhy is scientific textdifferent or actually in what ways isactually maybe just the same and wedon't have to spenduh like a ton of timeadapting these bird models to handleeverything maybe just the bits that arethat are particularly particularlydifferent particularly different inscientific textum soyeah that's extremely interesting and iat the vocabulary tokenization level iguess maybe like if you have unknowntokens like maybe i don't know likelet's pretend like convalescent plasmatherapy out all right some phrase likesome gene or something like that rightnever appears in wikipedia so it's noteven token it's unknown and then youcompletely lose the info and it makes alot of sense why that would be ahorribleuh information loss from wikipedia tothebiomedical papers and yeah i agree withyou i think the like it's more of thatlike the latent representationsthat it can do that kind of reasoningright with that kind of thingand uh so one other thing i wanted toask you about on this topic is what whatyour take is on you know prompting gbt3to try to get it to be knowledgeableabout scientific text do you think thatyeah so we have been playing wikividiiii quite a bit uh it is it isit is like exceptional it is it'sactually kind ofumshockingly like it just behaves sodifferently from fromfrom models that we've we've dealt within the past umi think like the like one one of themost fascinating things about it waslikeuh i think you could prompt it for for aparticular behav like um for a responsewhere the response was like a listlike a little establishment oh like itwould say something like umuh here's three reasons why and then itactually like maintains coherency itlike it actually looks like bullet onelist something bullet two listssomething and sometimes it might evenlike refer back to oh yeah as a chain onto like kind of bullet two somethingsomething like there's there's likei've never seen that kind of like selffresh referential um behavior in in ingenerative models prior to gpd3umat the time i yeah and then and thenumbutit definitely doesn't still doesn't workon on scientific pe testsumwe've been using it for various projectslikeuh trying to generate concepts likedescriptions for conceptslike technical jargon we've been usingit for summarization we've been using itforactually for just for extractive qa oreven attractive keyway with from shortsnippets umand there's definitelylike it it definitelydoesn't workit works reasonably well but it doesn'twork comp as well as just like kind ofmore accessible models just like takebart and then fine tune it on a littlebit of data and it'll outperform uh sortof just like this pure zero shot promptbased uh gpd3 model soum it's definitely not solved everythinguh the really cool stuff is it seemsreally good at doing kind of mechanicaloperationsumso if you ask it explicitly tosynthesize like a tldruh from some input it's pretty good atthat i think it's because tldrgeneration fundamentally is very muchlike a pick and choose these things andand you can do a lot of copying from thecontext that you supply and and for themost part those tlr summaries arewill look pretty good um so it'sactually quite good at that um it'squite good at like kind of rewritingum so scientific text if you just try toconsume likedefinitions of terms if it's just likeextracted contextum you'll have a lot of thesefor example something somethingsomething or likeas we mentioned before it's like thesekind of dangling phrases where if youjust remove these kind of phrases theseextra punctuation ones stuff like thatit would actually look pretty goodthe end result will actually look prettygood and gpd3 is pretty good at thesetypes of mechanical operations just likecleaning up text to be somewhatself-containedumbut anything beyond that uh i think ourteamwould be hesitant to actually like putthat output in front of real peopleif i could dig into the details a littlebit about how you explore that do youare using gbg3 as an inference api areyou exploring say supervised learningand uh maybe i don't want to ask aquestion that has too many thingspackaged into it so maybe let me just doit wheneverwe're using the the public api uh so asi guess it's a fundamentally umwith like kind of thewith whatever prompts um that they thatthey supply with like kind of theconfigurations for like temperature andwhatnot and then um it'sit's the in context learning settingfine-tuned settingso could you tell me how you adapt thatfor extractive question answering is ityou give it it's like kind of the t5style where it you first tell it like umquestion answering the answer is goingto be in this passage like some promptthat describes the task and then it hasthe context and then the question andthen itgenerates it right and you have to mapthe generated thinguh like you do like exact match with thetext similarity of the generated thingwiththe ground truth answer is that theexact setup yeah that's right orvariations of that we may or may notinclude like thethe the instructions for the um for thetask itself or it could just be likethree examples five examples of likehere's the thing here's a questionhere's the context here's the here's theanswer as a string and then again likeyou said the exact match for evaluationor some per perturbation of like theordering of these things you you kind ofhave to mess with gpd3um prompt form uh quite a bit to get itto work reasonablyand i think this is a great transitioninto our next topic where i want to talkto you about the different uh tasks indeep learning andi remember i think you were one of thefirst authors that really or and yourteam of course that developed this uhtldr abstractive summarization and to meit's just such a remarkably high outputspace that you have to have likecompared to classification where you sayhave two labels and it's just oneprediction of two labels whereas textgeneration you have say 50 000 potentialtokens and then you also like unrollthat into like50 or so you generate like a longsummary uh what are your thoughts onthat kind of difference in output spacei mean it's a pretty big question butyeah yeah um so tldrs um was a projectwith isabel cachola who's at jsu rightnow as a phd student um and with uharmand gohan my co-worker umanduh that was an interesting one becausewe actually did go into the projectthinkingumwow this is like an impossible task umthis is actually like super difficultbecause of this it's so the output spaceis like 30 tokens and we gotta compressan entire paper like a gist of entirepaper into like 30-ish tokens umpeople can do this our people have beenable to do this sensibly because you cansee on like open review which is thedata set we used um authors are actuallywriting they don't teal the ads fortheir papers for otherreviewers to read soat least it's not likehumans aren't stuck at going like whatshould i put down that at least there issome answer that the authors will writethat and webut trying to replicate that with themodel seemsseemed like really dauntingumbecause of this word that like what itcould be anything like you could justput anything into the art and like howwould you know um but surprisingly as wewere studying more and more uhthe teal there's the authors tended towrite umi'm actuallymore and more optimistic about this taskbeing actually simpler than it um thanwe originally thought um yesit is the output spaces it is like athird compress a giant document into 30tokens yes it is generated text so youcan just say anythingbut in terms of likegenerating if the goal is to generatesometldrthat isenough informationto for a user for like a reader who'slike kind of scrolling through like asearch page if you're like googlescholar or something going through asearch pageor going through conference proceedingsor just looking at some author's profileand looking at a bunch of lists ofpapers umjust enough information that helpspeople make a decision as to whetherlike a more informed decision as towhether they should investlike look into this paper or notumthe theyes there are multiple right answersbutyou can really easily findreasonable sensible right answers fromthe paper itself and just kind of cleanthem upi think this is a function of just howpapers are writtenif you look in the introduction if youlook in the conclusionum even if you kind of search aroundlike topic sentences in the umin a within within the paper authorstend to write in a manner that isvery much likeoh they skipped over the rest of mypaper but if i had to give them onesentence to like so that they had atakeaway um i'll at least put like likea main sentence in the conclusion and sothe task really becomes kind of thiseven if this is kind of what the bookpart is fundamentally doing um it kindof becomescan you find like a reasonable inputcontext that probably contains thesepromising sentences that are alreadyvery tldr like or summary likeput those in front of a model and thenjust the model just really needs to kindof shorten itmove things around to kind of get it tobe short pithy easy to understandself-containedand that type of operation is reallysuitable for for for kind of these likelarge models that we have todayyeah it's super interesting that itseemed like the large models seem to beable to uh like decompose the task likethatand i'm i'm so curious your thoughts onthis because i it seems like you've donesuch an exhaustive coverage of textclassification like natural languageinference fact verification questionanswering summarization so i'm i'mcurious and then kind of with whatyou're saying is um you could probablyjust classify like three salientsentences in a paper to do an extractivesummary right and and wouldn't that bemuch easier to like label and kind ofunderstand itthanyeah yeah so so this actually stumblesonto a project that i'm working on uhwith um uh uh lucas oldangmi who joinedour team recently umwhich is like trying to identify salientsentences and papers so there's no wedon't have any work out there right nowthis is actually like pretty like ithink we are like a couple months intoit umuh it is reallyit is really hard uh to findsalient sentences uh onceumif you i guess like there's like thesekind it's almost like there's like thestep function for like if you're ifyou're if your criteria is i want tofind like one or two or three sentencesmaxthat really get at the core of a paperum and helps a person make a decision asto what whether to click this paperum that's pretty easy umpapers are written this is kind of likewhat i was saying the papers are writtensort of with these paper with these withthese kind of summary like sentencesspeckled throughout so you can look likea lot of papers just have contributionsections where they just literally pullit out like we did this we did this withthis and these are the three things thatyou should care about if you don't careabout anything else in this paper um andso if your goal is to up like a tldr andjust want to capture that type of stuffthen the task really is find thosesentences synthesize them into somethingthat's kind of legibleonce you start going a little bit beyondthat into going okay well i wantsomething that's a little bit more likean abstract or i want something that's alittle bit more just like uh maybe iwant like a block like generate a blogpostabout like to summarize this paperso something that's like cuts the paperdown in linked by half but not you knowall the way down to like a tlr that'swhen it gets extremely subjectivebecause we recently did like a a studyum where we had like 12 people on thesame teamuh annotate the same paper and this isone of our own papers so we were allextremely familiar with it umand just had everyone just kind of readthrough and select like if you had topick like thirty percent of paperto highlight that was like salient whatwould you pick and it was like asidefrom those few sentences or those fewpassages which would uh be primepassages for input to a tldr modeleverything else was just like hyper lowagreement like nobody could agree thatwhether some of the math stuff wasimportant so nobody agreed like oh thisis actually a super important detailbecause if you knew this field then likethis is actually like make or break forthis paper like and like umso yeah i think that's kind of how ithink of this like teal the art is easywhen you start getting to like generatean abstract maybe that's kind ofeasy-ish also i think it's less easy butit's easy-ish because there's just somuchdata free data out there and then whenyou start getting into like i wantsomething that's a little bit longerthan abstractumlike kind of like a blog post orsomething like thator like a kind of like a i don't knowjust likeeditorially type type documents forthese papers umuh that's when it gets just likeeveryone's gonna argue about what isactually truly salient sothere are so many questions i want toask to attack that i want to try tomaybemaybe keep keep with one that i thinkcould be quick and then i want to askyou about these kind of data annotationuh efforts which i think is extremelyinteresting but um so this denseannotation where you take a a team of 12nlp scientists and they eachuh densely annotate a paper like writeit write the comments right highlightwhy i like thisand then do you think that the kind ofidea of say semi-supervised labelpropagation where you try todeploy that team for say a hundredpapers and it would probably be anexpensive endeavor but and then tryingto bootstrap that dense annotation tonovel papersdo you think that would workmaybe maybeit's hard for me to say just becauselikeum i don't even understand the phenomenathat we're annotating yet like when whatis sort of like the the function thateveryone is employing when they whenthey're like received with the taskwould just annotate what you think isimportant and somehow they map likesalient or important toi don't know k different criteria thateveryone sort of forms and then theyapply that annotation i don't know whatthat is yet i don't know what whatconstitutes an importanceumi don't know how to break down whatimportant means to people yet so so i'mnot sure actually yeahyeah maybe like umlike a like user embeddings some kind ofmodel of you know kyle and connor aregoing through the paper and you havesome kind ofuh representation of kyle and conor'sbackground knowledge to help youinterpret the annotationmaybe some kind of flavor of it likethat maybe so like if you're bringing uplike kind of like user embeddings oruser representations um you're gettinginto this realm of like personalizationum and and i guess that's kind of what iwas saying is like i don't know if thisuh if personalization is the right placeto go nowum in general yes i like things to bepersonalizedto to peoplebut practically speaking if we had tolike start somewhere we want to startwith kind of what's the most effectiveuh kind of making progress on this taskandi don't know what personalization is theis the thing like it could be kind oflike tldr's whereuh if there is like ashared conventionin it within a community where everyonekind of looks for contributionstatements everyone kind of looks forthese and then then it's not about umpersonalizing to a person's preferencesit's about understanding what theconventions are for how to read paperswithin this community and then justhighlighting those so like the biggestexample of this is like pico uh the picoframework within umuh medical papers like clinical clinicaltrial papers this is like a conventionthat people the uh you know umclinical researchers medical researcherswho read these papers have developedand refined over years and everyoneagreesthat likeplus minus some variation of differentframeworks but like that like yeah for apaper if you really want to summarizereally quickly what this paper is aboutyou got to know what the p participantsareyou got to know like where thepopulation is you got to know whatinnervation is intervention is you needa competitor you need the outcome andjust like if you can summarize thisextract this informationyou can go through papers really quicklyor you cantoss them in database do some nlp tosummarize and aggregate and sounderstanding that convention uhis sort of key and i like building toolstaylor.com is really key for thiscommunity but and i don't know what thiswhat like a pico type thing would looklike for other disciplines yet or ifit's like you can't there is no ego forfor for understanding what people thinkis import uh salient and therefore youshould invest more in like personalsectioni i think that is just such ansuch a fascinating kind of thing thepico analogy and that that's kind ofwhat brings me back to theone of the first questions i asked youabout is this idea of should we studydeep learning papers or say are any kindof scientific papers because i thinklike with um deep learning experimentswe can kind of identify say the symboliccomponents of what's not so like wecould say um you know it's the bertmodel architectureand uh the learning rate scheduler isconstant like we can construct the dagsort of of the dependencies and theexperiments and say like here is the thenormalization layers this is what waschangedand like we could maybeyeah like extract those kind of graphsfrom papersand maybe those kind of graphs could forone it could quickly illustrate thepaper like here is the paper like it'sbert it's this data set is constant andthen here's the thing that was changedthe normalization layers this is kind ofexample and maybe that would be one wayto like communicate the papersyeah i so yeah what should we bestudying i guess like these papersor like this discipline of papers forothers i don't know um i i try not tohave like like i guess like like i guessat a high level like in kind ofinstinctual level yes i think we shouldbe working on making medical researchmore like easier to digest and follow umbecause that seems like it's like ageneral good thing uh for people um forlike like aipay first like i don't knowmaybemaybe umit's hard to justify that that that thatworking on our papers is more importantthan working on like helping like adoctor readfollow follow the latest clinical trialsum buti think um it depends on kind of whatyour goal is the way i try todo research and what i sort of umrecommend to my mentees is liketry to be deliberate in your choice ofwhich discipline to to to a papers tostudy just because any project withinour space is takes a really long timeannotation takes a long time you got tohire experts and soi would say dothe thing that actually actually allowsyou to like completelycompletely finish the study on thephenomena you're interested in sofor exampleum if you want to build something usefulbroadly useful and that's it and you'relike interested in just trying tounderstand how can we use these nlptools to build something that's usefulumpick and you have a medical collaboratorgo with that if you have if you don'thave a medical collaborator um then iwould hesitate to say that you shouldbuild something for medical uh for themedical population becauseif the goal is to build something usefuland you don't have someone from thatpopulation to work with like how do youever really know um youi would say in that case build somethingfor yourself because at least you areyou know what would be useful foryourself and that and i think that canhelp people keep focused if you'reinterested in studyingparticular language phenomena then youshould like that you think areinteresting uh for example math and likesymbols and stuff uh in math papers umdefinitelyjust pick the papers that have as manymuch of that phenomena as possible andstudy that umsoyeah i guess it depends on kind of whatyou're trying to get out of what what'sinteresting and what you're trying toget out of out of out of this work umso um yeahi think my thesis on this is maybelike maybe two grandiose sort of it kindoflike um i guess kind of my motivationfor this is i think this is somethingthat dennis has said when they ask himuh you know why ai and he kind of sayswell intelligence is the thing thatsolves all the problems so if you cansolve ai you solve all the problems kindof in that sense that it's kind of likeand i and i definitely think that yeahthinking that like you can just build asuper ai that becomes a doctor just likeyou know without any kind of human inputi obviously understand that you wouldneed that kind of connection to makethat uh leap but i guess the kind ofthing about it that has really capturedmy interest is say openai's codex andthe ability of language models towrite the code so they so with theability to also write the papers digestthe papers and then also kind of likewrite the pie torch code and then youcould kind of completely encapsulatethem in that environment with thedatasets that were because we're likejust kind of trying to figure out how toincrease like the squad benchmarkimagenet benchmark right so so it's likecompletely encapsulated in thatenvironment which you say would be auseful generalization sort oflike the idea that it couldwrite its own papers ohpapers um there's actually some i thinkuh there's some interesting work fromuh kevin knight's group uh usc i thinkkanjigroup at uiuc who do likeautomatic paper writing i thinksomething called paper robot from uscand then there's like hangi's group doeslike automatic review writingumi i would say that thoselike i'm i'm actually a huge fan ofthose works um i don't view them i'm nota fan because i like i think that shouldexist that we like need systems thatwill write papers for us um but i dothink likefrom a research perspective from like uhwhat can we get out of understandingwhether these these like are like modernai toolscan and can't accomplish when giventhese sort of like kind of absurd tasksgenerate a paperuh i think just studying that helps usunderstand limitations of these toolsum and if and my takeaway from lookingat those works is likeokay what are things that these that wecan kind of reliablytrust these these models to to get rightyou know autocomplete type stuffautocomplete for boilerplate right isthe thing that people love the most umfor for forlms that write codeand so what is the kind of theequivalent of that for a paper umand then how can we buildtools uharound just this functionality butallowing a human to seamlessly stillkind ofdo everything else i guess it depends onhow sci-fi you want to like your liketimeline is if you're like if you wannaif you if you don't mind your researchbeing used likelikekind of super super far out then yes ithink you should totally you couldprobably work on like let's justgenerate the paper and end and not worryabout the human componentfor me i'm more of like a i'd like tosee stuff being used fairly recently andso uhfor me it's more about likeokay what can we figure out can weunderstand these to what extent thesetools are useful now or in the next fewyears and thenpartner with like kind of experts umto build kind of these like synergistictools uhyeah yeah i agree i think i i did getkind of carried away with the animationyeah i obviously love the human computerinteraction i'm not like trying to putmyself in us out of a job but with theidea of like just completely automatethe role of the scientist in the middlebut maybe we could um come down comedown from the clouds to can we automatescientific reviewing in that question ofcan tldr abstract or summarizationlike what kind of tools do you inv iknow obviously symantec scholar is likea platform with all sorts of things canyou imagine like you upload your paperand then it you know all these taskslike cytance classification whereyou know the model analyzes one of yourcitation sentences and then gives yousome feedback for your for for the humanin the loop for the sake of here's howyou write a better paperum that would be cool um i think i thinkanything in like the assistive writinguh space would be really interesting umour team hasand i guess like my own interests alsoare definitely right now on the assistof readingum just becausereading papers is really hard um and itseems like there's some reasonablepromiseum for nlp toactually help make papers make paperreading a lot easier um soandi also don't think thatum working on reading and workingwritingare completely like disjoint paths forresearch like i do want some sort ofassistive writing attack when i'm likewriting reviews when i'm umwriting papersum when i'm writing like a tweet about apaper thatumi think that like the tools needed to toenable that type of assistive writingtechnologyunder the hood probably has to uh islikesort of doing some sort of likecomplicated reading comprehension termextraction definition uh uh likedefinition generation linking toto of these terms to like other paperslinking a freight claims to other paperslinking terms to wikipedia like thesetypes ofuhoperations that you would kind of youwoulddevelop if you're trying to build anassistive reading uhas well so you know you can imagine asi'm writing a review i am actuallyreading the paper and so whatever helpedme write this thingthe pool probably helps me read it aswelland so hence right now the focus for usis like let's help people read and thenmaybe once that's pretty much let's helppeople let's add some more stuff andstart studying how to help people writeumbut yeah i think reading is like rightnow just like the the number one thingit's umi think it should be super interestingfor nlp people who are tired of workingon like little short textsyou know titles and abstracts aren'tsuper long umuhfull scientific documents are extremelyrich with like really difficultphenomena and models fall over whenyou're trying when you're trying to doanything on these papers umlike if you look at the performance ofour best models on casper it's likeawful um and casper is like uh can youanswer this basic question from theentire full paperthe papers have figurestablesum things are kind of out of orderbecause like there's like layoutsthat's part of part of part of um partof papers like there's like positioninginvolved of stuff there's sections andsubsections so things are organizedhierarchically there's likeinterruptions because of footnotesand so likejust like papers are this really richstructured document and the way we'vebeen building these nlp models is sortof just assumes that we can just liketreatthese documents that one kind of giantstring uh without her structure and soonce you move applied techniques builtunder this this this line of thinking onthese large paperseverything kind of just doesn't work umand so that'slike i feel like that should be likeextremely exciting topeople to work on um in addition toum this is like prime time to to applyif you're interested in informationattraction extract information from itfrom extract these terms extractrelations from papers and highlight themso that people when they're reading theycan actually see what's going on andthat's like a great application ifyou're interested in informationinstruction if you're interested inlinkingto databases yeah like literally it'sawful for me when i don't know a term atthe copy paste that term open a new taband google and search and find the pageand then jump back to paperapply your anti-linking thing to justlikemaking that just like a one-click thingwithout having me leave the paper solike there's like a lot of really usefulopportunity to apply nlpuh nlp techniques that people have beendeveloping forever um to pay person toand then we make our own lives easier sothat'sa nice bonusyeah and iwell so firstly i i will i really dowant to return to the topic of gettingtogether experts to annotate sayknowledge intensive things like gettingtogether 12 nlp scientists and i thinkwith q asparagus a team of at theuniversity of washington right and allthat and i really do want to get back tothat topic but quickly you touched onsomething that is just so important towe v8 and our vector search communityand as we've been partnering with ginaai we've seen their doc array and howthey're organizing this idea of you havea very complex object that you want tosearch through so you have to decomp youhave to segment it into differentembeddings so as you mentioned like youwould want say an embedding for theabstract and embedding for you not evenlike the whole introduction rightbecause like two pages of introductionto uhfirst you gotta put it into like 512tokens right and you can like averagethe embeddings of the 512 tokens but andthen you have say images from the paperand then like the tables even kind ofneed to be formatted differently likethe math for latex equationslike this kind of segmenting and i'm socurious about like overall how you'resegmenting are you right now is theapproachuh use something like lin form or likeyou know the try to get the sparseattention so that you could maybe try toput the whole thing into one transformeror is it some kind of hierarchicaluh you know segmentation and thenpropagation up however that mightworkhonestly nothing quite works so um okayokay that's not true um uh i mean umfrom from the folks that uh from like isbeltagey and armand gohan matt peters uhyeah too um with long former um we'vebeen using long-form for a lot of ourexperiments and sort of the question isi think that you need like even though ithink that you need some new modeluharchitecture that represents that canrepresent like this likethis like heavy amount of structurethat's in these papersumthat's still a hypothesis like i couldbe wrong about thatand it could be that models are just sopowerfulthat today that you can just take thetextlinearize it in any arbitrary way justlike take it and turn it to one giantthing and then just applyand like maybe with like a few trickslike add little set tokens between eachsection or something and maybe that'slike thethat just like is just better resultsand better performance than any sort offancy hierarchical language models likearchitecture you could come up with idon't i don't know umi think uh i think it's too early totell because there's not enough tasksthat actually make use ofum there's not enough tasks and datasets on scientific papers uh at scalethat allow people to study this problemright like if you can't measure whetherit's working either you you can't answerthe questionum there's just not enough peoplestudying the problem so you know i likei like having i would trust kind of iwould have more stronger opinion if ihad likeyou know a dozen labs just like kind ofsaying like this is the right way i'mlike okay yeah it seems like they'vethey've been studying this um quite abitumand uhyeah i don't think there's just beenenough experimentation um just justabout different ideas um like if onehierarchical model architecture itdoesn't doesn't work does that mean thatlike it just doesn't work or like or isthere some some other thing umso there's just like too much work thatstill needs to be donefor me to really have a strong opinionnested i i haven't i have a guess thatit that something special needs tohappen especially with latex equationsand figures and tablesumbut there's also like a an ink languagelikemaybe it's just about vision languagemodels maybe just like taking likescreenshots of of of tables and figuresand then just like toss them intosomethingand then do like a vlper type thing andthen that's itumso so i don't know yetyeah seeing the demonstrations offlamingo on twitter deepmind's new techimage uh captioning thing is definitelymade me think that that idea of the yeahlike screenshots the tables i'm it'slike you're pretty goodyou're good i was likeso this is one of those things likei invest a bunch of time trying todevelop like custom architecture that ithink is tailored science documents workor is it really just aboutuhsmashing these things togetherpre-training and then you knowwhere to goyeah and um so i'm trying to think of ifthis would be too off topic but maybevery quicklylike as we've been studying approximatenearest neighbor searchwe've kind of been thinking aboutwhether this could be the answer to verylong range attention like if you coulduhyou know have the the query keydot product instead turns into anapproximate nearest neighbor search withan enormous set of key like the keys arevectors right and you're so you matchthe querylike quickly i'm sorry this is kind ofdistracting from another topicdo you think that kind ofapproximate nearest neighbor search intotransformers could maybe be the solutiontoextremely large memory transformers orlike extremely large attentioninputsumi guess like a like aa cheap a cheating answer would be likemaybe maybewhat it means to be seen so so i guessumwe uh we have some peopleuhkind of joining recently looking intolike we were interested for a whileuntil like rich people augmentedlanguage models which i guess kind ofhas that idea right it's just like oh alanguage modelwe don't need a language model like thislike enormous language to memorizeeverything about about about text weshould be able to offload some stufflike knowledge to some other componentsuh embed them somehow and then i guesslike retrieve um when it's needed umthis seems like a really appealing wayof viewing the problem of likeespecially in science new things are areare being discovered new terms are beinginvented new ways of i guess like newfacts about the worldare constantly being being generated anduh it is impractical to just kind oflike keep training well maybe it'simpractical to keep training just likelanguage models that kind of keep keepup to date with this thing sorepresenting it separately and thenhaving a language mod that kind of likeand i'll either dot products into theseembeddings or or or somehow justretrieves passages and then learns howto encode those like text patches intoembeddings on the fly and thenincorporate them into the language modelseem promising to me umi know there's like the kill benchmarkwhich evaluates uhlike kind of like knowledge intensivemodels um and the retrieval augmentedmodels are sort of doing the best on thebenchmark but uhdefinitely i guess still need to seemore of it for science um and i don't soi don't knowit's like stillright um this this line ofworkyeah that's really interested in uh whenmalt piece from haystack came on theebay podcast you mentioned that thatapproach of retrieval augmentation itallows for better interpretabilitybecause you kind of see what it's usingto make its prediction it's better forupdating itand then i think uh i think it's betterfor maybe removing the biases that kindof thing because of kind of entangledinterpretability i think maybe there wasa third thing he had mentioned that i'mforgetting now and i actually read arecent paper uh the biomedical enhanceduhor biomedical evidence it's called likeliterature augmented clinical outcomeprediction oh i think that's fromakangshan uh and tom hopeless wong fromfrom our team probably yeah yeah yeahand it's that kind of thing where youretrieve from chord 19 right and ithelps you do the clinical narrativecompletionyeah that stuff is really cool yeahyeah that's so interesting andso sorry so let me come back to what wewere talking about about this idea of uhtaking in the whole paper as inputreally quickly so it sounded like maybeyour issue with it would it would behard to build the data sets maybe andand so i wanted to ask your opinion youmentioned open review earlier as a datasource and i've seen you usingtechniques like this beforeuh so so do you think and that like iyou know peoplereview papers they compress the paperinto you know strong rejectsexplanationor neutral like uhso do you think those data sets and fromplatforms like open reviewdo you think that could bea way to get that going and yeah yeah soiumi'm like a pretty strong believer inderiving data sets from real platformswhere humans are like actually using theplatform to do something sensible um asopposed to kind of like contriving atask and then hiring people to do it umso open review i it i think is a greatresource umfor forreallive sort of like um what are they uhblinking over like like natural orum it's fine but yeah you get the ideait's just like it just seems like it's areally greatplatform umi think my only concern with it is justit's just kind ofnot big enoughuh so you know more success to openreviews so we can get more databut it's limited to particular fields oflike particular computing related fieldsum so there's no like open review forbiomedical literature clinicalliterature um sothat's kind of that's kind of the worryso i think it's a combination of liketrying to be opportunistic like findingthese like really useful data sets andalso investing in techniques that helpyou likenot have to re build that open reviewfor a new discipline each time just likeyou want some way to do an adaptation tothese other friends when there is nodataandthis kind of takes me to anotherquestion i want to ask you related tothe uh q asper work where i think it wasyou you hire uhgraduate students at the university ofwashingtonto uh yeah yeahso so i was wondering about like if youhad a platform that would you know paypeople directly to do this kind ofexpert annotationwhat do you like i guess the questioni'm i'm kind of asking you islike with your papers would you bewilling to pay for such a review such anannotation of your own paper and thensome maybeyou know platform that ties into uhi like you also opt-in to let this beused as a data set such that thelanguage model could be trained on it tomaybe supplement you not to like put youout of a job as the annotatorwhich i guess this kind of idea wasdoing from the start right it's like ifyou're yeah yeahumjust uh just so i can understandquestion are you suggesting that weshould be paying reviewersit's an interest i think it's aninteresting thing because it theincentive system now does seem a littlebizarre to mebecause you're kind of reviewing it forlike you know the sake ofyou know like scientific rigor which isgreatyou know which is great but if i feellike if you added the payment i mean youmight get the bias towards like i wantto write a good review because they'vepaid me for it orsomething along those kind of lines buti mean i'm curious what you generallythink of it it's not like a startup thati'm launching or anythingno that's really that's interesting[Music]ii generally so maybe spicy take but likei definitely think um reviewers shouldbe compensated for their work becauseit's a lot of work it's a lot ofexpertise work in the community and likepublishing communities rely heavily onthisum we're sort of relying on volunteereffort right now obviously there's somelike i guess like for academia we needtoyou knowmoney like this destroys the sanctity ofit but i don't know we should pay peoplefor their labor soum i definitely review like review orwork should be compensated i just don'tknow if i don't i i'm not sure aboutwhether the author should be paying umthe reviewer exactly orcoming from yeah or like is it likethrough fees or something like that typeof thing i think needs to beexperimented a little bit because of iguess i don'ti'm not like crystal ball enough to likefigure out like what the ins how theincentives can align or don't align orsomething with in those types ofscenariosyeah that's a pretty interesting detailbut should the authors be because iguess it's like for me as a phd studentuh publishing papers it's like if i canget a paper into iclrit would beit would benefit my career from the restof my life sort of so i i would bewilling to pay for this kind of reviewright and it's kind of like as you puttogether these teamsthese reviews are kind of you know youknow i mean i don't know how like youknow obviously you communicate a lotwith your teams but the reviews are sortof thecommunication bottleneckat least with my ph in my experiencewith my phd labthe reviews is our kind of likeinterface with each other mostlywith you mean with like otherresearchers in the fieldyeah like at least at florida atlanticuniversity we have a little lab and wetrade we trade reviews that's how likethat's how we communicate we don't solike and i imagine a lot of other labsdo interface that wayso maybeyou know can it connects you to thebiggeryou know market by having it be paidbecause then it's more likeyou know more people would probably signup to that and offer their expertisebecause because again it's it's likeexpertise it's not like anyone couldgive you a thorough review of your uhyeah yeah augmented language modelyeah yeah i um[Music]it's interestingit's interesting i don't know um i buti'd be interested in seeingwork in this spaceum maybe some like kind of likelongitudinal studies that in inyeah and maybe it comes back to likecode reviews too like the same idea butfor reviewing your code your commits islike this idea ofkind of like science and also code andmaybe even coming way back into when imentioned kind of codex and why i seelike the codethe the language models that write codei think and the language models thatinterpret science i think should cometogether a little morei can see that i can see i can see thathappening forumi think for select things i think for ithink i can i can see it happening forum okay so like definitely for for afairly low level writing clarity typethingsum you know i would love it if ifsomething uh could tell me if like ihave a variable that's unassigned whichis very much like a kind of code thingbut when i'm like typing up notation andi'm like come coming up with notationand overleaf um if i just have likelike a subscript eye that is literallynever used i want someone to tell methat that's the case and that's fairlylow level it's just like checking likethis thing isn't it used anywhere elseumand then maybe i would i would likesomething which is like oh hey you'remissing the citationum in as you're writing and i thought ohyeah that'd be good just like prompt itso then i can figure out what to do withit um so there's like a lot of likelittle things that i have to keep in myhead and i just like don't want to keepthem in my head and it would be nice ifsome agent could like help remind me ofthese things and then as it startsgetting into the territory of likeit helps me interpret my results likethat's when i'm just like i don't knowi don't know if i enjoy this um yeah ialso don't think that code like codelike the the codelanguage model stuff is like even atthat space either like nobody's like idon't think they're creatively writinglike just like the idea of the softwarefor peoplerightyeah and so that's that's the otherthing he said that's what i currentlyuse uh literature mining tools for is toyou know i have an idea is this actuallya novel idea hit the literature or tryto find the idea of the paperis that currently how you mostly use thetools can you tell me about like how youuse semantic scholar and how that youknow because again like you have thismeta thing where you're developing thetools you have anything yeah it's a nicespace to be in yeah i do have the samething i i use um i actually use acombination of stuff because it's likemissing something is catastrophic um soi don't trust any one tool i use mascaragoogle scholarumand i use i use a lot of social signalsi think i use a lot i just use a lot ofjust like everyday check archivejust skim the the next batch of like 20or so papers for that day it's likethere's a lot of like you never knowwhen one tool is going to miss somethingand i also just ask people constantlyjust like have you seen anything likethis you've seen them like this um i doa lot of really manual traversal ofcitations umsouhit's all terrible it's all awful work umand we definitely need more things thatmake it make it um nicer because itreally is like no one tool is is iskind ofthe thingat the momentyeah salute to you for uh checkingarchive directly i have to get it dumbeddown through the twitter bottleneckbut yeah that's that's reallyinteresting and maybe like a concludingquestion on the podcast that i ask a lotof people and again i you're uniquelypositioned to answer this is what isyour information dot i mean you kind ofdid just answer the question but isthere anything lacking from that of yourkind of information dietfor my information dietumwow that's the thing so the thing withlacking is likeprobably but i don't know what it isi think that's the problem that's theproblem with this information diet thingright soum i actually don't know if there'ssomething lacking but there probably isuhand ummy information diet today is just like icheck archiveum almost every day i actually use thisman scholar recommendation feed um whichis quite goodumit's quite i mostly read papers thatlike teammates um and like friendspeople i follow will share on twittershare in slack messages and stuff likethat um and i actually i think like halfthe papers i read these days are are forlike peer reviewi like peer view duties i actuallyreally enjoy reviewing um because youget to see like what people aresubmittingum you get to see like like that's kindof like the glimpse into like what'shappening right now that's notin the public andumit helps you read in a different waythan if you're just kind of passivelyconsuming what's on twitter they helplike it keeps like kind of like the thecritiqueengine sharp umuhyeah i guess that's probably most of mydiet butif i had to pitch an idea for somebodyto go work on something i would lovesomebody tobuild like a browser extension thattracks the stuff that i'm readingyou know doesn't like do something weirdwith my data but like attractive stuffi'm reading summarizes it to me andmaybe recommends me like papers that orlike avenues for accessing papers thatlikei should be accessing but like i don't idon't even know exist i guess this islike i don't know what i'm missing in mydiet so i want someone to tell me thati'm not eating enough protein orsomethingyeahand maybeuh if it's not too much of a personalquestion could you maybe take me intolike a day in your life at the alleninstitute i'm like very curious whatthat entailsyeah so day in life um i think for me uhnowadays it's likeiam in meetings with uh i guess likewe're kind of entering this like summeruh spring summer and then kind ofthrough fall so this half of the yearit's mostly mentoring uhinterns uh who are like kind of likecurrent phd studentsumorundergrads who are kind of preparing forum their phd applications or to enter aphd program so it's a lot of mentoringuh people like kind of new people whoare showing up for this half a year uhwe're starting on new projects doing alot of ideation a lot of just likesitting in a room and just like talkingabout like should we do this or do thisuh run some quick experiments meet backagainas i get stuck and try to interpret theresults try and decide like should weinvest more in this direction should wepivot it's a lot of these kind ofuh really quick turnaround decisionsabout how umuh how to like go about answeringquestions that we're interested in umand thenthat progresses into more heavy dutydevelopment work um over time where it'slike every day is just like kind ofwriting code running experimentschecking those experiments and thenrevising or fixing some bug or somethingand then into like writing andsubmission or just like every day justlike writing reviewing the paperschatting with people umand and continuing revising umsodepending on the stage i'm in it's likeall of it is chatting with people all ofit is like developing or all of it it'sjust like writingsuper interesting well kyle thank you somuch for coming on the podcast and i ireally really enjoyed this conversationi love these topics and i thinkscientific literature mining and thework you're doing is justdefinitely my one of my favorite thingsto keep up with in deep learning andit seems like it's headed to such a suchan exciting application being realizedwith deep learning cool thank you somuch for inviting[Music]you", "type": "Video", "name": "Kyle Lo on Scientific Literature Mining - Weaviate Podcast #17", "path": "", "link": "https://www.youtube.com/watch?v=kUjhCsawgCo", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}