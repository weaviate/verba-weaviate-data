{"text": "Hey everyone! I recently spoke with Aleksa Gordcic about his vision for the future of video content on YouTube and it really ... \nhey everyone thank you so much for watching the Wii gay podcast I'm super excited to welcome Alexa gordich Andre bondarev Stephanie horvachesky and gunjan batarai Jana wellender Greg kamrat and Colin Harman email and Finn Professor Laura Dietz Brian Raymond Erica Cardenas Siva and Roman yushang Wu John dagdollen both wivier co-founders Bob Van light and Eddie and dilocker hey everyone thank you so much for checking out this update on the Wii VA podcast search uh hopefully from the introduction reel you saw that our next uh weaving podcast is going to feature Alexa gordich Alexa has such an interesting story of beginning as a YouTube content creator making deep technical paper summary videos then joining deepmind the top AI lab in the world arguably and then leaving deepmind to start ordus so ordus is about creating these next Generation content creator tools so building question answering or chat Bots over particular YouTube channels like say Jeremy Howard's YouTube channel and ideas like this so this really inspired me to give an update on what I've been doing with the wevia podcast search I've been taking these podcasts putting them into whisper and then taking the transcripts and you know searching through them and now with large language models and generative feedback loops I think it's right for an update so generative feedback loops broadly the ideas where we retrieve data to pass the large language model and then we save the resulting Generations from the large language model back into our database so there are two really like kind of boots on the ground immediately useful applications of this which is firstly to write a summary of the podcast and then secondly to identify the chapters in the podcast these are just like two tasks you have to do every time you upload a podcast as you're managing something like a podcast but at the end of the video I want to conclude with some I think directions for future work that can really take this to the Moon I think the future of content creation and consumption this kind of you know video YouTube podcasting I think all this is just you know gonna totally flip on its head with large language models and all this cool exciting stuff so I hope you enjoy the video we're going to start with some code and then we're going to conclude with some kind of big ideas about where I think this is going so thanks so much for watching before diving into it all the code for this project has been open source on GitHub we get slash weavate podcast search all the data the podcast transcript Json files can be found in this data folder the generative feedback loops folder has all these different scripts for writing the clip summaries the auto chapters the full podcast summaries recommendation is a demo that I did at odsc East where you show ref to VEC through the different podcast Clips as well as uh picking searching with the ref to VEC vector and then doing retrieval augmented generation with the nearest neighbors to the user Vector chat Vector DB Lang chain that's from Erica's blog post so if you want to chat with this podcast this is how you do that and then some other things like how you do a weeviate backup or how you get the data out of wevia into a Json with the cursor API how we create the schema which we'll get into right after this and then all sorts of things from how you upload the data and missing school thing so if you do check this out please leave a star it means the world to me and um yeah I really hope you enjoyed this this video will show you how you can apply generative feedback loops to the weevier podcast transcriptions so when we first presented generative feedback loops we have the example of Airbnb listings we had a data set that mostly contained tabular symbolic features like the number of bedrooms in the Airbnb listing what neighborhood it's in the minimum number of nights you needed to stay in and so on and so the first thing we did with generative feedback loops is we took this tabular data and then we put it into a prompt that wrote a text description of the Airbnb listing which we could then vectorize as say the zero shot open AI or cohere models and then embed into our wevia index to search through another thing that we did with generative feedback loops in this first presentation was we took these summaries and then we wrote a personalized summary by retrieving from a user class that had say Connor who likes to travel with his dog or Bob who needs a powerlifting gym when he travels so we showed kind of two things of how you build this index out of tabular data by retrieving data from the database feeding it into the language model and then saving that generation back into the database as well as retrieving from another class and all those kind of ideas of how we take data from our database put into a language model and then you know save the resulting generation because we're going to want to use it in the future so I'm so excited to show you how I've been using this concept for the weeviate podcast search maybe you're working with a similar data set or maybe this will just kind of inspire your thinking on what generative feedback loops are capable of and the role that this will play in the future of you know generative models Vector databases and all this cool exciting stuff so we're going to start off with the data schema showing how we design the classes properties cross references for this particular example then we're going to build a summary index of Clips where we take the raw what was said in the podcast and we transform it into a summary of the content that was said and then it's actually better to search through this summary index the vectorized representations of the summaries rather than the vector representation of just kind of the raw transcript so we're going to be saving these summaries back into the database and then using these summarized Clips to feed into our summarization chain for the entire podcast so it's a nice example of where we save something back into the database then keep feeding it into future chains and this whole idea of you know this kind of cascading Generations that we've saved back into our database so we're also going to use the clips to do the chapter segmentation Auto chapter analysis at the end as well so finally after the video with the Practical like just the Hands-On examples with that we see the results and we've tested things I want to present some future directions so here's a data schema for the wevia podcast search we have three classes pod clip podcast and chapter highlighted in green is the property that's used to vectorize each instance of the class so for every pod clip we have a clip summary a text property and that's what's passed into our vectorizer model that produces the vector that will represent each instance of the podcast clip similarly for the chapters the description that's written will represent the chapter the description will be vectorized to represent each individual chapter each description is vectorized RF is super redundant and then with the podcast the summary will be vectorized so also shown in red are the data types for each property so content is text speaker is text pod number is int there's some interesting things you can do when you're indexing a text to use as a category so for example with speaker we might want to use that as a categorical variable rather than you know like a text thing so so you can configure this in weviate to have the inverted index on using speaker as a categorical thing if you want to query like where speaker equals Bob inlight or Eddie and dylocker or something like this so you can do that with these text properties so similarly you can do say duration to have like a filter on say you want to only see chapters that are longer than five minutes because you know or blogger than let's say 10 minutes because that's more likely to be I don't know maybe a deep discussion I'm not sure but like you can use these properties to filter your vector search so as you're searching through the vector representations of the podcast clip you can also add these uh these filters as well which is something I always found was so interesting about weavate is this combination of filtered Vector index traversal so additionally we have cross references between classes so the way that this works is you can first search for you know a podcast that matches your query and you can see which Clips are contained in the podcast to then get out the clips or so on you could link chapters to pod clips and all sorts of other stuff I think there's an interesting discussion to be had about representing podcast classes with the summarization Chain versus ref devec but first let me show you the example of the summarization chain and then we'll come back to that discussion so I hope this is a good description of the data schema that's going to be used for this example now let's see quickly the Json dictionary that you use to create it and weeviate so we'll start off with a quick look on how to create these schemas with Wi-Fi so the first thing we do is we connect the Wega client wherever wherever this is running in my case I'm just running it locally on this laptop but you can also have weeviate embedded or say the Wii VA cloud service running it yourself these kind of things so first you connect to your we get instance and then you define the schema dictionary where you have the different classes that you want to create again we want to create three pod flip podcast chapters so with chapters we're going to turn on the text effect Transformers to vectorize each instance of our class and then we're also going to enable the generative open AI to do that thing where we are going to parallel I'll get into this more later but we select which model we want to use in this case I like the text DaVinci zero zero three uh so here's the next interesting thing is we're so we're defining our property so see summary versus content in this case we have skip false this means we're going to vectorize the class based on summary whereas content we're going to skip this we're not going to vectorize content speaker we're going to skip as well and speaker I did mention that you can have this you can enable inverted indexing on text properties to use them as categories I haven't done that quite yet but just quickly noting that this is where you would do that so after you define all the kind of you know text in date these normal properties now we Define cross references so we have in podcast which has the data type podcast that we're about to Define and you have in chapter with chapter that we're about to Define so I don't think there's anything else to novel about this you know podcast again we uh skip the other properties we only vectorize the summary uh these are kind of older properties I had when I was originally working on this but um so yeah so has clip has chapter chapter you know time start time and so on so now that we've defined our data schema we're going to be uploading the data that we do have and keep in mind that a lot of this data in the schema is going to be produced by an llm that we're then going to save back into the database but we start off by looping through this original data uh where we have say the content the speaker and the podcast number and then we're also going to be creating a clip number key that we're going to be using later to get the clips out and put it into the file so we're just going to be you know creating these objects creating cross references between pod clip in podcast podcasts and podcast podcast has clip pod clip and that's all we're going to be doing for now and we're going to be creating a lot of this data again with our LMS okay so now that we've defined our schema and imported data into Eva we're going to be summarizing each of the raw podcast clips into a summary and quickly before I show you the prompt to use that let's have a look at the final result maybe just to wake you up if we've been diving into the code too much so what we're doing is again so for each of these um each of the contents we write this summary so we can take a look at how well that um that works uh so in this clip we are talking about um competition and this paper summary so Connor shorten agrees that markets are underserved and competition is a big force that can show ideas he gives credit to the speaker for diving into graph neural network stuff so that was a pretty good summary uh here's another one or let's see one that maybe has more content to it [Music] um okay so this is Alexa talking about I already felt fairly comfortable uh so this is uh Alex is talking about how he got the deepmind so and the summary is Alexa gordage began his journey into machine learning in 2018 when he attended a summer camp organized by Microsoft he was inspired by the lecturers from deepmind and wanted to be at the Forefront of AI development so and so on so this also is a pretty good uh summary so so basically you get the idea of how um you see how these summaries are like you know more compressed information packed than kind of you know just raw how people speak and I think this is a really interesting component of this because you vectorize this to represent the classes and then search through and it gives you a better Vector because it's cleaner data so we're kind of using like the large language model to clean our data and there's all sorts of interesting things there's an interesting paper on using these to supervise training embedding models the summary and the original content as the positives a lot of interesting stuff to this so let's get into how we do this okay here's the code to write a summary for each of the podcast clips and then save the resulting summary back into Eva so we start off by connecting to leviate then we write our prompt to use the Eva generative search module generate prompt please write a short summary of The Following podcast clip speaker and then we template in the speaker value podcast clip and then we template in the content value so we select the properties that we want to select from the we gate class to put into the uh The Prompt then this is something I did I originally had a loop but what I found is it costs about a dollar to do this so if I did it for all I think 15 podcasts in there so I'm just doing one podcast at a time for the demo so a podcast goes 55 and you know just putting it into the wear filter template so first we get the total clips and again I just kind of hard coded that in quickly but so then we get the total Clips in the podcast now we're going to be looping through the clips with this offset so the way that this works is built in this generate search module into go language makes it really good at handling the parallel go Lang requests which makes it really fast but if you try to put in like all 70 Clips at once you're going to end up getting a read timeout if like one of those requests takes too long and so on so I do this 10 at a time by using the following where you Loop through the clips with this increment of you know plus 10 step sizes and then with limit 10 and then with offset so the offset just pushes it you know through the the data objects so um yeah so you just grab the podcast clip you subset sample the generate properties then you pass in pass it into the prompt defined above and it's pretty much as easy as that you get the ID out and then this is just what I did to kind of monitor how it's going and then you just update the data object by creating this new property for each of the summaries with the summary and then the generate a single result that comes out of the prompt you get the ID and then you update the the data object with this new property the class name and then the uuid for this object okay so next step is to write a summary for the entire podcast gas by using a summarization chain so firstly again we connect to alleviate I will pass in our openai key here because we're going to do this just with the the open AI API here in Python because we're doing it sequentially rather than parallelizing the request across the search results so we just pass in this is just a little wrapper I found that the I actually prefer the results of DaVinci to 3.5 turbo maybe someone has a comment but I didn't like explore it that that much so temperature of zero passing in the prompt Max tokens and all this kind of stuff so so first things first thing to do is to get all of the uh the clips and again we want the summaries not the original content so we get these clips with our weevier query and then what we're going to do is this local memory create and refine prompt so maybe if I or hopefully that's uh you can see that well but so we start off with an empty string for the current summary and we have the prompt please write a summary of the following podcast you will receive the clips one at a time as well as a summary generated so far current summary so far and then we put in our local memory that we're going to keep writing by saving the output of this call back into that current summary variable in the next podcast clip speaker percent speaker and then you put in the the next clip speaker said content so We're looping through the clips putting in the speaker set content as well as our local memory of the current summary of the podcast so far so hopefully that makes sense so then what we do is so what would happen with this originally is it would just keep adding sentences onto the end of the summary so you know then you end up going over the 4096 token window so in the middle of this while you're looping through the clips you then have please rewrite the summary to make it shorter please be careful about losing too much information when rewriting I found that that kind of works that second thing of but because it I think the current limitation of this is that it often will compress it to just writing these long lists with commas of each of the things and you know that's not great for the summary but so there are some interesting ideas but this is the first thing I tested is this kind of summarization chain thing so you just continue looping through that again printing it just so I can see it as it's running in the console and then saving it so we get the podcast ID that we want to be linking as we're saving back into the podcast object so we um you know we add this new summary uh property and then we update it by putting in the summary that comes out of this that current summary object as we've exited the loop and then we pass in the ID so I think the first thing to note about running this is about how you're going to be visualizing it and maybe there's an opportunity for a startup I've heard of tools like Lang flow and things like this I just haven't personally had the time to get into it yet but this is what I've been doing is just as it's looping through each podcast clip it takes that first podcast clip it writes the summary of the whole podcast from it then it rewrites it and I'm just kind of printing it to the terminal and I'm you know just watching it on the side as it runs and so I do think that you get a pretty good debugging with this like um I've learned from just watching this that when you rewrite it often the ways that this fails and the reason why the resulting summary is like okay is because it'll compress it into like a list you know like just like commas to describe terms and I mean at the end of the future work I want to talk about this kind of General understanding and how it that clearly doesn't understand what it's talking about really and I think there's a lot of interesting I think that's quite an interesting point but anyway so just the the Practical example I want to show you is just logging it to the terminal to visualize the chain honestly I think this gets the job done but I can't imagine with complex chains and you know when you're really looping and then Branch like each Loop has this kind of nested like you know summarize and then rewrite I can imagine maybe you want these more advanced tools for visualizing these kind of things so now let's get into what I think is the most interesting task for these generative feedback loops to be doing which is to automatically identify the chapters in the podcast this is probably the most useful time saving thing about this to be doing so again we start off with all that same kind of connect to evade and Define this open AI thing and there's probably a cleaner way of doing this but I don't think it matters for this so again we get the podcast Clips out and now we're going to be looping through these clips so here's the interesting thing so we start off with this empty dictionary of chapters that we're going to be putting in these dictionaries too so we're gonna have a list of dictionaries then at the end of the loop we'll Loop through this list and then we'll take out these dictionaries to write the key values into into the weva data objects so we have the keys chapter and then start and end which is going to be you know huge for how we then use this to annotate the podcast and then we have the Pod clip IDs to create those cross references from pod Clips to Chapters and then inversely chapter has pod clip so here's the prompt that I found to work okay with this and hopefully I find this interesting so your task is to annotate chapters in a podcast you will receive a podcast clip in the current chapter topic if the conversation in the clip discusses the same topic please output 0 if the content in the clip is very short such as a joke or a thank you or something minor like this please output 0 however if the conversation in the clip discusses a new topic please output one the current topic is and then you template in the current clip that you're at you'll receive the current clip as well as the previous and next clip for additional reference so previous clip next clip current clip and then so you put the current clip again twice as a reminder please only output either zero or one as described above so I think there are kind of two things to this prompt that make it interesting the first of which is showing how you can have this if else just in natural language to have it you know if this kind of natural instruction I find that to be very interesting and then I think this is the idea behind a structured output parsing there are you know more nuanced things you can do like where you output Json dictionaries and then you have like Json dot loads and then you parse out the key and things like that where it would um you know kind of generate like multiple things with one call but generally I found this to work surprisingly well every time I tried this it did only output 0 or 1 and I just you know see how I wrap the response in an in Typecast so I didn't find that this that it broke it or that it said um I don't know like a new top like say it would write a new topic instead of zero or one and not follow the instructions I found that this worked pretty well so then what you do is if topic response equals equals one now you're going to cut the ending of the current topic append this dictionary to the chapters and now you're going to be creating the new chapter object so you have this um please write an abstract description of the uh conversation topic discussed in the current podcast clip for the sake of reference you'll receive the previous and next clips as well to help further contextualize I think the abstract description of the topic so again passing in the previous current next please write a maximum six word description of the conversation topic discussed in the current clip so again we template all this stuff and then we you know save the chat the output into our chapter for the current topic which is what goes into the um the uh oh sorry what goes into the current topic template so earlier I misspoke and said that this was here as well but this is actually uh the current topic obviously but so anyway so so then we exit the loop and now we're going to be saving the new chapter objects so first we get the podcast ID that this was sourced from with this query you know run this and then um so then we have the uh four chapter in chapters We're looping through that list of dictionaries we have our properties that we extract the chapter the start the end then we get a uuid for this chapter ID we create the chapter object and now we're going to link from a pod clip in chapter chapter has clip pod clip chapter from podcast podcast has chapter chapter so that's how we you know again connect that graph full of cross references to add this structure to our data so that's all we need to uh extract these chapters from our podcast okay so we're similarly gonna just kind of visualize this in the terminal which I found to be a fine solution for this so what it's doing is it's executing that if else if we need to switch the topic or not it starts off with building a YouTube channel explaining ml Concepts through videos then it transitions to oh stays on that one for a while creating a startup orders and so on so so this is the idea of it executing this if else logic with the topics is it passes in these topics uh by looping through each of the clips and having that if else kind of uh prompting that and then the structured output parsing of if zero if one and then how that leads to being able to automatically create these um these chapters in the podcast okay so that's the end of the latest update on how far I've made it so far with uh summarizing the clips then applying a summarization chain on the clip summaries to summarize entire podcasts as well as extracting chapters from the clip so again it would mean the world to me if you check out the GitHub repository that contains all the code for doing all this you just need to replace your openai key but maybe you're probably just more interested in seeing the example and then applying it to whatever data set you're working with so anyway so here are some directions I think of what I want to do next with this project that I think are really interesting and I think you'll find that interesting as well so starting off with topic modeling for podcast summaries I really want to show you this ad atlas map and I think that topic modeling where you cluster the clips and then you look at the Clusters could have a nice structure instead of this kind of sequential summarization chain I I don't think the sequential summarization chain is going to be the best way to summarize these podcasts going forward then I want to talk about self-ask prompting we're usually thinking about it with question decomposition but I think it's really interesting for summarization as well I want to show an idea that I've been learning about from Lama index on retrieving from multiple information sources I think there's a lot of nuance to this I think I want to talk about personalized summaries podcasted blog posts and generally how working on this project has led to a new understanding for me on fine-tuning llms and hopefully you'll agree or have some feedback on what I'm getting wrong okay so what you're looking at is atlas from nomec AI which is a visualization of embeddings projected into a lower dimensional space so that we can visualize the Clusters you see these high level tags generative adversarial networks the neural architecture search topic modeling which is funny because that's what we're talking about these are the high level descriptions given to these categories so this is a pretty big topic map as you see in the title this is all of the abstracts of papers and nerves from 1987 to 2022 but you could imagine doing this kind of thing so if we zoom in on the generative adversarial Network thing like imagine all the podcast Clips are like this and they have some kind of cluster structure within the clips and then so I think summarizing each of the Clusters and then just kind of merging them into the one Summer from the podcast I think most likely that's the best way to summarize long content with these llms going forward so the first thing I want to do next is add in a prompt when it's summarizing the podcast clip to ask do are do you need more information more background knowledge about certain terms that you're summarizing so one thing I noticed just from watching the summarization chain in the terminal is that it doesn't understand say what graph neural networks are and so it'll combine the graph neural networks thing into the discussion of like just creating YouTube content broadly in a way that totally doesn't make sense demonstrates no understanding of what graph neural networks are which brings into the last thing on fine-tuning LMS but I think this general idea of we in question answering it makes quite a lot of sense who was the President of the U.S when superconductivity was discovered you know and then you asked when was superconductivity discovered and then in that year who was the president so you break up the question I think summarization change should adopt this same perspective to how that works which then brings me into a super interesting thing I've been learning about from Lama index which is about routing queries to different information sources so purely from the vector uh the we gate standpoint I think just the idea of selecting from different classes so say I have one class this week a blog post and other classes like archive and then say another thing is Wikipedia and then if I want to retrieve about you know I want to get like a definition of graph neural network say I retrieved that from archive if I want to get some uh maybe definition on just like when machine learning was invented maybe I get that from Wikipedia but this idea of routing these queries to different information sources I think this has huge potential in how this kind of summarization chains could improve and how you can retrieve from different sources to retrieval augment to generation pack The Prompt that facilitates a better summary the next topic is quite different but this is more this is more so a bigger uh project that kind of hopefully when I do generative feedback loops we've a podcast search part three I have this kind of thing ready where uh what I'm really dreaming of is to do personalized summaries so you give it the chapters maybe a short description of the chapter as well then you have something like you know a list of people who are interested in the podcast newsletter subscribers so you have the chapters of the podcast and you retrieve Alexa gordich is an entrepreneur building ordis or this is an interactive way to watch videos learn faster by using question answering capabilities and high quality summaries and the prompt is which of these chapters do you think would interest Alexa if none please output not interested so then you write this personalized summary to email Alexa to say hey do you want to watch this podcast but if they're not interested then you know don't spam them don't send them an email for the podcast that they wouldn't want to watch so I think this is this whole idea of you know this virtuous cycle the aggregation Theory from Ben Thompson I think this is that thing this you know personalization of content and this is you know this is particularly how I'm thinking about using it so hopefully that's useful for you to see like a real world example of how I think this can improve our podcast and our content uh so the next thing I think is super interesting is to write blog posts from the podcast and I see kind of two ways of why this would be interesting the first of which is to say like what is unique about this podcast so please write an analysis of what this podcast says uniquely about chapter and then you search within this podcast to get the top K from this podcast and then you say here are clips from other podcasts to compare this podcast as anything unique and then you do the top K from all the other podcasts from all podcasts and so I think that's quite interesting because now you have this comparison kind of thing that you're doing with with the semantic search so I I really love that idea then I think another thing is um you know I like to listen to like David Sinclair and like these kind of like biology things but I I often like I don't really know what they're talking about so I think this kind of like retrieving from some information Source like archive to write a background blog post so you know when I'm reading about like metabolic pathways or crispr stuff like this where I hardly understand any of it having some kind of like pre-flight blog post for me before I listened to one of these podcasts I think that could be really cool this project transformed my understanding of large language model fine tuning so I think on one end we're kind of just you know waiting around when is GPT 5 coming or when is you know cloud or cohere or some open source model going to just be this impressive model that can just do somehow understand topics like graph neural networks like specific things for your specific information needs so it got me thinking about you know you could imagine you take the prompts again like please write a summary current summary so far and then you know the local memory and then the next clip and the next summary and it outputs some kind of next summary and you would label that as I like this or I don't like this and that's the general a labeling framework of reinforcement learning from Human feedback that makes this so appealing is you just need to say like it don't like it and it can you know do that sequence of sequence learning and update the language model but I don't think that this will ever get you to that underlying language understanding so on the top is kind of what I think is the conventional wisdom that you would take some uh you know some model that's already been trained with reinforce and learning from Human feedback unlike General preferences of writing summaries of things and then you would just kind of try to like steer it in direction of your domain by annotating this kind of thing but I think the alternative idea is you have a pre-trained language model and then you language model some of your data set and then you do the reinforced learning from Human feedback so this is really inspired by my thinking the kind of the the news of the time as I'm recording this this uh video is Mosaic ml being acquired by databricks for 1.3 billion dollars and I just recently recorded a video looking at the MPT 30 billion uh language model that they open source and a detail of it that just blew my mind was the open sourcing of the language model and then the instruction tuning it just really helped me separate these ideas and you know I've had some conversations with Jonathan Franco on this podcast where he discussed um you know convincing companies to language model their data that's where you predict the next massed out token on you know say I just took like all of archive or I filtered it for information retrieval approximate nearest neighbor search like this kind of like weevier domain you know just language model podcast transcripts blog posts and so on I think that then you would have this base Foundation that then you could rlhf onto that so I'm curious what people think of that that's my latest understanding in what LM fine tuning might look like and how this project kind of thinking about how might get a better result from this kind of led me to appreciate that kind of continued pre-training concept a little more thank you so much for watching this latest update on the wevia podcast search and applying generative feedback loops to this data set to summarize Clips summarize podcasts and extract chapters from podcasts I think that these ideas of personalized summaries podcasts to blogs overall just for me at least having a data set helps just cement all these kind of ideas and ground them in some kind of application so please subscribe to this YouTube channel and leave a like on the video If you enjoyed it you can check out weviate weva.io also it's an open source GitHub project we get slash weavate and also please follow on Twitter at weeviate IO thank you again so much for watching ", "type": "Video", "name": "Weaviate Podcast Search meets Generative Feedback Loops!", "path": "", "link": "https://www.youtube.com/watch?v=I4Jle80AOaU", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}