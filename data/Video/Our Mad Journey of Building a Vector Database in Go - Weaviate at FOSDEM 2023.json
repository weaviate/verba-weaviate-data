{"text": "Building a database from scratch is no small feat. Doing so in Go might just be pure madness. In this talk, Co-Founder & CTO ... \nit's four o'clock so let's time let's look at our preview sorry now next talk I have been doing some math things and go but building a database I honestly have strong respect for so next up is ATM which is going to tell us everything about crazy Journeys and go thank you thank you yeah Welcome to our mad journey of building a database and go and um yeah it's pretty mad to build a database at all it may be even worse or even a matter to build a database um Ingo when most are built in very closer okay okay cool let me start over in case you didn't hear it so hi my name is Etienne Welcome to our mad journey of building a vector database in go so building a database at all could already be pretty mad doing it in go when most are built in C or C plus plus could be even even matter or even more exciting and we definitely encountered a couple of unique problems that led us to Great uh Creative Solutions and there's lots of shouts out in there and also a couple of wish lists so we just released go 1.20 and of course the occasional Madness so let's get one question out of the way right away why does the world even need yet another database there's so many out there already but probably you've seen this thing called chat GPT because that was that was pretty much everywhere and it's kind of hard to hide from it and uh chat CPT is a large language model and it's it's really good at putting text together that sounds really sophisticated and and sounds nice and sometimes is completely wrong um and so in this case we're asking it is it math to write a database and go I might disagree with that um but either way basically we're now in a situation where on the one hand we have these machine learning models that can do all the cool stuff and do this sort of interactively and on the Fly and on the other side we have traditional databases and those traditional databases they have the fact because that's kind of what databases are for right so wouldn't it be cool if we could somehow combine those two so for example on the query side if I ask Wikipedia why can airplanes fly then the kind of Passage that I want that has the answer in it is titled the physics of flight but that is difficult for a traditional search engine because if you look at keyword overlap there's almost none in um but a vector search engine can use machine learning models basically that can tell you these two things are the same and searching through that at scale is a big problem then there's that sort of chat GPT uh side where you don't just want to search through it but maybe you also want to say like take those results summarize them and also translate them to German so basically not just return exactly what's in database but do something with it and basically generate uh more data from it and that is exactly where VBA come in so V8 is a vector search engine which basically helps us solve this this kind of searching by meaning instead of keywords without uh sort of losing what we've done in 20 plus years of search engine research and now most recently you can also interact with these models such as chat cpt3 and of course also the the open source versions of it so bb-8 is written in go is that a good idea is that a bad idea or have we just gone playing mad so we're not alone that's good so you probably probably recognize these things they're they're all uh bigger brands at the moment than video it is going past um and some of those vendors have really great a blog posts where you see some of the like optimization topics and some of the crazy stuff that they have to do so if you've contributed to to one of those some of the things I'm going to say might sound familiar um if not then uh buckle up it's gonna get mad so first stop on our mad Journey memory allocations and that also brings us to our friend the garbage collector so for any high performance go Application sooner or later you're going to talk about memory allocations and definitely consider a database a high performance application that Lisa consider V8 a high performance application and if you think of what databases do like in essence basically you have something on disk and you want to serve it to the user that's like one of the the most important user Journeys in a database and here this is represented by just a number so it went for un32 so that's just four bytes on disk and basically you can see these are these four bytes if you parse them into go they would have the value of 16 in that 2 and 32 and this is essentially something very much simplified that a database needs to do and it needs to do it over and over again so the standard Library gives us the encoding slash binary package and there we have this binary.read method with which I think looks really cool it to me it looks like idiomatic go because it has the Iota reader interface like everyone's favorite interface and you can put all of that stuff in there and uh if you run this code and there's no error then basically you get exactly what you want you could turn those sort of four bytes that were somewhere on disk uh turned them into our in-memory representation off that un32 so is this a good idea to do that exactly like that well if you do it once or maybe twice could be a good idea if you do it a billion times this is what happens so for those of you who are new to to CPU profiles and go this is madness this is pretty bad so first of all you see it in the center um parsing those 1 billion numbers took 26 seconds and 26 seconds is not the kind of time that we ever have in a database but worse than that if you look at that profile we have stuff like runtime mluk GC runtime memo runtime M advice so all these things they're related to memory allocations or to garbage collection what they're not related to is parsing data which is what we wanted to do right so how much time after 20 seconds did we spend what we wanted to do don't know doesn't even show up in the profile so and to understand why that is the case we need to quickly talk about the stack and the Heap so you can think of the stack as basically your function stack so you call one function that calls another function and then at some point basically you go back through the stack and this is very short-lived and this is cheap and fast to allocate and why is it cheap because you know exactly the runtime of your variables or the life cycle of your variables so you don't even need to involve the garbage collector so no garbage collector cheap and fast then on the other side you have the Heap and the Heap is basically this this sort of long lift kind of memory and that's expensive and slow to allocate and why because and also to deal okay why because it involves the garbage collector so if the stack is so much cheaper then we can just always allocate and stack right so warning this is not real go please please do not do this um this is sort of a fictional example of allocating a buffer of size 8 and then we're going to say like yeah please put this on the stack uh and that is not how it works and for most of you you probably say like this is pretty good that it's not that that it works that way because why would you want to deal with that but for me just trying to build a database and go so yeah sometimes like this this something like this may be good or maybe not so how does it work go does something that's called Escape analysis so if you compile your code with gcflax Dash M then go annotates your code basically and tells user what's happening there so here you can see in the the second line um that this num variable that we used was moved to the Heap and then in the next point you see the byte stock reader which represents our io.reader escaped to the Heap so two times we see that something happened to the or went to the Heap we don't exactly know what happened yet but at least sir there's proof that we have this this kind of allocation problem so what can we do well we can simplify a bit it turns out that the binary uh or encoding binary package also has another method that looks like this which is just called U n32 on the little endian package and the it kind of does the same thing you just put in the buffer on the one side so no reader this time you just put in the Raw buffer basically with the the position offset and on the other side you get the number of and the crazy thing is this one line needs no memory allocations so if we do that again our 1 billion numbers that took 26 seconds before now takes 600 milliseconds so now we're starting to get into a range where like this is acceptable for a day for uh for databases and more importantly what we see on that profile the profile is so much simpler right now there's basically just this one function there um and that is that is yeah it's what we want it to do so admittedly we're not doing much other than parsing the data at the moment but at least we got sort of rid of all the noise and you can see the speed up okay so quickly to to recap um if we say a database is nothing but reading data and sort of parsing it to serve it to the user then uh we do that over and over again then we need to uh take care of memory allocations and the fix in this case was super simple we changed two lines of code and reduced it from 26 seconds to 600 milliseconds but why we had to do that wasn't very intuitive like that it wasn't very obvious in fact I haven't even told you yet why this binary.littlend.read why that escaped to the Heap and in this case it's because we passed in the pointer and we passed in an interface and that's kind of a hint basically that something might escape to the Heap so what I would wish is yes this is not a topic that you need every day you write go but maybe if you do need this would be cool if there was better education okay so Second Step delayed decoding so this is kind of the idea that we wouldn't want to do the same work twice and we're sticking with our example of serving data from disk but now while the number example was a bit too too simple so let's make it make it uh slightly more complex we have this nested array here basically um a sort of slice off slice of view in 64. and that's representative now for a more complex object on on your database of course in reality you'd have like string props and that kind of things but just sort of to show that there's more going on than a single number and let's say we have 80 million of them so 10 million of the outer slice and then eight elements in each inner slice and our task is just to sum those up so these are 80 million numbers and we want to know what is the sum of them so that is actually kind of a realistic database task for uh for an olap kind of database um yeah we need to somehow represent that data on disk and we're looking at two ways to do this uh the first one is Json representation and then the second one would be the sort of binary encoding and then then there'll be more so Json is basically just here for completeness sake we can basically rule it out immediately so when you're building a database you're probably not using Json to to uh store stuff on disk unless it's sort of a Json database why because it's space inefficient so if you want to represent those numbers on disk like space and base Json basically uses strings for it and then you have all these control characters you have like your curly braces and your quotes and your cones and everything that takes up space so in our fictional example that would take up 1.6 gigabyte and you'll see soon that that we can do that more efficient but also it's it's slow and part of why it's slow is again because we have these memory allocations but also the the whole parsing just takes time so in our example uh this took 14 seconds to sum up those 80 million numbers and um yeah as I said before you just don't have double digit seconds in a database so we can do something that's a bit smarter which is called length encoding so we're encoding this basically as binary and we're spending one uh in in this case one byte so that's basically a u and eight and we're using that as a length indicator so basically that tells us that when we're reading this from this that just tells us what's coming up so in this case it says we have eight elements coming up and then we know that our elements in this example is U and 32 so that's four bytes each so basically the next 32 bytes that we're reading are going to be our eight inner arrays and then we just continue then we basically we read the next length indicator and this way we can encode the stuff sort of in uh yeah in one contiguous thing then of course we have to decode it somehow and we can do that because we've learned from our previous example right so we're not going to use binary.littleindian.read but we're doing this in an allocation freeway um you can see that in the length line basically and um yeah our goal is to take that data and put it into our nested sort of go slice of slice of slice of U in 64. and um the code here basically you see we're reading the length and then we're increasing our offset so we know where to read from and then we're basically repeating this for the the inner slice which is just hinted at here by the decode inner function so what happens when we do this first of all the good news 660 megabytes that's way less than our 1.6 gigabyte before so basically just by using a more space efficient way to represent data uh We've yeah done exactly that we've reduced our size also it's much much faster so we were at 14 seconds before and now it's down to 260 milliseconds but this is our mad journey of building a database so we're not done here yet because there's some hidden Madness and hidden Madness is that we actually spend 250 milliseconds decoding while we spend 10 milliseconds summing up those 80 million numbers so again we're kind of in that situation where we're doing something that we never really set out to do like we wanted to do something else but we're spending our time on on um yeah and doing something that we didn't want to do so where does that come from and the first problem is basically that what we did what we set out to do was fought from the get-go because we said we want to decode so we're basically thinking in the same way that we're thinking as we were with Jason we said that we want to decode this entire thing into this go data structure but that means that you see we need to allocate this massive slice again and that also means that we need to in each inner slice we also need to allocate again so we're basically allocating and allocating over and over again where our task is not to allocate our task was to sum up numbers so we can actually just simplify this a bit and we can basically just not decode it like While We're looping over that data anyway instead of storing it in an array we can just do with it what we plan to do and in this case um this would be summing up the data so basically getting rid of that decoding step helps us to make this way faster so now we're at 46 milliseconds of course our our footprint of the data on disk hasn't changed because it's the same data that we're reading we're just reading it in a slightly more efficient way um but yeah we don't have to allocate slices and also because we don't have these like nested slices we don't have like slices that basically have pointers to other slices so we have better memory locality and now we're at 46 milliseconds and that is that is cool so 46 milliseconds is basically the time frame that can be acceptable for a database okay so quickly in recap we immediately ruled out Json because it just wasn't space efficient and we knew that we needed something more space efficient and also way faster binary encoding already made it much faster which is great but if we decode it up front then yeah we still lost a lot of time and it can be worth it in these kind of high performance situations if you either sort of delay the decoding as late as possible until you really need it or just don't do it at all or do it in sort of small parts where we need it no wish list here but an honorary mentioned so go 1.20 um they've actually removed it from the from the release notes because it's so experimental but go 1.20 has support for memory Arenas the idea for memory Arenas is basically that you can bypass the garbage collector and sort of manually free that data so if you have something that you know has the same sort of life cycle then you can say okay put it in the arena and basically in the end free the entire Arena which would sort of bypass the garbage collector so that could also be a solution in this case if that ever makes it like right now it's super experimental and they basically tell you we might just remove it so don't use it third stop is something that when I first heard it almost sounded like too good to be true so um something called simdi I'll get to what that is in a second but first question to the audience who here remembers this thing raise your hands Okay cool so you're just as old as I am um so this is the the Intel Pentium 2 uh processor and this came out in late 90s I think 1997 and was sold for a couple of couple of years and back then I did not build databases definitely not in go because that also didn't exist yet but what I would do was sort of try to play 3D video games and I would urge my parents to to get one of those new computers with an Intel Pentium 2 processor and one of the arguments that I could have used in that discussion was hey it comes with MMX technology and of course I had no idea what that is and it probably took me 10 or so more more years to find out what MMX is but it's the first in a long list of Cindy instructions I haven't explained what Cindy is yet but I will in a second um some of those especially the one in the in the Top Line they're not really used anymore these days but the the bottom line like avx2 and AVX 512 you may have heard them in in fact for for many open source project they sometimes just sort of slap that label and read me like yeah yeah has avx2 optimizations and that kind of signals you yeah we care about speed because it's like low level optimized and aviate does the exact same thing by the way so to understand how we could make use of that um I quickly need to talk about Vector embeddings because I said before that vva doesn't doesn't search through data by keywords but rather through its meaning and it uses Vector embeddings as a tool for that so this is basically just a long list of numbers in this case floats and then a machine learning model comes in and basically it says do something with my input and then you get this Vector out and if you do this on all the objects then you can compare your vectors so you basically can do a vector similarity comparison and that tells you if something is close to to one another or not so for example the query and the object that we have before so without any CMD we can use something called The Dot product the dot product is a simple calculation where basically you use you multiply each element of the first Vector with the same corresponding element of the second vector and then you just sum up all of those elements and we can think of this like multiplication and summing as two instructions so if we look out for shout out here to the the compiler Explorer which is super cool tool to see like what your go code compiles to we can see that this indeed turns into two instructions so this is a bit of a lie because there's more stuff going on because it's in the loop Etc but let's just pretend that indeed we have these two instructions to multiply it and to to add it so how could we possibly optimize this even further if we're already at such a low level well we can because this is our mad Journey so all we have to do is introduce some Madness and what we're doing now is a a practice that's called unrolling so the idea here is that instead of looping over one element at a time we're now looping over eight elements at a time but we've got we've gained nothing like this is we're still doing the same kind of work like we're doing 16 instructions now in a single Loop and we're just doing fewer iterations so by this point nothing gained but why would we do that well here comes the part where I thought it was too good to be true what if we could do those 16 operations for the cost of just two instructions sounds crazy right well no because Cindy I'm finally revealing what the acronym stands for it stands for single instructions multiple data and that is exactly what we're doing here so we want to do the same thing over and over again which is multiplication and then additions and this is exactly what these Cindy instructions uh provide so in this case we can Multiply eight floats with other eight floats and then we can add them up so all this perfect here maybe not because there's a catch of course or not Journey how do you tell go to use these avx2 instructions you don't do you ride assembly code because go has no way to do that directly the good part is that assembly code integrates really nicely into go and um in the in the standard Library it's used over and over again so it's kind of a standard practice and there is tooling here so shout out to Avo really cool tool that helps you uh basically you're still writing assembly with uh with Avo but you're writing it in go and then it generates the assembly so you still need to know what you're doing but it's like it it protects you a bit so it definitely helped us a lot so Cindy recap um using ABX instructions or other CMD instructions you can basically trick your CPU into doing more work for free but you need to sort of also trick go to use assembly and with this tooling such as Apple it can be better but it would be even nicer if the language had some sort of support for it and you made my saying and now okay this is this mad guy on stage that wants to build a database but no one else does and needs that but we have this issue here that was open recently and unfortunately also closed recently because no consensus could be reached but it comes up back and back basically that go users are saying like hey we want something in the language such as intrinsic so intrinsics are basically the idea of having high level language instructions to do these these sort of AVX or SMD instructions and C or C plus plus has that for example one way to do that and maybe you're wondering like okay if you have such a performance hop path like why don't you just write that in C and you see go or write it in rust or something like that sounds good in theory but the problem is that the call Overhead to call C or C plus plus is so high that you actually have to Outsource quite a bit of your code for that to to pay off again so if you do that you you basically end up writing more and more and more in that language and then you're not writing go anymore so personally that's not or it can be in some ways but it's not always a great idea so demo time um this was gonna be a live demo and maybe it still is because I prepared this running nicely in a Docker container and then my Docker Network just broke everything and it didn't work but I just rebuilt it without Docker and I think it might work if not I have screenshots basically that um I do a backup so example query here I'm a big wine nerd so what I did is I put wine reviews into vv8 and I want to search them now and one way to do it to show you basically that the keyword that you don't need a keyword match but can search by meaning is for example if I go for an affordable Italian wine let's see if the internet connection works it does so what we got back um is basically this this wine review that I wrote about a barolo that I recently drank it and you can see it doesn't say Italy anywhere it doesn't say affordable what it says like without breaking the bank so this is a vector search that basically happened in the in the background we can take this one step further by uh using the generative side so base this is basically the the chat GPT part um we can now ask our database based on the review which is what I wrote when is this wine going to be ready to drink so let's see you saw before that was the failed query when the internet didn't work no no it's actually working so that's nice um and here in this case you can see that so this is using open AI but you can plug in other tools can plug in open source versions of it this is using openai because that's nice to be hosted at a service I don't have to run the machine learning model on my laptop then you can see it tells you the wine is not ready to drink yet we will need at least five more years which is sort of a good summary of this and then you can see another one is ready to drink right now it's in the perfect drinking window so for the final demo let's combine those two let's do a semantic search to identify something and then do an AI generation basically so in this case we're saying find me an aged classic Riesling best best wine in the world racing um and based on the review would you consider this wine to be a fruit bomb so let's have sort of an opinion from the machine learning model in it and here we got one of my favorite wines and the the model says no I would not consider this a fruit bomb while it does have some fruity notes it is balanced by the minerality and acidity which keeps it from being overly sweet or fruity which is um if you read the text like this is nowhere in there so this is kind of cool that the that the model was was able to do this okay so let's go back now with the demo time by the way have a GitHub repo with like this example so you can run it to yourself and um and yeah try it out yourself so this was our mad journey and are we mad at go are we mad to do this well I would pretty much say no because yes there were a couple of Parts where we had to give get really creative and have to do some some yeah rather unique stuff but that was also basically like the Highlight Reel of building a database and all the other parts like it didn't even show the parts that went great like concurrency handling and the the powerful standard library and of course all of you basically representing the Gopher Community which is super helpful and uh yeah this was my way to basically give back to all of you so if you ever want to build a database or run into other kind of high performance problems then maybe some of those examples helped you and if you now want to try out bb8 you can see it on bb8io or the GitHub is vb8 vv8 you can follow me on Twitter at Etienne di or you can follow viviated vv8io I don't have a mastodon yet because that kind of seems to be the new thing but I was busy writing assembly code so I had no time to create one yet um yeah that is our mad Journey thank you very much [Applause] cool if you have yeah I see a question I'll try to run towards you I've been running all day so it might be slow hi um thank you for for your talk um quick question about a compression uh you talk a lot about data size um have you waited the pros and cons between the compression to reduce the size and so the the bandwidth needed to read the data and then the penalty of decompressing yes yes absolutely so basically the the uncompressed example was sort of the simplified version because 25 minute talk and had to cut down in real life definitely compression especially like using a un 64 in this example most of the bits are probably going to be zero so you can gain a lot from from a compression and we have an inverted index also in in vv8 so we have these like kind of posting lists and they're very similar because they they start from from zero basically go up so there's lots of overlap and lots of gains to be made with compression um yeah maybe something for for our mad Journey part two yeah so my first question is uh doesn't go itself leverage this simd things it seems like it should compiler should do these things sort of okay-ish the problem is basically um typically with like the AVX 512 where we wanted to do eight things at once this is specific to some CPUs and the go compiler doesn't doesn't do it as well typically from the way that the go compiler does it I could get like a two times speed up but never more sort of never in the uh yeah so Realms of eight times but in general and this is not so much a go specific thing like hand optimized uh um yeah assembly or low level Sim D is most likely going to be faster than what a compiler do simply for the fact that you know exactly what you want to do like how long the video they're going to be in this kind of stuff whereas the compiler basically has to guess and the second question is like did you try any other encoding like uh something like uh there was something called group parent I'm not sure we are using floats here but there are other encoding algorithms which might save the space more like yeah yeah this goes in the same direction basically as the the compression question so definitely more this is sort of a I don't want to say a fictional example but it's sort of a simplified example for the the purpose of a slide track ", "type": "Video", "name": "Our Mad Journey of Building a Vector Database in Go - Weaviate at FOSDEM 2023", "path": "", "link": "https://www.youtube.com/watch?v=K1R7oK2piUM", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}