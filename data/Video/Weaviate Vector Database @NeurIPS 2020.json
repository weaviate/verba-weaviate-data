{"text": "Weaviate at the scientific conference NeurIPS 2020 (https://neurips.cc/Conferences/2020/). An introduction to vectorization and ... \nyeah so um welcome againum this time again an online meetupbut organized locally in the netherlandsuh sothis meetup this session is organizedaround the theme of the nurbs as youknow this is a scientific conference onthe neurological information processingsystemsand here the latest machine learning andairesearch and papers are presented andas this year's nurbs event was hostedremotely local meetups are organized inorder to connect withlocal and fellow ai researchers andpeople interested in that field semitechnologies wereselected uh by the nurbs uh boards toorganize and meet up in the netherlandsso i want to thank a nurse for that uhfor this opportunityand uh yeah for you it's a niceopportunity to learn howmachine learning is applied in industryand how researcherscan actually benefit from an applicationof machine learning in industryso this meetup will or yeah thispresentation will consist ofof three parts mainly so uh first i willintroducehow um this machine learning applicationisis applied in industry um yeah thevector searchengine abbreviate mainlyand i'll explain how what factorizationis andand how it is done and why it is doneand then how it enables semantic searchand automatic classificationi will then briefly uh explain howweavieat can also be used forresearch in academia next to theindustry casesand after that we can have um yeah wehavetime and room for questions we can havean open discussion and meet each othermaybeso first first about abbreviate itselfso bv8 is the avi based semantic searchengineand i will start with explaining the theproblem thatuh lies behind what we've heard istrying to solveso we found thatuh 80 of data is actually unstructuredor unstructured textand the problem with unstructured datais that it's really difficult to searchthrough and it'sdifficult to classify and this is hardbecause you need todo some exact keyword matching or somefuzzy search but that's relativelydifficultcompared to if you just have structuredstructured textand within the ai and machine learningdomainwe are we should all be familiar withunstructured data and classificationand models and applications that can bebuilt and trained onunstructured data or data that ismanually structured in order to useimage learning modelsbut this means that it's rather laborintensive for the researchers anddata scientists to build all thesemodelsand in addition it's difficult to scalebecause these kind of models aremade for one use case only andthese two observations that we did inindustry thatleads to um yeah the fact that machinelearning andartificial intelligence are greatsolutions but it'sit's in fact uh not applied very oftenin uh industry cases yet uh withunstructured dataum and that happens because it's hard toimplement for non-data scientistsand it's rather labor intensive becauseyou need to do a lot ofpre-processing of the data if it'sunstructuredand yeah it's not scalable and that'sexpensiveand here's an example i can give on[Music]yeah unstructured text and how you canfindmeaning and additional information andinsights from thatso this example comes from a real lifeuse case of vv8in industry so we have a set of productpreviewswhich is essentially just a bunch oftext per reviewand yeah you can see there's a lot ofof uh meaning and a lot of uh insightscapturedin these reviews but these reviews arefree textso if we use uh traditionalyeah so there's a lot of informationhidden in thereand without any machine learning orabbreviatethese kind of reviews are mainlymanually analyzed by humans working inthis case at this production companyto see if how they can improve theproduct andmaybe if they can do more targetedmarketing to their customersbut yeah as you know this is of coursevery labor intensiveand mistakes can happen there and thenin addition so if we use traditionalmachine learning and search engines forthese reviewsa user will only be able to find reviewsbased on exact matching keywordsso this means that not always the idealset of reviewswill be returned to the user umbecause yeah with different words indifferent reviews you can actuallycapture the same same meaning or sameintention but they will be skippedbecausethe keywords don't match exactlyso that's that's where rev8 comes in andwith abbreviate you can do this kind offuzzy search so revade is a vectorsearch enginewhich it differs from a traditionalsearch engineand this means um yeah in a few words itmeans that it has machine learningmodels built in to index dataand the automatic factor representationof data enables to search through thisdataso you were able to find data withoutusing exact keywordsand linking data with their contactswhich happens automaticallythen allows to search based on theconceptsand fuzzy terms rather than exactmatching keywordsadditionally v8 is a graph like datamodel so it stores data as vectors in ahigh dimensional spacebut it's also possible to connect dataobjectsand you can make explicit relationsbetween data points becauseas we all know data in in real world isalways uh connectedand then vv8 is completely api based sowe have arestful endpoint but more importantly agraphqlendpoint um and with graphqlyou yeah you can make a really specificqueriesand search through your data andthese uh yeah because it's completelyapi based this means you can build anykind ofany type of application on top of vvasand you can use ituh or integrate it with other existingplatforms in your data infrastructurethen we v8 is cloud native so[Music]yeah you can use it and run it at anytime at any place ina distributed setting as well andyeah this means that also bv8 is easilyscalable horizontally and you can run ituhalso in the cloud in production becausewe offerkubernetes clusters for exampleand then yeah as you all know um rev8 iscompletely open sourcedso you can find all the code and how touse it andyeah everything is online on githubwhich makes italso excellent for your own research forexampleand then yeah lastly most use cases whatwe see right now isautomated classification and semanticsearchbut we also see that people use vvate asa yeah actual coredata infrastructure just to to store thedata inin factors instead of in a traditionalwaybecause it's really scalable and um yeahit has also highuh query performanceso i will now go over one uh but thecharacteristics one by oneto explain them in more detailuh yeah so first about this built-inmachine learning model andyeah factorization in general sowhat is affected search engine actuallysofactor search is sometimes also calledsimilarity search or neural searchand yeah what can you do with thisso imagine you have a big pile of datalet's say one terabyte in this casewhich can befrom multiple sources and could beunstructuredand there's probably a lot ofinformation in therebut it's hard to get it out there soquestions that you could think ofyou can ask this kind of data is um howmuchmoney did we actually spend on travelingpost covet 19or maybe you're thinking where can ifind publications related to machinelearningand you know as a user of your own datathatthe the answer to these data to thesequestionsis somewhere in this database umbut you don't know how to get it becausethere are several problems hereso first of all the set of data soone terabyte of data is really reallylarge so how are you going to find theanswers efficientlyor you can just iterate through aterabyte of dataum yeah unless you're okay with responsetimes ofor 15 seconds or so but in industrycasesthis is usually way too long we want ananswer right awayum yeah it needs to be near real timeso that that's the first problem withtraditional search enginesuh trying to find answers to questionslike thisand then the second problem is thatyeah that as i mentioned before in thispile of datamost of your data is probablyunstructuredand so if you think of a sql or sqldatabasewith all fields defined these kind ofquestions might workbut if your data is unstructured andcomes from multiple sources you don'tknowwhat you're really looking for so youdon't know what kind of tables or whatkind ofsearch terms you're really looking forand then the real problem umyeah is actually a third problem andthat's wherethat's how how should you deal with uhfuzzy text based search so now we'reaskingthis database um a human yeah or naturallanguage questionso how much money did we spend ontraveling post code with 19.um so yeah what are what results arelinked to this questionmaybe some of your data only mentions atrain ticket oran airline travel ticketbut doesn't have the exact words oftraveling in thereand with traditional keyword basedsearch you won't be all you'll be ableto find all the data related totravelingum yeah but so still a lot of companieswhat we seeis that they do this manually they needto tag everyinvoice or every traveling invoicemanually to findanswers to questions like this and thisis yeah of coursenot super efficient but with a vectorsearchengine like we've ate uh we can rely onmachine learning instead toautomaticallylike tag or find answers to to questionslike thisand yeah similar we canthink of the the second question sofinding publications related to machinelearning soif your publication literally mentionsmachine learningit might be it might be easy to find itbut what if you have publications thatare aboutartificial intelligence or umyeah or nothing no words like these areused at all but justtechniques like unsupervised clusteringor tools like tensorflow a string basedtraditional search engine won't be abletofind these publications if you'relooking for machine learningbut if you look with effective searchengine thenit automatically knows that tensorflowis related to machine learningand then um yeah if this question isyeah this problem is solved the thirdproblemwith the first research then we yeah italso comeswith another problem actually and that'show umyeah how to do this most efficiently inreal timeespecially so if you want to have aproductionquality search engine where you need tohave near real-time answers to yourquestionsthen yeah this is hard to solve andyou might be familiar with yes somelibraries thatthat solve these kind of factorsimilarity searches uh for youbut this differs from review because uhthese librariesfirst of all they take a lot of memoryand they leave all the dealing withfailures and crashes up to the userbut vv8 is not a simple library likethisand instead it's production focused andoptimizedfor for real searchso now let's take a look at whatfactorization actually is and how itworks within vvaso yeah factorization it's also calledmaybe uh wordembeddings or mapping from words tovectorsor some other search terms and what itessentially isit's a mapping of words or concepts tovectors and vectors are an array ofnumbers or coordinates basicallyof real numbers and vectors in this caseare represented in the high dimensionalspacewhich captures the meaning of thosewords and conceptsso and with what's unique to alleviateis thatnot only words and concepts aretranslated to factors but alsowhole data objects and that's what makesit a databaseand the factor mapping that happens withapre-trained model we trained a languagemodelor based on 20 languages actuallyand used that within vv8 but you canalso add your ownlanguage model to your specific domainif you wantso let me give you an exampleso as i mentioned yeah vectors aremapped to a high dimensional spacein our case this will be 300 dimensionsin which all the words and concepts withalso the data objects are placedin using coordinates and theircoordinates orthe vectors they are calculated based onthe meaningand the relation to yeah to other wordsum yeah here i have a three-dimensionalspacefor simplification because i think noneof us cancan think of 300 dimensionsand what you see here isthe pre-trained language model uh withinan empty review so there is this is justuhan empty viviate there's no data objectsadded by a user yetno data in the database it's just arepresentation of the pre-trainedlanguage model and this language modelwe need laterto give the context and the meaning andlike the smart feature to your dataso here are the pink dots you can see aswords and concepts from the pre-trainedlanguage modelwhich is as i said now available oralmost available in20 languages andwords and concept concepts like this areplaced in this vector space according totheir meaning which is derived fromthe context in a in a big training setso and the distances between thosecoordinatesuh yeah you can think of um yeah of adifference in meaningbasically so for example fashionis really close to sweater because theythey are conceptually really closebut they lie far away from concepts likemachine learning and ai because theythey don't share a lot of meaning butmachine learning and aiare in turn very close to each otherso and as i said this is just apre-trained vector spacebut there's no actual data of the userin there soyeah how does we view that thenuh here's an example so let's say wewant to adda data object we have a database ofpapers research papers and we want wewant to adda new paper about machine learningopportunities for healthcareso if we want to store this data objectumit vva takes on runtimeall the words that are defined in yourdata objectso in here we say we have a paperwe have a title so machiningopportunities for healthcaremaybe it has a summary with more wordsand maybe it has some referencesto other objects and and also here it'spublished in a publicationa demo preview it does in runtimeis take all the words andum yeah make basically one largesentence from themand then it uses the pre-trainedlanguage modeland it creates distances between allthose words in the spaceand it weighs all the words based on uhyeahamongst others the occurrence in thetraining set to determine the finalmeaning of the whole data concept so asyou could imagine thatmachine learning and healthcare are waymore important in determining themeaningof this concept then has an in becausetheythey don't say anything about themeaningso based on all that information anew vector space is for a new vectoror yeah coordinates is calculatedwhich is then placed inside we've ate[Music]and yeah we added here a paper aboutmachine learning in healthcareso maybe this this object is placed inthe middle ofthese three concepts and now if wequery for something fuzzy likea paper around ai or healthcarethen it yeah it ends up also in the samespace and it will return this paperthat's how vivian knows what's to sendwhat what to send back touh to the user on uh query timeso a little bit more about yeah thetechnical effectsso yeah this pre-trained language modelum that we providewith vvate is pre-trained by us usingfastnext and fastex is alibrary made by facebook uh forum yeah training language andfactorizing languageuh and words um yeah based on theiralgorithmso yeah we call this um or we callour own text to factory to fact modelthe contactionary because it givesreally the contextto the language and it enables puttingyour own data into contextuh to its language umand we just released a pre-trainedtext-to-effect model which was trainedby fastexand the previous language models weretrained touh using glove um which is umyeah another library but then fromstanford universityum but we found that fast x performs waybetter in clefclassification and in exploratorytestingand in addition fast text appears toperformway better than than glove withuh significantly significantly lesstraining dataum which means that 100 gigabytes oftraining data withum yeah of common crawlwith fast x performs better than onethousand with cloth so that's a hugeimprovement for usand um yeah but false textis a little bit different from glovebecause it represents eachword as an engram of characters insteadof learning the whole vectorsuh for words directly so in theory thisthis helps uscapturing the meaning of shorter wordsand allows alsothe embeddings to understand suffixesand prefixesand this also allows us to work withreally rare wordsthat weren't seen during training butonly editat user runtimewhich is of course really good for userexperienceand at the moment you can use vvatewith this pre-trained language modelwhich is uhyeah sufficient to work with a greatvariety of use casesand in january uh 2021 we are releasingthe yeah version one of wv8 in whichwe also allow uh custom factorizationmodelsand this means that that you can alsouseother trained language models with vvatean example is birdwe've experimented a bit with bird andwe see thatworks works really well if you havereally long documentsa really long text andyeah or you can imagine otherfactorization models likeum for specifically for questionansweringor maybe image factorization uh or dnasequences and so ondepending on your use caseum yeah so that's it for thevectorizationso then a bit more about the graph likestructure andthe apiso yeah as i mentioned we v8 stores dataas vectors and yeah in a highdimensional spacebut it's still possible to connect thosedata objectsit doesn't have thisit's not a graph database it's a factornative database but it's still possibleto make these graph connectionsand um yeah this is a just a smallexample soif we have a paper and it belongs to acategorywe can link this paper with thiscategoryand in a query this will look like thisso i will go into detailuh to queries later but here we have apaper and we're looking for a paperrelated to machine learningand i want to return the title so norelations herebut now if we also want to know to whichcategorya paper belongswe can also ask for this in the queryso we map these relations if we addthe data and we can create them backso i can show you this in real time alsoso here i have adata set of 1.7 million articles ofarcsave loaded in arcsiff isyeah a service for yeah 1.7 millionpapers in the stem fieldand we have loaded them in and i havecaptured the paper itself with theabstractand some other um yeah attributesuh their authors in which journal theyappear in the categoryand now if i want to query it usinggraphql it will look something like thisso i can make i can ask what kind ofdata we have in thereum here i can seewhat to ask for so let's see what kindof papers we haveand just our title for now and then weseeif i press enter we see at the righthand uh the titles of the papersthen i can also of course ask for moreinformationso for example the abstract uh[Music]yeah let's ask for this i get back tothe whole abstractbut this of course is just some randomquerying any database can do thisand now if i want to show the semanticfeaturesi can make a filterwhich is called the explorer filter[Music]and i can define some concepts that i'mlooking forso this is also what you saw on oneslide earlierbut let's say i'm looking for papersrelated to machine learningi type it in here and i want just thetitle backthennow i see um yeah the results which aresortedbased on how relevant they arewith my search query and i see heremachine teaching a new paradigm forbuilding machine learning systemssecond one is also something withmachine learningyeah and so on and we can make all theseseries uh way more complicated and so onbut i will leave thisthis out for now and then as i showedyou beforeit is we can show the graphyeah graph like uh data modelalso let's look at thethe categories to which it belongsso you can do this like thisand i just want to see the name of thecategory for nowso now i see the same results so this isthe same firstum uh[Music]yeah paper about machine learning and isee all the categories it belongs toso that is in real short uhhow it worksum yeah then we haveyeah as i said it's cloud native it'sopen source so you can find all the codeuh on github and it will always remainopen source as welland yeah what we see mostly now is thatit's used for automatic classificationand semantic searchand we have now definedthe most common use cases we see so wehaveum enterprise search we have uhclassificationsin erp systems erp systems ourenterprise research resource planningsystemsin which classifications are usuallydone manually at the momentbut this is not necessary if you usereviate and then we are alsoinvestigatingwhere you can help in cyber security inbiology withfor example dnac sequences or proteinsequences for exampleand more and then of course there'sa great potential also for future usecaseswhere we don't maybe rely only on textbut also in images or videosyeah and so onum then a little bit about uhmore yeah also the technical side whywhy we v8 yeah and the benefits overalternativesso earlier we answered the questions whyfactorizationand why this is better thanyeah not factorizing or traditionalsearch engineswell yeah the answer to this was thatfactorizationbased on the trained factorizedfactorization model like thecontextionarywill automatically add context andmeaning to your dataand which enables then yeah the smart orsemantic search and automaticclassificationnow as i said before there are existingfactorizationlibraries available also open source sowhat's now then so unique about vv8so yeah pure factor similarity ornearest neighbor libraries are supergreat and they can also have highperformancebut these libraries as the name suggeststhey're just simple librariesand they're no end-to-end solution soyou have to take care of your own datastorageuh you have to take pain to care maybealso your ownactual factorization they use a lot ofmemoryand they leave the yeah dealing withpotential errors or failuresto the user which is uh not veryintuitiveand yeah hard to to to grasp alsoso vp8 is not a simple librarybut it's a yeah really focused onproduction use cases and it's ahighly optimized search engine inproductionso yeah that's first of all thenas i said before the factor uh searchengineum rev8 is factor native soit's not only factor capable but it'salso optimized to handleand deal with and store factors so wehave a vector indexuh part of the core and with this vectorindex weit is capable of handling very verylarge data setsespecially if you also horizontallyscale your your own data instancesand we're using a state-of-the-artfactor indexing technique from researchwhich is calledhnsw and this is yeah the firstindex type we support but in the nearfuture we will support more typesand these will be plugable so dependingon your use case you canuse different types of uh of indexingand yeah a little bit about h sw so itstandsfor hierarchical inevitable small worldand this is a algorithm or a techniquewhich is optimized for nearest neighborsearch and which is fully graph basedso in short when you're lookingor querying some data objects or factorsand you want to know the most similarother factorfactors or data objects in your databaseinstead of storing the whole like alldata objectsof your data and searching through allof them what happens withhnsw is that it search searches onlythrougha small part that it has stored in itsmemoryso it's using very little memory insteadof the whole data setand when it finds yeah in this small setof dataone point that is close to your searchquery it will only browseto other data objects that are close tothat oneso um yeah it's highly optimized insteadofsearching through all the data and itwill still find of course thethe data object that is most similar toyoursearch query so nothing will be left outand yeah this makes it super fast andefficientand actually what makes rev8 reallyuniqueover other libraries or other searchtechniques is that it combines atraditional way ofyeah inverted index based searchwith vector similarity search so you cando bothfuzzy search but also still the keywordbase searchcombined in a really efficient mannerand then yeah finally soa bit short about v8 for ai research soas i saidpreview failed is open source so thismakes it really great for resourceresearch because you can just use itwhen and wherever you wantit is highly scalable so if you havevery largedata sets it's yeah it's optimizedespecially for that so that's perfect uhit has a high performancebased um on the focus on industrybut this of course is really good forresearch research as well if you want tohave areally fast query time and thenfor ai research in particularwe can talk about factorization modelsthatyou can experiment with yourself so as imentionedwe now have our own pre-trainedfactorization modelwhich you can already extend by transferlearningif you have yeah a limited set ofabbreviations or some domain specificwords you want toadd to this contextionaryand this works really well on like alarge variety of domain use casesbut if you want to add maybe likebiological sequencesor images or this bird model and thereare way moremore models available open source aswell thenyou can plug this in yourself from theversion we will release injanuary so that's really great toexperiment with your own machinelearning modelsso if you are for exampleyeah experimenting with new languagemodels newnatural processing uh modelswhat you want you don't want to careabout are you sorry you don't want toworry about the scalability or theperformance you can use vp8 in thebackgroundand use your own natural languageprocessing modelson top of itum yeah that's uh the presentation thati want to shareum so yeah thanks forfor coming and i hope umalthough it was uh way more technicalthan usuali hope you understand uh what it is andof course i'm happy also to explain thenot so technical stuff and how it can beused in industry or in your use cases ifyou're notinterested in the research and in the aistuff itself", "type": "Video", "name": "Weaviate Vector Database @NeurIPS 2020", "path": "", "link": "https://www.youtube.com/watch?v=tKGRyzelDjU", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}