{"text": "Weaviate at the scientific conference NeurIPS 2020 (https://neurips.cc/Conferences/2020/). An introduction to vectorization and ... \nyeah so um welcome again um this time again an online meetup but organized locally in the netherlands uh so this meetup this session is organized around the theme of the nurbs as you know this is a scientific conference on the neurological information processing systems and here the latest machine learning and ai research and papers are presented and as this year's nurbs event was hosted remotely local meetups are organized in order to connect with local and fellow ai researchers and people interested in that field semi technologies were selected uh by the nurbs uh boards to organize and meet up in the netherlands so i want to thank a nurse for that uh for this opportunity and uh yeah for you it's a nice opportunity to learn how machine learning is applied in industry and how researchers can actually benefit from an application of machine learning in industry so this meetup will or yeah this presentation will consist of of three parts mainly so uh first i will introduce how um this machine learning application is is applied in industry um yeah the vector search engine abbreviate mainly and i'll explain how what factorization is and and how it is done and why it is done and then how it enables semantic search and automatic classification i will then briefly uh explain how weavieat can also be used for research in academia next to the industry cases and after that we can have um yeah we have time and room for questions we can have an open discussion and meet each other maybe so first first about abbreviate itself so bv8 is the avi based semantic search engine and i will start with explaining the the problem that uh lies behind what we've heard is trying to solve so we found that uh 80 of data is actually unstructured or unstructured text and the problem with unstructured data is that it's really difficult to search through and it's difficult to classify and this is hard because you need to do some exact keyword matching or some fuzzy search but that's relatively difficult compared to if you just have structured structured text and within the ai and machine learning domain we are we should all be familiar with unstructured data and classification and models and applications that can be built and trained on unstructured data or data that is manually structured in order to use image learning models but this means that it's rather labor intensive for the researchers and data scientists to build all these models and in addition it's difficult to scale because these kind of models are made for one use case only and these two observations that we did in industry that leads to um yeah the fact that machine learning and artificial intelligence are great solutions but it's it's in fact uh not applied very often in uh industry cases yet uh with unstructured data um and that happens because it's hard to implement for non-data scientists and it's rather labor intensive because you need to do a lot of pre-processing of the data if it's unstructured and yeah it's not scalable and that's expensive and here's an example i can give on [Music] yeah unstructured text and how you can find meaning and additional information and insights from that so this example comes from a real life use case of vv8 in industry so we have a set of product previews which is essentially just a bunch of text per review and yeah you can see there's a lot of of uh meaning and a lot of uh insights captured in these reviews but these reviews are free text so if we use uh traditional yeah so there's a lot of information hidden in there and without any machine learning or abbreviate these kind of reviews are mainly manually analyzed by humans working in this case at this production company to see if how they can improve the product and maybe if they can do more targeted marketing to their customers but yeah as you know this is of course very labor intensive and mistakes can happen there and then in addition so if we use traditional machine learning and search engines for these reviews a user will only be able to find reviews based on exact matching keywords so this means that not always the ideal set of reviews will be returned to the user um because yeah with different words in different reviews you can actually capture the same same meaning or same intention but they will be skipped because the keywords don't match exactly so that's that's where rev8 comes in and with abbreviate you can do this kind of fuzzy search so revade is a vector search engine which it differs from a traditional search engine and this means um yeah in a few words it means that it has machine learning models built in to index data and the automatic factor representation of data enables to search through this data so you were able to find data without using exact keywords and linking data with their contacts which happens automatically then allows to search based on the concepts and fuzzy terms rather than exact matching keywords additionally v8 is a graph like data model so it stores data as vectors in a high dimensional space but it's also possible to connect data objects and you can make explicit relations between data points because as we all know data in in real world is always uh connected and then vv8 is completely api based so we have a restful endpoint but more importantly a graphql endpoint um and with graphql you yeah you can make a really specific queries and search through your data and these uh yeah because it's completely api based this means you can build any kind of any type of application on top of vvas and you can use it uh or integrate it with other existing platforms in your data infrastructure then we v8 is cloud native so [Music] yeah you can use it and run it at any time at any place in a distributed setting as well and yeah this means that also bv8 is easily scalable horizontally and you can run it uh also in the cloud in production because we offer kubernetes clusters for example and then yeah as you all know um rev8 is completely open sourced so you can find all the code and how to use it and yeah everything is online on github which makes it also excellent for your own research for example and then yeah lastly most use cases what we see right now is automated classification and semantic search but we also see that people use vvate as a yeah actual core data infrastructure just to to store the data in in factors instead of in a traditional way because it's really scalable and um yeah it has also high uh query performance so i will now go over one uh but the characteristics one by one to explain them in more detail uh yeah so first about this built-in machine learning model and yeah factorization in general so what is affected search engine actually so factor search is sometimes also called similarity search or neural search and yeah what can you do with this so imagine you have a big pile of data let's say one terabyte in this case which can be from multiple sources and could be unstructured and there's probably a lot of information in there but it's hard to get it out there so questions that you could think of you can ask this kind of data is um how much money did we actually spend on traveling post covet 19 or maybe you're thinking where can i find publications related to machine learning and you know as a user of your own data that the the answer to these data to these questions is somewhere in this database um but you don't know how to get it because there are several problems here so first of all the set of data so one terabyte of data is really really large so how are you going to find the answers efficiently or you can just iterate through a terabyte of data um yeah unless you're okay with response times of or 15 seconds or so but in industry cases this is usually way too long we want an answer right away um yeah it needs to be near real time so that that's the first problem with traditional search engines uh trying to find answers to questions like this and then the second problem is that yeah that as i mentioned before in this pile of data most of your data is probably unstructured and so if you think of a sql or sql database with all fields defined these kind of questions might work but if your data is unstructured and comes from multiple sources you don't know what you're really looking for so you don't know what kind of tables or what kind of search terms you're really looking for and then the real problem um yeah is actually a third problem and that's where that's how how should you deal with uh fuzzy text based search so now we're asking this database um a human yeah or natural language question so how much money did we spend on traveling post code with 19. um so yeah what are what results are linked to this question maybe some of your data only mentions a train ticket or an airline travel ticket but doesn't have the exact words of traveling in there and with traditional keyword based search you won't be all you'll be able to find all the data related to traveling um yeah but so still a lot of companies what we see is that they do this manually they need to tag every invoice or every traveling invoice manually to find answers to questions like this and this is yeah of course not super efficient but with a vector search engine like we've ate uh we can rely on machine learning instead to automatically like tag or find answers to to questions like this and yeah similar we can think of the the second question so finding publications related to machine learning so if your publication literally mentions machine learning it might be it might be easy to find it but what if you have publications that are about artificial intelligence or um yeah or nothing no words like these are used at all but just techniques like unsupervised clustering or tools like tensorflow a string based traditional search engine won't be able to find these publications if you're looking for machine learning but if you look with effective search engine then it automatically knows that tensorflow is related to machine learning and then um yeah if this question is yeah this problem is solved the third problem with the first research then we yeah it also comes with another problem actually and that's how um yeah how to do this most efficiently in real time especially so if you want to have a production quality search engine where you need to have near real-time answers to your questions then yeah this is hard to solve and you might be familiar with yes some libraries that that solve these kind of factor similarity searches uh for you but this differs from review because uh these libraries first of all they take a lot of memory and they leave all the dealing with failures and crashes up to the user but vv8 is not a simple library like this and instead it's production focused and optimized for for real search so now let's take a look at what factorization actually is and how it works within vva so yeah factorization it's also called maybe uh word embeddings or mapping from words to vectors or some other search terms and what it essentially is it's a mapping of words or concepts to vectors and vectors are an array of numbers or coordinates basically of real numbers and vectors in this case are represented in the high dimensional space which captures the meaning of those words and concepts so and with what's unique to alleviate is that not only words and concepts are translated to factors but also whole data objects and that's what makes it a database and the factor mapping that happens with a pre-trained model we trained a language model or based on 20 languages actually and used that within vv8 but you can also add your own language model to your specific domain if you want so let me give you an example so as i mentioned yeah vectors are mapped to a high dimensional space in our case this will be 300 dimensions in which all the words and concepts with also the data objects are placed in using coordinates and their coordinates or the vectors they are calculated based on the meaning and the relation to yeah to other words um yeah here i have a three-dimensional space for simplification because i think none of us can can think of 300 dimensions and what you see here is the pre-trained language model uh within an empty review so there is this is just uh an empty viviate there's no data objects added by a user yet no data in the database it's just a representation of the pre-trained language model and this language model we need later to give the context and the meaning and like the smart feature to your data so here are the pink dots you can see as words and concepts from the pre-trained language model which is as i said now available or almost available in 20 languages and words and concept concepts like this are placed in this vector space according to their meaning which is derived from the context in a in a big training set so and the distances between those coordinates uh yeah you can think of um yeah of a difference in meaning basically so for example fashion is really close to sweater because they they are conceptually really close but they lie far away from concepts like machine learning and ai because they they don't share a lot of meaning but machine learning and ai are in turn very close to each other so and as i said this is just a pre-trained vector space but there's no actual data of the user in there so yeah how does we view that then uh here's an example so let's say we want to add a data object we have a database of papers research papers and we want we want to add a new paper about machine learning opportunities for healthcare so if we want to store this data object um it vva takes on runtime all the words that are defined in your data object so in here we say we have a paper we have a title so machining opportunities for healthcare maybe it has a summary with more words and maybe it has some references to other objects and and also here it's published in a publication a demo preview it does in runtime is take all the words and um yeah make basically one large sentence from them and then it uses the pre-trained language model and it creates distances between all those words in the space and it weighs all the words based on uh yeah amongst others the occurrence in the training set to determine the final meaning of the whole data concept so as you could imagine that machine learning and healthcare are way more important in determining the meaning of this concept then has an in because they they don't say anything about the meaning so based on all that information a new vector space is for a new vector or yeah coordinates is calculated which is then placed inside we've ate [Music] and yeah we added here a paper about machine learning in healthcare so maybe this this object is placed in the middle of these three concepts and now if we query for something fuzzy like a paper around ai or healthcare then it yeah it ends up also in the same space and it will return this paper that's how vivian knows what's to send what what to send back to uh to the user on uh query time so a little bit more about yeah the technical effects so yeah this pre-trained language model um that we provide with vvate is pre-trained by us using fastnext and fastex is a library made by facebook uh for um yeah training language and factorizing language uh and words um yeah based on their algorithm so yeah we call this um or we call our own text to factory to fact model the contactionary because it gives really the context to the language and it enables putting your own data into context uh to its language um and we just released a pre-trained text-to-effect model which was trained by fastex and the previous language models were trained to uh using glove um which is um yeah another library but then from stanford university um but we found that fast x performs way better in clef classification and in exploratory testing and in addition fast text appears to perform way better than than glove with uh significantly significantly less training data um which means that 100 gigabytes of training data with um yeah of common crawl with fast x performs better than one thousand with cloth so that's a huge improvement for us and um yeah but false text is a little bit different from glove because it represents each word as an engram of characters instead of learning the whole vectors uh for words directly so in theory this this helps us capturing the meaning of shorter words and allows also the embeddings to understand suffixes and prefixes and this also allows us to work with really rare words that weren't seen during training but only edit at user runtime which is of course really good for user experience and at the moment you can use vvate with this pre-trained language model which is uh yeah sufficient to work with a great variety of use cases and in january uh 2021 we are releasing the yeah version one of wv8 in which we also allow uh custom factorization models and this means that that you can also use other trained language models with vvate an example is bird we've experimented a bit with bird and we see that works works really well if you have really long documents a really long text and yeah or you can imagine other factorization models like um for specifically for question answering or maybe image factorization uh or dna sequences and so on depending on your use case um yeah so that's it for the vectorization so then a bit more about the graph like structure and the api so yeah as i mentioned we v8 stores data as vectors and yeah in a high dimensional space but it's still possible to connect those data objects it doesn't have this it's not a graph database it's a factor native database but it's still possible to make these graph connections and um yeah this is a just a small example so if we have a paper and it belongs to a category we can link this paper with this category and in a query this will look like this so i will go into detail uh to queries later but here we have a paper and we're looking for a paper related to machine learning and i want to return the title so no relations here but now if we also want to know to which category a paper belongs we can also ask for this in the query so we map these relations if we add the data and we can create them back so i can show you this in real time also so here i have a data set of 1.7 million articles of arcsave loaded in arcsiff is yeah a service for yeah 1.7 million papers in the stem field and we have loaded them in and i have captured the paper itself with the abstract and some other um yeah attributes uh their authors in which journal they appear in the category and now if i want to query it using graphql it will look something like this so i can make i can ask what kind of data we have in there um here i can see what to ask for so let's see what kind of papers we have and just our title for now and then we see if i press enter we see at the right hand uh the titles of the papers then i can also of course ask for more information so for example the abstract uh [Music] yeah let's ask for this i get back to the whole abstract but this of course is just some random querying any database can do this and now if i want to show the semantic features i can make a filter which is called the explorer filter [Music] and i can define some concepts that i'm looking for so this is also what you saw on one slide earlier but let's say i'm looking for papers related to machine learning i type it in here and i want just the title back then now i see um yeah the results which are sorted based on how relevant they are with my search query and i see here machine teaching a new paradigm for building machine learning systems second one is also something with machine learning yeah and so on and we can make all these series uh way more complicated and so on but i will leave this this out for now and then as i showed you before it is we can show the graph yeah graph like uh data model also let's look at the the categories to which it belongs so you can do this like this and i just want to see the name of the category for now so now i see the same results so this is the same first um uh [Music] yeah paper about machine learning and i see all the categories it belongs to so that is in real short uh how it works um yeah then we have yeah as i said it's cloud native it's open source so you can find all the code uh on github and it will always remain open source as well and yeah what we see mostly now is that it's used for automatic classification and semantic search and we have now defined the most common use cases we see so we have um enterprise search we have uh classifications in erp systems erp systems our enterprise research resource planning systems in which classifications are usually done manually at the moment but this is not necessary if you use reviate and then we are also investigating where you can help in cyber security in biology with for example dnac sequences or protein sequences for example and more and then of course there's a great potential also for future use cases where we don't maybe rely only on text but also in images or videos yeah and so on um then a little bit about uh more yeah also the technical side why why we v8 yeah and the benefits over alternatives so earlier we answered the questions why factorization and why this is better than yeah not factorizing or traditional search engines well yeah the answer to this was that factorization based on the trained factorized factorization model like the contextionary will automatically add context and meaning to your data and which enables then yeah the smart or semantic search and automatic classification now as i said before there are existing factorization libraries available also open source so what's now then so unique about vv8 so yeah pure factor similarity or nearest neighbor libraries are super great and they can also have high performance but these libraries as the name suggests they're just simple libraries and they're no end-to-end solution so you have to take care of your own data storage uh you have to take pain to care maybe also your own actual factorization they use a lot of memory and they leave the yeah dealing with potential errors or failures to the user which is uh not very intuitive and yeah hard to to to grasp also so vp8 is not a simple library but it's a yeah really focused on production use cases and it's a highly optimized search engine in production so yeah that's first of all then as i said before the factor uh search engine um rev8 is factor native so it's not only factor capable but it's also optimized to handle and deal with and store factors so we have a vector index uh part of the core and with this vector index we it is capable of handling very very large data sets especially if you also horizontally scale your your own data instances and we're using a state-of-the-art factor indexing technique from research which is called hnsw and this is yeah the first index type we support but in the near future we will support more types and these will be plugable so depending on your use case you can use different types of uh of indexing and yeah a little bit about h sw so it stands for hierarchical inevitable small world and this is a algorithm or a technique which is optimized for nearest neighbor search and which is fully graph based so in short when you're looking or querying some data objects or factors and you want to know the most similar other factor factors or data objects in your database instead of storing the whole like all data objects of your data and searching through all of them what happens with hnsw is that it search searches only through a small part that it has stored in its memory so it's using very little memory instead of the whole data set and when it finds yeah in this small set of data one point that is close to your search query it will only browse to other data objects that are close to that one so um yeah it's highly optimized instead of searching through all the data and it will still find of course the the data object that is most similar to your search query so nothing will be left out and yeah this makes it super fast and efficient and actually what makes rev8 really unique over other libraries or other search techniques is that it combines a traditional way of yeah inverted index based search with vector similarity search so you can do both fuzzy search but also still the keyword base search combined in a really efficient manner and then yeah finally so a bit short about v8 for ai research so as i said preview failed is open source so this makes it really great for resource research because you can just use it when and wherever you want it is highly scalable so if you have very large data sets it's yeah it's optimized especially for that so that's perfect uh it has a high performance based um on the focus on industry but this of course is really good for research research as well if you want to have a really fast query time and then for ai research in particular we can talk about factorization models that you can experiment with yourself so as i mentioned we now have our own pre-trained factorization model which you can already extend by transfer learning if you have yeah a limited set of abbreviations or some domain specific words you want to add to this contextionary and this works really well on like a large variety of domain use cases but if you want to add maybe like biological sequences or images or this bird model and there are way more more models available open source as well then you can plug this in yourself from the version we will release in january so that's really great to experiment with your own machine learning models so if you are for example yeah experimenting with new language models new natural processing uh models what you want you don't want to care about are you sorry you don't want to worry about the scalability or the performance you can use vp8 in the background and use your own natural language processing models on top of it um yeah that's uh the presentation that i want to share um so yeah thanks for for coming and i hope um although it was uh way more technical than usual i hope you understand uh what it is and of course i'm happy also to explain the not so technical stuff and how it can be used in industry or in your use cases if you're not interested in the research and in the ai stuff itself ", "type": "Video", "name": "Weaviate Vector Database @NeurIPS 2020", "path": "", "link": "https://www.youtube.com/watch?v=tKGRyzelDjU", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}