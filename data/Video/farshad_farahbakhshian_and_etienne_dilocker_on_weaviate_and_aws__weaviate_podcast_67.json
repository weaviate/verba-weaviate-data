{"text": "Hey everyone! Thank you so much for watching the 67th Weaviate Podcast, announcing Weaviate on the AWS Marketplace! \nhey everyone thank you so much for watching the weba podcast I am incredibly honored and grateful to host these guests on the podcast this topic will be absolutely epic firstly we welcome back we VA Chief technology officer and co-founder Eddie and dillocker Eddie and thank you so much for joining the podcast again thanks for having me and of course I am beyond excited to welcome farshad farabakshian a principal Solutions architect at Amazon web services farshad thank you so much for joining the podcast hey thank you so much and by the way my My Title Here is actually gen AI specialist but a lot of times people think I'm a Solutions architect so it's all good nice to meet you all so could we uh kick it off with like um I think maybe you know some for our newer listeners could we maybe even start from the beginning Eddie and if you could kind of just you know what is weeviate even just kind of like you know alleviating the evolution of it that's led us to this point of you know talking about with AWS about running it on the cloud and kind of how far the journey has come gotcha yeah sure let me try to keep this brief because I don't fill the entire podcast with this and and hopefully as I saw on the the self-hosting part we can definitely dive deeper into that later in the security privacy Etc as box uh but basically vb8 is a vector database or for those of you who don't know what that is it's essentially the long-term memory for for a large language model so for retrieve login to generation for example definitely something that we're going to dive into I guess uh and bv8 is an open source Vector database you can use vv8 as a SAS offering from from us so that's our BB account service but you don't have to if you don't want to if you don't want to give your data away you can self-host it or you can help us help you host it in your environment in your Cloud tenant in your VPC that's one of the USPS of vb8 because that allows you to to stay in control over all of your data and then if you have that you can query bb8 with Vector search so dense embeddings use dens embeddings for for um for the modern day retrieval you can do the classic stuff so bm25 you can mix it with hybrid you get all kinds of other database operations and all that at Great scale and uh yeah that was that was my elevator pitch yeah I think that's awesome I think just you know this core of this hsw Vector index doing you know fast nearest neighbor search is you know and then especially with this retrieval augmented generation I think this has caused just a boom and you know the explosion of how often people are using these kind of things so uh farshad can you maybe tell us about kind of you know how you see the state of vector databases and retrieval augmented generation yeah I mean at a high level most customers that I talk to they're like oh we just want to fine tune everything right and you know fine-tuning is great it's it's how you teach your model new skills but it's not necessarily the best way to have it no knowledge um and so Vector databases is the most reliable way to do that and so one of the most common use cases that we see is hey uh here's a really simple one right like we want our customer service agents to be way more efficient right and we want them to very quickly go through a very relatively large knowledge base and retrieve precise information there's honestly not much reason to do fine tuning there unless you want your language model to sound different in how it interfaces with those customers but if you want to retrieve specific info like something in a data sheet Vector databases are very powerful that so pretty common architecture that we see come up as a strong llm plus that could be eight Vector database combined to offer that solution yeah it's amazing the chat Bots thing like the whole like chat with your documents you know chat with your specific customer tickets I think is just you know an incredible evangelist for all this and yes I think we've kind of set the stage of what the technology is you know we V8 Vector database helps you retrieve relevant information to put in the context of llms let's get into the Super exciting topic that you know we have our special guests the technical experts on you know what does it take to run this in the cloud so a couple things right there's so many ways of doing this in the cloud right now right so I'm gonna go top to bottom right um and let's kind of view it as I view the LM as kind of like the center that everything goes around um if you want to work backwards from a proprietary proprietary llm like say from like an anthropic um uh or a cohere um you generally are limited to however the cloud provider lets you access that in the case of Amazon we have a service called bedrock what Bedrock is is an API service that lets you do prompting and depending on the model provider actually fine-tuning as well right so you have your API portion of that what's interesting about Bedrock is that we never share the data or the prompting to the model providers so what's kind of interesting is that even though the model providers say that they will never use the data our customers are like yeah but we still want someone in between right so that's why Bedrock is one of the value propositions right so you have your API call goes to um the the model hosted on AWS but you can't access the weights it's all opaque and then around that is when you have to start thinking about like well what I want to do on top of that right but in parallel to that whole thing is you have open source models llama 2 just came out right I think it was last week kilometer two is doing really well right and it's also open source with something like llama two you can use services like Bedrock um but you can also use services like Amazon sagemaker jumpstart which the difference between sagemaker jumpstart and Bedrock is that it's not serverless users have to actually manage infrastructure as well um but some people like that some people like having more knobs to control um you can also fine-tune llama2 right so now once you start getting to some of these models that fine tuning is a part of it some people like to have data engines that they incorporate RL HF into it right so here's a really simple example a lot of folks that build diffusion models the way they uh get human feedback is they'll give you like four low resolution models and then they pick the user picks they wanted to make want to make high resolution and then now the model provider knows which one we've got the RL HF right um so then you have a data engine involved right and then this is all using managed Services we also have a ton of customers that want to pre-train their own models uh a lot of folks tend to actually do this on bare metal ec2 right so we'll get your a100 to look at your h100s we have our own proprietary chips called traniums as well uh which support things like llms and diffusion models and they'll basically take like no joke like up to like 4 000 gpus in one cluster to train one large job right so they get huge so there's so many different ways to do it and we're not even going to the inference side of it but this is kind of like the model piece of it right and then depending on how you're going to be using that model you're going to be interfacing with a vector database like like from leviate and you're obviously going to be using services like S3 to store your data Maybe maybe you have a data Lake involved as well um the interesting thing that is a little bit different on training versus inference is that when you're doing training all you really care about is like your time to train right it's just like train the model as fast as possible uh when it comes to inference all of a sudden you care about how everything plays with each other right it's like how tight does your vectorated database interface with your llm how tight does that eventually lead to your user experience right and that's probably it's going to be dependent on use case right so like for example if you have a live customer agent that's having real-time conversations with customers you're going to really care about latency at that point right but if something is like oh we have a week to get this response it's a complete completely different architecture so now you start getting into use case dependent architectures that decide whether or not things are going to be good and then once you start really camera latency model distillation becomes important as well right you want to get the model to be as small as possible some people want to get these models to work on the edge um you know I I can think of companies trying to build you know uh metaverse I mean whatever whatever you want to call it where latency becomes so important where I can see a world where you're going to have custom ml chips for inference to host these models locally to just super reduce that latency for those use cases but anyway I just talked a lot so I'll let you ask me any questions about that but I just wanted to give you like an overview of like what is this whole ecosystem look like from an infrastructure perspective yeah I'd say quickly with yeah I mean first I thought it was a really great overview I loved hearing all I mean I I think I first heard about like sagemaker with some of the stuff that hugging face was doing like kind of for me person like you know coming out of my PhD in machine learning and starting to like learn about what the products are and how this really works in the real world and yeah hearing I mean the Bedrock API just so many interesting topics I think um yeah maybe it would be great to kind of just talk about that integration between llms and Vector databases a little more you know just like um maybe Eddie you could explain kind of how we design our generative modules and sort of how we think about that kind of thing yeah yeah specifically I want to want to start with something on that part you mentioned the training objective of training this potentially as fast as it as you can train it and I assume that the cycles in which you would retrain they're relatively slow right or relatively long Cycles so I'm curious sort of how does the the ability to change your retrieval algor or industry algorithm for your retrieval system on the Fly Like does that sort of be before if you wouldn't do retrieval augmented generation and you would potentially have to retrain your model every time it has to get new knowledge in so how do these sort of how do these Cycles differ and and uh when do I use what basically yeah that's a super cool way to categorize it right because what we're starting to get into is like now we want to have a categorization and intuition for what do you want to train for and what do you want to use a knowledge base for um I wasn't a call of anthropic last week and I really like the way they think about it they split it into skill versus knowledge right so if you start to think about like what is a skill versus knowledge right let's let's use the lawyer example a skill that a lawyer has is understanding the law the knowledge that they have is recalling different laws right so you're definitely going to have use cases where you want to build your model such that it builds an intuition for what it's doing but you don't want to train it to understand every single law out there it's pretty inefficient to do that and it's probably not reliable right so if you feel that and if you if you categorize it into those two different buckets my my thought process my intuition tells me that you can do way less training right because if you try to use training to be have something be knowledgeable I mean you're all you can do is like approach what you can do with the vector database even then you could never be fully as reliable as a vector database right so I would say if you're building an llm um categorize it into those two things once you feel that your Alum has the right skill base to sound like what it needs to sound like whether that's a customer service agent or lawyer or programmer um or uh you know a musician whatever that may be then it's time to start focusing on the vector database and I'm willing to bet it's like a 90 10 ratio difference right if you start to start using knowledge for Vector databases you could probably do way less training that you thought you need to leftist differentiation skill versus versus knowledge this is great so I'm thinking in my head when you brought up the lawyer I'm thinking that the lawyer sitting in front of this shelf of books and the lawyer doesn't have to memorize every single book they don't have to learn it by heart because they can they can just look it up and essentially that's the retrieval part in in this this awesome yeah and I I really love just like the separation I think like kind of like operationally like if you need to keep fine-tuning the llm this is kind of one of the first things that Drew me to retrieve augmented generation is you know if every time you have new knowledge you need to retrain the model and then rebuild the apis and all that kind of stuff compared to this thing where you know the vector database is easier to update kind of like well how do you think about that kind of aspect of it of how just the retrieval augmented setup you know it lets you update it just with way less overhead than kind of maybe a more traditional mlops thinking of like keep training keep training yeah I mean I would even take it a step further I would be willing to bet that for a lot of use cases if you use a vector data base and you take advantage of a relatively large contacts window you may never even have to do any fine tuning at all right so the vector database is going to eat up a lot of the fine-tuning that you traditionally think you would have to do in the past um and we're seeing these models have very large context Windows right like today anthropics context window is a hundred thousand tokens which means you could input an entire Harry Potter book as your prompt right so that in combination with the vector database I start wondering well you're probably you're still going to need folks that do fine tuning right skills limited things Etc um but I think you're going to be surprised how much you can get away with with a vector database plus some good prompting okay that kind of brings us the opportunity to put that uh that training effort potentially into the vector database because the vector database also uses a retrieval model that commonly like like with the um the latest models from cohere and open Ai and all these these models I think so if this has been put in in the shade a bit like people don't talk about fine-tuning it as much anymore because they become sort of more general purpose Etc um but before that I think that was also a very very common topic about fine-tuning those those models and I'm thinking if you have that that skill set in your team to to train a model like potentially put that Focus instead of putting it on the the large language model just put it on the embedding model and and potentially get better domains for your uh domain there yeah like the um you know the search relevance improving the search all the knobs you can turn is so fascinating one thing I really love farshad is you bring up that like long context Windows is this idea of like searching across multiple information sources like you know you're you have the You're Building like a customer support chat bot and so it might search through one index of like previous you know customer support threads as well as searching through say the weba blog posts or maybe the you know the code base if needed but like combining all these different information sources they call that like Federated search and yeah I think just this idea of how you combine uh maybe like multiple indexes together for retrieval augmented generation I think that's one of the like emerging frontiers of just you know packing that prompt with as much information as possible to complete the task yeah no I think uh you're right I just actually learned about this recently I saw some like you know block diagrams of how this is done on the llm side um the llms uh look like donut trajectory to know which database to go to to do different things right so they're gonna like have an understanding of the question and then have like a probability of what each database is likely to answer this question and then go to that database right which uh is interesting because I think that's actually a little bit different architecture than before right previously a lot of folks would like just go directly to the data and then give that to the LM to generate an answer but now it's kind of the opposite now you have an llm accepting the prompt understanding it deciding which database which Vector database for example to go to and then joining in response right so that's really cool like we're starting to enter like oh this thing feels very intelligent right so I really like that um and I think in general I the analogies that I like to think about that we're going to eventually go towards is like the human brain where I have different parts of my brain to do different tasks right there's a part of my brain that does math which is probably not the part of my brain that fuels emotions and that's probably not the part of my brain that recalls information right and so what you start to look like is you you're gonna probably want like something that has logic to know what part of the brain to go to or which which LM to go to and then this example we're talking about feels like it starts to approach that because while it's not different llms that it's going to it's different knowledge parts of What's called the brain or the you know whatever and it starts to go to but if you can have something know where to go for different types of questions there's nothing preventing us from doing the same thing with L alums in the long term right hey this is a math question go to the math element the reason why that's important is because your brain would have to be so big if it didn't have efficient smaller parts you're going to do things and to make that analogy with llms it's like we can create much more efficient models over time and it create real products right there's going to be limitations of cost and compute for specific use cases unless we get these distilled smaller models to do very specific tasks right so I think it's really exciting to hear about this new architecture type this this reminds me a bit of that question of whether a hybrid surge is potentially zero shot or or few shot because in in hybrid surgeon in our case we have one parameter that can be tuned and that's essentially this Alpha parameter that tells you give more weight to either the embedding based search or give more weight to the the uh a keyword based search and if the the model has some reasoning like potentially it could narrow this down already it could say like hey this looks like a brand name I'm probably going to want more of a of a keyword match or something or just like like a product description or something so this could be a move in that direction as well to to sort of give the model as you say more intelligence more more domain knowledge about the back ends to just use them better and and do better retrieval from them yeah super exciting yeah that's brilliant Eddie and I love that like um you know the llm that knows how to search you know friends I've been talking to know that I don't shut up these days about the gorilla large language models which are like you know you've instead of using a 200 billion parameter gbt4 model or whatever to to make weeviate queries you train a seven billion parameter and then it's much more efficient to run and exactly what you were saying far shot I agree strongly with this that they're going to be kind of llms over all of the tools that that the LMS know how to conf how to control the apis like edian's explaining with taking a query and saying should I wait bm25 more heavily than Vector search for this one or adding the symbolic wear filters to queries and generally also I think just um there's kind of two things to it like large language model tool use the tools have apis and so the language model needs to like you know behave exactly format the you know they call this like constrained output sampling or structured parsing where it's like the llm needs to format the request perfectly so this kind of tuning I think is perfect for that and I think yeah I love the parts of my brain kind of analogy and how each one is smaller because if it's everything is a large model then it's like kind of expensive to tune but it comes into Vector databases too because like in addition to having specialized LMS to control each tool you also have specialized Vector indexes that contain different types of knowledge maybe editing you could talk about this is kind of something that topic that I always love thinking about is like when to separate my data into multiple classes in weave maybe also for new listeners we could quickly do what a class is and like when to separate it into different classes how to think about adding filters versus uh classes yeah the the original motivation in off of the sort of class-based schema in vv8 was to reflect the real world so in an earlier version of V8 we even had a distinction between something a thing or an action that would sort of you know describe either either things or action that's long gone because that led to more confusion in the API than it actually helped but it kind of it's It's where the history of where the classes come from like they were meant to describe real world things so if you have books put your books in the book class if you have um I don't know chapters or something put them in the chapter class and that that kind of like I'm already sort of going to the the next use case of this which is chunking up stuff so um uh for for these retrieval tasks very very common question or very common optimization problems basically how small like what what do I obtain a vector embedding for do I just create one vector embedding for my entire book great if you're just trying to identify the right book If you're trying to search through your book that's not granular enough do I obtain a vector embedding for every word in my book well great if you're trying to find words not very good if you're trying to find context because you're losing all the context you're sort of going back to the original steps of of vert to VEC so um they're most likely something like the paragraph or maybe if it's a long book and you're just trying to get a high level search and maybe a chapter in the book something like that and that is something that you can use the the uh vva class model for to retain that Association for example between a class and a chapter sorry between a book and a chapter in that book and that allows you to for example search through chapters but then make a relation to the book and then maybe do something without so if you could potentially sort of make these graph like connections and even jump to the author then authored these books and as for your questions like when should I use these links between objects of a class as opposed to a primitive filter this is mainly a performance optimization so A Primitive filter is cheaper to run VBA it has a bitmap index for uh for primitive filtering basically for for building these allows for filters so if you want to get the most performance out of it it's probably faster to to do it sort of on the object itself so essentially like a denormalized schema in in sort of SQL slash nosql terms um and and if you use these links it's maybe sometimes easier to reason about so that's kind of the the trade-off do you want something that's more easier that's easier to understand versus something that's more performant yeah amazing and yeah just understanding that kind of filtered agents like how filtered search works with the vector index and all that I think you kind of quickly mentioned desk SQL with a different perspective but like this I think also kind of in our broader conversation of just llms that use tools and like selecting which class book book chapter and that's like one kind of llm query routing task but there's also kind of like is this a vector search or is this an SQL search like if I say what is the average age of my podcast listeners not that I have a database that I have that but like that would be a SQL kind of query not a vector search query so yeah I think just all this kind of routing and you know it kind of yeah like this system architecture kind of thing of like yeah like you know I don't think it's quite the same topic as multi-tenancy but it's like kind of like how you have all these different classes that you run in the cloud then and like you know maybe we could talk a little more about like you know what does it take to and we could actually kind of pivot the topics kind of broadly into just like what kind of requirements are we seeing to be running Vector indexes in the cloud can I actually is it okay if I ask a question so it's related to the the question you're asking to Connor I'm really curious like what has been like the one or two or three use cases that relatively novel and we're really surprised to see how well it worked while using a vector database to get there like wow this works really well for this use case right have their band some of those come up and what were those use cases what was surprising about it was there anything interesting about the architecture to get there yeah I think initially sort of if we if we look at the pre-lm era the main use because of course Vector databases existed before before chat GPT basically so the the whole sort of using them as a building block for retrieval augmented generation is relatively new so before that I think it was mainly just search with contextual understanding so these like very small differences in in the input query like using the word not as a negation or something or for example searching for something that is that has wings but is not an airplane or something traditional keyword based based matching it would match for airplane and then it would also match for not which sort of completely mess up the the um the the result quality so I think this initially this was my my sir for me one of those moments where like okay wow there is such potential in this so so basically the classical semantic search in a sense uh and then post llm or post a Chachi PT um yeah it's gotten more into into all these kind of chat bot and and um there was this moment we're like wow this is this is so much bigger than just semantic search there there's all these all these systems that I think the first time someone talked to me about generative AI in general it was that was way back basically for me it was a bit hard to make the connection like okay yeah that's that's an awesome field but I don't think that's a related feeling at some point oh actually it is and that was that was such a um I think that's not not an exact answer to your question of what are the great use case but this sort of in my personal Journey that was one of the the things we're like oh wow okay there's just so much more to it and uh yeah seeing the first sort of chatbot applications uh that that users do uh um specifically serve the chat with your documents kind of use case that's that's such a great one this giving this sort of more interactive kind of kind of uh feel to it and then especially if this is like the typical demo cases that you would see they're they're always like take something from Wikipedia and this like general purpose kind of kind of General general knowledge kind of things but then seeing our customers implement this on like super domain specific data like in the financial industry or something thing that is so cool to me because it's it's yeah it's it's sort of this this very these very boring documents where I imagine people spend so much time going through them like extracting all the information and now you can automate it and you can automate it with something as simple as a single query that where you tell the large language model hey retrieve the right stuff and then do something with it so that I think was was for me uh definitely one of the eye opener cases maybe Conor maybe you have a couple more yeah I think probably the biggest eye-opener for me I think was you know Bob was teaching me about um oh it's called Uh like I don't think it's called disruption Theory sorry Bob I forgot the name of this thing but it's like but it's like about uh you know it's about like this virtuous cycle of how you kind of like have um distribution and customer experience and so kind of for me just like that personalization component that's achieved by llms that to me I think it like just how I can you know go through the Twitter you know like I like we all manage a ton of relationships right like in the technology industry and it's like it's impossible for me to keep up with what all everyone is doing every week for example and so just having llms that can like summarize what you know like say Michael going neural magic have done this week and maybe like connect it with what I'm doing and like automatically generate these reports for me just this kind of like you know the personal experience that you're able to give to potential users of your product as well as kind of like partners and Integrations by having the llms like combine knowledge sources that's probably that's probably my favorite one yeah super cool and by the way that was totally a selfish question on my part I just wanted to expose myself to what you all are saying being in this space um and yeah that's super exciting and I think you mentioned magic did I hear that correctly oh yeah well that's a friend of mine who's um you know the company he works at this company where they're working on LM inference acceleration it's just like an anecdote of saying like you know they they have they post these new updates like you know I don't know what their Cadence of how many blog posts they do per week but let's say they do like four per week and let's say I have 20 such technology companies I want to keep up with and now it's like 80 in a month and you know I can't be expected to read all this so like that kind of thing it that I think that just helps us connect with each other also like this kind of like social yeah I think that I mean I guess like one more thing I really enjoy just that I've seen is exactly what we were talking about earlier where we're talking about searching across multiple indexes uh Eric also weave she's been exploring like llama index has built these really great tools on top of weavate as the vector database and then they're exploring like the sub question query engine so I asked a question like a weeviate feature what is ref to VEC and then it routes the question to one index of blog posts one index of podcast transcriptions and then one index which is the leviate code base and I mean the code base we could talk about all sorts of things how you can use the levers of weviate to search through code bases but just seeing that in action the routing queries to different indexes that is a mind-blowing thing of retrieve augmented generation for me super cool awesome so okay so I I would love to kind of pivot our topic into a little more about um you know we we Eddie and I'd actually like to kick this off with kind of like um you know one question I've always kind of found really interesting is like so with AWS cloud services you have like all these different machines and I've seen some things like of maybe like benchmarking weeviate with like different you know different particular machines how do you think about that kind of that kind of thing so I've seen me talking about like instance types yeah yeah yeah um so I think what's helpful in this context is talk about like how do good how do how do customers go about doing this and and why do it this way um the best way to do this is when you're trying to work backwards from like what's the best instance type for me to do what I'm trying to do with Eva um it's just to do a proof of concept right so this is something I do pretty commonly with a lot of customers that I work with um we get an understanding of what they're trying to do like what are some performance kpis they're looking for and we'll do some testing across instances right so there's so many different instances at AWS right whoever you're working with whether you have an account manager or or like a specialist like myself um they'll be able to guide you on like what are the options to test maybe narrow down to like three or four different instance types and then do some testing um and and see where see where it goes right like sometimes you have you know are you focused on and when I say this your use case will determine what instance type you use and there's really common patterns for specific use cases right you have all these building blocks from AWS and you'd be surprised how often customers end up selecting the same exact instance for a given use case right and what that means is that there's a lot of tribe knowledge within AWS like when you're talking with someone and you're like hey this one use case it's like we're only going to have intuition for what's going to be us but we don't want to tell you we want you to get that conclusion on your own right so we're going to set up an environment for you to test it right because this is how the tech World works right like we want you guys to make the best decision for you and will help guide you get there but ultimately you need to get that data for yourself right so you know versus taking my word for it then you've built the best solution for your company right so that's generally how we do it uh ask your account manager for credits along the way so you're not paying for it right so usually we can do that as well there's obviously limitations right like you can't have like a one year long POC for example but yeah like we're more than happy to help and that's generally a good protocol to do it data data data great to hear this this use case-centric nature of it because this is exactly what we're seeing as well like benchmarks are always lying in a sense that they're optimized for one specific thing whatever you want to show in that that particular Benchmark and I think for especially for for users who are new to all of this if all they have is a benchmark they're just going to pick whatever in that particular Benchmark is on top even if that's not at all what they're neat what they need so for example often benchmarks are optimized for for either throughput or latency which is kind of a similar thing if you ignore that that doesn't necessarily have to scale like often they're single threaded and then you'd have multi-threaded queries and these kind of things um but something that that comes up way more often in in the discussions that we have with our clients is actually cost and none of these benchmarks optimize for for cost so the question is like what is your actual query load what is your throughput and then you see that that all these Curves in these benchmarks their actual throughput is so much lower and really what we have to optimize for is cost and then in bb8 that could for example mean turn on compression which is sort of another latency versus versus accuracy versus um uh versus a memory footprint a trade-off um but also of course machine sizing like do you really need the machine that has that allows you to to I don't know 100 have two thousand as opposed to 1 000 queries per second if you have one query per second and then you get these these machines that are optimized maybe for for more memory relative to CPU Etc and they're similar as as you said um their patterns we also see these patterns like either your your uh very sort of on the on the throughput side then on AWS I think this is typically what we see is like m6i or r6i machines uh or maybe you're more on the yeah on the operating cost side then we see more for for example uh the the I think m7g or the the AWS graviton I think they're they're called so um long story short I really like this it's it's so like yes of course we would like to to tell our users yeah just do it like that or it Compares like that but really if you want the best possible infrastructure setup you just need to adjust to your use case because every use case is different yeah I totally agree it's such a like honestly a distraction to be like oh this is the best thing for this and um the the best customers that I've worked with and when I say best customers what I really mean is what are the customers where I'm like wow I would join that company I really like the way they're doing things right so the customers that I'm considering best in that context um they don't follow benchmarks like they go down to what matters for me and not going back to like the AWS perspective nine times out of ten price performance is the name of the game so not just performance not just price but price performance which really means going down through a use case right or am I getting the best bang for my buck the only time price performance is like been kind of de-prioritized a little bit is if security gets involved like a security standard where we definitely can't use it this way even though it's the best price performance so therefore we're willing to play pay more to do this other way right but most of the time you're right and it's it's price performance right which was really use case dependent I guess kind of for me I'm curious about um you know price performance and how you tune it and then I'm like I've always been curious of me I I hope I I hope this doesn't go too into the kind of the details of like vector indexing and Vector search but this kind of like you know you can tune hsw by adjusting these type of parameters like EF you know Max connections basically like how densely connected that proximity graph is for how you route these queries and and you know I I think it's really interesting with you know we've eaten now has like product quantization which is reducing that memory by compressing the vectors so maybe um yeah I hope this isn't too boring of a topic like for people who aren't super excited about kind of nerding out about how exactly you tune the the index for the different computers but I'm sure that our listeners are you know are interested in that kind of conversation like so how do you think about uh tuning with this question for Eddie and who's really been you know in the weeds of this kind of thing now I'll try to make it exciting um so so in in a nutshell these are all parameters that you can tune for a specific trade-off and typically that trade-off is either index time query time or memory footprint so you mentioned Max connections for example that's a parameter that literally leads to to so agents W is a graph based index having more Connections in the index means more memory because each connection is essentially stored as yeah as a a number you can think of it sort of like the outgoing edges to other nodes are just un60 forms or something like that so this is one of the knobs that you can turn that make the index bigger but then because it's it's bigger it's better connected it's it's more sort of it's easier to go sort of this this uh small world example I think that we've used in another uh in in previous podcast so if you're if you're trying to go from from a person in Europe to a person in let's say Asia then you just need to know someone I don't know somewhere in the middle or so let's say have these like I think there's this theory that that with five or six or seven hop officer so you can basically get to any person in the world and and that's that's uh the nature of that that proximity graph so the more connections you have the easier it is to reach your goal with with fewer Hops and that's one of those those uh turning knobs and then um especially in in combination with product quantization that you just mentioned so product quantization reduces the memory footprint of the vector embeddings but not of the graph so if you want to get your your total memory footprint down and the fact that you're using a product quantization could be an indicator that you've maybe already said on that trade-off of query speed versus memory footprint and therefore operating cost you're already leaning towards reducing operating costs so then one way to potentially reduce it further is you could say in my index I want to take that that same or do that same trade-off and I want to trade off a bit of index memory footprint for higher latencies and we say like okay fewer connections which means the the graph footprint is smaller and then in turn maybe you have to search for it a bit longer so this would be the query parameter that you would have to to uh tune so this gives you um the effect on the infrastructure in your cloud is essential deck if you tune this well you could end up with the same use case either needing 16 gigabytes of memory or maybe 32 gigabytes of memory and um of course they're not they're not identical like you would have for example higher career performance on the one with 32 gigs as opposed to a 16. but if you take that into consideration then um it's it's easier to to plan your infrastructures they have lots of ways that said vv8 has reasonable defaults so um there is a certain size where all of this tuning doesn't matter and we try to pick defaults where it can just get started and probably you'll know when you have to tune them because you'll either reach out to us or you'll start running into like memory limitations where you look at your AWS Bill and you think like hey maybe maybe I can get that memory footprint down of it um so this is for for mainly for the heavy production focus in large use cases um but yeah it's good to know that you can tune these things if you want to yeah and so I get kind of like this is kind of the question that sort of you know selfishly I was hoping to get answered in this podcast is I've always been you know from when I first was learning about weave and I learned about horizontal scalability kind of you know and I've I think I have a rough understanding of how you Shard each of these kind of uh classes and then how you scale out shards across machines and so yeah I'm just so curious to hear you know hear your perspectives on how uh scaling out machines works in the cloud particularly you know in this setting where you're running like a weeviate cloud service in AWS and so wev8 has something like you know again I I understand this a little bit sometimes it's my best to set up the question but like I know you kind of have things like kubernetes where you have like here here's kind of like the code for how you would scale up to more machines so yeah maybe you guys can take the Baton from here like you know how are you how are you thinking about like how we V8 can have a service that then within AWS would say okay we need you know 8 30 two gigabyte RAM that kind of thing so I'm gonna set up egn for this question I'll give like the high level answer and then I have a feeling you're gonna know the very detailed answer how this actually works in in your use cases that you've seen so at a high level AWS offers something called Auto scaling which is basically like horizontal like vertical sailing is when you basically like increase the horsepower of your instance type whether that's like bigger GPU or more RAM whatever horizontal is basically you add more instances to do whatever you're trying to do and I if I recall from my ASA days this is like the associate Solutions Architects course that you can take to learn more about AWS you can do things like monitor like your CPU usage or your memory usage as triggers to know when to do some horizontal scaling um so now I'm going to pass the mic to eating to talk about like well what things do you monitor do horizontal scaling what are things you look for how do you actually architect it using orchestration like you touched some kubernetes right kubernetes is one way of container rising and then doing scaling um but yeah I'd love to hear like what are the specifics of what UC customers do yeah kubernetes and vv8 are kind of intertwined in the way but also they're not so there's nothing in VBA that that will make it kubernetes specific but it's just one of the the operations decisions where we said like you get so much for free with kubernetes service Discovery scaling rolling updates like all these these kind of things that we said like this is just how we want to run bv8 um technically you don't have to you could just spin a manual ec2 machines or so and just just uh build it that way um on the the sort of cluster architecture slash design part um what's sometimes a bit difficult in Vector search is that um indexes are hard to split up once you once you have them so sometimes you're actually forced to do vertical scaling which you kind of want to avoid because well if you have a replication then at least you can do it with with zero downtime but it's way easier of course to do horizontal scaling and one way that you can achieve that with vv8 is the multi-tenancy feature because the multi-tenancy feature schedules each tendon so each tenant is essentially like a lightweight chart in in vb8 and each of those those tenants is scheduled on exactly one or if you spread it out on multiple nodes but that means the next time you add a tenant it can be scheduled on a new note so if you would say your load is let's say you have a multi-tenancy application um someone can sign up for your let's say personalized chat Bots like a user can sign up can upload documents and can chat with their documents and then you have 10 000 users then each of those your users would be represented by one tenant in in BBA and of course these 10 000 users like the moment you start using V8 you would have some of them but your goal is to sign up more of them so ideally you add more users every day and this is where it gets really interesting because at some point you're going to run out of space and then you can very easily scale horizontally you can just say Okay add more nodes to the cluster and then all your new tenants that you sign up from from that point on they will be scheduled to those new notes and that gives you basically infinite linear failing for the number of of notes one thing that's that's not released yet but that's currently on a roadmaps then also to rebalance this and redistribute this so right now it's like once it's scheduled on a specific node it's fixed but that is an upcoming uh feature as well to then give you more control over moving that data around and this is this is one of the common cases that we see where people basically use horizontal scaling to um to increase the capacity of their their Vector search costs yeah I mean that's that's perfect you can ask for a better way to scale than aligning your horizontal scaling with number of users right so that's that's perfect yeah that was super interesting I I love that kind of yeah the the way that you firstly separate you know horizontal scaling and tie that with multi-tenancy and then compare it to Vertical scaling when you need it because of the vector index I think you know the the the cloud service that orchestrates managing that for large apps I I find that all to be so fascinating and kind of like one other topic I'm very curious about it's um kind of related to this idea of having like a product that manages cloud services is kind of what becomes different when you're running it like privately like you know we see this with like you know just as a random example last night I attended a generative AI Healthcare Meetup in Boston and they're all you know we need to run this privately so like what does that mean from the perspective of aw you know AWS understand it's like public Cloud I can just kind of like go to uc2 and get running how does Amazon think of our AWS think about uh private cloud um when you say private Cloud I just want to make sure we're talking the same page here are you referring to like someone owning like a data center within a Colo or are you talking about like something where hey um this needs to be all happening within my VPC within my own cloud single tenancy for this one company everything does not leave this this thing right so which one of those two you're referring to um I'm certainly interested in both I think um I think like kind of as I mentioned that like Healthcare in Boston thing the latter Probably sounds like that case where you're you know really like nothing can leave here and I I think the other thing that's really an interesting part of this topic is kind of like having the llm the large language model as well as the vector database both be in that kind of setup together I I think yeah yeah we can talk about both of those so um let me let me talk about the latter first because this is where I do talk to my customers about and I'm certainly happy to share like a perspective of what I'm hearing about like colos and data centers um so as far as AWS and keeping things extremely private um let's kind of walk through like a little mental model of what an architecture could look like so when you're using AWS you can use Virtual uh you can use vpcs right and within a VPC like you can be totally isolated from the rest of the internet or even other AWS users right um in that context you can easily have your vector database be in there right so everything's within your VPC right whatever instance you want to host we beat on for example can be there and then the question becomes well okay let's say that I want to um make sure that my prompting never leaves my bpc either I don't want anyone to see my prompting and I don't want anyone to see the responses from that prompting right um the the first level of like Standard Security that you can do is by using like a service called like I mentioned earlier Amazon bedrock with bedrock the way that it works is you effectively get an API within your VPC and it can be set up in a way where um basically via private link it connects to where the model is hosted which means you can't see the weights of the model but because it's private link and it's single tenancy this is all things you can forget with AWS uh what that means is nobody else gets to access that stuff right so you can almost think of it as like you have your VPC bubble and it kind of extends out to another cluster somewhere but it's all single tendency no one else gets to see it and it comes back to you so in that scenario everything's completely private and it's only for you um and we have customers that like for example have fedramp um security standards that they need to reach right so I just talked to a customer this last week that's using a different service called Amazon sagemaker jumpstart where you can host models uh in a similar fashion within your VPC um and let's say for example you wanted to use llama 2 right with jumpstart you would provision some instances um for your sagemaker endpoints that your vector database can interface with and that endpoint you get to decide what type of instance that is right so for example you can have llama 2 hosted on a100s you can have llama 2 hosted on inferential two and you manage that but what we do for you as a user is make sure that that instance is single tenancy and you're the only person touching that and no one else gets to see that so you're now completely containing how you're interfacing with these llms whether or not their proprietary or open source right and this does come up quite a bit and when I was making the comment earlier about like price performance and sometimes security this is basically what I was alluding to right like so if if for example a customer most nine times out of ten customers like I'm gonna as long as everything uh is done via bedrock and I know that it doesn't go to the model provider I'm happy security wise and then let's just focus on price performance but like what if a customer says no I need additional layer of security then maybe for some reason they have to use sagemaker jumpstart because they there's more knobs there right and at that point it goes back to price performance and in the case a lot of times what I see is like a lot of times you know these these models are pretty large and that means you have to use pretty beefy pretty large instances to hold them right so I don't know how much um off the top of my head like how much memory llama 2 takes for the 70 billion but there's a good chance that you either have to run it on a100s or inferential two right so if that's the case like these are beefy instances they get expensive so you you better and this is where you start to think about well maybe we should use a 13 billion or the 7 billion parameter version right so this gets in the whole price performance thing right um you're always working back for some use case but basically everything worked back Works backwards from from a privacy perspective keep everything within the VPC nothing leaves um that seems to nine times out of ten address security concerns from customers um there are other use cases allowance go into like the whole KOLO data center part of the question there are customers that have data that they just cannot put in the cloud period they cannot leave their own their own data centers right so in those use cases um obviously I don't have as much purview into that given that I work at AWS but in those use cases these customers tend to buy like pre-made clusters from other companies which you may or may not be elastic um that part I'm not sure about but I'm pretty sure that you know if you're if you're a super large company that owns your own data center you're probably going to find a way for it to be elastic and I have no reason to suspect that they couldn't do horizontal scaling in that type of environment as well but I think that most companies are able to use LMS in the cloud with the exception of companies that have data that cannot leave their own on-premises data centers yeah no that's really amazing and firstly thanks so much because I I hear that question a lot too and now I feel like I have a good answer right I think it's also interesting whether you need the vector database private or the llm private or both and and kind of maybe some nuances around that and yeah I think this kind of like single tenant all that kind of thinking around how you do this thing yeah all that's really interesting uh Eddie and I'm curious if maybe you have some thoughts on like um yeah that kind of like tenancy and the and the vpcs as farshad describes yeah yeah so with tenancy because I talked about multi-tenancy before but in a very different context so it's kind of a a difficult term because before multi-tenancy referred to our users having multiple users in this case now tenancy refers to sort of the isolation boundaries within AWS so um so so let's say our chat bot application from before is an AWS customer they don't want their data to be mixed up with competing chatbot application but still within sort of this single tenancy setup they might have multiple users so it's a a term used in multiple settings nevertheless the concepts are exactly the same for us so data that shouldn't leave a certain realm and this is this is something where um the the sort of again those using the same term the single tendency nature of your own bv-8 deployment in your private VPC in your Cloud tenant um that that gives you the capability to stay completely in control so I'm touching upon this at the very beginning if if you want just something fast up and running just use your VBA cloud service or you deviate cloud service in that definition of tenancy is multi-tenant so that means other customers that use the vv8 cloud service run in the same AWS project for example on our cloud service if you have these requirements for for strict isolation you can run it in your own um yeah we say in your own VPC and the VPC is in your Cloud tenant so it's basically whatever kind of boundaries you you want to Define and you can do this this is something that we're we're launching right now as we're recording this it's not public yet maybe it will be when we're depends on it depends on how long it takes to edit this or we're just about to launch on the bvl not on the on the AWS Marketplace we're going to deviate on the AWS Marketplace um and it's a one-click deployment kind of setup where we can do exactly that so if you have an existing VPC you can deploy into that VPC if you're starting from scratch you can create a new VPC and this is the the kind of setting where you can make sure that because your data that's contained in vv8 is then wherever V8 is deployed so exactly the same concept it can never be accessed by anyone else it can never be mixed up with someone else no one else has access you are completely in control if you tomorrow you decide you want to tear it down you want to delete the disks you are in in that kind of kind of control which is great for for all kinds of yeah security compliance Etc settings uh the the downside basically is that it's it's potentially a bit more operations effort because now you're running vb8 and this is where our hybrid SAS idea comes in so with hybrid SAS basically the vva team helps you run this so depending on how much data care you're willing to give away for example one thing that you could opt into is saying like hey I want to push my metrics to the vva team then what we can do is monitor the metrics so you can say like hey you're about to run out of XYZ or we see a pattern here there or maybe if something happens um then then and you're reaching out for for support to us and we can say like okay we we have the kind of Diagnostics uh tools that we need or another option is like even if you can't even share metrics then we can also in the in the sort of uh least common denominator would be we can jump on a zoom call and maybe there's maybe a sort of figure out what's going on together so you get like various levels of of support contracts and support uh settings in that that kind of hybrid SAS setting where um yeah the the sort of underlying thing is you own your data and we help you with the operations part as much as you want yeah I could I could see could see a lot of customers wanting to use the marketplace option you're talking about where it's within the customer's VPC um I would love to do a Blog with you where we show customers how easy it is to do and maybe like a use case that it works really well for um because I think a lot of customers would appreciate that awesome let's do that yeah I'm sure all listeners the podcast are going to be excited for that blog post as well and um so it kind of one you know Eddie and you mentioned you know we get on AWS Marketplace I think that's one of these big headlines with our podcast and like something we're super excited to announce and uh far sure I'm just really curious like um you know like we V8 you know the startup and now being on the AWS Marketplace if you can maybe talk us through like um you know the what does it take to get your cloud service on the AWS Marketplace what does it take to get your cloud service on the AWS Marketplace um well technically Eden knows more than me now so since he's since he's actually gone through it but at a high level um AWS has so many different ways to partner with um basically companies or startups um and what we do is there's two methods maybe more one method is through AWS Marketplace Marketplace basically makes it easier for you to consume um partner products and services through AWS Marketplace and what's kind of interesting is that if you like have you know contracts with AWS Marketplace can actually count towards that right so in other words there's incentive for companies to go out there and to use Marketplace to consume things um rather than going directly to for example BBA right so in other words there's incentive for me to use consume mean to consume VA through database Marketplace and go directly to them right and it makes things easier for billing reasons right you go through one place to do everything and the process for doing it is basically you apply um I if I'm not mistaken you can also in parallel be a part of the AWS partner network if you choose to and that has two categories one is consultant and one as a technology partner you can think of Consulting as like hey you can hire our developers to help you build whatever whether that's infrastructure or app level and we can do that and then at the technology level it's basically like SAS things right so we be for example be a technology partner and be on AWS Marketplace um it's not necessarily a difficult process but there is like a the process of going through like architecture reviews making sure things are set up and meaning standards with AWS um and the most important thing I would say is like if you're a company thinking about APN or Marketplace um I've I've never I'm gonna speak anecdotally here right I have to be careful here um it's not a you do this and you get additional Revenue it's not the way to think about it the way to think about it is this thing uh you've already got a fire you've got you've kindled your fire it's going and this thing is fuel it will scale you it will add much more opportunities to your existing opportunities right and the way a lot of Enterprises think about this is um you make it frictionless for me to consume your SAS product you're making it frictionless right so if if no one's ever heard of you before it's going to be very difficult for a Marketplace to magically get new customers but if you already have a presence in the world an Enterprise can say oh and you're in Marketplace so this is going to be easy for me to consume you that's how you should be thinking about it right so it does work but there's like a level expectation of how it works it's important to consider we have a lot of companies that make a ton of money off of marketplace and APS very successful for them but what I don't want to do is mislead someone and say you do this and you magically get Revenue through AWS right yeah that perfectly matches our experience I think the the motivation to onboard into the marketplace was because we had customers asking like hey I would love to buy this through the marketplace but you're not on it can you get on the marketplace yeah exactly awesome well Eddie and far should I I love this podcast I think it was such a great tour you know beginning with kind of Trends and Vector databases and retrieve vlogments generation stepping into this you know machine provisioning horizontal scalability multi-tenancy you know all these Cloud stuff I just it's so amazing so both thank you so much for joining the podcast and sharing this knowledge I learned so much thank you both super fun yeah same here thank you ", "type": "Video", "name": "farshad_farahbakhshian_and_etienne_dilocker_on_weaviate_and_aws__weaviate_podcast_67", "path": "", "link": "https://www.youtube.com/watch?v=uHFM4sVHNxk", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}