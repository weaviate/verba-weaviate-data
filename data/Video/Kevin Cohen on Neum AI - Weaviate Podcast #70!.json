{"text": "Hey everyone! Thank you so much for watching the 70th episode of the Weaviate podcast with Neum AI CTO and Co-Founder ... \nhey everyone before diving into thepodcast I want to highlight thisincredible article that new mayi haspublished that'll be linked in thedescription of this video the blog postis titled retrieval augmented generationat scale building a distributed systemfor synchronizing and ingesting billionsof text embeddings this is a reallygreat article going through all thingsdata ingestion and particularly buildingthis distributed messaging system fordifferent parts of this process fromconnecting to different data sources tochunking and embedding it's a superinteresting system and I learned so muchfrom talking with Kevin so quickly Iwant to highlight uh this imageparticularly because about uh 16 minutes16 and 1 12 minutes in the podcast we'regoing to reference uh this figure two inthe article this particular diagramabout uh how this messaging queue isstructured with uh doing differentthings like the chunking and how youkind of like isolate each PDF say sooverall this is such a nice articlethere are so many insights about Uthings like this celery task CU these uhdistributed import benchmarks and uhlooking at at things like the number ofthreads per machine and just all sortsof interesting insights so I just Ihighly highly highly recommend checkingout this article and thank you so muchfor watching this podcast where we'lldiscuss the things in the article butyeah I just wanted to make sure it'sknown that this is linked in thedescription and it is just a fantasticarticle hey everyone thank you so muchfor watching another episode of theweeva podcast I'm super excited towelcome Kevin Cohen the CTO andco-founder of n AI there's so muchexciting things happening in the spaceof say ETL for l M or data ingestioninto Vector databases this whole flow ofuh reading data connecting it todifferent sources like notion geub uhyou know S3 buckets all these thingsthen chunking metadata extraction EMBbedding so this is such an excitingspace and I'm so excited to interviewKevin Kevin welcome to the podcast thankyou Connor Thank you super excited tohave you to to have me here and andhonestly you bring this excitement thatit uh it motivates everyone here that'sthat's talking to to really get excitedas well so awesome so I think kind ofstaying on the exciting story can we getinto this uh you know what motivated youto start new Ai and how has the journeybeen so far yeah for sure uh this is athis is a quite a long journey but II'll summarize it uh me and myco-founder uh we're both ex Microsoftand uh we uh we applied to Y combinatoruh a couple months ago and we gotaccepted with uh this idea that we hadabout building a restaurant but uh andit's completely different to what we'redoing with neom but uh one the insightsthat we had what we were doing thisrestaurant bot uh playing with GPT andllms and and embeddings and all of thesethings is uh the the the notion ofsyncing the data from like Source into asome sort of vector storage where youhave the embedding so that you could doretrieval augmented generation and allof these applications because what wewere doing at the very beginning wassort of like very naive in which if uhthe source data would change we wouldnot reflect that into our um embeddingsand Vector storage and so if you wouldask a question to your to yourrestaurant bot like do you have thisitem on the menu the bot would answeryes but in fact you didn't have thisbecause you know let's say that thecontext had changed from the menu and sowe we got this insight into hey maybe weshould actually focus on the tech andbuilding a platform that um allowsdevelopers and people to actually makethese applications easier because weunderstand what's going on with the datasyncing the data pipelines theinfrastructure so that dat needs tohappen specifically for large scale andthat's what we decided to you know focuson and and and the birth of NE justfocusing on the on the uh platform sideof things yeah it's amazing I Idefinitely want to come back to kind ofthe experience at YC combinator and I'veseen a lot of AI companies and whycomary we'll come back to that but Iwant to stay on the the restaurantexample uh keeping the source of data insync with the vector database I'm verycurious in further understanding thatbecause I understand you know we8 to belike a you know it I guess there like indatabases there's like olap oltp liketransactional processing versusanalytical queries so I kind ofunderstand uh the vector database to besort of like a transactional processingthing like your restaurant perhaps wouldbe doing crud updates directly to WEAversus say it has a notion template thatit needs to keep in sync with WEA couldmaybe expand on that point a yeah yeahyeah yeah yeah so um the main thing withrestaurants isthat they don't want to uh get like newtechnology or new things on top of whatthey already have right so therestaurant really uh is based on likethe point of sales system that they haveand so there's tons of different pointof SES systems but that that that'spretty much what what dictates how thedata is stored in their systems andtheir databases and whatnot so if you'rebuilding an application on top of thatfor whatever needs you are you probablywant to connect to the point of sale sothat you can get the most upto-dateinformation and so what we were doingnaively as I was telling you is that wewere basically getting the um connectionto a point of sale uh let's say squareright and we would be uh dumpingwhatever menu items they had one timeand we would be chunking embedding uhand and and putting it into into avector database and we would do you knowyour general rag application where youwould ask hey do you have this item fora menu and things would work but as youcan tell if the point of sale systemwould get updated because let's say theyadded a new item or they removed an itemor they change the price or whatever itis then your your your your vectordatabase or or or the place where yourapplication is actually pulling the datafrom would need to have that contextwould need to know that somethingchanged because again the source ofTruth is the point of sale it's not anotion document is this eithercomplicated system or easy to use likedepending on the point of self that youuse that that the restaurants haveaccess to and so you want to make surethat the source and the destinationwhich is what we like to call it are insync so that if the your application isis is is is going and doing semanticsearch for example to a specific Vectordatabase like we then that thatinformation needs to be sync so that thethat so that the the bot can respondaccurately so that's what that that'spretty much the Insight that we had umbecause we naively just started with asimple basic idea and we figured well Imean this is definitely a problem issomething like this changes everytime yeah you've definitely sold me onthe idea I think like one of thearguments around the development of we8has been to have many differentprogramming language clients so we haslike you know addition to like Pythonand JavaScript there's also like Javaand goang and we had Andre on thepodcast who talked about like uh Rubyclient for we8 and so I think that's onekind of argument on this like connect itwith how restaurants or governmentdatabase systems integrate with thesethings but I like what you're saying alot where you know you have a squaredata like date Square operates like adatabase and then you sync it with thevector database and I can definitely seethat argument so yeah so I think kind ofDove right into the data connectorstopic our listeners we're doing this endto end of data ingestion and you I dothink data connectors though is just oneof the most exciting topics because Ithink about you know I I love thinkingabout rag for my personal note takingbecause selfishly I want to be you knowleveraging these Technologies and I feellike just this idea of like connectingmy notion with say the we8 GitHub issueswith the weeva blogs with my Twitterfeed this kind of like syncing of thesources sources it sounds so powerfulcan you tell me about just like how yousee the opportunity there yeah for sureman I mean and and and look this wasjust the restaurant example that reallysparked our interest there but um wherewe really want to go is um what otherapplications can actually be powered bythis and and how are Enterprises eventhink about even thinking about this uhthis type of problem and there's there'sbunch of people doing rag as youprobably know this uh and and really oneof the key main things that we want todive into is what are sort of sort ofthe the the business requirements thatare happening at this level ofsynchronization because um therestaurant idea is actually true andit's actually a problem that we facedbut um we want to try and alsounderstand which is another topic that Ihaven't touched but we will touch lateron the large scale sort of thing if youhave millions and millions and millionsof of records of data or unstructureddata like your notion files although Idoubt that you have millions of notionpages but uh it could be um so how doyou actually go and and and chunk thoseefficiently and extract those and embedthose and and we we'll talk aboutembedding scash and synchronize thoseand so really going into what are theselarge scale Enterprises um uh businessrequirements that will'll go and seemore into in in how business areadopting Rag and and more of the Genapplications that's where we're tryingto really focus on and and dive verydeep yeah that is super sold it allmakes a ton of sense and so I think okayso I think kind of this data connectionargument is is pretty solid I don'tthink anyone would disagree with theimportance for this so I think kind ofmoving then into our kind of endtoenddata ingestion topic The Next Step wouldbe this kind of like chunking andmetadata extraction and I maybe alsowanted to you know quickly get yourthoughts on chunking in metadataextraction because I want to like Ithink kind of well sorry I think kind ofconnecting this with the connectorsthing is very interesting because Iimagine like my notion Pages for examplethe reason I like using notionpersonally is I love having this likerecursive how I can like create a pagewithin a page kind of so having the themetadata of like the title of the page Iimagine with PDFs you the metadata islike that you know if your PDF has likea scientific paper it has like a titleand so you trying to look for those kindof things I think it's such a deep topicso at the maybe we'll also come back tothis at the end of thepodcast it's such a deep thought let letme just give you like the the the twosentences that I have on this is thatone of the things me and my co-founderhave been thinking about is the wholetopic of like extraction and chunkingand smart pre-processing that I think uhyou've also heard with with other peoplethat you brought in into the podcast tome that is still a research topic like Idon't think anybody has a chunkingmethod that works for everything thatdoesn't exist because as you said likescientific paper chunking is waydifferent than how you chunk Json or howyou chunk financial documents or how youchunk any different things have it'svery um requirement and businessspecific and so there's a bunch ofthings that that have been tried um uhin the industry and we've been thinkingabout some of these things simple thingslike hey we can allow the user tospecify what metadata they want becausethey know their context or we will do abasic recursive chunking and and andbasic metadata extraction based on somefields that you were saying like titlepage number whatever okay but then youcan go and and I'm sure you've heardabout this as well about the smart chunkand smart metadata extraction where youcan maybe leverage llms or even onstructure which I know that they theycame to your podcast right and and howcan um you correctly or smartly assignmetadata and chunk based on the type ofdata that you're having and so we'vealso done a little bit of that over atneom but it's it's it's definitely sucha large uh piece and and important pieceas well but it it it's still in its veryearly stages in terms of like to me it'slike a research uh topicso yeah I think it's so fascinating likeyou know learning from about like n andunstructured and seeing how this kind ofMarket is developing I could almostimagine these categories within dataingestion evolving furtherbecause the smart chunking thing makesme laugh because we were joking aboutthis at the AI conference and Eddieand's like we should do a blog post onSmart chunking and I'm like well itdefinitely sounds better than dumbchunkinglike add smart to itI I don't know what we've all convergedin industry to like smart chunking ifyou go to Twitter everyone's like ohyeah smart chunker it's like okay whatdo that what what did we have beforedumbchunker that's upgrade yeah that'sfantastic and I think kind of likethere's like Nils ryers who's been oneof the biggest like early pioneers of AIpowered searches been explaining thisconcept of like multi- discourse how ifyou have a paragraph that has multipletopics like you know a lot of the timesis I've been one of my favorite apps isum is searching through this podcast andsomething I've noticed a lot is whenyou're talking for a while you tend tochange the topic in the middle of thetalking and so that will cause a reallyfunky embedding so I think that's one ofthe best opportunities for chunking isto extract that kind of like this isseparated the topic and yeah exactly andso so yeah I think metadata is such adeep topic I may want to actually savethat for later let's now d uh sorry diveinto how you're thinking about uhembedding inference management yeah yeahthat is this is another big piece andand maybe at the at at the end we'll uhum we we'll talk a little bit about likehow how the whole end to endend uh Worksin terms of like the wholeinfrastructure and and uh and talk alittle bit about that but embeddings isdefinitely a huge piece here of courseyou want to do semantic search you wantto store the data in the vector DB youwant to have the embeddings but there'sso many things that come into picture sostuff like which model you want to useyou want to use an open source model youwant to host it yourself you want tohost it in replicate you want to host itin pH you want to just use open AI allof these things come into play umdepending on the model that you chooseyou have different dimensions in yourvector and so people at the verybeginning might just go with oh let'sjust go with openi it's very easy to useand yeah you can use that openi has 1536Dimensions with their Ada Z2 that's finebut when you're actually dealing with ascale and you want to insert all of thisinformation in the vector database thenthe dimensions that you choose for yourembedding models is crucial it can saveactually huge costs if you don't use amodel that has a lot of dimensions ofcourse if you have less dimensions thenmaybe your accuracy is is is um not asgreat as the one that you have moreDimensions so as in software engineeringit's all a trade-off but these are allthe things that you need to really takeinto consideration when when you want toyou know vectorize and and maintainingthis this source and this destination inin sync and the other very importantthing for embeddings is that um you knowif if you're building like proof ofconcept of applications or like hobbyistapplications where you know you have acouple of documents and you want toextract and you want to embed and youwant to store it's virtually very easybut if you have a large scaledata and you want to make sure that youembed and vectorize that and you put itinto the destination you don't want tore-embed everything if you alreadyembedded it what I mean by that is ifyou have you know gigabytes of data umthat you want to put into the into intoyour you you want to vectorize you wantto put into your vector database and youwant to run that let's say every couplehours or every every whatever Cadenceyou wantum you don't want to revalorize thethings that didn't change so you need tohave some sort of like embedding scashand this has again you know uh Lamaindex and L chain they've talked aboutthese things as well but how do youproperly determine whether somethingchanged so that then you can figure outif you need to do the um calculation anduse the compute of your model for foropen AI or whatever you use and then andthen store in Vector because chances arethat if the data did not change you didyou do not need to to to waste thosecompute resources or and that eventuallyURS towards dollars right and so youneed to you need to have this concept ofwell I want to embed and I I need to putit into the vector DB but I need to makesure that I don't do the I don't do itblindly uh you do it with with with umconscience that okay um this thing'schanged now I need to re-embed then I'mgoing to insert it insert it and sothat's the whole piece of like theembeddings management and the embeddingscash that we've also played around withso um we can essentially you know savethe costs to to the end user who wantsto move thedata yeah so I I want I think first i'say I think this is another category ofthat kind of um you know ingestionkeeping the source and the destinationin sync I love how you expl this kind ofmotivation it's quite novel and yeahI've seen this before like with umpeople who are doing like code search ontheir GitHub and when they do a new pollrequest they're saying hey should I revvectorize now this function and that'sprobably one of the big opportunities islike LMS for code putting your big codebase into a vector DB and themiscellaneous nuances that comes withthat but um so yeah it was kind of so II do I think I want to come back to thiskind of Park the um you know how we keepthe sources in sync and and learn moreabout the embedding cache but right nowI want to get into kind of the end toend flow of new that you describe inthis really excellent article aboutthese these three different messagingcues that you orchestrate you presentcelery and how that works for having uhrequests process documents and embedstore can you maybe walk through thearchitecture of how you have thesedifferent distributed messaging cuesyeah yeah for sure for sure maybe maybeit helps if people can see like the thediagram and the picture that that wehave in that in that blog post wherewhere where they can see how haveeverything uh links to but I'm you canput like a link to that um but look atthe core of at the core of it Connor isreally just a distributed system uh andin this case it's a rag pipeline thatyou need to do embeddings and store surebut at the very core is the problem ofan engineering distributed system whereyou have lots of data you need to beable to do retry logic you need to beable to do parallelization you need tobe able to do resource computemanagement and you need to be callingsome API some different providers youneed to use some CPU some compute um andthen you know whatever end to end thingyou want to do which in this case ischunk read um uh embed and thenvectorize uh that's your end to end butuh at the core of it is this distributedsystem and this is pretty much what meand my cofounder have had experiencewith in the past uh in Microsoft andwhatnot uh on building datainfrastructure and data pipelines anddata platform for distributed systemsand so um what what we ended up sayingis well what are the three main thingsthat we need to do in order for fordoing this rack Pipeline and and weseparated it in and as you as you saw onthe blog post into like three maintopics which is there's a request thatcomes in um into our service uh wherethat request is going to be um quicklyprocessed and figure out like whichsource this request is from for exampleS3 in in the case that we talked to theblog and then um we want to return tothe user right away saying hey wereceived your request we're working onit because it's it's an a sync operationof course it's something that takes timeand so then that um from from that specfrom from that first step we go and wewe we enter into the our distributedcues so that we can process multiple ofthese requests of course uh and and wesend them to our next step which is heynow I'm going to be now that I know it'sS3 and I have this many files then Iwant to process each of these filesagain naively you you don't need tobuild the distributed system if youdon't want and you just Loop througheverything and you process everythingand like it's going to take an eternityand not a very efficient thing right sothat's why you want to enter thisdistributed cue messaging uh distributeddistributed message cues um and and youwant to be able to paralyze and so heynow you are at the file level that youwant to process for example in thisspecific X S3 example and so okay nowI'm going to process each of the fileseach file might need to do some chunkingin and of itself and so now you need toprocess each file each chunk and now youmaybe want to say okay now I have eachof these chunks of this file I'm goingto send them to another processor againso that you can parallelize and anddistribute the the the different taskswhere the last essentially um cue thatyou have the last step here is okay Ihave this file from this location forthis specific chunk now I'm going tojust embed this thing and do the thevectorization like you do thevectorization and then you put it intothe vector database and so it's it's awhole process of like how do youdistribute all of this load in terms ofthe reading and the parallelization sothat when you get to the embeddings it'sjust this unit of operation that youknow you can just send to either an opensource model or whatever you want to dofor for vectorization and then insert itinto the vector database and we'll talkinto the the the vector database all ofthe we stuff that we did uh in a secondbut um essentially this is the wholeflow and so in the end it's adistributed system and and because ofthat you also want to make sure that youhave the appropriate logging and theappropriate dead letter Q mechanisms andreach TR logic and hey you had 100 filesbut you processed only 50 files what'sgoing on with the other 50 what's thestatus of this 50 etc etc etc so youhave like this State Management alongacross all of these tasks and and makingsure that you can return to the userinformation as well as the logs so thatyou know if they want to know what'sgoing on that they can so there's a lotof moving pieces and I know I talked alot but it's it's really the core oflike a distributed system where ifyou've had experience with this in thepast you could build it but then youhave now the the added it of theembedding cash and the embeddingmanagement and the vector DV nuances andand whatnot so um it's way easier ifpeople see the image as well buthopefully if people heard me in thispodcast then they can see that they cansee the the image and and and understandwhat we're talking about but um yeahI'll add the a quick intro to the blogpost and so people can jump back ifthey've already seen it I suppose but umyeah so I I really like this separationbecause I imagine with the processdocuments you know so you take a PDF andyou chunk that into like 50 Atomic unitsthat then are you know tasks in theembed store exactly and then you havethis fail Logic for if one fails andmaybe we need to go back to it yeah it'sall super fascinating and I think uhkind of listening I think to have abetter question to ask you next it wouldbe important for me and maybe ouraudience hopefully our audience iscurious as well to understand more aboutthe embeddings cache if you could tellme a little more about just particularlyhow you see that yeah yeah yeah so atthe I'll try to explain it in like thevery simplest terms because I thinkthat's that's what probably willresonate the most but you get the dataand you want to vectorize the data andyou want to get the embedding so thatyou can put them in the vector DV wellwhat you want to do is be able to havethis sort of cache so that for aparticular text let's say for aparticular um input that you'rereceiving let's say like the hash ofthis like like how how a typical cashWorks let's say the hash of this uh textis stored alongs the um uh embeddings uhof whatever that text was so that on thenext run whenever a new um unit ofoperation as you correctly put it comesin you can quickly check hey does thehash of this um of this text um uh hasalready uh been seen in our embeddingcach because if it already has then youdon't need to embed because you knowthat the embeddings are going to be thesame and you just wasted the compute forthat when you already have the dataright and so chances are that you don'tneed to rebed um for that particulartext because you already have that intothe vector database and so this iscrucially important when because one ofthe other things that we have at NE isthat you can run like scheduledpipelines right so um maybe I didn'tgive this overview but uh in NE you whatwe've essentially done is a pipelinemanagement and a pipeline platform foryou to run this end to end applicationsfor rag uh you can um trigger like aoneoff pipeline you can then schedule apipeline or we have some sort of supportas well for um uh listening for eventsfor for for for specific sources so thatyou don't even need to worry about thisschedule we just listen to whatever haschanged like CDC kind of thing uh sothere's there's some sources for exampleS3 can let you know if a file changedand then you can listen to that eventand whatnot we could get into that uhlater but the whole point is that youhave all of these functionalities and ifyou're you're if a source does not havethe CDC capability of you didn't use thethe the the CDC uh capability for one ofyour pipelines and you're rerunning thescheduled one or you were testing oressentially you're running uh pipelineson the same Source then you want to makesure that you don't do overwork you wantto make sure that hey this Source didn'tchange this source is still the samelike let's not embed it let's justreturn very quickly and so that's thewhole point of the embedding scash sothat uh essentially you don't you don'twaste the the the time and you don'twaste the compute and the cost on onreeding something that has notchanged yeah that that makes so muchsense I I think it's really nice it withthe whole theme of you know keep thesource and the destination in sync I'mreally starting to understand it betterand yeah that yeah that cach yeah ofcourse it makes a ton of sense if you'rereinges your whole notion page and allnot ex so I think now would be a greattime to transition topics you know we'vecovered kind of the high level theconcepts behind end to end dataingestion now we' started getting intothe queue and how the qes and new andhow that works and now I think would bethis really interesting topic of theopen- source wva you know WEA lettingyou customize the kubernetes and Iremember the first time we we spokeKevin you had a pretty technical issuetodiscuss can you can you tell us aboutyour experience with kind of customizingvva yeah yeah for sure for sure yeah Ihad tons of questions and and reallyprops to you guys for like two thingslike your your documentation is reallygood and then the support that you guyshaveum whenever we had a question is is isreally top notot so I'm really gratefulfor thatso uh in the end uh again uh what whatwe're trying to do with neom is aplatform where you can bring your ownVector database whenever you want to runthis pipeline right and so um for thiscustomer that we were that we were doingthis blog post for and and for some ofthe customiz session that we havewhenever a customer doesn't want tobring a vector DB that we useessentially our default one um in thisblock po we were using um in this inthis run we were using weband because it's open source we wereable to you know deploy it on on our ownkubernetes cluster and and be able to umproperly configure uh and so there's atons of things that um you can configurethat people can read on thedocumentation and whatnot but very veryvery simple at a very high levelis you have large scale data youprobably want to parize how you ingesthow you query the data into the vectordatabaseso very simple things like having amultinode cluster is super important andthat's one of the things that we want toreally do and and we really just haslike you know kubernetes confix whereyou can say these are the number ofnotes that we want this is the memorythat we want on each of these notes boomyou know you deployed it and and andeverything works out of the box uh andso at the very beginning we were havingproblems like well we have like sixnotes that we had in our kubernetescluster but uh web was only um listeningto One and so it was like as if we hadone and so oh what's the problem therewell we had an enable sharding Okay sowe need to enable the sharding so thatit goes and distributes the loads acrossthe different nodes so it was a lot oflike TR reading the dogs and talking toyou guys and understanding but uh thewhole point is if you have a multinodecluster that you can then configure withthe right resources in terms of memoryand CPU then you can effectively againparallelize how you're going to be doingall of these injections because I toldyou all of this about distributed cuesand all of this data infrastructure thatwe did and we've par ized a lot on likeM's end we have the tasks everything isretryable blah blah blah blah but ifyour ingestion is not like if your ifyour vector database here does notsupport any type of parallelization thenyou did all of this work and now it'sall coming to this bottleneck that itdoesn't work so why did you do all ofthis right so of course with we8 youknow because you can also distribute itand it's based on you know for for forfor the people that really know andunderstand and want to learn we it'sbased on like the Cassandra AR ecture soyou have all of this um fa tolerant andand replication and charting strategythat you can uh that you can distributethe the the ingestion too and lastly andand and and we'll talk more if you havemore questions but the other thing thatthat uh that was really important for usthat we Grant some quick benchmarks thatare there in the blog post if peoplewant to see is well of course we want totake advantage of W's parallelizationand at the very beginning we werenaively saying well let's just paralizeeverything right and so uh what what wedidn't understand is so we it has thissupport for every time you ingest youcan specify the number of workers thatyou want to um be using whenever youingest and so very naively we're likewell let's set that to whatever Highnumber we want so that everything isparalized and clearly this was wrong onseveral levels first you need tounderstand how many CPUs you haveavailable in your kubernetes cluster ifyou have four CPUs available and yousaid your number of workers to be 100well guess what things are going to blowup because you only have a certainnumber of threads that you can you knowingest this information and then theother important thing is let's say thatyou have I don't know 50 CPUs um butwith NE we were already doingparallelization so that differentthreads can ingest into w at the sametime so think about this if you have 50threads on our side that are allingesting into weat at the same time andeach of your weat ingestion has 50number of workers that you can paralyzethen effectively you have 250 right likelike soso or or even more like depending on the50 times even more but what I'm tryingto say here is that it's very importantto understand the parallelization thatyou have before you ingest and theparallelization that you have at youringestion we8 cluster per se so some ofthe benchmarks that we run andunderstanding this thing was well let'ssay that we have 50 uh available CPUs indevate then we have let's say um 25threads in NE working concurrentlybasically we we we we try to come upwith what what were these numbers and soif we have 25 then maybe we can set eachof these threads numbers of workers totwo two right so that if you have 25threads and each of them have two asnumber of workers in we you effectivelyhave 50 which is your maximum capacitythat wva can handle so I know I know Italked a lot about like technical Dethere but the whole point here was tounderstand what are your resourcelimitations on we8 how how how how highhave you uh how vertically have youscaled and horizontally you've scaledeach of your nodes uh into your weetcluster understanding that and thenunderstanding how much parallelizationyou're doing at the extraction andembedding and and everything thathappens before the ingestion so that youcan then make sure that you maximize thethe the resources that you're using we8but you're not you don't go over itbecause if go over it then you startgetting a lot of these batch connectionerrors and like I'm I'm I'm overwhelmedessentially the system is saying and sofinding that balance uh running theapprpriate tests and and giving thatcustomization to the user which is whatwe do in terms of how many CPUs you wantto run how many threats you want toparalyze was very key for us so I know Italked a lot about that but um it's itwas super important for us to understandthese things yeah know that was reallygreat and I just like I'm not going topretend to be an expert on this I'mtrying figur outas you know so so my question and I hopethis isn't a dumb question but if so I'mglad that there's never D question therenever Dum questions is um so I'm havinga hard time understanding theparallelization of this kind of like youknow uh process documents embed thedocuments and then and then with weatebecause my understanding would be thatthese would be like Networkrequests uh things but is it thatbecause you're managing like onekubernetes cluster that's doing bothnewe and WEA ingestion on the in thesame like infrastructure as code kind ofset up that that's why you need to bemore fine grained about this kind ofinterplay between the paralyzation of Nandweate right right right right right soand and this is what I was tellingbefore that um it's a great questionlike if you have a lot ofparallelization before and you don'thave any part like and and you just dumpeverything right into like a funnel thatthat doesn't allow any parallelizationthen you've effectively bottlenecked atat at at this stage um and and what'shappening here isthat if you have a lot of files that youwant to chunk andembed what you really want is tomaximize the resource the resourcescompute wise of how you do that becausewhat you could do is you could have onefile that you process that you chunkthat you embed and that you store intowe and you tell we hey do this as fastas possible use your maximum CPUs butthen you have 99 files that are waitingalso before you finish this thing and sowhat you want to make sure is that heymaybe you can process the files parallyou can have different tasks where eachof them are going to be chunking each ofthem and then you're going to be alsoparallelizing the embeddings becauseagain like even even if you use open AIright open AI has this uh rate limitdepending on the the the account setupthat you have but let's say it's like amillion tokens per minute or somethinglike that right and so let's say thatyou used open AI we haven't we did talka little bit about the dimensions andstuff let's say that you were using openAi and you were just processing um onefile at a time with different chunks soat any given time you're only usinglet's say 10,000 tokens it's like wellyou're not maximizing the throughputthat you could get with open AI so chunkmore things at the same time embed morethings at the same time maximize thatthroughput to get maybe the 1 milliontokens per minute for example so thatyou maximize the throughput there andthen everything can be ingesting in we8asynchronously um or or any other VectorDV but in this case was we8 andparallelize that part so it's a it'slike a multi-step parallelizationbecause you have different steps youhave different tasks you need to processyou need to chunk you need to embed thenyou need to ingest and so um yes it'sthis it's this fun and complicatedinterplay of like how do you paralyzethe things that happen before youactually ingested into the vectordatabase and how can you make make surethat whenever you ingest into into yourvector D like we8 stuff is also gettingparalyzed and maximizing the computerresources at your uh at your compute uhstore layerso yeah well that's all really amazingand I think the kind of yeah yeah likeI've heard of things like I've had someconversations with Michael going atneurom Magic where we talked about um Iwas always curious why there isn't morelike um you know blog posts about justlike here's how many embeddings we cando in like a minuteright and so I I don't know too muchabout this kind of like batching uh likeI've heard uh you know so like with uhwhen they're benchmarking modelinference they usually would like uhgroup it by sequence length so they'llhave like you know 384 sequencethroughput on deep learning modelinference and so I've heard a little bitabout that kind of stuff and the idea oforchestrating that with how you do yourchunking and then how you yeah all thatdoes sound super no it is fascinatingand look in the end is in the end is acompute optimization and computeresource usage that you want to do likeyou know when you submit a request toopen AI people might think oh it's anAPI I mean yeah it's an API but in thebehind the scenes there's some computethat is happening for for for thesequence Transformers to actually youknow to to embed this data and so youknow if you have one GPU then and wellyou can do so many texts at any giventime before the GPU starts to blow upthe same thing when you have like CPUparallelization on on on your computerthat you're running multiple tasks andmultiple things the more compute thatyou have the more polarization thatyou'll have but the more compute youhave the more costly is going to be umand so this is also like that that finebalance that we didn't talk too muchabout um uh that that we had to do someresearch in terms of well these are therate limits that open AI enforces umthis is the cost of open AI this isdimensions of an of an AI what if we usesomething like replicate and and this isalso a YC company where um you know theyhelp you with the with the hosting ofthe models um and essentially do all ofthis parallelization that I was talkingabout but from a compute and and andinference embeddings model layer so thatyou know you can submit these APIrequests to replicate but you can alsoconfigure hey these are the maximum gpusthat I want this is my cluster this isthe things that I want and so againdepending on how much you pay anddepending on on on the cluster that youhave um well you can maximize thatthroughput and and and um and get theembeddings in parallel before you go tothe the vectory but yeah this is afascinating thing Conor like it's athere's so many different layers and anddifferent compute resource optimizationsagain it's at the core of like adistributed system where you need to beable to pay attention to the differentparts that uh that you have and and howcan you maximize and and parallelizeeverything um and and and the reason youwant to maximize and parallelize stuffis well you wantto be able to run this in adecent amount of time and not just a geta pipeline that embeds gigabytes of datain two months you know like well CH nolike you want to finish this in you knowa couple hours so uh so so so that'swhat you wanted was thing uh but it'salways a trade so yeah I wonder if youknow mentioning replicate I find that sointeresting this kind of like um well II think from the perspect the firstperspective is just like you want to usethis MP net model which is like half thedimension size of open AI so it's alittle easier to use but there's alsokind of this emerging thing of maybe youum you know train your own embeddingmodel and that's like one idea but Ithink another thing that's prettyinteresting with this is like with weevaone of the biggest topics in weeva isthe development of multi-tenancy andmulti-tenancy is about your users haveusers and you have the scheduling oflike active inactive and maybe alsothere's something to like you know thisuser is particularly active they have a100 million vectors in their we8instance and so let's give them eightgpus for their embeddings make it extrafast whereas the inactive they maybe 20of them share a gpus right right rightright so yeah I think this kind ofinfrastructure is code and under youknow as you've really helped meunderstand better the these differentprocesses that could be running inparallel it's quite the system yeah yeahyeah so yeah so I think all that wasjust really fantastic and I think kindof transitioning into more open-endedhow you see the future topics youmentioned like large scale and I'mreally curious just like uh in your blogpost you mentioned putting a billionvectors into weeva how do you see thiskind of like billion scale plus Vectorsearch yeah and uh I love the questionbut I always I always am SK notskeptical but like how I respond to thisfuture looking questions because chancesare that I'm going to be wrong onwhatever I predict here uh but uh reallylike the the thing here is Conor is likewe're very early and you probably youguys probably know this better than thanthan us but like we're very early in allof this like Rag and and genapplications that are coming at at atlarge scale and at Enterprise scalethere's a lot of like Twitter chatter onlike hey I built this thing and you knowit's a couple PDFs and I use theseTechnologies and these things work andit's great but what we're really tryingto focus on is where are these billionVector opportunities like where arethese large scale opportunities andwe're fortunate enough that we'retalking and and working with a couple oflike um customers that have this sort ofscale but I want to be also honest withyou and it's like we're still yet to seelots of people that have this oh I have10 billions I need to be able to havethis uh mechanism to be able to storethem it's like I want to talk to you ifyou have that that sort of scale butit's like we we're also a very earlystage in terms of how how many playerswe're seeing that have this sort ofscale and that have the sort of uhbusiness requirements for large amountsof data and so it's not really aprediction to answer your question butI'm really hopeful um based on likehistory of how data engineering startedlike H how it has transcended in termsof like there's always going to be moredata there's always going to be moreneeds to handle all of these datainfrastructure data management so ifhistory repeats the same itself in in inthat same uh approach where ontraditional datalytics then the samething will happen here like of coursewe're just at the very uh basic layerright now where where the emergingtechnolog is coming but like can wereally be at a place where we'reposition ourselves here to help thepeople that uh that are doing the thereally large scale um requirements andand uh um you know be able to supportthem and and uh I'm actually very gladthat you know for example in web8 um uhone of the reasons we're using we8 forfor our uh for the pipeline run that wewere sharing there in the blog post isbecause of the easy to use and and andbeing able to scaleum horizontally and and handle all ofthis parallelization for such a largeamounts of data and and there's someother stuff that we didn't even mentionhere and and stuff that maybe for afuture podcast or or something that Ialso we didn't invest too much in butlike stuff like the grpc client that youfolks are working on which is greatlygoing to increase the uh speed of likehow you ingest and product quantizationand how if you have large amounts ofdata you can actually reduce the memoryfootprint that you want to have withtrade-offs of course on like the injetime and your search latency and whatnotagain everything is trade-offs but beingable to to have all of theseconfigurations for like large scale uhingestion and and and data I think it'sit's crucial and it's uh what we'rereally excited to keep working with andand finding this right set ofpeople yeah I love that answer yeah Ithink that's all perfect just like kindof the the the assumption that all thekind of database workloads are going tomove into also having this kind of likevector data Vector database native kindof thing AI native and um yeah I thinkall that just makes a ton of sense butI'm also kind of curious about like umyou know what what kind of applicationsmight be able to be built with billionscale plus Vector search that we've justnever or let's say trillion aquadrillion let see how high I cancount yeah yeah yeah yeah I would loveto know from from your point of view aswell like what what are your folksseeing because you are the store layerright so you probably have morecustomers other than than you know thanthan us that that actually did a billionscale Vector as well I don't know if youhave trillion Vector scale honestly Idon't know who has that much yet butlike that probably will come um but yeahI'm also interested in in what are thesort of things that that that you'reseeing because I think as we move awayfrom like your typical chat botexperience um into into moresophisticated uses of semantic search umwhere you know you have knowledge graphsand and and really be able to dosemantic search quickly over 100 billionvectors that you have in your store andyou get a result in subc like this isthis is the sort of thing that I thinkit's super interesting and um I don'tknow what are your thoughts in terms ofwhat are some of the applications umthat that can be powered with with thisscale but uh it's definitely excitingwhere we're going um so yeah yeah yeah Ithink to quickly I really want to go tothat knowledge graphs Topic in themetadata I think it's so related toingestion I I say just concluding mythoughts on like what are the newapplications available with quadrillionscale factor search yeah is I thinkwe're headed to like if you seen thatRick and Morty are you aware of thatshow where they have like theinterdimensional cable kind of thing andit's like you know it's like so we havethis concept in we called generativefeedback loops where it's like you putsome of your data in it and then thellms or the generative image modelsmultimo models will like do stuff withyour data like create new data out of itand I think it's very related to likeyou know we have had this researchcalled open-endedness there was you knowwork by Jeff Clon Kenneth Stanley DeepMind has an open-endedness team and sothat's where I think that quadrillionscale Vector search is headed but Iwould love to talk about knowledgegraphs because I think so so knowledgegraphs you know there's there's kind ofa couple things with this there's likethe you know thinking about graphdatabases and storing all your data aslike entity relation entity and thesetupal and fast indexing of these likeit's like a more native way to havejoins in database sense and then there'salso kind of like Knowledge Graph likeyou have this Rich metadata so you canhave like filtered Vector search so youknow if you have like I'm searching foryou know NBA players that play likeKevin Durant but but also were bornbefore 1985 and grew up in the westcoast of the United States so yeah yeahyeah yeah know that that's that'suh I don't know how to put it like wewe've gotten so many questions aroundthis learn like well I want to dosemantic search but I also have likefacts it's it's what we seeing peoplecall it I also have like facts that arenot really anything that you would wantto semantically query on but rather likedo your typical select where this equalsthis right and this is where themetadata is super important and hnswalgorithm right and and how can you putthe right metadata so that you caneffectively do the hybrid search whereand you probably know this more honestlymore in details but where you can do thepriv the hybrid search where you havethe your typical semantic search overyour text semantically data but you havethe filters that you can do based onfacts based on dates based on namesbased on places all these things thatyou want to like filter on so that youcan then have a more accurate umaccurate answer and I think um one thingthat we actually saw um uh for all thehate that L chain gets and and honestlyI think L chain is great uh there's thisthing in L chain called the self queryretriever I think it's it's what it'scalled but it makes it super easy in inin in transforming a query based on somemetadata that you want to pass and somemetadata that you have in your vectordatabase to really create a query thatdoes this hybrid search where based onyour natural language query some stuffis happening at the semantic level andsome stuff is happening at the filteratelevel and get an accurate response and Ithink that's that's huge uh I haven'tmyself played too much with that um uhto be able to talk to you for like 20minutes about it but it's something thatI think it's super important and and whywe're also so keen on whenever weextract data um at our preprocessingstuff that I was telling you making surethat a we support for metadata inputsthat people can bring or we also come upwith basic metadata extractions or smartuh chunking smart metadata extractionsnot dumb metadata extraction no but likereally um so that you can appropriatelyhave your vector information with yourembeddings with all of the appropriatemetadata that can be linked to thatVector so that you can do the the thehybrid search and whatnot so um againsuper fascinating stuff that um I don'tthink it's like a solved problem todayon like how you do this thing and that'show you see so many players doing all ofthese different uh um approaches uh andand hopefully we'll evolve as well intohow we do this better uh but yeah I Idon't know what are your thoughts onlike hi hbd search and and uh uh usagesthat you see within we8 from from thesupport channels and the customers thatyou have uh and some insights that youmight have on thistopic yeah well I I think that was greatand I I I guess I was super bullish onthis kind of extract all the metadatayou can have some llm that just looksfor anything that's symbolic and thenmaybe we use that in SQL style query butuh then I was talking to Nils ryersabout this again who I consider sort ofThe Godfather of AI search from coohhere and and he was skeptical about thisbecause it's hard to kind of unifymetadata and I kind of now I'm now I'vekind of come to understand and agree andI because to me it's like the onlyobvious kind of metadata to haveuniformly across things would be like uhtitle content source is three propertiesof a generic kind of unstructured vectorindex content being the thing that'svectorized title is like a prettyuniform thing you'll find it in any kindof PDF blog post they'll all have somekind of title and then you can maybe dolikelike in the elastic search in the bm25there was bm25f where you would keywordscore with two properties and title andthen the main content thing being themost common manifestation of that andthen Source being a common filter whereyou you know where Source equals Twitterequals arive equals GitHub for that kindof not- taking dog food so I you knowoutside of that being so I I think a lotabout this kind of what would be likethe standard schema for Vector indexesand I think that kind of like titlecontent sourcewhere content is the vectorized propertyand then maybe I think uh linking it toexternal tables that contain thesymbolic SQL style and but I love howyou brought up the self queringretriever from uh Lang chain and I thinkllama index and Lang chain they bothreally push this like um Innovation onllm query engines you know like havingthe llm do some intermediate do somestuff yep yeah and so yeah that's why Ijust think this whole like you know sortkeeping I love this this the source anddestination you you've just opened myeyes to that I'm not going to pretendlike I had a good understanding of thatbefore I spoke to you just now but likethis this like ingesting all these datasources and then having these like llmquery engines that can route queriesacross it and having this Rich structureI don't really think you know I I likethe knowledge graph idea I think it'squite cool but I don't really think thiskind of like entity relation entitynative way of thinking but you know I dokind of agree with this idea of likeextracting facts and maybe like havingsomething that's like is just a factrather than like a paragraph of aticallyWR yeah yeah yeah yeah know makes senseyeah I think you probably know this butthe space is evolving right and and itit's eving so fast uh I don't know likeevery day I go into Twitter and I see anew parad paradigm that it's like okaymaybe we should include this as part oflike how how we extract metadata youknow so yeah it's fascinating honestlybuilding in this space at this stage Ithink we're lucky um to to be able to bea part of it and as long as weconstantly iterate and we constantlyimproveand yeah things will fall through yeah Ithink my concluding thought also couldbe maybe that there's something toevolving your schema like maybe you canhave some kind of exper like we'reseeing a lot of um people are curiousabout how do we evaluate these systemsand the most common thing is the llmeval where the language model isprompted with like here's the queryhere's the search results what how howare they and and then it kind of canself-tune itself that way and maybe youcan also self-tune the the schema andthe metad it could be quite deep I thinkit's quite complicated but maybe so so Ihave kind of a concluding question Ithink a lot of our listeners will bevery interested in why combinator it'sobviously like you know the NBA ofstartups how has your experience beenwith and and I think also this yearespecially is really relevant for ourpodcast because it seems likey combinator has definitely made a bigpush on AI deep learning LM basedcompanies so I'm very curious like whatyour experience has been with it forsure for sure in terms of the AI thenumber of AI companies in in dis patch Idon't have a number um maybe there'slike public information but there were alot of AI companies majority on likeinfrastructure developer tools andwhatnot and and some of the likeconsumer side of things and andapplication Level um stuff but um it ithas been an amazing experience man it itit it has been something that I'vealways looked uh looked up for I spentfive and a half years in Microsoft andthen one year in in circle the thecreators of usdc for for those who arecrypto uh knowledgeable uh and I'vealways wanted to you know start my ownthing and and and be able to to to learnas I go and ship fast and whatnot andreally me and mounder have known eachother for like 10 years and it was myco-founder's idea on like hey let's justapply to W hater I'm like I mean sure Idon't know like isn't super hard to getin like yeah okay whatever let's justapply and the funny thing is that looklike we even applied with a differentidea right but but we show this energywe thought this we show this motivationwe show this commitment we showed thisyou know we're going to build we'regonna we're going to make it happenright and and you know from thelearnings that we had from from thatinitial idea this is how NE was uh wascreated essentially but um it's it hasbeen an amazing experience really uhthese people that really help you pushyou it's really an acceler for thatreason you're accelerating at a veryvery fast pace um for the first threemonths and then of course they tell youwhy comat doesn't stop uh here inSeptember when you end like you need tokeep getting your goals you need to keepdelivering fast you need to keepshipping keep iterating and so I thinkthe the network of of people that we metthe the advice that we received and andthe whole acceleration and and and beingpart part of this uh of of this wave ofpeople that are innovating fast it'sit's it's an amazing experience reallyan amazing amazing experience highlyrecommend for people who uh who who wantto build startups and and createsomething uh and want to meet morepeople so amazing Kevin thank you somuch for joining the weeva podcast justsuch an insightful discussion I'm soglad that our paths have crossed andI've gotten to learn these things fromyou I think you know all the you have somany great insights on this topic ofingesting data and I really love thiskeep the source and the destination insync I think it's a super unique angleas well as your perspectives on kind ofall the end to end stuff Kevin thank youso much for joining the wva podcastthank you so much Conor I appreciate ita lot man thank you awesome", "type": "Video", "name": "Kevin Cohen on Neum AI - Weaviate Podcast #70!", "path": "", "link": "https://www.youtube.com/watch?v=dWtqwt5cGjI", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}