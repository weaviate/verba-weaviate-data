{"text": "Hey everyone! Thank you so much for watching the 74th Weaviate Podcast feature Simba Khadder, the CEO and Co-Founder of ... \nhey everyone thank you so much for watching another episode of the we8 podcast I'm super excited to welcome Simba Kadar the CEO and founder of feature form feature form is a feature store and quickly before we dive into all these details of uh feature stores how they relate to Vector databases their impact on Rag and lolm applications Simba thank you so much for joining the podcast of course I'm really excited to be here awesome and also sorry I left this out of the introduction but I was super impressed by your lecture at the CMU DB seminar it's going to be linked in the description of this video just absolute Masterpiece you know master class on how these systems work and it was really really an impressive talk so uh maybe to kick things off we could talk about kind of a feature form and feature stores and how they fit in the retrieval augmented generation flow yeah um I think there's kind of a a a few parts to that like what is a feature store is kind of the the first component what is a feature is maybe even like a more um uh Lo I'll start there let me start like what is a feature and then I'll kind of build up to how I think it fits interact so um I'm going to first give the definition of a feature from like a traditional machine learning perspective um which is where we more commonly use the term feature so a lot of um in a lot of ml contexts you'll end up with this raw data set let's say you're Spotify and you have a set of listens you have like users and what they listen to you want to feed that into some sort of model let's say a model that makes recommendation now most models you can't really just give it raw data you can just be like here's my whole data set like now like learn you kind of need to take that take your domain knowledge apply it to essentially pull out signal from the data and that idea of like creating signals and and and and pulling signals out of the data um You can call a signal a feature and you can call that the kind of pulling of signals feature engineering so features are signal from your data in the Spotify example that might be things like a user's favorite song their favorite genre in the last 30 days Etc it could be something as simple as age or if you're a bank maybe it's something like how much money do they have in their bank account so those are features um in the rag workflow when I think of rag I think of uh it as a way you have a context window it's a finite size and the goal of rag is to optimize the amount of information in um that window so that the model can make the best prediction possible now when we think about that when we think of rag we kind of go of this really naive first implementation which works pretty well which is just take the question go to a vector DB get relevant chunks of documents based on that question it works pretty well that's why it's kind of was the first thing and really the first thing people learn but as we get get more sophisticated with rag we start to realize that hey like there's more than just relevant content even like what does relevant content mean how do we chunk things like are there different types of pools of content different types of searches you want to do or even just like hey I'm a bank and someone uh has a question of how should they split their Investments between stocks and bonds you probably want to know their age you probably want to know how much money they make you might want to know what they spend their money on their risk profile where they live like there's all these things that you'd want to know about them and those things are really just features and they often times don't even live in the vector DB because otherwise you'd have to kind of take your balance data for your bank and copy that into a vector DB which probably not optimal um so yeah my sense is that one that rag is going to become much more advanced rag is going to move away from just it's a vector DB to more how can we get the most relevant information that feed into the prompt and uh those things are features in my opinion and even the things you get from Vector DB in my opinion is a feature and so feature stores just a way to define these things to manage them and to um deploy them um they're not necessarily the the infrastructure but more the the framework to do this amazing yeah I love that perspective on um kind of more personalized Rag and kind of thinking of like the recommendation system problem is how you Source the context and and yeah the so this feature store thing I think there's just so much to it I'm so excited to kind of dive into these things can we maybe start off um something that I think just maybe uh introducing you to our listeners a little more is uh in in your talk you mentioned uh you built a recommendation system that serves a million monthly active users and I mean listening to you talk to the CMU series you are clearly super knowledgeable about all the things like you'll say this is a physical feature store and then you go like uber has this Airbnb has this like you clearly have so much experience with these kind of things can you tell me about this kind of career path of building recommendation systems yeah um so uh my career started um I I'll actually take I mean most of where I learned when I was back in school I spent most of my time really obsessed with distributed systems and that's where a lot of my focus was the reason why I was so obsessed with distributed systems was because there is never a right answer everything is kind of cve there's always a failure case there always heuristics and I found the it's almost like the organic like organic in the sense of like it's much more like building a biological thing than building like a highly engineered thing because it just has to be way more resistant to failure uh I just found it fascinating I just really found the the space of hey like anything can happen and you need to be able to handle all these cases it's impossible there's no like this is the right answer for this problem um so later it's funny that same kind of enjoyment of the gray area is what got me really into machine learning in very specifically recommender systems and with recommender systems in particular what I I just really um found interesting about them is there's no such thing as a perfect recommendation because if you're doing computer vision I always found computer VIs to be boring personally um because there was kind of it was very like obvious it's like there's a right answer or wrong answer it's really hard like very very hard technical space but I found that like just this concept of it's either right or wrong it to not be as interesting to me I love the more recommendation style where it's like hey there's this idea of serendipity which is the idea of like sending something to someone that they don't expect that they love and that isn't random but almost as random but you can't measure that there's no way to measure Serendipity there's no way to measure kind of the neurons firing in your brain when you see that thing or listen to that song or whatever um so that's why I got to recommender systems I actually ended up starting a company I was at Google for a little bit I was actually a software engineer very for not very long and I left um to start uh my first company mostly because when I was at Google I felt like I was kind of getting cut into like a maybe like a a cog like the right size piece into this need your machine and I just had all this all these ideas and thoughts and just things I want to explore and um you know even like I was on Google cloud and I had these ideas of hey I wonder what would happen if Google Cloud did this and had product ideas and they're probably bad ideas I was like 20 but I still was really driven to like learn why I just couldn't handle that that's not my job I just have to like not think about these things and just focus on my little sliver in the world so I left to start my first company um Triton Triton we built up um like you said we built we were doing personalization predicting subscriptions predicting churn for a 100 million users and most of our customers are media companies um like really large news companies uh mix of different thing anyone doing PDC subscription really um and when I was there um we were a pretty small team we're bootstrapped we were startup um we did pretty well um I think we we we served again 100 million Maus no no joke um and we built all the stuff in house that nowadays we call amops what's really funny is one thing that one piece of infrastructure I've probably built four times in my career at this point is a vector DB now the vector DBS that we built were much less sophisticated it was more like hey we have anoi or some other at the time it was mostly anoi noway I guess like hsw um and we essentially that's great that's like an inmemory index but that's like building it's like oh I don't need postest I have a b b tree implementation it's like you're missing a lot to make that actual database we'd kind of hack around it so I built all kinds of stuff one thing in particular I built um around our platform that was just so pivotal and so impactful for our team was we called our data platform but now days we'd call it the feature store and over time um I decided to open source that and uh eventually um uh in a much longer story of how I did it I ended up um just starting a whole company around it um and that was kind of the creation of future form which now would have been January 2020 so like uh about three years ago almost wow yeah so that kind of like ml Ops that like end to end I think um that when I was learning about feature form that it it's such a great example of the mlop stack from the training I even love how you have different features for training and inference you have this unique like batch versus streaming feature problem as well as the uh continuous model deployment I love how you tell anecdotes like with Twitter and how they have like these separate data teams and they're like hey could we have our feature we want these 15 features and they say well all right give us two months and then you so I think just this whole end to end there's so much like system orchestration going on here uh do you mind maybe like stepping into kind of like uh I hope it's not like too much of a tour but kind of like the end to end of the feature life cycle I want to call it and how kind of these yeah um I would love to um so let's uh so we've we've kind of defined a feature so it's like this a signal now in practice um like when you first learned machine learning or data science you're given this beautiful perfect CSV and you're told these are the five feature columns and this is the label that's it like that's all and now you just build a model around it and you don't really touch the data now that never happens um in reality the data is uh really just it's not it's just not clean it's it's kind of all over it doesn't have it's your job to take that and and uh fix it up clean it up pull out the relevant attributes pull out their all the aspects um but even coming up with ideas of like what like if if again like the Spotify example you're Spotify you're given you know users listen to this song you might come up with all kinds of ideas of like well I wonder what the beats per minute of a song is or I wonder like when it came out or I wonder what album it's on or I wonder you know there's all kinds of things you can come up with and think about um that you can use as features and a lot of that first step of the feature life cycle is typically in that data scientist notebook they're often times using something like pandas maybe with a sample of data and they're just analyzing the data coming up with ideas playing around with it trying to figure out if they can actually kind of siphon this thing out of the data that they're trying to uh uh kind of purify out of it and that's the first step that's I'll call it the experimentation stage there's a lot of problems there um one is um maybe not a problem but to something that's inherent to it is unlike software engineering where if I tell you to build a vector database like it'll take a long time but you typically will have some sense of where to go you kind of know what your end State sort of kind of looks like data science there it's not really true it's it's true like the the software engineering versus data science the engineering versus science is very very true in the workflow data science is way more um like you try things you throw things away it's just way more experimental it's way more lightweight but then you need to get these things in production and production is not lightweight and production needs to be very um I mean production ready and so when we have all these notebooks lying around they're Untitled we copy and paste between them we like send csps over slack between teams it's very common a lots of ml teams where they'll actually write their model in a notebook and then give it to a whole another team which has to write the whole thing to productionize it that's kind of what happens today and so um that's the kind of deployment piece um so there's two sets of there's a variety sets of problems one is like um how how do I deploy this thing it's really hard today how do I experiment how do I keep track of things in organized there's no workflow it's it's just missing um collaboration is is a lot of sending things over slack um monitoring like once a day is in production that's not the feature is is alive to an extense right it changes it moves things happen like you need to be aware of that um and then finally uh governance and access control um having that fit into a way that isn't just like really painful in the ml life cycle is really important and so you have all these things that you're trying to accomplish but as a data scientist you have you work at like I have tables and I have like spark submits and I have Untitled notebooks what's missing what I wish existed when I was with this was something I I would literally said like I wish I had terraform for features and then I you know being as creative as I am I said I'll build feature form and uh that's kind of the Genesis that's that's the life cycle and that's the problem that we aim to solve is to build almost like a framework above all of this stuff to make it easy to Define things to manage or toine features manage features and deploy features amazing so if we could stay kind of in this like end to endend view before maybe zooming in on feature engineering and zooming in on the inference cache and details like this I want to uh so another thing that really stands out to me is um you opened your talk with a slide on saying CFA Snowflake and S3 like how you kind of keep these sources in sync and that's that's kind of the opportunity I see between weate and feature form or you know Vector databases and feature stor there like um this kind of like personalization thing very heavily where you might search with like a vector that represents Connor and then you get some features from feature form and use that to rerank the search results so I think that kind of I'd love to understand how um you're because it seems like you blend just so many Technologies together as you mention like the inference cach you have like redis in there so there's like all this kind of orchestration of Data Systems happening yeah um and we actually support we8 so I have some examples of we we've actually done some uh uh things I can maybe send you and you can link in in the bottom um someone wants to get a a Hands-On view of that um so when you um build these features the features are kind of made up of uh there's a few abstractions there's the the data sets there's the Transformations which essentially create new data sets a set of columns make up a feature another set of columns make up a label and you might have many features in the data set you might have many different data sets that make up a set of features and then a train set which is what you train on is um a set of features mixed with a label um so you need those things in different places like features that get served in production need to be in like a low latency cache like reddis whereas when you're training you actually don't want to be serving training features out red as it's it's not the right tool you kind of can pre-process to make a file um like a training file and then you train off of that um you want to make sure those things didn't sync uh so how I see um the the kind of feature store in Vector DB it depends on how you define a feature s there are many definitions um there are three um that I I've I've talked through and I've written about um rather than getting into the differences of all three I'll just talk about what we do and how we think about it which we think of a feature store as an orchestration problem so kind of a metadata store and an orchestrator and and rag we8 case um the the open question is how do I so I'm building embeddings and the embeddings live in we8 and what we8 does super well is allow you to look up those do nearest neighbor look up on those Abed amongst other things but let's just keep it simple um how do I get things into we8 like how do I take I need to take the Tex take the content take whatever your user Behavior have user embedding um I need toe preprocess it sometimes I'll have to take features out of it um and then I need to embed it and the way I embed it the way I chunk it all those are variables right and I need to I might try different things with different results so I might end up with many different versions of my embedding table with different chunking different things where's that live where's that run how is that whole kind of workflow managed that's kind of what's been missing and we just see a lot of glue people are like oh we like you know hack together airflow or we have this like you know copy snowflake table to we8 do shell like we just have really ad hoc things and that's fine when maybe you're just trying one thing out but when you start to get into iteration mode especially as evaluation is becoming more and more critical in um the LM workflows um when you're evaluating you want to be able to be like cool that did act good now I need to see if I can make it better so where you're going to do well maybe you'll try different eding try different you'll try different documents maybe remove documents there's a million things you can try um but how do you keep track of all those of all those data pipelines of all those experiments how can you be sure that when you're doing all that stuff and you productionize it but you can be really sure it will continue to get updated as new documents come in there's all what's missing is this this orchestrator where we is like a PO piece of infrastructure R is for like key value search is a core piece of infrastructure spark is a core piece of infrastructure kfka is a core piece of infrastructure but it's almost like if I gave you a Lamborghini and and in Parts like here's the engine you know here's a wheel here's a steering wheel's a brake pedal um here's a uh you know like transmission here's whatever uh radiator um you're not going to build a Lamborghini if I just give you those things especially if I give you some what you're going to have is essentially some duct tape to like try to fit them together what's missing is kind of this like chassis like this thing this motherboard that you plug all this stuff into that provides uh kind of understands what the point of all these things are like oh I understand what Vector DB where it lives and what it's going to get used for understand what spark is and where it lives where it's going to get used for so we're a framework above all that to make it all work together coherently but we call ourselves a virtual feature store because we don't um own we don't actually store it data we don't transform the data like an r in Fr like we're not the the CPU we're not the ram um we're just a motherboard we just make these things work together correctly and allow and allow you to not have to worry about all the details of how do I copy things over where do I run things how do I set up schedules how do I set up triggers jobs Etc versioning management sharing collaboration there's all these pieces that come into play yeah I I told you quickly going in that I was expecting to get schooled and talking to you and I because I I hadn't considered I you know I when I was learning looking about featur store I I was definitely biased in this kind of um thinking about like tabular features and tabular machine learning and it sounds to me like you've also started thinking about kind of embedding optimization and as an aside I'll come back to the embedding thing I when I was learning about the I'm thinking very heavily about like large language model continuous training and I think you also have kind and and as I was learning about feature form I was like they've already kind of solved all these problems and but with so with the embedding optimization um I think that so so so with the features like kind of coming back to to the thing where it's like you know features are signals that help you produce the prediction and so so with the embeddings I I think you know most of us are just thinking of you know you kind of blindly chunk your text and just give the text to the machine learning model are you thinking more about signals that can go into producing embeddings um yes um definitely I mean our goal isn't necessarily to be the optimizer our goal is more to give you a framework to do that yourself like we're not going to decide oh you should chunk this way or you should whatever but just giving you a much easier way to like keep track of all those things you're doing making sure they run Etc so you might have a transformation in feature form which is chunk and you might have different versions of the chunk thing and you might try different ones of them for different applications that's very easy to do you just choose the name the name of the one you want to try out um so that's where it kind of comes into play there um I think of a feature as an input to a model um so prompt is a feature an embedding is a feature you know analytical like like your age is a feature um so from that perspective it kind of fit in actually it's funny building our Vector DB and llm support took one engineer one week because everything else was the same um so it was kind of insane how like it was like 95 Cent prom Spas is the same the only difference is driver been doing key value lookups we're doing nearest neighbor lookups and we have this new thing which is an embedding but in practice an embedding is just a vector of floats the other thing about embeddings is that um they've been around for a lot longer than llms have been around um I remember the early days where we had you know I the first time I used an embedding well I we used um I use like word Toc but maybe the first time I used a Transformer was like Elmo so I remember like when Elmo was like the really cool hot thing um kind of was like maybe the one of the more pivotal U models that came out um so embeddings and recommender system we used to actually feed them into I mean I do for a lot of things we would feed embeddings as features like in in a model in a de learning mall I would actually one of my inputs would be an embedding without any nearest neighborhood so that was pretty common like if I have a user and I want to make a recommendation about for the user it's really I will pull out a ton of like analytical features about the user but I might also generate a user embedding and that user embedding is going to be relatively predictive on you if I if I build it right so um yeah I I these things are much more tied together than I think people give credit for and the other thing that I believe to be true is that um I believe that in so I'm going to before I tell you what I think I'm going to explain why um because otherwise people are going to f crazy um so when deep learning became a thing everyone was like yeah I mean if you're learning anything but deep learning like it's a waste like feature engineering is dead now like everything's done like all we need is deep learning and that was a promise right and it didn't really happen um like after years after like deep learning became a big thing the most popular mom production was something like XG boost to a random Forest today I would bet you at the by far the most uh productionize model that exists is some form of XG boost um and I think that's going to be true in five years not to say that llms aren't going to like there's a certain set of things like like the same way that Bert kind of killed a lot of not killed but like let's say really made certain branches of NLP um kind of not as popular because they just don't work as well as B bir just works magically out of the box same with with LMS like there's a lot of places where we would historically build our own models where LMS just to do so much better for essentially free that doesn't really make sense to to not use an llm um there are certain sets of things that are brand new like co-pilots right like that was never possible we never had malls that were good enough but there are so many things where we will just like fraud detection right like fraud detection is not going to be something that we do anlm it just doesn't make sense the economics don't make sense the the just nothing about it really flows it's not the right tool for the job um and so I believe that all these things deep traditional deep learning models um uh LMS and just call them traditional models we all live side by side and what's really important what the core is these are more or less to like expressions of the data it's like taking your data and making it do um valuable things to the user and choosing the right tool for the job is kind of what the goal is but the features actually apply across all of them because in the end these are just signals about your data yeah so I I think there's so much to that the the end and I I guess um yeah so I've been really interested in kind of evaluation and kind of what led me to Andy Pavo and the whole rabbit hole is research is that self-driving database concept and I and so it's kind of like that um you know transforming your features and not like I think kind of the way that I had understood this before talking to you is I I thought of kind of the ingestion layer to Vector databases as like that's all it does and then the vector database and then evaluation products like arise AI or maybe ragus or llama index has products like this I thought these is like totally separate kind of things but you know if you couple that how you import an ingest and embed and chunk and you know with how you evaluate and then you connect the whole cycle and to iterate on it I can see you know obviously see a system like that I think it's you have to do it I think that's like because like I mean it's funny because it's just like traditional machine learning like with machine learning like your variables are your features you have some set of hyperparameters essentially the model um and I used to break the amlos platform into four parts there's the data piece which I just put broadly is a lot of parts of data but let's just say everything data that's where feature stores live um there's training which is usually where hyper pram their tuning experiment tracking come in there's deployment um which is just literally okay now I've trained this small how do I deploy it thereus things like Seldon there's you know that sort of problem space and then finally there's monitoring which is now that it's deployed I need to make sure it behaves correctly all these things have to be tied together because if monitoring says that the model is misbehaving the next question becomes why and uh often times it's related to the data um and so then like understanding what data went in understanding has the data drifted how has it drifted um understanding all that is is is pivotal as well as and then then you have to train it again right so then like having that training be part of it um keeping track of the different experiments you've done having some sort of training engine um so anyway I think these things fit together really cleanly like that is an mlops platform or an ml platform and I think that in LMS it's the same thing weirdly weirdly the same thing um because now um the data part is the same except now we use Vector DBS we use rag we use chunking it's like the the operations we do are different but it is just other variables like what what's the inputs is or the the features essentially what's the prompt um but the prompt itself is typically like a chassis where you feed in features and the features can be a near neighbor lookup it can be like I said like an age it can be a lot of different things there's the model training which nowadays is fine-tuning because you're not going to train LM from scratch deployment which is a much harder problem for llms than it was for Boost um because it's just a much much bigger model um and then finally the evaluation monitoring and I actually think that this is in my opinion the funniest part about about Rag and LM applications I think the most critical pieces are the data and the evaluation but the data piece like you can only get so far until you have evaluation you need both of them because once you have evaluation you need the ability to go back and fix things but the ability to go back and fix things means that you need to have some sort of like almost like a it's like if you don't have a stopwatch and you're trying to like oh I'm trying to make this thing go faster it's like really I mean what are you going to do right it's like use your best judgment like oh it looked fast you know um and so you need some form of valuation I think it's an open question I think we can learn a lot from how recommender systems do evaluation but I don't think anyone's really figured out evaluation and I think that's why in my opinion there's no such thing in the market today um that is an actual Enterprise grade LM application because um no one's really figured out evaluation so everything's more or less a prototype yeah I guess for me a couple things in evaluation that are exciting is you is again using the language model to like generate synthetic queries as well as to do that kind of s like llm evaluation of a question answer pair and you kind of prompt it like you know how is this answer and I feel like those two things kind of uh close the loop a little bit with now you have questions now you have some kind of evaluation but yeah I love this kind of entanglement of um data and and evaluation I I guess like kind of one other thing with the feature engineering that I'm really curious about is so I love learning about this kind of idea of like um my last 30 Spotify songs are this and you kind so you're kind of like so with rag it's like we're trying to give the LM like the perfect data but and but feature engineering is like you're trying to like find these signals and the right way to describe them like maybe they tell another story I remember working on a kagle competition that was like uh you're predicting how many yards they're going to run in handoff and so you're looking at like how heavy is the linebacker just like what whatever random thing about the play that you can use to describe the things so maybe if you could talk about like feature engineering and then like how you kind of transform your data to come up with some like new view of it maybe it's kind of I don't know if it's quite the same way of thinking about like CDC and materialization and that way of like ETL getting a new view of your data kind of but that same kind of idea of like I find I multiply the uh last three handoff plays by is this running is this a defensive player in the game some kind of combination like that um I think uh the example you're giving is such a great way to highlight feature engineering so let's say uh I pick a random play in the middle of a football game and I'm trying to predict um The Yards we're going to get like that's all I give you that's the question think of all the things you would you would in your brain process right oh who's on the field who's injured how have we been doing does this look like they're set up for a pass play have they been doing tricky things does this feel like a time they would try to do something big or tricky like you would take all of this information in right these are all features and the process of engineering them is more or less just like a pie spark function or something like it's not um there's not really much magic Fair the things you mentioned around CDC materializations these are are more the hard realities of actually productionizing this like once I come up with like I have a transformation I can write based on this to get all the features I need well then it's like great but now I need to like serve that to my M on production that's going to make a prediction on TV which means that I need to have that thing be constantly updated I need to have low latency for serving at the same time I need to be able to build training sets so I can actually train them like there's all these let's call them hard realities of like the what actually needs to exist but as a a data scientist what we're trying to do is just allow you to only think about what is the feature what's the logic you write that and then we'll handle the rest we'll make sure it's run we'll make sure it is where it needs to be like we'll do everything else for you you just give us the logic you want to exist um yeah and so one more um so so I I want to just kind of touch on this question really quick before coming back to like materialization and these kind of like database real system kind of questions is um so so I'm curious like so I think with language Model A lot of a lot of us think like um there's no more great no more training is needed right and so it's kind of like instead of um uh you know getting these views of the features for the sake of more training it's just like for the sake of inference kind of so and I think you've already been looking at that problem with training and separate training and inference features I hope I'm not I'm trying to like keep the separation of the topic flow with the with the streaming but like you're already already kind of thinking about that difference between training features and inference features and do large language models change that um a little bit yeah for sure because unlike a traditional model um you can just use an llm without fine-tuning it a lot people do I think what's going to happen is in practice is that people will find tune and use rag like you'll use both the other thing that happens is it's not training but think of it as like um back testing so now you're using the training set not to um necessarily uh train the model but you could use it to fine tune at the very least you can use it to back test if you're changing your let's say you're changing your prompt in some way you're like hey actually I'm adding a few more things I'm changing my chunking you know think of all the million things you might do to change your prompt and rag but now I want to see how it's going to do now you could just put in production hope for the best right but that's like again nowadays that's literally what people do but this is why I think that there's no such thing as like an an Enterprise grade system yet because in Enterprise if we had a new model um with no evaluation metric we would never just be like all right well let's just uh see if people like it um we would back test it and we would uh Canary deploy it and we there's a lot of things we would do so anyway back testing is one piece and actually think that if you're doing back testing you actually end up in this place where the data you use to back test ends up looking really similar to data you might use to F tune especially if you go through the back testing you find places where it misbehaves you fix where it misbehaves and then you use that to even further fine tun the model so um I think the workflow is a little bit different for sure the difference like training is is a different type of thing um but I I don't it's definitely still relevant it just looks different but inference is is much more the interesting thing is in traditional ml most of the focus is on the the training and in llms today most of the focus is on the inference but I think that's actually just by nature of the fact that the mall just works it's kind of pre-trained and no one's because you're not doing evaluation you're kind of just like spot testing it you know just like ask it six questions have your buddy use it do they like it okay cool I put in prod um so uh yeah that's my sense I I do think it's still relevant but it's relevant in a very different way and I think the evaluation needs to be fixed I actually don't just a point you made of like hey like we've are now using llms to like evaluate our evaluation um it's so funny I get it like LMS are so powerful but I just the extreme level of when you have a hammer everything looks like a nail is so funny because let's like really think about that for second we're using an LM to evaluate that might work pretty well by default again LM just work pretty well by default zero shot LM unbelievable how good they are but if I'm evaluating like even just a simple question of should I use llama 2 or gpt3 to evaluate my model um should I like like how how can you Char the value in ml the valuation metric you know the loss function is a function you know you can look at it you can understand it um you can kind of do that LM but you end up in this whole other set where now you might have how do you train your evaluation llm and make sure that how do you evaluate the evaluation um becomes the next question at least when it's a function there it's a little bit easier to wrap your head around what it's doing right um and there is still a little bit of like call it um emergent characteristics but when the actual function is also a model like when your loss function is another model I think what people are really doing is they're just kind of like Russian dolling the problem they're kind of like well put LM and another llm and another llm right because then what I'll do is I'll use a a llm to evaluate my evaluation model to see if it's evaluating correctly and then it's like you can keep doing that all day long but you're still not going to get to the end of the problem you're just hiding it yeah but I guess like um the cool thing about it is that then you don't need to collect as much uh like data like feedback data for the result and I think like my experience with we8 is we see a lot of you know especially once chat gbt happen and then there's just like so many people who are just kind of um software Engineers you like you know building some new thing like you know chat with all of why combinators lectures and there you but these kind of things and and so that's why I think that kind of uh self-evaluation it's it avoids having to have like I definitely you know these you know massive recommendation systems they have data and they proper ways of evaluating it but yeah so yeah that thing so so so kind of pivoting topic is a little a little bit I wanted to learn a little more about uh streaming features I first became aware of this with again like the um tabular data used in uh symbolic rer rankers like XG boost and how you would um you know like if I need Simba's last 10 uh like the last 10 items you viewed on Amazon and now I'm going to use that to bias your next search that I need that you need to have this streaming system and particularly if we could dive into details about like the log and kind of like like I'm really not an expert on this so I'm expecting to learn like everyone else but I'm very curious like what the difference with this is with this kind of logging compared to maybe like the LSM style logs that are used in databases and sort of how the problem differs yeah um great question so R calling in streaming I'm just going to call it time series um and I'll get into where stream I mean streaming is often used as time series but it's not required a lot of people have this problem even if they do batch because you might have a feature which you reprocess every hour so it's in batch but you still have the same problem space where you still need to like so the thing of Time series right let's say um I have a um feature which is um your favorite song genre in the last week and I'm using that as one of the features to make a prediction on what I should recommend to you next let's say this week it's you know typically almost every week it's Red Hot Chili Peppers or something but for some there was some strange week in your life where your favorite genre was Kitty Perry and I need to know what that feature was not now I need to know what the feature was at the point in time of my training point so the way I would train would be like hey okay I know that Connor listened to um this Katy Perry song um and March of you know whatever and I have this set of features to that my model takes as input so the way training would work is I would generate those features as they would have been exactly as they would have been on that day and then I would use that to to train I would pretty much have it predict and then I'd compare and see if it guessed right and if it was wrong I would then back prop and uh uh optimize the weights so that's one of the core problems which is I need to be able to generate point in time correct training data there's another similar problem which is let's say I want to build a feature and that feature is your same thing let's say your favorite drawing in the last week and I build that feature now I need to wait seven days until I have the first value of the feature right unless I can go back in time and use historical data so like introducing a new way to predict that you didn't have in your historical data but now don't you have all these other problems with just like you know XG boost is like a structured model and you all you also have the model problem of adding a new feature to to the model kind of um so in the end right you're not feeding these models of stream right in the end I'm feeding the model last you know favorite song genre in last week that from its perspective is just like some value right deci I didn't that it doesn't matter um but there some value so it doesn't actually know that it's a stream most streaming problem I actually had to talk to someone about this where they're like hey what time series models should I use for XYZ and I'm actually in my experience I found of a lot of Time series problem like for example if you are um making a model which predicts how much uh a store is going to sell in a quarter how much revenue can make in a quarter I would actually treat that quarter as a single data point so how much you know I would take all these features about that what season is it what's you know all these things I would include features from last quarter so they're still like timed features but I would include them and I would kind of flatten them into one row and then when I feed it in from the M's perspective this is a whole different row but when I train it would actually learn the time series difference because the time series the quarter and the season with all be features um so uh yeah I guess my sense of it is more like it's the time series problem is tough because of backfill and because of point in time correctness and the ability to if I give you a transformation to run that not just to get what the value is now but to be able to run that and get the value at any point in time so I can train my model is a hard problem it's not an impossible problem but if you're a data scientist like the amount of complexity and just like Nuance to get that right is just not only is it like really hard even if you could do it it's like why are you doing it you know um it's like much easier if just you give me the logic and feature form goes and figures it out and so the way we do figure like how do we do it which is kind of the log part um is that the a log so the way to do it is two steps it kind of let's call let's just make it two steps Step One is bat process all the historical data step two is start stream processing from where we left off with the batch data but you want to be able to use the same function for both of those but you know like batch functions typically look very different from streaming functions but the interesting thing is is that pretty much every streaming function can be used as a batch function so so we have you write streaming function then we turn it into a batch function apply and batch then continue to update on the stream and the way we do that is we create what looks like a log so every time event comes in it's not just and cough covers a retention period it's typically I think seven days by default Kinesis on ads is a single day by default so it takes an event it stores it for up to the retention period and it goes away but we don't want that because we need the ability to like go back in time and process those features so what we do is every time an event comes in we actually append it to a log which is really just like it's like a stack like an append only thing you just keep putting them on top and uh the magic ve is that log is simultane if you it's almost like that picture of the elephant with the Blind Men where like all of them say it's something different if you just look at what's in the log at a point in time you just freeze time and you look at the log it looks like badge because it's just this set of rows to process if you only look at the top of it it looks like a stream because there's this new events coming in so the log has the the the nice characteristic that if you can make something work on a log you can kind of handle both batch and streaming and unify them which is just one of the many problems that exist of ml which is unrelated to ml right like you're a data scientist like you are learning about things that have nothing to do with um uh you know um uh the Nuance of streaming and logs and data structures like you're much more about statistics and pulling out signalizing having domain knowledge so anyway that's that's that's where all those things come into play uh so it has this understanding so like if you have um uh so so uh I have a stream of everyday sales at a store and like you know I want to make the row for eight days ago and then when I'm when I'm doing that query eight days ago if I want to have like the average uh number of uh sales I made in the last 15 days or so I can kind of construct that in the batch sense because I have like this buffer of data that I can use to construct those kind of like aggregation statistics that I then put into one tabular row to make the prediction of you know am I going to make this how many sales am I going to make today and then streaming features um okay so and then I see how you have this point in time where like I need to be able I the data structures need to let me do the average from here back but I think I'm still missing the streaming features and and um like what makes it different a little bit yeah I mean it's not really streaming features as much as it's time series features but let me um maybe give an uh let me use that example of the store so I have a store and every day at the end of the day the store streams an event that event is it's total revenue of the day I have a feature where I want to take that revenue and I want to let's say normalize because I have stores everywhere I want to normalize that Revenue to be in USD or I want to let's say adjust for inflation or one of a million like let's say adjust for inflation um so I create my inflation adjust function I want to train a model on it so I'm going to now have that model predict um something like oh like what happened on you know seven or a year ago that event is no longer in Kafka if it was originally in Kafka Kafka or whatever Kinesis whatever use for streaming does not retain events definitely but I need to back test my or back fill my training set so I can train my model so that means that every time an event comes in the cough cut I simultaneously have to put it into um S3 and that's what allows me to create my training set but there's another point to this which is when I put that M production I want to make sure that I have the most current value of the feature the freshest value the only way to get the freshest value of feature means I have to be processing events as they come into the stream now what most companies do is they build two data pipelines they build a batch pipeline for training and a production pipeline for serving an inference pipeline but now you're maintaining and managing two pipelines and there's many times where they go out a sync or you change one and like a data science wants to change something but getting a production takes forever resolve issues of maintaining two different completely separate pipelines and completely separate sometimes written in different language like one in SQL one in schola that's pretty common um feature form allows you to write at once and we will both s handle the bat backfill processing as well as maintaining the up-to-date value as things get streamed in yeah thank you so much for that and I think that's so exciting for this kind of recommendation thing especially and you so I've already learned so much from this podcast but I I have to ask you this one more question because I think it's so relevant for with we8 we are into this thing called Product quantization Where we're compressing vectors and if there was like a distribution shift we would maybe want to refit the uh the centroids to compress them and I know you have some thoughts on like data drift and monitoring and I so I I would really love to get your perspective because I think we could learn a lot from that as well yeah I mean the idea of data drift um is uh it's funny I'm literally like I was just teach someone else about this and in front of me on my whiteboard there's like a picture of describing data drift but think of it like this let's say I have a feature which is age okay and let's say that distribution of users their age on my training set it's like a normal distribution the average age is like 40 for some reason my app goes Super viral with Gen Z now all my features that I'm serving in production the average age is at 29 now the problem I have is that my model was trained on a specific distribution and learned a specific distribution but my model in production is seeing a different distribution that might be harmless maybe the mall is like good enough and generic enough and generalized enough that it it can deal with that but a lot of times it's not even just having a warning sign of like hey what we're seeing in production pretty much just these ideas that features change another example is a when Co hit imagine being Amazon's recommender system it's like cool now the most spot item is toilet paper you know like for some reason like yesterday it was a I don't know like a Kindle and today it's toilet paper so you can imagine that that might like the model might not generalize well to that and it might just start completely misbehaving so the idea of data drift is more making sure that what you your original state of things like this would be common for like streaming um different embeddings I imagine not fully uh caught up on what you guys are working on there but let's say you're Streaming new items to embed it might turn out that you end up creating a lot of items that are in a specific set of of the embedding space and that changes the whole distribution the embedding space which in practice might cause like different things to happen um so just it doesn't it's not a bad thing it might require you to retrain it might require you to do a lot of different things but just seeing it before it becomes a problem um is a huge value problem um that that we aim to do yeah I think 100% like this is a really important thing especially because um in our case we're learning the distribution to compress it so it's like if it changes now the compression is awful and um so yeah and the if the age shifts and now you have this new distribution of features to retrain a model yeah this whole distribution shift is so fast it's always been one of these problems with machine learning and yeah amazing Simba thank you so much for joining the podcast I think maybe if I could ask you one kind of anchoring question is like what's kind of On Your Horizon that excites you the most um I think what excites me the most um there's a few things uh there's a few things I think maybe most relevant to maybe listeners here um is uh seeing rag get out of what I would consider to be a pretty simplistic naive approach of just like blindly chunk and embed your documents doing near a to look up throw into a The Prompt eyeball it does it look okay that's it to like really getting much more sophisticated of how we do retrieval and using things not just Vector DBS are going to remain a huge core piece of this but really using like all the different data you have in different databases using that making all come together and powering LMS um and I'm really excited for that I'm seeing it I'm seeing the way people talk about retrieval change day today and I'm seeing people start talking about and and and thinking about much more complicated uh ways to do retrieval which in theory should work better so I'm really excited for rag to explode um and become way less of this like follow this template like just chunk and then embed and then nearest neighbor lookup to hey this is what you're aiming to do you're trying to optimize for information here's how to to to do it and so anyway that's the thing I think I'm most excited about is like rag becoming Advanced shag um and people understanding that and using that and that that's what I think is going to make llms get to the next level and I mean copilot like GitHub copilot already does this and I'm just really excited for like everyone to start to get to that level of sophistication yeah well I yeah I've been really impressed with this just kind of like SQL router that takes a question is like vector database query or SQL query and I think you're you know really seeing into the future of how much more you could do with that and yeah this whole podcast has been really eye opening for me I was expecting to learn a lot about like the kind of XG boost for symbolic reranking as well all the system design stuff going in but now my I'm thinking about this whole end to end and how much orchestration can happen with the mlops and generally how much similarity there is with the feature engineering that has been in the in place and now for rag so Sim thank you so much for joining the podcast of course it was a pleasure thanks for having me ", "type": "Video", "name": "Simba Khadder on FeatureForm - Weaviate Podcast #74!", "path": "", "link": "https://www.youtube.com/watch?v=fNL_fodHMXE", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}