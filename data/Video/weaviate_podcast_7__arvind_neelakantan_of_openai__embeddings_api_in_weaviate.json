{"text": "Arvind Neelakantan, Research Lead at Open AI, talks with Connor Shorten about their newly released embeddings API, his work ... \nhey everyone thank you so much for checking out the wva podcast we have a really special episode today with arvin nila kontan a research scientist at openai this is such an interesting timing with this because openai has just launched their embeddings api which has been integrated with we v8 in version 0.10 so you can access the embeddings api and plug it right into wev8's vector search database to enable all these cool applications with these deep learning powered representations of unstructured data like text and code with these with the latest models so first of all arvind thank you so much for coming on this podcast thank you thanks for having me i'm super excited to be here and i'm so excited to get into all these topics uh i think your career in deep learning science is so exciting and i love uh you know reading these publications and kind of seeing your line of thinking with these things so to kind of kick things off can you tell me about what's new with the embeddings api sure um so we've had the uh open ai api to be used for more than a year now uh we started off with uh general models for text uh moved to code um so the new uh so we now have a a new uh api endpoint uh that can convert uh text and code to to to a vector uh and uh we we have models that are fairly general purpose so you can use these vector representations for many downstream tasks uh like clustering classification text search code search and so on so i'm really curious about in your research career what drew your interest into these contrastive learning methods was it these say recent computer vision papers like simclear moco and say the effectiveness of data augmentation to form these pairings what kind of first grabbed your interest into wanting to really explore contrastive learning sure uh yeah i mean those had some impact for sure i think uh the bigger one is uh when we put out the api uh um people were started to start to use it for search um so they would use um you know uh concatenation of the query in the document get the log prop from the language model uh and this was like a very high quality relevant score that was very general purpose um but that clearly does not scale uh to to many documents um so that was like one of the big reasons why i started thinking about this problem uh and also like if you follow like if you've been following like kind of the research community work on like search um kind of like the modern neural network models uh don't really work that well out of domain for text search if you have a large label data set uh then the embedding methods work quite well um but uh there were not really good solutions that are there are general purpose and work across a broad set of benchmarks uh and like keyword search uh with bm25 was was like uh was like a really hard baseline uh to be um uh for like unsupervised search so what's the say newest thing in the text embeddings is it the contrastive learning loss function compared to maybe in the past where you try to say label uh i know in natural language inference they try to label them as entailment or contradiction and then core question pairs you had similarity labeling so is that contrastive loss function is that the big thing that's powering the new advances yeah there are a few things um i think like one thing uh had to start with is uh like within like text embeddings um there was like two uh research topics one for learning embeddings for classification tasks and sentence similarity tasks and then there was a separate research topic around learning embeddings for such and uh it it felt like a little bit like not the most optimal route uh given that you know both are uh going after the goal of getting high quality text embeddings so so we kind of like wanted to take the shot of like can we get like an unsupervised model to produce embeddings that works well across us so yeah i would say like the the reason uh why our models i think are working well is um you know mix of like large-scale unsupervised training uh initializing initializing with good generative models um and like uh and then also uh contrastive loss with uh you know sufficiently large batch size yeah those three i think things put together are are giving us pretty good uh embeddings yeah it's so interesting that the self-supervised learning from massive internet scale data has enabled this zero shot generalization that maybe supervised learning was never gonna going to achieve was that was that always obvious to you from the early beginning of self-supervised learning or you may be hesitant on that idea ah no i mean it was clearly i mean uh the potential was was clear uh that you know there's just like way more unsupervised data uh and so the the potential is always there whether it's gonna work out is you know it's always a research question that you you know gradually build up and uh see if things are going to work um i mean it was also like a problem that had to be solved um if you look at like supervised models they do work on a particular data set but then they're very brittle when you test them out of domain uh they're not robust you know the adversarial attacks on unsupervised models uh you know is like super well studied uh you know both in language and other uh application areas so yeah it kind of like uh felt like you know supervised unsupervised learning uh is is really important uh and had to be studied yeah yeah so we've been learning about uh the different use cases people are building with the we va vector search engine they have such uh diverse data domains that yeah that idea of that zero shot flexibility is such an interesting part so are you uh with your latest thinking about the idea that you can have an off-the-shelf model that covers all data domains do you think you still maybe need to cut it up a little bit like say this is the biomedical bur this or like a burp being like the typical acronym when you're doing this thing where you put data domain and then bert to describe it so do you think you still kind of need like computer science paper news burt or are we really about to hit this point where you just have this one uh thing for general representations that's definitely the goal uh i mean like uh if you look at the paper uh kind of uh um you know we took uh like like the the the central theme in the paper was to take one unsupervised embedding model and evaluate it on like uh i think we evaluate on like some 15 search data sets uh seven classification data sets and then like uh six or seven more sentence similarity data sets and you know we use the same embedding model uh i think there's something to be said about uh if a model can work well on a large number of tasks uh i think it's it's uh indicating that it is uh robust uh which i think really really matters in the real world um so yeah we do hope that we can have you know most of the heavy lifting at least most of the heavy lifting done by a single model maybe you need a little bit of uh fine tuning for your domain but not too much so one thing i really want to get into as well is the um the large embedding sizes and kind of the ideas of how how much embedding sizes you have but quickly before getting into that and and we're going to later on in the podcast talk about this idea of prompting but i really want to get into this idea of the impact of data pre-processing and i've seen things like you know you have to avoid new lines and that kind of thing what's kind of the story behind the data pre-processing for testing this embeddings api out yeah i i i it's in the it's in the documentation uh it's basically we use slash in as a delimiter uh for uh for like knowing whether something is a query or a document so it just kind of uh it was like you know in in the hindsight it's like it was like a bad choice as a delimiter uh but basically that's the reason why like if the text also has slash in it can kind of mess up the model uh uh yeah but that's about it i don't think there's other pre-processing things you'd have to do that's kind of a tricky step right with the um with the delimiters on the lines and uh internet scale text scrapes do you maybe like the idea of using the html tags i've seen that idea coming up and where you maybe use line break or paragraph break and kind of have the structured data in the unstructured data yeah yeah i mean our hope is we can like get rid of uh the delimiters uh all together in the next one uh i i think if this was like you know getting the model to to uh to initially get good results we kind of um wanted to try out a lot of things and this was one of the thing that that stuck uh but yeah we actually think we can get rid of the delimiters all together in the next run and you know uh even the slash and pre-processing wouldn't be required so you prefer the idea of having a super long sequence left to right yeah that's really interesting and so the structure of the breaking probably not worth the time i mean yeah yeah it's basically you know the model can handle that yeah so on the topic of say the um 12 000 dimensional vectors and i think that has such an interesting play and uh and describing say using pca to try to take down the the size of it and then things like say you build up one of these vector indexes like we like to talk about the h and s w algorithm on weavgate and if you've got 12 000 dimensions in each of the vectors it makes it a little tougher uh we've also looked into say binary passage retrieval where you can have some kind of optimization where you learn how to have a one zero one zero representation how important do you think these high dimensional representations are to also achieving this off the shelf any data domain zero shot flexibility yeah so so if you look at the results in the paper um we do uh report results for like embedding dimensions from thousand to uh you know the largest one twelve thousand uh you can like cut it by half uh or more than half i think even the 4k dimensional vector one uh you know it performs really well um so yeah if you want the absolute best performance uh uh the twelve thousand one uh would would uh would be the one to pick uh and it is definitely uh uh you know valuable uh for certain applications that care about the the you know the last uh few points for for other ones you know uh even the uh the four thousand dimensional or the two thousand dimensional one already has pretty good uh transfer transfer performance on search and classification yeah i think that's such an interesting part of it is the uh dimensionality and how much that kind of makes it cost to run it and things like that can you tell me a little bit more about the evaluation of embeddings and your general thoughts on say using the embeddings to retrieve more context for the current input and that kind of decomposition of retrieve than read for downstream tasks yeah that's exciting yeah i'll come to the second part yeah for the first part uh yeah so we evaluate on uh you know as i mentioned like classification tasks linear in in a linear probe fashion and text search and code search and also sentence summary so uh so we evaluate on like four sets of uh tasks um and like multiple data sets within each category um yeah i think uh we decided to give more importance to uh text search and text classification over tech similarity uh primarily because the for the former ones are more clearly defined and have like a fairly agreed upon definition i think sentence similarity as a task is still a little bit big and not clear what the you know the end use cases uh basically um you know if you take two sentences like we talk about this a lot in the paper if you take like two sentences like you know uh jack loves jill mary loves chocolate uh are these two sentences similar i mean uh yeah kind of uh but are they really maybe not i mean it's so it's like it's uh it's quite weak um and so uh yeah we we like the classification and text search benchmarks i think they're more uh clearly defined yeah i i know our webva users agree that this uh we like to text search and think of that and i don't know if too many v8 users are fitting linear probes on the embeddings and using that for their downstream supervised learning test so i do want to get more into this retrieve than read decomposition where where the idea of you retrieve some context to say uses data augmentation for your input and we love this idea we love say this particular thing of solving this problem of hallucination where these generation models generate like url links that don't go anywhere and all sorts of things like that so we love the idea that you can retrieve factually correct information you have the interpretability where you can see what it's retrieved you can update what it's retrieved and then we like the idea that you don't have to store the end to end the data in the model that also does the downstream task so can you tell me more about your thinking about the retrieve than re kind of decomposition of tas yeah i think it's definitely an exciting direction um yeah it to me it does make sense that uh you don't want to store uh all the knowledge in the weights of the neural net um and um uh so so it's like we as humans also don't do that uh we look up information all the time uh to uh that can help us guide to do uh what we want to do so yeah i think like factual generation uh and things like that um i feel like the idea of like having some way to retrieve relevant information uh and then like our general models are you know starting to get fairly robust in the sense that they can start using information in the context uh and leveraging it well while uh solving a task so it feels like uh this is starting to get to work so i want to talk a little more about how that kind of changes the problem into the k-nearest neighbor regression and and some of the evaluations in the paper about uh not just zero shot but that k-nearest neighbor where you go to your training set you find the nearest neighbors and you use that to inform the current prediction yeah yeah glad you asked that question it's like uh one of my favorite uh experiments uh in the paper uh so we take this uh to give more context we give take this sentiment classification data set from stanford um so the task is given given an input text you want to say whether the sentiment is positive or negative so we evaluate on uh four different settings uh one is zero shot uh so here we basically embed the text uh get a vector and then embed the labels positive and negative and then zero shot is just uh whichever labels embedding is closest to the input taxes uh embedding this works actually fairly well uh it kind of like works better than like the supervised uh neural nets that were introduced along with the original paper uh uh and then we also try out uh zero shot with prompting uh so instead of just positive or negative we just make a simple prompt saying this is a positive piece of text this is a negative piece of text and then use that embedding we get a tiny boost in performance by doing that and then the third one which is something you talked about is k-nearest neighbors so we take the embedding of the input text and then look up k nearest examples in the training set and then the label is just basically the majority one ah this actually works quite well uh it's very close to linear classification which i think is super interesting basically it means you don't need any task specific tuning of parameters to get at least a decent performance and yeah i think this is like one of the uh one of the surprising findings at least for me from the paper yeah i think that's an extremely interesting testimony to the potential of that retrieved and read decomposition that you can get the k-nearest neighbor i guess just parsing it again because i think that's amazing that the linear probe fine-tuning for the task is outperformed by the k-nearest neighbor or on on par similarly like competitive yeah yeah yeah yeah with just uh k nearest neighbor i think that's so interesting as you're talking it kind of reminded me of say with clip the way that you have a label representation by the text sequence of this is an image of a cat this is an image of a dog yeah that's true it's it's it's uh it's exactly the same yeah i think in uh like meta learning papers too they have like prototypical networks where they also learn like an embedding of a yeah that's a good point yeah yeah that's such an interesting way to think about that problem to have embeddings for labels thinking about those one-hot vectors and that kind of way i think that's why i really want to talk more about prompting i think it's such an interesting kind of thing with uh with the open ai papers and so firstly i wanted to talk about uh there's a recent paper uh titled prompt burt where they're uh saying that basically if instead of doing uh x where you have your sequence and then mask and then you say index the cls token to get the embedding if you have the template the sentence x means mask these like templates is the idea of prompting this kind of thing like helps the representation in a in that paper in a massive way so i'm really curious if uh if you've explored prompting for not just the downstream tasks in the sense of say the few shot gpt3 idea but in the sense of producing embeddings through the api yeah so this is a good question um we've not really studied uh prompting a train time uh we explored prompting at test time uh so yeah i discussed those results with uh sentiment classification where uh it did seem to help uh i also tried this for search um so the beer benchmark has data sets you know across domains you know as a data set but covet it has a data set about you know financial stuff and things like that so i tried to do a prompt uh pro i try to do a prompt experiment where i add something like you know apart from the query itself uh i uh kind of have a prefix that says uh this is a query or asking for information about code this is a query asking information about finance stuff uh it didn't seem to help that much uh in my initial experiments uh i mean but it was like definitely not something that uh we studied very rigorously and i think uh there's something i think there's something there that area yeah so are you interested in uh in this idea of prompt search where you're searching for the optimal prompt again the idea of uh this is a query about code would be one example of doing it or say hey i have a question about code would be like another discrete way of representing your prompt and then another idea maybe is continuous prompt tuning where they put the prompt into the embedding space optimize it with gradients how do you think maybe the search for prompts differs from the search for factual information which we're most commonly kind of studying when we're talking about most of the search yeah i think this is one of the cool uh research directions uh that's coming out uh yeah i want to answer it with like uh you know with few different things uh uh first thing yeah as i mentioned like i think the factual generation stuff uh is really interesting to get additional information in the prompt uh uh the generator models are you know robust and sensitive to context that it can leverage that the second one about retrieving things from the training set uh it's uh it's actually one of the end points in open ai it's called classification uh that uh takes like the nearest neighbors from retaining set and puts it in prompt that does seem to work better uh than like you know randomly putting in some examples uh so yeah again uh yeah the whole idea of like constructing these problems on the fly at test time are super important i think they're starting to work quite well um yeah finally uh kind of like you know doing some kind of like more expensive search um for prompts uh either at train or test time um you know i think that again uh i feel like has a lot of potential uh in the sense that the unsupervised models are fairly general and have a lot of information and the in some sense the current kind of context you provide to them steers the model only to a certain extent to the task or the input you care about and i don't think it like actually kind of extracts all the juice out of the model and i think these methods for like searching uh for prompt i think are super promising to kind of you know get the model to focus more on exactly the thing you care about yeah and i've always loved the idea of like ensemble techniques and i think maybe having a bunch of different outputs that come from a few different prompt sources could also be another way of having like an ensemble at test timing and we love thinking about say connecting several different retrievals together like as we build these end-to-end search pipelines we like to have say bm25 tf idf a couple different experts depending on how you train them and you have all these different things and see i think like having a few different uh prompts could be a really interesting way to have like an ensemble is there like generally on thinking of say you have a separate kind of embedding for query and document and that kind of line of thinking do you think you should have a separate kind of embedding for input output kind of examples compared to documents of like information compared to like task informations do you like that kind of idea of separating or just one embedding for every kind of thing yeah i mean it's definitely more convenient if you can have one embedding for everything it's hard to argue against that uh um uh yeah our our hope uh and i think our kind of like first crack uh at embeddings is that we can probably get like one embedding that seems to work well across tasks and across like use cases um yeah so yeah otherwise i feel like you know uh the the the ultim the system you're ultimately building has too many moving parts you know errors propagate things like that uh and whereas now i think having like a single model uh that can do all these things uh feels like the right way to do it yeah so transitioning a bit i i love how kind of like the clip model this is like multimodal with text and code and i can't wait to talk more about deep learning for code and that particular application of just generating code such as codecs and that kind of thing but first kind of back to the text embedding side do you think it gets say like that kind of reasoning information that program execution from the code data that transfers into the text domains like maybe we have this decomposition of say factual retention and then like reasoning and reasoning is kind of a pretty abstract thing but it seems like one idea for that would be to look at say mathematical expressions in their evaluations and then just language model that or logic expressions language model that and similarly with like the code outputs just language model that do you think that kind of property of the reasoning of that kind of symbolic execution translates into the text applications yeah that's a good question uh yeah i don't know whether it has to be through like uh you know output of executing uh something uh i feel like this the signal there is quite sparse uh you know you get very few bits of information uh from like you know like if you execute a program and give like you know i don't know one number back and if the model is learning from that i think it's like uh the i think models might overfit um but yeah i think if you can instead like actually uh have some like reasoning logic or code that you're like can additionally you know guide the model towards reasoning i think that that is definitely super promising awesome so coming into codex and how i think these cpc embeddings and we talked about the retrieve then redecomposition how these could probably take codex to the next level with our search powering uh so before that uh generally what are your thoughts on deep learning for code is it like because to me it seems super exciting i love this topic of deep learning for code yeah i think it's super uh exciting and it's like uh you know it's amazing that it's like starting to work uh really well like uh you know i use uh copilot uh uh you know uh every day for programming and it's like uh you know it's like super useful uh you know if uh i i notice a huge difference when it's when i have it and when i don't it's just like uh very very useful as in very very usefulness auto complete uh don't have to look up in other places that much uh so and like you know just the whole kind of like just just from a user perspective of like you know i can if i write the comments well uh i can usually get a good first draft uh which i think is like you know uh takes a lot of uh uh frees up a lot of uh mental space for me to like think about other stuff so i think it's just like amazing that it started to work and yeah i think it's uh um you know it's one of the it's prob probably the best application of uh deep learning models yeah i i do agree i personally agree with that i think some people might want to get alpha fold two in there and have that one also yeah but yeah i've seen so many amazing testimonials of copilot and it's so incredible and i mean coming from co-pilot which is something that already works and we're not even in the clouds but to go into the clouds do you think this idea that you could just kind of loosely sketch like an idea for a deep learning paper and then it could write you pie torch code that would create the paper maybe even set up like a weights and biases callback like all the kind of things that you would need yeah i think i mean the current ones uh uh not sure uh you can do that i think it is like fairly uh uh more like uh specific uh you know you know you can definitely do things like you know set up uh weight and biases and do the thing but i i i don't know whether you can do something like you know make the transformer uh attend to uh million tokens in a rough thousand uh yeah that kind of high level make this attend over in quadratic like linear complexity to in the just a natural language description so coming down from kind of that big vision of it that i think is so captivating one of the most interesting technical details of codex to me is the repeated sampling and how you say have the tree structure decoding things like top k beam search where you go through the tree several times and you got that trade-off between diversity quality kind of similar to sampling from any generative model like an image model or anything like that and then you have this kind of maybe a deduplication step which is maybe something that i want to bring we've eat back into which i think and coming back to this search thing because having these search embeddings let you also embed the tree traversals to make sure you pass less noise and overall make codecs more efficient have you thought about that kind of putting cpc in the filtering of the repeated sampling for say codex yeah that's a good question uh i mean i uh i i i saw that the for example um uh the alpha code work um you know they sample a lot of candidates and then they do some kind of clustering uh but their clustering is based on uh input output behavior uh rather than embeddings uh so that's like feels like one you know place where it can you can easily apply something like uh embeddings there uh code medics there uh but yeah in general uh i do think that um you know uh using these embeddings to guide the search process it feels very natural yeah yeah just the general idea of uh the embeddings for traversing the trees i think is general applicability and kind of coming back to the the cloud idea as i said this idea that if you're gonna gen if you're gonna write code that generates a deep learning experiment it takes so much time to run it's not like code forces where you can or you can filter by input output behavior because you can just run it right so you would need kind of that search layer to yeah yeah for sure you know and i also really like the idea of say how this could help people with like their specific python libraries that they're building up and help people get kind of the adoption the question answering and that kind of support so i'm curious what you think about say the role of code language models code search tools generation for say facilitating open source projects yeah um yeah so so i think like uh the best results we get in the paper are on like code search very clearly uh you know our model seems to be really really good at code search across multiple languages and in some sense i feel like text search is you know is a super well studied problem uh we have really good benchmarks uh we have lots and lots of methods and papers and you know software libraries uh for for text search um and you know there are places where you feel like oh my god tech search sucks but in many places i think it does it does a decent job uh whereas i feel like code search i think has a much further way to go uh and it's uh and like i i almost feel like it's probably the case that code searches like lags a lot behind text search and in applications is because keyword search works really well for text and you can like kind of build your first application based on that uh where it's not that easy to do that for code search and you really need some way to capture the semantics of text and code so that you can do search um uh yeah i i really think our models can like start uh powering uh the next generation of good search tools yeah i definitely think so as well and um just a quick question how big is the impact of like the compiler and input output like because the the text search and the thing and the problem with is it's so hard to evaluate it right like because it we we say things like uh when we were interviewing charles pierce at kenius and they're building a scientific paper recommendation system and that idea of serendipitous discovery and fuzzy research where it's like here's a similar paper did it help you like it's harder to evaluate really compared with the code where it's like it's correct like it's not yeah yeah exactly i i yeah i think good search is like super uh well-defined task and yeah i i think um the the data set that's commonly used uh code search net uh is quite uh i think it's like uh the training service is noisy uh it's uh script from uh just like open source code but i think they have like a um at least a subset of test set that's like more clearly uh marked with like whether this is the correct code for the for the query and i think uh it's it's a it's a nice way to test these models yeah and i think like how we say with um unsupervised text learning we can use back translation from english to french back to english and i think one heuristic we've seen with code is uh i think this paper is titled break it fix it where they uh corrupt the code so it doesn't compile and they use that to kind of get the data and similar kind of back translation way of generative models that generate their own data in that kind of like data augmentation scheme interesting i see so so you like train training a model to go from like a thing that like doesn't work to think that works and like using the compiler as a way to get that data yeah that's pretty cool yeah because i think debugging is is a huge application of this so let's go i know from like learning how to code that kind of you know hair pull your hair out thing where you can't figure out how to get past the error or whatever it might be yeah yeah so does that kind of use of the ground the reward signal from the compiler the input output pairs does that inspire your interest in reinforcement learning and i don't even brought this up at all yet but generally what are you what's your thought on reinforcing learning for these applications i mean like you know like if you step back i feel like search is probably an application that has benefited the most from uh human feedback uh starting from click data uh i i think like uh uh uh like the the like all search engines uh benefit from that click data and yeah and i think with code there is this way of like verifying uh verifying your code through input output tests yeah i think i think all that is like definitely super helpful so we've talked about quite a few topics and um so one thing is we we mentioned say i think the best transition for this would be to bring back up that idea of reasoning and as we have these deep sequential neural networks they have several layers and they maybe reason through their representations that idea of like parsing and selecting the information so i'm trying to transition into this idea of latent knowledge representations and say exploring that in these models and say vector quantization is another really exciting topic so i'm curious what are your what's your thinking around uh just latent space exploration whether it's like a generative image model or one of these chat bot kind of neural assistant things huh like uh do you mean is it like specifically like looking uh looking up a knowledge source or is it more like you're spending some search compute to info like activations or yeah the the latter one where you're trying to search to find what's going on with this network what's it thinking if you want to use that kind of anthropomorphization of it and say it's thinking but yeah yeah yeah i get it yeah i think uh yeah this is kind of like related to like uh the idea that like you know especially once they have these unsupervised models that are very broad um they they don't get like uh it's like at a test time uh usually i think they they don't they're not given like enough context to focus on the tasks on on that particular input uh and yeah i think methods that can like uh you know let the model uh um you know uh do a lot more uh you know do do a lot more uh thinking as you say um at this time it's definitely a promising idea um yeah i feel like maybe things like say like uh pondernet or like early exiting networks where you scaled the capacity to kind of simulate it yeah and also i would say like you know uh we touched on this before the idea of like sampling a lot of things and then uh kind of uh finally deciding which one to use uh i think all of this kind of falls under this bucket of like you know uh let's have the model uh do a lot more things at test time and see uh if you can figure out a way to like get these models to perform even better yeah so transitioning like a bit into say how we can kind of understand it do you see discrete bottlenecks in the intermediate representations of the network as being a way that we can understand what how it might be reasoning ah yeah so i feel like interpret interpretability with neolets has always been uh challenging uh in and i i think i i don't know whether having discrete activations would help with interpretability or not uh and yeah you know if it's like going to be like you know uh vqbie codes and like that are like also very high dimensional uh they're the skin but still high dimensional i don't think you gain a lot of interpretability uh um so yeah so generally that kind of is it maybe just more so it's like a compression thing than it is that idea i think so i i think uh i i think those techniques are useful because they help us uh compress you know very long inputs especially in the perception domain to to like a you know you create a bottleneck so also on this topic of bottlenecks with say like architecture bottlenecks do you i think i know the common practice generally is to say apply them to have the transformer be isomorphic it has the original input sequence length by say the embedding dimension and then the the tension layers they never compress that and to say like in computer vision architecture as how say like you net we would turn it into a vector skip connection up sampling and a funnel transformer has applied a similar kind of idea and then another one of the most popular embedding embedding models out there uh siamese bur sentence bird they also have that um compression so what do you think about that kind of style of compression yeah i mean in general i feel like for text for you know context lens we usually have uh uh i i think like uh you know the opening api some of the longer models and even that i think is like 2 000 tokens and like a lot of the open source ones are like 500 tokens or even smaller at that stage when you're dealing with text i don't think you buy a lot by compressing uh because like like parallel attention uh for that context lens seems to be fine uh but i mean the moment you move to another modality um that has you know higher kind of uh or like lower signal to noise ratio uh um like perception tasks or if you want to do like text models that are just like way way longer context i think that's when again like this kind of uh bottlenecks in the architecture so let's start uh helping you out um you know like vision transformer kind of the first step is to uh you know do like a very rough um encoding of the image in in in a low dimensional space and then you run the transformer on top of that and that seems to be working quite well so kind of wrapping up i know we talked about so many things from the embeddings api retrieve then re-decomposition contrastive losses and the search for negatives and i mean all sorts of things from prompting etc with the as they'll be the show notes for people watching the podcast but so kind of um to wrap it up and a question that i'm curious about is um what is kind of like what motivates you like is it a is it your curiosity of the algorithms do you have a particular application in mind that drives you yeah i think a bit of both uh and definitely uh you know kind of look at um you know what are things that we can currently solve uh with the techniques we know you know what are things uh we know work well you know that things that don't work well so try to uh you know understand uh the current uh you know current capabilities of our models and trying to push that uh and also definitely seeing the other side too of like you know what are what are things that can have a good real world uh impact so yeah it's a bit of both so one more question like that uh so what is kind of your information diet how do you handle this like massive velocity of new information and deep learning oh yeah that's really hard uh i think there's like so much so many papers coming out um i don't know in some sense it's like uh you know uh i i don't know i feel like uh i i think jeff hinton has this like uh advice on like uh you know don't read too many papers uh uh it will stop you from being creative uh i i i can see where he's coming from uh i feel like um i try to you know try to basically bucket things into some kind of high level categories and saying okay you know these are you know these are the sets of ideas we have these are the sets of problems we have you know and like these are the capabilities that our current models have and here's where we lack and kind of try to think from there and kind of not think too hard from a specific papers point of view uh and yeah i mean of course then once you decide to work on something and you're trying to improve something then you start diving deeper into that area trying to understand a specific paper or a specific set of papers in a lot more detail uh but yeah uh for for like high level kind of thinking about what to work on next it's a lot more abstract than a single paper yeah that's something i've noticed and i imagine like once you get to your level you have such an abstraction over the categories that it really helps uh do that filtering whereas like i think when you're starting out you see like neural neural radiance fields and yeah you're like all over the place with these different things and i think the better you get the better those high level categories guide your the buckets as you yeah i think that's such a well described way of handling that yeah i think you also get better at uh reading papers over time like you know papers where you know you're like okay i get the high level idea from the abstract you know sometimes you go into the results and then you know when to actually go deeper uh i think you kind of like get better at that over time awesome so arvin thank you so much for doing the vva podcast i hope our listeners uh you know excited about the open ai embeddings api and using it in in the wpa search and powering all sorts of search applications so many interesting details to explore with with these algorithms and and what they can do so thanks again so much for coming on the podcast awesome thanks for having me it was fun chatting with [Music] you you ", "type": "Video", "name": "weaviate_podcast_7__arvind_neelakantan_of_openai__embeddings_api_in_weaviate", "path": "", "link": "https://www.youtube.com/watch?v=uFxfZ0vLsoU", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}