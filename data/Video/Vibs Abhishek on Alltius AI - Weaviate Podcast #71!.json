{"text": "Hey everyone! Thank you so much for watching the 71st Weaviate Podcast with Vibs Abhishek! Vibs is the CEO and Founder of ... \nhey everyone thank you so much for watching another episode of the we8 podcast I'm super excited to welcome vibs abishek vibs is the CEO and founder of alas which has created some amazing project uh products like no plus and vibs is also a professor at business school at UC Irvine covering things around information systems and AI so I'm so excited to explore these topics with vibs vibs thank you so much for joining the podcast thank you so much K it's wonderful to be on the podcast I'm a big fan of vv8 you've been using it ever since we started building alus so it's a pleasure to be on this podcast you know I'll give you a quick background about myself uh I've been doing AI for probably like 25 years or so so you know I'm I don't look that old but I am quite old U and probably the last 15 years I've been doing a lot of research on AI and teaching AI at schools like upen CMU and more recently at UC Irvine um you know I think what's happening with generative AI is really taken the World by the storm but this is sort of a childhood dream coming true for me this is what I wanted AI to do when I started working with AI in you know when I was a kid almost uh we can create these agents that have magical properties they can tell stories they can do cool things uh but more importantly I see many applications of them in the Enterprise space and so uh for the last few years I've been teaching courses on NLP at UCI and at CMU I've been working with large language models for quite some time and you know the for me the inflection point was gpt3 uh is how much power it had and how much it could do without creating these custom models for NLP which is what people have been doing forever in NLP right um and then that's when I started thinking of how can we apply this technology to solve business problems and that was a Genesis of alts how can we you know create these AI assistants that can help people in the workplace uh technology and the world around us is changing very rapidly and we are not learning things any faster so instead of you know uh trying to solve the problem of how do we learn faster can we have these AI assistants that go out and learn on your behalf and can help you do different things pitch better talk to your customers better improve customer experience for example and so that's what we have been building at all tiers uh for the last 18 months or so yeah that's also exciting I I had a look around Al's website in the blog post I saw things like DARPA finalist for learning assistance and yeah overall this kind of assistant platform you've built is so exciting I highly recommend people watching this video go take a look at it quick four minute loom video really gives you the overview but so this perspective I think you've created one of the most impressive uh user interfaces I've seen for managing multiple assistants or agent whichever word we want to use right assistance agents but like this kind of way of uh you know I upload documents and then say I have my chat with docs agent or I point it to my SQL database that has like I don't know sales data and then I have my kind of assistant that does SQL queries can you talk more about your perspective on multiple assistance and how you might want to manage that right I think you know uh thanks for pointing that out Conor the way we think about assistant is really they are related to a Persona right so if you have a customer service rep you have an assistant that faces that customer rep if you have a end user on your website that's a different Persona and an assistant can be created to solve their problem uh it can be an operations manager and you have an assistant for that operations manager so we have a fairly horizontal platform that we' have created of course we going very deep into certain use cases around sales and support uh where our customers find a lot of need uh but our platform rests on three different pillars the first pillar is knowledge right all the information that we can sort of understand or know to solve a particular problem right or to do a particular task and what we have been able to do is systematically try to build not only uh unstructured sources of of information like PDFs and docs and whatnot uh but more in line of that images and videos we have image understanding video understanding but in the last couple of months we've also been able to build in structure data uh so you can connect it to apis you can connect it to databases flat files csvs uh and so you know we believe that to do do a task right when we think about this person trying to do something that data might aside in many different places how can we pull in all of that data combine that in some meaningful way so being able to access different types of data sources is very very important for us so that's one pillar for us the second pillar is what we call the pillar of skills is once you have the data in what can you do with that right so think of a Q&A assistant that's a very simplistic assistant that lot of people are building today is I have now all of this documentation let's say PDF documents for example right can you answer questions on that but it goes even beyond that in the sense that maybe a question some pieces of information decide in a database somewhere some pieces of information aside in a document like a policy documentation uh and how do you pull that information in to be able to answer questions uh so we think of these skills being run by different agents um and you know that's that that is our distinction between agents and assistants an assistant mirrors a Persona a real person whereas an agent does a specific task right and assistant can be a combination of multiple different agents right um and so you can have an agent or a skill that is helping you pitch right create a pitch for your customers or helping you solve a customer support ticket right uh so these are what we call skills and behind every skill is an agent that's technically implementing what we talk about and then we uh have this model of we talk about channels and this is where we make them available right so maybe as a consumer I'm interacting with this assistant on a widget that's on a website or a mobile app it can be something that's integrated in slack right a lot of our def Tool Company that our customers have a slack or a Discord bot and we integrate there or it can be plugged into your support system uh you you know I think it's it's quite wide in terms we want to make it as widely accessible as possible so we have apis through which you can make uh the assistant that you have trained available across multiple different sources you can plug it into your front end you can plug it into your core system to do something uh so we keep adding these channels but really we want to meet where the user is right with that philosophy yeah wow I that that kind of knowledge skills channels I I think it's such an awesome abstraction there's so much to dive into there right I think um yeah maybe if we could kind of come back to the data part I'm really C and just kind of go through the full stack again microscope the so the data I'm really curious about like um there so there's a lot of stuff around like data ingestion that's coming up we recently our last podcast was with n AI where they're talking about keeping the source in the sync destination in sync so say you have uh your you know you have your SQL data in say postgress and then you want to sync it up with your vector database or you know all these kinds of different things and so maybe if we could just dive into like I've seen the the UI on no plus for how I would upload my documents or I also really really love this kind of like uh also using the web so if I if I it look for a question and I can't answer it maybe I do a web search and maybe I add that result to that back into the knowledge can we maybe just uh talk through kind of the concrete tools and perspectives on that kind of curation of the knowledge Corpus sure so you know I think we like what we realized very early on is we tried many different approaches right we started building very early in the space so we tried everything from fine cing a model uh to you know what we have to rags what we finally sort of decided is we would have to build something custom uh to both ingest the data so if you think about even PDF documents for example there are many different types of PDF documents uh so we use a bunch of different apis just to understand what is it a presentation is it a you know user manual is it something else how do we ingest that so even for PDF ingestion we have different ingestion uh apis um and then if you think about uh integration with custom some apis or publicly available apis what we have been able to do is we can read in the product doc the API documentation get a token and to some extent I think none of this works 100% like we discussed last time but we can get it to work 90% And then with some you know heavy lifting we can get it to work 100% but so what we do is we have this Ines framework that we have created that is custom or proprietary to us of course we use Lang chain for a lot of our orchestration H but once that data comes in it can go into let's say um a vector data database it can go into Knowledge Graph the knowledge graph helps us basically cross reference stuff and when we can't find an answer in the in the vector database we can go and look at one hop two hop uh things from there on uh to pull information from uh the vector database or wi Versa right we sort of use that in a very uh in a very Tiddly coupled manner uh but some of the API and the structured dat data that we have we don't typically pull that in because some of our customers are huge customers with millions of transaction records or billions of transaction records so there's no way possibly we can store that in any infrastructure that we have internally but what we want uh to do is have the ability to be able to query and pull this information as needed so when we retrieve information we have an agent that does the retrieval which figures out you know should I be looking at a vector database should I be looking at the knowledge graph should I be looking at doing an API call to pull that information in right so there's intelligence in the retrieval layer uh and you know for most of this we not actually going really to an llm to do like of course the llm is part of the querying and resolving from this utterance how do you get to the source of the information but we have not started even constructing the answer so there's a lot of uh steps in fetching the right information synthesizing it cleaning it up before we actually send this to the llm to be able to create the answer right uh for most part the final L that we're using is for some kind of uh you know human like answer generation or maybe some higher order reasoning once we have all the information we need to do some higher order reasoning to synthesize that information but a lot of the retrieval is sort of our own code technology that we have built and the reason we decided I think you know there are different ways of doing this when we were building we realized that for Enterprise use cases Hallucination is a very very big problem and we tried to solve it in many different ways and finally we realized that uh we have to really control the information that we putting into the llm uh or the prompt to finally generate the answer and the only way we could do that was through creating our own custom you know uh data storage data retrieval uh data recommendation layer to be able to do that yeah before moving on to skills and the second pillar I really want to unpack this a little more this kind of like SQL router as llama index calls it that's how I first became aware of the idea where you take a query like what is the average age of country music singers and you route that to the SQL database instead of the vector database and so I kind of want to dig on this a little more like it sounds like you're combining Vector search with knowledge graphs and I know ol GRA with Cipher I had quickly tried to learn it and like sparkle and I was but now with the LM I suppose it's easier so yeah how do you manage that like is there like a prompt template for how to formulate queries in each of the languages you have to kind of retrieve the schemas maybe yeah so you know uh on that currently we're using our own intent manager to be able to do that uh and basically the intent manager is simply um you know a classification model that we have built uh using bird but uh we tried doing that using the langin agent framework it wasn't as accurate as we wanted it to be like it would know when it's supposed to go to the you know Q&A uh you supposed to go to the vector database and maybe try to pull from an API so we ended up building this intent manager that's doing the routing for us uh of course we keep experimenting uh you know we take maybe three four steps forward one step back uh because everything is solving so quickly uh that's the solution that's been working for us I don't know if you have heard of other solution that works uh you know I think in Enterprise the the it has to work most of the time right I think and so that really constrains Us in terms of how much we can experiment or what's a what's a tolerance we can have right on some of these things not working yeah I think that's such a powerful idea that that like classifiers classifiers are the you know the introduction to machine learning but still just like one of the easiest models to train and but it also I guess it has this like I see the language models is they're so good at zero shot they can give you the proof of concept give you your MVP but then when you want to you know get get it really running so but this idea of classifying a pipeline I'm so curious like um so maybe this would be a good transition into skills and we could come back because I understand skills to be say question answering or summarization or reading from one of your blog posts about using LTS for finance finance and you things like topic literacy getting started research support analysis so so maybe if we could talk about these skills and then we could maybe come back to this kind of intent manager router classifier on top of that right so you know so the skills are like really tasks that people need to do right uh so think about I I have a customer who calls in they're asking me a question how do I answer that question very quickly right so that's a standard Q&A type assistant or a Q&A skill right part of assistant or it can be uh help me create a pitch for let's say some of our we have a couple of insurance customers and there uh someone will call in and say hey I am uh 75y old man in California with diabetes and I'm looking for an insurance plan that has good uh dental benefits right and so very quickly what we need to do is look at all the plans are there maybe 6,000 Plus plans figure out which plans match the benefits that this person is looking for uh and then say here are the plans so this is like just retrieval of that information right so this is so maybe building on the Q&A scale it just pulls up that information but then there's another skill on top of that that can sort of say help me Pitch the plan to that customer right and how do you pitch the plan is a different type of skill because this might require you to be in compliance with local regulation you can say some things in California that you can't say in New York and vice versa you want to have a certain brand voice you want to maybe not even the brand voice there is a sales manager who's your manager and they want you to be selling in a certain way so the pitch agent is then customized or can be customized for that specific agent uh and you know so that's the beauty of large language models is we can have all of this parameterization mostly in English and say here is a customer this is what they want this is how you should be pitching this is the general framework for pitching now create a pitch for this particular customer right of how do we sell this insurance better so so that would be like that pitch would be a skill right and we give ability to our our customers which would be like these insurance companies for example in this case how to customize and create that pitch uh skill for the particular use case um and you know I think that's why we going deeper and saying can you build skills for specific use cases for specific Industries which people can then very easily customize with adding in their own you know secret source to those assists I yeah I guess is the the first thing of like finding the the right plan for you the 75-year-old diabetes patient it it kind of like it kind of reminds me of this like when to use a classifier versus when to use embeddings and if like you have like 60 plans maybe it's better to use a classifier but if you have 60 Mill something yeah so I really like that nugget but then what you were finishing talking on this um you know customizing the script I this is what I think is just so powerful about the language models is in the traditional sense of fine-tuning maybe you have this style and then you have some examples and you fine tuna model but it's not as flexible to like now I want to change it to do this so yeah I'm very curious about how you think about that kind of you know how would you adapt it would you uh you know have this you change the prompt then maybe you generate synthe itic data and then you fine-tune it and then compress it to make it you know yeah I think it it relies on a few different things right I think the way that we have architectured a system it is pulling in the right you know because we have this huge back end that can pull in the right pieces of information uh so one way is you take an llm and fine tune it for a particular insurance customer on all the data right typically what we seen is that two challenges with this one is we need to have an instance then for every customer and if you know if you're hosting a version of L of 70b for every customer it just our cost structure just go out goes out of act very quickly uh the second thing is the bigger the model is so you know the bigger models are better at reasoning for example this is what we have found out and I think there's there are papers that talk about that but they're also more difficult to change uh because you need to update so many model parameters so the way we get around this problem is saying here is the data and we become very good at retrieval and saying okay this is how you pitch a plan and now for pitching a plan this is how people have pitch plans in your company we sort of summarize that put that in the prompt or it can even be pulled up when we creating the prompt at runtime uh and then that is instruction that then goes to the LM to say you know this is is this is the world put blinders on and then go and create the answer and so that helps us really match it to the specification not only the data is very accurate and the answers are not there's no hallucination but then match it to exactly how that particular company wants to or that Enterprise wants to make it available to that end customer right and then we can add layers around policy and compliance and all of these things things uh so that the llm does not you know go out of bonds and of course we do some post processing stuff on the an that are just generated to make sure that it is compliant and whatnot but really that's what is helping us you know being able to generate the answers in the way that our customers want yeah I think I guess yeah I think there's so much to this kind of conversation topic of like uh you know if you want to have a language model for each of your customers is like maybe we're imagining some kind of like reinforced learning from Human feedback where each of your customers is saying I like dislike this answer and then you're tuning the language model and there are ideas like maybe you have like the Lowa the low rank adaptation you have like these sparse parameters for each of those customers and maybe you do something like that but then you also mention this idea of like the general reasoning ability and if you fine-tune your language model in the specific set of conversations is it going to lose its ability to say format the API request for the tools you might have hooked it up with as well as right yeah so yeah so I think all that is so interesting and maybe we can come back to that topic as we come into agents but I kind of want to round out this third pillar as well this the channels that you're deploying these agents into so like I imagine a slackbot or maybe something that reviews P requests like what kind of channels are you thinking about yeah so yeah slack is definitely one of in fact that's the first channel that we started out we did not even have a front end slack was what we started with you know because I love slack as a product or maybe I loved slack as a product for the recent refresh that they went through um so we started out with slack we added the widget of course we have our own playground where people can go in and sort of interact I think that's more for the training purpose not so much for end consumption but some customers are using that for end consumption too uh there is an API that you can connect to uh we want it to be very low code or no code because the target persona for us is business users uh the reason we want to Target business users is I feel there's a lot of power in language models but if we are just depending on data scientists and Engineers to bring it to the market we just don't have enough people who understand these things well right so we want really to put them in the hands of business users and that's why we have tried to create a very very youve played with the tool it's very easy to use uh but we do have that API connection that you can train the assistant and you can use it uh to integrate with your front and or wherever you need to do that in your own workflow uh we keep adding more channels we are planning to add a Discord B we planning to add uh WhatsApp B uh so as in when we have requests from customers we keep adding there it's fair easy for us to add new channels um so yeah but I think the the idea is you know with with the philosophy of making it available wherever the end user is we want to be available in those those places yeah I think it's so interesting I I guess like for me I wonder with the Enterprise customers and you have this conversation do they have like some kind of you know data Lakehouse Warehouse I'm not super knowledgeful of the differentiation there but like and you sort of would say you know you have all these different sources of data I'm going to hook up like 50 agents to the particular sources and and then here's a chatbot you know this GUI you can talk to it when you like or it'll send you email yeah I think that's a general direction we're going uh I wouldn't say that we can handle like 50 agents especially 50 like agents that that acquiring databases I think there is a little bit of a climb for that but of course like 5 10 agents are not difficult to do uh typically you know any assistant for us would have anywhere between five to 10 maybe sometimes even 20 agents uh but there'll be like some of them will be like more like Q&A uh some of them would maybe big like API agents uh and so and so forth uh what we were also doing is earlier we had agent agents that could process different types like there was an image agent there was a um like a video agent now we combining all of that into one and saying okay you know because we're putting all of the data in one in vv8 for example with the right tags we can have just one agent that can go and index all of the data it has slightly different Behavior because you know when you answer from a video for example you need to be able to retrieve that segment from that video and show it in whatever the front end is right whatever the channel is um but um overall I think we consolidating some things on the agent side so that you don't have too many agents but that's the vision right like you should be able to connect it to as many sources of information as possible and our routing mechanism then figures out where to go and look for that information yeah I I think kind of like collectively settling on the these abstractions around what's an agent and then assistant so to recapping for myself the assistant it has a Persona like you are a news chatbot or like you are a data analyst and then an agent is like the skill so so the agent is like the question answering or summarization right do with this kind of abstraction do you see any kind of overlap like like when you're separating it separating it out into skills like question answering summarization as being like two of the most canonical things that people do like do you ever find like the question answering model needs to summarize stuff and so you know within yeah that's possible too right so so you know I think like again just to recap the framework right assistant is for a real person who wants to do something right and so really what the assistant is doing is running a workflow right it's a set of tasks uh an agent is doing a specific task right so so let's say for example um you know I'm uh I keep going back to this customer support and sales use cases but let's say uh I'm an assistant for a sales agent one of my skills can be compare different products right and what that might require is for me to go and summarize product a and product B and then do a comparison right so there is a chain of thought that runs which is this is how we summarize a and this is how we summarize B and then there's a comparison that needs to run and that's sort of the overall comparison skill uh right and that is just one of the skills that's part of that sales assistant right there can be another skill which is how do I negotiate price better give me a script to negotiate price better right so that can be another scale that the assistant can have right so our framework is fairly General in terms of how you can go in and create these skills you know it's almost like Lego blogs right you have some Primitives you have like these tools that you can put together to create agents right and then you can put agents together or skills and agents are synonymous in our case right you can put these agents or skills together to then create the assistant that does helps a particular person yeah I I love the obstruction I guess it's like um you can think of like the the question answer or the the compare documents thing is like a like a tool kind of like the API is like another one of the abstractions right is the is the tool so so then it so I'm kind of curious like when it finish its compare documents now it has this result do you save that result somewhere does that just go into the input window of the main assistant yeah so we have a dialogue manager that's doing all this orchestration right it will call if and the dialogue manager is the one that has the intent uh classifier also as a part of that so it will call the different pieces get the right information it has some memory that and that's not like that's not being powered by an llm that's like really like software that's doing all more like you know I would say something like dialog flow for example like it's really that dialog FL equivalent for us is that's the one that has memory and it's sort of storing all the intermediate steps and then trying to figure out what to do like even when you have a conversation should it maintain context should it not maintain context right uh when should it refresh the last three questions that were asked for example uh and figure out that we have moved to a different topic right so all that is being done by the dialog manager right now yeah well I so I was from our notes in preparing for this podcast I was inspired to read more about this mem GPT paper and I'm really glad I did because I think it put me in the right frame to understand this idea where they kind of separated it out into like the kind of like event part of the memory which is like the running history of the dialogue as well as like the working context and they have things like um you know when you search when you search you then have the search results and it's like what do I want to then take to put in my in context memory and so right yeah I think that is this abstraction of like the top level has this like its own event cue and then it calls the other model that then has its event queue and yeah and that's a very you know I just saw your podcast on M GPT it looks very fascinating and you know that's one of the things that uh I've been thinking about is can we instruct you know the our like no for example to retain some things in memory right and so I asked a question I said okay retain this in memory I ask a second question I say retain this in memory and then when I ask a third question uh and so this is this goes beyond context right I think one way that we maintain context is we just look at the previous you know interactions like any like one two five interactions right whatever it is but I think there can be a deeper way of thinking about this is there is like actual memory where we putting stuff in uh either automatically or by what the user is saying and then we sort of say okay uh now answer everything else from here on based on what's there in the memory right and now flush the memory and you know forget that that context so I think this is this is an idea that we' have been thinking about too uh and definitely I love to read more about M GPT I just sort of got a cursory view of that from your podcast which is very interesting uh but yeah I think these things are very like you know essentially if you think of it llms are just uh you know next toen prediction machines they're not full Computing devices right like you can't run an operating system so to speak right you using uh llms and so how do we get it to that stage like think of that as the chip right uh in the usual operating system parland but we still need to bet all of these other pieces and like rag would be one aspect of this right like you have some memory uh but then uh you know you have the hard disk and then you have RAM and then you have all of these things so I think you need to build all of these pieces uh to uh to make it really a powerful and a self-contained you know Computing machine for example yeah I love that kind of like hierarchies of memory and the analoges of like Ram is in context dis is like my you know Vector database or my SQL I guess one thing I'm I really want to ask you about is so this kind of idea I you know I I keep the history of our chat in my knowledge base or maybe I do this kind of like GPT cache idea where it's like question answering and you ask me a new question and the vector similarity to one of the questions I've previously answered is above like 0.9 so I just send you that answer but then I'm really curious so so that so that gbt cache idea is like I'm caching questions that real humans have come to my chatbot my customer service and they've actually asked this question but I'm really curious if there can be some some kind of offline processing where you give your the llm like a hundred of your documents and then it I don't know I don't know how exactly how this is orchestrated but it it creates kind of like new information like maybe new question answering pairs that it it like somehow has decided that's that's it's an amazing point so this is something that my co-founder Sid proposed a few weeks ago and we have actually added this feature it's in staging it do not moved to production which is can we create this fq list to begin with right so improve the you know the cost and also the the the response time given a set of docu like especially in this customer support use case we can do this right for generic things that are more generic and we create a list of questions beforehand uh and not wait for people to ask these questions that we start doing semantic casing on for example so that's actually an idea that we have been we have actually implemented already you know I think that's the the fascinating part of this right like it's like everyone is building in this space and we're thinking of these new cool ideas um and so definitely it has a big impact on you know how fast we are able to serve these questions because latency is a big issue uh and you know like we have now probably hundreds of thousands of users not current users but you know maybe in an hour trying to access a system so we speed is becoming an issue for us um and so these are some ways in which we can actually solve that problem and not really you know at runtime rely on the llm to generate these answers yeah I love it from that perspective of optimizing latency and cost I think that's kind of the origin of the GBC cach and and yeah that automatically populate FAQ it really inspires me into this whole and I don't know if I've like done too much daydreaming with this kind of thinking but like this way that you can use the LM to generate synthetic queries which then you could use to tune the system because you can also have the llm say hey was that a good answer to this synthetic question I came up with and it can kind of like tune itself that yeah that's a that's a very interesting point you know one of my friends is a professor at UCI in CS and he's done a lot of the seminal work in llms and you know understandability of large language models and one of the ideas that he proposed was how do we use the llms for be a fact checker or even checking the quality of answers we haven't done that at runtime yet we do you know I think some of the things for example when we try to maintain context we using the llm to generate the question we say okay here are the three previous responses can you generate a question uh which this is a continuation question but sort of create a self-contained question based on what was asked earlier so but there are ways in which you can you know offline process this and say hey uh tell me the quality of the answer be and the the challenge is there two folds I would say right one is typically you don't want you want the maker maker and Checker to be different like so you want you don't want the same llm to be answering this question you know it might be a little bit of cheating uh but there are also certain llms that are better for this job like gp4 is much better at this than any llm that we have tried out when you look at you know quality of the answer or you know doing some little bit of introspection so to speak um so yeah so some of the smaller models are not good at this some of the larger models are better and I experience dpd4 is probably the best when it comes to sort of answering these types of questions yeah I don't want to like I I I think maybe they'll stay in the clouds and we'll come back to some calary questions but when I was looking at no plus it kind of inspired me of thinking like you know like a notion for assistance kind of and so I was curious if maybe like you know someone in a different depart like you know across the company we have all these roles like you know product team marketing team sales team and so like if their assistants talk to you know if if the sales assistant talks to the engineering assistant to try to come up with these questions and and the I think that's a we haven't thought about that I think that's an interesting idea you know maybe we think about I think it's the experiment worth doing doing you know you add two assistants and you know we had these uh I think Google home versus Alexa type interactions uh it's possibly like it's doable right like I think when we integrate something in slack for example right you have to address it you know at no and whatever right at that assistant and it'll answer your question all we need to do is sort of put in two of them in the channel and see how they interact uh that might be interesting you know and I think that sort of gets in the line of automation right like how much of your task can be automated we haven't started thinking in the direction but I think it's a powerful idea of like our model of an assistant is very individual Centric but can we take it from that and say hey can we think about teams of different people interacting with like interacting through their assistants right um that might being testing yeah yeah I love it I think it's such an exciting direction of the future and it brings me to my next question that I wanted to ask you which is about uh when I was looking through no plus I saw things like how you have the um attribution of sources in the question answering so it's you know the the loom video it you asked it a question about the expense reports I think and it says hey this is the answer and by the way it's in this PDF you know just in case want to check it so I'm curious like just your perspectives on the user interface design another recent weeva podcast was Charles Pierce from tactic and they have tactic generate which is like you know a UI for comparison across multiple documents and so I'm just really curious how you think about the kind of the creating of the user interfaces that you know coming to our last example if we have agents that are talking to each other I might want you know some kind of new way of visualizing so it's you know I think it's I think it's a moving Target I would say definitely my like Sid is the one who is responsible for a lot of the thinking that goes behind the ux uh but one of the principles that we have gravitated towards is making adding as much transparency as possible right because there is a level of trust that needs to be developed especially in the Enterprise use cases people might be skeptical people might be afraid of what's happening so we want to give as much visibility uh to the customers to the end users right and our customers as as much as as much visibility as possible so I think that has uh defined a lot of the user interface uh I think the ease of use has also defined a lot of the user interface right how do you go in how do you create these assistants making it very easy for you know non-technical user to be able to do that the choices of words that we use the imagery we use all of that is to make it very simple for folks to start using it uh I I do think that there is going to be a learning curve in terms of you know I talk about this idea of how do we move from a world of clicks to conversations right uh I feel it's it's a very important idea not because I I've sort of thought about it but you know conversations are very natural for us and even when we started Al and you very you were very focused on how do we create these assistants only for SAS products right uh the the thinking that was driving my perspective at that point was you know for every tool there is a different language we need to go and learn that language this language is of clicks and then I have a workflow that I can articulate in words but then I have to convert that that workflow into the language of the clicks right so there are two facture points right one is I don't know that language and so I need to learn that language and then I need to do that translation right and what these large language models have done done by and large is not they're not perfect yet but you know I don't need to do that translation I can say this is what I want to do and the system understands what I'm trying to do and then it does the translation right and hopefully there's enough data that the translation can be done right think of a general Transformer model that's what it's doing right converting from like it goes back all the way to B right like translating from one language to another language right I think translating from a language to maybe machine instruction or clicks or whatever have you right uh I I think that's going to be the fundamental change that will happen in user interface design I'm not much of an us expert so I don't know what that uh user interface looks like in the future but definitely it'll be very conversational at it's I'm not saying that click will go away completely I think we use that also right we have we ask people to click on things and verify things and whatnot I think clicks are a good way to get you know very definitive uh input uh when you don't want to have too much you know uh ambiguity in what is being said or the kind of input that you want back from the user but there will be an interesting way to design these interfaces that you know I can only possibly imagine but maybe I can't even articulate in words too much okay so I hope I'm getting this right so with with clicks with going towards conversations instead of clicks is this kind of like the workflow Discovery almost like RPA kind of thing where you'd like you know if I'm researching uh books about database internals I don't know and I'm on Amazon and I'm looking through the books and so you kind of look at the clicks of how I navigate the user interface and and so maybe is that I think that can be one that can be one way of doing that right that can be that's probably what Adept started out doing um uh that you know this is what the user wants to do this is what they you know actually the clicks that they do on the page can we create a translation from that user interface to the clicks that they're making but I think it can go even deeper right I think if we can have these API Integrations then we don't we can just completely circumvent that layer right we can say here is what the user wants to do here is what our apis are able to do can we like this is like exactly what's happening for uh text to SQL for example right I type in a query and then that gets converted into SQL and then the right piece of information is fetched for me uh I would say it's sort of maybe that is the direction to go and sort of you can then completely circumvent this translating from what the user wants to do to clicks to then API calls to just directly go to API calls right but I think it will depend upon you know I think there are few companies that are trying to navigate that route to like I think what fix is another company that's trying to do this Adept was trying to do this uh but uh yeah I think conversations I think the my insight is conversations will become more important in the user interfaces uh and then they'll be figuring out from the conversation what actually needs to be done yeah I love it I I think the the code interpreter for me has really helped me understand like you know like putting uh writing code into a chatbot well I guess a lot of people probably already knew that with co-pilot and stuff like that but yeah I could definitely imagine just even the comparison across documents that it just kind of gives you the analysis in a chat bot and yeah is really exciting so I think kind of a concluding that clicks instead of conversations I mean sorry conversations instead of clicks is a really interesting insight and so I really wanted to ask you this question I think your you know duality of background is so fascinating to be CEO founder of a tech company as well as a professor at UC urbine and business school can you talk about maybe like what that joint experience has taught you yeah you know I think uh I've taken a very non-traditional path to becoming an entrepreneur uh like of course what the academic background has taught me is like expertise in this particular area in natural language processing large language models uh given that I'm a business professor a lot of my effort for the last decade or so has been how to get value out of AI and that's a very tough problem right like I think you think about even forget about generative AI but but more traditional AI a lot of these AI projects don't succeed and a lot of my effort has been how do you you know build AI should you even build Ai and if you build AI how do you deliver business value uh so a lot of that so I think all of that learning has really helped me think about use cases where what we doing at alts is Meaningful and it's very customer-driven very focused on delivering value to customers to Output to customers really know tangible dollar right whether it is on the revenue side or whether it's on the uh the cost side of things and so I think that has really helped me take a broader perspective not sort of as a technical person saying okay this is coool technology let's sort of find out Solutions of this but really looking at this the problem and saying okay how can we solve that problem using the technology that we had right and I think back when we started then generative AI was not cool so we not sort of saying okay how do we build a generative AI startup I think the problem that I started with was you know I've taught probably 5 6,000 students by now they struggle to use software how do we help them learn this stuff sooner and then said okay seems like you know after looking at bunch of different ways uh generative AI seems to be the right tool to build that platform to enable them to learn things right and as sort of that led to okay we can't only we don't we not only can build tools for helping them learn but can it actually it can be a true assistant to them right so that was the progression that we took as a company right and of course then CH GPD happened and then the whole sort of you know I think all of us are living in that World um but so I think that's really helped me make that transition or I would say inform a lot of the decision that we make at at all on a day-to-day basis yeah I love that kind of like the dog fooding of like how do I teach my students better while being an NLP professor and then I think that transitions so nicely to kind of like customer service support tickets which also is kind of like this education problem and I guess for me the big thing is with uh I think with the with the past a before chat gbt and large language models and for me it it took chat gbt gbt 3 wasn't enough for me personally but maybe we talk about that another time like the um you know you used to have this data lay in problem like to to train a question answering model like Bert on you know a new customer service data set you would have to have some domain expertise in that thing right and now it seems like the large language models can really help you bootstrap data in a really novel way exactly I think that's the real you know differentiator that you know these are what what we call gpts right general purpose technology so it's not general purpose Transformer general purpose technology that earlier in NLP you would have to curate data for a specific use case even if you wanted to do something like classification using bird you would have to actually go and get data for that domain for that you know particular use case uh to be able to train a model on that and I think what the you know GPD 3 onwards has done is you don't need that specific training data right you can get I think and you know I think it gives you a false sense of comfort I would say you you spoke about this earlier right it's easy to create a prototype and you get you can get started early uh soon uh but uh you know it still requires all this thinking and fine-tuning and you know putting all the guard rails in place to actually make it production ready so I would say it's it's generally in the right direction that these models are powerful enough that they don't need a ton of data to get started so earlier you have to start from scratch every time you wanted to build an NLP model now you're probably 50% of the way there just because of the large language models but you still have to do that heavy lift of how do you go from 50 to 80 to 90 to 98% right uh uh so but but so I think that's why I say it gives you a false sense of um accomplishment or success uh but you still need to walk that but at least you're able to start a lot of people are able to start in that Journey which they were not able to earlier yeah so I guess my last question would be the the 55% of the way they thing I think that is just one of the most interesting topics I bring it up on every podcast is this kind of like fine-tuning zero shot thing and I'm just curious like like so so do you think to get a really performant customer service let's say for how to fix an airplane or something like that like you have to fine-tune it with gradients somehow or do you think maybe you can just tune your knowledge base tune your retrieval maybe that maybe that entails fine-tuning your embedding or ranking model but like this idea that you instead of needing to find in the language model you can just get a really good context and then the zero shot model could get to that 95% I think that's a tough question you know uh our our stance is you can get a lot of the way there with without fine tuning uh like our experience has been that if you really want to find tune you need to have a ton of data to find tune otherwise it just it doesn't work and I think it can be we don't know how to find tune the model as well so it can be our lack of knowledge but unless we have you know thousands or hundreds of thousands of documents for example right uh it is possibly you can get much better results by just you know some kind of augmentation of the llm uh uh I think it also depends upon you know how diverse again the llm is in a certain you know I'll get a little bit technical here right is in in a certain space because of what it's trained on it's trained on the internet data and what space is the uh the other documents or the other contexts coming from right and if there is no overlap or there's very little overlap probably you know you want to fine tune it even with the augmentation and everything U but I think it depends on you know how much of that data is so you know I think companies talk about their data being private but it doesn't like customer support data pretty much people are talking about similar things maybe the context is a little bit different but it's not a very different vocabulary like think about two different languages for example if I've trained a language model on German and then I'm asking it questions in Hindi for example I'm not sure that it's going to do the translation even if I were supplying stuff in Hindi to to it right I think the current models are able to do some of that at translation because there's so much diverse language and content out there but I'm sort of making this very Stark case that you know there needs to be a significant amount of overlap between what the language model is trained on and what your domain uh has data on right um and I think if there is significant overlap maybe just you know using an augmentation works for reducing hallucination and whatnot uh uh and it doesn't have to be in the same time span right I think you learn the vocabulary the vocabulary is consistent over time the facts have changed and that's why you need the augmentation but if you're speaking a different language then then you need to do something to the language model yeah it's such an interesting topic I use I remember the wave where it was like uh bioert legal Bert and like domain Bert yeah I guess like for me I'm curious if like when especially as they scale the context l to like 100k if maybe uh you could give it the whole history like you know there are different kinds of cells like immune cells there are T cells and it's like the whole description of it and if you can just pack it in the general Reasoner can learn the language from the context but I think it's interesting it's an interesting idea I think some of the papers that I have seen on this say that when you have very big token uh lens then it relies more on things at the beginning and things at the end and sort of ignore stuff in between uh we haven't done too many exp like we just started trying out clot so uh in a big way to sort of you know see if this is actually true or not so we don't know yet and answer to that question uh but I think I I'll sort of take one step back I would say these are very complicated models and they're not well characterized right I think of as a computer scientist you think about characterizations right of system uh these systems are very ill understood like if you think about a support Vector machine logistic aggression decision tree we understand them well we understand the mathematical properties of those things uh given how complex like neural networks always has has this problem right forget about deep learning or even L language models even simple neural networks we have not been able to characterize them very well and so we don't know what the properties are of these models why like why does it forget the cont in between we don't know and if we don't know then how do we fix it I think it's going to be a challenge right and maybe we add some hacks to actually solve the problem which makes something else you know these are very complex systems right you change something here the whole network sort of coll can collapse potentially right so I think I feel you know as a scientist this is a great time to be alive to just characterize how these systems work uh but as a practitioner I think you know we just trying to learn how to make things do the you know learn llms do the things that we need them to do right so but it's it's I would say it's a fascinating time to be building in this space yeah definitely well vibs thank you so much for joining the wva podcast it was so interesting learning about these three pillars around knowledge based skills and channels I love that abstraction as well as how you separate assistance and agents this description of the hierarchy of assistant you could have and I was so impressed by looking through the no Plus platform I think it's just one of best uis of how you've captured all these different assistants so thank you so much for joining the podcast I love this conversation and learned so much thank you so much Conor thanks for having me on this podcast it was wonderful talking to you ", "type": "Video", "name": "Vibs Abhishek on Alltius AI - Weaviate Podcast #71!", "path": "", "link": "https://www.youtube.com/watch?v=4aTkpjejaUs", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}