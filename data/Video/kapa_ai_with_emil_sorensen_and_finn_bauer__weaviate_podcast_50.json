{"text": "Hey everyone, thank you so much for watching the 50th (!!!) Weaviate Podcast with Emil Sorensen and Finn Bauer from Kapa AI! \nhey everyone thank you so much for watching the wevia podcast we have a super exciting episode I'm so excited to welcome email and Finn they're building Kappa AI Kappa AI is uh it's a super impressive kind of like large language model with Vector database for your own uh code documentation blog post this kind of thing and personally it saved my life we have it in our wevia chat and I needed to say Hey how do I insert objects one by one with JavaScript I ask kapha it's like I got you correct answer so I am so impressed with this system and I'm so excited to be talking to email and Finn to hear about like how you to think about the the evolution of this space so firstly guys thank you so much for joining the podcast no thank you awesome thanks for inviting us awesome so could we kick this off maybe just like with the origin story of capital like how you how you came to be working on this yeah for sure happy to give a quick rundown um maybe by brief intro I'm Emil Finn over here we go back about six seven years met while uh while doing University initially studying Finance kind of looked at each other and said like ah you know maybe this world is not for us then actually jump ship and and went to the post-grad and computer science and kind of did a lot of tinkering back in the day um you know tried to get a lot of little side hustles off the ground and so on um eventually that wasn't the time sort of six years ago after the post grad so instead went to our separate ways um I was uh I was a consultant for a couple years Finn worked as a software engineer and then I think last year we kind of started looking at each other and like man there's there's a lot of stuff happening in the space you know we were trying to fine-tune our own GPT true models when that was the thing back in the day and both of us you know have a lot of friends that run like developer facing companies so we just kind of kind of like naturally began narrowing in on on this this this problem space which for us now is really just helping developers use um use uh software products um and I guess yeah yeah amazing and I I really want to get more I think you'll have such an interesting perspective on this kind of fine-tuning models and how people think about having their own custom large language models is a topic I really want to explore but firstly I have this question about you know when you have a new company like weavier for example that you've gotten to Kappa how are you thinking about like how fast can I get the documentation code like how do I get all this into these systems as fast as possible sort of right so maybe at the moment it takes us about like a day or two depending on how much like the big the backlog is um so we've found so far like because this tag is super new and everybody is still a little bit like uncertain what it can do and how it works you kind of need like a very white glove approach to this so like um we're not really doing this like on a self-serve basis at the moment so we like we just ask the companies okay what are your relevant sources um like what should your Bot know about and then we like we kind of do it for them at the moment and like hold their hand onto this in production um just because it's so new and nobody really knows how to work like how well it works what it can do what it can do um and we can we can talk about like how we ingest it if that's interesting yeah I'd love to dive into that topic I mean I can maybe speak a little bit to I've been trying to take this podcast and put it into eviate and so I you know I put it into whisper and I still I think white glove I interpret is meaning like manual cleaning still needed so like I get the transcripts and I you know specific terms like if we start talking about like the Laura paper the low rank adaptation like that particular word it'll probably translate Laura to like lore l-a-u-r-a the name rather than l-o-r-a like so these little things is what I'm finding so um yeah data ingestion this is the I'm so I really want to pick your brain about this like how much manual effort is still needed are you excited about things like you know unstructured the Lang chain kind of PDF parsing thing like how you think about data ingestion to these kind of systems um I like I think sort of all General experience so far has been like it would be great if you wouldn't have to do sort of any cleaning on the data like if you could just put a full Self Service that works automatically and you don't have to touch it at all and it's very simple that'd be great and we've seen like quite a few people or like projects doing that um it tends to not work that well because you get a ton of things in there like that mess with the performance and down the line um so you do have to like like if you if you can get to a very clean state in the beginning it'll work so much better in the end um that's yeah especially also how we've implemented is like try and get to the cleanest version correct this version of the data before you ingest it into the system like this yeah so maybe graduating from once we get the text from whether it's like a web scrape and we got to kind of get the HTML tags out or you know PDF parsing we might have some funky way it read like a two column layout or something like that so once you have the text how are you thinking about chunking it into Atomic units and kind of having a symbolic structure around it or maybe those are two separate questions maybe chunking first um maybe like even like one step before that would be like how do you even get to like a clean representation because so for us like um yeah like every company has like a lot of different sources that you would input it's not usually just a docs you have like different pages for the tour the dogs tutorials like discourse forums GitHub issues um and they tend to all have like different structures for every company so you do need some type of like General approach to doing this so which for us means that the like web scraping at the moment like having like a framework that can generally handle all kinds of pages um so that's sort of the first part uh to be able to even get everything and then the second step you can think about like cleaning chunking um yeah yeah I'm really I think a lot about like sort of mimicking the system you've built with our weeviate content and how that would look I think about like just like the symbolic filters where I'd have like where title equals blog Play post title and then I like you know whatever it is and then I keep that with each of the chunks um yeah so that yeah it's really interesting just like how you're parsing out the symbolic structure of someone's documentation I imagine I really want to dive into how you do you mentioned the Discord can you tell me a little more about how you mine out like Discord slack conversations and add it to this kind of retrieval system yep happy to maybe maybe jump in here as well I think maybe as a sort of a general theme what what Finn and I are slowly discovering as we're building out Kappa is that and this maybe speaks to the broader nature of like you know where this you know types of applications like composite in the ecosystem right is that there's so many of these like you know bits of Niche information that are specific to a vertical we're very much in the developer facing space right so that means like going out and talking to 10 of your customers to understand well how do you view your discourse Forum right okay you know you have a certain number of people that reply that means okay when it's someone from the moderator team we should probably ingest that into Kappa doing just a full thread or do we just ingest the question answer pair and the only reason like the only way you would ever discover this is by going out and having a lot of these conversations um and that's what we've done for by now I think a whole breadth of different technical documentation sources be that slack or discourse or GitHub issues etc etc etc or maybe for slack specifically like every company tends to have a slightly different opinion on how their slack should be ingested um so we make like with kind of a bit of a configurable framework to tailor it to like each company and then the case of slack like what's good is that everything is already in threats so everybody every time somebody asks a question somebody replied creates a thread um and then for example what we do like one company might do like okay take all the threats with the last year um only take threats where someone from a sort of whitelisted amount of people replied or like from a team or specific people and drop all the other messages and then take that sort of essay question answered here that's sort of like you need to get to some sort of non-noisy representation where you can be sort of short and nobody has said something wrong because you might have a threat where somebody asks a question then there's like two community members that say something wrong and at the end you have someone from the team that actually gives the correct answer and if you take all of that you're just going to confuse your Bot so you need to again back to like cleaning and getting like a non-noisier representation yeah that's such an interesting nugget of how you do this I mean I I kind of had two ideas like thinking firstly maybe like you know whether it's just a classifier to run cheaply or it's a language model prompted like this to say like you know is do you think the original answer is asker is happy with this answer and doing some filtering like that the whitelist thing very interesting um the academic in me who you know I love the idea of kind of benchmarking these systems so that you can sort of ablate the models and I can't wait to talk to you about the models but like do you do you maybe like take 50 of these threads and then have some system of you know replacing maybe the whitelisted person with the language model and then from there you get some sense of how well Kappa works like is there some kind of like like I guess I'm trying to get out is have you have you thought about this kind of like you know using like these like these abstractive question answering extracted summarization metrics like Rouge blue scores on these kind of bootstrapped examples of conversation threads where say I'm on the white list and you replace Connor with you know with Kappa and then you say here is the comparison of the final answer so so far we've just used the data as is um to keep its like so we can so you can go to the customer and tell them like hey this is exactly like what's going to be in there like there's not going to be anything generated you don't control so the company actually knows okay we only have this set of information because they tend to get a little like skittish if you start to generate some things and so on like it kind of slips the control a little bit um and you might introduce some sort of wrong things and some like something hallucinated that shouldn't be there so the moment it's just like I'm sure some of these things would work quite well um at the moment we're not doing it one thing we're probably going to do is like generate a title uh where you could go like hey here was it because slack for it doesn't have like a somewhere a summary title but you could generate one of the conversation saying like Okay this was what this was about um that would probably help um yeah I just saw uh Jerry and llama index tweeting about their uh like summary index where you summarize long things and yeah yeah it's such a cool idea and yeah the really cool first like I guess before transitioning to the hallucination problem which I think is such an open such a big topic for this application I I guess the question I really want to ask and get your you know experience of building this thing out is like you know how how do you tell leviate or whoever like how well Kappa is how good Kappa is like yeah yeah no it's a super good question I mean the short answer is you know the the potential accuracy threshold of an application like Kappa really also varies across projects right you know you might be lucky and and be dealing with you know a company like VBA that has fantastic dogs you know like a deep set of technical tutorials threads oh seriously right um and and for which you can actually get a bot that can be quite performant um and then there are other times where you know the company think they might have sort of great dogs but then actually when it comes down to it there's a lot of sort of holes so so the short answer is it's really just again having conversations with with the customers trying to understand like you know average accuracy metrics based on sort of thumbs up thumbs down that helps kind of proxy that um and then also just sort of setting expectations for where the tech is today yeah and I know from my experience using Kappa I've seen you know the thumbs up thumbs down and that that it will be well I I want to park that because I want to come back into this idea of owning your own language model and how you could bootstrap data that way but let's hit the hallucinations thing because you know naturally I think the the biggest thing is I yeah yeah how are you think about hallucinations right now I mean I think that the short answer is it's probably the thing we've historically thought the most about um you know I can I can name countless of experiments um conversations with academics like scanning through the latest archive papers like trying to figure out like how can we bring down hallucinations and there's probably not an approach we haven't tried but to be honest ask these models are getting more powerful for example you know upgrading from from our from our case we run an open AI as our last sort of off-the-shelf generation model just making that bump up from uh davinci03 to gpt4 already helps address a lot of that and then in addition you know we think a lot about sort of nudging Kappa to always produce an uncertain answer as opposed to a a sort of a potentially creative answer um which means today Hallucination is not really a problem we deal with in production fascinating so so the current state of just kind of prompting it like please ground your answer in this context and then just please don't make anything up in all capital letters or something like that just that alone seems to be strong with the strong morals but so I guess my question with with that well it comes into the transitioning topic of I'm very curious about how you think about the large language models because um you know so the running gbt for every query I imagine is kind of expensive so are you are you interested in the open source models like you know llama gbt for all like the Mosaic MPT these kind of Trends and then but then did they not have this hallucin like then you have to rethink about hallucination I mean the short answer here is incredibly excited about the potential um I mean for one from a cost perspective and potentially latency down the line right um but also very excited because I mean one of the common concerns right you hear when deploying these these models in production is you know ownership of data and transferring and so on so I would love to be able to offer Kappa on-prem um as a service which would be possible down the line with something like an open source llama Etc um but right now honestly it's not something we spend a lot of time thinking about just because the I mean the whole focus is really you know a slower but better answer is is usually preferred and um you're just you're very right like these it's it's quite expensive at this point which means from an economic perspective like some much larger context Windows aren't necessarily feasible for this use case because you have to look at it okay like how much is one question answered or one generation worth enter like in sort of the economic terms for like your customer and can I even use something like a 32k like context window like does it make sense um and for us like at the moment it's like cost is fine but like for example using like 30k context window will probably wouldn't be we couldn't sell that most likely fascinating I mean I think um yeah so I think the gbt4 32k and then I suspect that the anthropic uh 100K that both work with like this API thing I suspect that yes both those are quite expensive but I'm so excited about this mosaics uh 65 billion parameter oh I think it's 7 billion sorry 65 000 input window is um is something you can run in Transformers I think it's I think basically the way they're doing this is they call it Alibi attention and it's kind of like the newest sparse attention variant where sparse attention is you never have that like dense matrix multiplication where you multiply 65 000 by 65 you know and create these gigantic matrices that obviously can't fit on a single GPU and the whole problem but with with those kind of models it makes me think that maybe they can uh cheaply run it yeah I don't know too much about it but I'm the question I really want to ask you about these long input length models is like with retrieval and you know obviously you're working on aggregating all these information sources for companies like Wi-Fi documentation code based blog posts code examples this podcast maybe right so like do you think packing the prompt as densely as possible would would be the future of that I mean maybe at some point but right now for sure not I mean you really like if like what you want to get is like the smallest amount of relevant context to answer the question like that's ideal right so that's where you get the least hallucination the most crisp answer um if you I mean you can fit a lot more into the context movement we're doing right now that wouldn't necessarily it will make more expensive and it wouldn't necessarily improve the answer or make it worse um maybe at some point it's so good but right like right now in a super future I think you should definitely want this retrieval step um to give you like the relevant context I mean the more irrelevant things you add the less you the worse your answer gets hey everyone sorry we had a little bit of a recording a little bit of a connection issue but we're back uh so when we left off we were talking about these kind of long input length models and the trends in that I think it actually would be great to kind of pivot into this next conversation which is the trends in fine-tuning your own large language model which is we're seeing with Laura l-o-r-a the low rank adaptation it's it's like a trend in the sparse fine tuning where now you can fine tune like a seven you know a billion parameter model for like 20 bucks and or I don't know about 20 I think I've seen like a hundred dollars is arbitrarily obviously depends on the size of the model and how long you're gonna train it for but yeah so what how do you think this will change everything fine-tuning language models are companies going to want to own their own language model I I think and I'm just speaking anecdotally here also with conversations with other Founders in the space I think fine tuning has a lot of promise and where fine tuning can really deliver value to folks in the real world that's very much stylistically in terms of the the types of answers it generates but if you're thinking about fine-tuning from a like accurate knowledge retrieval step um I I think you'd still run into a ton of hallucination problems at least anecdotally speaking you know when we've done countless of fine-tuning experiments and and it's it's really you've just run into hallucination as opposed to having um like I think I think Sam Altman had a tweet a couple of months ago saying like like talking about some new opening eye pricing and history was something like you know I think most people would be happy to pay like two dollars for a thousand tokens but a thousand really high quality tokens and I still think that still holds true um also from I would say our perspective meaning I'd much rather have a a great off-the-shelf you know high accuracy High reasonability off-the-shelf model maybe just to add to that like um from what we've seen like with we had two customers so far that uh like in the past like sort of before like like sort of last year took a bunch of their support tickets so like some of them had like 3 000 emails for example going back and forth and find you in their own like I guess like what was available like I forget what exactly they find you but they had sort of what Kappa is now themselves but purely find you without any retrieval and use that to like generate answers to support tickets um and now I've moved off that and just use us um and yeah like we we basically took the same data that you used to fine tune put it into their bot um and it like just anecdotally to them it works better this way um yeah fascinating I think working on a vector database I think you might expect that my reaction to this news is just like you know thumbs up all the way yeah but I don't think these two classes of algorithms are like approaches have to conflict like this like I think you can still have a retrieval augmented generation with a fine-tuned model and yeah you still have the benefit of It kind of can cited sources a little bit you can update the information based it so I don't I would start by saying I don't see these as two like different things like if the Laura algorithm is successful that means and the vector databases and all that kind of thing but yeah I think something fascinating about it is like there's this like robustness like this General language understanding that comes from these large models with how they've been pre-trained and I do think like if you fine tune it you might lose that kind of robust like you as you mentioned like the hallucination problem becomes more so yeah and we've seen that like crazy with the there's a lot of experiments like clip there's this paper called wiseft where they show that like you know if you just fine-tune clip you lose like the robustness of it and it's really interesting yeah I mean um the idea of fine-tuning on support tickets I mean have you ever like the when I've talked to like you know I've talked to Jonathan Franco in the podcast about Mosaic ml's philosophy with owning your own language model and it's like have you seen someone who has just so much Text data that you would pre-train the language model on it and so it's not even like fine-tuning it's like the the language modeling objective is done with this data set maybe the biggest we've seen so far like you never really get a like passed again two three thousand pages of text sort of like Loosely speaking um nobody has like an insane amount of valuable data for like their specific I mean like the best case scenario would be somebody has like really really extensive talks and like apri reference that has a ton of like good GitHub issues in very like big discourse forum and then maybe like their past like conversations but even with like if that all is really big you don't really get past like 3 000 pages of text or something like that yeah that's really fascinating because it makes me think like good like good documentation should be sort of compact I suppose like right like it's also I mean it's so interesting like should companies be trying to create as much digital content as possible in this world or should they be like have a really refined compact documentation or and there or are there like two things you mix all right like you have the source of Truth highly curated and then like you know this podcast how we just fire it on and just start talking and creating content like two kinds of content I mean for some like like we sometimes debate this with the blogs of companies because some of these are like technical tutorials almost how to do something and that's relevant probably to developers but also some companies write a lot of blogs for sort of CEO optimization uh sort of some general news which is like Loosely related to them and it's that's not necessarily super valuable for you like the QA board um it does have to has to be like really really high quality text I mean one interesting thing regarding that as well is that so you will have a lot of relevant okay companies will have like a lot of tech and documentation around their product but to understand that you still need a lot of other like more fundamental understanding of things like programming and like other Frameworks and operating systems and if you take all that away like then their technical augmentation really doesn't make much sense so you need some like model that understands sort of all the context beneath what their product is about fascinating I mean especially it's like I think there are two things so this is like the gbt4 knows already knows Python and JavaScript so you don't need to go get the python documentation and retrieve from that but then it's like say like with weaviate like if we integrate like we integrate with Lang chain and then like Lang chain is continually evolving such that the large language models can't keep up so it so you so have you ever thought about advising someone to grab the documentation of like another library that they integrate with closely we have like we have a few of those customers like um react this has been like kind of a good example of that it's a lot of framework front-end Frameworks a lot of like things that interact with react for some of them we've put it in like as well um there's a little bit the like bit of a business question of well do I want my bot to know like that well about this other framework because data documentation will contain also some like oh how real how great is react on its own and things like this and it might lead a little bit away from their product so it's bit of an like it's kind of like a case-by-case decision if you put it in or not if it's like valuable um I think so far it's been kind of yeah successful for a few people yeah fascinating I think yeah I think that's something that even humans like as we create content we need to think about it's not even just the language models like am I just advising you to use Lang chain instead of the leviate generate module this is a little thing to hit that but um okay so the next topic I really want to ask you about is alleviate we're curious about this concept called that we're calling generative feedback loops to describe when the language model transforms the data in some way like maybe you know write us some like the summarization is one example like how do you think about transforming data and then saving it to further help with the retrieval that's I think that's a really interesting area and to be honest we haven't we haven't really explored a lot of generative steps at the retrieval stage um I think it's one like I see Jerry from llama index tweeting out a bunch of stuff on this topic right so it is a really interesting thing that probably could help with with retrieval um but maybe you're just one General observation is I think when working in in this space right we're always very quick um to quickly throw like a language model on a problem like oh great you know maybe we can summarize something here maybe we could do something here and I think that the general observation is it it kind of rarely works very very very well because by every step you introduce like another GPT caller every step you introduce one more thing you have to build a ton of guardrails to ensure you have a stable system to handle edge cases Etc so it's a lot of the time when sort of thinking about you have to really zoom out and think about systems design as you're introducing one more layer of noise that being said I think this is probably a stage that is definitely worth I explore any further um yeah it's fascinating I mean I think like it's like the large language model Overkill solution is the pro is it potential like I can say just from my experience of like you know I have 477 podcast Clips in this podcast search data set that you guys will soon be a part of and yeah and you know I as I Summarize each of these clips that cost me about four dollars to do 477 so you know and it linearly scales of course like that kind of thing so so yeah like I saw I saw Jerry what and one of the newer ideas is use the large language model to truncate the search results so say you get you know 10 search results language models says only take the first three and then to the next step and I think that might be it Overkill it's like also using it for re-ranking is yeah I think we had something like that in the beginning which didn't like kept throwing out like really random things and then we got rid of it yeah yeah awesome no go ahead Connor okay I was gonna ask kind of um you know wrapping of the podcast these are this has been a great coverage of just how you think about customer uh companies getting all of the like particularly software companies getting all their content into these systems uh thoughts on the trends and longer inputs and then fine-tuning and then just generally thinking about how you think about your content all these topics I want to ask kind of an open-ended question of like what what Topics in deep learning and AI is exciting you the most right now man's like where to start right I I think you know as sort of the founder of the space or Founders in the space it's it's always a daily battle between trying to you know dig deep into the into the next new thing to read that next like paper to begin implementing like another idea to improve the system and then also just um just kind of getting back to the basics which you know in our case is talking to customers and and Building Product right so but but I mean to answer your question without answering your question here I think maybe areas where we're really paying attention to now is like well what does it mean that you have 100K context window all right what does that unlock what new ways of actually helping out your set of customers can you do with this feature right um and you know it could be a ton right you know to your point maybe it is actually worth this like cost latency trade-off to you know have a longer call a more expensive call but a more accurate one right I think that is probably the thing that's at least keeping me personally up at night right now yeah I like that point a lot too like I I was very interested in like cross encoder re-ranking so that's like re-ranking search results with it not a language like a 80 million parameter it's still slower and yes I think also in this new world you're willing to wait longer and that's a very interesting Trend uh let me ask you two startup questions so first of which is this thing you talk about talking to customers I'm just curious on generally getting people's perspectives on your work like working on a deep technical product do you think Innovation comes from you know the customers and seeing their use cases or maybe like this kind of like reading the science like you're you're either like deeply in the science or you're at like the application layer and trying to see like what's the next thing to do from those two perspectives I mean think for us because we're sort of the application at the application layer so directly like try I mean we're productizing something and putting it to use so it's less of an infrastructure product so here I think relate like I think what's been super successful versus relationship management with customers sort of just like classic like be available hold their hand talk to them often and kind of like ease them into deploying it so that's been super important to us um maybe less so than like very very deep technical Innovation um but I guess this depends on where you are in this like sort of the stack I guess if you go down one level to alleviate that's a little different um and it's sort of sorry sorry no I was just gonna say and it's it's sort of right it's one of these it's sort of one and the other it's like it's totally true with Finn is saying it's like spending time like deeply empathizing with customers to understand what problems are they actually hoping to stuff can solve like when you go past the cool Twitter demo or um or you know Loom recording or whatever but at the same time you know it is also such a new and fast moving space so you do have to constantly pay attention to like what's happening in the science you do have to like every single like you know news blurb that comes out like 100k content you do need to contextualize that in from the perspective of the customers for whom you're trying to solve a problem for fascinating I'd say maybe like to give a quick example from weviate like the new and this isn't like I'm not like revealing private information this is like something that we set up a pocket like the the group by filter that came from like a customer request as an example but then like on the other end like the research we're doing on disk a n and like you know kind of like the PQ stuff and filter disconnect if you go into the weeds of it that's not really something that a customer would ask for because they don't think about Vector indexes like to know what the like to tweet to make it different so I think that's sort of the perspective of of how I see the abstraction layers and yeah it's so super interesting um so then the second question I wanted to ask you which with kind of like a startup question you already hinted at it is like this CRM the trends in growing a community and a set of customers is like do you think about like using the large language models to manage your sort of relationships with people and it is are we seeing like a huge breakthrough in that yeah I'm happy to take this I mean I I think I'd like to thank both Finn and I are kind of by this point quite like native with like all this stuff I mean most of our coding is done either with like with Kappa dog fooding it ourselves or like c54 or you know clutter or whatever the latest model is um but but this stuff I think it's just old school relationship management installed it that kind of works um I mean don't get me wrong we're always trying like new new launches scouring product hunt but um I think yeah nothing beats sitting down getting on a zoom call and actually talking to your customers yeah I agree fully with the value in doing that I guess I just like something that I find really interesting and this is kind of like the angle that we're taking with our generative feedback loop thing is like when a new blog post comes out or like you know we V8 1.20 comes out when we have this new set of features is like going into the CRM and saying in Target and saying like you know hey you would like this because you worked on this in the language model just does all that I'm sure someone will build like an awesome like um llm Native uh CRM I'd love to use that tool yeah me too so email and Finn guys thank you so much for joining the podcast I think this is such a great coverage I mean you guys are at The Cutting Edge of putting the documentation into these systems I you know just from I've used it the kapha and it's awesome so thumbs up on how well it works and good luck with everything thanks appreciate you having us on Connor and a ton of fun ", "type": "Video", "name": "kapa_ai_with_emil_sorensen_and_finn_bauer__weaviate_podcast_50", "path": "", "link": "https://www.youtube.com/watch?v=cjAhve_DopY", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}