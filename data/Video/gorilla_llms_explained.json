{"text": "Hey everyone, thank you so much for watching this explanation of \"Gorilla: Large Language Model Connected with Massive APIs\" ... \nhey everyone thank you so much for watching we've been on YouTube I am super excited to tell you about the gorilla large language models we're about to release a ton of awesome content about what we're doing with the gorilla models a podcast with shashir and Tianjin but let's kick things off with an explanation of the paper all the experiments all the details of it but firstly let's take a step back so shortly after Chad gbt you just blew everyone's Minds with its ability to communicate and complete tasks in natural language people soon realize that you can connect these large language models like trashy BT to external tools to enhance their capabilities even further these external tools typically referenced search engines or maybe code executors or calculators like the Wolfram Alpha calculator or just you know a basic calculator as well as say how to send emails how to book calendar appointments or how to you know purchase a flight ticket connecting llms with these tools has kicked off this area of research known as agents and all sorts of exciting things with doing this so what gorilla is adding to the table is it's treating formatting these API requests as a deep learning task so deep learning research has typically been organized based on demonstrations of input output examples like text classification question answering summarization and gorilla is leading the way in treating formatting API requests as a task to fine-tune deep learning models on they have an amazing ablation showing the difference between fine tuning with and without retrieval and the rule of retrieval in this which obviously with weavate we find this super interesting and then of course they have these really interesting details around using self-instruct to generate training data to learn to write API requests and all sorts of interesting details that of course we'll dive into in this video so thank you so much for watching and let's get into it so just in case you're in a hurry and just want the general overview of gorilla here's my attempt to compress the key ideas in two minutes before diving further into the details in the rest of the video the high level motivation is to equip large language models with tools tools are described as form adding API requests so open AI funks has been a huge like related work probably the current state of the art what I think most people are using for this where openai funcs introduced here's how to present a Json that describes the name of a function a description of what it does and how to format input arguments to that function to then pass to the openai large language models and I think openai is doing a little bit of fine tuning on these jsons I'm not exactly sure but I think it's either some fine tuning of those jsons or it's just kind of done zero shot where these large language models like gbt4 they are as we'll see in the details of the paper they are really good at taking some kind of description of a function and then learning how to use it like you can do text to SQL this way all sorts of ways if you tell it here's some kind of function and how to use it the models are really good at using it so examples include say sending an email getting the current weather and then importantly this is this is kind of the big detail of how do you write API requests is how do you format the input argue arguments correctly so in this case you have you know send email to say Bob inlight body you know how are you like these two strings or you can imagine it's like uh you know maybe you have like a date when do you want to send the email so formatting these these arguments correctly is the key detail now here's the super interesting thing the open AI models they are great at this but they are massive language models like you know 200 billion parameters and so on to serve these and to do inference is expensive so if you can com the gorilla models are compressing this functionality into this seven billion parameter model by fine-tuning that llama 7B the open source large language model from meta AI they're fine-tuning that model to do this and then the difference is that this is way more economical to serve this kind of API request if you compress it from you know hundreds of billions of parameters to seven billion parameters and they'll probably you know keep compressing it as this research continues to evolve so the key idea behind how it works is that we have these examples of apis we use self-instruct where we take an example of an API and then we prompt gpt4 to say can you write an example of an instruction that would want to use this API and then that then becomes the training data to compress to train this new language model the Guerrilla language models so there's also a really interesting ablation about using retrieval in this and this is I love this part because retrieval aware training is I think one of the most exciting ideas on The Cutting Edge where you use you take the prompt and then you retrieve and then you use the retrieve context during trading not just as like a you know fit after you've trained the model you're going to be training the models with this retrieval and they do a super interesting ablation exploring the details of trading these models with and without retrieval or at inference time than with and without retrieval so a lot of interesting stuff there so particularly what gorilla is is Gorilla this first research paper is about formatting requests to call Deep learning pre-trained models so I taking models from the hugging face model Hub tensor Hub and pytorch's torch Hub we instantiate these models based on some natural language instruction so if you have the natural language instruction I want to classify the gorillas in this picture then the gorilla large language model would write the python code to extension instantiate that like an image classifier from hugging face and maybe preferably one that's been trained on like animals and this kind of thing and so that's kind of the details is knowing which path to like some of the kind of input arguments that are required from this is mostly kind of which path to put into the argument but also sometimes say you know you have like the resnet object detectors they have like these different uh feature extractor backbones so there are some details to how you kind of format the the requests which we'll see later on in the videos we look at like the abstract syntax tree and all this kind of stuff but so this is kind of the first work of gorilla is you know learning to call model inferences and this is there's another paper called hugging GPT which is like this where you think of these pre-trained models as kind of like the API zoo and the collection of tools that the large language model can use is other models specialized to some particular thing like say you know gbt4 calls the dolly image generation model and orchestrates all this kind of stuff but the authors are collecting this API Zoo data set on their GitHub and I think this is extremely powerful the authors describe supporting web scale collection of potentially millions of changing apis it requires rethinking our approach to how we integrate tools it is no longer possible to describe the full set of apis in a single context many of the apis will have overlapping functionality with Nuance limitations and constraints so they're building up this data set that I think will just be super powerful I think it'll be insanely exciting to see how this plays out for example we're looking into adding our weeviate apis to this API Zoo data set to so to give you a quick sense of this before you know we have more content so I'm not going to dive into this too much yet I'll talk a little more at the end with some takeaways about how this you know how I think this connects to the weeviate apis but for example if you want to use the cohere re-ranker you know you have this graphql syntax which near text is how you use a vector search and we V8 limit is how you limit how many search results you get back and then you have this re-ranked syntax so teaching the large language models how to format requests like this yeah I think it's going to be a game changer for bb8 and so it's also exciting so in conclusion here are some of the key ideas so firstly API syntax following is a deep learning task for a smaller specialized large language model the use of retrieval aware training I think this is the first paper I've seen that's really doing this in an exciting way and that's where you're integrating retrieval into the training not in kind of like the end-to-end differentiable sense where gradients from the LM go back to the encoder but kind of just using this retrieval to supplement the inputs and help reduce that hallucination by turning the task basically to copying where you just have to kind of like look at the input window rather than like have all this information like in your brain so to say if you're a nola but and so then I think this API bench data set and understanding the potential of this API Zoo data set as this continues to evolve is so interesting and then understanding the details of how they evaluate these models okay so the first step to understand the experiments of the paper is to understand the new API bench data set the authors publish and create for these experiments and how how they use self-instruct prompting to generate synthetic training data to fine-tune these models for Tool use so starting off collecting a data set of model API calls they're taking models from the pytorch lets you upload models to the torch Hub tensorflow also does this with the tensor Hub and then of course there's hugging face the hugging face platform hosts and serves about 203 681 models so you know it's it's amazing this model collection that hugging face has created and it's really interesting to think of what will become of that this particular research research like hugging GPT treats these pre-trained models like they themselves are the tools that uh you know some kind of orchestrating model would want to call like say you know you call the image generation maybe you call the image segmentation model you call the you know the natural language inference the question answering like kind of the creativity of thinking about these tests and then you know because so here's kind of stepping into the details of all these models that it exists they filter these models out by picking the top 20 models from each domain and I think I should have bolded domain domain's a pretty interesting concept because on one level you could have domain in this case they're using it to mean what I would refer to as modality or they have seven domains in multimodal which means you know maybe it's like the clip model that you use text to search for images or you know this kind of thing they have eight in computer vision 12 in natural language processing five and audio two and tabular data and two and reinforcement learning so you know that kind of idea of like you you have like a podcast MP3 file and you grab the you grab the whisper model and then you put it into the whisper then you say grab like a text vectorization model to put it into leviate or something or you just I don't know summarization right out of whatever came out of the whisper model but this kind of like combining models I think is quite an interesting idea but to get a sense these are kind of like the models that they take to combine into this data set so each entry in the data set is an instruction API reference pair so in this case an API reference would be like um you know Auto model Dot from pre-training the hugging face sense and then like the model path and if it has additional arguments like what that entails so we have 925 models from hugging face 626 from tensorflow Hub and then 95 from torch Hub and the 1 645 API calls are organized into this Json that has these keys so the key so you know the keys in the Json are the domain so you know and domain would be like computer vision so like domain computer vision framework hug and face functionality like what it does the name of the API the API call an example of it the arguments it takes maybe the environment requirements like if you need like a GPU to run it or example code which I think is similar to API call we'll take a look at this in one second uh the performance this is a really interesting detail with respect to how they evaluate API calls with constraints because uh you know say you want a 10 million parameter image classifier so you can run it on your phone but then you also want it to have like at least 75 percent image in that top one accuracy so you you kind of trade this off with the constraints and that's a super interesting detail of this kind of paper and research and then descriptions like a natural description of what this is so let's take a look at an example of how these uh how this Json is formatted okay so I'm not completely sure I have all the details correct here but here is a basic overview so I went to the gorilla repository where you can see uh the API bench and then hugging face.train so I grabbed one of these and then dropped it into the three back ticks Json to make it look a little nicer uh so anyway so you have these keys that describe each of the apis so similar to the you know the open AI funks he sees like how it's what the arguments are to make the API call and um you know like how to how to call it so in this case instruction our customer is a robotic manufacturer of cleaning devices they ask for help on solving users questions okay probably okay so question answering so so here's a sense of it I'm not sure I've completely understand the details yet but basically thinking about how you want to organize these API calls into Json such that you can you know either prompt it to come up with the instruction of when you would want to use this or a description of what it is this is again that performance thing is so it gets like you know 78.4 I think this is recall within with the embeddings encodone model so you know on these different question answering data sets like trivia QA or Trek or web questions Squad so this is kind of a sense of what what it is organizing these apis in this kind of Json such that you can either self-instruct the gbt4 model to produce a natural language command like this of of when you would want to use it as well as like then this being kind of something you would retrieve to put in the input to then format the API call and all this kind of stuff so the author State we employed gbt4 to generate synthetic instruction data we provided three in-context examples along with a reference API documentation and task the model with generating real world use cases that call upon the API so you have something like as shown previously the DPR question encoder and the model would have to the gbt4 model produces some kind of natural language command of when someone would want to use this kind of thing like I have you know Airline manuals or whatever and I need to answer questions about it so then it knows like okay I translate that natural command into the API request for this particular model or as mentioned like the I wanted to detect the gorillas in this image to rep to then be mapping to that object detection model API so the authors constructed six instruction API pairs for each of the three model hubs and these 18 points are the only hand generated or like handcrafted data and this is a huge paradigm shift for deep learning research which this self-instruct thing this generative data augmentation synthetic data cannot be understated enough it most of these deep learning papers in the past the authors have been like hey we've labeled like a hundred thousand examples and this kind of like collection of this data set has been such a integral part for doing anything in deep learning and now by using they they could they collect like 18 examples again of how to write an instruction API pair then you just give that to gbt4 and it just generates the training data to then fine-tune gorilla taking as input the synthetic instruction and then the API reference and then the output is the correct API call so a high level overview of this full flow we've curated 1 645 API calls from tensor Hub tensorflow torch Hub and hugging face so then we build up this API data database of this which we use through the API reference retrieval later on but then from these examples we have the self-instruct we create 10 instructions per API so now we have a data set of 16 450 you know inputs being the natural language command and then the API is the retrieved API or maybe we use the Oracle context or we use the exact API in the input and so then these are used to Output the correct API call for calling this particular model that was used to create one of the 10 synthetic instructions per the API call so then we build up this API database so so again one of the instructions could be I want to see some cats uh dancing in celebration so this is like a call to the stable diffusion model so the input then becomes uh the task generate image from text um I'm not exactly sure about this detail I think that the it depends on how you want to format this for sure but I think one way of doing it would be input and then hashtag hashtag instruction I want to see some cast dancing and celebration and then reference API would be the retrieved API when you turn the say it's Vector search you turn this into a vector and then retrieve the nearest neighbor vector and then that becomes the reference API or the you know you can see already how like bm25 search would be terrible for this how you I want to see some cast Dancing in celebration you don't have any good keywords to return like an API for that so another great case of a vector search example but so then you format this input this input goes to gorilla and gorilla outputs the call for this model stable diffusion pipeline Dot from pre-train stability AI slash stable diffusion Dash you know 2-1 and then you get the generated image of the cats so before continuing on with the experimental details of the paper I think it's worth checking in quickly with our friends at hugging face and looking at this amazing repository of models they have so the paper it claims 203 000 I imagine this paper isn't more than a couple months old and we already now have nearly 300 000 models in the hug and face model Hub so you know we have say this stable diffusion model we have of course the Llama that's like the most famous model in the world right now I think this new Desi coder I've seen this is like a model that's been you know trained for code so you see how you kind of have like these different kinds of models like this you know Med llama 2 is probably like a llama tube fine-tuned on medical information or let's see what else we can find yeah I mean the stable diffusion ones these are like the generative image models I do think kind of generative image models in large language models those two kind of categories of models dominate most of the you know public discussion about these models but anyways you can see these kind of tags like you see text classification table question answering that's a pretty unique one like if you need to answer questions about a CSV table you might want one of these models uh you know let's see what else we have in audio text-to-speech that's a really cool one so yeah you see how um you have these different domains multimodal computer vision natural language processing it's crazy like the kind of Robotics taking these models that do like continuous control but yeah I just thought it'd be interesting maybe to just take a look at understanding further that there are that what gorilla is doing is you're training a language model to you know form to instantiate one of these particular models whatever is best suited for this task so like if you say I need to generate a video of cats dancing then I guess this text to video might be the way to do it maybe you want to decompose that too so like you know first writing a story about the movie and then doing it frame by I don't know but like you can imagine just this kind of repository is this the best way of thinking about the collection of apis maybe yeah it's definitely a pretty interesting thing okay now I like everything about this paper but this is my favorite detail so training gorilla ablating training with and without retrieval so quickly retrieval augments a generation most of the way that people are using it right now is you train the language model without retrieval and then at inference time you give it retrieval but what they're going to be doing is they're going to be training with retrieval as well and really collecting the experimental data on you know is this worth doing what's the state of this so to kick things off uh they're so they're going to be training with and without retrieval as well as ablating at inference time with and without retrieval so inference just refers to making predictions with a trained model if that wasn't clear so you know when they say zero shot they mean it has not been trained with retrieval and retrieval means trained with retrieval so they're going to be training llama 7B llama 7B a large language model by meta and the the finest open source model to date so this is how I understand how they do the training and I hope this is correct but maybe if someone else has an interpretation of this it would be you know if you could leave it in the comments and maybe we could see if that's more accurate but I think basically what they do with standard instruction fine-tuning is that you have the ground Truth uh API call to generate and so you force the model to you know so the model with its first token generated it might not be correct but you still can like force it to do those decodings and then you would multiply those probabilities out and then you have this proximal policy optimization where you just do like plus one minus one did I like it did I not like it and then you send that reward signal back through the probabilities of the entire sequence that generated that was the probabilities put to the ground truth API call so that's how I think they do this maybe another way to do it would be just kind of the standard language modeling loss where you just you know language model each of the tokens of the ground truth API call you could also maybe uh generate an API call try to execute it and then if it executes plus one reward if it doesn't execute minus do less of that so that this is how I think it works is they force the generations to follow the API call I'm not exactly sure what the standard instruction fine-tuning is but I think like maybe I'm getting off topic but like I think the way that hugging face has abstracted training models I don't know if it's really worth most of us getting into unless unless this is your thing unless you're like training models all the time but for you know our experiments with UVA gorilla it hasn't been a pain to just uh do whatever hugging face offers off the shelf but anyways so here's how retrieval augmented generation during training works so you have the natural language instruction like please classify if this image contains a dog I want to see cats dancing in celebration and then not just that as the input you add use this API documentation for reference and then the first retrieve search result so this is what we do all the time at inference time these days with retrieve augments of generation but what if we did this during training as well and what's so interesting about that is then you're training the model with gradients to read that API request which is super impactful for updating documentation you know like the weviate apis they're not just going to look like this forever or like you know however llama index all this stuff all these software tools they evolve over time and so you need to learn how to do the new API requests so having it be trained to read the request is seems like a huge unlock for this in keeping the models fresh with new information as well as generally it just makes the task a ton easier if you see the it basically the answer in the input so this is what they mean by doing this during training not just at inference time so the key benefits the other state which I think you know agrees with what everyone says that there's maybe one fourth thing I would add to this but this makes the llm adapt to test time changes in API documentation it improves the performance from in context learning so you know it's easier to learn this task it can transfer learn better because it has some representation of all the tote of like the you know the API reference so it learns better and then also it reduces the hallucination error the hallucination error is a huge one obviously for API syntax because you know it has to be correct you can't like hallucinate an API request or it won't do anything productive at all but generally this retrieval aware training I think has a huge opportunity to reduce hallucinations so the fourth thing I would add to this is when you decompose the retrieval from the reasoning you can also then have a smaller reasoning model so you don't you know this is like the atlas paper from meta which is saying that you know when you decouple retrieval and reasoning you these uh you could have like a 13 billion parameter reading model that performs just as well as a 300 billion parameter like fully end-to-end llm so this kind of retrieval also is economical in the llm you need because the task is just way easier it's just about like reasoning in the input not about like having remembered everything in the world okay before we see some data tables we need to talk a little bit about how we're going to be evaluating these models so again what we're evaluating these models to do is to select the right model the right model path and sometimes there are optional arguments as well to the model so these are examples of incorrect Generations so in this case uh this the gpt4 model is hallucinating extra arguments to the function like you know this ASR parameter in string or Source equals local the anthropic cloud model is you know instead of doing torch.hub.load is doing torch audio dot pipelines you know dot wave2vec ASR pipeline so it's like hallucinating or it's calling the wrong library to do this whereas so we wanted to do this particular kind of Syntax for loading this model so what the authors propose doing is decomposing the API call into an abstract syntax tree and then matching that tree with the uh with the ground truth or say you know you got some of it right so this is like if you made it to here right like you got the torch.hub.load maybe you would penalize that less something like this so yeah I did I didn't go too into this detail but I think it's quite interesting how you're going to be evaluating these models like maybe the dumb way to do it is just to do to keep the perplexity like the perplexity is the common lawsuits and language modeling where you're just multiplying out the probabilities the language model puts to the ground true tokens and you know that's typically what you do with language modeling with this uh you know the the probably the most interesting thing about this is if you uh did you know if instead of dense net 121 you did densnet 161 or 200 one it would still execute so you can't just purely evaluate this based on did the API call execute you also need to kind of see if it gave the particular model which will make sense too as we look at the ablation on uh API calls with constraints where you particularly want like you know eight eighty percent imagenet accuracy but less than 20 million parameters okay let's get into some of the results so the first thing to note before we dive into the results is they're going to be exploring again training gorilla without retrieval as well as training Guerrilla with retrieval so starting with training gorilla without retrieval just purely fine-tuning a language model to go from natural language instruction to API call the API reference is only used during inference not during training when when they do bring it out for to see what happens to Performance so the takeaway fine-tuned gorilla gets CDR performance 20 better than gbt4 nearly 11 better than cha gbt and then 83 better than the other open source models so you know you see that obviously obviously there's a gap between gbt4 Chad gbt and then most of the open source models so but this take away the fact that you can fine-tune gorilla to do a better job of these API requests then gbt4 zero shot or Chad GP zero shot this suggests quantitatively that fine tuning is better than just retrieval augmentation so you know what they mean is they you know so neither model has been trained with the API reference but they do also show what happens when you put the API reference in the input at test time not during training time and they do find that fine-tuning models performs better than just purely augmenting gbt4 with the ape guy reference so another thing before we look at the data table is that you know this is a warning for brag implementations they find that if you give it a non-optimal I mean this is obviously common sense like if you give it the incorrect API reference it will misguide the model of course so I don't know if that's like exactly like a mind-blowing takeaway but I mean it it of course is because when you're thinking about retrieval it really like that's why I think this cohere re-rankers thing is so exciting because for a lot of these retrieval augmented generation applications I don't think you're going to be latency bound because you know what's the point of making it super fast but not accurate prediction so kind of these re-ranking models I think become more valuable because it's worth taking the time to make sure you got the right API reference before you then do the generation so let's take a look at the data table so okay so obviously this is a ton to look at but uh so so gorilla of course is you know the model we're talking about and so gorilla zero so zero shot means all these models are just being evaluated with the natural language command I want to see some cats dancing right they just get that as input and they just have to write the API request to the hug and face model so they measure different kinds of uh you know the overall which is that abstract syntax tree matching thing the hallucinations when it um you know invokes the like just generates an API call that doesn't exist or errors I think is just when it makes one of the things right I'm not exactly sure the difference but I think probably we can just look at this overall metric so so here's the thing about uh be careful with the retriever so bm25 if you get the wrong context it goes from 59 to 40. as we saw earlier I think it makes a lot of sense why bm25 retrieval is hard for this kind of case because if you say I want to see some cats dancing you don't have any keywords that are going to match the you know the hug and face API documentation so I do think that result makes a lot of sense uh gbt index that being that was the earlier name of llama index uh returning so a site boost using Vector embeddings but definitely a lot to this and then Oracle Oracle is the you know the perfect API reference to have so you can see how much it boosts by having the perfect API reference so maybe in addition to seeing our gorilla model so we see gbt4 Zero shot doesn't perform so well but then when gbt4 has the you know 38 to 66 again you know that's the retrieval augmented generation thing is when you give it the context the large language models perform way better but then you can also see the detriment by not having it be the right context so here's another kind of view of what these models ended up looking like you know with the Oracle retrieval so just zero shot where you've just fine-tuned gorilla compared to gbt 3.5 and GT4 zero shot gorilla model performs way better but then with the Oracle retriever you push these super capable language models closer llama just with retrieval doesn't seem to be able to you know get up up into the left like these other models do so that's kind of interesting as well you see the input impact of uh not a perfect Retriever and so on so so this is again this is fine-tuning Gorilla without retrieval okay so now let's get into the results of retrieval aware training fine tuning with retrieval so shown on the right is the gorilla trained with the Oracle retriever so that means that during fine-tuning the gorilla is always seeing the perfect API reference so I want to see some cats dancing and then it's that stapled stable diffusion model API reference so it's you know perfect API reference so here's so we see with the Oracle that the results are much better so you know comparing this column with the Oracle one is train with it or maybe comparing this column with the zero shot gorilla without retriever when it's been fine-tuned without any kind of retrieval at all so you know so that's that comparison gorilla with the Oracle retriever then is completely unable to do this without any kind of retrieval augmentation and then here's another really interesting detail of it is that when you train it with the Oracle retriever it becomes less robust to noise in the retrieve which makes a ton of sense because you've changed the task from memorization to like learning to read and so if you're learning to read the wrong thing obviously that makes a lot of sense why that would mess it up or yeah so you know it would be interesting to see kind of like um ablating this further like instead of just um top one maybe you you have top three so there's there's definitely some more exploration they could do with uh you know how you retrieve the context during training do you put do you mix the Oracle with noisy results to make it to try to make it robust to noise that way and so on so the big takeaway is that the current retrievers still have a big gap between the ground truth retriever I think this is maybe confounded by because you could have like a I I think I think this task also is particularly bad for bm25 but you could generally you could do like hybrid search with re-ranking to try to improve it you can maybe fine-tune the embedding model as something that is not common to see but you know definitely could be a lever to pull this performance even further but yeah so the key thing is to note that if you're going to train it with the Oracle context and then at test time you have some noise in the return context you can expect the model to perform worse because you know it's being trained to copy perfectly okay now here is maybe the more important evaluation thing is test time documentation change as the authors State the rapidly evolving nature of API documentation presents a significant challenge for the application of llms in this field these documents are often updated at a frequency that outpaces the retraining or fine-tuning schedule of llms making these models particularly brittle to changes in the information they are designed to process this mismatch sorry an update frequency can lead to a decline in the utility and reliability of elements over time so I think this is probably the most important thing for this API documentation because you know as imagine we train the wev8 gorilla one time and now we want to say change the arguments to one of the function calls or deprecate some something or we want to introduce new apis this ability of retrieval aware training where you're training it to just kind of read the documentation that enables it to adapt to changes in the apis and that is a huge enabler for this kind of application they illustrate how you can upload the model registry to change the model path from PI torch Vision to Nvidia deep learning examples colon torch Hub and they show this kind kind of thing of how the retrieval aware model is able to adapt to updates in the documentation the next really interesting detail for these kind of models is how well they can follow constraints so particularly with deep learning models we have resource constraints and then performance expectations so you know we have these two kind of things parameter size which is a pretty good proxy for how you know the late how slow is going to be to make predictions as well as what kind of like whether you need like four gpus to run it or what kind of GPU you need to run it and so on as well as the lower bound accuracy so for example the natural language command might be invoke an image classification model that uses less than 10 million parameters but maintains an imagenet accuracy of at least 70 percent so this requires the Nuance of understanding the request as well as then how to format it into the API so you kind of have this confounding of these two tasks in one but it's learning how to simultaneously balance the performance you want with the resource constraints so this table is showing how well the models are at doing this this definitely makes the task more difficult you can see how the cloud and gbt4 they pick it up because now I think having this kind of um you know like ability to reason about the request as well as just how to format the API syntax because you need to you need to be able to like parse out this kind of you know what I want as well as the constraints then you need to kind of read the description read the performance and so on so I think this helps the more General models catch up definitely an interesting kind of aspect is this API calls with constraints particularly I would say resource constraints so hopefully that was a good explanation of the details of the gorilla paper more than happy to further clarify anything if you have any questions or ideas please leave them in the comments here are some of my personal takeaways from reading this paper so firstly I think it's really interesting to just kind of consider the evolution of this kind of research I think it started with react that showed how you could have zero shot prompting where in the input you would say hey you have a calculator a search engine if you would like to use it then you know please format your response like this and then we'll send the request to the external tool and this kind of thing so tools former I think was then that the paper that showed that you could fine-tune language models to use tools with gradients I think it was a little more entangled with uh the language modeling objective than this kind of gorilla work where I think the gorilla work is more specialized on particular tools rather than having gradients all the way in the end to end of how you use the tools to complete the tasks hugging GPT I think is another really interesting work connected to this which uh was at least for me the first paper that showed like this idea of like models or tools so I think gorilla is somewhere in between tools former and hugging GPT but I love the specialized Focus I love how they brought in the self-instruct for how you generate the natural language commands and bootstrap this data for then compressing from the large model to the smaller model to write these API requests so you know we're going to be doing more stuff on this wuvia gorilla so I'll just give a quick preview of this for now but you know we V8 gorilla is about you know training fine-tuning the model on the Wii V8 apis so starting with the search apis you know we it has a graphql API as well as like you know python JavaScript all that client libraries for how you write searches so you know this is similar to the web GPT stuff uh web gbt is where it has search actions like you open up a square bracket and say search and then the term or you and you get the search results and you say you know Open Bracket next page to see the next page of search results from say you know the Bing API or the Google search serp API and how you do that so it's also quite related to learning to search so learning to search is I don't think a massively popular area of research but that's where you would try to train a model to you know maybe copy like what what terms particularly humans search for so you know like how you know with how we use Google we might focus on these kind of keywords and so on so you try to compress that into LM so kind of similar to the API call with constraints in addition to this kind of how would you send the request to leviate you might want to entangle the gorilla model to also coming up with the search query and the search query isn't just the prompt so one really popular way to get better retrieval augment to generation is you take the prom and then you take that prompt put it to a large language model and say what would be the search query for this and then you send that search query to the vector database to get the context not just sending the prompt itself to the database so it was like this interesting layer in the middle as well now here's the thing that I think is really the like mind-blowing future of this is using Gorilla for software integration so you can imagine the natural language command build me a chat with data from my notion workspace titled wevia gorilla using llama index and weviate and you have these gorilla models that are that know how to scrape data from notion they know how to import data into Eva and then they know how to use the Llama index query routers across you know within with the weeviate data index there so you have this kind of natural language to build out all this software and I think the way that you kind of decompose the task into particular apis is a huge step closer to this kind of like autonomous software as we you know our trading models for the apis of particular software libraries you could really unpack the box here and think about kind of all open source software as a you think of all all class is in their functions as apis and you know this kind of I think the integration of Open Source software and science is about to become super integrated because of the way these large language models are going to be able to test new ideas by using the apis of Open Source software so a lot of stuff there for sure but I think kind of the interesting thing especially you know as we transition to Guerrilla for Integrations build me a chat with my data from my notion workspace blah blah is how abstract are these instructions going to be so in the paper they you know give you examples like I would like to identify the objects in an image or I'm going to the zoo and would like to track animals so it's kind of interesting like whether these instructions are kind of like atomicized to one model call or if you need to like you know really decompose the task sort of like you know self-ask multi-hop or this Auto gbt thing where you need to really kind of break apart the task like what are the subtasks it's not just like one the instruction is just some immediate task the next thing the authors kind of talk about is systems to execute apis during training and data set generation so you know as as you imagine uh generating the synthetic data would you try to execute it or when you're evaluating the models will you execute it and I think this will be you know building these kind of systems that work in the simulation during training will be huge for this kind of evolution of program synthesis using this kind of idea now this next thing is something that I think is really exciting so I I got this idea from listening to Bob in light uh speaking at the Llama index webinar bragging production and you know I worked with Bob on the generative feedback loops idea and I think this idea is awesome generative feedback loops are basically this idea of the language model transforms the data in some way and then saves it back in the database so say you have you know we started off with this Airbnb listings example where Airbnb listings you have symbolic properties like how much it costs what neighborhood it's in maybe you know nearby coffee shops or something like that and you would take those tabular features and then generate a text description save that text description back in the vector database where you vectorize it with like the open Ai embeddings and now you search through those descriptions of Airbnb listings there were all sorts of things you could do generative feedback loops the most recent example Bob gave in the webinar was process email threads to save summaries which can be searched through so you know imagine I have a you know a class of emails and it's like you know there's a thread where we're discussing something and then you summarize the thread indexes summary you add symbolic tags like who is a part of the summary yeah I'm sorry a part of the email thread in the conversation but anyway so generative feedback loops is about transforming the data because the the new transformation of the data might be useful like if it's a personalized advertisement or something like that or it might be better for a search index like if you have podcast clips and you summarize the content into the abstract description of like what was discussed and then you index that that'll be a better search index so that's one side of it and then on the other side of it we have Gorilla so right now what we're thinking about with a gorilla and there'll be a blog post soon more details on this is just kind of using the search apis but Guerrilla kind of I think where this research is headed is it can configure itself it can create new classes it can maybe even tune the product quantization parameters so it's kind of like this and I I got this phrase from Andy Pavlo who's a brilliant Professor on databases at Carnegie Mellon where he calls this self-driving databases and I think self-driving databases is used to describe kind of uh databases that like automatically optimize their index structures and I think that's that's probably the missing piece of this gorilla plus generator feedback loop so I haven't gotten into that part yet but I think this kind of generative feedback loops and Guerrilla we're already seeing this kind of autonomous database the database that takes the data and does all sorts of stuff with it because it has all these different levers to you know it can query itself it can import data to itself sort of create new classes and then it can transform data with these LMS so anyways those are some of my takeaways I think probably from the highest level I think just the leviate gorilla will facilitate the learning curve of learning to use the search apis if you just have to say Vector search in podcasts you know and then it will just translate that into the graphql or you know you want to add a filter like vector search and podcasts uh published after July 5th 2023 and it will just add the wear filter for you and we V8 syntax so I think that's probably the lowest fruit is just facilitating the learning curve to learning how to write these kind of query apis but I do think the potential of this kind of research is just through the roof thank you so much for watching this explanation of the Gorilla research paper if you're interested in this kind of work we have all sorts of cool things planned this is hardly the End of This research on llms that control apis the weeviate Guerrilla models and all these exciting things we can do if you're interested in this research please subscribe to the channel we'll be publishing our podcast with shashir and tianjun the original authors of this paper on Wednesday and please subscribe generally to the webia YouTube channel for all kinds of videos and podcasts and things about this whole world of retrieve augmented generation Vector databases and large language models you connect with us find wevian weeva.io open source on GitHub wev8 weavate or on Twitter at weba underscore IO thank you so much for watching I hope you found this video useful and exciting ", "type": "Video", "name": "gorilla_llms_explained", "path": "", "link": "https://www.youtube.com/watch?v=LkV5DTRNxAg", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}