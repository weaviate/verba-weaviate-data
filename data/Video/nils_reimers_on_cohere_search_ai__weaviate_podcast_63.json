{"text": "Hey everyone! Thank you so much for watching the 63rd Weaviate Podcast, I couldn't be more excited to welcome Nils Reimers ... \nhey everyone thank you so much for watching the weeva podcast I could not be more excited about our next movie podcast we're hosting Nils rhymers again on the wevia podcast to discuss all sorts of things about Ai and search sort of last time when we did our first weekday podcast and those we were presenting the cohere multilingual embeddings models and we similarly have a really exciting integration to announce with weavier and the cohere re-rankers so you know I love kind of having this the new update about cohere and weavate as well as diving into all these academic topics with you because I just think you're one of the world's leading scientists on search and it's just so much fun exploring these things with you so firstly thank you so much for joining the podcast yeah I'm super happy to be here um exciting times so I don't know probably like half a year ago I was here talking about multilingual so semantic search is amazing for multilingual setups so especially as a European um it's it's nice to see work outside of English be done and yes super happy to talk today more about re-rank uh challenges and semantic search everyone faces at some point and yes solutions for that yeah I love that first podcast how we you know describe this kind of evolution from building the sentence Transformers library to now a cohere and how your Evolution and thinking about the maintaining of this data set kind of we discussed like this kind of closed Source versus open source model kind of category and we had these big topics around like how do you handle distribution shift what's the state of sparse vectors and the future of that so I'm going to be kind of uh kick this podcast off with the cohere re-rankers sort of what goes into that and maybe for like levia users listening like you know sort of like what they can expect from adding this to their search yes so um so recently like a couple months ago we launched a new model a re-rank model which is like a super cool thing so there's still a lot of people who sadly don't use vv8 who are like stuck on like old lexical search systems uh sometimes proprietary where they are not able to change and it was re-ranked um what you do the the API looks really simple you put in the query you put it in like a collection of documents top 100 top 1000 documents and it's as Dimension as the name says it does a re-rank so you've given a thousand documents and that spits your like which documents are the most relevant one so so this is like super easy to add for anyone who's not yet on vb8 Whose stock was like old solar implementations or external apis that I need to use but also in in vv8 itself um it makes really sense to do both so I do like semantic search for some bettings um get the top 100 top 1000 out of like embedding search and put it into re-rank um so so s talked previously and in many other settings there are certain challenges and semantic search with some bettings and which will also talk later more about uh looking forward to that so so one challenge semantic search and embeddings have is like really complex queries so so if you have like multiple facets where you say give me um I want you to know the information how Google employees changed in Europe between 2010 and 2015. so at some point it's like really hard for embedding models um to to get the produce the right embedding so so it's like basically your beddings are like shooting in the dark um so so it doesn't know like which information do you have in your vector database um out of your search query it does like a shoot in the dark and then hopes that there are like 10 documents which are close by giving you the information how Google employees changed number of employees in Europe changed in 2010 to 2015. and re-rank is really well on these types of queries so it used different type of Technology um where it looks at all the documents you provide in like all the hundred documents you you provide and then makes a decision based on these Android documents okay which one is the most um yeah the best for your query so it's not like a shoot in the dark I produce the embedding and then I hope to get the top top 10 with the information I really understanding reading all these documents you provide in like in the top 100 list and then telling you okay these are like the top 10 documents uh with the most relevant information and so what we see is like really strong boost and search quality especially on complex domains complex queries where you have like multiple aspects so not like a simple fact like who's the founder of Facebook but okay here's like a really complex query like okay which test company was large exposure to Europe reported a revenue increase of at least 20 over the past year so so if you have like these really complex queries um we rank is extremely excellent on that yeah it's amazing kind of that separation of you know complex queries like when do you need to use re-ranking I find that all very interesting kind of yeah just just from the beginning of like you know you retrieve your candidates with bm25 and then you re-rank them or with Vector search I I always love we had like this illustration early when we first started looking to ranking models it's like thinking about it like a fishing boat where you cast like a big fish net into the ocean and then you get like a thousand fish onto the boat and then the fishermen on the boat are like looking for the green fish within the fish and I always love that angle because it shows like how the ranking models are like more accurate but slower and I feel like that's you know but you're adding a really interesting extra Dimension that there are some particular kinds of queries like the multi-discourse thing where you have this kind of funky query that you would really need to have that high Precision you know document and query as input I guess I think we can really dive into this topic of ranking generally and what ranking means in search but quickly I kind of want to understand a little more about the training of the ranking models that cohere is is this like kind of the same data set used to train the embeddings but you train a classifier instead of an embedding model is that a cartoon City uh yeah it's it's similar based on some of my data sets so last year we spent a lot of time to to get like a big data set in many languages so got like roughly a billion question answer pairs in English half a billion in non-english languages and then yeah this this is the basis for the embedding model and also like for the rewrite models So based on these last amount of questions people have asked and answered in the past the model luckily it knows a lot about domains like really long tail about yeah the weirdest subtopics you can find on the internet and and yeah really gets this knowledge which which make them extremely powerful yeah I love that conversation topic of how you think about curating this data set obviously you know one of your earlier works with the sentence Transformers and you know we're getting a billion pairs together that had really inspired my interest into this whole kind of cat I think that was just a huge evangelist for all this so it quickly you mentioned like long tail and you know I remember that one billion pairs it had like Reddit stack Overflow how are you currently thinking about that data set aggregation I'm sure like for listeners understanding how cohere would think about something like this is a really interesting part of it yes sir so One Challenge was in the field in general is um how quickly is the knowledge getting outdated so we're producing like new new words new product names new company names new events Games movies and so on so so this is like in general like a challenge um it's hitting especially hard the embedding models so as mentioned embedding models it's like shoot in the dark so so you have like one shot you hope to produce the right embedding without like having a lot of possibilities to compare results and so they are like really really subjective to these issues so if there's not something new happens um they are not aware of a lot of embedding models that they are trained on like old data like Ms Marco from 2016 with a bird model which was also trained on like 2017 2018 data so if you ask the models anything about Corona still thanks Corona is a nice beer a nice Mexican beer and then if it asks if you ask like okay what are the symptoms of Corona it will say okay you get hangover and drunk by it um so so that's like a really really challenging for the embedding model um so here you wouldn't need like constantly updated information uh which was like hard in terms of deployment because you you need to re-enteric or your vector database so so if you go in like a billion scale this this becomes a really slow and expensive and for the re-ranking model as you mentioned it's like this fisherman and say okay look at the fish look like okay where's the green fish um these are a lot more robust in terms of like domain shift out of knowledge out of time so here you don't need to to update them as often and also the the update is much easier so so with some bettings if you update the embedding model you need to re-encode your whole Vector database which is slow and expensive and painful if you have like a billion embeddings at the re-rank model you yeah it's just like one API call on top of the top 100 results you get from the vector database so it's like really easy to update to switch it and yes so here we were aiming to have like always up-to-date knowledge um in these models yeah that is a super powerful point the robustness of the ranking models and I hope you have people listening know that yeah you know that that kind of that problem of updating either the corona example is perfect I won't even add anything to it but but just that kind of continual learning problem and understanding that the cross encoders the way they attend to the query and the candidate documents it makes them more robust to I mean we I think we were all seeing this zero shot ability of models and so it kind of brings me I kind of want to talk broadly about ranking models I understand that like you know a lot of ranking models use kind of symbolic features as well like say it's like Google search it probably has like you know some information about me like you know I'm 27 I'm a male and it will use these features as well as kind of the content and the quick like the query content as well as these symbolic features and you'd maybe give that to xgboost and there's some new work that's showing how you can maybe uh prompt llms with these kind of features do you think about those kind of features in in these kind of systems yes sir so this is still a kind of like a big challenge big issue with semantic surge um for semantic search queries they work nice if I ask like what's the average lifespan of a cat especially with VP I find the information but if I search like let's say on it on Spotify for um updates on AI like I want to listen to like some cool podcasts updates on AI um if you just search on the text bar on the text field you might find like a show which has like yeah updates on AI but this show might be from like 2015 it might be really poorly rated like one out of five stars and then there's like another show which is like I don't know like a week old really highly rated what the name is like not like updates here live I like I don't know weekly digest and machine learning um you as a user would prefer like the recent one the popular one the highly rated one or like the old one and so this is like a big limitation um as you mentioned a lot of people what they do is like XG boost and I'm all learning to rank so basically in the most simple form um you have like some semantic match from ether like re-rank or embeddings uh you have like a score on recency you have a score on popularity and then you do like a linear interpolation and say okay I don't know 50 is semantic match uh 30 is is um recently and 20 is popularity and then you play around with the three ways to give you like the final ranking um in general um it's not really fun to do this um it's really really brittle so you spend a lot of Cycles to play around with these three weights uh like chaos recency should be like 10 20 50 percent if you just for like recently too high only the the most recent podcast will show at the Top If you put like popularity too high only the most popular as shown even if you like I don't know do like a really Niche query long tail query with okay I'm really interested in this one uh tiny podcasts you're not able to find it and then you try to balance it between these three factors and so yeah so I think there's a lot of potential here to make it better while we're also like working on it like rethinking like how can you um yeah learn these as part of the model yeah I think um like if I can um yeah just kind of continuing on the potential of this I think you know having the llm and prompting it where you get each search result is like a Json dictionary and the keys are like content and then the text content and another key might be you know date published is probably a good example when you want to sort by recency and sort of as you can just prompt the llm to like you know please uh you know re-rank these but prioritize recency you know but like not at the cost of it it's not relevant like that kind of soft fuzzy prompt in it and it's still able to kind of interpret that and I think the distillation of the LMS into the cohere re-rankers for that particular tab I love that topic of like distill llms into some specific task and I think the ranking is just the perfect perfectly set up for this so I think this would actually transition really nicely as we come into our academic topics to talk about temporal queries and how you're thinking about that yes attempt um it's a challenge so it's like really a lot of people go go into like semantic search so so you see a lot of people really testing around and yes sadly embeddings itself they don't have like a temporal understanding so let's say you have your email inbox you you search for it which time is my flight going and then with semantic spiritual you get the information hey your flight is going uh 5 p.m on August 4th 2010 so it's fetching you like an email that's 10 years old um because yeah for for the embedding there's like no Temple notion in it so so if if there's Nema from 2010 that has exactly your words hey your flight is scheduled at 5 PM it will find and show you this result so so what what you often have in search is like these yeah not not these implicit expectations or assumptions from users so if I search for at which time is my flight going I want to know like probably the next flights or something today tomorrow this week so I'm looking for like my next flight that's going in e-commerce you often have it when I search for shoes um there's like a lot of shoes on Amazon but yeah it makes a difference and I'm a man a woman uh am I like what's my age do I have kids do I have like son or daughter and and what age are my kids and then depending on these um it makes a difference so maybe I'm looking for my uh shoes for my daughter who's like two years old um so so maybe I put just kids shoes in in the best case it shows me like um shoes for girls two years old uh but yeah this is not in the query there's somewhere outside and so it's a fascinating problem um how to encounters where yes semantic surge what was the mattings hitting certain limits so so in the embedding space you you can't have like these temporary relation and then yeah people start to hack around and say Okay um cementing match 80 percent recency 20 percent uh try try to use thresholds let's say okay find all emails um at which time is my site going so find all emails with a threshold above 0.7 and then rank them based on recency so like a lot of hacks and tricks going on in the space to really go from like a shiny demo to an actual production use case yeah I have kind of a couple perspectives on this temporal queries thing I think like for I think it's also quite heavily related to kind of filtered Vector search and for people to understand that you do the hsw graph traversals and you have this like allow like one way that we do and we've either I could I could go on about these things that I probably saved up late for later but like you'd have an allow list of nodes that match the filter IDs and so like date greater than July 5th could be like one of the Criterion that lets you Traverse the embedding proximity graph but I think there are quite a few things this the first is like like there's kind of like an emerging category of startups around like metadata extraction from unstructured text chunks so we had Brian Raymond from unstructured on the podcast yesterday I listened to Doc ugami presented llama index and there's Chima Lang chain law minute they also have like these data loaders and so like using language models or I think kind of like especially talking in those rhymers like llms we both kind of understand these just mean like deep learning models it doesn't necessarily have to be like 200 billion parameters to do the task but like training specializing models on extracting metadata that you can then label each year text chunks with the date that if it's captured in that which that's why I really like the example you just presented and it added a new perspective is when you have that query that's like you know I need flights for tomorrow you I guess you need to have some kind of query understanding layer in the middle of that D and so that makes me ask you would is that the next cohere uh model integration that we'll be doing is the aquarium intent uh yes uh so if you're search for which time is my flight going tomorrow um you need to to in some some integration um I'm not the biggest fan of like metadata extraction so because then you're kind of like limited to like um yeah which meta data do you have what do you extract and doesn't match what's like the the um the thing you you have when we have something like um at which time is my flight going tomorrow like finding the right email is kind of challenging so so it's not like the most recent email you got was flight information because this could be a flight somewhere in the future it's probably not an email which is like 10 years old but it can be somewhere in between like somewhere in between like one year and yesterday so it's like really really hard to express this as heart filters or as SQL where you say okay give me I don't know if I asked like at which time is my flight going find all the flights emails I got and then sort them by recency and pick the most recent one um so here we're currently developing your smarter way so New Foundation model in search um where we don't have to do like the tracks with metadata filtering and going from unstructured data to structured data and then language like SQL or whatever or however you express your filters and your ordering but really do it like in the mall itself because I mean it's it's always peers but observed for me that we have like LMS and we take LMS to transform unstructured data to structured data to then be able to search to search on it so why not like directly use it in the alarms and be able to do let them do the job and and yeah extract what's needed and then tell me okay what's done there yeah I think um I guess there's there's kind of I have two kind of reactions to this the first of which is there's kind of like filters that are like totally unrelated to the content so like it like if I have a you know shoes again and it's like how many of them are in stock there's like no way of having the embedding you know how many I have in my store kind of so I kind of see like that component to filters but then again like how an llm or whatever metadata extraction model similarly would not be able to derive that and so yeah I agree with you it yeah it's very interesting I guess I I probably would just need more time to see it all play out to have a strong opinion of my own about this but I agree with you that like if the llm can extract the metadata from the chunk itself then you probably also could capture it in the embedding and there probably is something to that yeah I guess yeah I mean these metadata floaters they are nice and e-commerce we see that I'm like on the left bar we have like a sidebar where you can filter on the price where you can filter like the brand the color the size um here it's always like a question it's like yeah you always have like these hot filters where smk give me anything I don't know like some item let's say a smartphone that's below like I don't know anything below 500 and then I have like a really hot filter that's like anything below 500 but what's the case if there's like a smartphone perfectly fitting what I wanted so I was like looking for Christmas for a new smartphone it should be like small so that fits well in my pocket camera quality was important Factory lifetime was important I didn't want to spend more than 500 dollars but these five hundred dollars is like kind of like a soft limit so it costs like 500 and one dollar and it's like the perfect phone yeah I take it also if it costs like 550 or 599 dollars I said yeah it's so much better than anything else and I would buy it and it's uh so I think this is at least what I see in Vision like long term and we can get rid of these filters and just phrase it as a search query where I say okay I want a smartphone um should be small enough to fit into my pocket um should have like good camera quality good battery life time and I don't want to pay more than 500. and then the model can decide itself and see yeah here I got like a phone and got really highly rated good feedback reviews on users the size is like really small so so it's a tiny phone um but it costs 520 dollars but I think yeah 520 it's good good enough and are you willing to to go like twenty dollars over your budget let's say yeah perfect so I'm willing to do this this reminds me back to our original conversation on ranking models where I could have like yeah like that that like boost filter is kind of like what I like talking calling it and then we get discussions of like what an API would look like but like yeah like you know please rank it higher if it's less than 500 but you know you don't need to be so strict about it and that kind of thinking about how you do the iPhone um yeah like um I guess it kind of also reminds like it's this topic again it's very similar to the llms extracting metadata it's the ranking models it's like if the ranking models can do it then can the embedding models do it as well but I think like having that symbolic control it just makes it so much easier I I guess one other idea there's this paper called nhq which is like you would have embeddings for each of the filters as well and maybe you could have like a multi-vector ranking thing where you have the pooled vector for the entire you know Amazon iPhone this iPhone that we're talking about this the description of it and then you also maybe have like vectors for each of the labels and so it's still kind of like embeddings so sort of yeah maybe that kind of thing but so I think kind of I kind of did want to stay a little more on your skepticism of the metadata extraction with language models because I think it ties to another one of you know the topic so we've premeditated is like um I find it very related to kind of chunking and long document representation because I feel like what happens is um your motivation to extract metadata a lot of the times is because you're like parsing a PDF and you know it's like a semantic category like you know it's there's only a hundred tokens in this part of the document but like for a reason it's got like a title like uh this is like this uh legal clause and then here's like another one right like so so yeah so maybe if we could kick it off with your perspectives on Long document representations and then maybe we can see we can tie that back to this kind of extraction topic yeah so so I mean for now metadata extraction it gets the the things done like in many cases so just say like long term like I don't know five ten years I don't Envision the field that we're still taking LM extract metadata put it into like some relational database for like yeah find ways how we can do it directly but yeah to the topic long documents um yeah long documents are challenging in an embedding search um for a long time sequence lengths have been a challenge in Transformer models like birds it's like 500 above tokens and this this got recently fixed was like longer sequence lengths um like like llama 2 what we have now open source models up to like 32k um but it's sadly not solving it for embeddings or search so so yeah you can deploy it but there's a quality of all these models is just terrible and the issue is that embeddings can encode like one one Topic at a time so they can store like one fact so so let's say like the founder of a company and can encode these like Max Zuckerberg founded Facebook um but they struggle if you have like multiple facts like let's say you have like a long list of all U.S equities so they're like 2 000 or more uh us equities right now with the phone number information it's like okay should I include it like at Facebook or Google or at Amazon or Nvidia um so so you get somewhere some strange embedding in the middle uh which is has nothing to do with the specific information um so so what you need to do is like take the long document um break it down into individual facts and that's the first challenge like how do I know okay this is like a new topic so I'm am I still with the old topic talking about Mark Zuckerberg and fond of Facebook or is it the new topic about Bill Gates and how he founded Microsoft um so so that's one challenge second challenge is contextualization um so you might have like a header and then you know okay this this paragraph is um belongs to the header so so for example in every reports you have Apple annual report 22nd um in the text itself it never mentions Apple it always mentions we or our company and it also doesn't mention 20 22nd anymore it says our company saw a lot of growth this year and then the question is like okay who's like our company who's like this year and so here the contextualization approaches are kind of interested in where they take the long document and you decontextualize it so if you have a paragraph like our companies or a lot of growth this year you rewrite it to Apples or a lot of grills in 2020. and then also to look at like okay can we split it into like individual topics which we can then encode as paragraph in the vector database yeah I love that uh it just I just had the light bulb go out but like you know I love this idea of like well we're kind of trying to call these generative feedback loops where you use some kind of generative model to like kind of alter the data that is indexed in the database or or just save for any reason kind of but this particular case of like you know like something I love is the summary index where you take like you know these podcast clips and say I'm stuttering like uh right and it kind of will compress that into a summary of the content that I say and then you embed that summary and I love that I think it's a really clever way to build uh better search indexes and as you mentioned disambiguating you know our company means Apple that that to me that sounds like another kind of like generative feedback loop use a generative model to fix the data there so then there were kind of two things I parse out of what you're saying um the first of which is kind of a uh like generally you know language models are getting along your contacts things like Alibi attention you know the details of how that works I think maybe you're relevant for the podcast but like generally we're seeing models that can take 8K 16k 32k and it's like will Alibi attention work its way to embedding models maybe if I could quickly get your temperature on that and then we could come back into like multiple facts and maybe yeah because we and then we also have like structure graph data so I think it would be a nice transition to that but let me quickly take your temperature on like Alibi attention it's clearly you know being impactful with anthropic cloud and you know all these kind of things will that kind of thing help embedding models badly not as as mentioned um embeddings only work if you have like one fact in a per text and there's like not so many texts out there where you say okay I have like 8 000 tokens but it's just I don't know repeating Max Zuckerberg founded Facebook um totally if you take like a 8 000 token article let's say from Mark Zuckerberg or Facebook there's like a lot of information um about Facebook early who founded it early days nowadays transitions to meta and so on which are like all different facts and every effect needs like a different embedding so so yeah we can add Alibi to it that's not the issue yeah if you input it like full Wikipedia article and then you search on it um search quality is terrible so the so then this brings me back to the metadata extraction thing because identifying facts isn't that kind of similar in a way like because you have to say like okay this is one fact this is two facts in this unstructured text Chunk m yes it's kind of metadata so so it's uh currently like a hack a trick that gets the things done so so it's totally okay in search for search for the past 50 years it always has been like using hacks to get things done um challenges always but when you do like this the contextualization which I mentioned earlier like our company had the best year or this was the best year for company where you replaced this year was 22nd and company Apple um challenges there like how much should you decontextualize like how much information should you provide into that um it's kind of like easy for like this really simple one sentence but if they go in like an annual report you have like Apple annual port 2020. subsection I don't know Europe subsection um I don't know iPhone subsection uh Market uh share of iPhone in Europe and then you have like some information like complex information on it so so it's like probably a lot of like context information to understand this paragraph um well then the question like okay how much do you include do you just include it's Apple 2020 um or is it do you also include like all the information what I talk about like a market in Europe for the iPhone with like certain subset of customers um and then it becomes like really really challenging again to find a good trade-off where you say okay you have like this one sentence but to really understand the sentence you might need like full paragraph or full page of context information and then the question was also like how to embed it where you say okay this this is the sentence and here's like the full context to understand the sentence yeah I love that decontextualization I think that's a wonderful phrase like we've seen like Disney yeah like one example right now we know Donald Trump is front of the uh court is tried um due to and we have like a lot of context information where he was not accepting the last U.S elections and yeah Trump fans were storming U.S capital um so so there's like a lot of context out of context information where also the question is like okay if I Index this news article where Donald Trump has is this trialed in Washington do I also provide like all these background information which led to the trial or not yeah that's because like um so you would take like one text Chunk that's like um Trump fan storm U.S capital and then maybe you'd want to add to the chunk like because they believed the election was quite a topic maybe find another example but but that kind of thing of it makes me think like about um you know models at data ingestion that again that topic of like the summary index kind of thing but where you use more models in the process of parsing out your data and building up the index and I think it's all very interesting but yeah I think this would be a great transition to talking about uh graph data I'm not sure if we're if you you know like maybe to transition like there's kind of two ways that I see about thinking about multiple about organization of facts themselves you would have like you know Nils rhymers works at cohere and you know like author sentence per and like you have these these relations and then when I just grab the node nose rhymers I can really easily parse out all the facts Associated but or you would take each fact have a natural sentence for it and then embed them and search so I yeah I love kind of I don't have a great understanding of when to use which but I think those two now headsets yeah let's see it's an interesting so interesting road ahead of us so everything searches just at the beginning um current technology is totally fail as mentioned if I ask like any search system um at which time is my flight going um they all fail possibly um so so this was like an interesting aspect what we need to see in terms of research in Industry um what Innovations come in like like do we use like these knowledge graphs where we represent facts and say okay this is pretty soon they have like this information and their cve like past Publications past company engagement and so on um like more in a symbolic way or do we do a lot more in like kind of like an abstract way where we keep like a text record and then the model is able get to find the relevant information and quickly um yeah quickly recent about this and yeah right now like finding these disinformation and then judging is it relevant or not is this kind of challenging so we all know large language models they have challenges with hallucination so so if you ask them what are the symptoms of some fake disease like these these I just invented um they give you like really compelling writing what is this disease what are symptoms what is the treatment because LMS transform us they are terrible in not knowing and knowing that they don't know it and that's that's basically like the challenge in search lie you need to decide is this text providing me the information is it answering the question or not like at which time is my flight going um so here they're like really really bad at and we need yeah a lot of Innovations and breakthroughs and this space you know I I I do have to like hypothesize that the kind of cross encoder would be really great at that like at which time is my flight going and then boy yeah hopefully I mean and then it kind of retrieves from your calendar it's kind of like the it this whole thing is our the way our conversation is evolving is really inspiring me into the whole like retrieval augmented generation as well as like kind of the agents thing I think the agents thing ties into at which time is my flight going and you've hooked it up to tools like my calendar and so and then like that that's kind of like there's all these like models in the middle I found this topic so fascinating because like they're like sorry if this is too much of a tangent but I like this paper called gorilla quite a lot where gorilla is like a large language model fine-tuned to use a particular tool so like in the case of the calendars don't have too many apis but it's like how do I format a request to get uh like nose rhymers is this Friday and like get everything to understand when your flight is but you know generally actually let me kind of just broadly how how do you feel about retrieval augmented generation right now and I mean it seems to me obviously from alleviate like this is like you know a huge evangelist I know you know cohere has like the Corral system Coral system and yeah yeah yeah it's a massive topic um so so we have this Coral system which is like an intelligent Enterprise agent so so you plug it into your Enterprise data and then you can ask questions like okay um we we have this customer give me an overview of the customer engagement where are we which are the latest topics and blocks from this customer so instead of like you as manager like pinging the sets wrap um you get it directly from your CRM data so I always have the feeling people put data in Salesforce but yeah look at it and instead of like Ping people together summary so hopefully we can change that so yeah we're truly augmented generation is fascinating and we have like really strong models here on the one side for search and because this is still a bottleneck and on the other side in generation and citation so so big issue um was generated models of us generative model hey um I don't know what's the size of this company spits me out like some number like when I say okay what's the revenue of Apple it gives me some number but it's I don't know is it true or not and and I don't know if I used to ask like these generated models hey how many students study them in my University where I did my PhD gave me a number but it was like a bit of like off by some few thousand students and that's like really hard to to to judge it and here citations is relevant to say okay I can go back to the primary source and say okay the UI got the information so for example if I ask the model in an Enterprise setting um and what's the the pricing discount we communicated to that client you don't want to have like some hallucinated so so you don't want to say yeah we we said 30 discount and then you think okay it's 30 discount but actually you just communicated like 10 discount um or or is it okay in terms of like legal questions is it okay to use um your software in such a such setting and then the model says yeah perfectly good but then you say now well how can I trust it and I think here giving the citation and say hey yeah you can use the setting here and the source is this email from January where we ask those questions and based on the answer they provided we think um yeah you can use the software in the setting so so you can verify it as a human if it's actually true or not yeah I think there's so many topics to explore I mean the kind of uh give the citations I think that's one of the most clear applications like uh reasons to do this like as exciting as L alums are the hallucination problem and having being able to look at the search results there's so many interesting things in that though like the the question of what is the size of this company I've been loving saying this recently is that every weevier class is jointly a vector index a Json index and as well as an SQL index with we don't have an SQL API we have an aggregate API so the syntax is a little different and it's a little interesting thing of how to interface that but like you you Tran you take this what is the size of this company and you know is that a vector search query person I think it would be better off Translating that to like a you know symbolic aggregate like you know this but then this coming back that clear right and that they're the challenges again do you have this in like a relational database or aggregate like you have like a column yeah let's say employees or Revenue um it's also a bit unspecified size of company do you mean like Revenue profit employees yeah um in the past it could also be like physical size of the company like how many square meters is the company um and and here yeah I think that that's the challenge was metadata to come back to that so so if you ask like okay how many employees work at this company if you have a column if you extracted this it's good if you just have like The unstructured Meta like unstructured annual reports um yeah it can just look into a lag and say okay fine in the unstructured meter data okay um this is the um this is the size of the company like the number of employees of the company then where it becomes interesting is um if you do like combinations of like inference of information when you say okay how many employees does Apple have in Europe it sounds so maybe in your database you just have like I don't know from Bloomberg you get the data number of employees at Apple but you don't know like employees in Europe for Apple like from unstructured data like some news articles some and we report some press release they mentioned okay we have like I don't know 100 000 employees roughly 30 of these are in Europe and then you can infer from like these two information nuggets okay Apple a good estimate is like 30k in Europe and so that's that's fascinating like where we need to rethink um how databases work and then we will see a lot of Innovations going a lot more into like unstructured only because we're structured you will need to think beforehand um that someone will ask like how many employees does Apple have in Europe and and this is kind of like a rabbit hole where it says yeah you could ask like okay how many employees are in Germany how many employees like France and then it gets like you wouldn't need to think about this beforehand when you create the metadata and Abstract the data yeah well all that is why I believe really strongly in kind of the longevity of Frameworks like llama index line chain like Haystack Gina is like this kind of you know like the sub quite like self-ask prompting is kind of like the Academic Way of like there's a paper called neural databases from James Thorne and others which is like yeah exactly I mean you painted the picture perfectly you decompose the question into like maybe a symbolic question as well as a vector search and then you later on kind of pull it and I think that kind of you know semantic layer of orchestrating these kind of LMS and all that will be really interesting um I kind of wanted to stay as sorry to be picking the topics but like when we're talking about retrieval augmented generation a little more and talking about kind of reducing hallucinations as being maybe one of the biggest drivers of this kind of framework I wanted to get take your temperature on retrieval aware training this like for example I mentioned the gorilla thing earlier the way the gorilla thing works is you retrieve the AP you're you're giving it a natural language command like in the example of we via to apis you'd be like bm25 search in pod clip return speaker content and it will take that natural command and then translate it into the graphql that could execute against the weba database so the way this is trained is you retrieve the API reference and that goes in the input alongside the natural command and the schema to you know output the graphql query and I feel like this kind of retrieval aware training could just be so powerful I'm curious like if cohero is thinking about doing this would be om is obviously in the search and all this kind of stuff yes that's uh it's it's a big Focus so so internally and also to offer customers we have a really nice wreck model um which which stuff these uh so in Italian rack versus like Quarry Foundation um how do you break up your your query into sub queries execute on these how do you get the data how do you create like the subset of data that goes into the rack model let's say you have like a query um how did the revenue develop for the big five test companies during covet so so this is not like a single query so so if you work with the inner reports of companies it's not like a single query it's like a set of queries so you get need to get the data for Facebook from 2019 from 22 for Google for for Amazon and so on and then you need all to prompt this into the generate model um challenge is was a lot of generating model out there and like llama open Ai and traffic they they have not been trained on this and we trained like extensively on this so on the internet you often see like the final result so you write the final blog post but you don't see like the research that went into the the blog post like what were the search queries you formulated uh when you were informing yourself about the model and so so this is currently what we're doing it's like showing the research process like how does a human search for information which information do they take into account before formulating the response so bring this capability to the model and then also be able to reference back and say okay this this part of the information I got from the source this part of information I got from that source so here um yeah we will see at least from here you will see a lot more on Foundation model from from this um models like really understand to search what to search for how to reference back information um as it's kind of like a vital event yeah I mean and so going forward I mean it's it's also the shift we saw with humans I don't know when I started to program we still remember people try to memorize like every function you have in C so so like okay you know all the the functionings because it was like really slow to search so you had like a big book and then you don't need to go in the back in the index and see okay which function do I use to I don't know sleep my process for a second but this really changed was like Google and stack Overflow or nowadays yeah I don't know there are some functions we always search um like I always search what I like the arguments for the select function python and yeah with co-pilot it's awesome yeah less and less knowledge needs to be in our head and and we're doing more and more storage in the background and then use the search results to enter to the next token and the same will happen with generate model they they will learn how to search how to take search results into account so I think the kind of really interesting philosophical thing for me is I remember like you know Bob had showed me this notebook from Nick Frost which was like query formulation so you take the prompt and you don't just send the prompt to the vector database you first send the prompt to an LM to ask it what would be the query for this to then query with that and then answer the question that way and I think kind of for me it's like you know this difference between learning how to search I think this is kind of there are a few things to this but I like the first thing there's like how to search with Google search like Bing API serp API and like kind of like the web GPT search actions where you just kind of think about like what query to send maybe like next page of results versus kind of the gorilla thing which is like how to particularly use the apis of like weviate and these search databases Vector database search engines like crazy you want to use and then I think there's also this kind of really really interesting emerging category of like end-to-end rag where you would maybe put gradients from the reader back to the encoder and it so it's kind of learning it in a very neural way like none of the symbolic layers have had the Bing API or the weeviate API but it's just going kind of and it also would you kind of use weaving to have to store the vector embeddings from the encoder even if it's learned end to end so kind of these three things that hopefully that's a not too ambiguous of a question but like yeah I mean the final one let's end to end um it's it's interesting my really really challenging I mean there was this original ritual paper which had like retrieval um within the pre-training of the model so so before it predicts like the next token the missing token it looks into like a massive Vector database to find like similar text um people were extremely excited about that at that time but feels like there's like basically zero follow-up so I haven't seen anyone like reproduce or or work on it um as a challenge is um in these settings you you make it like often like too easy so so in your text you have like a lot of duplicates and then the question challenges is um when you ask like okay what's what's vv8 and then it retrieves the information there's a similar text like a new duplicate for the model it's like really really easy to just copy paste the words from from there so so you don't get like any reasoning or knowledge into the model um if you're not extremely careful so you have to control like how good is the search quality if it always finds like an exact duplicate of the text you currently do next token prediction on it it will not learn anything just like yeah do do predict the next token and we'll copy the the token from the search results and that's a challenge other challenges engineering wise end to end s you have yeah many moving Parts which you need to to fix and it's like really hard to improve upon individual parts let's say if you have an end-to-end system and and you see okay search quality is not great um because it doesn't take like recency popularity into account let's say you do like search over news you ask okay give me in insights about the US president's elections and then gets like information from 20 years ago in an end-to-end system you can't go in and fix this one system and say okay make the search results better or make the generative better it's always like if you want to fix one thing you have to work on the hill pipeline search and generate like search error and generation search and generate and this makes it like a really nightmare to to work on enhance yeah uh yeah I I agree 100 with that I think yeah yeah and I I guess the only yeah with the rag into end thing I'm not sure really where my sentiment is at on now and I don't want to put out like a bold too too strong opinion but I mean what you mentioned about kind of parsing out the search results this kind of brings us to our original Top This Is Why I'm so excited about the cohere re-rankers is like you know I read that paper lost in the middle that's about how you can't just give it like 20 search results and expect it to find the answer in the 20 and that's why I think the re-ranking having that top one is just going to be so powerful and yeah kind of you you mentioned retro and I I would love to get like to dig it a little further with you is that I so retro is the idea where instead of retrieving to put it in the input so like you retrieve text put the retrieve text in the text input and then Transformer you would retrieve the embeddings and then you put that in layer like eight out of 12 in the Transformer and I'm not sure I exactly understand the math but I think it's something like you know in that you can like transpose the the key and that query key value kind of attention multiplication such that by retrieving the way that you could you could put like a thousand embedding so you put like a gigantic amount of memory kind of in the middle of the Transformer so so yeah like I'm not sure like um I think the question is like is that maybe a better way to do long context I think might be a way of thinking about it yeah it's set sets yeah it's a challenging to make predictions [Music] um I mean yeah long context um I mean yeah we have these memory networks they were really popular in 2015. that's lstms so there was like a big hype on memory networks and inputting stuff in memory and and scaling memory which also didn't share to be a realistic so I think kind of yeah we we need a mixture so so in Lim pre-training having like on the one side like Close book is good so so it's the same with with let's say students um if we're educating teaching people how to program um it's good to have like some sometimes like close books um tasks to say okay here's a function I don't know written a CSV file transform the data write out a Json um without looking at stake overflow programming references or I don't know asking co-pilot to do this so this really helps students to learn understand Concepts and to memorize and it also makes sense in other settings to say yeah you can use stack Overflow or or python references to look for it you can use stake overflow for it to look for Solutions um but yeah in general for for many settings nowadays like really hard to find examples to say okay if if you give a task like um I don't know give give me all the prime numbers between 1 and 100. as an exercise to students yeah people put it into Google find it on stack flow overflow or ask me how I am get the code but it's not really preventing them from learning um so so I think for lmms we need something similar something needs to be closed book say okay here you don't have like zero uh references like like and say coding write me find me all prime numbers between 1 and 100 and print them as a list and do this as close book do like another step where you say okay you're allowed to search python um references so here's like the python reference handbook and I give you like some hints and say okay these functions can be helpful or make it like really um like yeah allowed to use everything and say okay you are you're allowed to use all the Google oil stack Overflow but then um yeah more on mostly goes here and says yeah I just copied the results so here you as a teacher need to be like really good and creative and find tasks um where there's not a solution yet and so that the student be like a human or a bit like no I'm still is able to learn something um uh I love that that I always love these like deep learning training algorithms that are inspired by human learning I've never before that thought about the distinction between when you learn something when you learn better by having a be closed book versus open book it's extremely interesting and novel thing I've never heard before um yeah yeah it's just really interesting I love like curriculum learning and I kind of think generally the interesting thing is kind of the extensibility of rag sort of like you know right now the way we do rags with llms is you know the llms are trained with language modeling but we just augment the input data and it works that way and I think with retro there's another paper called memorizing Transformers that shows we can and I think retro is like you can retrofit I think that's like what the name means that you could take an existing LM and extend it with that layer eight attention so yeah that's all super interesting so yeah also so I think kind of an anchoring podcast is you tease that maybe you had a bit of an announcement but not too much and if we could talk about this kind of I think we've already sort of talked about it a bit in the podcast but this kind of foundation model for search yeah so um as mentioned search still has many challenges uh long documents temporal information multiple fields multi-modality uh popularity recently so so yeah my team is currently working on like New Foundation model for search to really address all these aspects yes it looks really cool uh so looking forward to a talk a bit more on this in the future I really yeah we want to tackle most of these problems so obviously there will be challenges going forward and we hope to to do like a leapfrog jump like in this this space yeah stay tuned for for more and more announcements on that yeah that's of course we're exciting and I I really want to also ask about kind of you know like I think cohere offers fine-tuning embedding models and I guess kind of my thinking about how fine tuning Works generally is that the better the foundation model the more effective fine tuning and then there are all these questions about like what are you paying for like some kind of guarantee of model performance do you should you get to have the weights or just like have the API host all these kind of questions and so I'm curious like with New Foundation model for search and then you know will that be able to cover will you not even need fine tuning because I also feel like kind of the sentiment on fine tuning is like some people hate it some people love it um in general I think we we try to get away with as little fine-tuning as possible so so fine-tuning always in the past means you need a lot of data and yeah no one wants to create like a lot of data um it says for example we had like one customer says okay we love search results that contain a table because tables are really relevant for our users so like a markdown table [Music] um how can we get them at position one and with fine-tuning it would be like embedding models yeah annotate 10 000 examples where you search for where you have like two results posters relevant but the one with the table should be more relevant and then fine tune the model where obviously they say yeah no way sorry we're not Google we're not able to create like these 10 000 examples so so one one way is how can we put it into the foundation model that we give like these instructions for Search and say okay um if you find two results both are on the topic prefer the one was the markdown table or in podcast um first check is is the content relevant to the search query and then um prefer like recent podcasts and popular podcasts over non-reason non-popular podcast um and the other aspect is like how can we still include these preferences or get the preferences quite quickly and encode it in the model because preferences can be different depending on the companies also a new search you extremely care about recency if you are run an archive okay here like historic news articles you don't care so much about recency um recently can change on users some users say okay I'll just listen to podcasts at most months old I would say no it's super interesting to also listen to podcasts 10 years old can depend on the setting like in sports yeah no one listens to like 10 years old spot podcast focused on Sports but in other like let's say art I don't know True Crime you say yes it doesn't matter if the podcast is like 10 years old or one month old it's still True Crime it's a nice story um yeah I care more about like popularity and then is there like a good personal fit and so this is like going forward big your working block fast um how can we enable non-expert users to really steal the model first approaches was the prompting that you are have like some easy way to communicate it and then going by yarn prompting have efficient ways for your show like 50 examples 100 examples and then based on these hundred examples the model is able to to understand you um yeah I think that's always been the goal of deep learning is trying to learn with as few examples as possible because of the painstaking effort of collecting big data sets I love that kind of maybe language models could generate synthetic queries from your documents if all you have is documents of that angle I I love I mean I think it's been one of our number one topics throughout this whole podcast is how do I boost based on does it contain a markdown table and you know maybe the again the ranking models could take that kind of prompt of like please if it has a markdown table or if you've extracted that metadata where you have like uh contains markdown table true and you know you promote those results so so yeah all of it it sounds so exciting Nils thank you so much for joining the Wii VA podcast it's you know so great for me to learn about how you're thinking about all these things I'm sure all our listeners will love this and yeah thank you to our listeners and everyone at the wevia team that enables me to have conversations like this this really helps my education and knowledge of deep learning and search Nils thank you so much great thank you so much ", "type": "Video", "name": "nils_reimers_on_cohere_search_ai__weaviate_podcast_63", "path": "", "link": "https://www.youtube.com/watch?v=KITxQzV97jw", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}