{"text": "Hey everyone! Thank you so much for watching the 57th Weaviate podcast with Charles Frye! Charles is an educator at Full Stack ... \nhey everyone thank you so much forwatching the wevia podcast I'm supersuper excited to welcome Charles FryeCharles Frye is an educator at fullstack deep learning and Charles issomeone who I've been following alongwith just for me it feels like my entiredeep learning career I've always beenseeing what Charles been posting onTwitter and I always feel like I've beenlearning things from Charles so I thinkalso just the timing of this podcast isperfect because you know full stack deeplearning and we have all these new toolsVector databases large language modelapis what has happened with zero shotlearning and I think this is going to besuch a great podcast of covering whatwhat is full stack deep learning nowlike ml Ops and then full stack like howwe usually use the phrase full stack andsoftware engineering and how has allthis evolved so Charles firstly thankyou so much for joining the podcast yeahthanks so much for having me Connor andthanks for that really uh really kindintroductionmaybe uh before we kind of dive into itcould you give listeners more of abackground on just like how you cameinto deep learning and sort of yourpersonal Journey yeah yeah I came intoum came into machine learning and anddeep learning through a path that'sincreasingly deprecated these daysum I did neuroscience and biology firstum in undergrad I did some research onuh giving people MDMA and asking themhow they felt and shooting lasers intoMouse brains to see what the patterns intheir biological neural networks looklikeum so uh but then when it was time tolook at doing a PhD I looked around andit seemed like the ml world was reallypicking up this was 2014 or so so uhsort of the early applications ofconvolutional networks to really hardcomputer vision problemsum and that looked like it was going tobe like successful in the coming decadeand so I did a pivot and did a PhD inthe Neuroscience department at UCBerkeley but studying artificial neuralnetworks studying initially some likeapplications to neuroscience and theneventually kind of the mathematics theoptimization of neural networks and thenfrom there from the PHD at Berkeley havegone on to kind of end up in this roleas an educator in the ML and deeplearning space sort of sharing ideasresearch and how and sort of combiningthem with ideas from engineering and thedevelopment of applications initially atweights and biases it's kind of like aeducator developer relations kind ofperson uh and then now at full stack uhteachingum you know teaching peopleum most recently about how to buildstuff with large language modelsyeah amazing and I think um Also toclarify for listeners full stack deeplearning is kind of the key conversationtopic and also the actual name of thiscourse which is awesome and and yeah Ithat is coming from a PhD inneuroscience at you know a school likeBerkeley and then coming in that's justso impressive and I think it's alsointeresting I I mean maybe I'd like tostay I think this would be a greattransitioning topic because the yourbackground in weights and biases andexploring convolutional networks the wayI understand weights and biases and I'msure you're gonna you know help me withthis is like this kind of hyperparameter tuning because I remember inthe early days of training convolutionalnetworks like the learning rate bashsize how many layers the number ofhidden units all of this was sosensitive and it was about finding thishorrific combination of these thingsmaybe I'll quickly explain a little moreabout your perspective on ML Ops at uhweights and biases yeah um so definitelythat was my view of it when I kind oflike first came across itum that was something that really stuckout to me this sort of like hyperparameter sweeping stuffum but as I like worked there and sawthe product develop and saw the usecases for ML develop like the thing thatreally really mattered is monitoring andobservability forumfor experiments for the likeexperimentation phase of building an mlpower product I think they've likeexpanded since uh I left like two yearsagoum and they recently announced aproduction monitoring beta so it's notjust for thatum but the thing that I saw was thislikeum with you know a small amount ofinstrumentation a few lines of codebeing able to get like system metricsmodel metrics all that in one placealongside with like what code was Irunning I remember I ran into adistributed systems like Heisenberg likea bug with um a bug that sometimeshappens and sometimes doesn't uh becausedistributed training is just notdeterministicum or is not deterministic unless you'vespent all of your time engineering it tobe soum and uh and ran into what turned outto be a a Heisenberg a random bug but Iwouldn't have believed it unless I couldcompare two ways and biases runs rightnext to each other this was the state ofthe disc this was the state of the discthey're identical one has the bug onedoes not have the bugum so that kind of like that's the sortof like software engineering like bugsmashing kind of side that bleeds intoall the way up to the hyper parametertuning being able to see what happenedwhat your uh what your configurationsthose hyper parameters where what thedownstream uh results werehmm yeah absolutely fascinating andwe're certainly diving back intomonitoring and observability the topicand distributed training the topic butlet's maybe uh let's set the stage forour listeners of um you know full stackdeep learning maybe currently I'm justyou know we're thinking about this kindof like data loaders Vector databaseretrieval augment to generation to largelanguage models how do you see kind ofthe most common you know end-to-endstack for buildingdeep learning applications yeah yeahthat's a great question so yeah so weoriginally had the you know full stackdeep learning course and actually yeahJosh Tobin and Sergey Kari have myfellow instructorsum put that together when they were uhgrad students at Berkeleyum and that course treats full stack asmeaning I can train the models and I canlike put them into production and thenmaybe close the loop and take insightsfrom production and turn them into uhstuff that is used to train the modelfurther so it's sort of likeinlike Andre carpathy's vision of the likedata engine and Tesla where it's likeyou get inside you like collect data bythe models being out there in the worldthat gets added to your training processin some way either as part of evaluationor directly as something you calculategradients withum that the full stack deep learningengineer envisioned in that class is theperson who can cross that divide or sortof go through that entire Loopum so being able to do enough you knowenough Pi torch and GPU training anddistributed training stuffum to work at a reasonable scale maybeyou know not able to implement uhMegatron LM from scratch uh but uh atleast uh capable of consuming thatum all the way up to putting somethingin a product and understandingum what makes it a good product versus anot good productum and uh that everything that comes inbetween that with large language modelsthe full stack developer actually startsto look maybe a little bit more like afull stack web developer where a fullstack web developer is somebody who candesign a user experience but can alsolike talk to databases and can talk toexternal services and so it can buildlike something that looks like a morelike a web application station and atleast for now when so much thecapabilities are in models that aretrained by somebody else available as aservice don't require fine tuning andonly require like adjustments toprompting or adjustments of program andcontrol flow around them the the role ofthat that the beginning part that partthat looks more like training modelsthat looks more like um you knowthinking about uh thinking a ton aboutPi torch pytorch lightning hugging faceuh that partuh becomes a little bit less relevantI'd love for it to all come backtogether againum in the coming like year or two as weget better open source models asfine-tuning returnsum as maybe even training from scratchin certain domains returnsum I think we'll see that uh that cometogether again and then the full stackis nowum uh once again all the way from thethe gpus to the databases to the userresearch uh and backamazing it that's kind of exactly whatI'm hoping to kind of figure out my inmy understanding is how this can comeback to that and how the the new thingscan still relate to the old ways ofthinking about ml Ops and so beginningwith the I remember a quote from one ofour earlier podcasts with Chris Dawsonhe this was back when it was just gbt3and he described it as it's the perfectMVP tool like minimum viable product youyou just plug it in and you can see likewhat this could look like because youmentioned like the data engine thatAndre carpathy describes to a lot ofpeople that's like that's a lot ofoverhead to just get running with thatright like so maybe we can just it wouldstay on this a little more like usuallyto start building something with deeplearning you had you were had a datacollection task like a wildly difficultdata collection and labeling task yeahyeah I totally agree with that I thinkthese Foundation models are clearlyextremely good at building demos uhbecause now my Twitter timeline is 90demo um but and then there's like a gapto get to to the final product and Ithink one of my favorite examples ofthis came from Nat Friedman talkingabout the development of copilot he saidthey like stood up like four differentlike distinct language models for codingyou know with ins inside vs code GitHubso four distinct products that like in aweekendum like they just tried they triedautocomplete intellisense they triedlike a chat bot type thing and theytried like a PR bot type thing andum they got all of them to like MVPextremely quickly and then there waslike an extended period of turning theminto uh like a sticky actual product andthe only one that actually made it thatfar was co-pilot initially they'rethey're going further with with copilotX nowum and I think that's that's likeillustrative I think it's kind of commonin the development of applications thatthat mvp is easy and the rest is hardwith Foundation models I think part ofwhat makes that eventual developmentinto user product hard is thateventually you're gonna get have to getto the point where you have unique datathat is driving unique Model Behaviorandfor a long time maybe there there werefewer even demos because it was harderwith the original gp3 API to likeinclude contextum and the patterns hadn't reallydeveloped and now that we have thisclear pattern for injecting into promptsvia retrievalum or uh injecting into the model'sGenerations via tool use we now haveways to with again without trainingmodels and without really needing tohave to set up a full model creationpipeline uh let alone like an actualdata engine you can get a much betterproduct and that starts to fill in kindof the middle area between justprompting to like training a modelentirely from scratchum you know based off of uh proprietarydata or or data specialized to yourproblemum so yeah I see it definitely see theemphasis on the kind of beginning partbut it's also still very much early daysin the development of applications wecould find that there's just the rightpatterns that lead to promptedFoundation models building the the wholeapplication themselvesyeah I think from the first example yougave with trying out four different codecompletion models I think that's thestage a ton of people are in right nowwhere you you're trying MPT 30 billionalpaca you know vicuna you're kind ofrotating out these cheaper inferencemodels which one of these will work formy uhit was four it was it was differentproducts right the same models werebeing used underneath like say modelfamilyum and of course they had a problem ofdeciding like which model to choose andthey ended up going with a small fastmodel for autocomplete because an autocomplete pick a the way copilot works itdoesn't matter if you're wrong becauseusers will just ignore your suggestionsand people are used to ignoringautocomplete they're used to ignoringintellisenseum whereas if you do it if you're a chatbot like people don't like being toldwrong thingsum if you're doing a PR bot people don'tlike reviewing PRS that turn out to belike fatally flawed uh and so the likequality needs to be really highum and so that was that was their modelevaluation problem that they had thereum but that's like nested inside theselike like what even product are webuilding are we building this um uh arewe building a are we building PR bot arewe building uh um a autocomplete uh andthat you know key takeaway is the MVPsfor all those were easy the productssome of them are still under developmentyears lateryeah it reminds me of like uh like theearly days of like when Jasper startedgetting popularity and I think Facebookthey started having papers where itwasn't about just the the writingassistant wasn't just about writing moretexts about like editing your text andwhat you mentioned like how exactly dowe want these language models tointerface and and I think that's thequestion now with Lang chain and yeahand that's was being so something yousaid earlier I love this quote you whenyou said uh unique data to achieveunique Model Behavior and then wediscussed uh better retrievers and youknow we're getting into of course how weVA interest is this like retrieval tothe large language model and so this isthe kind of the question that's on mymind more than anything else is toachieve unique Model Behavior with ourdata we've plugged it in we've we'vefigured out the user interface we'vestarted collecting data should wefine-tune the retrieval models or thelanguage models or bothyeah that's a great questionum fine-tuning the language modelis a little bit more challenging and Iwould and there's a little bit less togo on in terms of what it means to makeit better like you might need userpreferences you might have some likecases where you have specific behaviorthat you wantum but you know even despite theadvances in efficient fine tuning Istill think that's going to bechallenging and then you have a modelserving problem uh it's a lot easierwhen there's only one version of a modelum much simpler version systemum but for so for retrieval I think ofthat as like a very broad problem rightthis is like a search or retrievalproblem where there's data that lives insome big slow thing like a disk or adatabase and we need to load it intosome like very memory sensitive placeum and that memory sensitive place isthe like context window of the largelanguage modeland so retrieval via embeddings is onevery good way to do that it's a good wayto do that because retrievinginformation from a vector is how aTransformer computes and so it's likelyto be the case that the things youcalculate with uh with Vectorsimilarities with DOT products uh areare likely to be uh similar to what thelanguage model will do internally uh butthat is just one of many ways to getinformation out of a large store andinto uh the small hot cache uh where itis needed and so yeah so I see that aslike probably the first like the zero ifplace to start is like your data yourpre-processing as always nobody wants totalk about it but that's the place whereyou should spend your time how do Iparse these documents am I pullinggarbage out of my data store uh so justlike classic elt and data validationproblems uh so definitely that's likethat's step zero and then it's like lookat the retrieval process of thatincrease that's increase its qualityprobably maybe it involves passing somegradients through a retrieval model buthonestly I think you can do a lot withlike metadata filtering and and otherclassic information retrieval techniquesand then those things uh that improvesthe inputs to this Downstream uhDownstream modelyeah definitely a lot to unpack I thinkum maybe if we can just stay on the thelast topic of you know the metadata andhow much you can bootstrap we when weoriginally released hybrid search wherewe're combining say the zero shot openAI models with bm25 and that maybe youcould also say re-rank that with a zeroshot cross encoder which could also kindof push it a little further and it'sit's like yeah maybe you can just kindof get pretty far with these zero shotmodels when you orchestrate it like thatespecially because now the largelanguage models you can kind of promptthem with the filters that they can addto make it filtered Vector search or youcould just entirely separate your dataout into different indexes and ask itlike which one to search and llama indexthey call that the composable indexesandyeah yeah so there is kind of maybe twodistinct patterns one is retrievalaugmented generation in the like moreclassic sense feels silly to callsomething classic that's only like a fewyears old but we'll call it cleverum but yeah there's like a classicversion of it which is loadinginformation into the Transformer at likeboot time you might think of it likeyou're starting to run your Transformeron its uh on the on the context it'srunningum and you're just saying what do I needto load into that things like Ram oncethe program starts runningum so that actually it's really more ROMwhat do I need to load into the ROM ofthis thing for it to run like this islike Commodore 64 style like I'm justlike I'm punching the tape and sayinglike you know this isum you know this is the documentation ofmy of the library that I want you toanswer a question about and that isum the distinction there is that partwe think of that as like separate fromthe behavior of the language model andthat it so it's amenable to likesoftware 1.0 or traditional ML and IRtechniques and then the other directionto go is to say no language models arelike reasoning engines language modelsare executors of of like complex programflows and retrieving from informationstore is just the program flow so likeyou know make it this this me this islike retrieval via tool useum and I can see there beingapplications where one or the other ismore valuable I think the moreconstrained your application is like theless open domain its questions are andthe more straightforward the questionsare to answer once you have the contextthe more it looks like peer retrievalaugmented generationum and so like for the main littlechatbot uh uh retrieval augmentedgeneration application that I run whichanswers questions about our coursematerial and about language modelingpapers uh the goal is for it to just bea Searcher of that stuffum and we and so the the open-endednessis a little bit less and so there's lessneed to be like oh like tell me which ofthese like 50 possible tools and mayberun a Google search to get informationEtc but for very open-ended maybe thingsthat are starting to tickle the edge ofbeing like an agentic system then youmight want something where retrieval isjust one of the many tools that thelanguage model has access toyeah I think that's just completelybrilliant is thinking about retrieval asa tool use compared to well so I thinkto break this apart a bit I think youmentioned several times a Transformersattend over vectors and you know I hopeI think our listeners probably most ofthem are aware of neural architecturesand how they work generally so you knowwe have say 12 layers of Transformer andso one idea is instead of justretrieving to put it in the input youretrieve vectors to put it in say layer10 out of 12 or 8 out of 12 and and itdifferentiates on the memory you couldyou could have ideas where youdifferentiate through the retriever oreven just you keep that as semisemi-parametric and you just you know gothrough the one one to twelve with thewith it injected into eight so so thatkind of architectureto me that diverges from just retrievalas tool use just because I don't see howany other tool could be put in themiddle of an arc and and then I thinkit's extreme like yeahyeah yeah I would say that the that sortof retrieval augmentation that is bakedinto the architecture which goes back toat least retroum for this crop of of language modelsand um I'm sure there's some uh Burtpaper that that did it way earlier andand there's a Schmidt Uber paper from 93or whateverum but yeah but that style of doing itwhere you are tightly coupling theinternals of the model and the retrievalsystemI would believe that for narrow tasksyou can you like you can differentiatethat and then like get really highperformance for really low model sizesand really quick latenciesum but the that like tight couplingthere makes it harder to be like oh wellnow I want to do a Laura fine tune it'slike well how does the Laura fine tuneinteract with a retrieval thingum and it's like oh I want to useshortcut Transformers I read this thingabout shortcut Transformers where youcan stop four layers in and then likepredict the next token immediately andit's like oh well how does that interactwith it so like the more you start toadjust the internals versus treat thisthing as an interface where you uh whereeither you're interfacing with itsinputs or its outputs like are used asinputs to your interface the the moreyou like break that abstraction andstart injecting into the internals ofthe model the more Fragile the systembecomes the like harder it is to adjustand right now at the very least there'sjust so much open field in terms of uhwhere where you just think in terms ofthose external interfaces so for like Ilike until I see like many papersdemonstrating or or many applicationsdemonstrating that that's like gives you10x 100x quality at decreased latency orsomething I would strongly recommendlike not not not opening that Pandora'sBoxum uh and sticking to the interfaceyeah it would have to be produced bysomething like an open AI I mean I thinkjust the last two days we've seen twomassive fundraising announcement I thinklike um I think recalabs inflection AIraised like 1.3 million which is funnyenough the exact same numberso that's kind of funny and so yeah itwould have to be one of someoneproducing a foundation model that comeswith the retriever end of it and yeahLaura early evening definitely topicsthat we're going to come back to butfirst I want to stay on this tool usekind of concept a little further so Iwas in uh Berkeley as well last weekendjust didn't go there just I went thereto see it and I and I met the authors ofgorilla and I thought gorilla gorilla isa large language model that's beenfine-tuned to use particular tools soit's like um it's still black boxy youknow like it's not we're not talkingabout putting the latent space of thistool into the middle of the Transformerbut we're talking about giving it someexamples of how to say how how to Surgeor more like how to use like GitHub CLIto like that's one of the examples thatI was just like wow that's pretty coollike this whole like um where the toolis to execute code or to file a pullrequest or to review another pullrequest like pulling all those levers II find that to be so fascinating so thatkind of idea like I remember web GPT hadlike search actions kind of like youknow discrete black boxy kind of how touse it what do you think about that idealike fine-tuned to use tools kind ofyeah I mean I think that we have alreadyseen it like you know if you ask me thatquestion a month ago I would have saidoh geez what we really need is a highlycapable Foundation model that has beenfine-tuned on some kind of tool usepattern and to be honest what tool usepattern would I like I'd like somethingthat looks a little bit like an open APIspecum so this is something I spent sometime working with um the folks atSpeakeasy who do API tooling developmentand and um uh just sort of like helpingthem understand the llm landscape andwhat that could mean for them so I gotsuper into this a couple of months agoand learned a bunch about the sort ofAPI tooling world but it's a way toclearly describe what is available viaan APIum if you've ever used Fast API andyou've seen that little like free docspage that you getum if you use gradio for a long timethey also showed you that docs pageum that the you know it's the likebrightly colored thing with likedifferent types of requests and pathsavailableum it's like it's such a well-specifiedthing that you can build automatedtooling off of it uh and so like I wouldhave said uh a month ago like yeahsomething that's like close open APIspec as like how you describe your toolsuh and then like the model knows how toproduce a request that goes into thatAPI and so you fully separate out theexecution of the tool from the model andyou train the model how to work with thewith that specification which means youaren't training it how to use a specifictool right like tool former and Tom bothkind of are like oh yeah let's like youknow let's fine-tune some model to likeknow when to use a calculator and knowwhen to search Wikipedia right and reactuh as a pattern is also like let's usefew shot prompting examples to describethe format for a tool uh and then themodel will try and pick it up and bothof those approaches work okay butthey're not extremely they're not superflexible and like what you want is theis like like a flexibility of tool usewith model fine tune right so react hasa lot of flexibilityum at the cost of of token budgetsum and then a little bit of uh reducereliability like a tool formery patternis going to be like extremely reliablebut you have to have examples you haveto fine tune on a specific tool uh sofine-tuning on a tool specific on likebroad examples from a tool specificationthat happens to also be a spec broadlyused by people to describe services thatthey are making available seems like theright move and with Json schema poweredfunction calls open AI has done exactlythatum and so there's like an incredibleopportunity here with tool use that Ithink you know we're just beginning toscratch the surface of even the peoplewho have been thinking about this likeconstantly since it came outyeah that was such a great tour of all Ilove how you brought back tool formerthat I think if you're mentioninggorilla it's good to cite that one aswell and then yeah the react tool useand um maybe I so yeah the open AIfunctions thing you mentioned yourexperience in open in the open API specsand I I do find that whole Jsonformatting it functions the universalinterfaces and what openai'saccomplished with that I really likethat but if we could transition just oneI want to talk about a specific kind oftool which is this kind of like text toSQL kind of idea where you would finetune a model to generate SQL queries Ithink just if we could stay on thislittle microcosm of just this oneexample and think through like is to meit sounds like and it's something thatappeals a lot to wevia and is like youknow like having a say there's thispaper called tiny stories where theheadline of the paper is how small canwe make these models and they still areable to talk to us and so it's like ifwe could have a you know a 30 millionparameter model that we could just wrapup in a Docker can container served on aCPU I think you could probably Onyx andmakes neural magic make it run fast butlike that kind of thing of something wecould put into eviate and I'm sure thereare other examples out there and otherkinds of softwareyeah yeah I definitely thinkumyeah so the the flow that I justdescribed is very much something thatyou need a large scale model to handleum because you're going to be picking upsemantic information from the naturallanguage in a spec it's sort of likeit's like it's reading documentationwhile also kind of like parsing thecontents and then we're generating thisuh like in this particular format and weprobably also want to be able to likeinteract in natural language as as partof building those things yeah so that'svery like large very capableum uh language model territory So youyou're expecting to get that from afoundation model provider uh so withbut if you have a specific domain whereyou want to uh where you want to use alanguage model but you want it to likesort of interact with symbolic softwareuh more directly than you yeah you getand that that domain is fairly narrowyou're only generating SQL you're onlygenerating python codeum then maybe you can get away with muchmuch smaller models I would worry alittle bit aboutI think if you took an extremely largemodel and you somehow collected atrillion tokens worth of SQL and naturallanguage paired with SQL it's likeyou know interesting to think what kindsof capabilities would you get out ofthat like could you just ask it to writeyou knowumto effectively like write your whole uhyour whole databaseum like as opposed to just like writingone one one queryum but yeah uh that's that's definitelythe the like narrower tool use caninvolve narrower interfaces than justsome like generic thing like open API orJson schema or whatever and can just belike yeah we want something that thatgenerates SQL will run that SQLyeah I mean this conversation is reallyinspiring my interest to run thisexperiment where you would like use gbtfor whatever to generate a big trainingdata set to then distill the SQL thinginto I guess like every input would takein the schema and the natural languagequestion you know like the kind of likespecial token separation like you dowith like Bert and stuff and yeah I meanI think because I don't know if we'vehad this kind of experiment where youwhere you scale the data without scalingthe model and then into a narrow domainI'd like to call that like generativedata augmentation some kind of phraselike thatyeah definitely people have started tolook at that I think there's like withinstruction fine-tuning that you knowwhich includes the like crop of uh SouthAmerican camelid namedum instruction following fine tunes likellama uh or sorry like uh alpaca vacunaguanaco Etc I think we've run out Ithink we're out of uh uh people willhave to find a new mammal to name thingsafterum but uhlike that cropif you look back even before that peoplewere looking at generating data with GPTto even gpt3um uh to generate instruction look likeinstruction type dataum and have found that to work likepretty well I wannaI forget whether the flan data set isconstructed in that manner it might itmight not have been but super naturalinstructions natural instructions andSupernatural instructions bothconstructed in that manner so there'slike a proof of concept with one of thesort of like biggest changes withlanguage models which is the switch overfrom like modeling texts to modelingresponses to instructions so I canimagine that that kind of like scalelike make a big data setum uh with a capable model and fine-tunea really small model I can see thatworking uh working pretty well in a lotof domainsyeah I don't want to get too off topicbut yeah when I first saw that like um Ithink it was called like decisionTransformer I think it's also a Berkeleypaper where you where yeah you um itshows how similar like medical treatmentdecision modeling is to language it'sstill like sequence to see the keydifference being the sparse rewardcompared to like the here's thesupervision episode but here so I thinkI want to stay earlier we had mentionedthis General topic of what I think isthe biggest question in full stack deepin like well I say like full stackretrieve augments or generation maybe isa better phrase for it but like shouldyou fine-tune the retrieval model shouldyou fine-tune the large language modelor both and what is this and so this isone topic as we're talking aboutgenerative data augmentation we'retalking about sampling data from largelanguage models to train smaller modelsso I love this idea where you use thelarge language model to generate apotential query for a document and thenthat's what you use to supervise thetraining of the embedding models I thinkit's so profound because you know Iremember in the early days of wevia wewere talking about like we were buildingsite search it was like a thing beforeno we have it now but before peoplewould troll us on Twitter and say likewe've made us new app search for theirdocumentationso we were like okay how are we gonnabuild this and one idea was like allright let's everyone write 10 questionsfor each of the pages right and that'show we're going to train our embeddingmodel isso so that kind of like it solves thedata problem of you know the languagemodel can write queries potentially yeahand I would say that it solves the coldstart problem for data is a better wayof thinking about it it's like your dataengine needs a cold startum and the language models are onereally good way to do that they mightyou know I'veyou know a lot of there's this likeemerging intuition about the highestcapability language models like ubd4 andClaude that they are like a um Highpercentile crowd worker uh it's like soso actually better than your mediancrowd worker on many tasks uh andbut with crowd workers it's not likepeople just like hooked up a directum data stream through Kafka fromMechanical Turk that just like spat outum data that went straight into tensorsand turned into gradients like you knowthere were systems around it and therewere and user data and like data thatyou purchased dog food data um like youdescribed for weediate those were alladditional sourcesum and so language model generatedcontent is an additional data source ithas its own uh issues like languagemodels really like language modelcontent so they might give you Rosyevaluations of your performance uh onthatum and I think you might lose you'regonna sample well from the like highprobability regions of documents atleast high probability regions ofdocuments produced on the internet thatend up in the in these pre-training setswhen it the like long tail of requestsis going to be really challenging I knowJoe bergamum at Vespa has talked about how there'slike the bulk of queries and the longtail of queries and the long tail ofqueries like is in ends up being nearlyas important as the bulk because youknowum in this in the same way it is forrequests right you look at your requestsyou look at your your you're you'rerunning an API on your request andyou're like 99 of my requests areresolved in 100 milliseconds or lessthat's great like all 99 of my users areseeing a Snappy website it's like no nohold on one percent of the time whileyou're doing some tasks in this websiteyou hit something that takes maybe fivesecondsum and a user like that even if it'ssampled at random if a user is liketaking many actions they're gonna see itpretty soonum and so similarly even though like thelong tailof potential requests is is rare thoughtof as individually it might be everyuser has a request that they make that'sin the long tail and so language modelsin generating data are likely togenerate things that look like the bulkof queries not things that look like thelong tail uh and there's some researchon this curse of recursionum uh that that has modeled this uhphenomenon and and shown it empiricallyand that's going to be a problem forgetting to that sticky really highquality productum and soI would suspect likeum actually fully solving the like thethe problem of needing data train modelsis always going to involve a data Enginewith many diverse sources not just askgpd4 to generate you 100 new data pointsyeah I love that and also that curse ofrecursion that's an awesome title yeah Imean it itum I give a quick shout out to a previewa previous podcast with Professor LauraDietz on perspectives of using largelanguage models to annotate relevancejudgments as an awesome paper and alsoI've got this Haystack cup here Iunfortunately I put it in the dishwasherand that suck it down a little bitbut but they have this tool called Cupidfor uh labeling query it's just likesome interesting things for peoplelistening maybe it could be interestedin referencing it but yes this long tailthing this is so interesting and I'dhate to kind of I don't want to get toooff topic with generally this kind ofretrieval language generation thing butlike I've kind of thought of like classimbalance and the inability to performwell on minority sets of data is kind ofthe Crux of self-driving cars I know Iknow I'm pivoting topics yeah but likelike do you think like like I I'vestarted to think that self-driving carswith deep learning will never berealized because of this problemyeah I definitely have entertained thathypothesis in the hypothesis in the pastand now you know there's probably avideo of me on YouTube Somewhere sayingthatum but uh there's two things there oneis that in you know empirically I canget in a cruise automobile and take aride acrossum across San Francisco with no driverit like I can pay money to do that andso it's kind of happeningum and there's lots of deep learninginvolved at lots of different parts ofthe stack there's also lots of expertsystems involved butum you know and lots of traditionalcontrol and and linear quadraticregulators and stuff but but there'sdeep learning in there for for forespecially for perceptionum so so it's it's it is happening it'sit took twice as long as people expectedtypical for an engineering projectum and took a lot more Capital infusionthan than people expected but it seemsto be happeningum I would say there are still long tailissuesum and people complain about behavior ofthe carsumand thatislike resolving that is still a hardproblem I would say that language maybehelps a lot with this long tail problemin that um I don't know if there's likea figure in the gpt4 white paper fromopenai that shows how multimodalityhelpsum and it's like asking it to describethe content of a picture and the pictureis of a man hanging off the back of ataxi cab doing ironing and if you are aself-driving car and all you ever do islike consume lidaryou will drive many many billions ofmiles before you encounter a man hangingoff the back of a taxi cab doing ironingand so like you or or even yeah like youyou might not even encounter any ironinguh you know for for billions of Miles uhbut if you have a if a component of thestack that's that's part of theperception is trained on a broader dataset like the breadth of the entireinternet hundreds of billions oftrillions of tokens then it has a chanceto find out about all these smallerConcepts that can be composed togetherand also to like knock out further intothe tail the things it has never seenbeforeum and so yeah like that that I thinkwill help with that kind of problem Ithink thereum are problems with using really largeTransformers uh you know in a uh in acar from from power consumption andum uh to like latency and all theseother things but at least that feelslike an in principle way to um to help alot with uh the long tail problem as itappears in self-driving carsEFS and I definitely could talk to youfor a while about self-driving cars it'sso interesting but let me kind of bringthis back to kind of Surge and likepeople building chat Bots and thingslike that with our thing is like I guessI think about this like a lot of theselarge language models they've beentrained on like you know internet scaletext or maybe like Wikipedia you knowthe breath is enormous but another thingabout the fine-tuning like you know ifthe you know full stack Discord is aparticular pocket of that space rightlike it to me just the intuition is thatit would make so much sense to fine-tuneit to put it over into that space yeahyeah definitely I think um the Laurapaper is truly incredible the low rankadaptation paper from uh from Microsoftresearch and it's like like people areaware of this like fine-tuning method umbut they like and they're aware thatit's good and they should use it orwhateverum but I feel like people haven't seenthe paper has a bunch of really greatinsights in itum and one of those one one class ofinsight that's in it is thatum they actually like analyzed what thehell is going on in these fine-tunedlayers like what are they doing and whatit looks likefrom what they have in that uh in thatlittle uh more sciency section it lookslike what they're doing is reorderingthe like eigenvalues of every Matrix sothey're taking like Loosely speakingthey're taking the concepts that thelanguage model already has so likeoperations language model already likesto do and re-ranking them and saying noactually this behavior is reallyimportant and it's behavior that we usedto think was important is less so likelet's shrink our behavior in OneDirection and like increase our behaviorin another Directionum so like more so than say likediscovering an entirely new direction tofocus inum and that sort of explains how you canget really profound changes in themodel's Behavior without necessarilyincreasing their intelligence orcapability which is what we've seen fromlike uh Lima and the false promise of ofimitating uh proprietary lens paper thatyou aren't necessarily making and eventhe guanica paper like you aren'tnecessarily making the model that muchsmarter by fine-tuning it you're justlike changing the manner in which itproduces outputs you're revealing thingsthat it sort of was already capable ofdoing that was in the probabilitydistribution over the vocabulary at theend that was present inside of those uminside of those big residual streamvectors inside the model but was likerelatively down weighted because itwasn't important during pre-train or wasit important during rohf and supervisedfine-tuning but in your uh finalDownstream fine-tuning case it's superimportant to uh to like you know usethis particular style of diction or toum like refer to a particular noun andso we can just sort of pull those thingsout and reduceum uh relatively other thingsyeah it's amazing it's so there's thereare kind of two things to that I want topick apart I mean quickly there'srecently we had the voting podcastStephanie horvachesky and gunjan butterare you where we talked about how youcan put image embeddings into the latentspace of like level lens and somehowthat works like somehow it's able to todo that and like mentioning theeigenvectors so maybe for peoplelistening you know weight Matrix youtake that Matrix and you decompose itwith singular value decomposition youhave like USV that kind of thing andthen the eigenvectors like that diagonaland like the rank of it is like how manyso like the diagonal of this eigenvectoris is like uh how import how theimportant components of it I you knowthinking about projected and residualcomponents is always ayeah yeah I I'd say the like the likeshort version of that um the like verysoftware engineering version of that isthat the eigenvectors Define theindependent things going on inside of amatrixum so you could think of them as likelike a bunch of asynchronous operationsall going onum and because this is linear algebraeach one has a different like weight onit a different importance and so thoseare those are the weights of those uh ofthose eigenvectors actually you know forthe real math nerds these are allsingular values and singular vectors soI'm not committing a math crime uh hereuh they're they're real valued um andand maybe even positiveum but uh yeah so they each of theselittle asynchronous procedures has animportance weight attached to it andwith when I say that we're likere-ranking them I'm saying that we takesome like one of the processes was likesimulate what Homer Simpson would say inthis situation and that's not thatimportant in web pre-training becauseonly a small fraction of documents onthe web have Homer Simpson in them butyou do still need to learn it if youwant to get to the quality of languagegeneration the gpd4 has but if you'refine-tuning the model to sound likeHomer Simpson all of a sudden thatprocess becomes really important and soyou increase its weight you increase itsrankum so that's a a hand hand wavywhiteboardy uh uh software Engineersapproach to understanding what thatmeansyeah it's so there's definitelyapplications of this in Vector search aswell like you know there's this papercalled hsw finger and it looks likethere is a lot of opportunities for usto look at like you know projectingvectors onto other vectors but I do stayup like I I've always thought that likedisentangled representation learning wasjust like an unsolvable problem likeeven if it's the eigenvector where youyou know disentangle the components ofit II I don't know like because sometimespeople when you're explaining Vectorsearch to people you'll say like thisdimension of the vector says how much ofHomer Simpson is this one says how muchof this it is and it's like no that'sactually it's totally like entangledacross the vector yeah yeah and so yeahthe um the goal of eigenvector Gcomposition is to like find a basis inwhich you know to sort of rotate thething around so that you can actuallysee oh this is the Homer SimpsonDimension and this is the plural versusnot plural Dimension or whatever andthere's been limited success in doingthose kinds of things to understand theinternals of Transformersum but yeah definitely uh individualneurons individual components of thosevectors don't seem to have a huge amountof like independent meaningum and honestly I don't think you wouldnecessarily want that to be the case youcan if you try to store information inthose like n Dimensions you're stuckwith the N dimensions of your embeddingspace and that n is pretty large for alarge trans former I forget what it isfor gpt3 like low thousands probablyum maybe 10 000. but like real like it'sa it's that number you and you're andyou're and it's the amount you can storeif you pick different dimensions tostore different things is linear if yourelax that and say I don't want to havelike n completely orthogonal directionsum I just want them to be like mostlyorthogonal to each other I want theirdot products not to be exactly zero butI'll let them be likea little bit different from zero um andthat I'll still consider that or yeahlike I'll still consider that likeum those two unrelated vectors now allof a sudden you have exponentially sizedmemory storageum and so it's like what people havefound from looking at Transformerinternals is that's what they're doingwith that with that Transformer uhresidual stream there's like you know ifyou want to know what's in it you haveto look at every Dimension and do like aDOT productum with some query rather than lookingat some specific Dimension to figure outwhether a piece of information is liketrue or false or what value it hasyeah I think without a question there'smore like that it's so interesting andsuch a great topic but I think like ifwe could pivot into maybe explainingLaura a little more and how low rankadaptation works so again so you cantake the weight Matrix and you candecompose it into eigenvector like youcan Recon construct The Matrix from thedecomposition and so the idea is you canjust update these uh the low rankprojection with the fine tuning and thenyou and then you have like this sparsefind and it's like the latest update insparse fine-tuning where to update likea 12 billion parameter I don't know whyI said 12 like big model to you canupdate a subset of it and then if abillion needs to count as as big youknow even though like Megatron Ln paperI want to say is like low 10 billions inscale but now that's like oh that's likea small Fun open source Model 12 yeahum yeah I think something that you hadtold me about in our brainstormingsession is this and something likesomething for I think more casual likepeople who are just kind of hackingtogether applications with alleviate inthe opening API in this idea that if theLaura fine tuning thing becomes moreaccessible one idea would be if we havea Discord chat bot you have a separateset of weights for Connor and a separateset of weights for Charles you know andso on what do you think about that ideacan you tell me more about it yeah yeahit's really cool I think we don't havethe infrastructure place to do that likeand there's a bit of a bearish so a bitof a bit of a bearish signal on thatfront if you look at the um imagegeneration world uh which uh I I youknow I spend a lot of my time thinkingabout the language modeling Worldum and that you know that's what I'mspending a ton of my time and thoughtbut I recently built like a little QRCode art generator and so I like checkedout what's what's going on in stableFusion world andum they have been productizingFoundation models for way longer thanthe language modeling world has becausethey had their moment of like broadaccessibility earlier right we stillhaven't quite had ourum stable diffusion moment in languagemodels uh which is to say like anextremely highly capable model withcompletely open weights that can be runon consumer gpus at like and all theseuh like all these things we're stilljust a little bit short we need like umwe need something as smart as textDaVinci 2 but that you can run on like140 GB cardum and so maybe even by the time thispodcast is released that'll already betrue like who knows it could happen anysecondum but yeah so so in the in this worldthey people have been people who havebeen building stuff and they havediscovered different uses for fine tunesand like control net as a way of likemerging multiple fine tunes together butbut so far as I can tell they don't haveanything that looks like that dream thatyou just uh put out there of like I haveone set of Weights that's very smallmegabytes in size that adjust thebehavior of this language model for oneuser and then I have another one equallysmall that I can hot swap in actuallywith like a Cuda operation it's justMatrix math to swap out a Laura finetuneum and then so like on the Fly switchover and now I'm doing it for adifferent user different user Persona adifferent region whatever your yourapproaches for segmenting out your finetunesum that doesn't seem to be the waythings work it seems like mostly peoplelike download the entire weights forevery model that they want to use andthen like even if they are related bysome lineageum so it's a bit of a barrier signal forwhether this is going to actually getadopted in the language modeling worldyou need some kind of like shared mannerof expressing where these weights shouldgoum and and making it possible to likeget merge uh like one fine tune withanotherum or uh yeah or all these things thatyou would want to make it really umuh really good so like right now thereare probably people doing thatinternally who've built uh some Jankversion of that but there's no like openstandardum that everybody can use that thatsolves that problemyeah I I I guess I think like there'scertainly potential for that in thefuture I guess for now we're seeing likeyou kind of do like role playingprompting or like personalized promptingwhere I'd be like here's an example ofhow Conor writes his emails kind of andlike or like you know kind of likesbasketball could you tailor thisadvertisement to Conor and then it'slike hey come play basketball yeahprompting yeah as always prompting takeslike 30 seconds to try and gets you 70of the way there yeahyeah and it works like retrievalaugmentation Works super wellum and then like you know hammering downthat last uh bit then takes you know youhave to you have to move over to usingfine you have to move over to likeredoing the pre-training um yeah uhwhatever anduhyeah so that'suh like a pretty pretty typical patternum and so starting off with like peruser prompts is a good way to at leaststart seeing okay what do users needbefore they prefer a userum uh a user specific model to the basemodelum or what are the right ways to breakthis down like what information is userspecific versus what is generic andmaybe like you can imagine like ahierarchy of like you have individualusers all the way at the bottom you havea single model for all users all the wayat the top and then you have differentways of partitioning your users andhaving a specific prompt or fine-tuneum or or other kinds of differentBehavior at each point in that uh inthat power set of partitions and uh likeuh figuring out what the right ways tothink about that are with prompting is agreat way to save yourself a lot of painand time uh and any 100 timeyeah I mean I think it's kind of cominginto like it's like the the Grand Archtheme of our podcast is like you know mlOps full stack deep learning and I thinkthis is coming into this kind ofmonitoring and observability sort oftopic with respect to like uh embeddingspace visualization where like it's hardto just segment Charles and Connor youknow however many people because we'reso abstractly defined but we have thislike visualization of clustersand yes maybe you just set the stage howare you thinking about like yeah thiscluster visualization I think of it asjust a super powerful way for people tostart collecting data and then for morepeople who are you know software whodon't who don't come from like trainingoptimizing Cuda kernels like people whoare just trying to add it to their appsI think this idea that you can startcollecting some input output examplesstart labeling them on what I like whatI don't like and then maybe just seethose colored green and red when you putthose inputs back into the latent spaceand I think these kind of charts I thinkthat's a huge middle ground for goingfrom this zero shot rag stack we'retalking about into the you know the mlOps World the weights and bias all thatlike I think this middle part of thisVision that I think that could be a hugeevangelist for this yeah yeah I think umI personally have benefited a little bitfrom that like um I had a little uh forthe uh for the Discord botum I pushed just like pushed it throughT Snee actually using the like built-inversion of it inside of weights andbiases upload just like upload somevectors from the vector store with theirmetadata attached and then just projectthem look at them in 2D and itimmediately revealed that there was likea large chunk of data from some PDFsthat looks like it's like mangledUnicode bytes or like somebody likeincluded their latec program inside oftheir PDF in a weird way and so it'sjust garbage it's just like garbagedisgusting it's basically a failure ofmy like data loading and validationpipeline to not recognize that that wasnot suitable input and now because dotproducts are are pretty goodum like I'm not ever seeing that inproduction like I'm not seeing thosegetting pulled in and like mangling mycontext but I'm wasting money embeddingthose I'm wasting uh database spaceholding on to them and maybe there isgood information in there if I had adifferent pre-processing pipeline topull that out so that's like you knowthat that's the kind that like very baselevel Insight that comes from not evenfull-on observability but just the likebarest forms of monitoringum and the like bearish forms ofdashboarding and visualization there's aton of value to be unlocked thereum and that's yep that's definitely whatI saw when uh when working at weightsand biases that it's just like justbeing able to see the GPU utilizationreally easily and being able to be likewait a second it's like way lower thanit was three weeks ago what the helljust happened like that is will give youa ton of gains that going furtherrequires you to do a ton of additionalinstrumentation and a lot of hard workthat often is like not very long lastingand needs to be like adjusted and andhas needs like constant engineermaintenance and these like simple thingslike just take the vectors and tissuethem and look at them every once in awhile gets you a huge chunk of the wayyeah it's a it's just amazing like whenyou're curating data sets for languagemodeling you have so much data youreally have no clue what's in it so thiskind of yeah Unicode clusters and I'veseen that for myself it's pretty funnyobviously I I would love to transitioninto this topic of GPU utilization alittle more right admittedly know verylittle about it but this kind of yeahlike what maybe if you could just setthe stage with any topic that interestsyou and we could go from there oh yeah Imean I don't have that much superinteresting to say about it and now thatpeople are mostly using things viaum you know that that story and thatcomment came from uh my time awaits andbiases and at a time when there was muchless Foundation model useum and a lot more a lot more trainingand running things yourselfum so I don't know that I have too muchuh interesting to say there except thatif you've never run a profiler thatgives you like both profile level statsand critically like a trace viewum uh for a GPU accelerated job justlike stop what you're doing and like geteven just like an end this tutorial runit and then I you know ideally also amodel that you care about you haverunning uh in production or or thatyou're you're working on just to like ituh substantially improved my intuitionfor what pytorch was what Cuda was uhlike What GPU acceleration means forneural networks just to see thisum uh just see this trace of uh like ayou know like a flame graph type Tracewith uh for the CPU for the GPU in itsstreams and to relate that back to thekinds of metrics you might see in NvidiaSMI or in weights and biasesum it's uh something that web developersum like don't even know how good theyhave it like they can be running theirapplication in a browser they play likelike three buttons and all of a suddenthey have a perfectly recordedreplayable visual trace of of uh of ofwhat happened in their program and likeit's that's a similarly awesomeexperience for understanding the browserthe Dom the uh like the event Loop likeall the pieces that make a webapplication workum you can get a ton of insight fromlooking at those traces same thingapplies to uh to tracing your GPUaccelerated uh workloadsyeah that's that's been one of the fromjoining weeviate and seeing the insideof like a legit engineering companycompared to like my PhD that's been oneof the big eye opening things for meI've ever seen those like stacked Tracegraphs and thinking what is that or it'slike you're spending this much time onthe distance computation this much timelike updating the end like all this kindof stuff with the hsw so with GPUprofiling do you usually look at like Ididn't know about like idle uh like idletime between when you're kind of likesending the gradients and you're lookingfor the next like just batch of datakind of pretty abstractly but do youmaybe profile it like layer by layerlike how fine-grained does profiling GPUutilization get yeah um so theprofiling tends to be like here's theoperations that you ran like with somelevel of granularity here's how longthey took like on average and here'slike some some uh quantiles so that thatcan be useful but I find I I findand that is especially useful whenyou're like really knocking down likeand just continuing to go and like makesomething faster and faster and fasterso that's very much a post productMarket fit kind of like this is a reallyworking feature and we're now spending10 of our time running this workloadlet's make that let's try and get that10x fasterum we don't it hasn't changed in aquarter we don't expect it to changethis year let's spend some time on itum with tracing where you just see likea single or a small number of executionsthat is more useful as like a thing youdo anytime you're you're runningsomething because you can catch thesevery gross features that are likeunintuitive about the behavior of yourprogramum just like as an exampleum like I was Consulting with somebodyand they were like I'll take a look atmy deep learning stack and I umlike just quickly ran their modelthrough this profiler and it was likepart way through their forward passthe CPU stops the GPU stops there's thislike giant block of like something isgoing on not in pytorch but elsewhere inPython for like a third of every forwardpass and it turned out they weredynamically constructing a python classon the forward pass just like you knowin Python you don't often think thathard about like am I dynamicallyconstructing this class or am Iconstructing it ahead of time and thenlike you know uh like referring back tothat definition it's like normally youdon't have to like that's not slowenough that it slows down the executionof your program but once you're in a hotLoop like uh a model training Loop youknow 100 milliseconds mattersum 100 milliseconds can be an eternityeven if um you know depending on on whatyou're runninguh and so just like simply moving thedefinition of this class from like ohit's like dynamically constructed in themiddle of the forward pass to oh weconstruct it ahead of time and and justreuse that definition just like yankyanked that entire 100 milliseconds outand saved a third of like shaped 30 offtheir training timeum and so that's the kind of insightlike you can maybe see it at the levelof Kernel utilization which is the usualkind of metric people look at butdiagnosing it requires looking at tracesand profiles to be like oh aha here's asurprising like place where time isbeing spent and this is what's going onwhen those kernels aren't being utilizeduh that's is amazing I think it I thinkit's a really amazing topic I feel likethis is the thing that really separatesthese like super valuable AI companiesand I think a lot of like I think a lotabout training embedding models and Ithink kind of two perspectives of likehow you initialize the loss functionyeah like you might create a class theloss function is something that peopledo and with these um you know with thecontrast of loss and how you how youbuild that in I think also just like thethe data loaders and how you're going tosample the positive negative Pairs and Ithink there's probably just a crazyamount of opportunity to optimize thesekind of things but yeah so so I I thinkthat was a really really interestingdiscussion but I want to kind of stepback into a more high level space andmaybe if you could tell me about yourexperience with building the um the fullstack deep learning uh Discord q a botoh yeah yeahum yeah so I put that togetherso what what is this thingum it's q a over full stack deeplearning the course full stack llm bootcampum and then also my uh llm paper litreview notion database uh so all thoseare our sources andum we got some YouTube videos we gotsome web pages we got some PDFs and it'sbasically like natural language searchover those documents natural languagequestion answering over those documentsand I first built it back in January forthe scale AI hackathon which was areally sort of like fun there's now likea hackathon every two hours in SanFranciscoumlike yeah uh got a text from a friendlike oh do you want to go to thishackathon tonight I'm like uhunfortunately I'm going to a differenthackathon like three blocks awayum but yeah this was back in Januarythey um they had the hackathon and I waslike well what you know what can I whatcan we do right now q a was starting toemerge and like retrieve augmentation ofstudying through marriages like a commonapplication patternum and theum and so it was like clear could bedone pretty straightforwardly and youknow we get a lot of questions in ourDiscord where it's like oh you know letme point you to this lecture where wetalked about that oh here's the paperthat's relevant to that question andit's like oh I'd love to automate awaythat part of my jobum so that I leave like if there's asuper interesting question maybe I canfollow up on it or um uh if the questionisn't easily answerable I can follow upon itum so that was like the vision and itwas possible to put together like a avery Jank demo in like an hour and thento have like an actually decent ux bythe end of the hackathon like um like ithadyou know embeds and I want to say I evenalso shipped the like give Emojifeedback feature of like thumbs upthumbs down uh within the hackathonum but the the GitHub history can tellyou whether I'm I'm uh lying at thispoint or not uh but uh but yeah so thenit was that's that is we've stuck withthat we've continued to run it and andit's sort of one place where I get someexperience with like what is it what isit like to have one of these thingsrunning in production so like on thedelightful sideum when I switched it over to gpt4 oh itgot it on smarter and also it's nowmultilingual so I woke up one morning tofind that um somebody had been askingquestions about radio in Swahili and uhfrom what I could tell as a non-speakerabsolutely just using Google translateand and chat GPT it was responding influent Swahili with uh with like correctanswers and sources and links to myvideosum so uh that's you know if there's likedelightful X experiences of usersdiscovering features of these models andways to use the ways to use theapplication I would have wouldn't haveanticipatedum and then there's alsoum you know the sadder Parts where it'slike oh geez Like You Know It uhsometimes just like completely madeup and like sometimes users ask itreally weird stuff and it responds in aweird way back to them uh and uh oh yeahand sometimes it goes down sometimesopen AI goes downum sometimes the modal goes downum like uh yeah it turns out it's youknow interacting with a with a databaseis hard and the code you write in anhour is not good code for interactingwith the database so those kinds ofthose kinds of discoveriesum how exactly that happens in the LOMworkflow have also come out of sort ofrunning this thingI think the first question I want to askyou about this is you know as a contentcreator and I feel like that's kind ofthe mutual bond we share is making deeplearning content for a long time and youknow understanding sort of the like howyou try to grab everyone's attention andall that kind of good stuff is like theis like this ability toyou know automate the content creatorturn the content creator into thisartifact I mean you mentioned sometimesyou get bad responses but it's like itseems like this will just completelychange how we think right now of contentcreators like uh you know I don't knowMr Beast is like if I don't know if he'sa good example of one but like you knowthese these you know Educators peoplewho I think Thomas Frank is a is aYouTuber famous for notion tutorials orlike General lifestyle advice and soit's like turning himself into a chatbot shortly this will change just socialmedia broadlyright yeah yeah that's an interestingidea I think yeahum framing it as like automating myselfum is more fun and cute than it is likea legitimate statement of what thesethings are capable of I definitely havebeen you know I've been experimentingwith the uh with audio generation withuberduck and 11 Labsum to like generateum you know things that sound like me ona podcastum I promise I'm not using that rightnow I don't know how I can prove it toanybody listening to thisum but uh theuh so like you know you there definitelyseem to be some pieces that are therebut like full end-to-end automation isalways challenging right if you lookeven at like very sophisticated softwareshops there are limits to what they'reable to automate even in this likein principle Halcyon world of softwarewhere everything is bits flowing throughmachines that should behavedeterministicallyum and so I would say I'm a little I'mbearish on the sh in in the short termon things like fully automating yourselfum but finding ways to like take thingsthat you are doing with your time thatare low value that are repetitive butcontext dependent or like requireinterfacing with natural languageum you know uh that are in the end likekind of low effort things that you aredoingum taking those and replacing them uh sothat you then take that time you'respending on that and then can re-indexonto the higher effort stuff that feelslike a moreum that's like achievable right now likelike there are there are wins you canfind right now for that and so that'sthat's how I'm my current mental modelfor it um the point in which uh contentcreators are defined as as points withinembedding space plus a smallneighborhoodum you know that uh that feels uh feelsfairly far awayyeah I guess it's just like I thinkaboutyeah like exactly like you say like theum you know I the automating the moreyou know repetitive low-level tasks tolet you think in higher levels ofabstraction is like always amazing Ijust think also kind of like the multilike you mentioned the how do I fix thisradio bug in Swahili like I think theway it unlocks the global market it italso can create like hypercompetitiveness now because like youknow the top five percent can also scalelike they can deliver the personal touchthat through the language models andstuff like that yeah and yeah yeah so Iguess it just I feel like the contentcreators have actually the market isgrowing and yeah definitely definitelyand and yeah I would say it's likepeople already from the like lastgeneration of machine learning peoplethis idea of like drudgery or or certainkinds of like menial tasks could beautomated and like you know many of themcould have been automated with aspreadsheetum uh much less or much less gpt4 butthere's all also the the you yousuggested this initially as like youknow a content creator automatingthemselves as a chatbot they're that islike that's maybe not the same thing aslike full automation like fullreplacement of the role served by acontent creator with a machine but itdoes it is like a much less machine likeum thing that people were doing was likeyou know interacting with a personunderstanding what they want and tryingto give them what they wantum it's just that often when you aredoing that you only need to do it at avery like superficial level or it's youknow once you have the context of thedocument that has the answer it's likeas straightforward as copy pasting witha little bit of editing like that's muchfurther along the Spectrum from uh whatwe think of as machines to what we thinkof uh what we think of as labor formachines to what we think of his laborfor humansum and uh so definitely there is a pushin that direction from from highlycapable language modelsyeah I agree 100 I think like there'sthis other concept of like the languagemodels that produced the novel contentlike the language model you know throughmaybe retrieve augmented generationwhere it like retrieves this part of thepassage as part of this tries to come upwith like novel insights I don't knowexactly like but like you've tuned thelanguage model however it does this itproduces some kind of blog post and thatmakes me think even more of the power oflike your personal brand because it'slike as the content platforms becomeflooded with this kind of content it'slike that Charles tweeted it that or youknow however the medium platform that'slike what that that stamp of human brandis going to be so valuableyeah yeah I would sayum the essential post-modern artist isthe DJ whose Artistry is in test andselection and presentation of what aprevious uh mode of Productions artistscreated and it with uh language modelsperhaps we can effectively all becomeDJs of textum mixing together things generated byum generated by models uh with withmaybe just adding a nice light four onthe floor beat underneath them as we aswe mix them togetheryeah I I mean like for me my opportunityhas mostly been not in producing novelpapers but more in just echoing what Ifind is interesting and like that'sthrough experimenting with thealgorithms and what is successful likeif I review like a paper from SergeyLevine on offline reinforcement learninggonna do better than me just saying heythis is these are just my thoughts onthis topicyeah so I wanted to ask you also atechnical question about this which isyou mentioned notion page YouTube theofficial course material you have thesedifferent uh sources of data and thenyou put it into like a vector databasehow do you think about like the schemadesign for different sources do you havelike a yeahum well uh I've been using mongodb formy uh for My Level of storage which is acommitment to not have a schema I I youknow the you you can you can have a aloose an application layer-definedschema which is uh easier than adatabase layer defined schemaum but the the real thing there is it'slike still figuring out what exactlydoes this look like what should whatmust it have what might it have in adocumentum yeah I would say like your questionswhen you have like multiple sources hmmthere's like a certain scale at whichyou can no longer be very precious aboutyour handling of sources and you have tobe very generic I think you end upgetting like worse results if all you'redoing is saying likeum documents are going to be likeannotated with what are their H1 and H2headingsum but like you know coming frommarkdown or HTML but that that meanssomething very different in differentwebsites like it has a particularmeaning like H2 has a particular meaninginside of our online course notes butthat's like not what it means on someother pageum and uh yeah we have chapters in ourYouTube videos I use the chapters aslike metadata during processing andlater and that's also like chapters canhave different meanings when you'reconsuming different types of YouTubevideos and like thatlike injecting a little bit of knowledgethere can go a long way to increasingthe quality of generations andfundamentally like why do we like why dowe have this it's because we need totake documents that are longer than thecontext length and eventually get themdown to the context length and you wantto start I think with like splitting upyour documents at their joints likesplitting them up by sensible like waysto split them chapters headings whateversub document type thing and it's andthen only after that when it's time todecide what's going to go into thelanguage model do you actually chunk itto something smaller like maybe a fewhundred tokens at a time but you justmake sure you don't cut across any likereally serious seams in your documentum so like that's one of the that's oneof the like primary things to thinkaboutum and then I guess the other thing it'ssort of like a um applicationarchitecture kind of level is like Ithink of the vectorthe like embedding vectorsand Vector storage as an indexinto this information so an index in adatabase is something like we've alreadystored the data and then we add an indexright so we might add an index that'sprefix reg X based and this is it likepre-competes a bunch of stuff for us andstores it in a data structure that thenallows us to make certain queries reallyreally fastand that's effectively how we how likeevery application that I have used andlike embeddingsum for has been to do that right andlike when I when you think of a databaseit's about a lot more than just like anindex it's like I want to like I want toversion these things maybe I want timetravel maybe I want stored procedureslike it's a whole bunch of other stuffum and so Vector storage like feels asas I have used it in a rag pipeline alot more like an indexum and so like you kind of want toseparate out this is the current stateof this document Corpusmay be split into subdocuments this isthe current state of it and then this isthe index over it using a particularstrategy of chunking a particularembedding model whatever that then iswhat the application directly touchesum yeah yeah I think like my currentunderstanding is that the best way to dothis would be to have one class for justtext chunks like all every Chunk fromevery Source however long it ends upbeing like 100 tokens or you Max it outat the 512 tokens all lives in onevector index with symbolic propertiesfor like notion YouTube or like title ifthe page has such a thing and then sofrom that one index where you build onehsw graph with like you know howevermuch you have possibly you also kind ofwould add that extra structure inseparate classes so like you know thisis like the master chunk class and thenand then you have like the notion classor like the YouTube class I kind ofthink like that kind of symbolicseparation is the best way and then youcan kind oflike we have this one thing and we'vemade this called the group by filterwhere it's like a book has like it let'ssay we have two different books and onehas ten chunks another has eight thatmatch our query like in the top 100 ofour query you know so you would group itand then you would rank the at the booklevel based on how many passage passageshave matched it so I think that's likeone way of just using metadata again insearch that we talked about earlieragain but there's this other idea oflike maybe using graph structure to flowembeddings through the graph it's kindof like a Knowledge Graph idea or likemaybeby graph graph convolutional networksthat could like that would like takeeach chunk and use its kind of contextin the graph of like you know blog postssites this other blog posts in additionto just this kind of like where itSource from in addition its relation toother things I just think there's likesome cool things we can do by thinkingfurther about this I'm sorry that's nottoo concrete but no yeah I don't know asmuch about theum Knowledge Graph world in the graphdatabase world I think like definitelythe and like people in NLP have beenthinking a lot about this it is kind ofit feels to me a lot like the branch ofNLP that has beenum like basically obliterated by therise of large language models um butthat doesn't mean they can't have atriumphant returnum so I I have talked definitely talkedwith some people who are thinking aboutknowledge graph approaches andum yeah I can see that working prettywell for being like Oh what are therelated what are related documents maybethey don't embed the same maybe they aresemantically very different from theperspective of embedding but they'rerelated by this by this graph and yeahthat's uhyeah that's that kind of uh link basedranking is obviously very good forsearch or elseum you knowuh yeah I wouldn't have been using Gmailto communicate with you about a Googlemeet umyeah uh but yeah so uh yeah I seethere's there yeah the the thing that Isaid about like there's an index and aseparate document store like that's uhyou get you can enhance that a lot bycreating a full-on database withintegrated search that uses metadatathat metadata is used to constructindices that have both like a vectorhnsw type index but also have themetadata as likeum additional flags that are used toconstruct indices that allow like veryrapid querying on on sub componentsyeah I I guess just like broadly likewith this topic I think that there is areally interesting opportunity forVector databases and graph databases tobe used together more I think the lowhanging fruit is you totally separatethem entirely and you think of graphdatabase more in like the graphanalytics kind of way like think aboutlike a Twitter like graph like Conorlike Charles's tweet that kind of graphwhere you where you then compute likepage rank kind of centrality metrics forlike a particular tweet and then you usethat as a feature for re-ranking wherethat's like one of the tabular featuresfor XG boostyeah and like so I think that's the mostlow-hanging fruit but I I don't knowlike there's there's another idea tolike deep walk no Tibet where you likewalk along the graph to find thepositives and negatives for thecontrastive optimization but yeahdefinitely I mean there are there issome interesting I want to say it was ablog post but at the very least it waslike a Twitter thread like Lang chainhas has started to incorporate somegraph databases and it's starting tothink about like craft databases astools like there's uh you know graphdatabases have not been as successful asmaybe you might have predicted in 2015at the height of nosql and social mediayou might have been like graph databasesare gonna are gonna replace SQL and likeit's the only thing anybody will everuseum and that's like the opposite hashappened like SQL databases have gottenbetter at storing graphs and gottenbetter at storing uh uh documents andother things but um but the at the veryat the bare minimum representing graphstructures inside of databases and usingthat as part of the querying searchingranking process very like very clearly aton of opportunity thereyeah yeah so it's so I have so manydifferent ways of just conceptualizingthis in my head like you could betalking about like a neighborhood ofhouses something where there's like agraph structure that maybe you embedinto a vector or like a molecule whereyou embed that graph structure into avector or yeah or the graph is itselflike something that you derive out tocontextualize the vector and yeahyeah yeah yeahI just think like I don't know there'sthis one paper I saw that was calledPrime kg from Harvard biomedicalinformatics Institute that was likedisease drug Target relations and theneach one has has like unstructured Textdata about it that's using and embeddingand there's like something aboutclassifying sub graphs as well that arelike affiliated with some particularlike metabolic pathway like there's justsomething to this and I and the reasonI'm trying to connect this back with theDiscord bot is I also like to thinkabout like I'm you know I have aweeviate podcast transcriptions data setthat you'll soon be in Charles and likeI like to build a graph of this with thewegate blogs and the weeviatedocumentation and I'm thinking like howdo I design this data schema and that'skind of what I've settled on now is likeI just have the chunk master class andthen I have links to these kind of youknow podcast transcripts blogs likeseparate they have their own index aswell but this master thing is reallywhere the meat of it is interestingbecause it I mean just yeah it's justpretty I think pretty interestingthinking about the data schema with thatwith like you can use symbolicproperties to filter your search throughituh but it's yeah I don't know it's justlike digging into it further yeah yeahthere's a yeah there's definitely a lotthere and I'm I'm sure yeah the peoplecoming from the ml world have a lot tolearn from the people who've beenthinking about search and retrievalum with and without uh ml involved atall and uh yeah part of what is excitingabout this moment of like rapiditeration and Rapid building of likefull stack applications is that nowpeople from all kinds of differentdisciplines of software are looking atthis paying attention to it and beinglike wait a second I recognize thatpattern that's what we've been doing for30 years you're still doing the dumbversion of it let me let me give you thebenefit of like three decades ofthinking about this problem here's likehere's concentrated nuggets of betterways to do thingsum and so you can see that sort of likehappening with um yeah with retrievalwith tool use with uh yeah languagemodels for code generation with uh yeahyou with ux and and design with um yeahwith uh constructing defensiblebusinesses and thinking about the likethe whole uh the whole business aroundthe features or products you're buildingwith language models you know amazingwell Charles I'm so happy with ourcoverage of the full stack deep learningI think we really you know explainingkind of the zero shot MVP get startediterate on the ux instead of the modeland then this data engine what might bethe middleware for realizing that is itworth realizing it and all the new Ithink it was such a great coverage letme kind of wrapping up this podcast askyou like a broad question of like someof the directions for the future I knowthis is this question can kind of putpeople on the spots like what you knowpredict the future but like what ismaybe not necessarily something that itcould be something that just like youcan answer this however you like ofcourse but like whether it's somethingthat you just want to see happen likellm inference costs like nothingbasically or it could be you know Idon't know just a just like a dream likejust what you oh sorry or it could besomething that you just think verypractically this is just the force isheaded in this direction so kind of likeDreaming or practically however you wantto take the question yeah yeahum I appreciate the breadth of questionum yeah yeahuh I'd say that thinking on let's callit likeyeah on like a one to three year timescale the phenomena that I think aremost importantare the decreasing cost of inferenceand the incorporation andmultimodalityandand like maybe the longer end of thelike three years up to maybe five yearsthe combination of those two thingsunlockingum applications in robotics hmmso cost of inference is going down likeit's just we are in we're in like a uhsuper Moore's Law like with the cost ofcognitive capabilities like you set someyou said some Benchmark of like I wantto be able to do at least these 10things with a modelum and then you can just watch thatplummet the cost just absolutely plummetI think the Ada embeddings from openaihave gone down by probably 10x since uhsince like Novemberum and uh likeyeah like with GPT uh three two turbothat was uh our 3.5 to 3.5 turbo likethat was a a 10x reduction in cost andlike six months or somethingum and they're not doneuh sothat is I expect that phenomenon tocontinue there's competitive pressurethere's Hardware Innovation there's allthe work being done to get like thingsdown to in three inference and you knowum and shrink these things down put themon consumer cards there's just like ohand like always the Dark Horse of um oflike specialized acceleratorsum and like people can just like youknow you the architecture's been fixedfor long enough though like attentionhas been on that for long enough peoplecan just like really just keep crankingthat number down andum we have seenwhile retaining like the same level ofcapabilities uh so I would expect thelike we're just going to continue to seethe cost of inference going downobviously we'll continue to push thefrontier of capabilities and so thefrontier of capabilities might alwayscost like the same amount or might go upin cost to get the frontier ofcapabilities but those capabilities arealways they're moving up it's like youknow you have to be in the rightreference frame to think about this andso in a future where it isum like gbd 3.5 turbo capabilitiesare 10x cheaper in a year and then 50xcheaper in two years like that's adifferent world than the one we're inright now so I've had people ask me likewhen am I going to get chat gbt NPCs inmy video games and it's likenot today unless you're unless you'rewilling to Shell out like a dollar aminute to play a video game but a dollara minuteum becomes like a Pentium in it in liketwo or three years and that's like veryvery like people like that unlocksapplications of these models that arecurrently blockedum I think you knowmulti-modality is one of those thingsthat we are getting into smaller andsmaller models we're getting into higheruh higher quality uh and fasterinference Oakland Flamingo V2 came outlike within the last week uh of thisrecording and that's like an improvedum multimodal model people have beenfinding better and better ways to doimage language multimodalityum and those models as mentionedum earlier like the gpt4 self-drivingexample like you unlock a lot of uh uhtransfer learning opportunities betweenimage and vision that make the imagemodels or the vision models uh or sorrythe image models or the language modelsboth betterum and so the trend that brings thosetwo things together that I'm excitedaboutum on this time Horizon is roboticsum the general discipline in whichself-driving technically should be putum where if you can run GPT 3.5 turbo inmilliseconds for you know on somethingthat's drawing 10 watts of powerum then you have yourself like a robotthat you can talk to to deliver commandsum so long as that language model caninterface with the the robot body aslike one of its toolsum perhaps via lower level reinforcementlearning uh style policies that umthat's just likewe don't have general purpose robots fora number of reasons but in a couple ofyearsum they can't be smart enough and theyaren't as uh easy to interact with whenthey're doing tasks as humans thatexcuse is goneum the servos and the uh what do you dowhen when it swings around and breakssomebody's nose uh those kinds ofquestions of the maintenance uh thosequestions remain hardum but not the intelligenceyeah super compelling picture for thefuture I inspired by all those things aswell I I think with the maybe one thingto stay on I mean I think um this thiskind of like NPCs in video games withsuper cheap L alums one idea that'salways stuck with me that I find to bereally fascinating is um I had you Shangwoo on the podcast who discussed chatArena where it's like you know Charlesand Connor speaking in simulatedconversations like a million simulatedconversations between us and like thisidea of just like creating this like Ilike to think of it as we takegenerative models and we create theoutput Space by sampling from them andthen we use Vector search to look inthat uh the latent space of thatgenerated output space to see like asany of these like now is like more tosearch through or something like thatinteresting yeah yeah definitelylanguage models are useful for theirability to allow us to like explorelanguageum like like people people focus a loton the like the Practical applicationspeople focus on like oh I can use thefact that this generates language andrequires language as one of itsexpensive inputs to like accelerate andamplify like but those are likeDownstream of this like fundamentalthing which is we now have machines thatare like capable explorers of languagespace capable producers of utteranceslike um yeah and that is maybe like verylong term uh at the scale of likedecades probably a very interestingphenomenon about language models howdoes it change the way that we speak howdoes it change the way that we interactwith language that now that um uh theproduction and exploration of languageis no longer bottlenecked on a humanbeingumproducingyeah it was fascinating it yeah I thinklike there's this thing that one of thebig dangers of language models is thatthey're so persuasive that you likesimilar to how a recommendation enginenow is already kind of persuasive forextreme ideas by like keeping you ininterestthat the yeahyeah the whole exploring the languagebut I love think about that and thenalso like when you draw when you drawthat to code as well how it can likeexplore the space of what code can do bywriting all sorts of code and you haveto have abstractions that you share withother llm programmers and they're likeokay I like the subtraction like yeahyeah we've barely begun to see what theimpact of these things on programming isgoing to be right like um yeah changesin Technologies produced by programmingum like reflect back and changeprogramming languages so like the riseof developer tooling like really reallyhigh quality developer in editordeveloper tooling is changing the waypeople think about languages like rustwould not be nearly as easy and fun atleast in the in the good times to writeum were it not for like the variousforms like intellisense code completionlike the compiler and the analyzer uhare like directly interfacing with thedoc man like if I had to write it on apunch card you know or like write it ina static document and it also compilethere then get back an error message asopposed to this is the type I'minferring you get it on Hover like thatthat has already changed the behaviorallanguages python is getting typing overum its own dead bodybut uh and that part of that is becausepeople love this developer experience ofbeing able to like see those things andpreviously the fact that python didn'thave types was like its selling pointum and and that's just that's that'sjust a very humdrum technologicalchanges around like the like languageserver protocol and um and likeimprovements in um in in developerenvironments and text editors so toimagine yeah what's possible now thatproduction semantically meaningful textis not bottlenecked on humansum I think we'll see some pretty radprogramming languages in like a 10 to 20year timelineyeah Miss and that's a funny like I alsolike came into python like I rememberthe JavaScript like you don't have towrite the types I'm like oh nice liketypescript it's like oh no it's backyeahbut yeah Charles thank you so much forjoining the weba podcast this is trulyone of my favorite podcasts we coveredso many topics and I learned so muchthank you so much yeah thanks so muchfor having me Connor this was a this isa blast", "type": "Video", "name": "Charles Frye on Full Stack Deep Learning - Weaviate Podcast #57!", "path": "", "link": "https://www.youtube.com/watch?v=TztVMhZxORQ", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}