{"text": "I am so excited to host Dmitry Kan on the Weaviate Podcast!! Dmitry is a world class expert on emerging trends in search ... \nhey everyone I'm super excited to be publishing our latest podcast with Dimitri Khan we're discussing some of his latest Works around his keynote talk at the haystack European conference and this latest blog post on neural search Frameworks a head-to-head comparison so I think it would help a lot to see the visual that Dimitri's created to describe the neural search pyramid so you see from this visual of the uh the pyramid going from the parks and nearest neighbor algorithms to the vector databases neural Frameworks encoders and application business logic and user interface so a big part of this podcast is going to be Dimitri and I debating these components uh you know what should go where and then sort of the abstractions around it but hopefully this visual helps a lot with what we're going to discuss in the podcast I also thought this would be the perfect podcast to debut the Wii V8 podcast search app searching through the wevia podcast with weaviate of course so here's the GitHub repository with the Eva podcast search all you would need to do is clone this repository then run Docker compose upd and then python3 restore.pi to restore the backup with the full transcript with this podcast with Dimitri in it so I'm really curious what people think about these two lines of code for restoring data sets because this is kind of the thinking so far around how we're planning to package up the beer data sets so if you've test this out please let us know how you think about this experience and I really hope you enjoy searching through the podcast so once you're running the podcast search app and weaviate you can ask it all sorts of questions like what is the neural surge pyramid and it'll semantically match it with the part of the conversation that talks about it and all sorts of other topics that were mentioned in the podcast so maybe if you don't have time to listen to the entire hour and 45 minute podcast you can just search through it with the wevia podcast search app so thanks so much for checking this out and I really hope you enjoy the podcast hey everyone thank you so much for checking out the weva podcast I am beyond excited for this episode we have Dimitri Khan one of the most influential speakers in search technology Dimitri is the host of the vector podcast he's a senior product manager at TomTom and he's recently given this incredible keynote uh the haystack European conference 2022 so firstly Dimitri thank you so much for joining the weba podcast hey Connie thanks for having me it's always it was always a pleasure to talk to you awesome so could we maybe kick things off with uh the big the keynote where Vector searches uh taking us and some of the key ideas behind that yeah for sure uh so the keynote I had an honor to give this keynote at the haystack conference as you mentioned in Berlin in late September last year and uh we've been a bunch of uh actually people from Vector search Community including bb8 and Pinecone and quadrant and also users this was especially interesting to see how far ahead users gotten and they they've been asking very precise questions you know like how do I choose a model and and which database to prefer and so on and so forth this was very interesting and some of them have been already trying things and so in that sense I wouldn't say like a year ago maybe I felt like I could um share more and educate more and even explain the basics uh nowadays it's more like okay we already we already understand the basics okay can you explain what what what practical step I can take to actually Implement Vector search or I have tried you know doctor query it doesn't work that well what should I do you know and one of the key points in the um in the Keynote uh key points in the keynote I have uh had was uh this um work that was done by mckinsian and and Google in 2021 so almost like two years ago now uh where they call out the issue of search abandonment costs in for the U.S retailers so it's only U.S retail um which kind of like loses 300 billion dollars a year uh and and why they do this why why this happens is because of the search abandonment issue so you know practically speaking users start typing it you know a query they probably see some autocomplete some some Dynamics going on on the website then they check the results and they cannot find what they look for and we discuss with you in the vector podcast just recently um that the query language the the user language is so different right user to user um I have a bunch of examples on this front as well when when when search and e-commerce doesn't work um and so what's interesting also is that 64 percent 64 of retail website managers do not have a clear plan for improvement so they kind of lose the money they don't know what to do with it how to fix this and that's like more than half of them right it's like a big big number um and another issue that kind of surfaces now that I have this product management had on um it's interesting to see that 85 percent of global online consumers view a brand completely differently after this search is unsuccessful so in some sense the brands themselves have nothing to do with the search engine which is basically like let's say an aggregator or whatever like Amazon type of thing right but like if search doesn't work they are they think it's brand problem because they cannot find the item they are looking for right and so they change their perception of the Brand This is how important it is to make the search work yeah yeah it's super fascinating is it reminds me maybe like these kind of numbers there's something about like maybe 98 of data is private and the motivation of the sort of indexing and searching through private data and as as well as this retail thing like having these kind of numbers but um I'm kind of thinking like so with the retail and fixing it is it the responsibility of the brands like whose responsibility is it to implement these The Cutting Edge search engines I mean you know we think like with alleviated it's our responsibility to build it and make it available and you see it as being like more uh more independent uh e-commerce retail stores so like not just Amazon you know being sort of the front layer but more and more uh individual retail sites taking on uh you know this pipeline of as we'll talk about neural surge Frameworks like having their own uh we V8 Gina AI kind of setup yeah I think that's that's an excellent question and I think it kind of like depends on the situation the Practical situation with engineering and and sort of um assets and resources that this company has and also like the focus that they want to have right in some cases companies might want to like Outsource this and just ask somebody to build it for them and and this is this is an interesting opportunity you know for um players like for example semi or ovestpa you know if if you guys have a cloud offering which is kind of like end to end you know solving a particular issue for you know starting from actually creating the embeddings all the weight of all these backups every infra element is taking out care of and then on top of this you can actually prove that you know I can actually retrieve the the um the items with with higher relevancy then I would do myself because what does it mean for me to make it myself I would start like scrambling reading stack Overflow hiring engineers and that's like a long path right and maybe I'd rather pay money and just get to the market sooner and so I think uh that's an excellent question but there's another facet to this um I guess is it like in your question is it Brands um concern or is it like uh uh let's say a neural search framework a vector database concern or is it like a retailer concern in some sense I think it's like a it's a joint ecosystem right it's not like you can just say hey please fix me the house and and I'll pay you but they might fix it their own way they might also break some things while they do this but you are paying so money don't always buy everything so I I think I still believe that it's better if these players would collaborate in some ways for example you know if we're talking talking about neural search implementation you know what's important is like for example how you catalog an item what metadata you have right and so and also how up to date it is right so somebody needs to be supplying this data to you and so maybe the brands could be supplying this data to you as they change to introduce new products um but then everything else you know the pipeline should be already in place and it should be easy to use you know be it an API or some other way like Amazon I remember they they have an office they have a service of delivering data on tracks right because it's much faster is it deliver on trucks than to upload it because it's a lot of data but if it's like a medium-sized store like maybe you don't need a truck to deliver your data you know on batch level or whatever like you could build an API um I hope I answered I answer your question there is another another another perspective also to keep in mind even if you have the engineering force in place like in-house building uh like if you take like an open source database or open source neural framework it might take you a while to to gain the you know to to accumulate the knowledge to gain the momentum to realize okay this is what we can build from POC to actually productizing this um maybe it's easier if you just go and kind of like Outsource this resource this cost to a managed database like in case of vbait or in case of Pinecone um and focus on something else right focus on that specific you know cataloging issue or maybe bringing a classifier that will do the product classification on the Fly and things like that right it's kind of like put your effort and focus on what matters and then later maybe you can change your mind you know the moment you grow maybe you will decide to build your own Vector database but but before you before that happened you need to kind of like pay your bills right and so don't don't get ahead of yourself and also if you partner with a clever you know player which is like we have a lot of them now on the market um you know you might win and they will also learn you will learn together so I think that that's an interesting perspective to to also think about yeah it's a it's a brilliant way to open up this kind of as we're also talking about neural surge Frameworks and you've written this great blog post that outlines a different neural surge Frameworks and as you mentioned I mean it's just a you did a brilliant job just covering it and this idea of uh you know the overhead for learning it the learning curve is something being something that's important and at weviate there's a huge focus on uh ux and being developer friendly creating content to try to educate especially with the new releases we try to have even alleviate Air Show to you know try to explain how to use all the stuff and what it is as much as possible and I think this is going to be such an interesting discussion because it's there there are parts of it that are that should live outside of leviate I think parts that I think maybe should be built into weed and I also want to just quickly set this up before we get into it that like this what I say on the podcast about the relationship of webgate and neural search Frameworks is my personal opinion it's not like an official statement of with respect to like how I see what should be built where so I really want to kind of transition this into maybe we could kind of do the tour of the neural search framework starting off by maybe just the high level of how are you currently defining a neural search framework oh yeah that's that's an excellent question and and maybe I can also intro like I created this you know mental diagram for myself and I kept selling it everywhere and seems like people get it and some of them even like reach out on LinkedIn and say hey I got your vector search pyramid thank you for creating it but really I you know my goal was just to put things in their place like on a bookshelf so you can like reach out and and and and give context and and and and ground your discussion and so in this Vector search pyramid which probably can also share you know essentially like on the base on the base level you have the KN and a n algorithms and to some extent I also covered some of them not all of them you know I know that Pinecone for example did a great job publishing a lot of material on this you know how each algorithm works we also happen to invent one algorithm by gpq which is just a modification of product quantization um and then the next the next layer and I did publish a year ago now a Blog about not all Vector databases are made equal and so you have mailbox bb8 Pinecone you know GSI quadrant Vespa bald and also those players that kind of like yeah I I didn't find yet the proper phrase I I kind of say catch up but on the other hand it's not like ketchup it's like being bald and actually going into this space for existing databases like uh redis and elasticsearch and solar and they add Vector search functionality as well for their users and then there was this next layer that I couldn't like quite wrap my head around and I was thinking what exactly is this and of course genome and Haystack sort of like in some sense the pioneers of of creating the terminology and so I think at some point they were calling themselves neural search Frameworks um recently I saw that Gina is calling themselves something like amylopes for neural search so so of course these things morph but but I kind of like decided to stay with the same term like neural search framework and I kind of labeled every system I could come across but there are some exceptions as well by the way there and I'll get there but basically the way I Define it and I guess we can share the the blog post itself is that basically reading from the blog neural search framework is an end-to-end software layer that allows you to create a neural search experience including data processing model serving and scaling capabilities in the production setting so like just to unpack this so let's say if you take a SQL database and you want to build a website right you will still need to figure out okay where do you get the data how do you process it before it gets in the SQL database then how do you normalize the tables if you do that right like foreign keys and all this thing which index type to choose and things like that so so that it actually breatheses and works and can scale um but like this definition may sound to some people as uh Emma lops machine learning operations but there is a key difference that first of all mlops it's like a wide area it's it's a wide field you know it it concerns itself with a lot of things like I don't know model training you know model versioning deployments serving monitoring you know data drift fine-tuning a lot of a lot of things right and and the application is also quite diverse like I don't know I could be building um I don't know face recognition system or something like that right so but like neural search framework focuses only on neural search so it's like and and also another question that frequently comes up is like what's the difference between neural search and Vector search in a way there is no difference it's just like how you take which angle let's say neural search you could think of it okay I have a deep Learning Network and so that's probably why it's neural because it's like deep learning the you know neural network but if you take the stance of let's say geometric space right so you you embed your object into multi-dimensional geometric space and now you need to and each point is a vector so now you need to basically find a vector which is relevant for your query Vector so you are doing a vector search right but but this is kind of like mechanics of it um so yeah I think this is how I Define it yeah I think I'd want to start with connecting with our earlier conversation of the lost money in retail and more and more brand stores trying to build their own retails and I'd even maybe extend that to people with their own blogs looking to have searchable things on their blogs and you know also paying the bills and not worrying about this thing I kind of want to start off with say like the uh data pre-processing layer like I see kind of with these neural search Frameworks they Define these pipelines and I think pipelines is sort of the key uh term here and I I'm going to talk about what I what other pipeline I think should live in weaviate and what I think should live outside obviate and I think maybe starting off with just like the data ingestion part like maybe you have some kind of API that you query to get the data you have maybe like a PDF parser with some kind of OCR how are you thinking about that first part of the data ingestion layer because maybe if I could just add one more thing to transition the question when and coming into like running things in production uh Gina AI they have these executor pattern and you know I learned a lot about this on the wevia podcast with Maximilian work I highly recommend that if you're curious about learning more about this pattern for listeners but this kind of way of scheduling like a Cron job that say you know every two hours it's going to query this API like or say every day it's going to hit the archive API to get the new batch machine learning favors parse out the text and chunk it in the PDF then vectorize those chunks and and then maybe put that to Wii game and then yeah I don't want to let's start just on that first part of yeah I think it's it's another excellent you know topic to think about because um I've been Consulting a few startups that are trying to build Vector search right and uh they sort of get quickly far ahead you know because they have let's say a database a vector database they have their data obviously they've chosen the model to vectorize with um they're already scratching the relevancy side of things but but then someone comes in and says hey we just received another batch of uh objects we have half half a million you know objects to to index can you please index them and and by the way I have a demo tomorrow right so so what options do you have right and and and literally some of the developers which would reach out to me and say hey any any options anything to save this situation and you know like of course um naively what you can do is that you can start writing a python or Java you know concurrent app which will start unpacking this you know data sets you know reading from S3 or something like that uh and then vectorizing you hear the problem that oh you need to use GPU to speed things up and that's quite costly so we need to kind of like rethink a lot of things and not make a mistake if we launch this in batch mode and it will run for two weeks so you go back to your manager and say it's gonna take two weeks and they're like what and how much is it five thousand dollars oh my God so like each time we crawl the data you're gonna need five thousand dollars in two weeks it's like going out of hand right and so um especially what you what you mentioned in Gina and also there is a framework called txtai which I learned just you know by kind of like Googling inside GitHub if I can say that so so they have this notion of um kind of this workflow so for example as you said in Gina you can do um you you can have an Executor that will uh read uh kind of like an archive and and basically iterate PDF files and then transform uh parse the textual content out of a PDF file and then it proceeds to the next stage right um and and a txti for example also has you know some connectors for other types of data like in computer vision space or automatic speech recognition space and so they have different like workflows that will help you set things up really quickly they have one demo app um on hugging face spaces uh which essentially what it does is that it goes to Hacker News front page it scrapes you know the top links and then it indexes the titles you know of that top page the top on that front page and then it shows shows a search box so it basically indexes all these titles you know embeds them uh on the Fly um and and and then basically when it's ready you have the search box appearing on the screen and now you're ready to query it and so it is that easy and they show you know how easy it is of course it's not what you will use in production setting most likely you know like in production you want to have like a workflow that routinely goes and checks that front page you know like a cron Tab and then indexes that and you still have the the index that is serving the queries so you have some offline index which is being prepared and then you do the swap you know and things like that but they also simplify things like deploying on kubernetes so you can scale things up because like literally if you if you write your own python app and one of the startups by the way there was a a bottleneck specifically in this component that would read the objects one by one and then it would try to classify the object and then embed it and then it proceeds to the next step and it was like I think it was like single thread application which would kind of like take forever and it was very convoluted right because if you don't have the framework in your hands you start Reinventing the wheel and so most likely you will kind of cut the corners and you will be basically wasting time unless you have a lot of time which usually is not the case in startups like you need to build something really quickly um and so this is this is something that I think is probably um you know hasn't been in the minds um of the makers maybe like more the than a year ago but I think it becomes more and more important that you don't just bring the vector search core functionality you don't just sell the statement that hey move to neural search and all your problems will be solved but you can actually show the path to get there right and with these workflows that process the data access the data quickly um and allow you to do this repeatedly is going to win more customers yeah and I think um well yeah I'm sorry if maybe I'm slowing it down too much but I I think maybe from The Next Step if we've gotten the data and now we're vectorizing it I kind of want to if we could talk a little more about the decisions with vectorizing the data I think it's so interesting you mentioned like a single thread approach where you're not taking advantage of like batching on the GPU or parallelization on the CPUs and we've recently added things like Onyx support for the textavec um thanks to March and antis who who got this done and like this kind of model inference for the vectorization's sake also I mentioned on the vector podcast that I'm really excited about our partnership with neural magic and what they're doing to sparsify these models so they can run super fast on CPUs and so I'm and maybe one more references in our Levy podcast with Sam bean from you.com he describes how they combine uh the spark Big Data technology with the Onyx acceleration for the CPUs and how they vectorize that way I know you've done a podcast with Max from mighty uh can you tell me how you're thinking about the vectorization layer sorry one more thing is uh with weeviate I think kind of another interesting thing about this is how we have separate Docker containers for weviate as well as these deep learning model inference containers so you can kind of scale them up differently with I think things like the kubernetes helm chart and it's a little more complicated about that like you can scale them up oh you can just use weba cloud service if you know as you say you want to pay your bills some other way so uh can you tell me about how you're thinking about the vectorization space yeah for sure and I think um so I think it's kind of like it's it's great when you describe how you guys build it uh I I take the stance like of looking at it as an outsider and in some cases I'm basically the the middle level between you know the the customer and and then sort of participating in the decision making and I'm not fully aware of how things are implemented inside the vector database for example but I know that somebody's got to pay the bill in the end right even if even if you use a vector sorry A Cloud solution still the bill will come your way right and guess what it's either you or your manager and that will have to pay it and and also guess what uh because of the low margins and I just had a podcast uh with the uh GSI product manager uh where he says that uh uh you know a lot of these things are now on the Raiders of even big players like Amazon uh or any any big player that you think is a big player they still might have very low margin like Google might have very low margin on their web search right so it's very important for them to optimize things and so I think this article in the budget if I can say so is very important as you build the um um the neural search experience um and so if you need to pay like as I was saying five thousand dollars uh you know each time uh it's going to be prohibitively high and so it will slow you down right like eventually you will say hey maybe we don't evolve you know as frequently maybe we will just do it once a half a year and and may also die out so it's kind of like important to address these heads-on and in one of the startups I actually recommended Marx's work and so I said hey let's um support them can you port the model to to Onyx and basically make it available as part of Mighty and what Mighty does is that it basically moves your computation from GPU to CPU at a comparable so the quality is exactly same you know get the same embeddings but you pay less right and the speed of doing this on CPU is comparable as well as it would be on GPU so in that sense maybe you can dedicate GPU cluster more to things like model fine tuning with matters but to uh but to that production side of things like when you compute the embeddings already on the existing model and then how you productize how you serve things you don't need gpus necessarily maybe in some edge cases you do uh but then you know like okay why we do this probably it pays the the bill and also bring something on top so it makes sense to to do this right so so I think this is this is interesting that I've also learned in the past year that um you know how things kind of get created right so first you get that hype of new tech like on a big date a few years ago and then comes this realization that oh yeah yeah we do have a lot of data oh sounds like Hadoop is the way to go but then all this data shuffling or how do I upload the data into hdfs like you know and all these connectors arise and like all of these things and then somebody comes over and says hey no no Hadoop anymore like let's do something else so but but but I think every step in this in this journey is important you know if there was no hyper so to say in the beginning saying like you know like Bob uh one Lloyd was saying I was just in the airport of the circumference with Google and I've realized hey there is something here you know let me build this model and maybe it can reason semantically about text uh and even if even if it wasn't doing 100 perfectly it was already showing the the way to move forward right but then all this other items which are more like mundane infrastructure level and sexy no one talks about them on sales presentations so like product level maybe even right so you don't say this to users hey you know what I spent five thousand dollars to embed the items and now you can find them and you don't do that right it's usually devops people it's usually you know heads of units that will say hey did we spend five thousand dollars again can you do something about it um so that you go back and say how which options do I have and so I think then it becomes important to focus more and more and I think Max by the way gave an excellent presentation and he showed also how he scales so it's not just a Docker image of Mighty that you know you can send a an object and get an embedding but he also shows how to scale it out and and basically still save on on on on money and and and and make it efficient in terms of time time so it becomes like a trade-off of how soon you need it how much money you have you know how much money you can burn on this and things like that but basically he's is working on a very efficient implementation so I can recommend that yeah it's so interesting I think like you could completely abstract it by sending the embeddings to the open AI embeddings API or coheres embeddings API and there's like that model as a service model on the cloud API thing or there are these you know do-it-yourself options and I think one thing that I think the Frameworks contribute a lot is with the ease of like dock array embed or you know the Deep set cloud with Haystack and how you can just have your python code to embed and then put it in the database layer so I think I'm skipping over a little bit over that little layer of you have the vectors and now you need to import them into ev8 because I think um you know and I we we have like a new python client and uh dirt cool wyek is the expert on this and hopefully he'll be back on the podcast he was on our 1.15 release podcast we can talk about that but uh passing that import layer let's talk about um the a n index part and I'm curious like how you see the entanglement between the A and index and the database part like is there a chance that it could be separated I don't know if we get like the you know hnsw product quantization it's written in golang it's very like in the core of wevia do you think these two things could be separate layers of the pyramid one day yeah that's also a great question uh and and the podcast that I mentioned with yeah from from GSI is one example where the in an index can become its own entity um because JSI is basically JSI offers a an APU component right so associative Processing Unit so think of it as the same um you know system in in the family of processing units like CPU GPU and it's kind of like next uh stage in a way targeted specifically at um uh neural search but not only that and and so in principle uh vv8 or like quadrant or Pinecone what have you or elasticsearch could be basically that computation layer so in some sense it's like a middle layer which receives the data it knows where the data is it knows everything about relations between objects and so on but it doesn't need to select on a certain scale it could also do the vector search for you right and so for example using quite efficient hnsw algorithm or uh product quantization or something like that um and I know that you guys also implemented dknn is that right yeah so like I mean so this is when companies will concerned with themselves with the cost issue and they say hey for our use case it seems like you know Ram is way too expensive can we actually move closer to the disk and we have plenty of ssds they're cheaper uh and dknn which I think is kind of like a a derivative from Zoom paper essentially basically moves closer to the disk it does that expensive uh sorting of uh full Precision vectors as the last step but everything uh before that happens uh on other layers like with lower granularity of vectors and they also solve the problem in hnsw um and again sorry you remarkov but like he might he might he might disagree for sure but like uh in that paper they claim in in this scan and paper they claim that thousand nodes in the hnsw graph that they have built were unreachable from any point from any entry point and so they have they have fixed this problem so they increase the connectivity of the graph and so you know this fundamental issues are being solved and you can then focus on things like okay how do I quantize my vectors uh to what extent and this again is a trade-off between granularity and and sort of like later issues that you will have right so for example with uh precision and vectors overlapping uh to the single point so you need to disambiguate them in some way and there are ways to do this but it's also may become expensive in some cases um so so this is interesting that I think and this was also one of the questions on my keynote as well is there a place for Hardware companies and maybe startups even to appear on this scene of course we know that Nvidia and Intel are working on this they've been competing head Zone on billion scale and then competition challenge last year but also of course GSI does it and I think yes of course building a hardware startup today may sound scary but at the same time I think this could be very interesting to tap into and so maybe in the future already today you know like uh there is a connector between elasticsearch open search and GSI Hardware so basically the way I think about it is that in elasticsearch you can compute facets right so you can build I don't know nightly jobs that builds facets and then displays them in some dashboard or sends them to the users so all of this computation could happen in elasticsearch but that other expensive computation with neural search Could Happen outside so you don't need to kind of like suffer from the fact that now you need to balance between these two it's fairly expensive processes at scale uh but you could kind of like uh uh distribute them you know how we do with vacuum cleaners we charge them and then they go around the house and clean right so they don't use the electricity and you can use the electricity for some other purpose so exactly the same idea I think may may kind of like enter the scene uh at some point so immediately I thought of just so many things as you're talking so packed with knowledge I think um well I really want to kind of understand the Apu a little more because I don't quite understand I think I think I want to quickly touch on some other things and we could come back to that um so I think a very interesting thing is about the incremental nature of the a n index in database entanglement the the key distinction between like vector database and Vector Library amongst other things in my opinion is that you have to incrementally update the vector index compared to like build once and then you have the static index search and I think that's I think hnsw is amenable to that but like product quantization making it like online uh because what product quantization is is you have the vectors and you're clustering them for each of the dimensions to reduce the Precision by representing say 32 bits with instead the centroid ID of that index in the vector and then you can like couple it and slide it that way to like have the Kings and I know that you understand that I'm just kind of saying that for the listeners but so the kind of incremental thing is how I see that distinction but um but yeah could we come back to the hardware and what makes it different I know that like Sarah Bross is building this big chip with the thing of you know we're you know like gbt3 is limited I think I think 4096 is the token limit at the time of this and I know it's I was recently reading deepmind Sparrow paper and it's fascinating to see these massive props like when you see a prompt that's like you know a 1200 token prompt you're like wow that's quite a problem like this idea of building custom Hardware to overcome the quadratic attention or I know there's like sparse attention but like generally to have massive inputs to Transformers so so that's kind of my frame of reference for understanding custom hardware for deep learning and I think the tpus are a similar idea where it's like a big GPU so what are the details behind the Apu how is it optimized for vectorization yeah that's a great question I think we can share a couple links on that we've also happen to build a demo which we presented at Berlin passwords last year so we took um you know 10 million images and from Leon data set and we have used clip model to vectorize them and then build a demo where you can compare how you know in image Vector search compares to let's say in title Vector search versus complete keyword approach right so no vectors involved and so we've used Apu as our backend uh you know far back and uh for storing these vectors and actually uh you know Computing uh the nearest neighbor set and so basically the way uh Apu looks all sort of like on the inside is that uh first of all it's it's kind of like this Paradigm of compute and memory so if you take kind of like a traditional server in a way traditional um where you have a CPU on RAM um when you will run the uh let's say hnsw algorithm or some other algorithm whatever it will have to kind of like go back and forth between memory and CPU right because like some things are in memory that you keep as a state but then in CPU you still need to do the computation right and so you you basically constantly like change the uh the context and so um you kind of lose some time uh in doing that uh in in the Apu they they pushed as far ahead as possible to go after uh massive computation like basically um they do it entirely in memory so they don't involve um any CPU or any other U units right so um and and and so they have like 48 million um Ram cells so it's highly packed maybe we can provide an image you know like one one chip that you can insert into the server and I think you can insert a couple of these to you should insert a couple of these to scale to 1 billion items right and if you have less you can insert maybe one um and so so basically each of these units uh contains like 48 million um Ram cells and then and then they have like so they roll roll up to like units that basically do the programmatic bit logic computation and uh in principle if you um if you quantize the vectors to the bit level then basically you are what you need to do is kind of like bit logic right so like big computation like multiplications or what have you um as you do the vector search and so basically like you can send um let's say 20 queries at at one single time and you might have one billion items to check against and they can massively basically run this parallel computation for all 20 queries at the same time so you will have to pre-compute the vector representation for your vectors but beyond that they will do this math massive computation and then they will return you results for each of these query vectors and then you can do whatever you want on the on the front end right so for example one use case could be you have stored queries right and you want to like see what uh new data items are going to be hit by these queries every single day right so for example for financial industry this might be very important to stay on top of things what's happening do I need to sell my stock or buy stock or do I need to do nothing and so um so this basically then becomes suitable not only for similarity search but also for image processing itself so I think they're using the same units even in space in some cases because so if you don't have I I don't know I'm getting beyond my knowledge here but like if you have like this complex Hardware uh with like CPU and like all these magnetic fields involved like in space you might get hit by the radiation which like shines directly on that device and so it melts or whatever but like if you have less of that so you have you only have memory you can seal it in certain way and then you don't you don't need like a cooler maybe even like because you don't have the CPU itself but of course you do need some cooler I guess or whatever so I think they're using it also in space so it's kind of like a versatile Tech uh but it's also now purposed uh for Vector search and we will like quite surprised I don't want to like oversell because I don't work for JSI but I had the exposure and and kind of like first-hand experience um and what surprised me is that it goes to like millisecond level and you have indexed like 10 million objects so if I if I was using um let's say elasticsearch without scaling without sharding and indexing 10 million objects it wouldn't probably go with millisecond it would probably I don't know like it would probably be like tenths of milliseconds or hundreds of milliseconds I'm guessing um it depends on query of course uh but like here I was like super super surprised like it was going like I don't know 10 milliseconds or something like what did it even do anything or maybe pre-cache everything but no it did compute from scratch so that was interesting yeah yeah that's mind-blowing and that kind of frontiers of computation thing is so fascinating reminded me of this the joke we had where when we decided to name weviate air weviate air and Ian goes now we can't build airplanes sending the weevier satellite but yeah so I think um yeah it's really fascinating I think kind of coming out of the approximate nearest neighbor you know the data structures the vector compression such an interesting topic I feel like this is kind of the most technical aspect of it in my view so so coming up one layer then we would have say how you vectorize the queries generally and you mentioned with the custom Hardware how you could have some particular way of batching the queries and you know with the offline query use case but so coming up maybe one more layer and this is sort of the part with the neural search Frameworks or maybe I'm going to get myself in trouble because I think that this kind of stuff should live in weviate we have well it already kind of does like we have the Q a Transformers uh Library where you can you Loosely couple the vector search with then passing it to the you know extractive question answering model we have summarization and this kind of thing so how do you see that next level of the pipeline where you you have the research results and now you want to process the search results a little more so you you might well I guess maybe one other little thing I'm not a little thing that I missed was the in the combination of vector search with symbolic filters and you know as a minute with how that would work with the a n index but so just as a note of for the completeness of the coverage but so now we have the results and we want to process them with maybe question answering summarization uh and and and we'll kind of if we could start from there and then talk about other things we could do yeah actually maybe I can take a step back um I uh it knew when I was working on this blog post about neural search Frameworks uh it took me several months because I didn't see the picture coming together yet um when I when I got the idea of publishing this and and just to give examples Beyond Gina and um Haystack you also have um managed neural Frameworks like Victora also relevance AI um but but and Muse but I also included habia um so and and like this is where I my you know my my head was blown away like I was like hold on a second like so we move we basically if we are looking at the vector search pyramid uh as we step as we take a step um you know upwards basically we are moving closer to the user right so because we we don't concern ourselves anymore we kind of like assume that n and layer is solved to an extent we can fine tune it but it's solved Vector search database is solved because it exists and I can pick a variety of them um so what what is unsolved right so why do we need another layer like why can't I just like you know sit down and have all these components in my hands and kind of use it as a Lego building blocks and come up with my app in principle I could right I could take vd8 I could take quadrant whatever and I could then build everything and vb8 offers a lot of these opportunities like um as you said like for vectorization inside the database so I don't even need to worry about that part right um and then you have these components for summarization and things like that and things of course blend uh now that we move to neural search framework level um but it still was very logical if you look at if you take a look at the blog you will see each card um you know for each of these neural Frameworks will have a specific field saying does it use any existing Vector database and if not what is being used if it's kind of publicly available and so if you look at for example Gina June is using uh vv8 quadrantelastic search and radius using as in you can choose one of them right depending on your setting Haystack kind of the same but they also offer an opportunity to implement directly with files and coming back to your earlier Point why you need to choose a vector database over um and an algorithm is because NN algorithm might not support the symbolic filters right so like if you take H and s w lib um you know of the Shelf it won't support the symbolic filters right and the same goes to files it doesn't support symbolic filters so it means that you will have to have some external database maybe SQL database where you will have to do this post processing step so after retrieving the nearest Neighbors From the somatic perspective nearest to your query now you need to pause filter them you know using the metadata filters but that can actually kill the whole list right so you will have to either over query and then hope that after post filtering some items will remain or you'll have to repeat it several times as effectively delaying the response to the user right so you don't want to do that so like you know databases like vv8 quadrant and so on have the support to do this in place right in the same stage as they said yeah so I think this is important right yeah and that that actually did change my perspective a bit because I'm I could see like with the with the doc array having a connection to like a graph database separately from wevia where you aggregate the things and then those go into the re-ranking layer the question answering layer and yes I can see that I think but then coming back to our not worry about too much and just your core value if if we can offer that kind of like how hybrid search is now in wevia and you can have the keyword search and the vector search and that kind of flow into your ranking thing that is very interesting um so kind of one emerging thing with the neural search Frameworks and it comes back to this idea of looking at different indexes a graph database a vector index uh maybe the face static and maybe you have some application where your data doesn't change and so then the face index makes a ton of sense so so you so there are these new things called Lang chain where the idea is it's uh the search framework is around something the the chat you I think I like to say Chad gbt instead of gbt3 because gbt3 is impressive but at gbt is super impressive so maybe we could call it gbt 3.5 or or four or whatever but like so basically the idea is it's like the orchestration layer where you tell the language model uh so like one way of doing it is this Chain of Thought prompting where uh you you get the few shot examples are showing you how to like decompose questions such as to like illuminate the compositionality like if you're asking um uh did uh Thomas Edison use a laptop so you you'd want to break the question down first to like okay when did Thomas Edison live when were laptops invented and then you've taken the two facts so it's like this layer on top that tells the language model like here are the different data sources you have access to uh so what do you think about that kind of like how how does chat gbt technology will influence uh the neural search Frameworks and then I and then after that I want to get into like the generalization to images audio G like Gene maybe like other kinds of data types but I think maybe just sticking with text would be a good way to sort of set the stage for this yeah for sure um and I wanted to still blend also with um kind of like what the value prop is in this neural Frameworks and and maybe as a segue to chat GPT how Chad GPD could change things so like if we take the example of um Haystack um so for example what they allow you to do is that query comes in you can have a node and and the way they model this is they have a dag type of thing right so they have a directed The cyclic graph and so they the query gets classified let's say with a query classifier that's one node and then after this query is classified it can depending on the class that is predicted it can either go to dense retriever or it can go to a keyword retriever right let's say maybe it's based on length or some other features that you know work so you you have a boundary in your classifier and and then but maybe in some cases even it could go to both of these and then you will have a further node that will read the results from this retrievers and will merge them and then present them in some way uh using I don't know RF method or some reciprocal rank Fusion or some other method so and you can like play with this you can you have you can have like different nodes do different things like um OneNote could be if you classified the query as a question you could do a question answering but if it was like a table uh related like SQL table so you classified it as a SQL compatible query so you could go to that node and say hey can you also query the table um you can also do like document similarities new documents come in so it doesn't need always to be that doesn't always need to be like on the retriever side it could be as part of your backend pipeline uh somewhere where you need to do document similarity and then decide whether or not to even compute an embedding for this document maybe it didn't change or maybe it didn't change enough to Warrant a new embedding and so you might discard it and so on but you also have these other notes which we talked about earlier about document extraction process so you extract things and you know proceed to to the embedding layer coming back to your question about chat GPT um I I had an exposure to it of course I I actually uh well asked it can you name my blog post because I was still stuck there and and I grounded it and I said hey I wrote another blog post about Vector databases and this is how it was called not all Vector databases that may take well maybe you can play on those words or something but it decided not to use the same sort of words and just gave me neural search Frameworks I had to had comparison I was like oh boom cool like you cannot imagine like you can do the work and then it leads up to the posting and you're like how should I name it when you go to your friends your wife and they'll how should I name my blog post like how do I know so in this like really strange situations you can reach out to systems like chat dpd like if I went on uh tactical or being a Google or something and I asked the same question I probably wouldn't get an answer I would get a bunch of links and like what should I do with these things like so this is this distracts me more than it gives me value but in chat GPD I got an instant answer and I was like I like it I spent maybe five minutes thinking about it and I liked it and I slapped it on the on the title that was fine so so I think in some sense maybe to me this wasn't even a search experience uh in this kind of basic definition or sort of uh the way we used to it uh definition that okay I I need to type something and then I need to examine links or examine some output go check the results and then decide myself like am I satisfied or not uh in chat Deputy you don't have any URLs coming back to you not yet at least maybe they will be added who knows but like today it's more like a companion that you can talk to and in some sense I was dreaming of such a companion maybe you know when you study you have all these books and papers and everything but can you really quickly make sense of oh can you can you like find an answer to that specific you know nagging question like you you had during the lecture uh it's super hard right so you of course you can go to search engine and start typing all these queries but here you can have like um kind of like a sensible discussion in a way of course I know some people uh even like hysterically are laughing at the results and so on and so forth so maybe it's not uh purposed for all situations and also for all audiences it was actually a discovery for me uh there was one linguist uh that I was following he said that um he cannot use General web search engines because every time he types something they don't understand what he they don't have the data it's not it's not even about understanding they don't have the data and so he needs to go to libraries and like read books that are not indexed in this search engines and things like that so for this very specific Niche use cases Maybe uh chat GPT might not work it depends on the data again um but I think it was surprisingly clever right if I can say so about AI um uh it wasn't always static and you you explained it well that it takes different paths in the tree when it computes the answer um and and the other question I asked like can you find a bug in this code that I wrote and it just licks memory at some point and it gave some sensible suggestions and and I felt like I know that it's kind of like a silicon there but like I cannot maybe like yeah um like I I still need to examine some caution and sort of not fully trust um maybe for Life sensitive situations or something like that you know or medicine or insurance or something like that but like things that I know it has indexed and and and humans have written that you know and and and and it has been sort of vetted multiple times and so also uploaded a bunch of times on stack Overflow if you were talking about coding and so there are some there is some evidence that this might be the answer but but I think it was still surprising that how it changes the perception of search even if we can talk about search in this case that it it actually generates the answer you know search engines don't generate answers today like Beyond maybe okay you.com and Google the youtube.com I think is more advanced in this but like Google has this Snippets you know where it says you know probably the answer is this um and so they commingle it with URLs uh but like in charge GPD you don't have any URLs it just talks to you and then you can continue the discussion I don't know it was fascinating but I still don't know if if this will make it into the necessarily search experience like so in search I I think it's very functional you know if I walk down the street and I see something on uh like on the shop window I take a picture and I say I want this um and so it finds by the image um so that that is still a search experience for me so in some sense maybe in the future you know we will have control F on everything in the world right so like as I walk everywhere um I can kind of mentally press that Ctrl F maybe in some device maybe on top of me or like a glasses or something I don't know uh VR uh but like today a lot of places miss this and and still there are a lot of contexts and situations when you ask yourself what is this do I know this you know and you have some other like uh subsequent questions but there is no way to ask them because you don't like you can pull up the phone and start typing and it's freezing weather and you're like oh my God it's like not it's kind of like a deteriorating experience but I think it could be so much more um interactive and multi-modal and I think neural search especially enables multimodality situations right and experiences so that you can actually like not constrain yourself to to the point that am I uh asking like a textual query or I just have a query I have something on my mind right or maybe I saw something can you tell me more about it um so I think I think maybe chat GPT might push us in that direction that not only it will find things but it will also reason about things and help you reason but but the creativity part I don't think it will it will disappear I I don't think at least not now I don't I don't see how AI can solve creativity part like create things for you yeah it did create the title um you know but but maybe a more creative person than me could actually create a better title right and things like that so yeah it yeah there's a lot that I want to unpack and I I do think this the creativity is kind of like a characteristic of compositional generalization and novel I suppose but I I want to just kind of tell you about one other idea that relates to how you had chat gbt uh come up with the title for your blog post and so I want to credit uh Bob Van light and Jerry Liu the creator of gbt index they included me on this call where uh they were you know hashing out their understanding of the gbt index top level indexing and I just think this idea is so profound on how we use chat gbt and it's the idea of when we search and we get like 15 results as you mentioned we need to like parse through the result and so like one thinking was like how about we use a Crossing like a high capacity cross encoder which is like going to be another like let's say it'd be like maybe like an 80 million parameter Transformer they there are papers where they use like big T5 models like billion parameter T5 models to re Rank and there's like this paper where you have the density on yes or like query and then you put the query document document and then yes no and you re-rank with that and you use high capacity models similar log prop that kind of idea but so this idea of like how do we parse through a bunch of results and then another idea was like okay well maybe we use a question answering model and we'll re-rank it based on the confidence of the extractive question answering model and we'll try to calibrate the question now answering models to to demonstrate uncertainty maybe Bayesian networks something like that but this new idea of having gbt summarize the results by having the original question and then saying please summarize these results you'll receive it one by one and then it receives it one by one updates a summary maybe as you mentioned like you would want to have the reference it could maybe say like oh and also please like you know keep a cue of the most influential uh results as you've been parsing through it and it's like ability to reason and do this I've been playing around with this a little bit I think is just super profound and that so that kind of summarization across results what do you think of that idea because I just am mind blown by it yeah this is amazing um I'm not as deep in this topic yet but I just understand surface of it listening to what you just explained you know there is still this trust component as well that and again this trust is just how we perceive it it was designed that way and we think this is the trustworthy way to uh that the information that I'm getting so if I get the URL from the search engine I can click and then see it with my own eyes and see when it was published by whom and maybe I can even reach out to that person and ask some questions um so if I have not provided with this information and evidence how do I know that this is true right so um or maybe does it even apply to my specific situation uh maybe it gave me way too generic answer and so I think um it would be interesting to see if if I have a list of people that I follow on Twitter let's say and I trust them on specific topic you know let's say the way I trust how you Corner publish you know thinks about specific papers or some breakthroughs and recent implementation in Vector search space um so when I have a specific question maybe I could say hey let me first check what corner thinks about this right so if chat GPT could kind of like bias the answer and include some of the hints from you from your timeline not not from your timeline but from the things that you published on Twitter um on that specific topic um and then include that as a supplementary material or maybe like a chapter sort of you know I mean the answer so I can I don't need to um you know go and check Twitter now I I can actually go directly to the information right um and I can read it um and then if I have a question maybe if I reach out to you I might ask a very specific question rather than saying saying like can you point me to a paper you know um in this topic so so I think I think um maybe Chad DPT made that first important step it created also a lot of like maybe controversy or some people uh I remember like one case on on stacker um on uh Hacker News was uh so this guy is parking his car in in wrong I guess in the wrong lot so like in the wrong spot in the in the parking lot and then he gets a fine uh and then he's like out of options and he's thinking okay what if Chad GPT can help me here and so he asked Chen TPT to write an email to this official uh you know saying oh yeah I put on my app and I paid for it but apparently I I put my car in the wrong um you know slot and a spot and so and so it was like like it's you know like how in certain cases certain situations like we when we zoom in on a situation and we're a little bit like stressed we lose words and phrases and that's probably why psychologists and Consultants exist because you run towards them and you say hey I have a problem calm down you know what happened that they will help you to walk through the situation and then they will say say like this you know dear official whatever I happen to park my car in the wrong spot I understand this is a big mistake uh however I paid for it in my app here is the receipt do you think and then it's like the computers they don't um I think this is also a good thing like to some people it's bad thing and it can be um I guess developed towards that they don't feel that I have a lot of emotions right they may pretend to have emotions like in ex machina movie right but like but I think um they are very calm and like okay your problem is this you know like Sheldon Cooper okay here is the answer and and sometimes it may help you and actually in that case the official came back and said yeah I understood the situation happens sometimes you know I will just issue a warning this time you don't need to pay fine and you are good don't do it next time that's fine right so probably I got carried away a bit but but I think but I think that coming back to your question like if I can if I know that there is a very interesting book or blog post and it's in my browser history it's accessible and I can give access to it uh to chat GPT it can go and personalize to my interest however biased it is I don't care maybe I do want to be biased I don't want a random blog post published somewhere well you can add it if you want but still can you bias to what I have read and I have already forgotten you know in several decades that I leave I've forgotten they read the book can you remind me of that uh snow yeah I think there is a way to take chat GPT to the next level in in terms of personalizing the answers yeah and I think well personalizing the answers I think it comes well coming to our other podcast where we talked about ref to VEC and the personalization vector and how that can filter the search results and then those are the results that go into chat gbc's context but I think the other thing you're saying one I think like this prompting is so interesting like one thing I've played with is write a write an argument between Ilya sutzka CEO of or I don't know what his job title is it open AI but I write an argument between Ilya satsgiver and Clement de lang from hugging face about closed versus open source models right like you prompted like that you give it like a background about each of them and the argument and I think also what you say about polite writing like one of my favorite ways to use chat gbt is I'm writing something I give it a sentence and I say could you suggest seven writings of this and it's funny you mentioned that it's just silicon because sometimes I'm like I'm like oh seven that's kind of a tall task maybe just three I think that kind of comes into also like when it's no longer free it might change how we use it a little bit too like like the way I use the GPS 23 API I'm like well I'm paying for the tokens generated so let me not prompted to give some long generation but so I think this has been a great coverage of Chad gbt and continuing on the pyramid I kind of want to hit what I see at the top level which is just the user interface like the you know like someone with CSS skills how they contribute and how they fit into this and I've I've seen uh like Gina now uh you know I know we have some stuff that's not out yet but like on this kind of the search interface and I know like usually you know you just have like a search box right like it's just the bar is like kind of the interface but say you're doing like image search and you want to like click on the images you want to fuse that with the search box and I don't know do you see like a lot of in I know like you.com and it kind of comes into what you're saying with how you have the evidence as well like you.com they have this interface where they split the search results and then the chat gbt like this interface do you think there's opportunity for Innovation at that layer yeah uh I think one one part is innovation and another thing that I've been thinking about is let's say there is a existing player and they have a search engine let's recommend recommender system or something and they're thinking okay can we experiment with Vector search but we are probably not like hundred percent sure yet is it going to fly or you know can we expose it only to sort of like the power users in some sense uh I think genome had this kind of like um early stage Demos in some sense early stages in maybe you don't go with this in into production but it helps you to reason about um and and test um what's more important I think um the influence of neural search on your um you know search engine experience so they had um I think they had this search engine where like a demo where you type a query um and then you have this slider I guess so you can say you know have more weight put more weight on keyword results and then you slowly blend this into the neural search and then and then you can choose like okay it's going to search directly in the images let's say using clip model um but still combine the results so still ask the keyword Retriever and then combine the results from the dance retriever from the clip model and sort of like show me where these results land right so if because for some queries it doesn't make sense to check the image because it's succinct enough and it looks like it's going to match a specific metadata you know item filter or maybe title of that object so there is no reason really to go and examine the image because it doesn't contain this information and vice versa 2 like in some cases when I say I don't know like can you give me a picture of a bear eating a fish in the beaver you know and there is no such style because you do have an image of a varies in the fish in the river and you're like yeah maybe if the model and we have done this in the clip search engine demo that essentially it it surprises you blows your mind that the clip model can um summarize its understanding so to say to this level that it will match anything you type like that so you say a bear and it understands what the bear is it's not an author and things like that right so this is very interesting of course it goes back to you know contrastive learning and people have enough negative examples which are semantically negative and not just random negative examples and things like that but they also uh came across I wonder if I can pull it up um but when I published this blog post about neural um search Frameworks um uh I came across uh this company called Nuclear So I didn't didn't yet include them in the blog post I need to study this a little more but there was some interesting thing that they offer um a way to compose the UI as well right so it's not only this is coming back to your question you know can we uh sort of make some break breaks throughs there of course if it's an established player they have the dedicated front-end team they will probably figure out what they want to do and they have an existing product but if you are on the verge of experimenting right so you're still there you you have your your like mind open in many ways you you don't know what UI will be in the end they offer Beyond you know um a database font structured data they also offer you a number of components on the front-end side so you can basically compose the UI the way you want um and I think like you.com I think you mentioned also experiments with chat GPT like answers right so not only the URLs but also kind of like an answer which is more interactive and maybe you can continue the discussion there in that box um I also want to like give an example which which was like pre-neural search era in many ways at Alpha sense like like when I look at document search engines you know let's say it's an article search engine or patent search engine uh usually you will have like um several stages as you go through the UI as a workflow so first you need to type the query then the screen changes to the list of documents sorted in some way then you click on the document again the screen changes and it opens the document right what alphasense did really really early on like 2010 was to have what we called a three pane UI so you have like you have a search box box at the top uh you type the query press enter you get a vertical uh column you know a column with the results not too narrow you can still figure out what is what you can read titles you click on that it gets the snippet pane so you can actually quickly gather okay is this relevant to me or not so you have like several stages but it's all on the same screen almost like an Amazon checkout page right kind of and then as you click on the snippet it pulls up the document however big it is maybe let's say PDF document thousand Pages it will load only the necessary chunk of that document and it will jump from that clicked snippet to the relevant section of the document so now you basically have all these you know tools in the same view and I think this is very powerful because it saves a ton of time you know because you you need to always kind of walk the in the shoes of the user what does the user want to achieve with your not with your product they hire your product uh to get some job done right and so is your UI efficient enough uh in in in uh you know in satisfying that specific uh request and so so I think this was very interesting and then some competitors tried not only tried but copied you know this this View and and then the history goes on but but but I think this was a very interesting breakthrough even like before uh neural search but with neural search a lot more doors open because and also it brings that complexity layer that now product managers engineers gotta simplify so like you need to simplify the complex always because users will not have time to figure out really complicated designs or uis however flashy flashy your UI is if it's not functional it's going it's not going to fly so you need to kind of like simplify the complex and now these open doors with multi-modality right so like all of a sudden your query can go directly into inside the image now it pulls up the image you need to explain to the you you cannot highlight like a snippet inside the image right and say hey there is an arrow here that's the bear you will you are asking for so like inside the patent like in patent search I spent some time uh I was part of the board in one big Enterprise basically examining the patent to be the patent applications and a big chunk of work for a patent presenter would go into examining the prior art and so they need to examine a ton of patents and figure out whether or not they overlap with that specific button or not right and so this means that the search the search workflow changes from get me something on the screen and I'll decide if it's good enough to can you give me everything there is on this topic so it's like a long long search you paginate like how until like 200 page 200 or something and you gotta have like 100 results on the per page or something like that so they spend like days examining just one query and then you could go back and say oh my query is missing this term you know let me change my query boom I have again the new list of results and now it would be cool if the system showed me the difference right and this was another feature we had at Alpha sense you know as new documents come in let's say you have a uh let's say if it's a public company and they publish uh a 10K report which is like a yearly SEC filing um it might include portions of a quarterly report from a quarter prior and so and also like that 10K which was published a year ago they don't actually rewrite it they just change some numbers right you know like performance our Top Line whatever so like you don't want to reread 700 Pages again to learn has something change with this guy you just want to see oh the Top Line changed and also you know they spend a lot more on production of that sort and like okay now I need to go back to my Excel and input these numbers and see what happens to the stock price prediction right so I mean always think I don't know if I answer your question well enough but like always like forget about Vector search forget about learning to rank you know however sexy those are and those are super cool I mean I'm excited myself always go back to what user wants right like if they're driving to that destination as we compute in TomTom always remember I'm sitting in the car it's freezing it's dark I want to get there so so show me that top result as soon as possible so I don't need to type that long you know uh sitting in that freezing bed or whatever so so it's not I think it's a good exercise to always kind of go back and maybe talk to the users as well um in some cases it's Mom uh maybe complex but I think still that pipeline could be established and you can start asking questions okay what are you trying to find what is your normal sort of workflow and use case what are you trying to optimize for and they will give you very interesting sometimes confusing answers maybe so you can drill in and find a sort of a more detailed version of what they wanted to say um but then it comes to it rolls up to this bigger picture ah I got it uh we are just missing one button here something like that and then yeah yeah um Barbara taught me about this jobs to be done framework for thinking about this and and I think at the use and the jobs to be done it's like this business school thing about like why did you hire the donut to do the job of uh something to eat on the way to work I think the user interface layer you think the most about the job being done so I think that was a great and yeah the job to be done has cascading effects to the whole pyramid and the different requirements all the way down I think this is a great uh summary of the pyramid and then a transition into a really fun topic to wrap up with which is um this idea of renaming Vector search to maybe relevance application yeah yeah no I actually wanted to pick your brain on that because I know you the moment I posted uh that podcast and if you have uh responded and that was like a long answer and I was like oh I got Connor and he's like really passionate about this topic I think this is important like I I will I will tell this disclaimer it's not my idea but I like to throw in some thoughts which are of higher level um and maybe the goal is to not change the course of the industry but more like in form and give another perspective and I think this perspective came from Doug Turnbull on on one of the episodes in one of the episodes of vector podcast where he said if I was to give an advice today to the vector search engines to the vector databases I would say stop calling yourself a vector database and it was like a cold shower already like stop calling yourself a vector database and call ourselves what and why but he spent a lot of time in search right he wrote the book relevant search as well corroded um and he's uh co-writing AI powered search as well and so he said well think about the nutshell of what you're building right it's like what um new area you are discovering right on one hand we are all building or participating in the construction sort of like rise of new industry but in the end of the day it either solves or doesn't solve some use case or plethora of use cases right so we claim that we are moving to semantic search level right something that didn't happen before something that was hard to achieve and you know maybe with custom synonyms uh synonym tables or with bootstrapping another Shard to handle that language now we have multilingual models so they handle multiple languages and the same representation and so it's the same shared geometric space which is kind of like super cool like enhance I can ask my my question in one language and get it searched in another language then you still need to deal with the answers like do I need to translate them back to the source language or something but that's another story um and so so he claimed or he kind of like suggested that in a way we are solving towards uh relevancy and so he said why don't you call yourself a relevance oriented application and he wasn't stubborn on that specific term or like that specific phrase but I guess the key word for me that stood out its relevance right because and then I I also weared my hat of a product manager and thought okay if I go to my boss at Tom Tom and I say hey let's um bring this semantics into the mix and guess what we need a back to database so let's let's acquire back today based license or something I think the first question would be what the heck is a vector database it's like vector clocks I used to have some quarries or something no I'm just I'm just I'm just joking here but I'm saying that you know or like it becomes like this kind of uh uh engineering uh lingo um or like they say Greek right so speak English I don't understand you and so then you say oh you know Vector databases it's like a new breed of databases um it's like the next stage Sequel and it's like sequel wait a sec let's just forget all this it's it's um it's basically like moving everything using deep learning moving everything to deep learning okay uh hold on um so semantics you get the word semantics right so like we move to semantic uh level searching and um by the way we can ask natural language questions now oh natural language questions what is that so it's like well just just normal questions instead of typing keywords you can basically you see what I'm doing I'm basically like stepping back and back and back and I'm kind of like degrading the terminology from oh this is like a super cool aircraft uh you know and I've used all these materials to build it all these glowing buttons like what can it do it can fly you to Mars Mars I don't I don't want to go too much this is what I'm saying like like step back as far as possible back and say you know remember we have this problem in in in TomTom sometimes people ask questions so they say can you drive me to a lake or something so it's like they don't type like an address because they don't know the address of the lake like doesn't have an address maybe it has the coordinates and so we have certain percent of these queries and maybe we can tackle them with this new tag then you can maybe say Vector databases are the new breed of databases but like yeah I guess when you when you enter a discussion with on a prepared customer uh you might not start maybe it's not a good idea to start with Vector databases some technical term instead you know talk about semantics or relevance or something like of that sort yeah let me see if I can do uh can you drive me to the lake I think maybe let's see if that example can show the different intents that that could entail so so maybe you would want it to be asking it like are you capable of this task and it would say like yes I could show you how to drive to the lake maybe maybe it would find another question where someone else had asked something like that or you know what you want probably is like the Google Maps directions current location to the nearest lake like so I've been seeing this like intent ranking papers like task aware retrieval with instructions and the instructor model where uh yeah like there's different intents for different search tasks like so with core I duplicate question detection like the academic data set you're looking for another question not the answer to the question and and so it's I don't know too many k like it's kind of like the idea where you encode The Domain in the task like you say search me a paragraph in scientific literature compared to in Reddit like it could have like find me a you could say like find me a conversation on Reddit which is a different intent than um find me the answer from Wikipedia is this helping when I'm like this kind of so I don't so that's kind of how I'm thinking about it I mean I still think I I still think the vector search database I like that so much because it kind of comes all the way back to the pyramid where it's like this coupling of the a n and then the database stuff and just maybe it's particularly how it's done in weeviate but that those two are so tightly coupled sort of I see it as that that's sort of like what I see as the novelty but yeah it's really fascinating I think relevance podcasts welcome to season three of the welcome uh relevance podcast yeah actually I was I was I was surprised or maybe um it confirms the bias in a way that when I was writing this blog post about neural search Frameworks one of the players is called relevance Ai and when I was talking to Daniel vasilev who is the CEO of this company and co-founder um he was advising me to even stop using the word vector and embedding and I was like what do you mean and he was saying well you know our user base and our Target user base is not necessarily Engineers it's not necessarily deep learning researchers it's um anybody it's uh someone working in HR and they have a bunch of CVS and I guess they want to like take a look at them from a different angle quickly find a candidate maybe plot some characteristics you know quickly cluster them things like that and so and so they don't um they don't uh you know so the progression of thought is not like this I have a bunch of CVS and then based on the embeddings I will compute the Clusters and then I will run Vector search no they don't even know what these things are they go on relevance AI platform they upload CV so they point to some archival like Cloud URL and then the system pulls them down and basically extracts all of these things that we described in this in this episode you know the workflows it does it behind the scenes so you don't need to worry about it and then it embeds the uh the documents and then it basically gives you the search prompt so you can search them right so so they they target I guess a completely different um user base right so like if you if you if you contrast that for example what Haystack is doing and I was also talking with Malta a lot as I was prepping the blog post um he was saying that this is kind of like a development kind of like an IDE in a way right so like um deep set cloud is kind of like an ID for integrated development environment for um an engineer or researcher or maybe both of them collaborating and they can even chat so you can find references of the discussion that happened earlier and like go through that and things like that and make decisions together and then plot metrics so it's a different it's a different like user base right like the target user ideal user is somebody who like understands coding and understands the concerns of scaling and cost management and whatnot and so they are much closer to the actual uh platform creation right so but like if you took like someone reached out recently to me on LinkedIn uh in product management capacity and they said yeah we know that you published all of this but can you give us like a couple blog posts um or maybe a podcast which introduces us to this space so I I don't think I will send them like Haystack URL right like because it will be way too technical for them and and hashtag has like excellent documentation but it's purposed to engineers and researchers in a way right and then of course there is and and there is a product for every of these um choice or sort of business model right so like a product to be created so for in in the case of haystack they have deep set Cloud so you can basically subscribe pay for it and don't need to host and like worry about how I scale if something breaks whatever right same with vb8 you know you guys have a cloud as well and things like that um so so I guess to wrap up this thought is that it kind of like depends on what your ideal Target user is right um and also what um use cases you have uh tested your Tech with you know that another use case like this we know how to how to work through it and so that's why in each card in the blog post you will also find use cases um list right and and I tried to Focus not on if I could not only on sort of tech level or kind of like algorithm level thing but actually specifically on the end user facing use case for example in the case of haystack they have you know document information extraction but they also have um you know getting Revenue numbers from a financial report this sounds more like a specific use case now right or like reason for a legal claim so it's not it's not it's not like um okay I have this neural network how do I plug in that into quadrant of V8 Pinecone right um is there a plug-in architecture that I can Implement you know what I mean like so it's it's a different level of abstraction and it's a different level of discussion as well and so so I think when Doug I believe kind of like deciphering his thought process was to kind of stop talking only maybe only about tech and and by the way I love Vector database myself that term my podcast is Vector podcast many people think it's about Vector search but it's not only Vector search it's it's like I in the in the about uh on YouTube I actually write that it's like a vector as in what Vector you have in your profession and life in a way but many people come back and say it's metric on the creator of vector search podcast um it's not Vector search but it's just Vector podcast but but but that's okay yeah Vector podcast very cool so I guess to wrap up again uh really close on often this that um so first of all a it's it's about the niche it's it's the not the niche but the sort of your ideal user that you're going after at B think about are you limiting yourself unnecessarily in the way of sort of how many users what type of user you could reach with your um with your system with your engine because you said for example bb8 code in principle eat away some functionality from neural search Frameworks so it kind of like begins to occupy these two layers and this Vector search pyramid right um or Bland them and that's why and that's fine but like is it is it limiting to limiting to say that it's only a database and it's only a vector database maybe it's more than that right so that's just a couple thoughts there yeah it's really enlightening and I want to give credit to Sebastian Whitlock and the devrel team at weviate that's helping me think like this like um I guess like me personally I have a background in like doing academic research in the PHD and you know reading machine learning papers and thinking very along the lines of like vector search database makes a ton of sense right well like um like I think one example is um Erica Cardenas on the weba team also she created a like a dog image search demo and so it's like there could there are like layers to how you want to present this you could it could be like just point us to a folder with images in it right that was like the the most simple thing to do or it could be like okay well we're gonna actually encode this into a base64 encoding we're gonna use a resnet we're going to use the rezna 18 is you know probably don't need the 32. so it's like how much detail do you want to cover and I think it's super interesting and and like by communicating dating that way you'll unlock the like creativity of the people who are thinking it at the higher application layer and you know paying the bills the other way is good because I wonder how many times we've said that phrase in the hour and a half yeah yes it's like decision makers right sometimes you sometimes it actually is going the opposite way it's like um you may spare some time as an engineer or researcher uh test some algorithm show results impress yourself impress your colleagues and then this rumor will travel to the level of product management decision making above product management as well and they will say wow this is super cool can we release this to prod now right um does it happen frequently I don't know it depends on the company I guess it depends on the culture and I guess we could also spend some time talking at some point you know how companies are structured and how you have this I think it's Conway law right so that um your product is a result of how you have uh organized people in the company because they will have like teams silos maybe at some point even and some teams might not be talking to each other as much as you think they could and so your product will be a product architecture will be a result of this information architecture which is also very interesting um but for startups it's probably not the problem for startups you can talk to anybody anytime right but you still need to get job done so that's that's also another perspective um so yeah I mean this is very interesting topic and in general um a lot of angles to take and if you always like remember whom are you talking to and and just to wrap up on the thought like if the if the Breakthrough doesn't happen from the engineering level can it happen let's say and product management level because you have resonated with how they think and you know what type of issues they're trying to solve and you have stepped back from the technical terminology and you started talking the lingo and they said here is what Vector search actually is then they will go back to the the drawing board and say ah this is what it enables us to do and now we can search inside the images wow yeah yeah well amazing I think this was an amazing podcast beginning from talking about the opportunity cost of bad search in retail and then digging it into okay so you're building a neural search framework there's all these components to it your famous pyramid diagram and walking through every step in detail then coming to thoughts on chat gbt yeah and I think just the user interface the job is to be done this whole thing and the renaming of vector search has been such an incredible podcast uh Dimitri thank you so much for your time coming on the weba podcast and thanks also so much for hosting me on the vector podcast I'm such a fan of the vector podcast and what you're doing it's so it's so interesting to hear all the different characters and I think sort of the role you're playing and being like the mediator of the market this way and hearing all the diverse voices hopefully I didn't give up too much of what we're thinking but this kind of diversity of hearing what everyone's thinking it's just so fascinating and thank you so much thank you so much Connor for hosting uh the end this was amazing time always uh glad to and happy to exchange with you all these thoughts and uh almost like in the brainstorming fashion yeah and I'm excited for the future of this with Chad gpg and all the breakthroughs that you guys are working on and and many players on the market as well thank you ", "type": "Video", "name": "dmitry_kan_on_neural_search_frameworks__weaviate_podcast_34", "path": "", "link": "https://www.youtube.com/watch?v=1ebCUCUJraE", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}