{"text": "Hey everyone, thank you so much for watching this explanation of \"Gorilla: Large Language Model Connected with Massive APIs\" ... \nhey everyone thank you so much forwatching we've been on YouTube I amsuper excited to tell you about thegorilla large language models we'reabout to release a ton of awesomecontent about what we're doing with thegorilla models a podcast with shashirand Tianjin but let's kick things offwith an explanation of the paper all theexperiments all the details of it butfirstly let's take a step back soshortly after Chad gbt you just bleweveryone's Minds with its ability tocommunicate and complete tasks innatural language people soon realizethat you can connect these largelanguage models like trashy BT toexternal tools to enhance theircapabilities even further these externaltools typically referenced searchengines or maybe code executors orcalculators like the Wolfram Alphacalculator or just you know a basiccalculator as well as say how to sendemails how to book calendar appointmentsor how to you know purchase a flightticket connecting llms with these toolshas kicked off this area of researchknown as agents and all sorts ofexciting things with doing this so whatgorilla is adding to the table is it'streating formatting these API requestsas a deep learning task so deep learningresearch has typically been organizedbased on demonstrations of input outputexamples like text classificationquestion answering summarization andgorilla is leading the way in treatingformatting API requests as a task tofine-tune deep learning models on theyhave an amazing ablation showing thedifference between fine tuning with andwithout retrieval and the rule ofretrieval in this which obviously withweavate we find this super interestingand then of course they have thesereally interesting details around usingself-instruct to generate training datato learn to write API requests and allsorts of interesting details that ofcourse we'll dive into in this video sothank you so much for watching and let'sget into it so just in case you're in ahurry and just want the general overviewof gorilla here's my attempt to compressthe key ideas in two minutes beforediving further into the details in therest of the video the high levelmotivation is to equip large languagemodels with tools tools are described asform adding API requests so open AIfunks has been a huge like related workprobably the current state of the artwhat I think most people are using forthis where openai funcs introducedhere's how to present a Json thatdescribes the name of a function adescription of what it does and how toformat input arguments to that functionto then pass to the openai largelanguage models and I think openai isdoing a little bit of fine tuning onthese jsons I'm not exactly sure but Ithink it's either some fine tuning ofthose jsons or it's just kind of donezero shot where these large languagemodels like gbt4 they are as we'll seein the details of the paper they arereally good at taking some kind ofdescription of a function and thenlearning how to use it like you can dotext to SQL this way all sorts of waysif you tell it here's some kind offunction and how to use it the modelsare really good at using it so examplesinclude say sending an email getting thecurrent weather and then importantlythis is this is kind of the big detailof how do you write API requests is howdo you format the input argue argumentscorrectly so in this case you have youknow send email to say Bob inlight bodyyou know how are you like these twostrings or you can imagine it's like uhyou know maybe you have like a date whendo you want to send the email soformatting these these argumentscorrectly is the key detail now here'sthe super interesting thing the open AImodels they are great at this but theyare massive language models like youknow 200 billion parameters and so on toserve these and to do inference isexpensive so if you can com the gorillamodels are compressing thisfunctionality into this seven billionparameter model by fine-tuning thatllama 7B the open source large languagemodel from meta AI they're fine-tuningthat model to do this and then thedifference is that this is way moreeconomical to serve this kind of APIrequest if you compress it from you knowhundreds of billions of parameters toseven billion parameters and they'llprobably you know keep compressing it asthis research continues to evolve so thekey idea behind how it worksis that we have these examples of apiswe use self-instruct where we take anexample of an API and then we promptgpt4 to say can you write an example ofan instruction that would want to usethis API and then that then becomes thetraining data to compress to train thisnew language model the Guerrillalanguage models so there's also a reallyinteresting ablation about usingretrieval in this and this is I lovethis part because retrieval awaretraining is I think one of the mostexciting ideas on The Cutting Edge whereyou use you take the prompt and then youretrieve and then you use the retrievecontext during trading not just as likea you know fit after you've trained themodel you're going to be training themodels with this retrieval and they do asuper interesting ablation exploring thedetails of trading these models with andwithout retrieval or at inference timethan with and without retrieval so a lotof interesting stuff there soparticularly what gorilla is is Gorillathis first research paper is aboutformatting requests to call Deeplearning pre-trained models so I takingmodels from the hugging face model Hubtensor Hub and pytorch's torch Hub weinstantiate these models based on somenatural language instruction so if youhave the natural language instruction Iwant to classify the gorillas in thispicture then the gorilla large languagemodel would write the python code toextension instantiate that like an imageclassifier from hugging face and maybepreferably one that's been trained onlike animals and this kind of thing andso that's kind of the details is knowingwhich path to like some of the kind ofinput arguments that are required fromthis is mostly kind of which path to putinto the argument but also sometimes sayyou know you have like the resnet objectdetectors they have like these differentuh feature extractor backbones so thereare some details to how you kind offormat the the requests which we'll seelater on in the videos we look at likethe abstract syntax tree and all thiskind of stuff but so this is kind of thefirst work of gorilla is you knowlearning to call model inferences andthis is there's another paper calledhugging GPT which is like this where youthink of these pre-trained models askind of like the API zoo and thecollection of tools that the largelanguage model can use is other modelsspecialized to some particular thinglike say you know gbt4 calls the dollyimage generation model and orchestratesall this kind of stuffbut the authors are collecting this APIZoo data set on their GitHub and I thinkthis is extremely powerful the authorsdescribe supporting web scale collectionof potentially millions of changing apisit requires rethinking our approach tohow we integrate tools it is no longerpossible to describe the full set ofapis in a single context many of theapis will have overlapping functionalitywith Nuance limitations and constraintsso they're building up this data setthat I think will just be super powerfulI think it'll be insanely exciting tosee how this plays out for example we'relooking into adding our weeviate apis tothis API Zoo data set to so to give youa quick sense of this before you know wehave more content so I'm not going todive into this too much yet I'll talk alittle more at the end with sometakeaways about how this you know how Ithink this connects to the weeviate apisbut for example if you want to use thecohere re-ranker you know you have thisgraphql syntax which near text is howyou use a vector search and we V8 limitis how you limit how many search resultsyou get back and then you have thisre-ranked syntax so teaching the largelanguage models how to format requestslike this yeah I think it's going to bea game changer for bb8 and so it's alsoexciting so in conclusion here are someof the key ideas so firstly API syntaxfollowing is a deep learning task for asmaller specialized large language modelthe use of retrieval aware training Ithink this is the first paper I've seenthat's really doing this in an excitingway and that's where you're integratingretrieval into the training not in kindof like the end-to-end differentiablesense where gradients from the LM goback to the encoder but kind of justusing this retrieval to supplement theinputs and help reduce thathallucination by turning the taskbasically to copying where you just haveto kind of like look at the input windowrather than like have all thisinformation like in your brain so to sayif you're a nola but and so then I thinkthis API bench data set andunderstanding the potential of this APIZoo data set as this continues to evolveis so interesting and then understandingthe details of how they evaluate thesemodels okay so the first step tounderstand the experiments of the paperis to understand the new API bench dataset the authors publish and create forthese experiments and how how they useself-instruct prompting to generatesynthetic training data to fine-tunethese models for Tool use so startingoff collecting a data set of model APIcalls they're taking models from thepytorch lets you upload models to thetorch Hub tensorflow also does this withthe tensor Hub and then of coursethere's hugging face the hugging faceplatform hosts and serves about203 681 models so you know it's it'samazing this model collection thathugging face has created and it's reallyinteresting to think of what will becomeof that this particular researchresearch like hugging GPT treats thesepre-trained models like they themselvesare the tools that uh you know some kindof orchestrating model would want tocall like say you know you call theimage generation maybe you call theimage segmentation model you call theyou know the natural language inferencethe question answering like kind of thecreativity of thinking about these testsand then you know because so here's kindof stepping into the details of allthese models that it exists they filterthese models out by picking the top 20models from each domain and I think Ishould have bolded domain domain's apretty interesting concept because onone level you could have domain in thiscase they're using it to mean what Iwould refer to as modality or they haveseven domains in multimodal which meansyou know maybe it's like the clip modelthat you use text to search for imagesor you know this kind of thing they haveeight in computer vision 12 in naturallanguage processing five and audio twoand tabular data and two andreinforcement learning so you know thatkind of idea of like you you have like apodcast MP3 file and you grab the yougrab the whisper model and then you putit into the whisperthen you say grab like a textvectorization model to put it intoleviate or something or you just I don'tknow summarization right out of whatevercame out of the whisper model but thiskind of like combining models I think isquite an interesting idea but to get asense these are kind of like the modelsthat they take to combine into this dataset so each entry in the data set is aninstruction API reference pair so inthis case an API reference would be likeum you know Auto model Dot frompre-training the hugging face sense andthen like the model path and if it hasadditional arguments like what thatentails so we have 925 models fromhugging face 626 fromtensorflow Hub and then 95 from torchHub and the 1 645 API calls areorganized into this Json that has thesekeys so the key so you know the keys inthe Json are the domain so you know anddomain would be like computer vision solike domain computer vision frameworkhug and face functionality like what itdoes the name of the API the API call anexample of it the arguments it takesmaybe the environment requirements likeif you need like a GPU to run it orexample code which I think is similar toAPI call we'll take a look at this inone second uh the performance this is areally interesting detail with respectto how they evaluate API calls withconstraints because uh you know say youwant a 10 million parameter imageclassifier so you can run it on yourphone but then you also want it to havelike at least 75 percent image in thattop one accuracy so you you kind oftrade this off with the constraints andthat's a super interesting detail ofthis kind of paper and research and thendescriptions like a natural descriptionof what this is so let's take a look atan example of how these uh how this Jsonis formatted okay so I'm not completelysure I have all the details correct herebut here is a basic overview so I wentto the gorilla repository where you cansee uh the API bench and then huggingface.train so I grabbed one of these andthen dropped it into the three backticks Json to make it look a littlenicer uh so anyway so you have thesekeys that describe each of the apis sosimilar to the you know the open AIfunks he sees like how it's what thearguments are to make the API call andum you know like how to how to call itso in this case instruction our customeris a robotic manufacturer of cleaningdevices they ask for help on solvingusers questions okay probably okay soquestion answering so so here's a senseof it I'm not sure I've completelyunderstand the details yet but basicallythinking about how you want to organizethese API calls into Json such that youcan you know either prompt it to come upwith the instruction of when you wouldwant to use this or a description ofwhat it is this is again thatperformance thing is so it gets like youknow 78.4 I think this is recall withinwith the embeddings encodone model soyou know on these different questionanswering data sets like trivia QA orTrek or web questions Squad sothis is kind of a sense of what what itis organizing these apis in this kind ofJson such that you can eitherself-instruct the gbt4 model to producea natural language command like this ofof when you would want to use it as wellas like then this being kind ofsomething you would retrieve to put inthe input to then format the API calland all this kind of stuff so the authorState we employed gbt4 to generatesynthetic instruction data we providedthree in-context examples along with areference API documentation and task themodel with generating real world usecases that call upon the API so you havesomething like as shown previously theDPR question encoder and the model wouldhave to the gbt4 model produces somekind of natural language command of whensomeone would want to use this kind ofthing like I have you know Airlinemanuals or whatever and I need to answerquestions about it so then it knows likeokay I translate that natural commandinto the API request for this particularmodel or as mentioned like the I wantedto detect the gorillas in this image torep to then be mapping to that objectdetection model API so the authorsconstructed six instruction API pairsfor each of the three model hubs andthese 18 points are the only handgenerated or like handcrafted data andthis is a huge paradigm shift for deeplearning research which thisself-instruct thing this generative dataaugmentation synthetic data cannot beunderstated enough it most of these deeplearning papers in the past the authorshave been like hey we've labeled like ahundred thousand examples and this kindof like collection of this data set hasbeen such a integral part for doinganything in deep learning and now byusing they they could they collect like18 examples again of how to write aninstruction API pairthen you just give that to gbt4 and itjust generates the training data to thenfine-tune gorilla taking as input thesynthetic instruction and then the APIreference and then the output is thecorrect API call so a high leveloverview of this full flow we've curated1 645 API calls from tensor Hubtensorflow torch Hub and hugging face sothen we build up this API data databaseof this which we use through the APIreference retrieval later on but thenfrom these examples we have theself-instruct we create 10 instructionsper API so now we have a data set of 16450 you know inputs being the naturallanguage command and then the API is theretrieved API or maybe we use the Oraclecontext or we use the exact API in theinput and so then these are used toOutput the correct API call for callingthis particular model that was used tocreate one of the 10 syntheticinstructions per the API call so then webuild up this API database soso again one of the instructions couldbe I want to see some cats uh dancing incelebration so this is like a call tothe stable diffusion model so the inputthen becomes uh the task generate imagefrom textum I'm not exactly sure about thisdetail I think that the it depends onhow you want to format this for sure butI think one way of doing it would beinput and then hashtag hashtaginstruction I want to see some castdancing and celebration and thenreference API would be the retrieved APIwhen you turn the say it's Vector searchyou turn this into a vector and thenretrieve the nearest neighbor vector andthen that becomes the reference API orthe you know you can see already howlike bm25 search would be terrible forthis how you I want to see some castDancing in celebration you don't haveany good keywords to return like an APIfor that so another great case of avector search example but so then youformat this input this input goes togorilla and gorilla outputs the call forthis model stable diffusion pipeline Dotfrom pre-train stability AI slash stablediffusion Dash you know 2-1 and then youget the generated image of the cats sobefore continuing on with theexperimental details of the paper Ithink it's worth checking in quicklywith our friends at hugging face andlooking at this amazing repository ofmodels they have so the paper it claims203 000 I imagine this paper isn't morethan a couple months old and we alreadynow have nearly 300 000 models in thehug and face model Hub so you know wehave say this stable diffusion model wehave of course the Llama that's like themost famous model in the world right nowI think this new Desi coder I've seenthis is like a model that's been youknow trained for code so you see how youkind of have like these different kindsof models like this you know Med llama 2is probably like a llama tube fine-tunedon medical information or let's see whatelse we can find yeah I mean the stablediffusion ones these are like thegenerative image models I do think kindof generative image models in largelanguage models those two kind ofcategories of models dominate most ofthe you know public discussion aboutthese models butanyways you can see these kind of tagslike you see text classification tablequestion answering that's a prettyunique one like if you need to answerquestions about a CSV table you mightwant one of these models uh you knowlet's see what else we have in audiotext-to-speech that's a really cool oneso yeah you see how um you have thesedifferent domains multimodal computervision natural language processing it'scrazy like the kind of Robotics takingthese models that do like continuouscontrol but yeah I just thought it'd beinteresting maybe to just take a look atunderstanding further that there arethat what gorilla is doing is you'retraining a language model to you knowform to instantiate one of theseparticular models whatever is bestsuited for this task so like if you sayI need to generate a video of catsdancing then I guess this text to videomight be the way to do it maybe you wantto decompose that too so like you knowfirst writing a story about the movieand then doing it frame by I don't knowbut like you can imagine just this kindof repository is this the best way ofthinking about the collection of apismaybe yeah it's definitely a prettyinteresting thing okay now I likeeverything about this paper but this ismy favorite detail so training gorillaablating training with and withoutretrieval so quickly retrieval augmentsa generation most of the way that peopleare using it right now is you train thelanguage model without retrievaland then at inference time you give itretrieval but what they're going to bedoing is they're going to be trainingwith retrieval as well and reallycollecting the experimental data on youknow is this worth doing what's thestate of this so to kick things off uhthey're so they're going to be trainingwith and without retrieval as well asablating at inference time with andwithout retrieval so inference justrefers to making predictions with atrained model if that wasn't clear soyou know when they say zero shot theymean it has not been trained withretrieval and retrieval means trainedwith retrieval so they're going to betraining llama 7B llama 7B a largelanguage model by meta and the thefinest open source model to date so thisis how I understand how they do thetraining and I hope this is correct butmaybe if someone else has aninterpretation of this it would be youknow if you could leave it in thecomments and maybe we could see ifthat's more accurate but I thinkbasically what they do with standardinstruction fine-tuning is that you havethe ground Truth uh API call to generateand so you force the model to you knowso the model with its first tokengenerated it might not be correct butyou still can like force it to do thosedecodings and then you would multiplythose probabilities out and then youhave this proximal policy optimizationwhere you just do like plus one minusone did I like it did I not like it andthen you send that reward signal backthrough the probabilities of the entiresequence that generated that was theprobabilities put to the ground truthAPI call so that's how I think they dothis maybe another way to do it would bejust kind of the standard languagemodeling loss where you just you knowlanguage model each of the tokens of theground truth API call you could alsomaybe uh generate an API call try toexecute it and then if it executes plusone reward if it doesn't execute minusdo less of that so that this is how Ithink it works is they force thegenerations to follow the API call I'mnot exactly sure what the standardinstruction fine-tuning is but I thinklike maybe I'm getting off topic butlike I think the way that hugging facehas abstracted training models I don'tknow if it's really worth most of usgetting into unless unless this is yourthing unless you're like training modelsall the time but for you know ourexperiments with UVA gorilla it hasn'tbeen a pain to just uh do whateverhugging face offers off the shelf butanywaysso here's how retrieval augmentedgeneration during training works so youhave the natural language instructionlike please classify if this imagecontains a dog I want to see catsdancing in celebration and then not justthat as the input you add use this APIdocumentation for reference and then thefirst retrieve search result so this iswhat we do all the time at inferencetime these days with retrieve augmentsof generation but what if we did thisduring training as well and what's sointeresting about that is then you'retraining the model with gradients toread that API request which is superimpactful for updating documentation youknow like the weviate apis they're notjust going to look like this forever orlike you know however llama index allthis stuff all these software tools theyevolve over time and so you need tolearn how to do the new API requests sohaving it be trained to read the requestis seems like a huge unlock for this inkeeping the models fresh with newinformation as well as generally it justmakes the task a ton easier if you seethe it basically the answer in the inputso this is what they mean by doing thisduring training not just at inferencetime so the key benefits the other statewhich I think you know agrees with whateveryone says that there's maybe onefourth thing I would add to this butthis makes the llm adapt to test timechanges in API documentation it improvesthe performance from in context learningso you know it's easier to learn thistask it can transfer learn betterbecause it has some representation ofall the tote of like the you know theAPI reference so it learns better andthen also it reduces the hallucinationerror the hallucination error is a hugeone obviously for API syntax because youknow it has to be correct you can't likehallucinate an API request or it won'tdo anything productive at all butgenerally this retrieval aware trainingI think has a huge opportunity to reducehallucinations so the fourth thing Iwould add to thisis when you decompose the retrieval fromthe reasoning you can also then have asmaller reasoning model so you don't youknow this is like the atlas paper frommeta which is saying that you know whenyou decouple retrieval and reasoning youthese uh you could have like a 13billion parameter reading model thatperforms just as well as a 300 billionparameter like fully end-to-end llm sothis kind of retrieval also iseconomical in the llm you need becausethe task is just way easier it's justabout like reasoning in the input notabout like having remembered everythingin the worldokay before we see some data tables weneed to talk a little bit about howwe're going to be evaluating thesemodels so again what we're evaluatingthese models to do is to select theright model the right model path andsometimes there are optional argumentsas well to the model so these areexamples of incorrect Generations so inthis case uh this the gpt4 model ishallucinating extra arguments to thefunction like you know this ASRparameter in string or Source equalslocal the anthropic cloud model is youknow instead of doing torch.hub.load isdoing torch audio dot pipelines you knowdot wave2vec ASR pipeline so it's likehallucinating or it's calling the wronglibrary to do this whereas so we wantedto do this particular kind of Syntax forloading this model so what the authorspropose doing is decomposing the APIcall into an abstract syntax tree andthen matching that tree with the uh withthe ground truth or say you know you gotsome of it right so this is like if youmade it to here right like you got thetorch.hub.load maybe you would penalizethat less something like this so yeah Idid I didn't go too into this detail butI think it's quite interesting howyou're going to be evaluating thesemodels like maybe the dumb way to do itis just to do to keep the perplexitylike the perplexity is the commonlawsuits and language modeling whereyou're just multiplying out theprobabilities the language model puts tothe ground true tokens and you knowthat's typically what you do withlanguage modelingwith this uh you know the the probablythe most interesting thing about this isif you uh did you know if instead ofdense net 121 you did densnet 161 or 200one it would still execute so you can'tjust purely evaluate this based on didthe API call execute you also need tokind of see if it gave the particularmodel which will make sense too as welook at the ablation on uh API callswith constraints where you particularlywant like you know eight eighty percentimagenet accuracy but less than 20million parametersokay let's get into some of the resultsso the first thing to note before wedive into the results is they're goingto be exploring again training gorillawithout retrieval as well as trainingGuerrilla with retrieval so startingwith training gorilla without retrievaljust purely fine-tuning a language modelto go from natural language instructionto API call the API reference is onlyused during inference not duringtraining when when they do bring it outfor to see what happens to Performanceso the takeaway fine-tuned gorilla getsCDR performance 20 better than gbt4nearly 11 better than cha gbt and then83 better than the other open sourcemodels so you know you see thatobviously obviously there's a gapbetween gbt4 Chad gbt and then most ofthe open source models so but this takeaway the fact that you can fine-tunegorilla to do a better job of these APIrequests then gbt4 zero shot or Chad GPzero shot this suggests quantitativelythat fine tuning is better than justretrieval augmentation so you know whatthey mean isthey you know so neither model has beentrained with the API reference but theydo also show what happens when you putthe API reference in the input at testtime not during training time and theydo find that fine-tuning models performsbetter than just purely augmenting gbt4with the ape guy reference so anotherthing before we look at the data tableis that you know this is a warning forbrag implementations they find that ifyou give it a non-optimal I mean this isobviously common sense like if you giveit the incorrect API reference it willmisguide the model of course so I don'tknow if that's like exactly like amind-blowing takeaway but I mean it itof course is because when you'rethinking about retrieval it really likethat's why I think this coherere-rankers thing is so exciting becausefor a lot of these retrieval augmentedgeneration applications I don't thinkyou're going to be latency bound becauseyou know what's the point of making itsuper fast but not accurate predictionso kind of these re-ranking models Ithink become more valuable because it'sworth taking the time to make sure yougot the right API reference before youthen do the generation so let's take alook at the data table sookay so obviously this is a ton to lookat but uh so so gorilla of course is youknow the model we're talking about andso gorilla zero so zero shot means allthese models are just being evaluatedwith the natural language command I wantto see some cats dancing right they justget that as input and they just have towrite the API request to the hug andface model so they measure differentkinds of uh you know the overall whichis that abstract syntax tree matchingthing the hallucinations when it um youknow invokes the like just generates anAPI call that doesn't exist or errors Ithink is just when it makes one of thethings right I'm not exactly sure thedifference but I think probably we canjust look at this overall metric so sohere's the thing about uh be carefulwith the retriever so bm25 if you getthe wrong context it goes from 59 to 40.as we saw earlier I think it makes a lotof sense why bm25 retrieval is hard forthis kind of case because if you say Iwant to see some cats dancing you don'thave any keywords that are going tomatch the you know the hug and face APIdocumentation so I do think that resultmakes a lot of sense uh gbt index thatbeing that was the earlier name of llamaindex uh returning so a site boost usingVector embeddings but definitely a lotto this and then Oracle Oracle is theyou know the perfect API reference tohave so you can see how much it boostsby having the perfect API reference somaybe in addition to seeing our gorillamodel so we see gbt4 Zero shot doesn'tperform so well but then when gbt4 hasthe you know 38 to 66 again you knowthat's the retrieval augmentedgeneration thing is when you give it thecontext the large language modelsperform way better but then you can alsosee the detriment by not having it bethe right context so here's another kindof view of what these models ended uplooking likeyou know with the Oracle retrieval sojust zero shot where you've justfine-tuned gorilla compared to gbt 3.5and GT4 zero shot gorilla model performsway better but then with the Oracleretriever you push these super capablelanguage models closer llama just withretrieval doesn't seem to be able to youknow get up up into the left like theseother models do so that's kind ofinteresting as well you see the inputimpact of uh not a perfect Retriever andso on so so this is again this isfine-tuning Gorilla without retrievalokay so now let's get into the resultsof retrieval aware training fine tuningwith retrieval so shown on the right isthe gorilla trained with the Oracleretriever so that means that duringfine-tuning the gorilla is always seeingthe perfect API reference so I want tosee some cats dancing and then it's thatstapled stable diffusion model APIreference so it's you know perfect APIreference so here's so we see with theOracle that the results are much betterso you know comparing this column withthe Oracle one is train with it or maybecomparing this column with the zero shotgorilla without retriever when it's beenfine-tuned without any kind of retrievalat all so you know so that's thatcomparison gorilla with the Oracleretriever then is completely unable todo this without any kind of retrievalaugmentation and then here's anotherreally interesting detail of it is thatwhen you train it with the Oracleretriever it becomes less robust tonoise in the retrieve which makes a tonof sense because you've changed the taskfrom memorization to like learning toread and so if you're learning to readthe wrong thing obviously that makes alot of sense why that would mess it upor yeah so you know it would beinteresting to see kind of like umablating this further like instead ofjust umtop one maybe you you have top three sothere's there's definitely some moreexploration they could do with uh youknow how you retrieve the context duringtraining do you put do you mix theOracle with noisy results to make it totry to make it robust to noise that wayand so on so the big takeaway is thatthe current retrievers still have a biggap between the ground truth retriever Ithink this is maybe confounded bybecause you could have like a I I thinkI think this task also is particularlybad for bm25 but you could generally youcould do like hybrid search withre-ranking to try to improve it you canmaybe fine-tune the embedding model assomething that is not common to see butyou know definitely could be a lever topull this performance even further butyeah so the key thing is to note that ifyou're going to train it with the Oraclecontext and then at test time you havesome noise in the return context you canexpect the model to perform worsebecause you know it's being trained tocopy perfectly okay now here is maybethe more important evaluation thing istest time documentation change as theauthors State the rapidly evolvingnature of API documentation presents asignificant challenge for theapplication of llms in this field thesedocuments are often updated at afrequency that outpaces the retrainingor fine-tuning schedule of llms makingthese models particularly brittle tochanges in the information they aredesigned to process this mismatch sorryan update frequency can lead to adecline in the utility and reliabilityof elements over time so I think this isprobably the most important thing forthis API documentation because you knowas imagine we train the wev8 gorilla onetime and now we want to say change thearguments to one of the function callsor deprecate some something or we wantto introduce new apis this ability ofretrieval aware training where you'retraining it to just kind of read thedocumentation that enables it to adaptto changes in the apis and that is ahuge enabler for this kind ofapplication they illustrate how you canupload the model registry to change themodel path from PI torch Vision toNvidia deep learning examples colontorch Hub and they show this kind kindof thing of how the retrieval awaremodel is able to adapt to updates in thedocumentation the next reallyinteresting detail for these kind ofmodels is how well they can followconstraints so particularly with deeplearning models we have resourceconstraints and then performanceexpectations so you know we have thesetwo kind of things parameter size whichis a pretty good proxy for how you knowthe late how slow is going to be to makepredictions as well as what kind of likewhether you need like four gpus to runit or what kind of GPU you need to runit and so on as well as the lower boundaccuracy so for example the naturallanguage command might be invoke animage classification model that usesless than 10 million parameters butmaintains an imagenet accuracy of atleast 70 percent so this requires theNuance of understanding the request aswell as then how to format it into theAPI so you kind of have this confoundingof these two tasks in one but it'slearning how to simultaneously balancethe performance you want with theresource constraints so this table isshowing how well the models are at doingthis this definitely makes the task moredifficult you can see how the cloud andgbt4 they pick it up because now I thinkhaving this kind of um you know likeability to reason about the request aswell as just how to format the APIsyntax because you need to you need tobe able to like parse out this kind ofyou know what I want as well as theconstraints then you need to kind ofread the description read theperformance and so on so I think thishelps the more General models catch updefinitely an interesting kind of aspectis this API calls with constraintsparticularly I would say resourceconstraints so hopefully that was a goodexplanation of the details of thegorilla paper more than happy to furtherclarify anything if you have anyquestions or ideas please leave them inthe comments here are some of mypersonal takeaways from reading thispaper so firstly I think it's reallyinteresting to just kind of consider theevolution of this kind of research Ithink it started with react that showedhow you could have zero shot promptingwhere in the input you would say hey youhave a calculator a search engine if youwould like to use it then you knowplease format your response like thisand then we'll send the request to theexternal tool and this kind of thing sotools former I think was then that thepaper that showed that you couldfine-tune language models to use toolswith gradients I think it was a littlemore entangled with uh the languagemodeling objective than this kind ofgorilla work where I think the gorillawork is more specialized on particulartools rather than having gradients allthe way in the end to end of how you usethe tools to complete the tasks huggingGPT I think is another reallyinteresting work connected to this whichuh was at least for me the first paperthat showed like this idea of likemodels or tools so I think gorilla issomewhere in between tools former andhugging GPT but I love the specializedFocus I love how they brought in theself-instruct for how you generate thenatural language commands and bootstrapthis data for then compressing from thelarge model to the smaller model towrite these API requests so you knowwe're going to be doing more stuff onthis wuvia gorilla so I'll just give aquick preview of this for now but youknow we V8 gorilla is about you knowtraining fine-tuning the model on theWii V8 apis so starting with the searchapis you know we it has a graphql API aswell as like you know python JavaScriptall that client libraries for how youwrite searches soyou know this is similar to the web GPTstuff uh web gbt is where it has searchactions like you open up a squarebracket and say search and then the termor you and you get the search resultsand you say you know Open Bracket nextpage to see the next page of searchresults from say you know the Bing APIor the Google search serp API and howyou do that so it's also quite relatedto learning to search so learning tosearch is I don't think a massivelypopular area of research but that'swhere you would try to train a model toyou know maybe copy like what what termsparticularly humans search for so youknow like how you know with how we useGoogle we might focus on these kind ofkeywords and so on so you try tocompress that into LM so kind of similarto the API call with constraints inaddition to this kind of how would yousend the request to leviate you mightwant to entangle the gorilla model toalso coming up with the search query andthe search query isn't just the promptso one really popular way to get betterretrieval augment to generation is youtake the prom and then you take thatprompt put it to a large language modeland say what would be the search queryfor this and then you send that searchquery to the vector database to get thecontext not just sending the promptitself to the database soit was like this interesting layer inthe middle as well now here's the thingthat I think is really the likemind-blowing future of this is usingGorilla for software integration so youcan imagine the natural language commandbuild me a chat with data from my notionworkspace titled wevia gorilla usingllama index and weviate and you havethese gorilla models that are that knowhow to scrape data from notion they knowhow to import data into Eva and thenthey know how to use the Llama indexquery routers across you know withinwith the weeviate data index there soyou have this kind of natural languageto build out all this software and Ithink the way that you kind of decomposethe task into particular apis is a hugestep closer to this kind of likeautonomous software as we you know ourtrading models for the apis ofparticular software libraries you couldreally unpack the box here and thinkabout kind of all open source softwareas a you think of all all class is intheir functions as apis andyou know this kind of I think theintegration of Open Source software andscience is about to become superintegrated because of the way theselarge language models are going to beable to test new ideas by using the apisof Open Source software so a lot ofstuff there for sure but I think kind ofthe interesting thing especially youknow as we transition to Guerrilla forIntegrations build me a chat with mydata from my notion workspace blah blahis how abstract are these instructionsgoing to be so in the paper they youknow give you examples like I would liketo identify the objects in an image orI'm going to the zoo and would like totrack animals so it's kind ofinteresting like whether theseinstructions are kind of like atomicizedto one model call or if you need to likeyou know really decompose the task sortof like you know self-ask multi-hop orthis Auto gbt thing where you need toreally kind of break apart the task likewhat are the subtasks it's not just likeone the instruction is just someimmediate taskthe next thing the authors kind of talkabout is systems to execute apis duringtraining and data set generation so youknow as as you imagine uh generating thesynthetic data would you try to executeit or when you're evaluating the modelswill you execute it and I think thiswill be you know building these kind ofsystems that work in the simulationduring training will be huge for thiskind of evolution of program synthesisusing this kind of ideanow this next thing is something that Ithink is really exciting so I I got thisidea from listening to Bob in light uhspeaking at the Llama index webinarbragging production and you know Iworked with Bob on the generativefeedback loops idea and I think thisidea is awesome generative feedbackloops are basically this idea of thelanguage model transforms the data insome way and then saves it back in thedatabase so say you have you know westarted off with this Airbnb listingsexample where Airbnb listings you havesymbolic properties like how much itcosts what neighborhood it's in maybeyou know nearby coffee shops orsomething like that and you would takethose tabular features and then generatea text description save that textdescription back in the vector databasewhere you vectorize it with like theopen Ai embeddings and now you searchthrough those descriptions of Airbnblistings there were all sorts of thingsyou could do generative feedback loopsthe most recent example Bob gave in thewebinar was process email threads tosave summaries which can be searchedthrough so you know imagine I have a youknow a class of emails and it's like youknow there's a thread where we'rediscussing something and then yousummarize the thread indexes summary youadd symbolic tags like who is a part ofthe summary yeah I'm sorry a part of theemail thread in the conversation butanyway sogenerative feedback loops is abouttransforming the data because the thenew transformation of the data might beuseful like if it's a personalizedadvertisement or something like that orit might be better for a search indexlike if you have podcast clips and yousummarize the content into the abstractdescription of like what was discussedand then you index that that'll be abetter search index so that's one sideof it and then on the other side of itwe have Gorilla so right now what we'rethinking about with a gorilla andthere'll be a blog post soon moredetails on this is just kind of usingthe search apis but Guerrilla kind of Ithink where this research is headed isit can configure itself it can createnew classes it can maybe even tune theproduct quantization parameters so it'skind of like this and I I got thisphrase from Andy Pavlo who's a brilliantProfessor on databases at CarnegieMellon where he calls this self-drivingdatabasesand I think self-driving databases isused to describe kind of uh databasesthat like automatically optimize theirindex structures and I think that'sthat's probably the missing piece ofthis gorilla plus generator feedbackloop so I haven't gotten into that partyet but I think this kind of generativefeedback loops and Guerrilla we'realready seeing this kind of autonomousdatabase the database that takes thedata and does all sorts of stuff with itbecause it has all these differentlevers to you know it can query itselfit can import data to itself sort ofcreate new classes and then it cantransform data with these LMS so anywaysthose are some of my takeaways I thinkprobably from the highest level I thinkjust the leviate gorilla will facilitatethe learning curve of learning to usethe search apis if you just have to sayVector search in podcasts you know andthen it will just translate that intothe graphql or you know you want to adda filter like vector search and podcastsuh published after July 5th 2023 and itwill just add the wear filter for youand we V8 syntax so I think that'sprobably the lowest fruit is justfacilitating the learning curve tolearning how to write these kind ofquery apis but I do think the potentialof this kind of research is just throughthe roof thank you so much for watchingthis explanation of the Gorilla researchpaper if you're interested in this kindof work we have all sorts of cool thingsplanned this is hardly the End of Thisresearch on llms that control apis theweeviate Guerrilla models and all theseexciting things we can do if you'reinterested in this research pleasesubscribe to the channel we'll bepublishing our podcast with shashir andtianjun the original authors of thispaper on Wednesday and please subscribegenerally to the webia YouTube channelfor all kinds of videos and podcasts andthings about this whole world ofretrieve augmented generation Vectordatabases and large language models youconnect with us find wevian weeva.ioopen source on GitHub wev8 weavate or onTwitter at weba underscore IO thank youso much for watching I hope you foundthis video useful and exciting", "type": "Video", "name": "Gorilla LLMs Explained!", "path": "", "link": "https://www.youtube.com/watch?v=LkV5DTRNxAg", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}