{"text": "Thank you so much for watching the Weaviate podcast! We are super excited to host Sam Bean from You.com! As well as ... \nhey everyone thank you so much for checking out another episode of the wevia podcast this is going to be a super fun discussion we're talking about you.com the spark connector with weeviate and generally billion scale Big Data Technologies and we V8 and all these exciting things around search interfaces and so on so firstly I'm so excited to welcome Zane and John for the first time on the web podcast thanks so much for joining the podcast guys hey everyone thanks Tony awesome so I'm also super excited to welcome our external guest uh Sam Bean uh Sam is working on you.com Sam thank you so much for joining the podcast and uh can you tell us a little bit about what you.com is and um yeah maybe we could just start from there yeah thanks for having me um so you.com is a um a newer search engine it is a um kind of started as like a a play on hitting the niche between personalization and privacy um we've since evolved the the mission considerably and just recently we kind of announced uh the latest um big kind of press that we're making in the search space which is kind of democratizing search and what we're calling the open platform and so this is kind of like a mixture of some developer tooling uh DSL and it allows basically third-party developers to create um hook up their data and create search widgets and applications as we call them um that can show up uh natively on on our search results page so um we kind of you just tell us like a little bit about what keywords it should show up for and we handle like the ranking and the retrieval and and rendering all of that and so really great discovery tool for third-party developers who want to kind of plug into a search engine so it seems like the the whole kind of search interface is being redesigned at u.com in addition to of course like the retrieval technology can you tell me about like more about the widgets and sort of how you're thinking about how people interface with search engines might want to obviously like you can generate an image you you have these things for you know having the code assistant tool you tell me about the thinking behind the interface yeah sure so I think that we've seen apps kind of take a more prevalent role in the search space like you probably haven't called them that on on your your main players in this first search space like Google or Bing but you you've seen them like your side panel Wikipedia panes um those are kind of all things that we call an app which is basically just a way to take our search data and curate it and decorate it in such a way to kind of give it like a special presentation and enrich the user experience a little bit more than kind of your your standard Blue Links are to give a better experience and and basically what we do is we operate like a a very partitioned um index and so basically we are able to um reach into specific slices of the internet um you can think of them as like vertical slices of the internet instead of being very vert or horizontally um searching and then and then we know a lot about the data because we crawled it we've enriched it with a lot of like machine learning systems that we have internally um and then we can kind of like using what we know about the data that is from that domain we can create like a much more curated experience for the end user and so things like our stack Overflow app uh to your point some of our like our code generation applications um are you right application are all kind of like very Innovative uh app s that are in the search space yeah that's extremely interesting the whole um just kind of the high high level idea of like a symbolic filter might be how we think about it like in specific leviate land but this idea of like I want to search through stack Overflow I want to search through Twitter Reddit like you this interface around picking the source of your information and searching and having that smooth integration um so I I kind of first became familiar with u.com with um I I first started following Richard Sasha's research around these kind of systems with the co-surge system it was this like covid-19 information retrieval system and it had this like brilliant diagram that really outlined it uh can you tell us about kind of the originsstoryofyou.com yeah so I think that um the way the way that you hear is that there was there was Richard's had this idea for this this kind of like um this vertically integrated like search index uh for a while and I think that he was working on it for a bet and then um with his work at metamind UH being acquired by Salesforce becoming uh uh entering the c-suite at Salesforce and then kind of like um doing a lot of like amazing work there on on the Transformer Technologies especially like Ctrl and I think Cove are two of The Shining examples um meeting Brian McCann who's our CTO I think that they they realized that it was a good time that Google's grip on the search space was kind of weakening a little bit because they're they are I think fundamentally their attention is marred by what is dominantly uh more and more like a marketing company um and so we thought that it was a good time you know that um there's there's an opportunity um there's a lot of people around the globe who are kind of wanting privacy more and more and sort of give people that kind of ability to to have a private entry point into the internet um and also giving I think what what we feel are some pretty fresh takes on on the search experience um it's a lot of responsibility being like the entry point into the internet for a lot of people um and it felt like that was kind of that responsibility was falls on a few select players and the way that um data privacy was being handled for being such a high responsibility um that it was it was kind of like it's it's one of our Central tenets that you know trust facts and kindness are very are very Central to to the company and so I think taking up the mantle of trying to create a new entry point and Gateway into the internet um with with some of those things at the center of it was I think largely the the motivation for for getting into the space so that was kind of like a uh that's a constellation of ideas of of kind of how we got here but it was it's kind of a uh globetrotting uh tail yeah it's super interesting um and I really do want to come back into privacy and the focus on it and that like the particular details of how to deliver it and uh the new technologies but uh I think kind of a conversation we're very curious about uh can you talk about how you came across weeviate and then we can dive into the spark connector yeah sure so um we we in building kind of like our own indices of the internet we knew that we wanted semantic search capabilities we kind of knew that the way that um transfer learning and NLP was advancing and where had it had gotten to when we had started the company would give us a pretty unique advantage to build those capabilities as foundational units um I think that obviously Google and Bing have a lot of those capabilities um but their systems are largely kind of built off of lexical search and then a lot of the semantic pieces are kind of like retrofit onto their their retrieval systems and so we knew that we wanted to have these things as kind of like first class citizens from the ground up um that semantic search was going to play a really important role um and we were kind of like looking around for what semantics are databases we wanted to use and I think that very early we were connected with um the people that we V8 and found that kind of the the open source ethos was a really good match for this that it was a really good match for the company's kind of like background and being able to contribute to things like this in the space was really important to us um and so it made a lot of sense to to start to invest in the ecosystem and learn the Technologies I guess a separate question I had before we get into the spark connectors um in the future do you see yourself using all the different capabilities instead of just uh semantic text search do you also see yourself using semantic image search audio search those sorts of capabilities yeah I think that I think that we're trying to think of especially with where what we're trying to do with our you imagine Suite which is like a an image generator from natural text something that is very um very popular as of this year that trying to kind of create like this very multi-modal search experience across images and audio and text is going to be important and being able to handle having one system that kind of handles vectors in like a very Vector first way instead of trying to like cluge a bunch of systems together which is I think how traditionally you would have done this um and I think that especially with what we V8 is doing with the hybrid search um that I think is going to be a a really big player for a lot of people even if they don't even know it yet um because people who are like operating these search indices right now know that you basically need two different systems right like there's no system that does lexical search and semantic search together you have to do that work and it's not simple code to write it is it is hugely complex to kind of figure out how to like run those systems in parallel and how to how to stitch all of the data coming back together and how to rank it all uh very holistically having a system that supports hybrid search um at its core is is really important and so just something as far as like architecture Simplicity um in the search Space is really important especially from a maintenance perspective maybe we can talk a little bit about for people that are new um what uh can you talk a little bit about what you use spark for at you.com yeah we use spark for almost everything um as far as like our our data ecosystem goes it's all spark native so our are kind of like Eventing and logging system on the back end is all um streaming Kafka with streaming spark jobs which we use to kind of um take all of our real-time uh data that comes out of our system for alerting and machine learning use cases and then we flattened that out into like um into a Delta Lake and so we we use we looked at like hoodie and Iceberg but we ended up going with Delta because it felt like what databricks was doing with spark was um was just a little bit more fully featured at the time and so that all integrates really nicely for those who don't know Delta is basically just parquet files which is a compressed columnar format that spark integrates really well with um and Delta basically adds another layer on top of that which is just this big transaction log that allows for acid transactions on a data Lake which was previously impossible um and then allows for things like versioning and vacuuming and and optimization and a lot of the things that you can kind of do in spark etls to kind of create um your your historical data Lake and then we also have a number of like for for our analytics for our a b tests for our um machine learning data sets we have a number of enrichment jobs that all run through Spark and then even like model training we we run GPU accelerated spark clusters for model training um for like data parallelism and model parallelism um and then like I think past that um also our entire like crawling system and indexing system and and web parsing system it's it's all spark so um a number of of cron jobs that are um just big scraping jobs that just dump data again to uh parquet and then a number of spark connectors that kind of read from the the data Lake and then make uh then index all of that data making it kind of like searchable for the end user and so from that perspective when you're kind of like running in a very spark first ecosystem you don't want to have like part of your system writing um with kind of like spark connectors and then you're running kind of like a cluge of like maybe some python code or Java code that's also running off of um running off of the same data but running in a different environment it's just going to double your maintenance and double your your monitoring expenses um as far as like cognitive load and having to keep track of all that and so it made a lot of sense to with our eye on um using we V8 for a lot of our our semantic search and possibly hybrid search use cases that we would need we would need something like this to to fully integrate the the weva database into our into our uh data ecosystem so we kind of reached out and we we asked if there was anything we could do to help and uh as they say the rest is history you you mentioned um I guess this is a good segue into alleviate but what types of data sets are you considering using uh for Aviator I guess what's the roadmap for using wev8 with the with the stack that you have right now yeah so um I think that leviates data model makes a lot of sense for from our perspective um the way that you can the way that you can Define different schemas and the way that you can create uh the indices and and Define your indexes based on uh what the objects are meant that that grafted really nicely onto our already our data model which was you know this this very like um vertically aligned search index where we already have these these um a data model per domain that we're crawling um some of them are more General than others but for things like um your data model for a stack Overflow page is going to be wildly different than your data model for like a recipes page um some of the things are going to be the same your title your description your url but a lot of the components and a lot of the what is going to be a part of that data model just quickly diverges and so that grafts really nicely to how uh leviate thinks about schemas and data objects and meant that having a mapping which which we did from a raw HTML page to then a spark data frame and then being able to map us that spark data frame which we already had the code to kind of like get us to that point and be able to Marshal that to alleviate data object meant that you could you we could think of very easily going from a web page to an object in deviate very seamlessly using Apache Spark great well it's it's been interesting being kind of uh part of the channel and hearing a bit more around the process and developing a spa connector because I feel it's something that not not many people do um a lot of the times uh you know they're created one off and it's fairly infrequent so would you be able to talk a bit more about how you how you went across developing the star connector and what um what sort of learning resources you use to create one yeah obviously it it can't comes with um all of all the spark connectors are different um because they each require a certain amount of domain expertise with you know be able to Marshal the data into the end the sync as they call it in spark and obviously definitely was it was not a uh one person's effort so I definitely want to shout out everyone from weviate who helped out and absolutely um Sam stalinga who um was spent a bunch of time with me pair programming and did a bunch of the lift on developing the the spark connector with me um I think that largely you can you can start out building a spark connector by figuring out how to do it in a UDF or a pandas UDF and then spark a UDF is like a user-defined function and it's just some arbitrary code that you tell spark you're going to Loop over this data frame um we're going to tell you which columns from the data frame are going to get passed in and then you just execute that code per row and once you can kind of run your your you have your data integration working in a UDF format it becomes a matter of you can do that forever if you want to like at that point you're not going to get a ton of like the actual performance improvements because you can parallelize that UDF in a massive way in the same way that you could with like a like a native data frame writer but you're always going to be kind of like making changes to that you're always going to be kind of like cutting Against the Grain with what are the opinionated like spark idiomatic ways of doing things and so things like error handling things like checkpointing things like um retrying a lot of the stuff that you would use spark for um because you don't want to think about that because it's hard and like you have enough things to do on your plate that you don't want to think about like what retryologic needs to look like in a data pipeline or or or error recovery that you kind of delegate all of that complexity into Spark so it makes sense at some point once you're dealing with a certain level of complexity to try to graft all of that code into kind of like what what is the opinionated spark way of doing things and that usually means extending a number of the the core kind of like abstract classes that are available in spark so I like I said like the spark data frame writer um there's a number of examples that you can look at it be it like neo4j or elasticsearch um I think that there's even some examples that you can kind of look at but there's just a number of of core classes that you need to override um but largely once you have that code operating in a UDF it just becomes a matter of grafting that into kind of um the the objects that that spark kind of expects um and then being figuring out how to deploy that code in a jar format into a spark cluster and then and then accessing it as a um as one of the writers but it it really becomes a matter of like you're either gonna have a UDF and you're gonna run a a map you're going to map that UDF over your data frame versus being able to say just like take the data frame and I'm going to call it just dot write dot format wev8 dot save and then you're you're done um and so you kind of you take all that logic that you might have to maintain or or evolve in um in a way that is going to be more difficult and then try to once again like delegate all that complexity into a central place which is the the Wii V8 spark connector and then as a community we can all kind of swarm on that and make sure that it's the the best in class for writing data into eviate and then lots of people get to access it and it becomes very easy to kind of turn your brain off which is which is the point right make it really easy to take your spark data frame pump it into eviate and not have to think twice about it yeah what what I love uh with the spa connector is it just makes it really easy to plug in different data sources so uh when we're doing the sphere test uh we could just read a parquet file and and load quite a lot of significant amount of data really quickly um you know other data set um other databases like reading from Cassandra becomes easy um so I'm interested to know you talked a little bit a bit before about the pipeline we're using streaming in Kafka with their data coming in are you planning to add spark streaming support to this connector I think it's definitely we have an open issue for it um I think to to all of the people who love open source there's an issue uh you can open a pull request uh if you want to go take a look at a really great example there is a um I think a neo4j their their library is set up so that um you basically just need to ex it's very similar to um to the batch um operation it's just like a different uh a different API so you're using for batch or using the smart SQL API or the tables API if you're in spark 3. for streaming you're you're just you're you're operating on a streaming data frame um and so it's a slightly it's it's not gonna be a ton more work but I think that um I think to answer your question yes there's there's a plan to add it I don't know where we're gonna have time anyone out there who wants to take a shot at it please do because because um I think that being able to make data accessible in real time and we V8 would be an awesome feature great thanks so I guess the other thing is Israeli the design Choice when you're using waviate to bring your own vectors or to use one of the um Effect one of the modules pre-existing to create vectors uh and probably when you're at Large Scale you want to have more controls so you're leaning more towards creating your own vectors um do you have any insight around sort of how you're approaching creating vectorizing umu.com yeah I can I can talk a bit about that um so I think that if you are going to if you're going to use the vectorization modules internally in vv8 I think that is going to take a bit more thinking up front with how you're going to do your resource planning I think that whenever you're talking about writing a ton of data and then you are simultaneously using the database on the other side for a user facing feature you're obviously going to have to think of read write isolation and then if your database is also doing your vectorization you almost have three operations that you now have to isolate because writing a ton of data and then trying to vectorize that data and then also trying to read data and um for your users now those all have to be isolated because your vectorization can take down your reads your rights can take down your reads your rights can take down your vectorization your vectorization can take down your like like all of these things now are kind of like operated in the same on the same computers and so I think that when you're going to really large scale unless you have um if you ever if you have a bunch of devops resources then like go for it sounds like a lot of fun if you don't have a lot of devops resources um I think that the that error handling and the the resource planning in spark is probably going to be a bit easier um I think there's probably a bit more um going wisdom on how to do that kind of resource planning that you'll find on the internet just because I think spark is used by more people less people are are using Vector databases right now than big data Technologies you'll be able to find a bit more on how to do that resource planning it's going to mean that um your you you only have to think about the read write isolation for what is going to affect your end users if that if your vv8 database is hooked up to um some sort of client application that that people are using which hopefully they are um and for us I think that we were able to we've spent a lot of time thinking about um the best way to run these Transformer models ourselves and so we've spent a lot of time figuring out like how to quantize models how to run graph optimizations how to like run like what what is the most efficient way to run Onyx and in a spark um in the spark environment because that stuff isn't actually um doesn't work out of the box and so if you have that kind of domain expertise which we do then it makes a lot of sense to leverage that and and to figure and then just use what you know about Onyx and what you know about model quantization and stuff like that um and get get the most out of that if you don't have a lot of that then then then maybe it does make more sense to delegate that complexity and to leviate because I know you folks know quite a bit about what it means to to run large language models as well um so I think it it really depends on the composition of your team and where your domain expertise lies I know that's a non-answer and it's really a long-winded way of saying it depends but for us it makes a lot of sense to kind of we'll take the complexity of running the models um because of again what we know about Onyx and quantization in the hugging phase framework and using like the hugging phase Optimum libraries um and kind of write that code ourselves and then we only have to worry about the read write isolation for our database can I ask a clarifying question is that the Onyx so it runs on the CPUs and that's kind of what the spark framework is built around yeah so you can so you can so there are actually Frameworks that you can run on GPU so I think cudf which comes out of uh Rapids you can actually run GPU accelerated udfs at this point uh it's a bunch of work and uh it's not cheap and if you know how to run um large language models on CPU with Onyx uh which I've done a talk on for a couple different NLP um talks uh there is a way to do it it requires um it requires a little bit of finesse but yeah the the short of it is is that Onyx is not Onyx infant sessions are not pickleable and so because of the way that spark works it in pi spark you run udfs by pickling your user code and then you broadcast that down to the worker nodes and then those worker nodes unpickle the code and executed against the data it's the whole like pushing code to data versus pushing data to code inversion um but you can't do that because you can't pickle Onyx infant sessions and so you have to come and see you have to come up with like a file broadcasting system and you basically have to broadcast the Onyx files down to the workers and then you have to actually load in those Onyx infant sessions on the Fly um and then you can get a if once you have that working you can basically infinitely parallelize that because you just you just make a copy of the Onyx files on all of your workers and you just load them on the Fly and um you pay a little bit of i o penalty for reading reading from the file from disk to create the the object in memory the Onyx inference session but with enough parallelism if you're doing big enough batches that you're pushing through the Onyx infant session it ends up becoming um economical to do that and then and then you can get in theory unlimited throughput um we can get like that's how we've we've been able to get to like um embedding like in indexing like whatever billions of documents in um in short order I forget how how fast we did it but it was not a ton of time maybe a few days yeah so that that leads nicely to um I guess performance questions around using Sparkle for Eva and caviar learnings around what you what you found works best um so do you have any insights from that and maybe I could add some some other comments as well not as many as you do so I I would love to hear what you have to say I know that I think we actually spent some time together uh working on something similar to this but I know that the the hyper parameters around creating your graph for for running hnsw is very important um and I think that what I found is that running the running the vectorizers ended up being what what consistently killed my jobs and eventually pushed me to um to to bring my own vectors but this this is something where I am still a student if I'm being brutally honest yeah so you see I guess it does the larger scale use cases um you just have more control if you bring your own vectors um you also need to be careful around um GPU configuration if you're using one of the modules um as an alternative you can also use the the cloud modules like to open Ai and click here um but around performance just just writing to Eva to I obviously it's number of nodes in shouting as well so by default default VBA will actually configure a class of the number of shads of your cluster size so if you have a cluster of size three you'll have them three shots and and then when you're writing the data the um in terms of the actual hmsw if construction is the most important parameter along with Max connections um where EF construction is basically how far it searches in the graph and match connections is how many edges each vertex has so so those are definitely um sort of parameters to optimize and then batch size as well so we do have we do have with the python client we have Dynamic batching and it's had a few iterations because it it is um it it is quite hard to get right and to not sort of push too much and create the batches too too large and overload with yet uh so with spark it's if you've got a fixed size cluster it's just around optimizing the batch size to be um what you expect and one thing to to help with that is really setting up Prometheus metrics and just monitoring the the batch latencies um one thing we have done recently which will be in the next releases optimizing batch latencies and making the compactions with Alice in trees faster so there should be some performance improvements there it's super similar advice whenever anyone asks me how to how to get the most performance out of a spark job it's usually like well you look at the metrics and then you turn some knobs and you see what the metrics do and then you keep turning knobs and you do that for a week and then you've got like a great and then it's great um and there it really isn't like a ton of shortcuts to it right yeah that's right um and I guess the other thing is more the ratio of the number of spots Spock nodes you need to alleviate um so generally if you're loading from a very um from a data Solace that's quite sort of basic or quite easy for spark to load like parquet um often a ratio of one to five or one to four is is fine so um you will need more we the up notes and Spark nodes um generally if you're loading data yeah it's never bad practice to decompose and those in those situations if you find yourself with a bunch of parsing logic and then a bunch of like indexing logic just pull those apart you parse you dump to parquet and then you have another job that just reads parquet and indexes and you get a little bit more introspection because of the way that spark um the uh the the dag execution works it's all lazy and and it's it's super unclear when things actually happen and it can look like some operations take a really long time when they shouldn't because really that that's what actually triggers the eager execution of all the operations behind it um yeah yeah and there's uh there's one interesting piece of news is that um all the hugging face data sets are Auto converted to parquet files now um so that came out I think last week and that's yeah that just makes it easier to load um all those heavy and place data sets into read the edges while using the Spy connector so sorry can I ask a dumb question really quick so what so parquet Json CSV uh like sorry I'm still kind of catching up what's the difference between parquet like what does it add to it parquet is the the the the company line is his compressed column in our format that makes all of your dreams come true um I think that it is is basically a way to if you're reading Json or Json lines or CSV the sheer amount of input data um requires there's a few different things if you want to read all the data in a uh and it's Json you have to you have to read every line um it doesn't support any filtering right because you don't know anything about the internal structure of the data um with parquet when you have certain things like um spark predicate push down filters enabled you can do things like say I want to read in only the data where the date is between a week ago and today and if you were to read Json um unless you're you're doing your data partitioning yourselves and then you like only read in the subfolders that actually have the data if you're if you're partitioning data by date um you can do all that but parquet will do all that for you um and basically you can do push down filtering such that you only bring in if you're reading Json you would have to read that data into memory and then you would perform your your filter but you you end up having to bring all that data into memory to do the filter with parquet you can do things like um you will only ever bring in memory or bring it data into memory that is uh matches your your filtering if you have your your predicate push Downs turned on um and so that's really nice because it's compressed there's just fundamentally less data to read and the file sizes are smaller spark is really a pin like really finicky with having um lots of of tiny files or too many big files it likes its files between 250 Megs in a gig um and so kind of like restarting um data to be of that size is a lot easier with parquet um so those are those are a few of the reasons so it's you can't read it it's it looks like gobbledygook um to a human but to spark it can do a bunch of fundamental out like optimizations against its reads and rights that makes it easier to get data into spark and get data out of spark yeah it's so interesting thanks so much for that I think I've like I'm just kind of starting to get in the water of the large-scale Imports and I've seen this problem where you're trying to Chunk Up the Json but you kind of have to read the whole thing and then parse through it but yeah this sounds really interesting and uh yeah let me kind of step out of the way because so uh learning so much from this but yeah John if you could take it over because yeah yeah another way to think about it is um what what Spock sort of focused on doing was decoupling the Historical dataway house we had storage and compute tied together and so um basically spark is the compute and you are separating the storage onto it's no longer a protocol just for that database um to read it's actually sort of an open format but it still has the um the properties that that make it efficient for reading so it's sort of a binary format of column now so you can come just query The Columns you need and stuff like that yeah it's a good cause the there's the push down filtering which means you can get a subset of the data row wise and then like John said if you want to select if you're saying only selecting two columns it has you can do that by not having to read in the entire every single file so that makes me think it's is it similar to hdf5 if you're familiar with that yeah because I've I've used hdf5 uh previously and at the image format uh no it's another data set format yeah so it is it is similar in some way to um one differences you can have multiple data sets in one hdf five file but in other ways it is quite similar to RK it's um yeah it is it is similar but I guess the large-scale big data use cases more lean towards using parquet or can and sort of file formats like that and so one last question I had was just around um so so using spark uh when using spark with a search engine I immediately think are using page rank the page rank um feature in spark for generating features or potentially generating other sort of um metrics uh that you're using as kind of uh ranking inputs so you're looking to store those in webiate as well so I'm not sure yeah if we were going to like if we're going to create features based on like some graph formulation of our of our web crawl data I'm not I'm not sure how I'm not sure how I would probably combine combine that with leviate and that could totally be because I'm not an expert in the technology yet but I'd love to hear if I I think for me I'm always looking for Simplicity over anything else I think that like when you're working in Big Data like this when you're working with machine learning Technologies like this the simpler you can make it for yourself when you're kind of like drowning in all of this very complex technology like the the worst thing you can do is is to over complicate things because it's always just gonna be a kiss of death for your product and so I think if I could yes because I'm always I in my at least in my infrastructure if I can get out of one tool what's going to take three otherwise be it like elastic for lexical I don't know like um using weeviate for semantic and then using like neo4j for graph if there's a way to do all of that in weeviate it's always going to be the preferable solution for me so I'd love to hear kind of like how you would think through combining graph features or doing like graph based queries in in weeviate and how you would kind of complement that with the semantic capabilities yeah so I'm all thinking that um the sort of the end result of the graph calculation you would still do the graph calculation in spark but then you can render to ev8 and yeah I see this more of an extension of our hybrid search capability so once we um with the M25 combining with um dense retrieval then I think the roadmap there is just adding more features around re-ranking and different ways of combining different signals together learning to rank and stuff like that trying to like combine features that are going to learn over time with the the scores that you get from bm25 and you know like your cosine similarities yeah that's a cool idea like where you have the um Vector surge score as a feature in the bm25 score as a feature and the other symbolic features and then you just use like uh the gradient boosting machine I think is a very popular machine learning model and yeah that's super interesting and I'm I'm so interested in this hybrid search rank Fusion thing because I think this kind of idea of like reciprocal brand Fusion where you just kind of merge the ranks in each list is a very powerful kind of abstraction for sending the queries to multiple things and I'm also kind of I'm sorry if I'm going in too many topics with this but like this idea of in like we could have external information sources as well so maybe we connected to like the Bing API it could be like a part of hybrid search and then we fuse all this information together with your specific leviate data yes I'm just I'm curious like your experience with hybrid search and you're thinking around it I it's still a pretty New Concept for me kind of just I'm started with just the bm25 vector search but I'm curious if you have any more ideas on this kind of idea yeah I think it it's always again the cop out is it depends I think that like your use case is always going to Define where you want to go so like I think that for people who have extremely high Precision needs and they're okay with being low recall um doing things with um like your keyword matching and your electrical search is always going to be like the easiest way to to get to a very high Precision system I think that what we found historically is that using semantic search as a way to start to like beef up your recall so if you have like a pass of lexical search to get your very high Precision items and then maybe you have a second pass of semantic search that can kind of like enrich that what you're returning and improve your recall but doesn't really hurt your precision um I think where it starts to get more challenging is is if when you don't have those as like two separate passes where you have either business logic or um you don't have like something in between that's that's very human legible um dictating how you intersperse those results I think that's when you're gonna be best off with some sort of like you need some sort of way to kind of like Benchmark and and be able to like evaluate the system against some sort of data set that if you're in the search space you're probably collecting either by human annotators or um by kind of reverse engineering and data mining from your own like interactions data sets from the people on the site but even then like it's it's always a challenge because with search um you you are never going to have all of your counter factual data right because like if someone searches and they don't find anything or if they did find it and they leave like do you know why people just bounced you know are all clicks a good click if people is like clicking it and then they they like come back to the site and then they click on another thing if I click on like the top five links like there's there's all these different in the search space all these different like interaction patterns that don't map cleanly to your standard label data set you could like just Benchmark a machine learning algorithm against and then do your hill climbing exercise um and so I think that I think that it make like what worked for us is starting very high Precision with lexical search using um using semantic search to improve recall and then when moving to a more complicated mixing algorithm trying to find some sort of way to to either um Benchmark against either hand annotated or finding finding ways to to data mind that out of your system but like I said like that that step is really tricky in the search space to get clean label data I never made the connection to the counterfactual in the relevance judgment before that's super interesting um maybe this is uh too revealing of a question but can you tell us about how you're thinking about collecting data u.com yeah so we we've announced this is this is public you can go see like um these announcements I don't know I won't get in trouble but um but we've done we do a number of like third-party studies where we get kind of like we do like blind tests of of relevance and we found that um a lot of the work that we've done have slowly gotten us to a point where we are pretty competitive with Google in in certain niches and so I think we are always trying to we're always trying to ladder up right so you have you have your top of the ladder you've got your third party like like studies that you do um that are very expensive you have humans doing it you have to pay for it like humans are slow so you don't have a ton of labeled data coming back for you so you have like but it's also the the richest data that you're going to get you get like human insights you get people explaining their thought process um and then you have kind of like going down a run you have things like you know you have your user metrics like things like um like retention and people who are like conversion metrics so things that are still very high high signal to noise but and our and you get a little bit faster feedback than at the top where you have to do a whole study um but still very slow feedback and then you kind of like you you start to go down the ladder towards like some of your click signals some of your hover signals um things like that that you can kind of glean from your click stream data and you're usually trying to as you go up that ladder you're trying to find like how strong the correlations are how to formulate um what is a good click versus a bad click and then doing like different regression analysis to figure out like uh if we if you hover for five seconds versus three seconds like is does that have a better like correlation with with our retention metrics and then how do those retention metrics then ladder up to our like our actual studies that we do and at each step of the ladder you're trying to figure out like how much you can kind of you can glean about what because what you're trying to do is you're trying to like use the higher the faster feedback signals like clicks and then project into the future what your your studies are going to show like what does that actually mean for the overall relevance of your search results um it's it's really hard uh we're still figuring it out I think that most people um now I think if you if you talk to people who work at Google or Microsoft they'll tell you that they still have human labelers they still they still require humans to go and label things they just probably have much more refined ways of of laddering up and forecasting out what they're they're more fast feedback signals are going to mean for those those studies yeah that's interesting that that answer makes me think if we're thinking about it as a ladder are you training individual models at each rung of the ladder or is it more of an ensemble where you have the lower rungs that are informing the models at the at the upper end it's it's more of the the latter and so you've got a model that's going to tell you you know for different and I think there's lots of literature out there that'll tell you like how to what does it mean to to try to calculate user satisfaction from raw click stream data and you have different formulations and then you try and for all of your different formulations you try and figure out like what does that mean for um you're more user you're more user-facing data so how do you take click data and correlate it with user data then how do you take the user data and then correlate it with um your your your scores that you're going to get back from your more long-term studies yeah super difficult problem the data is probably very messy very sparse too so awesome that was such a great coverage of these topics uh Sam it was such a master class in these uh in these topics especially the engineering details the spark the data collection at u.com the widgets the search interface it's so much learning to unpack and I'm so happy we recorded this podcast so thank you so much for joining the weba podcast yeah thank you so much for having me it's a pleasure and looking forward to building much of cool stuff with you guys in the future awesome thank you ", "type": "Video", "name": "sam_bean_zain_hasan_and_john_trengrove_on_youcom_and_spark__weaviate_podcast_32", "path": "", "link": "https://www.youtube.com/watch?v=pP8CZbDkUKU", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}