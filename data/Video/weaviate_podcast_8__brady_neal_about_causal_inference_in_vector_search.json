{"text": "Brady Neal from Oogway talks with Connor Shorten from Henry AI Labs about causal inference and many more. See the ... \nhey everyone thank you so much for checking out the seventh episode of the we vva podcast today i'm joined by one of the heroes in deep learning brady neal who has made an amazing course explaining causal inference he's had joshua benjio come on his youtube channel to explain the latest ideas and things like system one system two thinking the consciousness prior and what causal inference might be adding to deep learning brady is currently the founder of oogway ai a decision-making ai startup and he's also a phd student at mila so brady thank you so much for coming on this podcast thanks for having me connor so to kind of tip it off could we get into just right away what is causal inference and what does it add to deep learning yeah so so cause of imprint is essentially you're trying to infer the effects of your actions what are your actions causing and i guess the way it can be related to deep learning is so they're not necessarily competing so you can use deep learning in causal inference there's a basically you you might use machine learning models to help you do causal inference and deep learning models are our good kind of machine learning model um and then there's also some insights for how causal inference can help out with topics that deep learning people care about so for example deep learning people care a lot about out of distribution generalization or or compositionality uh that kind of stuff and and there's some causal inference insights that that can help out there so could we start with this idea of um of compositional generalization and causal inference is this where so say um we've seen things like disentangled representations or say you're trying to have that low dimensional vector representation be interpretable as high level variables and maybe you're trying to draw causal pathways between these high level variables is that kind of the thinking around what causal inference brings to that kind of dimension of you can compose novel configurations of the high level variables and then having say the uh directed acyclic graph pathways helps you helps you like structure the compositional testing is that the right way of thinking about it yeah i think that's i think that's very good way of thinking about it so like one way a lot of people or at least maybe a few years ago you would think about disentanglement was this sort of statistical independence between these variables right that means that these variables are independent they don't they don't affect each other at all um which is which is like kind of weird if you if you think about objects in a scene and you see like two balls and and one hits the other they it affects it it affects that one um so yeah i think it it makes more sense to think about disentanglement as you're kind of you're disentangling out the factors in this causal graph so like one ball and two balls in the scene then maybe there's like a background or something in the scene and then you can you can draw arrows for which ones affect each other and which ones don't and such yeah but so yeah i think that's a good view of disentanglement is figuring out the causal graph and the variables in the graph so do you think um so say with these uh generative adversarial network architectures how we have a random noise saying like the um the latest style gan framework they start off from a fixed representation and then you have the random noise that comes into the pathway and it hits those intermediate kind of layers is that a good example of say these this unobserved phenomenon that you model in causal inference where say with these randomized control trial diagrams how you have some unobserved factor of variation that you kind of work into the model is that similar to injecting noise and intermediate activations for deep neural networks yeah okay so i'm not sure so so this this recent style again thing i'm probably not familiar with it but i can i can imagine what you're talking about injecting noise into layers of the network uh that almost to me is almost like you know you have this function that you're learning and you're you're like injecting noise in parts of the function that that's how like i think of that yeah so i don't know if i'm familiar enough with it but but if people haven't like explicitly stated that they're trying to capture causal relationships and and stated it in more than a way like an nlp you see the phrase causal language modeling that doesn't that doesn't mean like what we mean in causal inference that just means they're they have like ordering to the stuff but so yeah if they haven't said it say it that way then i'm guessing they're not trying to capture causal relationships yeah we are familiar with with the this recent style game thing can we stay on this um this causal language modeling that you just mentioned seeing that so what it what is the um so i guess the current thinking is because you're using the leftmost context to predict the rightmost mass token you say it's causally the left is causing the prediction of the right but is that maybe not a honest interpretation of cause of that causal phrase yeah well i get i get how people like people read from left to right so i guess i guess i can see how they're thinking about it as causal right like um as you're reading from left to right your brain might be predicting the next words or something and those words that's predicting our function of the previous words because you're reading from left to right so it's like causing you to predict the next words uh in your brain but in terms of objects in a story you know it might be that the the causes mentioned later in the sentence where the effect is mentioned earlier so is so is that more akin to say like named entity recognition not like named entity recognition but that task of uh assigning labels to each token so you would say label this token as a cause token and an effect token is that would that be like a more way to bridge the gap between nlp and causal inference yeah yeah i guess uh yeah that could make sense you train a named entity recognizer thing to yeah to get the cause and the effect it's um yeah and then you have a data set right where you try to where you try to do that that could make sense you would have to have a model that looks both ways right you wouldn't you actually wouldn't want to have a causal uh language model i guess right you would want it to be able to have the future and the the past context and the sentence so once you have the cause and effect labeling should you map these tokens into one of these symbolic dag graphs that like would distilling your model's predictions into one of these graphs be something that can maybe facilitate explainability and then want to go and get into counterfactuals next but do you think that kind of distillation between uh maybe if you first fit your training data set and then uh on your maybe even just within the same training data set you go from the cause effect labeling and you distill what it's predicting or what the representations it's learned into these symbolic dags and then the dag is the thing that you run inference with because you can look at it and say okay i can see the pathways that are causing this prediction yeah so so i actually haven't there are people who do research on um causality in text um but i'm not that familiar with it and i think a lot of it ends up being like sort of propaganda um kind of research right like how effective was actually yeah i don't know i'm not totally sure on that stuff but uh but yes you could you could take text and then try to uh figure out what are the variable like the relevant variables that you that make sense to attend to and then those are your variables in this causal graph and and you could model causal relationships described in the text i'm i've never done something like that before but i could see someone do maybe there's a paper that does that kind of stuff in terms of explainability like causal graphs i definitely think aid explainability you know like any time someone's asking a why question um you you could potentially reframe it as like what caused this you know there's a why this well what caused this and um so causal grafts you know you can have the the thing that you're trying to explain and then there's these proximate causes the things that are closest to it that have caused it and then further down in the causal chain there's things that cause those proximate causes right and so the the freight i'm just using uh terminology here in case people want to google it but there's approximate and then there's like ultimate causes where um it's essentially that's the stuff at the very beginning of the call of chain but you know usually you can take causal chains very far far back um and yeah so uh i kind of lost where i was going with that oh explainability right it was unexplainability and so you can discuss explainability at various layers in the the graph you know you can discuss it well here was ten days ago uh versus here was like one day ago so say you're trying to explain covid covid stuff you know then those time ranges uh there's different time ranges that make sense to look at and stuff so i've seen a lot of interesting things on say unifying structured and unstructured data where what you do is say you go through the table and you parse it into natural language do you think going through so you learn you create some kind of causal structure with the approximate and ultimate causes you think maybe having some kind of parser that turns those into natural language phrases and then just go ahead and language model those phrases do you think that kind of distillation of from the causal model to the language model could be a useful path so is it uh let me make sure i try to understand so you start with a causal model and then you want to produce some language thing what is the point of language thing is it to like explain the causal model or yeah so so you maybe um traverse it and then annotate and then say like um x had this impact on a which caused this impact on b and you but then you phrase it as a natural language sequence and just add it to the training data i i see so you have someone cause a model and you could potentially generate many different sentences that follow that model like uh right like these it's like having a grammar i guess and they're generating a bunch of stuff that follows the grammar yeah that makes sense and you could use that as uh as as data so so in this kind of thing we're talking about now i think mostly um trying to understand how we go from maybe this kind of idea of observational causal inference where we try to discover causal structure in say an internet scrape of text data and then there's kind of like i guess the experimentation side of causal inference and you touched on that quickly this idea of text is treatment and i've seen these papers where say um like the propaganda studies where they're they're studying how different ways that you um the different styles of text that you use to present news the the impact of that is a treatment so it's like a b testing but directly using text and representations of text to be the uh to be the treatment have you seen these kind of studies and do you think that kind of stuff is interesting yeah yeah there's a word for this right like there's some phrase where it's like you frame it uh this way and then people overwhelmingly answer yes and then you frame it a different a slightly different way but it's the same meaning and people overwhelmingly answer no i figure what the phrase is for that there's some term uh in like uh psychology for this anyway but yeah that stuff's really interesting right they're essentially doing an experiment where what they've held constant is the meaning of the the say they're doing a survey they've held constant the the meaning of the survey question um which is like controlling for factors right causal influence roughly you could think of as just controlling for factors you've people have probably heard that phrase before maybe doing it in a fancy way but it's controlling for factors so they're they've controlled for the meaning they've held that constant and then they're just changing the phrasing so the idea is just the phrasing is enough to have very different impacts on people's uh psychology right their understanding of the question or of the statement so so with that idea of trying to you know use the causal inference to take action and use a particular kind of text to influence people or whatever whatever the um the setting is what's the difference between structuring that as a causal inference problem or structuring that as a reinforcement learning problem yeah well so so i haven't thought a ton about the relationship but i've thought a little bit right so reinforcement learning you're you're essentially trying to figure out what actions you can take to maximize reward um you know whatever or utility whatever your uh the thing that you want right and in causal inference you're kind of doing the same thing right you have some outcome that you care about and you're trying to get the best outcome you're trying to you're trying to well if you're doing it prescriptively if you're doing causal inference decide what action to take like what decision to make then you're trying to figure out which uh action take to take the best out to get the best outcome and i guess it's just that in reinforcement learning it's more you have all these trials where you can like experiment a bunch maybe in continual learning where it's like only one one go right you when you die it's it's the end it's it's a little bit different but in reinforcement learning in normal you have so many trials you can die infinitely many times so you can really test out like everything given enough time and so i guess causal inference you maybe you could view it as like trying to be a bit more efficient with the actions you try out and how what is the information you're using that using to do that it's like thinking about the causal structure of the problem what are the factors that could be confounding um my actions and outcome and and that i need to control for to get a better guess at what action might be the best action at a given point yeah it makes me think about say model-based reinforcement learning where you're trying to learn the transition dynamics as well as how to act in it and i think maybe with the causal structure to learning your world model is more uh like you have the explainability you have the high level variable compress compression but then you still kind of need to go from the high dimensional observations into the high level variable space right so maybe things like self-supervised learning and how they say one paper is curl where they do contrast to self-supervised learning with the observations to try to facilitate that representation and then from there we can maybe construct these variable models for our dynamics and our transition models so once you have these transition models can you just unroll a counter factual by just you know the proposing some kind of action or is it because it's out of the distribution space that the model is just gonna compounding error and maybe causal inference adds something to that idea of counterfactually querying your learned transition model yeah so kind of factual share being like um what would the or or even maybe potential outcome is a is a more intuitive word in this context like what would the the result be if i were to take a given action you know the potential or what would the outcome be what was the potential outcome so yeah if you have a perfect model of the world you can just run the dynamics of that model and change the different actions and you'll see which actions will produce um better things and that's like that's basically what reinforcement learning does yeah when it's run in a simulator where the simulator is taken as the as the world they basically have that um yeah and then counterfactual just to clarify some terminology a little bit like when i use counterfact people use primary engineering differently when i use counter factual it's like so you start off with two potential infinitely many potential outcomes as many actions as you have that you have a potential outcome for each one of those actions and then once you take an action like um uh say i take a pill then i realized that potential outcome the outcome when i take the pill and then the other action like don't take the pill becomes a counterfactual right so uh it's counter to fact i can no longer observe it unless unless you have a model of the world right so we actually uh i have a blog post on this so a blog post and a paper so you um if you have the the model of the world then you can observe all counterfactuals because you just run through your model yeah i blog post on this and then there's a paper called real cause that helps people essentially observe counter factuals and there's there's various work where people use deep learning to try to try to get at counterfactuals so maybe from this um one of the early examples in your course of uh sleeping with your shoes on and that leading to whether you're woken up hungover or not or whether it's really because you've been drinking if if you unroll those into sequence so it's like several of these decisions can you take me through what it takes to generalize that kind of thinking into you know you have five potential like it's not just shoes or drinking it's i don't know whatever whatever leads to going out like call from your friend or something like that yeah okay so you're talking about a sequence of actions and then the okay so say you have a sequence of actions and then the outcome is uh whether you wake up with a headache or not uh yeah yeah so um so for people unfamiliar with this example it's just like the idea is that you you notice that um people who go to sleep with their shoes on often wake up with headaches so it must be that uh when you sleep with your shoes on they cause you to have a headache i like amusing examples is why i chose that example but but you know it's really just there's some confounder uh that's causing both of them which is like you went to sleep uh drinking and so okay so you're talking about that's one action is a go to sleep drinking but maybe there were actions that preceded that one yeah so yes you could think about essentially like like the picture i have in my head is it is a tree right um like weight but y has this nice tree diagram where so like this is you at a given point in time and then this is one path of actions that's where you would end up this is another path of actions another path and you can view it as like a tree of many paths branching out every time you take an action and then so in that example you kind of followed one path that ended up at going to sleep drinking but you could you could follow other paths there um and each you know it's like the butterfly effect thing right where uh one thing a long ago could have a outsized effect on let's say whether you wake up with a headache or not right like um if you decide to take a nap in the middle of the day or or maybe no maybe that's a bad one i don't know anyway uh if you get put into a coma right then you're not gonna wake up with a headache uh because you got put in a coma a year ago and it's like a long coma so uh so that kind of reminds me of say the um like the mu0 monte carlo tree search of rolling out several pathways and then trying to pick from the pathways and to me that's kind of an interesting example of a neurosymbolic ai system where the monte carlo research rollout is like a symbolic program that you layer on top of your neural program do you think causal inference is like the way to bridge these neural predictions with some kind of symbolic reasoner and is that kind of the maybe the best way to be applying it in with the current state of deep learning tools um i think that if you ask a lot of causal inference people uh they'll say yes but you know pete whatever people are most familiar with is the is the tools that they want to use so i mean causal inference is useful for um just when you're trying to infer effects of actions and you don't already know those effects right like if you have a simulator or a accurate model of the world which is like very hard to have right but if you were to have that then you don't need causal inference really or i mean you could say that the causal inference is just running the thing through your world model that's that's you doing causal inference and you've kind of you've kind of pre-done all the work of adjusting for confounders by building that world model i guess um but yeah so so always the question is should we use causal inference to build up that tree yes so the question is just kind of about this again this bridging between just neural predictions and then some kind of symbolic program and i'm thinking about whether it's about uh just this general concept of distilling your representations into this kind of tree so i don't know whether you can and and then the the connection of because you know the um the connection between a to b or like did i drink beer last night did i uh uh take a nap in the middle of the day with these vectors it seems also kind of hard to connect them i'm kind of just trying to imagine this how to how to construct these causal inference pathways from these vectors and then how to kind of traverse potential causal pathways when you're trying to uh yeah like trying to make an inference on your learned world model or you're still kind of building it i guess yeah well okay so i'll try to separate a few different parts of causality out so say you you know you a causal graph right variables and arrows connecting the variables saying which variables cause each other that's the kind of picture to have in mind um learning the edges or the arrows of the graph like what causes what is called causal discovery and machine learning people have been mainly doing a lot of the work in this right but it they've been doing a lot of work um whenever you have experiments it makes it way easier whenever you can do experiments like in gene regulatory networks right so genes interact with each other to produce to express themselves in your uh phenotype you'd write like uh just how you look or whatever right something that actually comes out into the real world and uh that people do tons of experiments there that helps a lot with discovering which genes cause each other okay but starting with like implicit in that was that we already know what was a gene like we knew the variables in the graph and so the keyword there is kind of new so i call it causal representation learning there might be one or two other names for it um there's definitely one or two other names for it like uh eberhardt is the last name of like one of his students is one of the first people who started writing stuff on this um that's like learning the variables right which you could think of as the disentanglement problem if you think about it from causal graph and um yeah so i that's a that's like an emerging area of research mila there's a few people at mila who are actually doing a lot of work on that or they used to be amelia when they graduated and um yeah so that's something i'd recommend people doing research interested in like a frontier of research they should check that out but essentially you could yeah you could learn that whole graph but then there's not just the variables and the edges it's what are the functions that map from variables to edges right so the causal mechanisms is what they're called and that's where you know like it's nice to use deep learning right so it's like you essentially fit for every for every node all of its parents in the graph that's a function for mapping that's like the inputs to the output and you can fit deep learning models there and stuff and and so essentially you could learn all of that learn causal representation learning so to learn the variables learn the edges and then learn the mechanisms and then that would give you a world model essentially and that you could run any question through like any uh kind of factual and stuff if you have all of the variables right so then that's where no you you end up having like noise so you you model some stuff as like this is unobserved noise um ideally there's not confounding like unobserved confounding that's that's where uh things get tough to do causal inference when there's unobserved confounding so i want to get a little more into unobserved confounding but but quickly just to because i need this understanding of is what's the difference between modeling the parents as a conditional probability distribution compared to this uh do calculus idea yeah so for these causal mechanisms you could think of them as just conditional probability distributions or you could think of them as conditional probability but you put the word do in one of the find the conditioning bar for some of the variables right and do there is meant to mean what would happen if i were to intervene on this variable like i were to physically set it to that thing and okay so how is it different from a conditional probability distribution well in regular distributions you're not intervening on things you're just given a data set you're just observing some data and then you can you know through like bayes rule or whatever probability stuff you can compute conditional probability distributions um right so it's really like the do the do operator is the more intuitive thing i think to humans uh we think about intervening on things and it's actually the conditional probability thing that's um less intuitive like that's where people uh that's why confounding is so important because it it will confounding will affect conditional probabilities like they won't do what you intuitively think they do the conditional probabilities but then um if you have a a do behind the do operator like do change this action to this action it will do what you intuitively think it do like it'll be measuring what would happen if i were to take this action and and so do calculus i mean is a very complicated thing right so do calculus is like how do you get from conditional probabilities to putting do operators in there you know because it's like well all we observe is conditional problems we just observe data we can't say we're not allowed to take actions how do we get to the what calculations do we need to do to get to the things where we're saying if i were to take this action this would happen and that's where do calculus comes in that's where causal graphs come in to try to uh kind of inform how you would do the do calculus it's just like a sequence of operations um yeah do calculus is very fancy so it's like that's the most uh general way to adjust for variables so you it can produce many different algorithms do calculus can so at a at a basic level would i if say i just introduced a second conditioning variable like a binary do variable and i'm conditioning on that additional zero one do variable and maybe that conditioning could be injected and say like uh the gain and the scale and shift parameters of batch normalization to like to like shift the distribution such that you kind of know that you're um doing and and have more layers of control integrated whether it's like other kinds of normalization schemes to just let this do conditioning variable have more control would like a like a binary do conditioning variable would that be something that would maybe be useful for say like um say it's like a reinforcement learning agent and you're trying to again learn this transition dynamics model as say you're just um in like an atari game and you're saying i'm gonna do hit left for like four turns in a row to see what that how that changes my visual observations would that additional conditioning variable be enough to sort of simulate the impact of of deliberate action yeah well so if you can actually yeah and and so it does get a little bit confusing and do calculate or in judah pearl's notation right the guy who made do calculus with his with his students um it does get a little bit confusing that he uses do um behind the conditioning bar but you could think of that as like a totally separate probability distribution like think of the the do thing as in like the subscript of the probability distribution that's that's maybe a more um nice way to think about it and so it's like they're separate distributions but one is what you would get if you just observed the distribution you have right away and then i think the thing you're talking about is well what if i go and change the program or the simulator or whatever and i add in this this parameter that allows me to to change it to um like actually i can actually intervene on that parameter and those are like two different distributions right so so one is like where you haven't done anything with that parameter another one is when you've changed it to to zero or you've changed it to one um well maybe one of those is the same as uh when you haven't done anything but those are basically different distributions and they reflect what would happen in under two different conditions when you do that intervention or you don't do that intervention and so when you can do that then causal inference is easy right all you do is just run the experiment you know you don't have to do any adjusting for anything because you have access to the the world model essentially and you can you can just run the experiment so in another kind of sense of these world models and kind of transitioning a bit to say so say with coming back to text data say we know things like negations if you insert a not it should flip the label or say named entity swapping if you say some specific fact about chicago and you swipe chicago with dallas you know that the faction like the question answering system shouldn't produce the same answer can we use these kind of data augmentation interfaces as our kind of intervention tool where say our simulator now has the knowledge that if you insert negations or if you swipe entities and the kind of things that are included in say like a like a checklist kind of system for deploying an nlp model and just benchmarking its robustness can we use those interfaces to explore causal interventions in our in our text data okay so you you have a like is it like a grammar where you can put a not in front of things and then that produces a different sentence or it produces a different meaning yeah and i think that's that is a good way of thinking about it just like these grammars and just the formal languages and the kind of idea that you have this kind of precomputed dictionary of how it should flip the label but i guess that would be abstracted to the agent so the agent doesn't see this grammar it just starts uh it just starts perturbing the original text sequences and the grammar flips the label when it's when it has flipped the label so so your agent isn't exposed to the grammar it's but the grammar is doing the label switching that the agent is can hopefully do the causal discovery of the relationship relationships between the words because as it say inserts not and then the grammar flips the label maybe that could inform it on the relationships between the whole part of the text sequence so so the agent is allowed to say like hey i want to put a not here and then and then it can observe what the label uh yeah like that kind of strategy yeah well so basically it's like um agents allowed to run that you're giving them access to a certain kind of experiment right like where they can so so they're always observing this this label and then you're giving them access to a kind of action like where they can add this not not to things and then see what happens yeah and so if the agent doesn't know the rules of the grammar uh it needs to learn those from taking actions right uh if you could just tell it the rules right you could encode it that that's not the deep learning way but uh if you were to tell it the rules then it would have that knowledge too but um but yeah i can learn it from uh experiments and that's uh just generally like to generalize a bit like that is a thing where um you're trying to learn causal graphs or you know or something else well yeah does it say causal graphs from performing experiments and people come up with algorithms for that yeah so like in reinforcement learning you're trying to learn you know what what actions to take to maximize your your utility uh so it's a little bit different like that the objectives of the thing but then just to come back to something earlier it could be that learning the causal graph like this is kind of one of the hypotheses of model-based reinforcement learning is that learning to cause a graph or some some world model actually will help you uh with your utility objective which as humans feels like intuitive that that that should help right it's but it might be it's hard to engineer that kind of stuff so one other topic in kind of coming coming away from uh the world models and data augmentation and the idea of learning interventions and producing counterfactual predictions through these world models is i was curious about this idea on whether you can use causal inference to distill the complex pathways of these giant deep neural networks is is there a way that we can kind of we have say 20 layers of computation and you know they're wide and there's all sorts of things going on could we kind of do causal structurally causal structure discovery within these activation graphs of these big deep neural networks to kind of compress it into like symbols that kind of explain why it's making its predictions yeah i mean uh i don't know is the answer right but uh like that certainly it would be a cool thing right so people who say they need to be able to understand the neural network better and they don't they're not either they don't know about or they're not satisfied with just like perturbing the inputs and seeing how it you know sensitivity analysis preterm inputs and seeing how it changes the outputs um like um what are the there's a few tools a few tools that do this uh i forget what they're called so it's like for any black box function they'll do a sensitivity analysis and try to give you some interpretable results do you know the things i'm talking about uh i've heard of a technique i think the most famous one is called lime yes lime and then shap and uh yeah okay yeah so those things so if you're not if you're not satisfied with those things right but you want to get you you essentially want to like hey here's a neural network it has all this we've taken all this information from our data set and injected it into the weights of the neural network um great now the neural network's useful but like i want to understand um i want to you know i see the data and i see stuff now i want to go and look at the neural network and see that same stuff like pull the objects out of the neural network and stuff um yeah i'm not sure if someone's tried to do something like that but yeah maybe it's like rather than learning doing rep calls or representation learning on a data set you're doing it on a on a neural network right like you could view the two as similar right there you're basically trying to pump information from your data set into your neural network over the course of training yeah that'd be cool i i that would be a very cool thing to to do it sounds like a cool idea yeah that kind of idea of like abstracting it into it's you're like say the teacher student and the student is like the inference network and the teacher has like information about the student network maybe it's used to train it has been the most common thing like meta pseudo labels teaching with commentaries massive lists where the teacher is labeling the data or augmenting it or doing something but the idea that the teacher has the compressed causal structure of the student and is used for that kind of interpretability so i think we've touched on a lot of interesting topics around causal inference and i think we could go on and on about all these different things and thinking about what it might add to deep learning but i'm really curious about oogway ai can you give me the overview of what it's about yeah well and so to draw a line from called inference to boogway so to ugly is about helping people make better decisions and the the line there is is i think something that i mentioned earlier it's basically just um the decision that you want to make is is one that will give you the best outcome um well at least for me um and you know anyone could say what their outcome is right people could have different ideas for what is the best and um so causal inference helps you figure out what will what action will lead to the best outcome right anyway so yeah so oogway is about uh helping people make better decisions and and essentially like so so where people here there's like a an app for um i guess a consumer app that we're building that will help with that and and uh it's supposed to be for all decisions right so i know that sounds like academics will use the word ambitious right which is a which is negative i think to say um but yeah so so we have some ideas about how to actually uh make this feasible uh which i can get into and uh but one other cool thing about oogwa is just where um like i call it company 2.0 i think there's going to be a next generation of organizations coming where you know basically when the the concept of a company was invented i don't know like 1700s or 1500s whatever that was you know we could kind of take it for granted now but it was a really uh valuable concept that produced lots of value for the world and i think that it might be the case that we're getting to uh the next level of that concept like say company 2.0 which i can talk more about and so uber is trying to be one of those early organizations that are like that and the idea is like it would be cool if everyone could feel like they're not wasting their time very much uh you know they're trying to they're actually like maximizing the amount of value they're they're producing um from their time maximizing uh their interest and what they're doing like people who want to learn new skills can go and learn new skills and uh and just generally like people care much more about empowerment in our generation than like one or two generations ago or sorry fulfillment uh being fulfilled self-actualized so i think that there are some tools that can help a lot with that in this next if this is the next generation of companies and so who is trying to to have all that stuff uh you know you can't maximize all those things but you know there's some some trade-offs of course but uh yeah we're trying to that's the the goals so can you take me through kind of like the engineering architecture of combining these different tools like combining uh like a causal inference part of it with say the we v8 search would say the hugging face models and is that the kind of the architecture is it's like a routing through the composition of different uh like ai as an api kind of things okay so the yeah so the architecture for this in the the first image here basically you can see that there's a a consumer on the left and then there's developer on the right and essentially you can think of developers as putting all these apps into a sort of like app store like thing uh i call app store 2.0 for for similar reasons as company 2.0 and and basically the user is going to see some unified interface where they're routed at any given time they're routed to the sort of ai decision app that is most relevant for their given decision right so you could start off with let's say i cared about um deciding um what restaurant to go to you could have an app that first is just a general search app right so it's kind of horizontal so in the um in the second image here uh it's horizontal in the sense that it works forever everything like google is a general search app and then you could later on be routed to a different app that's say focused on on restaurant decisions after you it's gotten the information from the search and yeah so so different apps here and they can have different scopes so they can be across everything like horizontal or they can be narrowly focused to just one one thing and um the reason that we decided to do this so initially we were looking at like initially the plan was like hey ai is getting way better and better um you know gpd three's got this cool stuff and so we you know we tested it out and it's like okay it's cool but it's it's kind of like a novelty thing where people like it initially but then it doesn't really solve their problems well enough to be a good product and and so then we started developing more specific stuff you know that involves actually fine-tuning the models once you fine-tune them you can get pretty good stuff but then that takes developer time so the the kind of realization was like okay to get good enough quality for a lot of decision apps you have to have some non-zero developer time and so we um so that's where okay let's try this app store model and um over time we can automate like more and more of that it can be so that it requires less and less developer time but then like does that look attractive to developers you know if um they're they're getting they're they're spending less hours you know they might be worried about compensation so that's where it's like well you have to compensate them in equity um like ownership of the company right so then as ai gets better it helps oogway be better and helps increase the value of oogway but because the developers were at least partially compensated in equity they're kind of hedged against the idea of like ai making it take less and less time to develop these ai apps um and also you know it's like okay well that you need a huge uh developer supply to do all that stuff right to cover all decisions and so the insight there for like why this is kind of good timing is is i think that the ai developer supply is kind of expanding to the software developer supply which is like a larger supply right as we get tools like hugging face and and haystack that helps helps with search and you know open ai as we get more in tools like that um it just makes the barrier to entry for ad development much easier and kind of can expand it to general software developers which were which we're starting to see a bit of oogway with like people who are not ai experts developing ai stuff too yeah i think this is an extremely exciting idea so to first let's try to like just to kind of unpack each thing though the first thing i want to talk about is some so these big models like gpg3 and maybe we can use prompting to try to get it to do more tasks or we can say what they're really doing is the mixture of expert style of say like the latest glam model with the trillion parameters from google it's looking what like what they want to do instead is have kind of sparse routing activations instead of one massive dense architecture and so something that i've been really curious about and it looks like you're already set in motion on doing this is applying that kind of mixture of experts kind of layer on top of say the hugging face models api where uh i think they have like 25 000 models that are probably more actually i don't know i think probably around 25 000 but the idea of like of having a layer on top of that that learns how to route classifications into the hugging face model like the hosted models and then and then that's probably maybe like maybe is that more powerful than gpg3 or what's the comparison of that kind of idea compared to prompting in gbt3 yeah yeah that's really cool yeah we call it the routing team at uh at ubi right so like who's in charge of like routing stuff uh yeah i mean it certainly seems that right like take the macaw from the allen ai institute they're supposed to be gpg3 at kind of general purpose question answering so certainly if you and like that's on hugging face right um so yeah so that's just one example of where if you routed everything to gp3 but question answering stuff you write it to like macaw say then uh this is one example right there's probably other ones then it should be better than gpd3 so so yeah so something that i'm really excited about about uh building out at wev8 is our uh vector search demo api so right now we have wikipedia wikidata and um and news publications and i'm working on adding uh the keras code examples and chaos api reference as search apis so similar to uh querying a model and just getting an inference on and and we have say the t5 that unifies every task supervised learning task into language modeling kind of so you can have this one interface for querying the 25 000 i have no idea if that number is correct but the x amount of models that are on hugging face similarly how does that generalize to say not just querying a model for uh for its inference but for querying a search engine where each search api you can have the different retrievals so you can have tfidf bm25 uh expert different ways of getting the representation from sbirt you can have prompt burt so you can have like this stack of retrievers the stack of re-rankers and further processing and you can have symbolic filters into the retrieval so how does the transition oh sorry this is like going off a little bit but the transition from a mixture of experts over the abstraction layer on top of models to the abstraction layer on top of search apis okay so so it's you're talking about um the things that you're retrieving are models from hugging face right yeah i think that's another way of looking at it is you could you could structure uh this task as retrieved and read where you're retrieving hugging face models as and then read and then re-ranking the models further like with that kind of coarse-grained and then fine-grained stage one stage two processing pipelines and that's definitely another really interesting idea i guess what i'm getting into more is so like the idea of say uh information augmented models that uh query like document indexes so like say the chord 19 data set from the allen institute where they have 140 000 papers related to cova 19 supplementing it with that knowledge store so so the idea of the abstraction layer on top of wev8 vector search apis is you have the chord 19 corpus you've got say archive pubmed wikipedia reddit conversations twitter and they're all in separate search apis and so you're you're the orchestration layer is going how do i want to route my queries to these different information sources compared to these the other idea which is these different uh like say like compressed brains for the model inference so okay so you're talking about um like you're doing you know uh like a search engine right um where there's various components right there's retrieval re-ranking and stuff um but but the routing is happening to the to the data sets right so uh rather than just within one data set you're right you're essentially finding the the relevant documents uh there's a there's a layer before that right that's that's uh writing to the data side okay so sorry i i think i i think i understand that okay so as we think about building these like meta abstraction layers that orchestrate a like it's it's so it's sort of like instead of trying to put an entire data distribution in one model we simply we break up the information sources into different apis and then the oogway layer is you know is the thing that routes through it and says here i've went and searched the these specific knowledge sources and this is what i've produced kind of so there yeah so there's models but then there's also information sources right and so i guess the way i so i haven't thought a ton about this right and it's like not that i'm the main one who will be uh implementing the search stuff but the way i thought about this was that this is like the search um apps job right like that they're doing um they're doing search right like like um you know trying to do google like stuff and you know because google's trying to do this stuff really well right um and you know and probably succeeding reasonably well like they're they uh it's probably non-trivial to improve on google but yeah so all this stuff about like routing to different information sources in my head i think of it as like um one app a search like a search app but maybe it could make sense to think about as sub apps maybe right like i'm not sure um but but yeah that's how i think about uberway and it's like the search stuff is then just like one step in the decision making pipeline like one flow of that you might do in decision making is i search for stuff i filter it down and then i have some options and i evaluate the options right like that's one example um and and that in that example search is kind of like the first step so like a common thing with google is people will go to google and then they'll they'll click into the first link they'll skim through it click out click into the next week skim through it click out right and there's this like process of downloading tons of information into your head um that you think probably you could download this information faster or you could be targeted to this information faster and that's all about improving uh search it's just getting that information but then you have to like grapple with that information so if you're looking at products to buy you have to think i need to remember oh what was that product i liked that i read in the first link when you're reading the third link and you're trying to evaluate them but you have to click back and forth between the two that's because google is focused on search so you could have a an interface that helps more with those other steps sorry that i kind of kind of yeah went on a tangent after answering the initial thing so i guess the kind of the analog i'm trying to make is say um we we went on this transition where you compared gbt3 to say a mixture of experts over hosted models on the hugging face model hub and this idea of a mixture of experts is so exciting because this attention operation it's differentiable it performs this differentiable routing that is causing this boom and mixture of experts i'm trying to make the analog that say from gpt 3 to the model hub the hugging face model hub is equivalent to google to these we v8 demo or these search apis that we're trying to build with wva where you have a collection of different information sources and so you're routing through how you want to occur these different information sources compared to the tasks similar to gpg3 of trying to compress the in all the information into one api which which is maybe too grandiose and requires this i mean i know i guess like the point of google is doing this kind of orchestration layer but i think as maybe as you kind of fine-tune it for decision making and for personalization it has a different kind of flair to it but so one other question i want to touch upon and as i'm looking at this architecture is this human in the loop human computer interaction and we've read there's a lot of really cool papers that are coming out about how people are using writing assistants chat bots to help them write what kind of specific things they're using so what kind of like how does human feedback kind of play into this loop for routing between the different ai apps as well yeah well so we think of oogway is it helps people make decisions it doesn't make the decisions for them maybe in the future it'll be a subset of people who or for a subset of decisions they wanted to be made for them but that's not really something we have in our sites right now uh or like maybe in the future you want something to like automatically schedule your calendar for you or or something and you don't have to worry about it like i think there's actually an app called motion i think that that's or a company called motion that they're trying to do that but yeah so it's just helping the person right so for example i'm trying to figure out where to move to right now and i care a lot about uh climate stuff right so i look at maps of of um sun like sunlight hours uh so it's like a us half an inch in it map and it's a heat map of the us for sunlight hours and then i do the same thing for like uh snowfall like i you know i want some snowfall and for the maximum temperature in the winter in the summer i don't want it to be too hot so for user like me the idea is like it gives me that information um and then maybe i could also tell it like okay i've identified that i have these three constraints that i would like or these like 10 constraints and then you know like that i want this snowfall above this amount i want sunlight hours above this amount i want maximum summer temperature below this amount and it could it could also take that and say okay here's a place that satisfies those this is like for me for choosing where to live but for other people it might be different they might want different um information or and they might have different criteria but the idea is that uh they could they could give the criteria um to oogway and then oogway helps them see how the different options rank according to that criteria so like a common thing if that people might do is they make these tables where they have the options in the rows and then their criteria in the columns or a transpose of that um yeah and that's like a very simple thing that you could think where it's like we we could build those tables for for users um if they want those tables right not everyone wants those tables so it might look different for them sorry i think it's going to hit them so i'm thinking about this this kind of idea of this application of trying to find the perfect place in let's say uh like the united states that you want to live based on the climate so i'm thinking about this routing between these different apis and so with the we va search engine what you could do is you could say one of these apis is um like all of reddit data and you can give it the symbolic filters the symbolic text filters uh that discuss climate and that are discussed in the united states but maybe you want to match those regular expressions for the symbolic filtering for the retrieval and then you could prepend like a chat bot that takes that kind of information and is maybe personalized to this user is that kind of the thinking of just like so you take the the we v8 reddit api to retrieve information about this particular kind of idea of um you know i want to have a some sunlight some snowfall and then you have some kind of maybe like a reasoning model from the hugging face model hub that maybe has been fine-tuned on some task of in-domain data where about weather conversations i don't i don't know if such a data set exists but is that kind of the thinking of routing between these different kind of like like open source ai's api kind of tools yeah um yeah so i think that sounds right like the um so the user might be taken to like a reddit conversation that's relevant for them is that the kind of thing you're thinking of well i guess so i guess what i'm thinking is we're decomposing this task into two parts where we use the we va reddit retrieval to get like 50 000 of these conversations and then and then we further pass them into a re-ranker that you know compresses it down into the salient points like maybe like an abstracted summarization model from this massive context or something and then maybe some other kind of chat bot layer that you're and you're just kind of plugging in you're just like api requests from yeah yeah absolutely right so um [Music] so my uh co-founder federico he built a um a thing that goes through several of those steps um where he's like doing search and then he's extracting things and then eventually trying to summarize them um and and yeah so the idea is that like a a person a developer could go and develop whatever they they want potentially with help from this sort of like core tooling right so there's all this nice core tooling right like so there's there's hugging face there's webv8 haystack um so we wanna like use all those you know we don't wanna build uh our own versions of those i think but then there's places where so we were using haystack right and we were having like it's just not as easy to use as hugging face you have to handle hosting and stuff and so we were building our own like hosting stuff like well let's just make this an api call um so that using haystack feels like using hugging face for our for any developers that decide they need search stuff um but yeah so people could string together whatever they they want um and then basically they can developers can get direction from from users um and so like you don't you don't take solutions from users right you take problems from from users and then you you try to figure out a solution and then we have some like product people who like interface like oh here's what the users seem to are having problems with how do we solve this problem and like ai people are like ah there you go and it's uh yeah and but so so search wise right now we don't have anyone so we're pretty young right oogway launched its discord maybe like two weeks ago it's just kind of this decentralized uh community building this stuff together and um and yeah so it's basically open where anyone can see like what's going on and stuff i even thought about putting a search on top of discord to better help people understand like uh what you know so they come in they're like what what's going on here who who uh who do i need to talk to about x and then that's where like search is is relevant for putting that kind of stuff on top of discord but we're young we haven't built that stuff yet um but yeah everything's like open source it's supposed to be people can um help they can use tools from open source and then they can also push back out the other way where like we have a uh say we have a thing that makes hugging fake or uh haystack easier to use then we would have that open source for anyone to use yeah so let me quickly uh point on two things and then i want to get into the um this decentralized startup idea so the first of which is um so we we interviewed um uh malte peach from uh haystack on the fourth we've a podcast i remember what number it was but um so they have the deep set cloud gui did you check out that with respect to setting up the apis with haystack yeah so i think they were in like beta and i looked at their beta and it was like it didn't like i wanted to not just do uh i think they were doing like search plus question and answering and i wanted to just do search or something it's basically something i could do with their python library i couldn't do in their um thing at the time and then when we asked them about scale like how many documents they could handle it was basically like they they weren't planning to handle a large amount of like over 10 million documents anytime super soon so we had to host it ourselves and stuff to handle like more documents and and uh yeah but but you know maybe in a year then they'll they'll have solved these issues right it's just kind of like a timing thing that they might be too young yeah and i think um trying to help people understand the combination between we've eat and haystack and gina ai and how these different things plug in together i think that's one of the core goals of this podcast and this content is to try to figure out how these things fit together and so i yes i think if you just want to use search and then you want to do further python processing i think we v8 alone might be what you need because it has that kind of python client where you can still just do the get the near the nearest neighbor kind of filter and then further process it with that so the second thing before going to uh decentralized startups is um so we've partnered with um katie they've recently uh integrated a question answering system into our slack so similar to what you're describing with discord where you have a question answering in slack and yeah this idea to me is so exciting uh like duplicate question detection i know in our weevie it's like i'm probably asking questions that just make me look like a rookie because i don't you know i i i can't find the exact question when i do like the current slack search and so this idea of trying to get rid of asking duplicate questions alone i i think like just the burden on open source software development and getting new people i think solving that alone is huge but i love that idea of adding question answering to uh you know discord slack community group chat so so now let's let's talk a little more about decentralized startups because i'm really curious about your um thinking around this and like um you know like this tokens and what's daos i don't understand any of this i'm just kind of curious about how you're thinking about this kind of idea yeah well okay so on the whole like crypto stuff and dao stuff uh i know a lot of people will be like uh groves um so i'm i'm not like dogmatic religio like religious about that stuff it's just that we had a problem that we were running into uh namely that uh the non-zero developer time problem per decision where the solution was um some crypto stuff right where we can essentially put in a program comp like equity compensation and and you can make it so that that program is not gonna you can trust that it's not gonna change a bunch in the future like it's a common pattern with uh some some past startups some tech tech companies where in the beginning it's like hey everything's free everything's great and then later on they're like well hold on we have all these shareholders that we need to maximize profits to the shareholders are not necessarily the users um we need to extract as much value um out of you know for like companies where the business models are ads the the the users are the ones um like their attention they're essentially selling their their attention right to the advertising uh the ad company the ones that are running ads um and so you know they're trying to extract that value and and so when you you can make it so that um you don't have to worry about the organization like changing like that in the future where um where it goes against like users say one by you can you can make these things hard to change right so like when you think about setting up governments like the united states for example they tried to make it so that it's hard to change right like the president um especially at the beginning was not supposed to uh have too much power nowadays people might think they have more more power but they still don't it's not like they're a dictator in terms of the amount of power they they have right like the in the beginning the u.s i think there's some story about where um i think george washington would answer the door to the white house himself and there wasn't like someone else to answer it was just him just to give an idea for that stuff but uh let's see you can put checks and balances in companies as well or or future organizations and then you you can set up incentives such as like who are the people who are making uh decisions who are the like the shareholders in modern companies it's like well that's just whoever like has um uh decision making rights right so like uh equity plus this like governance stuff is i think how you commonly see it allocated in modern companies but you you can get those governance uh rights so just for making decisions organization and just give them to whoever right so if you want it to be that one-third one-third of it's allocated to users then there you go and then that's very different incentives from companies where their users are only like 0.1 of the shareholders or whatever right especially because the users are probably not don't have very much money so so they uh it's usually people who have more money are able to control more of in the shareholding votes right but um yeah so in short um we can make it hard to change uh like you can put out a smart contract right that encodes the equity compensation and then you can make it so that when there are changes um it's it's the the incentives are aligned it's not just um people who want to maximize profits but it's also people who are actually using the product so if the people using the product don't wanna they're not in favor of the super big like extracting tons of value out of or out of them then then that can you can set up incentives that way yeah i can definitely understand that the idea of the users adding value to the to the product or even like even aside from ai just like network effects from things like you know twitter linkedin facebook youtube like just the general idea that you create the content you're what's making the platform valuable and that idea shared ownership and i think with ai systems it's going to be even more like even more on the nose about that where you're creating the data that used to train the model to replace you almost in in your role of the network but like so just kind of this idea of organizing companies this way but when you have like a decision like a vote is to be made like who poses a decision worth voting on is it like a reddit forum where you say like i propose this the acquisition of this company and then it's upvoted to the top so now we're gonna like vote on this acquisition deal yeah well so i think so people have heard of dows a little bit um the essential it's supposed to stand for decentralized autonomous organization it's that's it's not a very accurate name right for a lot of um a lot of them but um a lot of the ways they have been deciding this stuff is they have uh it's it looks just like regular public companies to me like where it's the amount of shares you have that you that you bought right so it's proportional to your the amount of money you have is how much voting power you have um yeah i mean i this or or then the other one is one person one vote which looks like a direct democracy so one is like uh i figure what the word is where it's like wealth um like a plutocracy maybe where it's like your your voting power is weighted by your wealth versus direct democracy or both the models that people are using i mean um this might sound bad to some people but i think both of those are are clearly uh wrong like especially like direct democracy i i know is like a popular thing but it's um that's a good way to get mediocre decisions i would say so so you know there are pareto distributions and people's competences and um competition is a different thing so for a given decision there might be like one or two or three you know some small number of people who are like way they're gonna make a way better decision than anyone else and and so this is part of the company 2.0 stuff is uh we're we're not we haven't done this we haven't succeeded at this yet but we're trying to set up a system where you're able to select out the most competent people for a given decision and you know it's probably not going to be the founders for all the decisions um and and then you can let them make that that decision like potentially in a small group and uh the pareto distribution premise is kind of like a big big thing there and why do you want that it's like well if everyone um has some equity in oogway the the better decisions uboys makes the the more value way will create and the more valuable you'll be so it actually helps everyone for um all like people as part of oogway for the the best people for a given decision to make that that decision and uh yeah so this is a this is a thing that we're working on now it's it's very hard problem but but uh i think there's new tools coming out that make it easier than it was in the past like modern companies look much more like autocracies i guess where or like the ceo but then i guess there's a board right who can who who uh controls stuff so that's more like allegory oligarchy like yeah i'm really excited about like um also just like ai education tools and even even it doesn't have to be like super ai even just like youtube videos that are distributing more information to kind of supplement the democracy one person one vote rather than the concentration around the experts who make all the decisions i think having just better education tools whether it's like an interactive kind of quizlet style thing where you ask them questions and they you know this kind of whole system and then yeah like you earn tokens to kind of weight your vote further by doing the education i think could be an interesting idea and that kind of thing i'm just like i've never thought about this before but i'm thinking but so one other thing with this that i want to talk about is like what about like the laws around accredited investors do you think that's just like is that a law that's trying to protect people from you know foregoing their salary and then taking these tokens that might not be worth anything or is it something that you think is maybe preventing people from participating in startups say yeah well so uh a credit just for people who don't know a credit investor you know basically means you have one million dollars in in a net worth not including your home or yeah or it's something like you make you've made two hundred thousand dollars uh in salary for the past like three years or something like that right it's all about um how much money you have or or make they're they're adding some stuff about like weird tests you can take um that look like like certifications that traders get and stuff to become accredited but that's really not how most people are going to be like most normal people are going to become accredited so it's really about wealth and the idea from the sec so that they're the ones who regulate whether you can buy ownership in a company um before it's on the public market so when it's private so they they say it's about investor protection they're protecting people from making bad investments um it's a little bit weird that you can't become accredited from some more like knowledge based thing then if that's the case but so that's it and and there's yeah but if you want to be compliant with the sec nowadays there are exemptions that you can use so if you like when people when companies raise venture capital they're often using the regulation d 506 c or b exemption and um that allows you to raise money from from a venture capitalist basically and the angel investors and stuff who are accredited investors but if you want to raise money from non-accredited investors like just regular people you can do that under regulation cf for crowdfunding is what it's called you can raise up to like five million dollars and stuff but there's there's other rules that make things like rules around advertising not being able to publicly advertise stuff that make things difficult so that's why a lot of crypto people end up just ignoring all of the rules and uh just doing their own thing and they'll become like anonymous so that their the government won't know who they are i guess i i don't think it's that hard to deal anonymize people but uh that but they think they're anonymous at least yeah i'm obviously not anonymous so we've talked about a lot of topics from causal inference and deep learning to oogway ai the decision making layer orchestrating all these different ai as a service apis and talking about decentralized startups and thinking about how we're going to how the future of company 2.0 and organizing companies is going to be with the advancement of these ai tools and i think of ai tools particularly i think nlp tools are going to have a massive impact on the way that we organize these companies things like you know question answering systems in our slack chats and just the way that we can trade information with each other is also interesting so just kind of one last question i'm curious about like um your information diet and kind of how you're organizing your time between are you you know like running a startup are you mostly putting out fires and building new stuff or like how do you wait doing that and then say like knowledge discovery reading papers and doing the kind of scientific experiments what's kind of like your strategy for kind of continuing to develop your knowledge and build this tool um yeah so i guess it's i try to figure out what are the most important things for me to spend time on i make sure to spend time on those things like recently i had to learn a lot about legal stuff um but also just my personality i'm uh i need to be learning about just like random stuff so uh i'll often just be learning about random stuff that it's like not obvious that i should be spending time learning about that at that time um yeah so i think that's that's it i do you know i ai stuff it's like i read it if i need to be reading it or um or if it just like has randomly caught my attention i guess is what i'm looking at ai stuff so how often do um like when say the new paper tweets that are coming out so frequently how how does that kind of like and i have that same kind of tendency of just like being curiosity driven and you feel like you need to read it just because it's gotten your attention how do you kind of like put the blinders on that kind of stuff and stay focused about getting too caught up in all the latest headlines that come out seemingly every day yeah well okay so when i guess when you said the phrase putting on fires when there's fires it's not so hard to have blinders on i think um yeah some chemical thing in your brain makes makes it that way and then when there's non-fires i don't know i'm i'm um like i am a i'm a fan of kind of like maximizing motivation and uh it's a little bit radical but like like i think that discipline is maybe a bit overrated and that motive motivations under natural curiosity is underrated so i just kind of i'm more just go with the flow when uh when's when i want to be looking at stuff but uh but yeah i mean you you need i think different like a diversity of people on that right you i think you need people who are more like they're really good at focusing and it's hard to distract like you can't distract them and then people who are more um all over the place i i think that you need a a uh diversity in in organization of those kinds of people awesome yeah so thank you so much brady for coming on the wvva podcast loved hearing about oogway ai it sounds like such an exciting project and hopefully you'll come back on the podcast to discuss the update of it in a few months so thanks again for coming on the podcast nice connor thanks for uh thanks for taking the time ", "type": "Video", "name": "weaviate_podcast_8__brady_neal_about_causal_inference_in_vector_search", "path": "", "link": "https://www.youtube.com/watch?v=t7g9s1GWcB8", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}