{"text": "New hybrid search ranking algorithm: https://weaviate.io/blog/weaviate-1-20-release#new-hybrid-search-ranking-algorithm ... \nforeign [Music] this is super exciting we finally hit double digits we I have a great crew here with me so I've got Derek Zen JP and Philip so Zen and JP you already know Dirk and Philip they're new to Vivid air so it's super great to have you guys with us and this is super exciting um so yes as always thank you for watching and listening it's always great to see you all keep coming back for more and if you like this kind of content yes please subscribe and then then you always get a notification and not just for vivid air but we do loads of really cool stuff with podcasts and various fun videos and we also have some more new ideas around tutorials Etc so it's definitely worth subscribing to our Channel and of course one of the best things about this kind of content and videos that we are creating it is live therefore you can ask questions you know so don't feel don't be shy feel free to ask questions always as we go and we'll try to help them out but also if you don't have questions but you just want to say hi and tell us where you are watching us from uh then this is also a good opportunity to introduce yourself to the rest of the community community right so today we have a few really cool topics to cover so first off we will be talking about multi-modal models and applications so JP sorry Zen we'll talk about that uh the next one we have AutoCAD feature so that's something that is brand new that was added to viviate it with like the 120 release so JP and Derek will talk about this um then we have new hybrid search ranking algorithm and that's also like something that GP and Doug will continue talking about then we we talk about PQ and risk scoring this is actually a pretty cool topic that Zen will cover so if you're interested in like various types of maybe indexing and the kind of clever stuff that we do behind the scenes to make within more performant and you know more memory Savvy even or you know to get the best out of your vector database this is a really cool topic and then last but not least we'll have like a really cool demo by JP around like the knowledge base app so this should be really really cool and one more reminder you know please subscribe you know like if you arrive late you know make sure to subscribe so uh and basically let's have fun during this session so um let's kick it off with Zen and Zen okay what can you tell us about the multimodal multi-modal models and associations so let me let me kind of justify why I wanted to talk about this uh in this episode first um I've been on this hype train of multimodal models recently I've been reading all the papers in this field um I've been looking at implementations for uh Summer State some of the state of the art multimodal models from from meta from Carnegie Mellon and I wrote about this in the previous blog and I wanted to talk about this as far as how how this is going to impact uh what we're releasing would leviate going forward um so let me step back and let's talk about what multimodal models are right um the only multimodal model that we have right now in weaviate is the clip model and it's multimodal because it understands multiple modalities of data right you can pass in text and it will embed it into vectors you can pass it images and it's going to embed it into the same Vector space and the whole point of multimodality is that you can actually understand and search data across modality so you can give it an image and and then obtain images or text you can give it a text query and you can obtain text or images back so you can do this cross modal search um and so this really excited me and I started exploring new papers around this and now what people are trying to do is uh go go beyond just text and images and the motivation here is of course as humans we use all of our modalities to understand a data point where if you go to the park and you look at uh and you see a dog passing by you can see them not just in images but the the time series of them moving you can smell them if they're close by enough you can see uh you can hear them there's all these modalities that go in and into our brain and that's how we understand a dog and the power of this is that every modality is adding a specific amount of information another example of this is I was recently on the plane um flying to Seattle last month and I didn't have uh these um though the wired headphones I had the Bluetooth headphones so I watched the entire movie without listening to it and I got pretty much all of it so humans are very good multimodal reasoning engines where I can show you a movie without giving you the audio behind it and you'd still understand pretty much all of it because we can fill in the blanks of the missing modality um and so the main thinking behind this is if we want powerful reasoning engines which these large language models are we need for them to understand other modalities of data right now they only understand language and some of them understand uh image so like the new Bard model understands images um but in the future what we'd like for them to do is understand audio understand moving time frame so video understand maybe even motion so if you look at your phone there's a bunch of accelerometers and gyroscopes in here uh what if you could give the data from Those sensors to a multimodal model and then get it to understand what type of motion the human is performing running walking all sorts of interesting stuff so um I went down this rabbit hole and then we started reading this paper called image bind from meta and we're in the process of uh implementing this into into Eva and there's going to be more information soon um but the main idea behind this now is that you can take any type of data embed it into Eva and the vector space is going to represent all sorts of modalities now um and then you can do cross-modal search so you could pass in you could literally talk to your database before you could write in text and you would get relevant answers back but now you could record yourself and your query could be an audio recording your query could be a video your query could be um could be motion sensors actually as well so you can take a motion sensor vectorize that data point and then retrieve back any of the relevant uh data points back hey Zen um so on the on the topic because I think there's like so many things that you could probably recognize and in terms of modalities so uh back in the days when I was doing like way more more public speaking I got this gadget so it's something that you put on have you seen it yes yes I have to say so this this oh you have the same so this thing actually you can uh like feels like as your different joints and your bones move inside so if I make this gesture this gesture show you some fingers you know I'm not gonna show you you do this all that that's kind of like one of the modalities so you could if I do this you could kind of translate modality of like oh yeah this person is happy or something right if I show this like oh this person's angry so this could be yet another example like strange modality and uh yeah so yeah that that device by the way is from a company in Canada and it and it recognizes uh muscular activity uh in your arm and this is similar to what uh the the people at meta are doing with their image buying model one of the modalities that they process is accelerometer data so because there was a time where meta was super interested in these special input devices and they they were there were a lot of Buzz around these so that they take over I'm not sure if it was this startup but at least they um yeah to get a replacement for for my for a computer mouse yeah I'm not sure if they bought this one but I I know this particular device that Sebastian is wearing um yeah but they are they are looking at the new interface right the whole player on the VR they want to be the next screen um no but this could potentially be one of the modalities right if I'm moving my arm like this or like this what is the closest thing in Vector space to this movement action um yeah and I really like your your example in your blog course around this infant which is them growing up and uh getting a higher level of abstraction and then being so like like fine tuning or having a model which is then generalizing better so if it's uh if you have more data yeah the other thing that's really interesting about this is that you can do better classification if you have multiple modalities yes right exactly it was the point yeah like if if I show you a really weird image you can only inspect the weirdness or the anomaly uh component of that image through the image modality but if I show you a really weird data point through six different modalities you can you can view it from different modalities and say okay it's abnormal in these modalities but it's normally in the other modalities so you can do anomaly detection much better across modalities which is why humans um like if something seems really weird like you get a weird feeling you can't explain it it might be that all the other modalities are normal one modality is going off the charts right so the number of applications you can do with this is really amazing I'm really excited to see what people do with this and one of the main modules the clip module people love so I'm hoping they can take this new image bind module and build on top of it one more technical detail by the way so in image bind they actually start off with the clip module and they use the clip module to localize and and bring together all of the other modalities so if you have an image of a lion and the word lion in the clip module in the the clip data model those two vectors are already going to be closer together and now if you project in the sound of a lion roaring they use examples that are similar versus dissimilar to bind the the um the embedding space together across these modalities that's why it's called image bind because they're using concepts of images to bind the other modalities closer together um so hopefully the the idea is that you could still use the bind model if you're only doing image and word search and that would work just as well but if you wanted to do audio search down the road going with the bind rather than the clip model would just be a better idea now cool so you can go on I was gonna say so then from uh like from a weeviate user perspective you can now is it correct to say that you can now kind of um embed information or like vectorize information in any of the N modalities this model would support like it kind of up to end uh different modalities in your data point right yeah if you if you have the modality if you have the data point for that modality which is one of the main challenges of the space Philip go ahead you were saying um but it's limited to 10 modalities are uh no this one has six modalities so it has image audio it has accelerometer it has depth and it has uh video and it's got heat radar so um okay because the the high modality multi-transformer that hits yeah the high mmt from Carnegie Mellon that has 10 different modalities that one is super interesting as well the difference between these two is that this model uh Bill the the bind model built a model per modality so you have a model that understands Vision a model that understands audio and then they bind them together the high mmt one is one model that they can keep on adding modalities to and the more modalities you add it looks at what information what incremental information you're giving to the model and it says okay this is a useful modality because now I know something that I didn't from the other mod the mod modalities that I had and so that builds one model up and it's it's a lot more scalable cool so it's like a Voltron of older models nice exactly the more the more modalities you give it the better it becomes and you can give it all sorts of modalities they they went into like structured document understanding that that model can understand like a word uh documents as well so what are the use cases that you think we could do with like maybe database because like the thing I I can think of is like hey if I left if I were to vectorize maybe a video and then I could do either search based on what I can see in the images just like you were saying you could watch a movie without even like putting headphones on or I could just search based on audio right so let's say I search for a fight scene and then you know maybe you could see it or maybe there's just a fight that you can overhear right so yeah so it's kind of interesting but what will be other examples you would you want to use it for I think probably the most relevant application would be performing searches let's say you have a data point that that you have three different modalities for right you have text for a modality you have video for a modality as well as the audio for that modality which if you take any YouTube video you have all these three modalities you have the transcription you have the video itself as well as the frames and you have the the sound recording for this you could do you could vectorize all three of the these data points as separate queries and then you could do more refined search before you could say okay show me anything that looks like this image now you could say I have this image or video represented as three different modalities show me a more accurate representation of what this uh what this is in multimodal space um I I think that would be one really cool application um the other application could also be if you only have one modality you can look for other modalities uh as output from your vector database if you have a large enough data set um this was one of the challenges that they talked about in the paper where it's very difficult to find Rich multimodal data sets How likely is it that you have the picture of a lion the sound of a lion and then the motion of a line in accelerometer data it's quite rare So training this thing was a bit of a nightmare I think they used seven or eight different data sets that they stitched together to even train this thing to begin with because I think that the the use case in this case wouldn't necessarily even have to be where I have a video and I abstract three modalities from it what you could do is like have a collection where you just throw all my videos all my mp3s and and all my pictures on all my PDFs and then I can you know search through all of them because that's like the ml model can understand all of them so I don't have to have five different collections I could actually just lump it in it's like just search to my unstructured data right like it's all there yeah exactly cool did I hijack you did I hijack your train of thoughts oh no I'm good uh any other questions uh we can take those when can we have it ah that took it we're aiming for the we're aiming for for the next release but no promises yet right uh we're just going through finalizing it testing it um well there will be another blog post around all the cool things that you can do with it um all sorts of multimodal searches I'm really excited about that but um yeah the next release is the we're aiming for that but uh so it sounds like if someone mashed the Subscribe button on the YouTube channel they'll probably get some notifications about videos or content about it in the future that will for sure happen and if you do subscribe you might uh it might come out earlier no promises but that might just happen it's a good excuse to uh give another call out um and then while before we switch topics we have somebody saying hi from Colombia which is uh code ego so that's good to see people always saying hi you know like we hope you enjoy it if you have any questions uh drop us a question or just you know keep saying hi or send us encouragement so that we keep going thank you very cool audience from all over the place I know I know it's it's nice to see people from everywhere perfect in this case might be good more a good moment to go to AutoCAD feature uh so I will leave you Dirk and JP to handle it cool I mean um the last release was so jam-packed with features right so we talked about it uh last podcast with Etienne and he's one of being on Connor's podcast talk about it as well but yeah we didn't have time to cover some of the other features that would be a good time and one of them was um autocut and we've got Derek here from the core team is that the right yeah group term and uh yeah so Derek and I work together to to write about it um so I thought I'd lean on Dirk to uh sort of the hard questions like hey what is AutoCAD um can you just tell us a little bit like at a very high level Dirk about what it is uh yeah so you do a search you get results back and then um they often get bad after a while so we have one two three good results and then the bunch that don't have good scores that are not really relevant to your search and you don't really want to show them to your users and um obviously you could you could play around with the limits to try to only include the ones that you really want to show and not include the irrelevant ones but it's not really possible to set the limit to a value that works everywhere so what AutoCAD is doing is it depends jumps in the score so you have I say let's say one 0.9 0.85 and then the scores go down to 0.5 0.4 so on and then you do a cut at the jump in this course and throw away the rest of the results and only for the good ones and you can set how many of the jumps you want to do and um important who those that you want so maybe for your use case you want to go for the second jump or the third to get everything that might be relevant or you want to really do a hard cut off and only show the best ones and then you use one yeah I think it's really cool because like obviously uh you know Vector search working on similarity you have to specify some some sort of threshold to get results back whether it's you know just hey get me 10 results or results within this distance from my query and that's a little bit painful sometimes because you have to uh figure out what your threshold is and what I really find interesting about AutoCAD is that you know you create this natural groupings too as well as like thresholds you can go hey give me two like you said groups of results that are like could end up being not necessarily close together but you know um they're gonna become natural groupings of this first group is within this threshold and then kind of get this outer ring of results and then so on as you go it kind of like this nice mapping of vector distances so yeah I think I think it's really cool and hopefully uh our users kind of find it useful as well because for me when I you know do demos and people are like oh I see you're getting eight results back or three results back why did you do that and I go because that's how many feet on my screen is usually the answer for demos but it's really hard question to answer when people say things like oh what threshold should I use if they're using a distance number and it's like how long is a piece of string whereas I think AutoCAD really lends to kind of natural groupings of these these results so that's really cool um and and you kind of need uh result sets that lead to these groupings right like you kind of need some meaningful distances from results which is why it works well with Vector search yes basically you take the distance we get from the vector search or bm25 search and that's the input to Auto AutoCAD we don't use anything else um and it's really simple inside so it basically it's a very simple way of calculating derivative and it uses that to to detect the jumps and then cuts off when other people are talking about using some complex models to do that and it probably works better for some cases but it's a very very cheap way to get useful results and obviously can always be improved later yeah is the idea then that if I run a search and I get like a couple of great ones and then I have like a bunch of like good ones then the good one gets cut out because you know they're not as good as the great ones but if I only get good ones then the good ones will be shown because there's nothing better right exactly so it doesn't look at any absolute values it only looks at it's a relative thing right sorry again it's a relative thing so you can Define I want to the first or the best ones and then there's maybe some some space between them and then I went also the second ranking and the third ranking and I can do a config thing that I want to First pairs or the first two pairs exactly that's how it works yeah I mentioned that it doesn't know how good your scores are so you get some scores back and you don't know if a 0.8 is great or not great so it can't do any absolute ranking you can only look at the relative distances between the different objects and then group them by by using these jumps in this course and then cuts it off depending on how you how you configure it it's sort of like when you go for a holiday and you're looking for a hotel and then sometimes you if you book in early there's like loads of great options so you don't want like the crappy hotels to pop up but when you're like booking like on the day when you're flying you go like okay at least show me the average ones don't cut them out but don't show like the ones where you get murdered I'm also taking this tent and the highway yeah yeah so like Vegas can be chooses right yeah I guess yeah because when we were talking about what image to put in the blog I said um let's fill out this really obviously the horrible sketch uh that it was like wave yet as a robot grabbing you and like some sets of results so it's like oh it's basically telling we've yet hey get me one set of results or two sets of results uh how good are they don't know whether it just goes and grabs them for you as it's convenient so yeah the hotel in that which is quite funny uh quite good someone sounds like someone who's been on holiday recently yeah but also very specific to to your role Sebastian or like looking for a hotel for the same day no no I'm actually a pretty good planner So like um it only happened once in my life when uh when I had to book a hotel on uh on the same day but that's a not a story live on the screen and um uh I think that kind of leads in well with our next discussion which was going to be I have one more question this is super important so every time I learned about a new feature the first thing that I think of is how how could I break it or try to understand it it's a troublemaker's end so um you talked about using the derivative um is there a minimum threshold to the derivative let's say I ask for 10 answers back and they're all great so the derivative is not going to be very high right it's going to be a relatively flat derivative and I ask for one cut it will it just make an arbitrary cut somewhere or how do we handle that no um basically if your scores are very linear and there is no no jump in them then it will not cut off anything it could also be if your results are all bad [Music] okay so there is some some minimum threshold for the derivative needs to be that high before the cut to happen not an absolute threshold it kind of it detects that the that the derivative changes okay um I think it's called the knee in English I'm actually not 100 sure but it's a very very fast but kind of crude way of calculating it um but it basically if if you don't have jumps that are large in your data set it doesn't really do anything because if you can't really group them if they're on a line or something okay it doesn't make sense to to to to say this is the first group this is the second group okay makes sense awesome JP now you can do fine oh thanks a question um yes no that's a really good question um yeah and then to that point right about about um how those groupings are derived that's led to a change in the um I always get this term wrong so hybrid Fusion ranking algorithm did I get that right deck so close Philip you want to say something um nope I just want to ask because of fusion the word Fusion but I'm pretty sure you elaborate on it so relatively confusion yeah um do you just want to share my screen Sebastian there you go so this is a picture from the blog post and this is how hybrid search works so hybrid search be doing a vector search and a bm25 search and each of them have their own scores and now we want to combine the results from both different searches problem is the scores are completely different things and you can't just add them up it just doesn't make any sense because they measure different things a vector search you get distances and for bm25 search you get something out of a formula which method like the frequency of your term and how many documents you have and the average lengths and a bunch of other things that go in there so you can't just add them up and um the methods we used before to add results up just look on the rank of each result in the in the individual search and then use the rank to fuse the results that's why it's called Fusion and the old algorithm was called ranked fusion and just to kind of show how it how it works this is a very simple notebook very hacked together so don't look at my code too much but um what what rank Fusion does it looks at the rank in your search results and then assigns the score to it so the first results in your this is the actual formula for it so the first results in your search gets the score one divided by 0.60 plus one which happens to be 0.1 s0.01 blah blah blah blah doesn't really matter the second one gets a bit less the third one gets a bit less it doesn't matter what the scores actually are only the rank counts and you do this for your for your two searches and then you just add the numbers up and that's your final score you get in the output and just to show you the what that means so here are two different input scores you know here in the first one you have a really good one another really good one and then a better one when the second one you have a really good one and then two bad ones right pizza crap and these ones are really good but both lead to the same results with ranked Fusion because this is the first rank this is the second rank it's the third Rank and nothing else matters yeah I I printed out below what the what the ranked Fusion scores would be for these two scores and it's exactly the same and um now to compare the old one the old way of using things with the new one I um made a small example here but before I go into the I just want to explain how how I do it now or maybe first the problem so the problem is with this you get a very regular scores out of your Fusion algorithm and if you have very regular scores you don't have any jumps in your scores and so AutoCAD doesn't work that is what we just discussed this is it's not exactly linear but it's very close to being linear and um so the AutoCAD algorithm cannot find the jump in these scores and so it will mostly give everything back and maybe cut something off at the end that's that's a really low low rank in both searches and um now what new algorithm is doing is it's normalizes the scores between zero and one so you get your scores from vm25 your scores from from the vector search and you just say the higher score is one the lowest one is zero and everything will be normalized in between according to the relative distances between them so if we scroll down to these these examples so here the the higher score is 500 the lowest one is zero to zero nine so this one will become one this one will become zero and the other one's relative to that so this will be a really low number and so on for the vector search this will become a one this will become a zero and because this is so close the maximum score this will become like a 0.9 whatever it doesn't really matter in detail and what you have with that is the the relative distances in your scores from the individual search other insurance course yeah Frank Fusion you only have to rank in the final score everything else get lost so you don't know if a result was almost as good as the first one or almost as bad as the worst one you only look at the rank and with the new one you have like the relative um yeah the relative distribution of the scores also in your final result what's important is what you don't have you still don't know if 500 here is a good one you know this could be your best score you get but it could still be really bad you don't know if 500 is a good score and a bad score um and now we can kind of with this made up example so the first document of 500 second one for the six and so on and Vector score is also second document as a six false documentation 5.98 I just want to show the outcome for both algorithm so here we do it for ranked Fusion so we just look at the rank of each document we sum up the score for the given Rank and this is what you get in the end you see the second one is the best one and then you have the other ones very close together and only the one that was worse in all in both searches at the end but if you if you look at the input the first one was so much better in the keyword search than the other ones and here it's it's on the first rank yeah the fourth rank but the the scores are almost the same so it's still a really good result here but you only get it on second place now if you go to the relative scope Fusion the one is the first result right because it gets a really high score from the vector search and from the keyword search and so if you sum it up you get a really high score together so it's the first result you get back from the vector search then you have this group here that has almost the same score and here is the one that was bad in all in both searches and if you would run AutoCAD here you would get a cut here and one here and you get this groups of very good results good results pretty bad results so I guess it retains a lot more of that information in the scores right like before all that was kind of like getting lost because all we kept were the rankings and what we ended up getting in the hybrid score was either you know I kind of found that if it scored well in one of the other searches you tend to come up to the top whereas here the how far you are I mean because it's still relative so the top results still tend to do really well but it was if you're the same if your second result and you're very far away from that top result you don't benefit from that as much yeah exactly yeah I think the the this jump in the scores is probably the best way to see it so I could change that to five and you wouldn't see any change here sorry I need to rerun everything um it looks exactly the same as before right I this this discourse now much lower but the the right Fusion is still exactly the same whereas whatever do the same one here you suddenly get a completely different result because now the this score is almost the same as those two but again now you get all of those first groupings or results probably in your in your uh one group autocut yeah exactly yeah that's really neat um so what are the two ranked Fusion math no hybrid fuchsia method it's called again one is called rank Fusion in what is called relative score of fusion and you can set it with the python client as a when the hybrid search there's an argument Fusion type and there's just an enum you can import and then set it to relative score or to ranked and that's how it works and just for graphql it's the same Fusion type and then you put in one of the two options in um rank Fusion is the default so the old one we might change in the future and you'll be very happy to hear if your results get better like what we tested got better with the new Fusion algorithm but it would be great to hear from all people yeah I saw that when we ran some benchmarks internally there was some you know like much improved recall results with the hybrid search it was really cool and um yeah it's obviously it doesn't like it's still compatible with not setting the result because of the defaults if you don't change anything it should still behave as it did before so that's obviously neat as well for people in the I have a question in the example here in the Jupiter notebook the um the alpha is 0.5 so both the um the dents and the sparse search are equally weighted in this example correct um if I if I change that waiting does that does the re-weighting uh work better with this relative search as opposed to the old research we had I think it's more exactly the same so it basically would have a 0.5 or something or 0.7 here and here and for the relative year and here but the the base of it is exactly the same you still only look at the ranked for rank fusion and you still look at the relative scores for the for the new one okay and then is there any reason why I would not want to use this versus the older algorithm well I guess we just wanted to get some feedback for now everything that we have tested is better but you never know right so it would be creative if people say yes it's better for us and then we switch it over and on for everyone Zayn is just creeping its way to how to break it yeah I'm just thinking about this just seems like a an overall better thing it retains more information like it just seems like all good I'm wondering if there's anything bad about this but apparently so like you said the testing we've done it doesn't it doesn't seem that way but this is why the community can use it and and tell us and we'll go from there I guess exactly sometimes there's stuff you haven't thought of so it's always here all the time all the time cool thanks dick um yeah I think that was that was really cool and it was cool to for me to sort of be like you know writing up about it and and yeah everything kind of you said as you're showing me this stuff was like oh this is like x amount better and this is my better so uh everyone everything seemed really positive when you guys were developing it and and implementing it so obviously if anyone has experiences would love to hear about it um because I guess one of the questions I had for you Dirk was set the same as Zen's like when would you use one algorithm versus the other and at the moment it kind of seems like generally we're kind of keeping the old one for legacy reasons as default but maybe we'll think about moving towards this in the future is that right yeah if we get good feedback and nobody complains that that scores the results are much worse than we will switch it over sounds good cool thanks very much so how can I start using it what do I need to do you just if you use the python client that's what you add ah and like just go to the to the documentation check for Fusion ranking methods and then there are examples for Python typescripts and craft UL oh it's missing here but it's already implemented so it's also what the example yeah um and is there a way to make it a default uh even outside of the query where it's like hey I like it I just I want it everywhere or do we just have to use the flag every time you just have to set it right now for each search okay sounds good so let's see if Zen can break it before the next movie there oh it's good that I if I break it then we fix it and then so this type of thinking on the limits not just helps with understanding but also good product so absolutely in the past I work with this guy Frank and uh he he was the like the torment of all the engineers because whenever we release like a new product like our new version of the product he always found like this obscure example on how to break it so we always had to kind of like do uh pre-frank release and then Post front release that was pretty fun and no matter how much the QA team tested the tools like Frank always found something so uh maybe Zen you're the new Frank yeah sounds good to me cool cool um all right should we move on to PQ and risk scoring because that sounds like it's along the similar subject so uh then what is this all about and um how did we get to hear and give us a story yeah so let me start off with um I'll give a quick intro to what PQ is before I talk about the rescoring part that we added with this uh 1.20 release um so I think it was 1.8 um 1.18 when we initially released PQ which stands for product quantization um and the main motivation behind this was now when you take your unstructured data and you vectorize it all of that so the the huge vectors are stored in memory and that increases a lot of the memory load the requirements of uh what what your what your computer needs to be able to run vv8 um so product quantization basically says for all the vectors that you're storing I'm going to in memory store a compressed version of them and I'm going to have the original representation of them in disk and the way that it works is it'll take your vector and it will chunk it up into smaller pieces and for every piece let's say you have a vector that's a hundred numbers long 100 units 100 dimensional it'll chunk it up into units that are five numbers five numbers five numbers and so you've got a bunch of these kind of vector chunks now and then it trains an algorithm to say what are the what are the average representations of these of the first chunk of all of your vectors it'll take the second chunk and it'll say what is what is the average representation of these vectors um and you can increase that by saying okay I want 10 average representations for the second chunk the third chunk the fourth chunk so another way to think about it is if you if you take a huge Vector it's almost like specifying a person's location down to the XYZ coordinate everywhere so if I wanted to know exactly where Sebastian is right now I would give the XYZ coordinate and and wherever he is in Europe and then that would be my Vector representation for Sebastian but what PQ does is it gives you a more coarser representation so we can say okay so I don't really care about whether what XYZ coordinates of action is in I only want to know what his home address is or which country he's in or what continent he's in so you can keep on zooming out further but you see the problem now is the farther you zoom out the less precise of representation that you have for every Vector so with product quantization you can take an exact vector and you can say I don't really care about exactly the coordinates of this Vector I'll look at some average representations of this Vector the different chunks of this Vector um and so forth for that compression you lose accuracy you lose recall because now the vectors are not exactly precise but the advantage of this is you can save a lot on RAM you require less memory to store all of these vectors so this is one of the features that we implemented in 1.18 and we've been playing around with it users have been using this usually how this works in practice is you need a specific number of vectors already in leviate and then you can say use the vectors that are there to approximate what the distribution of vectors looks like and then any new Vector coming in can then be compressed in real time so there was a portion where I would put in a hundred thousand vectors into eviate and then I would enable product quantization and then it would go into read-only mode and it would calculate averages of these vectors and then when it came out of read-only mode the next time I put a vector in it would compress that vector and it would have the real representation of that Vector on disk but only a compressed representation in in working memory and depending on how much you compress and squeeze the vectors you can you can one tenth of the memory requirements or 1 100th the memory requirement the more you compress it obviously the more recall and accuracy you're going to lose but now that's a knob that you have control over uh if you value recall then you don't compress as much or you don't compress at all if you if you want speed then you can um then you can actually well if you want if you want less memory requirements if you have less Ram then you can crank up how much uh how much uh you're compressing and you'll be able to store more vectors so that's the story of everything that happened till now and one of the problems with the previous implementation was that we realized that there was this trade-off the more you compressed uh the the more uh loss of recall you would have because the the vectors are not more accurate anymore if I tell you that Sebastian is in this continent it's harder for you to locate exactly where he is right and if JP is also in the same continent uh if then you can't differentiate between them that was the main problem so that was the story up until 1.20 if you share your screen if you share my screen now Sebastian I can talk about the modifications that we've made to improve uh improve PQ okay so there's a couple of improvements that we've made here the first Improvement uh that we've made oh wait before I talk about the improvements any questions about PQ any any thoughts there guys nope okay this sounds good to me so far yeah okay so one of the main changes that we made here was well we did some uh memory management to make sure that you can query uh compressed vectors faster uh these are details that I'm not gonna not gonna cover you can read more about these um the first thing that I want to get to is fit time on large data sets so in my explanation I mentioned that you need to have a specific number of vectors already in review before you can enable PQ because the the average representations of vectors are learned from vectors that you already have in leviate so before we recommended that you have ten thousand a hundred thousand two hundred thousand vectors pre-loaded and then you enable PQ to learn the um the the average representations of the vectors that you can then compress at those vectors with and new vectors with um and what we realized was people would put in a lot of vectors and then they would enable PQ and their and their V8 instance would go into read-only mode for a really long time so now what we've done is we've added this hyper parameter the training limit which says you can set this limit but it defaults to a hundred thousand meaning that if you have a million or 10 million vectors already in vva and you enable PQ not all 10 million will be used to fit the PQ algorithm only a hundred thousand will be used so the the time that you have to have the vva instance in read-only mode is only going to be capped by however long it takes to fit it on a hundred thousand data points 100 000 vectors so this is a a pretty good kind of quality of life Improvement um the the only I guess 100 000 vectors is enough to understand the underlying distribution of the vector segments so this is good enough if you want more then you can modify this parameter to a million and then or a half a million and then it'll use a half a million vectors to learn the PQ compression algorithm then others hundred thousand vectors like randomly sample that they like the first hundred thousand how does that work I believe they're randomly sampled but I would have to talk to some of the people that implemented it to know but it would make sense that they're randomly sampled and because we're only trying to understand the underlying distribution of the vectors it makes sense to just randomly sample them yeah 100 000 versus a random hundred thousand because yeah so this is so does this goes back to the question of sampling bias let's say you let's say the order in which you pass the data in was really weird right let's say you're passing in images and you're vectorizing the images and for some reason let's say you have a grocery store and the first 100 000 objects are all images of products from the uh vegetable aisle right or the fresh produce aisle and the next 100 000 are uh our images from the serial aisle or the milk aisle now your hundred thousand objects are going to be all uh fresh produce and so the Learned PQ representations are going to be biased towards vegetables so it if you take the first hundred thousand your your PQ algorithm is going to be all off as opposed to if you shuffle them and you pass the data in and then you randomly sample from that data um then in that case you'll get maybe I don't know 10 000 vegetable uh vectors ten thousand milk vectors ten thousand serial vectors and in that case you have a better approximation of what is found in in your vector space Oh yeah that makes sense yeah cool cool yeah and what what is involving that learning is it to figure out which maybe parts of like which dimensions are like the biggest differentiators between different vectors so if there's like a few fields that are always like between 1 and 0 and 98 but there's some others that jump a lot like what's there yeah so the main idea behind learning the uh PQ compression it's called a code book because anytime you give me a new Vector I can I look through the codebook to learn what is the vector uh what is the compressed representation of this and so the training essentially takes the vector cuts it up into a predetermined size and then it says I take all of the first segments of all of my 100 000 vectors here and then I learn what are some good average representations of these so for example if if we're on this call and we say I want to learn about I want to represent people not through their XYZ coordinates but the continents that they live in right or the countries that they live in rather um if you only have two countries that you can use it'll take the most numerous country and it will say okay this is a location for for an average location for this person right um so that's what you're learning approximately okay uh 100 000 works well because it's uh it's enough to learn the average distribution um if you and you can also uh parameterize over how many approximations per segment you want to learn so for example if I say I want to learn the average country of this call and you say I only want one country then that average country is going to be somewhere in between all of our countries but now if I say okay you can learn three countries three average countries then we'll get I don't know Germany will get Canada and we'll get uh somewhere in Europe which will be the average of where JP is and where Sebastian is so those averages are what you what the algorithm is retaining now I don't need to know exactly where JP is and exactly where Sebastian is I can just remember the average of where you guys are and every time a new person comes in who's close enough to JP or Sebastian I'm going to turn their Vector into the average Vector of where you guys are and then so now you only need to remember a bunch of averages as opposed to exactly the precise coordinate to the vectors and that's where all the memory saving happens cool yeah I have a question yes how to break it you can so you can break it if you don't uh allow for enough uh centroids so when you fit the algorithm it's called the k-means algorithm if you set the K means if you set K2 low then it will squish all of the vectors down to one centroid and it will lose all information um so if another answer to that is if you compress too much then all all useful information in the vectors is lost it's almost like saying all the vectors now project to one data point and you can't learn anything from that data point so and that's when whenever when I made this the decision that I want to to then yeah reduce my storage however it's possible to go back so is it useless or uh so yeah that's a good question can you uncompress I I'm not sure if you can if you can go back Derek maybe is it I know there's a way to enable PQ um isn't it why we store the full Vector on the dish yeah this is what I this is what I was about you don't have to uncompress that you have full Vector is just not in memory yeah this is this goes back to the second part of the of the rescoring component here because the main reason in 1.18 why you lost a lot of recall was because you're now uh ranking and comparing distances that are compressed um but now because we're storing the uh the exact representations of the vectors on disk and not in memory we can do this rescoring um whether you can switch back and forth I'll have to get back um maybe there's someone in the chat that can answer this um I'll have to look into that but I I think you can um but I want to talk a little bit about this rescoring component which is the main kind of punch line of the release Here the main idea behind rescoring is one second let me I'll talk about it and then I'll show you experiments of before and after so now what we do is uh rather than show you the uh rather than show you the uh retrieved vectors uh based on distance calculation so if you pass in a query vector and rather than using the compressed versions of the vectors to return the closest vectors what we do is you pass in a query Vector we use the compressed representations to learn what are the let's say 100 most closest potential vectors and then once we know the hundred closest vectors out of the millions of vectors that you have in your vector space what we do is rescore the hundred vectors we go back into disk and we say these are the hundred vectors that are the most important give me the uncompressed perfect representation of these vectors and you recalculate all the distances between those uncompressed vectors and the query vector so now what this enables is if you had distortions in the ranking of those hundred vectors due to PQ compression those are completely eliminated because now I'm re-scoring them using the original representation of the vectors and this we're doing experiments around this where before uh this is what we had in 1.18 where the more you compressed the the more the less recall you had the more Distortion you uh introduced into the system and the less likely you were to get the uh the correct Vector back so before you were paying a price uh in terms of recall for your memory uh savings but now because we have this rescoring component we're running experiments on real world data sets where if you look at this graph for example there's pretty much no difference between the compressed recall and performance versus the uncompressed performance and this is because when we narrow down on the few vectors that are important we actually rescore using the their uncompressed representation which makes sense here the only way that this would the only way that you would lose recall is if due to the Distortion you uh you weren't able to obtain the 100 best vectors and so going back to the question of how could this potentially be broken or what could go wrong if the Distortion is so high that the 100 vectors you get are not the right ones then retrieving the uh the uncompressed version of them wouldn't help that much but from the experiments that we've done so far the uncompressed versus the compressed representation um is is quite basically you're not paying a price anymore for the compression so there's no reason not to do it um there's examples of this as well I'll also show you another um a couple of graphs here where we took uh five a half a million vectors of the dbpedia data set and we vectorize them using the ada002 model so the vectors are 1 500 dimensional I believe here and again you can see the compressed and uncompressed uh performances there's no difference because of this rescoring component which is all important and giving us these results when we go up to a million vectors you can see that we the the the throughput the query per second here suffers a bit in the compressed representation um we're also running more experiments using the sphere data set so we're running um we're compressing and comparing to the uncompressed representation of a million sphere data points 10 million sphere data points so we're we're going to put all that together into a Blog and then we're going to release it so that if you're planning on using PQ you can actually look at how much data points you have what the dimensionality of those data points are and then and then what the um what the curve before compression and after compression looks like so that you can judge for yourself for your application what you want to use okay but overall this is this is all great news because if if you looked at the performance before there was a material price you were paying for uh the uh the lower uh memory but now for the most part you don't you don't pay a price so it I think the default um you can you can enable PQ and then if you see that your throughput is is lower or you're suffering on recall then for your application you might want to do a similar analysis to this where you look at uncompressed and compressed versions to see for your application if it makes sense or not so so then basically if you've got a large enough data set where memory might might start to become an issue you know that's when you should be looking at PQ right yeah exactly so let me give you let me give you a couple more specifics here so on this graph on the left hand side the if you look at memory requirements for this guy um the uncompressed uh vectors that takes about 5 000 megabytes of memory the compressed Vector takes about 500 megabytes of memory so the uh the orange line here is uh 10 the memory requirements the ram requirements of the blue line over here and the performance is almost identical um for one tenth the memory price which is which is pretty amazing um on on here the the uncompressed version is so this is a million of the dbpedia data set the uncompressed version here is uh 18 of the sorry the the compressed version is 18 memory of the uncompressed version so that's pretty impressive because the biggest one of the biggest challenges we've seen like with Vector databases was like uh yeah spending ton of money on having more and more RAM but now even like large data sets are affordable for pretty much any business right so if before like you were saying like oh yeah like I have a billion that's going to be expensive now you may have a billion you go like actually that's okay I can do it or if you have like even 10 million he's like yeah or maybe you could even run this uh 100 million in your MacBook or something now and still be able to get a pretty good results MacBook or whatever machine you're using so I am I'm actually absolutely excited about it I've actually now I want to run it on my computer and see how far can I push it on my just local machine until like the whole thing falls apart and the great thing about this is that it's all configurable where um if you don't care about recall as much you can keep on compressing sure you'll lose recall at a point but you can uh like whatever memory requirements you have you can compress uh to it to a certain degree to fit within those requirements um if you're not compressing too much you're not paying any price if you're compressing way too much then you have to calibrate for what is the price and recall that that you're paying there right and and then um where can people find all this good info about PQ and uh improvements in 120. yeah so the the blog post is your main go-to outside of that you can go into documentation here uh here we think there's links below oh is there are there links one second okay so if we go here yeah these are the two main pages so if you want to learn more about PQ so on a conceptual level some of what we covered here you can read through this if you want to learn how to configure PQ um this is available right here right so if you go into the vector index config scroll all the way down to PQ over here this is I've zoomed in quite a bit here but so you can enable PQ over here and this is the new hyper parameter that I was talking about you can set this uh over here it defaults to a hundred thousand so you can read all about configuring as well as the concepts behind this as well and then a small plug here we're gonna we're gonna talk all about this in a in a new blog post that's coming out soon as well nice nice and all those links we also have in the description of the video or if this one is not there yet we should definitely add it after the session yeah yeah it's all in that release blog so many there's links to all the documentation Pages within each section of the blog so you can follow through after you read about the feature as to how you configure it and and the conceptual page to describe what it is at high level and so on so yeah is there is there like any performance difference in terms of how fast the queries are because if we don't have to do like the precise scanning yeah like you actually you actually pay a price in terms of throughput so as you can see here um if you have a a compressed representation you can you can run uh less queries per second because of the Distortion that the compression uh compression introduces and also because of the um the code book lookup every time you have a vector you have to look at the the code book so that takes uh some time as well so it adds a little bit of latency the compression it's almost free but not quite yeah you can also tweak the um the latency so you can play around based on the the parameters of PQ you can you can trade off memory with uh with your latency as well as recall but if you look at the hnsw parameters you can tweak those for uh latency versus latency versus uh recall as well right so there's a bit of juggling between the parameters between QP and then H and SW going on or maybe you know the savings you make on on RAM like where you know spend less money on RAM and then put more powerful CPUs or with more cores and maybe you could actually recover some of that performance just purely because you have more raw power uh to do other parts and calculations so it could be that if you look at it just purely from like Financial point of view you might end up spending similar amount of money or less and yet get like something faster because you know you move your Investments all around like you know we draw investments from ramp and more into the you know like a CPU land and you're doing well portfolio management yeah yeah an economist on on the plane you know yeah nice all right awesome that's all I had to to do with PQ and rescoring folks perfect any more questions oh one one way to break it will probably be to increase that limit right to 10 million back to and then that would oh yeah that would that would move it into read-only for a long long time because now it's going to learn the distribution over 10 million vectors um before before we had this limit if you had put in 10 million vectors in there that's what it was actually due um yeah so that's I propose we call it the read-only comma yeah that would be perfect hey um so the last but not least we have them the demo from JP so uh JP do you wanna show us your knowledge base app yeah sounds good um I'll be pretty quick so let's imagine if you can share my screen or at least my app of course a high chat map so the motivation for this is that I think twofold right one is that I run these webe workshops every so often and quick plug we've got one coming up tomorrow 4pm Bridge Standard time so if you can join us uh find the link uh maybe on the descriptions but also on our social channels um and and people like really enjoy seeing you know practical examples of what you can do with weaviate as well as like you know sort of like the simple queries that we show in introductory webinars right so that was one motivation the other is that sometimes I get a bit lazy like I recently got a PlayStation and one of the things I wanted to do was like hey what games do I get right and there's like all these great YouTube videos on like video you know game reviews and whatever else but I don't really want to watch 15 minutes of videos each time I want to like you know learn about a game or like a feature or something and I thought I would put the two things together and build this fairly simple app so um it also so what I wanted to show here is just kind of like how it's abstracted so here I'm just instantiating the app and or what it does at a very high level it creates a wavier instance and dumps all this text information that I give it into one collection right so I can give it YouTube videos I can give it like a text file so this is like some text of what kubernetes is just from the website I can add Wikipedia articles just by giving it the title so these are all kind of wrapper functions for doing things like that I can even add YouTube videos just by giving it the URL like so so this is the release podcast um that Connor and Etienne did and then I can do things with that content which is like really really cool and look I'll show this more in more detail in the workshop but uh writing these wrapper functions are really really simple so that's why I'll be showing that in the workshop so here I've got a function to summarize a particular entry so what I'm doing is summarizing the YouTube video that I imported above um I won't go through a lot of details of what's Happening under the hood but basically what it does is it gets the YouTube video downloads it gets the transcript through open Ai and then ingest it into uh into this waviate database by chunking and the chunking is fairly simple too and I can go hey what's this video about and it says it's Etienne talking about wavy at 1.2 and you can learn about all these things like memory usage shots multi-tenancy and so on uh so you know again I can get a high level review or view of this content without like you know listening to it so I can say decide from this summary what I want to do with the video going forward right or I can go or actually maybe not limited to this specific content I can go well based on everything that's in the database what can I learn about if I give it a particular topic so if I go um and this is a little bit of prompt engineering on my part under the hood I can say well maybe I want to learn about waviate but I don't know what kind of things I want to learn about So based on the content that's in the knowledge base I might learn about hybrid search um you know module system blah blah blah and in a similar way I can just give it a particular prompt and and do something with the content um and for example here I imported a video game review and this is just uh it's quite verbose the way it does things the the particular library and I can say oh what kind of kind of Gamers might this particular game appeal to and I can ask that to this video content right so it'll say this game blah blah blah might appeal to people who enjoys action adventure games and emphasis on storytelling so it's all you know stuff we've seen before in terms of retrieval uh augmented generation right so that's fairly standard but um and and just I wanted to show people just how simple it is and how easy it is to put these types of apps together using wavy8 um I have I want to write another wrapper function which I haven't done is to summarize multiple videos which I haven't done that or multiple bits of content um but yeah that's the really high level summary of what I was trying to build with this app and I've got even just a fairly simple text search wrapper so I can just search for a particular phrase and it gets me you know what I a number of chunks right so if I search for the phrase multi-tenancy it does a vector search and I get different chunks back right so I get like Source path title and so on so yeah that's the app um I'll stop raving on about it um it's really inspired by partly by my laziness and and not wanting to watch these kind of like you know YouTube videos or reviews and something that's potentially a little bit you know a little bit long um but but you know for for demo purposes as well because people really seem to enjoy um playing with the generative search module which is really powerful in terms of making your data Dynamic and interacting with it and um I think the topic of chunking seems to come a lot too right I've got this long piece of text or I've got this video which can't be fit into one object and and to be honest you probably don't want to anyway from retrieving information from it um so I wanted to show that as well so yeah that's me um and that's that's the that's the app and um it's not open sourced just yet it's on GitHub but I haven't I haven't released it um but I'll do that as well for people to check out in in the near future and if you want to see it as a bit of a preview come to the workshop tomorrow and then you can ask me questions about it and then check it out in more detail and one question or two questions yeah um is when you get a transcript from open areas there are also a timestamp included so it's possible to jump to to a specific time stamp in the video we'll transcript this so I believe you can get time stamps I haven't set that up I've seen other people write apps with timestamps I haven't done it so all I do is to get the whole transcript through whisper and chunk that up because um no sorry I get the YouTube video I chunk that up because the whisper API has a limit as to how big the file can be so I kind of make it certain it's like 15 minute segments or something um get transcripts from there and then they just get chunked based on whatever word limit that it has so it's like a fairly basic chunking mechanism I've got at the moment with a little bit of overlaps it's like a sliding window type deal um and all the bits can be swapped out so I'll be building in like other chunkers and all that stuff and we should be able to build in timestamps as well because yeah it'll be really good to be like oh I want to find out the piece a bit of this video where they talk about like the I don't know gameplay demos or like uh what where they talk about performance or whatever it is or if I want to look at atn's podcast and look through the bits where he talks about the hybrid algorithm right so something like that um that'll be really useful for sure that would be super nice to have one single point of Tools around documentation so we are filling in our docs to written docs the YouTube videos and whatever it is multimodal documentation or how to use vb8 and then yeah having this one single point of entry for for people want to learn deviate or try yeah I think that'll be cool and that was one of my other motivations was too because I want to learn about you know if I want to learn about containerization and kubernetes right I can just dump in all this information into it and start asking it questions and if I kind of get the prompt good enough and kind of careful about looking at the results hopefully it doesn't hallucinate too much right in terms of generating results from the from the retrieved text and I think that'll be really cool for sure and I haven't checked out like typing uh sorry getting the language model to produce code but I'm sure you could do similar things if you give it enough like it's possible but then yeah but um I was part of several projects doing this because one of the hackathon projects did this and also um one of the problems is that then the dependencies so the model is hallucinating the dependency or the versions of the program so one of the tricks is then to build the chain you're handing in the error error you're getting and then yeah doing the next iteration yeah so yeah the the detail around that gets gets quite tricky for sure I think the tricky part is with like maybe like false positive like what if we give somebody call a piece of code that is just not a good advice or not a good you know piece of code and then they kind of maybe learn the wrong way or they just go like oh this is all broken right and I think we've all seen it way too often um but yeah I I think this would be cool because we could combine many of the things we talked about today because if we use uh the multi-model models we then we can search across all of those different results then include Auto cut to kind of exclude maybe some of the poorer results right and then throw in some uh you know PQ and then like reduce the amount of memory and maybe you could run the vector search on all of the Wii documentation videos directly on your machine right you don't even have to have it in the cloud or somewhere right and uh I wonder if you if we could even shrink it so much that we could put it on a mobile phone right and then I could take it you know offline you know take it on the plane and stuff like that that'll be pretty awesome for sure yeah let's go guys love it um so we have like some uh good uh nice warm words from our audience so Brent uh says he liked your presentation JP um so thank you Brent uh we're really happy to have you here uh Felix also enjoyed today's sessions and uh so he he liked our presentations and uh of course uh we didn't forget about codigo um I'm not sure if that's actually your name or maybe this is just like a in a different language like a coder in go maybe I don't know I'm gonna claim ignorance right now but uh but but but also the avatars is let's go so it could be yeah but it could be like you know Sebastian go like I don't know if codigo is a is a formal name or maybe now it is right uh and anything else I was gonna say if your name is codigo you probably have to pick up Gora like you can't be like Oh I'm a Dev my name is I don't write go it just wouldn't work right I know right like so um you can set your set up your your kids for success or failure depending on what framework foreign yeah like uh yeah I have a cousin his name is as400 he's not doing well anymore this is great this is great um I think that's it I think this this was an amazing session um thank you all for uh for joining thank you for for the audience just like uh one final uh reminder so thank you for watching thank you for listening and then remember to subscribe uh because this way you uh you get a notification from YouTube whenever we do cool stuff um and if you have any questions even after this you know what to find us just come to um the wibit community uh definitely find us there we are there we're pretty active and then we are working on all sort of things but also if you build something really cool and maybe you would want to join one of those sessions um we're always open to having cool guests um or if you have idea on a topic right like hey I'd like to see more of this or that um I'm pretty sure we'll do something about the long chain the next time you come around because we've heard that this is a popular topic but maybe there's some other topics you are all interested so but yeah thank you for watching us and until the next time um and yeah have a great day hi everybody bye bye ", "type": "Video", "name": "Weaviate Air Episode 10", "path": "", "link": "https://www.youtube.com/watch?v=u5cQ9VjBoAU", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}