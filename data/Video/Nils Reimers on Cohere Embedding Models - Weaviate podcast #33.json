{"text": "Thank you so much for watching the 33rd Weaviate Podcast! This episode features one of the heroes of Deep Learning for Search ... \nthis interview is recorded shortly after publishing our integration with coheres multilingual embedding models in weeviate to try this out for yourself please check out this excellent blog post written by Sebastian woodaleck this is linked in the description of the video I really hope you enjoyed this interview with Nils rhymers I learned so much as the interviewer and I really hope you do as well as always we'd be more than happy to answer any questions or discuss any ideas you have thanks so much for watching hey everyone I'm so excited about our next wewva podcast we're hosting Nils rhymers Nils rhymers is one of the most influential scientists in this whole area of deep learning for surge he's done so much incredible pioneering work especially with tech search and I'll kind of hop into this The Narrative as I see it and then ask Knowles to kind of hop in on the origin of this kind of research but to me it kind of looked like we started with this thing where we can extract representations of data from the intermediate layers of deep learning models and then what happened is we started optimizing directly for those representations using contrastive learning loss functions and I think this was super successful in computer vision with papers like simclear Moco and then Nils and his team they started showing how you could do this with Siam these bird Network sentence bird and developed the whole sentence Transformers Library the beer benchmarks and really showing how successful this technique could be so firstly Nils thank you so much for joining the podcast yeah it's pleasure to be here uh so could you kind of take it in on that uh kind of what I left off could you kind of take over the like kind of the origin story of how you came to be working on uh this kind of like Siamese encoding of representation learning yeah sure um that was not really planned that I go into that field so back then in 2018 I've been working on clustering arguments so in our research group we're interested in like controversial topics like should you still run new club nuclear power plants and we wanted to Cluster arguments on this like opposing or supporting and and opposing Arguments for that and back at that time methods were like Universal sentence encoder and infrastrant um the big problem here was that they only released the model like Facebook or Google only released the model but did not release any training code did not provide like really a lot of information how to train these models and this was like a bit but I don't know was bit annoying for me because I really wanted to have like some metal models that I can train specifically for this task and yeah that started to get me on the journey how can you train the embedding models we also said okay that's like really frustrating you have these models but they work really poorly out of the box for like really special scenarios like argument clustering so I thought okay let's start a library um that makes it extremely simple to train your own embedding models at that time birth was recently popular so I thought okay let's figure out how to use bridge for clustering information and that was like the origin for sentence for sentence Transformers and from then on since 2018 I've been hooked on the topic uh because representing stuff in Vector spaces open up so many opportunities and cool applications you can build with that that's so interesting uh yeah I learned a lot about topic analysis from talking with Martin grutendors on our other podcasts about bird topic and we also came on cohere talks and talked with Jay and uh yeah that that topic analysis such an interesting point to it um so maybe uh coming into it um so can we start out with so learning representations of text with deep learning um I'm really curious about if you could kind of tell this whole story sort of like I know about like the whole in batch negatives the info nce um you kind of tell the story around just like your investigations into sort of how you sample positive Pairs and negative pairs model size data size training requirements all these kind of things yes so in the original sentence for paper I followed the the approach from infrasound using nii data and training like cross entry which I totally can't recommend to do anymore so so actually the original first sentence per model performed really poorly even such the benchmarks were really good at that time whether the benchmark scores were good if you really apply them to a bit more complex data um they perform really poorly they also perform really poorly worse than Universal sentence and code at that time so this was like really nagging for me like okay how can you train better models and how can you evaluate better models so this is like two major pillars of my research not be justified hey I got both numbers on some arbitrary Benchmark because most benchmarks are pretty bad but first how can you assess them that they're really good and how can you make them better the infancy lost when I started was bit it existed but it was like really really unknown so the universal sentence encoder method they trade already was influency loss or I mean there are so many names for it but it was not really described in the paper so it was like really hidden and then if you follow up some some other papers they had a really complicated formulations for influence e loss um and yeah no training code was available but yeah at some point um what's the help of a student help of mine we actually implemented one of the really old implementations of the influency loss from Google and I'd share like really good results and then it's also made them a lot more sense than the previous cross entropy classification loss um big things which are relevant here is like batch size so that's a simple trick to increase your embedding performance increase the batch size but sadly at some point you run into limitations of GPU memory and because we don't have like an average right number of GPU memories in my research lab back then um we invest a lot of Time how to make the batches better specifically adding Hardware Gators to it but here you need a lot of cleaning so so really you want to push anchor and positive close in the vector space and anchor negative distance in the vector space but this loss extremely sensitive if the data is unclean so if anchor and positive it's not really positive pair that's extremely hurting the performance similar when anchor negative when it's not really a negative but a positive another positive that's also extremely hurting the performance of the model so spend a lot of time thinking working testing how can you make it nice and clean and have like really high quality data at scale uh it's so interesting and uh another kind of podcast we did was with Ori ROM on uh the spider algorithm learning to retrieve without supervision how um you look for overlapping terms to form the positives and as you mentioned the noise and the negatives I've always been so interested in that kind of thing um so what so what are you thinking currently about the positive negative sampling scheme is it uh you know like just adjacent paragraphs or positives and then if you do that at scale it kind of like makes it all right um when I'm a big fan of is like to use more powerful so-called cross-encoder model for data cleaning so often you start with like some anchor positive pairs these are often quite easy to to get like you take scientific applications and you say okay the title and the abstract that's probably like a good pair that should be close in the vector space or you go on um stack exchange and you take a question and you take the highest ranked answer and you say okay that's what's a good pair that should be closed in Vector space but how do you get like really high negatives so heart negative is for example I don't know for example you have a question how can I sort a python list in descending order and then the positive would be how to do it and the hardest negative would be to say okay this is how you do it but in ascending order so there's like a slightly tiny detail everything else matches but there's a slightly tiny detail and that makes it not a valid answer and finding this is like really really hard so how can you find like a negative that's so close to the correct answer but it's still still negative and often you overshoot so you get like the the negative that's also where person would say yeah that's another positive um so here I really like Crossing folders Crossing covers a lot more powerful than by encoders um so so you first drain a cross encoder and then the cross encoder can do cross attention between the query and or the the candidates and can really see it like these fine details like okay is it like really matching all the aspects you're asking your question and then at the end if you like a score and then you use this cross encoder uh you go over all your triplets black anchor positive negative and say okay here we have like actually negative pairs uh or negative candidates yeah it's a I think there's a there's a bit of that that I want to unpack and I think um you know being here talking to nose Runners I want to just dive right into the the details of the technical questions um but so maybe for our listeners quickly the cross encoder is this idea where you take the query and the document as input to like a high capacity Transformer that will output a score and it's slower than the buy encoders but it's like super accurate because of the high capacity of the Transformer uh so I want to come back into the the you know you mentioned the stack exchange and I think the work on the 1 billion training Pairs and then the diversity of the beer benchmarks I think those are so interesting I really want to come back to these topics but quickly again talking in those rhymers I have to ask this question um what about like a knowledge distillation from the cross encoder to the buy encoder where the cross encoder it kind of sounds like that's what you're getting at where it's filtering the data could it you know have like a soft label where the score that comes out of the Cross encoder is the um what the dot product should be yeah and yeah I'm super a big fan of this so Sebastian Hostetter established this in a really cool work where he showed how you can distill Knowledge from Cross encoders to my encoders and which led to the March and MSE loss which solves a lot of these issues so in classification construction of training you need the positive to be really positive to the query and the negative must be really a negative to the query so you spend a lot of time cleaning and trying to get like the hardest possible negative that is still a negative and not yet a positive but I was machinimus he lost you take the triplet query positive and some other candidates and pass it through a cross encoder to get like estimates from the cross encoder how close are the two candidates to the query and then you transfer this knowledge to a buy encoder and this totally eliminates the issue of getting really clean data so you can run it with like really dirty data which is nice you can run it with like really really hard negative so far with the she tried buying folders and my negative is like too hard for my encoder it extremely hurts your performance but now you can still still run it so I'm a big fan so I in most cases I moved away from cosmetrical training to margin MSC training downside here is a bit more overhead so constructive training there it's like really easy to get training examples to go on stack Exchange you download it you get the question you get like 400 million questions the highest ranked answer so you get directly 100 million pairs you can use from constructive training but of course machinimously loss you have to do negative mining you have to have a good cross encoder you have to take the cross encoder to score all query anchor uh query positive negative triplets so there's like a lot of overhead involved in that uh so sorry to ask a clarifying question but the margin MSC that's where you don't you have to kind of explicitly set the margin in the triplet loss and I always thought for that reason it was sort of funky like because it's kind of like a alpha minus Max uh kind of thing right all right you take the cross encoder you asked like two questions like Quarry and candidate one and query can you take two what's the relevance and this gives you like two relevance so the one has for example I don't know relevance 10. the other has relevance five and then you use this distance so you say okay the the distance from candidate a to candidate B is like five and then you transform this to a by encoder that the by encoder if you compute the dot product between query candidate a and candidate B should also be five in the vector space so basically what you do is you arbitrarily pick three points so you have to query you have candidate a candidate B you ask the powerful cross encoder what's the distance and then you teach the buy encoder to match also the distance in the vector space so the crossing code that teaches the the buy encoder about distances in the vector space okay fantastic yeah awesome so yeah yeah I think that idea is going to be super powerful with the because you you want the cross encoder is so kind of slow especially if you just go into a billion documents and then maybe you want to retrieve a thousand re-rank a thousand so I think that kind of distillation will be super impactful uh can you tell me about I want to dive into the effort on collecting the billion data pairs on uh I think it's like Wikipedia stack exchange Reddit uh there's like a big collection uh he told me about the effort involved in that yeah um so so yeah and last year at Target phase we had like this community event um where we got like um some resources sponsored by Google Cloud to work on tpus and Jacks and then we said okay it would be cool to to Really train embedding models on a lot of training data so far a sentence Bird model have been trained on like Wi-Fi small training sets Universal sentence encoder have been trained on like billions of training pairs but Sally training was not available so we we put the efforts together and found like okay what are good resources get labeled resources one is like I don't know Ms Mark or NQ all this data set here are duplicate questions to just collect them bring them to a standard format and then stack exchange have a lot of data available but in XML format so it takes on pre-processing time to go from XML dumps to high quality Q and A pairs uh similar for Reddit there are like some big dumps available on Reddit so it took some time to process it to get like um comment and or answer post and comment structures and then yes I spent a lot of time to get the data clean the data and then train a model so in general big Advocate on data qualities so people should spend a lot of time to get like high quality data and then truly the the models you apply and the methods you apply it's like rather straightforward after that yeah it's really interesting um because the contrastive learning is self-supervised like the auto aggressive predictive Mass token and so I think maybe the focus on data quality is less so because the philosophy is like we're taking advantage of all this unlabeled data so can you maybe tell me a little more about how you think about data quality in the self-supervised thing where you have just the crazy amount of data so how do you clean it do you de-duplicate it um yes so in general what we see is that data quality is critical like in every aspect be it like generative model or an embedding model um what works nice is often like a two-stage approach first you you train with some approach um unlike noisy data like you take a burden at work and you run Mass language modeling unlike a lot of data um or you you run like this inwards close tasks and a lot like a lot of data so where you take like a paragraph and randomly take a sentence from this paragraph and then this is your pair but what's really critical is the second stage where you're training like on high quality data for the tasks you want and here data quality plays like an extremely critical role so out of the box bird was MLM or ICT train doesn't work well for search but if you then show like some search examples you can get really good strong results and yeah I think it's valid you can't focus on both so first pre-training or for more on the fine tuning step I don't have a git I don't have a intuition which is more important should be more focused on the pre-training or should you more focused on the and the defined tuning I would say me personally I've spent more time on fine tuning and tried to find like high quality question answer pairs or triplets to fine-tune and the model on this yeah it's so interesting and I I definitely want to dive into the new cohere multilingual model and see what details I can tease at you but um with the the sentence Transformer all mini LM that's been a just super impactful model it has an incredible zero shot kind of ability is that the that's the pre-trained model correct and it hasn't been fine-tuned yet so it's like the pre-train on massive now the the r mini iron model has been pre-trained on these like billion pairs like question answer pairs duplicate questions and so on very interesting and yeah I think uh yeah the the zero shot ability of those models to encode all these domains I think that's just a huge thing um in our last podcast Chris Dawson had said that uh like a lot of these tools are like the perfect MVP minimum viable product tool that it solves like 80 of the cases out of the box and I think it's very interesting as we talk about this kind of philosophy of fine tuning so I think this would be a great topic to kind of pivot into the philosophy of coheres models and as you're developing these super powerful models um how you're thinking about pre-training and fine tuning and generally anything you want to talk about sure so yet here we recently launched our multilingual embedding model um this week Monday so so far we had like these really large data set collections in English so because in English it's easy you go on stack exchange you download the XML files and then yeah you have to do the hard work to clean or extract data from the XML files but at least you have the the big dump of Stack exchange and this gives you already like 100 million question answer here so if you train a model on this per month extremely good so far an issue with multilingual models has been data some people previously used like machine translation to translate Ms marker to other languages but here the issue was multilingual models is that they learn a U.S Centric machine translated view on topics so for example in Ms Mark called Big topic is like how do I do my taxes in the US so if I do this and take it with machine translation translated to German and the model learns in German how do I file my taxes in the yes and how how do I file certain forms but it has like a blind spot how do I do my taxes in Germany and me as a German German speaker I wanted to do know how do I do taxes in Germany and how do I fill in much like certain tax forms in Germany and so this was like a big missing spot of these a lot of previous models also most previous models only worked on like sentence level which like really bad for surgical you want to match like a query to like a long long paragraph or longer document so here we took like the same recipe as described before so we first collected a lot of data from the web so found like a lot of resources like tens tens of thousands of these uh websites communities FAQ pages on on websites news articles and so on and then try to find like quality or training pairs in here like question answer pairs basically um did a lot of filtering to really filter out um high quality question answer pairs here big challenge is like to scale to a lot of languages so in English there's like a way how you can look at like okay what's like a good good pair what's a bad pair you can do like some wreck X some filtering like I could for example I don't know you count the number of white spaces to see if a paragraph is well formatted or is just like some random string but the challenge is if you do it across 100 languages it's like so diverse so for example if you go to Chinese you don't have like white spaces so how do you know if a text is like actual proper Chinese text or is it like just some random Chinese words and so somehow you have to find like ways to to scale to these hundred languages and then the third step was like data augmentation finding good hot negatives cleaning them and then taking these triplets and try trying the embedding model and yeah overall we got like 900 million English triplets uh close to 500 million non-english triplets and then passing this to a model which gives you again like a model that has seen a lot of topics you can apply it to your taxes or online gaming or beauty or fashion or computer science because likely in these 500 million non-english questions it has seen such a topic or similar topic already uh it's so interesting uh there's so many things I want to unpack I mean yeah first aid income there's also this miracle Benchmark that's going around on multilingual and I think that's about oh man I'm trying to guess now but it's not I don't think quite 500 million if you sum total all the languages other than English and I know it's 32 million English so it sounds like yeah a massive data set and that's this is kind of the thing I wanted to get a little more into with this kind of private models public models thing is I think with the private models and the kind of business model around it it makes sense it makes sense that you know coherence or say open AI on the other side of the fence can build these like massive data sets and and be kind of incentivized to build and have a sustainable thing around building 900 million English 500 million sorry uh non-english terrorism so maybe could you talk more about just like the challenges of collecting data when building the sentence Transformers project compared to now a cohere and so yeah I mean that the challengers are already similar so you have to do work which is for a lot of people painful so a lot of people don't enjoy working on data so people wanna find the best loss function and try to I don't know add some skip connections to it but people do not really enjoy working on data like download this or find like all these XML files from stack exchange for example open the stack exchange format deposit find the right pairs on this so so people do not really enjoy this but I think that's where a lot of value comes from um obviously if you process a lot of data it also requires a lot of compute so that's that's true that's kind of been erected so if you have like a billion training Pairs and you want to train on this this takes a lot of time but also data cleaning so we we pass like these billion training pairs through a cross encoder um so first you have big cross encoder and then you need to do like inference on a billion pairs so that's awesome significant amount of time yeah well that's super interesting and um yeah there's that that whole thing is really interesting I want to kind of come back into the multilingual thing and uh something that's always scared me with multilinguals kind of as you say about the Chinese you don't know when the like the white space or say the period here is or like the uh line break for paragraph heuristics all that is so useful um and I'm also jealous of say the Europeans I know it's semi say uh Bob and Eddie and they all speak multiple languages whereas me I only speak English uh so like with the multilingual dude what was your uh interest in diving in do you have any hesitancy on maybe the domain specific thing like debugging your Swahili model right it would be a little like difficult yeah that's that's definitely hard to to debug this um first I would say there's like a big big need for this so me as a German speaker I mean it's always nice if I go to English there are like so many models and systems available but if you even if you go to German and German I mean it's still like a high resource language has a decent amount of population it's quite powerful economically but still the number of things you can get is like really limited and it's like really frustrating to see that in English you can build these amazing semantic search systems which works extremely good for Q a retrieval but even if you go to German there's like hardly any models available and then if you go further down the the language letter to like more and more low resource languages it's getting harder and harder to build this second motivation is multi building multilingual search was lexical search is like really painful so if you build this in elasticsearch for example you need to know the language so you need to know the language of the document for every language you have like a different index because every language requires a different tokenizer requires a different stopwatch list requires a different stammer so at the end let's say you want to Target I don't know European Union was I don't know 20 languages you have like 20 different indices and elasticsearch everyone each has like a different tokenizers demo stopwatch list for each document you have to run language identification to see is it a German document or a French document and then the Big Challenge comes at query time is it like a German query or a French query or an English query for some really short queries it's like a big it can be English and German or French and Spanish and so how do you query these different indices because the Spanish query you have to add like a Spanish index and so on was extremely retrieval so you can have just you have just one index you take your text you pass it through the embedding model that gives you some in settings to pass it to your Dems to your vector DB you don't have to do any language identification don't have to build different indices don't have to do different stemming and stop work list so it's like super easy to build like densport tree with 500 languages which is like completely painful and possible because like lexical search hmm that's super interesting I think the um yeah I thought the last I saw I saw a strong hybrid search results on multilingual I'm really not uh yeah it sounds the whole separate stuff whereas I you know I don't know what the stop words are in Korean so it does I see how that kind of domain uh shift sounds really uh challenging them so I I kind of want to Pivot to sort of an idea I've just seen recently and get your opinion on it um this thing was called this paper was called a task aware retrieval with instructions where sort of the interface is in addition to that search box you also explicitly describe your intent and then you have like an embedding of the intent that you would use to re-rank the candidates in a vector search way as well so what do you think about that kind of idea because you mentioned the problem of okay I gotta understand that this query is Korean or uh what's a neighboring language I'm not I'm not like a great language guy but like this idea of you explicitly describe your intent as well yeah m M yes so so in the beer Benchmark we have like one one data set which is really interesting it's called abuana when you want to find counter arguments so you have an argument say nuclear energy is super safe and then you want to have retrieval to find like counter arguments to say okay no the energy is not safe obviously out of the box models if I search for nuclear energy is safe finds like different entries different arguments also mentioned nuclear energy is one of the safest energy sources so so there are the questions like how can I intend to tell them all of my intent that I don't want to have arguments that are similar but arguments that are opposing and the paper you mentioned we use this in terms of like kind of like instructor say okay find like contact women nuclear energy is safe or find a similar argument or find evidences um I think yeah it's it's a nice idea especially if people want to build this into their product so so if you have like search engine and you want to find some arguments with supporting and opposing evidences and different perspective so I think it's like an easy way for machine learning engineers and search engine Engineers just to prepare the prefix and say okay now I want to search for posing arguments or supporting evidence or opposing evidence for this yeah um so I'm kind of there's a few I kind of want to stay on this idea of well I think because we could go into like the kind of the prompting in the chat gbt and that idea of instructions and intent and the interface around it but I kind of want to stay on this like multi-vector search like vector re-ranking uh have you had a chance to see The Colbert idea and do you have any thoughts on it this kind of like you keep token representations to re-rank in a vector search way yes so yeah I'm quite familiar with coborn um say a code word is a nice idea um so far challenge with code word is like deployment costs so the idea of cohort is like you take a text and you don't encode to a single embedding but you take every token and your method to like like to an individual embedding and this gives you like kind of like an explosion of embedding so if you take a paragraph or summary tokens it's not resulting like single embedding but to 100 embeddings and this down the line ex that's the the cost for the vector DB to be exploded so because the cost of vector DB is like 100 times higher and this slide's really significant so if you want to do semantic search Vector search on English Wikipedia it's not too big it's like I don't know 5 million articles but you pay like roughly two thousand dollars a month to run like AWS instance on this if this is like 100 times higher so that's like um like 200k months to run like coverage like out of the box non-optimized covert so search is not not only like finding git system but always perfect trade-off like good systems versus costs around the search and here's like interesting to play around with like different parameters to say okay for which setting which provides like the optimal setting optimal trade-off between cost and performance um and I think here finding like generic answers is like really hard to say okay I don't know code word is always better or Dent embeddings is better or spasm headings is better because I think it can also depend where you are do you have like a small data set and one really high quality then code word is good if you have a massive data set uh and you're okay with somewhat good quality then covert is probably like way too expensive and you you have to use like other approaches like binary embeddings that yeah there's a lot a reason I'm still reading this paper from uh Christopher Potts and and a big team I just remember that name but um about like the the cost trade-offs of like splayed the starting with bm25 is the cheapest thing and then like explain and then they have this like played engine behind Cobra but yeah I this General topic around the cost of the search systems is so interesting is where alleviate releasing uh like disk a n and the vamana thing compared to h s w that has the in-memory thing and uh you know the product quantization these things around uh making it run let's say CPUs gpus I'm also very interested in a company called neural magic that's doing like sparsity for the model acceleration that whole topic is so interesting and uh so maybe transitioning um the last time I heard you uh give this lecture uh it was a really great lecture cohere talks um and you had four key points and I kind of want to walk through them and I think this would be so from uh Colbert I want to talk about uh sparse vectors I don't really I don't know if Colbert is quite a sparse Vector idea it's kind of that idea where you have like a representation of each token so I kind of want to talk about um like bm25s Blade the this idea that you have like a mass language modeling head that projects the token into a sparse uh representation uh so can you tell me how you're thinking about sparsity these days m um I like the split model I'm a big fan of it um I really like it because it's like really close to a mass language modeling so so the issue was dancing batting models if you train bird mom gives you like really bad dance and batting models out of the box weaker than averaging word to wreck but this mon objective display just like really closed between like split and MLM you could take a word you'll see okay what are possible synonyms here and then you put all these possible synonyms for every token into like a sparse vectors so here you have like a really close match between fine tuning and pre-training which is extremely nice um also they have shown really strong results like out of domain so big challenge for density matting model is like new words so people create like new new things like I don't know chat GPT um free Trend and some embedding model has like no idea what it is but like Fast models they just projected to like New Dimensions you can search for it so as soon as you search search for it you find it which is also a lot of issues um topics which I'm really interested in which they certainly also don't solve as like long document encoding so split works on a paragraph level so so maybe up to a few hundred words but in most cases you have like really long documents like thousands tens of thousands of words I don't know in some industry manufacturing centering you have PDF documents of 10 000 Pages how to repair some I don't know machine and they're the question like how do you can even kill these loan documents and sell it doesn't work with blade and also doesn't in principle work with like like spice Packers from Guiding my thinking with these four things I've kind of started to put long documents in multi-discourse uh kind of together into the same category where uh yeah I mean yeah long documents is such an interesting problem I think with the chat gbt and like the dialogue models particularly it's making a lot of people think about this so here's maybe what I've been thinking about and I'm really curious what your thoughts are on this is um is sort of clustering the windows and then because you embed the windows of the long document then you kind of cluster it and then you have sort of a representative centroid that you kind of try to pack into the context window sort of and maybe also in addition to you know the retrieving facts and it's some kind of prompt design to overcome the long document limitation yes I think here like you need to differentiate between these two things like more chat GPT where you operate on like some history and in terms of of retrieval so in terms of retrieval um Big Challenge is kind of like co-references so if you take the annual report from Apple you just have like apple on the front page of the annual report so the annual report is located on 200 pages long and then down the line on the paragraph level it just says we so instead of like apple they just we you can't have a paragraph where they write like we saw a big increase in demand for our products in this market and the other questions like what is we what is our products and what is this market and you as a human you know okay it's the Apple annual report you're in subsection I don't know iPhone sales in North America and then you can fill in like the blanks but they are the questions like how do you fetch up this co-co ref well how do you fetch up this information um in chat GPT challenges um hear what works so far is like increasing the context lengths because I don't know if you start a blank session um it takes some time so if it has like context length of 8 000 so so right now the best models can do like up to 8 000 work pieces that's like eight pages so it takes a time until you write like eight pages of organization with check GPT so you can just do massive attention for tokens but probably over a long time if you really build up I don't know if you have like a lot of interaction with systems we need to find ways to to build up some type of memory so like we do it with humans we know this person I don't know has these Hobbies this work these I don't know travel to this location and then we can query this somehow and use this in our current context so so ether we have to find like way to build up like some small memory which we can quickly officially query what we have to do have to have select some query thing into it like Okay um on what topic is the person working so so that I can create like a follow-up question on this but still that's I don't know really I think really early I feel like how can a system quickly access memory and builds on top of this to get like new ideas from past history yeah it's I mean the whole external Transformer memory idea is something that we're just crazy excited about with weaviate and the you know the vector index and all that but um I really quick I want to go a little deeper into this co-reference resolution it's it it sounds like such a problem and I think it communicates the idea well so like three paragraphs eight pages well let's so in the context of retrieval models we'll say we can only take in like two paragraphs as input it's like four paragraphs to go it said like uh yeah apple is the and then we are and then we use apple so does that mean we maybe need like a co-reference resolution parser to parse our text before it's vectorized or do we maybe need to create like a graph of edges the link yeah um yeah that's that's good question where we're currently doing research on that um it says uh it depends kind of the the data set so right now a lot of people work on Wikipedia about Wikipedia is cut what kind of borrowing because it's like really safe uh contained so if you take a paragraph from Wikipedia I don't know you take a random paragraph from the Barack Obama article it will mention Barack Obama so like every paragraph mentions Barack Obama but in Android reports they I don't know Apple just say we and then on the front page you know it's Apple annual reports you go on page 100 here's the section about I don't know iPhones a man's page 150 years iPhone sales in North America and then you have the paragraph like we saw a big increase for our products in this market and so you kind of need to do like co-reference resolution across like many pages so potentially like going back 100 pages to know okay this is the Apple annual report from 21. and there's the question like how can we do this because then we come back to like a whole problem so right now all people in co-reference resolution are mainly using Transformer networks which are again limited to like I don't know 512 word pieces so but how can we do this co-reference revolution possibly over like hundreds of pages and it's awesome I can really complex settings so that we really know um how to do it and yeah I think that the best way if we chunk to paragraph and encode them is to really rewrite the paragraph so that if you just see the paragraph and nothing else you has all the the relevant contacts information so if I rewrite like we saw a big increase in our products on this Market in the last year and rewrite it to Apple so a lot of increase in demand for the iPhone and the North American Market in 2020. then it's nice you can find it for a lot of search questions uh but yeah open questions like how do you do this rewriting especially this requires again to resolve long context resolutions yeah that's so interesting I think of that as like a data augmentation maybe more than a pre-processing because you're kind of trying to encode the invariance of uh the original paragraph Without sorry the original paragraph Without The co-references resolved and then the new kind of like instead of we it's apple and then you're trying to say like the invariant to these two Transformations and I think it's very interesting is you kind of the problem eats itself like the co-reference resolution models they also are limited to the context so yeah that's a pretty interesting thing um and then obviously we can also change like I don't know maybe they have like some subdivision and then we can mean I don't know not only Apple but maybe I don't know for Europe or so or some you have like some parent company and then there's like some uh child companies as a weekend refer to the chai company or can it refer to the whole group or to some product team so have you thought about like um entity parsing entity extraction it's because it's a little different from the sparse search with the high with the hybrid the bm25 is like if the query is how to catch an Alaskan pollock it's like it'll be emphasizing the Alaskan pollock sort of with the keyword matching thing but there's also this idea of like entity extraction Alaskan pollock and maybe it's that multi-vector ranking where you search with how to catch an awesome Pollock and then you re-rank with just the like Alaskan pollock um yeah I mean that the researchers still really are Valley in this field so um big limitation as so far data sets um a lot of data sets are built on Wikipedia which does not really require to take a lot of contacts into account then the question is it just entities or is it also other things what an entities so that's also like like an open research question is it just fine to resolve like we and this and last quarter or do we have to also resolve other things and yeah in my experience often in these I don't know these these examples you make up like I don't know we saw a big demand it's kind of easy but then if you go to actual sentences you say okay it's like a lot more nuanced which context information do you still need to understand this paragraph um maybe context information can also map to like a table with financial data it can map to some graph and then the question is like okay maybe they have like some pie chart and they're discussing the pie chart how can you contextualize the pie chart into this paragraph so that if you just encode the paragraph have all the necessary information and without running out of context links or Wetlands again yeah that's another really interesting topic I think there's so many things to talk about but like that idea of um the tables and the graphs and getting that into the text domain it also it kind of reminds me of the same data augmentation pre-processing we discussed with the co-reference where where you can parse the tables like I've seen this paper on uh it's like learning to reason across Wiki tables something some kind of like that where they're parsing out the tables to go from table structure to text structure uh and then because then you kind of unify the domains without having to have like a visual uh component to it yeah anyway so um another topic uh kind of a finishing on the four things is um uh so the topic of uh unknown words and distribution shift um maybe kind of maybe actually those are kind of two topics with um because uh maybe we'll start with uh distribution shift because it's so exciting so uh I recently read this paper that was really interesting called Ood disk a n where they're describing the distribution shift and if you build up an image index and then you're searching with the text queries they're going to be out of distribution for the h s w practice or the proximity graph structure is the vamana but and that kind of idea so uh how are you thinking about distribution shift these days yes so distribution shifts are interesting on multiple aspects so language evolves so so that's big pain point of language or nice feature of language you see that things evolving and meaning so I don't know like Corona before 2020 it was like many beer then selling in 2020 if someone talks about Corona it's connected to a virus similar if you talk about like alphabet or amicron it used to be some Chinese and some Greek characters maybe relevant format method mathematical questions now people talk about yeah there's like a new Omicron wave in in Europe or stuff like that or there's a new Army Corps environment where it's like extremely hard to to really update the models here and a big challenge is also um in terms of like first stage and second stage review also so if you take an embedding model that's like no idea about Corona virus maybe on new training data you can update the model that Corona now has a new meaning but then you come back to need to think about like do you need to re-encote your whole Corpus so you do you have to take the embedding model and re-encol like the whole Corpus if your Corpus is more like a million documents it's okay what if your Corpus is the web like Google or Bing you have to re-encore like all documents on the web just to create like the new embeddings and so there's like an interesting thing or question like okay can you make this a bit more efficient so which can you kind of like transform the embedding so do can you do like some partial updates so do you can just update that Corona now has like two meanings the beer and the virus and can you find like the documents you want to update but everything else you can keep the same um be able to decide when do you do the updates or when do you train again this new embedding model and so on um so so many many open on soft research questions here right now people do like the most stupid thing they completely update their betting model and then encode the whole Corpus again even such that 99 Prime percent are currently attached from from the update so so exciting research questions to know okay what do you need to update and how can you update this and also there the interplay between first and second stage so maybe it's okay to have like an outdated free stage retrieval that does not really know what is Corona but then have like a more recent up-to-date second stage retrieval that and also Corona can also uh Nina virus now and not only beer yeah I thought what you just said was incredibly inspiring uh because there was this argument about with the whole like uh we V8 augmented gbt thing where it's like um you know the big language model doesn't need it can be updated for these new facts by retrieving it but then it's like well what about the retrieval model and and it that and then there's that problem of you retrain the retrieval model on the new information like the new omnic uh omnicron uh variant and then um and then you have to re-encode and the whole document and if you have like you know hundreds of millions or whatever and we thought about this idea that is kind of interesting that I'd like to pitch to you as well as like uh like maybe we could use that proximity graph structure as of prior to try to propagate the representation space changes and and that way we can maybe only re-vectorize like 10 000 documents and it could be they could be kind of cleverly selected with their kind of centrality in the proximity graph and then we could maybe have some sort of graph neural network that could propagate representation changes I know this is too grandiose of an idea of uh what do you think about that idea um I think they're there you quickly run in the problem or the challenge to know what has changed so so let's say what has changed in 2022 and then maybe for some top things you can name it but it's like really hard in the long tail to say okay what has changed my other like new and some new online games popular online games they're like some new patch which introduces new characters which have like a new acronym and now and one refers to them as like a different acronym or so so I think that's like the the challenge like really what has changed between now and the year before and to really enumerate this to not only say okay not only like I don't know Corona change the meaning but like really in the long tail what has all changed the meaning and where do you all need to do the update um I think that's that's the biggest challenge how to automate this and how to find it um also in terms of preferences um if I search for like how can I get Corona in theory how I catch the virus and how I catch or how I buy the beer should both be relevant so if you ask like an annotator they should both be annotated as relevant but probably if you see a search in like Corona and people say how do I get Corona you probably want to rank the virus higher than the beer and then maybe hopefully in a few years you want to rank the BR high again but there yeah I also the question like how do you build up this popularity so it's not only relevancy in search but also in popularity of terms like um nft popped up at some point for non-fungible tokens so if someone search for nft we're meaning unfundable tokens but there were meanings for nft before so so there's like a book series Not For Tourists also every related with nft so how can you learn okay there's like a new meaning which is a lot more popular also crypto I started myself to work on cryptography so crypto 10 years ago you referred to cryptography then Bitcoin and and uh and so on happen and now if you talk about crypto everyone talks about like Bitcoin and stuff so yeah I think um in this well yeah this idea of kind of updating the unknown words it sounds it sounds pretty difficult and I don't even have anything to kind of propose but that's kind of why I also really like this intent stating as well in the search engine interface where uh how to catch Corona and then maybe if you just said your intent like I'm looking for beer I it's like a different way to search and it's more verbose than usual but if it gives you better results maybe especially especially with like the kind of the idea that search interfaces are moving into like generating you a long answer instead of just necessarily returning documents uh and maybe you'd be willing to put more effort into your query I think generally the search interfaces uh thing is evolving um yeah maybe you kind of yeah I want to ask you a question question about search interfaces really quick like um just a second one on this intent I mean it's kind of old ideas like I don't know before that it was named query rewriting I mean we as a human do it like I don't know if I search for Corona I get the wrong results I search for Corona beer so we already trained on this um I search for like how to sort a list and I forgot which programming language I add like python to it and then also in the back end like I don't know even when Google was created was smartly adding query writing to it so when I look for I don't know like an Italian restaurant it was adding smartly in the background the program location I'm in and we're looking for like Italian restaurant in timestarter so it's kind of like an old technique this this query rewriting heading trying to guess the intent and adding it yeah that's super I I saw this like learning to reformulate queries with reinforcement learning where the action space is are the keywords and then you can add them subtract them boost the importance of them uh yeah yeah it's super quiet I guess I'm kind of uh losing interest in that idea due to the challenge of maybe a reinforced and learning query writer it sounds like a lot of overhead in the in the search pipeline thing yeah that's true yeah so interesting uh so anyways Nils uh thank you so much for joining the weekday podcast I think we covered so many interesting topics I'm really looking forward to re-watching this and you know rethinking about all these ideas um thanks so much for your contributions to search broadly yeah and I'm really loving these uh cohere talks you're giving I think you're doing another one in about an hour and uh so I'll see you over there and I'm really looking forward to it and thanks again great thank you so much for being here bye bye ", "type": "Video", "name": "Nils Reimers on Cohere Embedding Models - Weaviate podcast #33", "path": "", "link": "https://www.youtube.com/watch?v=TKJh2H5Prcs", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}