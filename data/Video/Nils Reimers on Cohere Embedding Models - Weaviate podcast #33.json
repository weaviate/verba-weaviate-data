{"text": "Thank you so much for watching the 33rd Weaviate Podcast! This episode features one of the heroes of Deep Learning for Search ... \nthis interview is recorded shortly afterpublishing our integration with coheresmultilingual embedding models inweeviate to try this out for yourselfplease check out this excellent blogpost written by Sebastian woodaleck thisis linked in the description of thevideo I really hope you enjoyed thisinterview with Nils rhymers I learned somuch as the interviewer and I reallyhope you do as well as always we'd bemore than happy to answer any questionsor discuss any ideas you have thanks somuch for watching hey everyone I'm soexcited about our next wewva podcastwe're hosting Nils rhymers Nils rhymersis one of the most influentialscientists in this whole area of deeplearning for surge he's done so muchincredible pioneering work especiallywith tech search and I'll kind of hopinto this The Narrative as I see it andthen ask Knowles to kind of hop in onthe origin of this kind of research butto me it kind of looked like we startedwith this thing where we can extractrepresentations of data from theintermediate layers of deep learningmodels and then what happened is westarted optimizing directly for thoserepresentations using contrastivelearning loss functions and I think thiswas super successful in computer visionwith papers like simclear Moco and thenNils and his team they started showinghow you could do this with Siam thesebird Network sentence bird and developedthe whole sentence Transformers Librarythe beer benchmarks and really showinghow successful this technique could beso firstly Nils thank you so much forjoining the podcast yeah it's pleasureto be here uh so could you kind of takeit in on that uh kind of what I left offcould you kind of take over the likekind of the origin story of how you cameto be working on uh this kind of likeSiamese encoding of representationlearning yeah sureum that was not really planned that I gointo that field so back then in 2018I've been working on clusteringarguments so in our research group we'reinterested in like controversial topicslike should you still run new clubnuclear power plantsand we wanted to Cluster arguments onthis like opposing or supporting and andopposing Arguments for thatand back at that time methods were likeUniversal sentence encoder andinfrastrantum the big problem here was that theyonly released the model like Facebook orGoogle only released the model but didnot release any training code did notprovide like really a lot of informationhow to train these models and this waslike a bit but I don't knowwas bit annoying for me because I reallywanted to have likesome metal models that I can trainspecifically for this taskand yeah that started to get me on thejourney how can you train the embeddingmodels we also said okay that's likereally frustrating you have these modelsbut they work really poorly out of thebox for like really specialscenarios like argument clusteringso I thought okay let's start a libraryum that makes it extremely simple totrain your own embedding models at thattime birth was recently popular so Ithought okay let's figure out how to usebridge for clustering informationand that was like the origin forsentence for sentence Transformers andfrom then on since 2018 I've been hookedon the topic uh because representingstuff in Vector spaces open up so manyopportunities and cool applications youcan build with thatthat's so interesting uh yeah I learneda lot about topic analysis from talkingwith Martin grutendors on our otherpodcasts about bird topic and we alsocame on cohere talks and talked with Jayand uh yeah that that topic analysissuch an interesting point to it um somaybe uh coming into it um so can westart out with so learningrepresentations of text with deeplearning um I'm really curious about ifyou could kind of tell this whole storysort of like I know about like the wholein batch negatives the info nceum you kind of tell the story aroundjust like your investigations into sortof how you sample positive Pairs andnegative pairs model size data sizetraining requirements all these kind ofthingsyes so in the original sentence forpaper I followed the the approach frominfrasound using nii data and traininglike cross entrywhich I totally can't recommend to doanymore so so actually the originalfirst sentence per model performedreally poorlyeven such the benchmarks were reallygood at that time whether the benchmarkscores were good if you really applythem to a bit more complex dataum they perform really poorly they alsoperform really poorly worse thanUniversal sentence and code at that timeso this was like really nagging for melike okay how can you train bettermodels and how can you evaluate bettermodels so this is like two major pillarsof my research not be justified hey Igot both numbers on some arbitraryBenchmark because most benchmarks arepretty badbut first how can you assess them thatthey're really good and how can you makethem better the infancy lost when Istarted was bitit existed but it was like really reallyunknown so the universal sentenceencoder method they trade already wasinfluency loss orI mean there are so many names for itbut it was not really described in thepaper so it was like really hidden andthen if you follow up some some otherpapers they had a really complicatedformulations for influence e lossum and yeah no training code wasavailable but yeah at some pointum what's the help of a student help ofmine we actually implemented one of thereally old implementations of theinfluency loss from Googleand I'd share like really good resultsand then it's also made them a lot moresense than the previous cross entropyclassification lossum big things which are relevant here islike batch size so that's a simple trickto increase your embedding performanceincrease the batch size but sadly atsome point you run into limitations ofGPU memoryand because we don't have like anaverage right number of GPU memories inmy research lab back thenum we invest a lot of Time how to makethe batches betterspecifically adding Hardware Gators toit but here you need a lot of cleaningso so really you want to pushanchor and positive close in the vectorspace and anchor negative distance inthe vector space but this loss extremelysensitive if the data is uncleanso if anchor and positive it's notreally positive pair that's extremelyhurting the performancesimilar when anchor negativewhen it's not really a negative but apositive another positive that's alsoextremely hurtingthe performance of the modelso spend a lot of time thinking workingtesting how can you make it nice andclean and have like really high qualitydata at scaleuh it's so interesting and uh anotherkind of podcast we did was with Ori ROMon uh the spider algorithm learning toretrieve without supervision how um youlook for overlapping terms to form thepositives and as you mentioned the noiseand the negatives I've always been sointerested in that kind of thing umso what so what are you thinkingcurrently about the positive negativesampling scheme is it uh you know likejust adjacent paragraphs or positivesand then if you do that at scale it kindof like makes it all rightum when I'm a big fan of is like to usemore powerful so-called cross-encodermodelfor data cleaning so often you startwith like some anchor positive pairsthese are often quite easy to to getlikeyou take scientific applications and yousay okay the title and the abstractthat's probably like a good pair thatshould be close in the vector space oryou go onum stack exchange and you take aquestion and you take the highest rankedanswer and you say okay that's what's agood pair that should be closed inVector spacebut how do you get like really highnegatives so heart negative is forexampleI don't know for example you have aquestion how can I sort a python list indescending order and then the positivewould be how to do it and the hardestnegative would be to say okay this ishow you do it but in ascending order sothere's like a slightly tiny detaileverything else matches but there's aslightly tiny detailand that makes it not a valid answer andfinding this is like really really hardso how can you find like a negativethat's so close to the correct answerbut it's still still negative and oftenyou overshoot so you get like the thenegative that's also where person wouldsay yeah that's another positiveum so here I really like Crossingfolders Crossing covers a lot morepowerful than by encodersumso so you first drain a cross encoderand then the cross encoder can do crossattention between the query and or thethe candidates and can really see itlike these fine details like okay is itlike really matching all the aspectsyou're asking your question and then atthe end if you like a score and then youuse this cross encoder uh you go overall your triplets black anchor positivenegativeand say okay here we have like actuallynegative pairs uh or negative candidatesyeah it's a I think there's a there's abit of that that I want to unpack and Ithink um you know being here talking tonose Runners I want to just dive rightinto the the details of the technicalquestions um but so maybe for ourlisteners quickly the cross encoder isthis idea where you take the query andthe document as input to like a highcapacity Transformer that will output ascore and it's slower than the buyencoders but it's like super accuratebecause of the high capacity of theTransformer uh so I want to come backinto the the you know you mentioned thestack exchange and I think the work onthe 1 billion training Pairs and thenthe diversity of the beer benchmarks Ithink those are so interesting I reallywant to come back to these topics butquickly again talking in those rhymers Ihave to ask this question um what aboutlike a knowledge distillation from thecross encoder to the buy encoder wherethe cross encoder it kind of sounds likethat's what you're getting at where it'sfiltering the datacould it you know have like a soft labelwhere the score that comes out of theCross encoder is the um what the dotproduct should be yeah and yeah I'msuper a big fan of this so SebastianHostetter established this in a reallycool work where he showed how you candistill Knowledge from Cross encoders tomy encoders and which led to the Marchand MSE loss which solves a lot of theseissues so in classification constructionof training you need the positive to bereally positive to the query and thenegative must be really a negative tothe query so you spend a lot of timecleaningand trying to get like the hardestpossible negative that is still anegative and not yet a positivebut I was machinimus he lost you takethe triplet query positive and someother candidates and pass it through across encoder to get like estimates fromthe cross encoderhow close are the two candidates to thequeryand then you transfer this knowledge toa buy encoder and this totallyeliminates the issue of getting reallyclean data so you can run it with likereally dirty data which is niceyou can run it with like really reallyhard negative so far with the she triedbuying folders and my negative is liketoo hard for my encoder it extremelyhurts your performance but now you canstill still run it so I'm a big fan so Iin most cases I moved away fromcosmetrical training to margin MSCtrainingdownside here is a bit more overhead soconstructive training there it's likereally easy to get training examples togo on stack Exchangeyou download it you get the question youget like 400 million questions thehighest ranked answerso you get directly 100 millionpairs you can use from constructivetraining but of course machinimouslyloss you have to do negative mining youhave to have a good cross encoder youhave to take the cross encoder to scoreall query anchor uh query positivenegative triplets so there's like a lotof overhead involved in thatuh so sorry to ask a clarifying questionbut the margin MSC that's where youdon't you have to kind of explicitly setthe margin in the triplet loss and Ialways thought for that reason it wassort of funky like because it's kind oflike a alpha minus Max uh kind of thingrightall rightyou take the cross encoder you askedlike two questions like Quarry andcandidate one and query can you take twowhat's the relevance and this gives youlike two relevance so the one has forexample I don't know relevance 10. theother has relevance fiveand then you use this distance so yousay okay the the distance from candidatea to candidate B is like fiveand then you transform this to a byencoder that the by encoder if youcompute the dot product between querycandidate a and candidate B should alsobe five in the vector spaceso basically what you do is youarbitrarily pickthree points so you have to query youhave candidate a candidate B you ask thepowerful cross encoder what's thedistanceand then you teach the buy encoder tomatch also the distance in the vectorspace so the crossing code that teachesthe the buy encoder about distances inthe vector spaceokay fantastic yeah awesome so yeah yeahI think that idea is going to be superpowerful with the because you you wantthe cross encoder is so kind of slowespecially if you just go into a billiondocuments and then maybe you want toretrieve a thousand re-rank a thousandso I think that kind of distillationwill be super impactful uh can you tellme about I want to dive into the efforton collecting the billion data pairs onuh I think it's like Wikipedia stackexchange Reddit uh there's like a bigcollection uh he told me about theeffort involved in that yeahumso so yeah and last year at Target phasewe had like this community eventum where we got likeum some resources sponsored by GoogleCloud to work on tpus and Jacks and thenwe said okay it would be cool to toReally train embedding modelson a lot of training data so far asentence Bird model have been trained onlike Wi-Fi small training sets Universalsentence encoder have been trained onlike billions of training pairs butSally training was not availableso we we put the efforts together andfound like okay what are good resourcesget labeled resourcesone is like I don't know Ms Mark or NQall this data set here are duplicatequestions to just collect them bringthem to a standard formatand then stack exchange have a lot ofdata available but in XML format so ittakes on pre-processing time to go fromXML dumps to high quality Q and A pairsuh similar for Reddit there are likesome big dumps available on Reddit so ittook some time to process it to get likeumcomment and or answerpost and comment structuresand then yes I spent a lot of time toget the data clean the data and thentrain a model so in generalbig Advocate on data qualities so peopleshould spend a lot of time to get likehigh quality dataand then truly the the models you applyand the methods you apply it's likerather straightforward after thatyeah it's really interesting um becausethe contrastive learning isself-supervised like the auto aggressivepredictive Mass token and so I thinkmaybe the focus on data quality is lessso because the philosophy is like we'retaking advantage of all this unlabeleddata so can you maybe tell me a littlemore about how you think about dataquality in the self-supervised thingwhere you have just the crazy amount ofdata so how do you clean it do youde-duplicate itum yes so in general what we seeis that data quality is critical like inevery aspect be it like generative modelor an embedding modelum what works nice is often like atwo-stage approach first you you trainwith some approachumunlike noisy data like you take a burdenat work and you run Mass languagemodeling unlike a lot of dataum or you you run like this inwardsclose tasks and a lot like a lot of dataso where you take like a paragraph andrandomly take a sentence from thisparagraph and then this is your pairbut what's really critical is the secondstage where you're training like on highquality data for the tasks you wantand here data quality plays like anextremely critical role so out of thebox bird was MLM or ICT train doesn'twork well for search but if you thenshow like some search examples you canget really good strong results and yeahI think it's valid you can't focus onboth so first pre-training or for moreon the fine tuning step I don't have agit I don't have a intuition which ismore important should be more focused onthe pre-training or should you morefocused on theand the defined tuning I would sayme personally I've spent more time onfine tuning and tried to find like highqualityquestion answer pairs or triplets tofine-tune and the model on thisyeah it's so interesting and I Idefinitely want to dive into the newcohere multilingual model and see whatdetails I can tease at you butum with the the sentence Transformer allmini LM that's been a just superimpactful model ithas an incredible zero shot kind ofability is that the that's thepre-trained model correct and it hasn'tbeen fine-tuned yet so it's like thepre-train on massivenow the the r mini iron model has beenpre-trained on these like billion pairslike question answer pairs duplicatequestions and so onvery interesting and yeah I think uhyeah the the zero shot ability of thosemodels to encode all these domains Ithink that's just a huge thing um in ourlast podcast Chris Dawson had said thatuh like a lot of these tools are likethe perfect MVP minimum viable producttool that it solves like 80 of the casesout of the box and I think it's veryinteresting as we talk about this kindof philosophy of fine tuning so I thinkthis would be a great topic to kind ofpivot into the philosophy of coheresmodels and as you're developing thesesuper powerful models um how you'rethinking about pre-training and finetuning and generally anything you wantto talk aboutsure so yet here we recently launchedour multilingual embedding modelum this week Mondayso so far we had like these really largedata set collections in English sobecause in English it's easy you go onstack exchange you download the XMLfiles and then yeah you have to do thehard work to clean or extract data fromthe XML files but at least you have thethe big dump of Stack exchange and thisgives you already like 100 millionquestion answer here so if you train amodel on thisper month extremely goodso far an issue with multilingual modelshas been datasome people previously used like machinetranslation to translate Ms marker toother languages but here the issue wasmultilingual models is that they learn aU.S Centric machine translatedview on topics so for example in Ms Markcalled Big topic is like how do I do mytaxes in the US so if I do this and takeit with machine translation translatedto Germanand the model learns in German how do Ifile my taxes in the yes and how how doI file certain forms but it has like ablind spot how do I do my taxes inGermany and me as a German Germanspeaker I wanted to do know how do I dotaxes in Germany and how do I fill inmuch like certain tax forms in Germanyand so this was like a big missing spotof these a lot of previous models alsomost previous models only worked on likesentence level which like really bad forsurgical you want to match like a queryto like a long long paragraph or longerdocumentso here we took like the same recipe asdescribed before so we first collected alot of data from the web so found like alot of resources like tens tens ofthousands of these uh websitescommunities FAQ pages on onwebsites news articles and so on andthen try to find like quality ortraining pairs in here like questionanswer pairs basicallyum did a lot of filtering to reallyfilter outum high quality question answer pairshere big challenge is like to scale to alot of languages so in Englishthere's like a way how you can look atlike okay what's like a goodgood pair what's a bad pair you can dolike some wreck Xsomefiltering like I could for example Idon't know you count the number of whitespaces to see if a paragraph is wellformatted or is just like some randomstring but the challenge is if you do itacross 100 languages it's like sodiverse so for example if you go toChinese you don't have like white spacesso how do you know if a text is likeactual proper Chinese text or is it likejust some random Chinese wordsand so somehow you have to find likeways to to scale to these hundredlanguagesand then the third step was like dataaugmentation finding good hot negativescleaning themand then taking thesetriplets and try trying the embeddingmodel and yeah overall we got like 900million English triplets uh close to 500million non-english triplets and thenpassing this to a modelwhich gives you again like a model thathas seen a lot of topics you can applyitto your taxes or online gaming or beautyor fashion or computer science becauselikely in these 500 million non-englishquestions it has seen such a topic orsimilar topic alreadyuh it's so interesting uh there's somany things I want to unpack I meanyeah first aid income there's also thismiracle Benchmark that's going around onmultilingual and I think that's about ohman I'm trying to guess now but it's notI don't think quite 500 million if yousum total all the languages other thanEnglish and I know it's 32 millionEnglish so it sounds like yeah a massivedata set and that's this is kind of thething I wanted to get a little more intowith this kind of private models publicmodels thing is I think with the privatemodels and the kind of business modelaround it it makes sense it makes sensethat you know coherence or say open AIon the other side of the fence can buildthese like massive data sets and and bekind ofincentivized to build and have asustainable thing around building 900million English 500 million sorryuh non-english terrorism so maybe couldyou talk more about just like thechallenges of collecting data whenbuilding the sentence Transformersproject compared to now a cohereandso yeah I mean that the challengers arealready similar soyou have to do work which is for a lotof people painful so a lot of peopledon't enjoy working on data so peoplewannafind the best loss function and try to Idon't know add some skip connections toitbut people do not really enjoy workingon data likedownload this or find like all these XMLfiles from stack exchange for exampleopen the stack exchange format depositfind the right pairs on this so sopeople do not really enjoy this but Ithink that's where a lot of value comesfromum obviously if you process a lot ofdata it also requires a lot of computeso that's that'strue that's kind of been erected so ifyou have like a billion training Pairsand you want to train on this this takesa lot of time but also data cleaning sowe we pass like these billion trainingpairs through a cross encoderum so first you have big cross encoderand then you need to do like inferenceon a billion pairs so that's awesomesignificant amount of timeyeah well that's super interesting andum yeah there's that that whole thing isreally interesting I want to kind ofcome back into the multilingual thingand uh something that's always scared mewith multilinguals kind of as you sayabout the Chinese you don't know whenthe like the white space or say theperiod here is or like the uh line breakfor paragraph heuristics all that is souseful um and I'm also jealous of saythe Europeans I know it's semi say uhBob and Eddie and they all speakmultiple languages whereas me I onlyspeak English uh so like with themultilingual dude what was your uhinterest in diving in do you have anyhesitancy on maybe the domain specificthing like debugging your Swahili modelright it would be a little likedifficultyeah that's that's definitely hard to todebug thisum first I would say there's like a bigbig need for this so me as a Germanspeaker I mean it's always nice if I goto English there are like so many modelsand systems available but if you even ifyou go to German and German I mean it'sstill like a high resource language hasa decent amount of populationit's quite powerful economically butstill the number of things you can getis like really limited and it's likereallyfrustrating to see that in English youcan build these amazing semantic searchsystems which works extremely good for Qa retrieval but even if you go to Germanthere's like hardly any models availableand then if you go further down the thelanguage letter to like more and morelow resource languages it's gettingharder and harder to build thissecond motivation ismulti building multilingual search waslexical search is like really painful soif you build thisin elasticsearch for example you need toknow the languageso you need to know the language of thedocumentfor every language you have like adifferent index because every languagerequires a different tokenizer requiresa different stopwatch list requires adifferent stammer so at the end let'ssay you want to Target I don't knowEuropean Union was I don't know 20languages you have like 20 differentindices and elasticsearch everyone eachhas like a different tokenizers demostopwatch listfor each document you have to runlanguage identification to see is it aGerman document or a French documentand then the Big Challenge comes atquery time is it like a German query ora French query or an English query forsome really short queries it's like abig it can be English and German orFrench and Spanishand so how do you query these differentindices because the Spanish query youhave to add like a Spanish index and soonwas extremely retrieval so you can havejust you have just one indexyou take your text you pass it throughthe embedding model that gives you somein settings to pass it to your Dems toyour vector DByou don't have to do any languageidentification don't have to builddifferent indices don't have to dodifferent stemming and stop work listso it's like super easy to build likedensport tree with 500 languages whichis like completely painful and possiblebecause like lexical searchhmmthat's super interesting I think the umyeah I thought the last I saw I saw astrong hybrid search results onmultilingual I'm really not uh yeah itsounds the whole separate stuff whereasI you know I don't know what the stopwords are in Korean so it does I see howthat kind of domainuh shift sounds really uh challengingthem so I I kind of want to Pivot tosort of an idea I've just seen recentlyand get your opinion on it um this thingwas called this paper was called a taskaware retrieval with instructions wheresort of the interface is in addition tothat search box you also explicitlydescribe your intent and then you havelike an embedding of the intent that youwould use to re-rank the candidates in avector search way as well so what do youthink about that kind of idea becauseyou mentioned the problem of okay Igotta understand that this query isKorean or uh what's a neighboringlanguage I'm not I'm not like a greatlanguage guy but like this idea of youexplicitly describe your intent as wellyeahmM yes so so in the beer Benchmark wehave like one one data set which isreally interesting it's called abuanawhen you want to find counter argumentsso you have an argument say nuclearenergy is super safe and then you wantto have retrieval to find like counterarguments to say okay no the energy isnot safeobviously out of the box models if Isearch for nuclear energy is safe findslike different entries differentarguments also mentioned nuclear energyis one of the safest energy sourcesso so there are the questions like howcan I intend to tell them all of myintent that I don't want to havearguments that are similar but argumentsthat are opposingand the paper you mentioned we use thisin terms of like kind of like instructorsay okay find like contact women nuclearenergy is safe or find a similarargument or find evidencesumI think yeah it's it's a nice ideaespecially if people want to build thisinto their product so so if you havelike search engine and you want to findsome arguments with supporting andopposing evidencesand different perspective so I thinkit's like an easy way for machinelearningengineers and search engine Engineersjust to prepare the prefix and say okaynow I want to search for posingarguments or supporting evidence oropposing evidence for thisyeahum so I'm kind of there's a few I kindof want to stay on this idea of well Ithink because we could go into like thekind of the prompting in the chat gbtand that idea of instructions and intentand the interface around it but I kindof want to stay on this likemulti-vector search like vectorre-ranking uh have you had a chance tosee The Colbert idea and do you have anythoughts on it this kind of like youkeep token representations to re-rank ina vector search way yes so yeah I'mquite familiar with cobornumsay a code word is a nice ideaumso far challenge with code word is likedeployment costs so the idea of cohortis like you take a text and you don'tencode to a single embedding but youtake every token and your method to likelike to an individual embedding and thisgives you like kind of like an explosionof embedding so if you take a paragraphor summary tokens it's not resultinglike single embedding but to 100embeddingsand this down the line ex that's the thecost for the vector DB to be exploded sobecause the cost of vector DB is like100 times higherand this slide's really significant soif you want to do semantic search Vectorsearch on English Wikipedia it's not toobig it's like I don't know 5 millionarticles but you pay like roughly twothousand dollars a month to run like AWSinstance on thisif this is like 100 times higher sothat's likeum like 200k months to run like coveragelike out of the box non-optimized covertso search is not not only like findinggit system but always perfect trade-offlike good systems versus costs aroundthe searchand here's like interesting to playaround with like different parameters tosay okay for which setting whichprovides like the optimal settingoptimal trade-off between cost andperformanceum and I think here finding like genericanswers is like really hard to say okayI don't know code word is always betteror Dent embeddings is better or spasmheadings is better because I think itcan also depend where you are do youhave like a small data set and onereally high quality then code word isgood if you have a massive data setuh and you're okay with somewhat goodquality then covert is probably like waytoo expensive and you you have to uselike other approaches like binaryembeddingsthat yeah there's a lot a reason I'mstill reading this paper from uhChristopher Potts and and a big team Ijust remember that name but um aboutlike the the cost trade-offs of likesplayed the starting with bm25 is thecheapest thing and then like explain andthen they have this like played enginebehind Cobra but yeah I this Generaltopic around the cost of the searchsystems is so interesting is wherealleviate releasing uh like disk a n andthe vamana thing compared to h s w thathas the in-memory thing and uh you knowthe product quantization these thingsaround uh making it run let's say CPUsgpus I'm also very interested in acompany called neural magic that's doinglike sparsity for the model accelerationthat whole topic is so interesting anduh so maybe transitioning um the lasttime I heard you uh give this lecture uhit was a really great lecture coheretalks um and you had four key points andI kind of want to walk through them andI think this would beso from uh Colbert I want to talk aboutuh sparse vectors I don't really I don'tknow if Colbert is quite a sparse Vectoridea it's kind of that idea where youhave like a representation of each tokenso I kind of want to talk aboutum like bm25s Blade the this idea thatyou have like a mass language modelinghead that projects the token into asparse uh representation uh so can youtell me how you're thinking aboutsparsity these daysmum I like the split model I'm a big fanof itum I really like it because it's likereally close to a mass language modelingso so the issue was dancing battingmodels if you train bird mom gives youlike really bad dance and batting modelsout of the box weaker than averagingword to wreckbut this mon objective display just likereally closed between like split and MLMyou could take a word you'll see okaywhat are possible synonyms hereand then you put all these possiblesynonyms for every token into like asparse vectors so here you have like areally close match between fine tuningand pre-training which is extremely niceum also they have shown really strongresults like out of domain so bigchallenge for density matting model islike new words so peoplecreate like new new things like I don'tknow chat GPTum free Trend and some embedding modelhas like no idea what it is but likeFast models they just projected to likeNew Dimensions you can search for it soas soon as you search search for it youfind it which is also a lot of issuesum topics which I'm really interested inwhich they certainly also don't solve aslike long document encoding so splitworks on a paragraph levelso so maybe up to a few hundred wordsbut in most cases you have like reallylong documents like thousands tens ofthousands of words I don't know in someindustry manufacturing centering youhave PDF documents of 10 000 Pages howto repair some I don't know machineand they're the question like how do youcan even kill these loan documents andsell it doesn't work with blade and alsodoesn't in principle work withlike like spice Packers from Guiding mythinking with these four things I'vekind of started to put long documents inmulti-discourse uh kind of together intothe same category whereuh yeah I mean yeah long documents issuch an interesting problem I think withthe chat gbt and like the dialoguemodels particularly it's making a lot ofpeople think about thisso here's maybe what I've been thinkingabout and I'm really curious what yourthoughts are on this is um is sort ofclustering the windows and then becauseyou embed the windows of the longdocument then you kind of cluster it andthen you have sort of a representativecentroid that you kind of try to packinto the context window sort of andmaybe also in addition to you know theretrieving facts and it's some kind ofprompt design to overcome the longdocument limitation yes I think herelike you need to differentiate betweenthese two things like more chat GPTwhere you operate on like some historyand in terms of of retrieval so in termsof retrievalum Big Challenge is kind of likeco-references so if you take the annualreport from Appleyou just have like apple on the frontpage of the annual report so the annualreport is located on 200 pages longand then down the line on the paragraphlevel it just says we so instead of likeapple they just we you can't have aparagraph where they write like we saw abig increase in demand for our productsin this marketand the other questions like what is wewhat is our products and what is thismarket and you as a human you know okayit's the Apple annual report you're insubsection I don't know iPhone sales inNorth Americaand then you can fill in like the blanksbut they are the questions like how doyou fetch up this co-co ref well how doyou fetch up this informationum in chat GPT challengesumhear what works so far is likeincreasing the context lengths because Idon't know if you start a blank sessionum it takes some time so if it has likecontext length of 8 000 so so right nowthe best models can do like up to 8 000work pieces that's like eight pages soit takes a time until you write likeeight pages of organization with checkGPT so you can just do massive attentionfor tokensbut probably over a long time if youreally build upI don't know if you have like a lot ofinteraction with systems we need to findways to to build up some type of memorysolike we do it with humans we know thisperson I don't know has these Hobbiesthis work these I don't know travel tothis location and then we can query thissomehow and use this in our currentcontext so so ether we have to find likeway to build up like some small memorywhich we can quicklyofficially query what we have to do haveto have select some query thing into itlike Okayumon what topic is the person working soso that I can create like a follow-upquestion on this but still that's Idon't know really I think really early Ifeel like how cana system quickly access memory andbuilds on top of this to get like newideas from past historyyeah it's I mean the whole externalTransformer memory idea is somethingthat we're just crazy excited about withweaviate and the you know the vectorindex and all that but um I really quickI want to go a little deeper into thisco-reference resolution it's it itsounds like such a problem and I thinkitcommunicates the idea well so like threeparagraphseight pages well let's so in the contextof retrieval models we'll say we canonly take in like two paragraphs asinput it's like four paragraphs to go itsaid like uh yeah apple is the and thenwe are and then we use apple so doesthat mean we maybe need like aco-reference resolution parser to parseour text before it's vectorized or do wemaybe need to create like a graph ofedges the link yeahum yeah that's that's good questionwhere we're currently doing research onthatum it says uh it depends kind of the thedata set so right now a lot of peoplework on Wikipedia about Wikipedia is cutwhat kind of borrowing because it's likereally safe uh contained so if you takea paragraph from Wikipedia I don't knowyou take a random paragraph from theBarack Obama article it will mentionBarack Obama so like every paragraphmentions Barack Obamabut in Android reports they I don't knowApple just say we and then on the frontpage you know it's Apple annual reportsyou go on page 100 here's the sectionabout I don't know iPhones a man's page150 years iPhone sales in North Americaand then you have the paragraph like wesaw a big increase for our products inthis market and so you kind of need todo like co-reference resolution acrosslike many pages so potentially likegoing back 100 pages to know okay thisis the Apple annual report from 21.and there's the question like how can wedo this because then we come back tolike a whole problem so right now allpeople in co-reference resolution aremainly using Transformer networks whichare again limited to like I don't know512 word pieces so but how can we dothis co-reference revolution possiblyover like hundreds of pages and it'sawesome I can really complex settings sothat we really knowum how to do it and yeah I think thatthe best way if we chunk to paragraphand encode them is to really rewrite theparagraph so that if you just see theparagraph and nothing else you has allthe the relevant contacts information soif I rewrite like we saw a big increasein our products on this Market in thelast year and rewrite it to Apple so alot of increase in demand for the iPhoneand the North American Market in 2020.then it's nice you can find it for a lotof search questionsuh but yeah open questions like how doyou do this rewriting especially thisrequires again to resolve long contextresolutionsyeah that's so interesting I think ofthat as like a data augmentation maybemore than a pre-processing becauseyou're kind of trying to encode theinvariance of uh the original paragraphWithout sorry the original paragraphWithout The co-references resolved andthen the new kind of like instead of weit's apple and then you're trying to saylike the invariant to these twoTransformations and I think it's veryinteresting is you kind of the problemeats itself like the co-referenceresolution models they also are limitedto the context so yeah that's a prettyinteresting thingum and then obviously we can also changelike I don't know maybe they have likesome subdivision and then we can mean Idon't know not only Apple but maybe Idon't know for Europe or so or some youhave like some parent company and thenthere's like someuh child companies as a weekend refer tothe chai company or can it refer to thewhole groupor to some product team sohave you thought about like um entityparsing entity extraction it's becauseit's a little different from the sparsesearch with the high with the hybrid thebm25 is like if the query is how tocatch an Alaskan pollock it's like it'llbe emphasizing the Alaskan pollock sortof with the keyword matching thing butthere's also this idea of like entityextraction Alaskan pollock and maybeit's that multi-vector ranking where yousearch with how to catch an awesomePollock and then you re-rank with justthe like Alaskan pollockumyeah I mean that the researchers stillreally are Valley in this field soum big limitation as so far data setsum a lot of data sets are built onWikipedia which does not really requireto take a lot of contacts into accountthen the question is it just entities oris it also other things what an entitiesso that's also likelike an open research question is itjust fine to resolve like we and thisand last quarter or do we have to alsoresolve other things and yeah in myexperience often in these I don't knowthese these examples you make up like Idon't know we saw a big demand it's kindof easy but then if you go to actualsentences you say okay it's like a lotmore nuanced which context informationdo you still need to understand thisparagraphum maybe context information can alsomap to like a table with financial datait can map to some graph and then thequestion is like okay maybe they havelike some pie chart and they'rediscussing the pie chart how can youcontextualize the pie chart into thisparagraph so that if you just encode theparagraph have all the necessaryinformationand without running out of context linksor Wetlands againyeah that's another really interestingtopic I think there's so many things totalk about but like that idea of um thetables and the graphs and getting thatinto the text domain it also it kind ofreminds me of the same data augmentationpre-processing we discussed with theco-reference where where you can parsethe tables like I've seen this paper onuh it's like learning to reason acrossWiki tables something some kind of likethat where they're parsing out thetables to go from table structure totext structure uh and then because thenyou kind of unify the domains withouthaving to have like a visual uhcomponent to it yeah anyway so umanother topic uh kind of a finishing onthe four things is umuh so the topic of uh unknown words anddistribution shift um maybe kind ofmaybe actually those are kind of twotopics withum because uh maybe we'll start with uhdistribution shift because it's soexciting so uh I recently read thispaper that was really interesting calledOod disk a n where they're describingthe distribution shift and if you buildup an image index and then you'researching with the text queries they'regoing to be out of distribution for theh s w practice or the proximity graphstructure is the vamana but and thatkind of idea so uh how are you thinkingabout distribution shift these days yesso distribution shifts areinteresting on multiple aspects solanguage evolves so so that's bigpain point of language or nice featureof languageyou see that things evolving and meaningso I don't know like Corona before 2020it was like many beer then selling in2020 if someone talks about Corona it'sconnected to a virussimilar if you talk about like alphabetor amicron it used to be some Chineseand some Greek characters maybe relevantformat method mathematical questionsnow people talk about yeah there's likea new Omicron wave in in Europe or stufflike that or there's a new Army Corpsenvironmentwhere it's like extremely hard toto really update the models hereand a big challenge is alsoum in terms of like first stage andsecond stage review also so if you takean embedding model that's like no ideaabout Corona virus maybe on new trainingdata you can update the model thatCorona now has a new meaningbut then you come back to need to thinkabout like do you need to re-encote yourwhole Corpus so you do you have to takethe embedding model and re-encol likethe whole Corpusif your Corpus is more like a milliondocuments it's okay what if your Corpusis the web like Google or Bingyou have to re-encore like all documentson the webjust to create like the new embeddingsand so there's like an interesting thingor question like okaycan you make this a bit more efficientso which can you kind of liketransform the embedding so do can you dolike some partial updates so do you canjust update that Corona now has like twomeanings the beer and the virus and canyou find like the documents you want toupdate but everything else you can keepthe sameumbe able to decide when do you do theupdates or when do you train again thisnew embedding model and so onum so so many many open on soft researchquestions here right now people do likethe most stupid thing they completelyupdate their betting model and thenencode the whole Corpus againeven such that 99 Prime percent arecurrently attached from from the updateso so exciting research questions toknow okay what do you need to update andhow can you update this and also therethe interplay between first and secondstageso maybe it's okay to have like anoutdated free stage retrieval that doesnot really know what is Corona but thenhave like a more recent up-to-datesecond stage retrieval that and alsoCorona can alsouh Nina virus now and not only beer yeahI thought what you just said wasincredibly inspiring uh because therewas this argument about with the wholelike uh we V8 augmented gbt thing whereit's like um you know the big languagemodel doesn't need it can be updated forthese new facts by retrieving it butthen it's like well what about theretrieval model and and it that and thenthere's that problem of you retrain theretrieval model on the new informationlike the new omnic uh omnicron uhvariant and then um and then you have tore-encode and the whole document and ifyou have like you know hundreds ofmillions or whatever and we thoughtabout this idea that is kind ofinteresting that I'd like to pitch toyou as well as like uh like maybe wecould use that proximity graph structureas of prior to try to propagate therepresentation space changes and andthat waywe can maybe only re-vectorize like 10000 documents and it could be they couldbe kind of cleverly selected with theirkind of centrality in the proximitygraph and then we could maybe have somesort of graph neural network that couldpropagate representation changes I knowthis is too grandiose of an idea of uhwhat do you think about that ideaumI think they're there you quickly run inthe problem or the challenge to knowwhat has changed so so let's say whathas changed in 2022and then maybe for some top things youcan name itbut it's like really hard in the longtail to say okay what has changed myother like new and some new online gamespopular online games they're like somenew patch which introduces newcharacters which have like a new acronymand now and one refers to them as like adifferent acronym or soso I think that's likethe the challenge like really what haschanged between now and the year beforeand to really enumerate this tonot only say okay not only like I don'tknow Corona change the meaning but likereally in the long tail what has allchanged the meaning and where do you allneed to do the updateum I think that's that's the biggestchallenge how to automate this and howto find itum also in terms of preferencesum if Isearch for like how can I get Corona intheoryhow I catch the virus and how I catch orhow I buy the beer should both berelevant so if you ask like an annotatorthey should both be annotated asrelevantbut probably if you see a search in likeCorona and people say how do I getCorona you probably want to rank thevirus higher than the beer and thenmaybe hopefully in a few years you wantto rank the BR high againbut thereyeah I also the question like how do youbuild up this popularity so it's notonly relevancy in search but also inpopularity of terms likeum nft popped up at some point fornon-fungible tokens so if someone searchfor nft we're meaning unfundable tokensbut there were meanings for nft beforeso so there's like a book series Not ForTourists also every related with nftso how can you learn okay there's like anew meaning which is a lot more popularalso crypto I started myself to work oncryptography so crypto 10 years ago youreferred to cryptography then Bitcoinand and uh and so on happen and now ifyou talk about crypto everyone talksabout like Bitcoin and stuff so yeah Ithink um in this well yeah this idea ofkind of updating the unknown words itsounds it sounds pretty difficult and Idon't even have anything to kind ofpropose but that's kind of why I alsoreally like this intent stating as wellin the search engine interface where uhhow to catch Corona and then maybe ifyou just said your intent like I'mlooking for beer I it's like a differentway to search and it's more verbose thanusual but if it gives you better resultsmaybe especially especially with likethe kind of the idea that searchinterfaces are moving into likegenerating you a long answer instead ofjust necessarily returning documents uhand maybe you'd be willing to put moreeffort into your query I think generallythe search interfaces uh thing isevolving um yeah maybe you kind of yeahI want to ask you a question questionabout search interfaces really quicklike umjust a second one on this intent I meanit's kind of old ideas like I don't knowbefore that it was named query rewritingI mean we as a human do it like I don'tknow if I search for Corona I get thewrong results I search for Corona beerso we already trained on thisum I search for like how to sort a listand I forgot which programming languageI add like python to it and then also inthe back end like I don't know even whenGoogle was created was smartly addingquery writing to it so when I look for Idon't know like an Italian restaurant itwas adding smartly in the background theprogram location I'm in and we'relooking for like Italian restaurant intimestarter soit's kind of like an old technique thisthis query rewriting heading trying toguess the intent and adding ityeah that's super I I saw this likelearning to reformulate queries withreinforcement learning where the actionspace is are the keywords and then youcan add them subtract them boost theimportance of them uh yeah yeah it'ssuper quiet I guess I'm kind of uhlosing interest in that idea due to thechallenge of maybe a reinforced andlearning query writer it sounds like alot of overhead in the in the searchpipeline thing yeah that's trueyeah so interesting uh so anyways Nilsuh thank you so much for joining theweekday podcast I think we covered somany interesting topics I'm reallylooking forward to re-watching this andyou know rethinking about all theseideasum thanks so much for your contributionsto search broadly yeah and I'm reallyloving these uh cohere talks you'regiving I think you're doing another onein about an hour and uh so I'll see youover there and I'm really lookingforward to it and thanks again greatthank you so much for being here bye bye", "type": "Video", "name": "Nils Reimers on Cohere Embedding Models - Weaviate podcast #33", "path": "", "link": "https://www.youtube.com/watch?v=TKJh2H5Prcs", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}