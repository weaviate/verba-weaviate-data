{"text": "Hey everyone, thank you so much for watching the 54th Weaviate Podcast with Andrei Bondarev!! Andrei has contributed an ... \nhey everyone thank you so much for watching another episode of the wevia podcast I'm super excited to welcome Andre bondarev Andre is uh works at source Labs a consulting firm and he's recently built a ruby client for wegate and Lang chain so I think this will be such an interesting podcast diving into you know the the nuances of uh AI software Consulting with these new waves and deep learning tools like you know Lang chain leviate and obviously like the llm apis and all these things so Andre thank you so much for joining the podcast thank you hey super excited to chat with you today awesome so could we kick things off with uh Source labs and sort of what you're seeing in the kind of uh Consulting space with the new uh AI tools sure uh swear um sorry we're on a small software development firm uh it's called Source slots uh so we build custom applications Technology Solutions for uh small medium but also large Enterprises so we work with both uh uh venture-backed startups uh we worked with Y combinator uh companies Tech Stars uh accelerated companies but also with uh all the way through Fortune 500 companies wow that's interesting um I haven't heard too much about like um this is kind of like a like a like a stat I've heard of like Staffing contracts where people need to build some like say you need to build a weba database so you would you know a start up like a y combinator startup would hire these firms to like quickly build the new technology can you maybe walk me through more about like what the business of that looks like sure so we don't uh we we do some staff augmentation where companies just want to pour more fuel basically on the fire and kind of accelerate the development of their technology or uh the execution over technology roadmap but we also love to do Greenfield projects and and kind of come in and assess what the problem is what are the outcome what are kind of business objectives you're trying to reach and and how to how to get how to get you there um and we've been uh this year we've been especially hyper focusing on AI uh ML and llms of course yeah I'm super excited to dive more in into the AI the new kind of Stack that's the emerging but quickly could you tell me like um like maybe for you know entrepreneurs listening to this podcast like how did you get into this kind of space yeah so actually uh we just finished executing on a large uh project for a client uh I'm not going to disclose who the client is but it's a uh it's a well-known public company um and they have a set of tools um that they built um that provides the data analytics layer on top of public policy both National and global um all around the world um so public policy like laws rules regulations Etc um and we just concluded in a massive project where we rebuilt our whole kind of application and a large part of it was rebuilding their search engine uh and so we built that in um uh in the last six surge um and we did both the the front end the back end work the setting up the data pipeline configuring the indexes uh the apis ETC and um as soon as we finished it uh late last year um I kind of uh came to a conclusion that you know what we had built is essentially kind of outdated now um and this was uh hap you know and as as the vector search database has kind of started uh being uh very popular in the market um you know I figured that um building a purely a keyword search uh is uh as a as a core feature of your product is no longer very attractive I mean uh when you're building a search experience now uh you need to make sure that when a user interacting with your system your large data sets um that the system also understands the intent the the meaning and what you call the the semantic search uh and this kind of led me down the path to uh explore the the current capabilities the current systems the vector search databases uh and I I discovered weeviate and prototyped a couple kind of pet projects with weavate uh so I love hearing that background I've learned so much from like uh the people at open source connections and that that whole Community around and uh Byron vorbeck has recently done a talk on from keyword to Vector at Berlin buzzwords and this kind of evolution of the uh the way I understand it is that elasticsearch was quite a massive thing and you know a lot a lot of Consulting developed around helping people Implement that and add key research and now this this kind of the shift to Vector search and yeah I've done all that so interesting I love the The Mention Of You know the semantic Surge and fuzzy meaning intent understanding all that kind of uh like abstract stuff but yeah so quickly I think kind of staying on this topic um with your transition into Vector search what was kind of like your Eureka moment like what got you over there um again uh some of the some of your products I've I've prototyped um it it seemed like um you know I didn't when I'm searching through a large data set as I mentioned um I no longer have to search for exact keywords uh exact keywords that are found in in those data sets right I can I can search for synonyms and similar uh semantic Concepts right similar Concepts that are uh Concepts that are similar in meaning um and get to information that that I need uh and I've really enjoyed I've really enjoyed we V8 uh so I've looked at a couple different solutions I really enjoyed we need like I love the module system um so I kind of I I like how it's uh it's an end-to-end solution um and there's a lot less uh there's a lot less Plumbing that you have to do as opposed to as opposed to some of the some of the other Solutions and um and uh my my uh tool of choice although I've played with a lot of different languages throughout throughout the years throughout my career my tool of choice uh has has always been Ruby uh I love the language I love the ecosystem and and um kind of my my journey into the vector search and the open source work uh uh was kicked off by by me just writing uh a uh we V8 uh API client in Ruby uh that's super cool really quickly before I obviously we're gonna dive into Ruby and that's so interested in the client um so it sounds like you know you're exploring these new tools and I love to mention the module system because I think it'll tie so nicely with this as well as I want to then understand uh how you like came into Lang chain and sort of your early thoughts on using that yeah so obviously um it's not a I I think the space is so um is so young right um and people are prototyping and and uh people are doing a lot of prototyping and exploring different approaches um it's not exactly a plug and play just yet right so some of the things you need to think about when you're indexing your data into a vector databases like the context window for example right and how do you determine the perfect uh context size how do you split up your data right A lot of times you can't just split on uh a certain uh character uh uh limit threshold uh because you you kind of lose the the semantic boundaries right so for example uh you know a lot of times like when you're searching for an answer uh an answer might be a combination of the different different chunks uh different chunks of data so so there's that problem it's very chunking um and uh also there's a lot of kind of underlying configuration uh that needs to be set up when you're configuring the the vector search databases like the the uh the what what kind of embeddings you're using uh what kind of uh what kind of formulas you're using whether it's cosine or dot product to kind of calculate the uh the similarities yeah I think uh something like um well firstly I think I'm curious about the text chunking things and the solutions to that offered in Lang chain sort of I know about like the recursive character splitter that particularly I think it's you know like delimiters that are cooked in like if you're splitting up python code or stuff like that or markdown where it looks for these particular symbols and and then just has the 512 tokens but uh is there more and you know it for maybe a little more context in the wevia podcast we've had Brian Raymond from unstructured who's presented this idea of you know using these like visual document parsing models to maybe extract the structure of like you know this this chunk of text is an image caption these kind of things like like what are kind of the Innovations in text chunking that are that you're seeing yeah so you can actually so one approach I've seen uh and you know I think I think folks are coming up with new approaches uh weekly uh one approach I've seen is where you would actually feed some data to an llm and and app and prompt it to suggest the chunks based on based on uh the meaning that that body of text represents um also uh you know one approach that we're exploring within uh the project that uh we're we're going to talk about shortly uh link chain RB is is actually splitting up on uh on a custom function so for example passing passing a Lambda passing up function and then writing your logic uh that would split the text um when it's being parsed right so one example I'll give you is for example uh if you're importing a legal document right and it has several different Provisions uh you know intellectual property non-compete in terms of service uh Etc so for example you can uh uh you probably want to split that document by those specific sections and it's not you can't configure a set character chunk uh but you could say that uh as the text is being parsed right if you encounter this string which is uh an enumerated heading of that contract section then slice it right uh so that's another approach yeah I think we we get lucky with like web pages where you'd have like that you know like angular brackets slash UL or at the end of a list and then you know right don't split this list into two separate chunks for embeddings but yeah as you mentioned just kind of detecting like that kind of shift from free text like if it's um yeah like laws and it starts like the new the title of a new law and so you know and the chunk even if it was only 100 tokens then it's just not just like a 500 token window and so yeah that's super interesting and I think one more topic on Lang chain I think actually it makes sense to kind of cover this before coming into the Ruby the Ruby ecosystem particularly is so with Lang chain you mentioned um you know the configuration of the embedding models the configuration of the vector databases uh so so all right my understanding is you're seeing the value of a framework like Lang chain as it lets you you know rotate out those kind of things is it it's like the I don't know what a good word for that is but it's the orchestration behind Okay cohere has a new embedding model so switched out with open AIS or you know alleviate you know want to tweak something here so you can easily tell me more about how you see that yeah so I I think there's always uh there's a lot of benefit about uh uh about using an opinionated framework right and actually again we're going to get to uh the Ruby World shortly uh a large uh part of Ruby development is uh Ruby on Rails right so building uh web applications uh apis uh in a extremely opinionated framework right and that's and and that's and that's uh that's why and how it uh it took off and became so popular because you didn't have to think about all these low-level decisions right um the whole approach was that hey we've you know we think these this is the best way to build 95 of the uh web applications you know it covers 95 uh of use cases it fits 95 of the use cases uh and and this is the opinionated framework that uh I think would work really well so um so they so I'm drawing a parallel to building llm uh based applications is that I think uh developers uh can also benefit from this opinionated approach yeah I think that's the um in our podcast on llm agents um uh Colin Harman was describing this idea with like symante Colonel and how it's a little bit more like decided on you know these are the these are the abstractions and and rather like whereas contrastively uh llm framework like Lang chain is a little more open-ended where it's a little more open source Community Driven less of um you know a rigid consolidation of you know these are the tools to use it's more the discovering yeah I think that that whole thing is a really interesting uh just thinking about like what is a software framework generally as like kind of a market category what kind of thing does it strive to do I found that learning about that is all really interesting I'm super cool so I think it would be great to dive into Ruby and I'll try my best to keep I know a little bit um yeah so as as I've mentioned uh Ruby has been uh a tool of choice um of mine for a very long time I've tried uh lots of different languages uh and kept coming back to Ruby um a lot of people talk about developer happiness um uh and I will also mention that uh It's always important to use the right tool for for the job and and I found that uh the problems that I that I was solving that uh Ruby was was was uh great at solving those problems and so um Ruby's still very popular um there's still a lot of companies all around the world using Ruby uh they're building they're supporting maintaining existing products or starting and building brand new applications in with Ruby uh obviously Arabian rails is uh huge within within the Ruby community so um I thought that as as a ruby Community we ought to have uh a solution for building llm based applications uh because um as as this um AI wave uh has been kind of rolling in um I think product managers are coming to their engineering teams now and are asking hey we're aware of these capabilities we'd like to add some of these capabilities to our existing products um here's a business case for it how can we accomplish this right and then as engineering leadership team leads Architects uh uh they have to decide whether you uh add python right as kind of the most popular language within that space in the mix um and if you do that you need to you're spinning up additional containers additional compute uh separate Services right you have to figure out whether you want to support additional complexity additional overhead and also whether you have capabilities on staff to do that um or uh if it's a ruby shop um whether you'd like to uh use a solution that your engineers would be familiar with right um and I think there's also and this is this is a uh uh a larger conversation I think there's also um kind of uh uh people are reflecting on and on all of these complicated uh microservices architectures now um uh this this path that uh we as uh the software development Community have uh gone down uh people are looking at all that complexity and and are you know thinking twice now before uh before doing that um so I I see a lot of people kind of consolidating uh their architectures into into a monolith uh into a model of uh application architecture because it's slot um uh a lot of times it's a lot easier to manage so taking it apart a bit because I don't understand this too well so a microservice architecture is where you like containerize every function and turn it into like an API and then it and then the system interacts with it that way and then that way if something's in Python it's you know it's just a container with an API so it's like whatever compared to where a model is where the whole thing is in Ruby so you put it in and it's all one application right right right right uh another way to to think about it is uh the microservices architecture is when different parts of your application um are strung together uh uh by Network boundary basically have Network boundaries right so from one component of your application to talk to another component of your application uh there needs to be a network call right uh and you know it needs to be a request for example Over The Wire um whereas a monolith application is where different a monolith architecture is where different parts of your application different components of it are are hosted within a single uh with within a single kind of memory unit so then you know where with my deep learning hat on I'm thinking about like model inference serving and so are you able to I know like they so I I know a little bit about like bindings where you could you know like you can have the inference in C plus plus and it can have python bindings to it can you so can you do that kind of thing to run inference of a deep learning model within Ruby so that the inference lives in the same application um so any any of those libraries where python wraps uh underlying uh C functions uh you can theoretically rewrite those same libraries in Ruby um and and call into those same C libraries um you know does it does it make sense to do that uh you'd have to think really carefully about that yeah it's so interesting I mean I guess I'm just thinking about like um I guess like some some things with weeviate it's like I think about if we could have like a model inference directly in the database compared to doing networking requests like if we would want to just like to throw out an example like if we wanted to do this kind of like re-indexing idea where you have a model and now you fine tune it and you have a new model and you want to update the vector space like if we could do that inference directly and we get that's what kind of motivates me asking this question about like when you would build in model Arc model inference into some kind of software architecture compared to uh you know like API requests across the wire and uh I guess it's like where my head's at with that kind of thinking but uh pivoting topics a bit I'm really curious about like clients in general like you know a ruby client on top of a database how do you think about like database clients um well uh so for example in in uh if we take if we take we V8 right um so you guys currently have a uh an arrest endpoint and HTTP arrest endpoint um and a um for schema management um also indexing um and largely the capabilities for querying the database are uh there's a graphql layer in front of it um and I don't personally find graphql to be super convenient uh in in terms of constructing queries um I think Json just just feels a lot more intuitive but also um more in line with kind of the data structures um that are native to different programming languages so you know actually and that's that that's exactly the reason why um I uh initially wrote the the Wii V8 uh Ruby API client is to abstract away some of those complexities like communicating with a graphql client um so so because like I know with python it's like you get back the request then you parse it where you're like you know data get so like they're they're you you instead of have having to unpack the the Json that comes back from the graphql you already kind of parse it out and then and and then you just create like a syntactically nicer kind of interface to the database is that generally the idea right right correct so I think the other topic uh is you know I understand very little about this Eddie and explained it to me and I just kind of nodded along in the podcast like this difference between uh like graphql rest API grpc apis like like what is really the difference I I just kind of for me I just think about it as like what the input output looks like yeah so it's it it's basically different uh communication protocols uh so grpc there's a couple different Frameworks uh I'm personally most familiar with Apache Thrift um and um it's it's um it's quite flexible um you can configure uh you can configure uh your transport layer uh to send data in lots of different formats uh binary uh Json uh Etc um I find I've found that it's a little bit challenging to debug actually um and it's also there's there's also um an overhead in in maintaining it because basically the way it works is is um uh kind of the the API server the API host declares an interface um and then you compile that interface into um uh client libraries and so you need to make sure that all of the clients that communicate with that um without server have the latest version of those client libraries so there's a little bit over overhead in terms of making sure that all those client client libraries are synced um but there's also there's also benefits to using a uh jrpc um to string Services together um you know I think I think um uh the obviously the the most common way of stringing Services together is still um is still uh HTTP uh Json apis um and I find that to be uh most uh most convenient to kind of program against maybe just like a quick thing out of I know extremely little it is there more opportunity to kind of innovate on that with respect to like because I've heard about like streaming Technologies I also don't really know too much about what is under the hood but from the perspective of you know like vector database your your trying to send your data to this embeddings inference API as fast as possible and get the vectors back or you know say you're sending prompts and you're getting the response back from the large language models is there like a lot of opportunity to kind of interface on how you do that networking um in terms of streaming data from Vector search databases I I suppose just I'm broadly asking about the technology the whole like the whole entire networking technology category and if this particular like you know how how we're currently thinking about you know how apis provide the Deep learning embedding and then language model inference Services if that is already as optimized as it can be or if there's room for yeah yeah it's a good question um I know um the um large language models a lot of them um uh I know open open AI does this uh you can you can stream completions right you can stream chat completion and so it's sending you it's sending you tokens as it's producing them um uh within our open source uh project we've added uh We've added uh support for uh a couple uh a bunch of other uh LMS uh so for example we support Google Palm uh I haven't I haven't seen assuming capabilities within Google pawn yet uh although it should definitely add that so I think um so I think VL alums themselves are um pretty optimized um obviously when you're uh when you're adding a lot of data uh when you're adding a lot of data to a vector search database uh I think there's an opportunity to stream stream that data yeah um as as uh as is being indexed um but I'm not um I'm I'm not sure what what's some of the other kind of uh opportunities are yeah it's it's really interesting I think that whole category of model inference just the whole all the software behind it whether you're you know doing the Onyx optimizations of the model or like you know the Triton servers neuromagic is interesting or say you step into like the distributed computation you know area where you have like Ray and Spark and I think there's so much to that kind of topic um yeah so so kind of coming out of this topic a bit I I think it would be really interesting to just kind of talk about to transition back into just the meta around the evolving software space can you talk about like your experience in software and how that's changed with the latest you know breakthroughs in large language models and Vector databases um yeah so um one um really big use case uh that also I've referred to earlier is obviously uh search experiences right so um building the keyword searches uh no longer um adequate um so I think we ought to um offer uh in a lot more uh kind of extensive elaborate uh search experiences right so semantic search um chat Bots um are uh obviously becoming extremely popular um so there's a lot of there's a lot of things and workflows that could be in business processes that could be automated there with software um and and also uh uh um yeah doing uh q a over your data sets uh so for example in in our uh link chain uh RB project uh we have um uh We've we've built an agent that does a Texas SQL search so um you know previously uh previously there that there's a whole set of tweet there's a whole set of tools that uh data analysts uh used right to make kind of interfacing with underlying databases much easier and I think um and I think these llm based capabilities make it uh even more so convenient to do that yeah I love that you brought up the text SQL thing it's so interesting to me like um you know just seeing the trend in like you know like social like entrepreneurs on social media like I remember you know there'd be a lot of like here's how I started my social media agency you know I was like I make 20K a month at 17 with my social media agency and it'll be you know and it's like oh I help businesses like run their Twitter Instagram and that kind of thing and recently I've been seeing on you know on YouTube with this kind of thing it's like oh I'm 19 years old I make 100K a monthly with my um with my AI automation agency and this kind of like chat bot and is like the skills that you need to build these kind of systems are are becoming so different like or you know it's like no code is finally like really a super serious thing do you think like um and I think there's even kind of this is a topic that I think about like all the time is this kind of like zero shot uh AI where you don't need to train it at all and so the you know you just kind of plug it in and you know you can be 19 years old and set this up uh like do you think like that kind of thing of like training the models compared to just sort of plugging in the zero shot LM zero shot embedding models setting it up that way what what's your current stance on that so first of all I would I would uh I am always a little bit of I I'm always a little bit of uh skeptical of uh YouTube influencers but claim to claim to make uh uh 100K a month so but I you know uh back to your question I I think you have to uh you have to push off of um your uh product requirements uh in in business use cases um so there's a lot of times where a generic they call them found foundational models right there's a lot of there's a lot of times where a foundational model is either is either an Overkill um and really you could just get away with uh a simpler model doing basic classification tasks um so there's a lot of times where foundational model is either an Overkill uh or it's not best suited to um to uh to accomplish the things you'd like to accomplish um so you know the answer is uh it it really depends yeah I mean um again referencing the unstructured podcast with Brian Raymond he discussed um building a foundation model for visual document information extraction and I it was such an interesting idea I hadn't considered this idea of like a task specific Foundation models but it may like I'd always kind of thought of this idea where you have this algorithm called knowledge distillation whereas like you use the large model to label data to then train the smaller model you know is optimized on that data and the key idea is instead of humans labeling data where you have like a one hot Vector label distribution that you optimize towards you have this like soft distribution over the class label so you know it's like 0.97 0.001 it like produces this distribution that lets you distill the model but so like that's kind of the technical algorithm behind like compressing the models and then you know but just I guess we're seeing this take off where it's like regardless of how that model is produced like what I'm saying is maybe it's you know gbt4 distilled into the a visual document compared to where you just train that visual document thing from scratch and then you serve it like do you think that could also be an emerging startup category is like Foundation model for you know I don't know I can't I guess if I like creative but like some particular thing like maybe it's like like visual question answering has its own uh you know its own Foundation model it's not gbt4 and then it's smaller or something like that yeah yeah no absolutely absolutely and there's there's a lot of uh there's a lot of companies that are trying to do exactly that um and offer uh task uh offer um apis um that are that are specific for specific tasks right so whether it's summarization whether it's editing whether it's paraphrasing um whether it's chat um and uh yeah I think I think there's uh I think there's a lot of opportunity there yeah I just remember this as I passed the mic back I I was in Berkeley two weekends ago and I saw these researchers working on this language model called gorilla and so gorilla is like a large language model optimized for Tool use and like so I think especially as the creator of Lang chain Ruby I think this would be a great topic is like how do you see tool use like just this whole topic of llng using tools and how open-ended that seems to be yeah so when we when we talk about tools um we're also talking about agents right um and I I believe that the current consensus is that um the the current agents um for kind of General problem solving um are not very powerful right um so there's emerging techniques um but uh the current conceptualization of agents and and the current techniques and the way they currently work um uh they're not super useful um so so obviously there's um there's a Chain of Thought there's uh uh so for example in LinkedIn RB we have a react agent um um there was another research that uh came out there's another research paper that came out recently uh called uh tree of thought uh which is uh which is kind of an evolution on top of the chain of fog technique um and uh an agent uh coupled with uh with tools uh is basically uh exactly what kind of open AI offers with with its plugins um but I'm not I'm not sure uh I'm not sure that the uh I I'm not sure about the agents are uh super useful just yet so we got a lot of that I want to unpack I want to start with the well okay so we're definitely coming back to open AI the plugins and now we have this new functions API and that's so interesting then the you know hot today I meant I meant the functions yeah oh I think both those things are I think that's just that whole thing and seeing how openai themselves are clearly like exploring it as well it's like a whole I think this is one of the you know evolving things but uh so I quickly want to come back to that reference of building it in Ruby so when I think of tools I think of like you know it uses like a calculator or it uses a code executor so like and then we talked about monolithic architecture versus microservice architecture I think of these tools as like a micro service architecture correct like where you know the email API that you know calendar these are all like API so so would you cook these tools to access into Ruby so like the calendar the implementation of a calculator is it stuff like that is that I don't I don't know if I have the great example right now but maybe you do yeah yeah well so so so one example that's uh mentioned in the length chain are uh length chain RBS uh readme is uh this uh uh example of uh prompting uh prompting an llm a prompt using the react agent uh to calculate the distance between New York City and DC uh uh in in uh in the number of full-length soccer fields right and so uh and so the way uh and so you're asking uh an llm to break down the steps and generate uh generate uh a set of actions right so reasoning and act um uh generate a set of actions uh and action inputs um that it would uh it would need uh in order to arrive uh to the answer right and so it might say that the first action uh that it needs to do is calculate the distance between um New York City and DC in in miles right and so if you expose it a tool to be able to do that right um it will uh it will try to use that tool right and so you obviously have to execute that tool in in the code directly right so it would call uh you would call the Google Maps API and ask it for to go to send it back uh the distance between uh DC and and New York City right you would then take that observation that result and send it back to the llm and ask it for the next step right and so it would say well now I need to know what the what uh what the length of a uh full-size soccer field is right so you would call some sort of service uh that could give you an answer to that question uh you will then take that result package it as a as an observation send it back to vllm right so now it has two pieces of data right and now it just basically needs to you know take the overall distance and divide it by uh by the length of a soccer field to calculate the total number of of fields needed right to cover a distance um so that's so that's an example of tool usage yeah I think what you're saying is inspiring something I've been thinking about as well with um with go Lang [Music] so we Face written and going and um golang is really good at like uh you know handling concurrency control with a ton of these parallel API requests and like one thing that's built into alleviate weave has the generative search modules with which what they do is you know they take say you search a query and then you have you know let's say 20 search results and you want to send each one in parallel to the openai API it's really good at like you know synchronizing all those requests and then and I think I haven't I don't have a super concrete example yeah but as you mentioned the kind of like self-ask prompting and question decomposition I suspect that there is something to like when to lock these like uh asynchronous kind of API requests and you lock the shared resource so that you don't override it and so I do think that kind of orchestration there is a lot to that and I think using languages like going and I don't know what Ruby has with that kind of maybe you could talk more about ruby and how it handles concurrency and that kind of thing or I don't know too much yeah I mean short short answer is a lot of these uh high-level languages uh don't handle concurrency all that well um you know python Ruby it's kind of a similar story so interesting um yeah yeah I mean I'm definitely not a programming language I only know like golang because of the situation in which but yeah uh yeah so that whole kind of thing um yeah so maybe we could talk a little more about the orchestration behind particularly you mentioned the chain of thoughts tree of thoughts it sounds like you definitely need to have a lot of um you know like templating in place to kind of guide the apis to do that kind of thing especially I mean well Chain of Thought to me is just where you tell it to like break your plan down and or or the self-ass prompting that you just mentioned where you need to take a question and then say do you need to decompose this into two questions and so on like how do you build the build a template around that to constrain it to follow this kind of behavior yeah so we have uh we have output parsers um specifically specifically for before to uh uh to ask for a specific uh ask the lln for a specific type of uh output right so if you want to get uh Json back you can supply your Json schema and say Hey I want you to fill out this Json um um uh and uh we have uh prompt management um so you can create your own templates uh you can fill out the templates you can create new templates from scratch you can save them uh so we're currently support yaml uh and Json uh formats um and we also uh we also uh look at uh we constantly have our eyes on the emerging research to make sure that our prompts our templates are using the best possible techniques right and there's and that's that's that's that's what the you know that's kind of the gist of uh prompt engineering is being able to construct these uh concise yet detailed clear instructions for llms to give you the output that you're looking for right yeah I love that topic I think um like the discovery of prompts that work well is pretty interesting I mean I think like people obviously kind of Snicker at that a bit the idea that it's like engineering to be doing that kind of tweaking but I think I kind of respect calling it engineering when you're talking about like um you know like the the way that the models switch and the new the you know like gbt4 might react differently to the prompt and gbt3 does or you know cloud or you know MPT 30 billion so that maybe there's some kind of compatibility to the prompts and the models or yes what do you think about yeah and is it engineering yeah yeah so I think I think um that's that's that's a it's that's a great question and I think the fact that we call it engineering um actually points to our yearning for prompting to be a much more precise discipline right um I mean current prompting is kind of a hack right um because it's it's I mean humans have a hard time communicating with each other right in in coming to a consensus of problem solving and um right and it's and uh when you're when you're interfacing with a system in in natural language uh it's it's difficult to get it to do exactly uh what you wanted to do right um and so and so and so I think when we call it engineering it's because we wanted to be uh much more precise um and which which is why there's a growing concern and kind of um area that people are exploring to to evaluate the effectiveness of different prompts right so being able to assign different scores uh to input output sets right um also referring back to the uh the whole uh you know tree of thought uh research so uh you get a model to produce uh different intermediary steps and for each step you ask it to produce different samples and then you have to rate those different samples like how How likely is this sample to get you to the best possible answer right um and so there's there's evaluation there's kind of prompt evaluation uh input output evaluation taking uh taking place at at a different uh uh uh at all the different uh layers of the stack um and I'm not sure I'm not sure anyone had uh had solved this uh just yet um but um yeah I think that's that that's why that's why we're referring to it as as engineering um yeah I love that like with my um like my experience with doing a PhD in machine learning where you're you know collecting all these metrics about the experiments and I think there still is a lot to that like um you know how you do the input output evaluation deep learning has like a long history and like these benchmarks like imagenet classification or like Ms Marco information retrieval where you or like Squad question answering because I think question answering is a good one that people are thinking about when you like think about these kind of things we have like just collecting examples and then you know looking through where it fails and seeing is there maybe you like visualize the embedding space of all your inputs and use and you color it like green and red where it's failing and then you look in this distribution particularly it fails can we do something about that and that kind of generalization testing thing with respect to all you're doing is tweaking the prompts instead of like thinking about some whole another algorithm of training like I think mostly it's been like you know the generalization testing is done to compare two models trained with like massively different efforts right compared to this earth like just two different prompts and and cannot have a profound difference in the test Suite right right right uh and this is this is also a an area of concern in in uh um in application testing right so um you're testing input and output and especially if you're if you're not using a uh 0.0 temperature setting right which which means that uh you're you're going to get different results right um and how do you account for all of those different test cases in the application um so we're uh and this is one area that we're currently thinking through uh as as we're building out link chain RV awesome super cool Andre I think this is a really great coverage of these topics I think kind of a wrapping up question it's I'm really curious about sort of maybe the Breakthrough in the next breakthrough in AI deep learning that you know would excite you the most it sounds maybe like you're an agents guy and you want to see the something with agents or maybe just the llm inference costs going down what kind of things excite you oh that's uh that's a good question um I think I I would love to see the open source models catch up to uh open open AI right um it seems like it seems like gpt4 uh and uh open AI uh they're the leaders uh in the space right now um and obviously there's uh there's an emerging uh field of Open Source models right um some of them are getting really good um it's also becoming much much cheaper and uh kind of cost effective to run those models locally so I would love to I would love to see open source catch up to some of the commercial offerings so that we can we can use much more powerful uh llms and run them locally yeah or on on the on the cloud hmm yeah that topic is I mean it's super timely with as we're recording this podcast Mosaic ml has just been acquired for 1.3 billion dollars from data bricks and you know super lucky to have had Jonathan Frankel on this we be a podcast twice of you know not that the collaboration has to end now or anything but you know we've just you know been working with the Mosaic ml people they're super talented and and it makes me think a lot about this open source thing because I was so excited with the MPT 30 billion because I was also kind of like thinking like this group of people I you know I think they they're super talented I think they can do it they'll create the open source version of you know gbt and or drive the cost of it down all sorts of interesting things um do you think like with like if with them going to databricks what do you think kind of the predictions of what they would continue to do yeah to be honest I I as as you know things are moving so rapidly and there's so many people in the space right now um doing very interesting things I I have not had the opportunity to play around with uh mosaic if you see that oh okay yeah I mean I I don't mean to say I guess like MPC I mean there's definitely like a bias towards what the latest thing is and that changes every single week like one week exactly yeah exactly yeah I mean I do think uh databricks had published I think their model was called dolly or something so I do think it's quite realistic that you know they just team up and they keep pushing the open source and that would be and that that combination would be the horse I would bet on but I think there's also like uh there's like stability AI has one um I don't I know just like kind of like vicuna I think that one comes out of Burke I don't know too much about about the whole replicate replicate has a replicated offering as well uh we support that within blank chain RB and actually um to kind of bring it back to link chain RB um what we're striving to do with link chain are B is provide a set a set of abstractions that software Engineers can can understand [Music] um and have the flexibility to swap out the different llms right whether whether it's a SAS offering and you're integrating with an API whether you're running uh whether you're running a uh vicuna model locally uh we want to make sure that you can easily swap those out um because as as you've pointed out uh as we all know the space is moving really rapidly um what this what this base is going to look like and who are the leading models right like in you know uh in a couple months in three six nine months from now on is going to be completely different right um but we want to make sure that if you're integrating LinkedIn or be within your stack and uh you decide to switch to a uh more cost effective model or just some more powerful model down the road you can easily do that yeah I think that puts Lang chain and llama index these kind of tools that are really interesting Market position is how they let you rotate out the um you know the models as well as the tools and you might want to think of a model inference as a tool like again we talked about you have a foundation model for a super specific thing and so you you know whether you use gbt4 to come up with the plan and then it routes it to other model inferences you think of that as a tool I think that you know that kind of idea but and then there's also kind of like the data you know like the database things like I hear people discuss like how Lang chain lets you you know ablate the different Vector databases and that makes total sense obviously like if you want to say you know you want to use weeviate you want to try the whole Vector database Market I think it's also really interesting for you mentioned text to SQL and you know we V8 does have an aggregate API so it's not like you can't do that thing in weba but yeah their particular kind of join operations and relational database stuff I can imagine you'd want another kind of dative tool as well say like maybe the graph database stuff has something to offer so yeah that whole positioning of it to uh wrap around these different Services I think that's super interesting right right and actually and actually uh one uh maybe a question that I would even post to to your uh the rest of the weeviate folks is is that um I like how um I like how this how you guys structure your schemas and and you can create schemas that pretty much resemble a traditional database right you have those different data types that can that can mirror your database um and I've wondered whether you can actually use it alleviate as your as your primary database and whether this was in whether that was the direction that uh aviatus is heading down you know I'd say there are the kind of like the efficient data structures for each of the properties like inverted indexing on categorical Properties or say like building up trees to search through you know numeric values um the thing about this that I think is the most interesting is the connection between um how you would join tables in a relational database compared to how we think about joining classes in weviate because each class in weeviate has its own Vector index and so it's like you know I think like with a lot of relational database design you'd have like customer you'd separate like the customer and the purchases and then like the brand like your you know the supply chain you separated out into all these tables and so and then you kind of like join along and I don't know I don't know if you want to send embeddings like back through a graph like that that's that like if if that kind of thing just because I think there are a lot of like under the hood optimizations behind how the relational algebra of like when you're joining the tape you have like conditions for joining the table so there's like an optimal order to how you join the tables because like this one create this one reduces the cardinality to like five thousand versus like 50 000 right so I think those kind of optimizations uh can be made in Wii VA but it's not something that I personally am super knowledgeable about but to me the thing that I think about more is this idea of like particularly in like the recommendation case there's examples of like graph neural networks where you like flow embeddings through relations like that like you know user liked product product has Brand these kind of things and so you can like slow embeddings through a graph and I think I think that's where like knowledge graphs are gonna merge with Vector databases in some kind of interesting way but yeah that whole topic of understanding joining tables or classes that that's something that I don't have a great answer for but I think it's a super interesting topic got it yeah I mean what I'm curious about like you probably have more experience in databases than I do honestly like what do you think about kind of like um yeah like what are the components of an SQL database system that Vector databases should you know make sure to add to make it a make it a One-Stop shop for your data um that's that's yeah that's a very good question um I think um I so I obviously uh Vector search is um such a large massive uh growing space that traditional data traditional database vendors are looking to add those capabilities right so there's a there's PG Vector uh postgres extension right uh we're aware of some of the other database vendors and adding those capabilities yeah to their products um and um you know there's there's there's kind of pros and cons there um the same way that there's pros and cons to um uh having a monolith um application architecture or a microservices uh Centric application architecture yeah because it is isn't there there is something to when you compute there's like indexes that you compute with um SQL where you're gonna join like 20 tables together like you've got like a payroll software right and so you so there would be like a memory overhead to building up those kind of indexes right something like that yeah and so the question then is um yeah I don't know I think it would probably be a case-by-case thing I I do like just the general thinking of like if you're asking a question like you know what is the average age of uh customers who bought an Xbox game controller in March questions like that that like I think you would always want to use an SQL system I can't imagine like a semantic search llm thing being the best answer to that particular kind of so right right so that's why when I love that Texas SQL topic and then I like to say you know leave it has an aggregate API I don't know if maybe we could build a SQL API I just don't know what even that would entail but yeah right right but it would interest sorry yeah yeah I mean if you're um you know if you're trying to get the if if you have a list of products and you're you're merely just trying to get the uh the cheapest product or the most expensive product than than yeah and as a traditional SQL query will get you just just fine right but if you're searching the product description uh in your certain searching for a uh uh if you're searching product description um and you're searching for a product that lets you accomplish certain things and then yeah semantic I think a semantic search would be more suitable I mean I I did like just when Mosaic was acquired by databricks I did a little more homework on like what is data breaks and what stuff like and they bring they bring themselves as like structured unstructured semi-structured data Lake and and so I generally think this idea that like unstructured and structured you know data schemas have more to learn from each other I think semi-structured is where you like I think like snorkel AI is one of the pioneers of this idea of like programmatic labeling where you like predict missing structure from your unstructured data and yeah just I think there's a lot I think that's a super how do you unify these things I think that's one of the areas of innovation in these new kinds of databases yeah right right um yeah and this this this problem uh this dilemma has been has been around software development for uh for decades yeah yeah it was super cool Andre thank you so much for joining the podcast it's such a fun discussion I think the you know the Ruby Klein hearing about the motivation behind it this whole of space and Lang chain tool use also interesting thank you so much thank you thank you it was a pleasure thanks so much ", "type": "Video", "name": "Andrei Bondarev on AI tools in Ruby! - Weaviate Podcast #54", "path": "", "link": "https://www.youtube.com/watch?v=BnHFE5Y5Vvc", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}