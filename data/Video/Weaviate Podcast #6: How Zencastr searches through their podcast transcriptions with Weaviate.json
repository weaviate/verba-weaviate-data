{"text": "Alex Cannan, a Machine Learning engineer at Zencastr, talks with Connor Shorten about a really exciting use case of applying ... \nhey everyone thanks so much for checking out the sixth episode of the we va podcast today i'm joined with alex kanan a machine learning engineer at zencaster where they're bringing all sorts of cool applications of deep learning uh search applications to podcasting such as what we're recording this podcast on right now so alex thank you so much for uh being a guest on the podcast and could you please tell us about zencaster and the kind of things that you're using the wv8 search engine for hey connor um yeah thanks for having me in the panda we viet podcast uh my name is alex i'm a machine learning engineer at zancaster um and zancaster is basically for those who don't know just a podcast recording platform um among other things um but yeah basically we've uh been working on transcriptions um over the past few months or we released transcriptions a few a few months ago really um and we have sort of started to employ wev8 as a way to smartly search through podcast transcriptions um but yep you mean before we beat um this is something we've been attempting to do for quite a while so we went through a lot of like really early iterations of our own idea of like a vector search database um i remember i started out with just like tf idf keywords and word to vac on those keywords so um you know seeing like a fully managed like vector database system like wev8 uh really just knocked my socks off and it's been wonderful to use so far yeah that's super cool that the um trying to build your own vector search database i used to do things like that too where when i was experimenting with say uh gans and c410 i'd uh generate the image encode it into the latent space with an image classifier and then search through a vector representation of my own data set just with the dot product with everything i had in the database and that kind of transition of trying to um like build up your own vector search database compared to these platforms like we v8 and then plugging into say hey or a haystack pipelines or the gina flow of connecting it to these whole search engines it's really interesting how little of that kind of stuff you have to build yourself so how easy has it been for you plugging in the vva components like the vectorizer the modules and all that into the podcast transcriptions um it's been great uh we actually ended up settling on a sentence transformer before we even knew about uv8 that we v8 supported so it was very simple um setup in terms of just you know the way the install page has like a docker compose where i could just select the exact transformer we've been using but yeah um it's wonderful to have really i mean a lot of like machine learning tools um tend to have like pretty loose ideas of deployment meanwhile vv8 has been pretty rock solid for us um which is something we really appreciate but yeah it's been very easy to set up and i like it yeah i think the case of uh podcast transcriptions is a really interesting topic around this idea of whether you can just use an off-the-shelf model or whether you need to fine-tune it on your data set and because i think with with podcast transcriptions you're not going to be able to have a massive data set unless maybe you use something like federated learning where you don't store everyone's conversation in your zencaster database because you know maybe people don't want to have their podcast in a central database that's used to fine-tune the sentence transformer so have you found a pretty good result of just using the how the wev8 has the docker compose that you just point to say a hugging face model weights and that's all you need to get up and running with this pretty much um i mean my reasoning personally is basically podcasts are essentially just conversations so if we just if there i mean there are like plenty of conversation data sets out there um like fisher for one it's like a pretty great conversation data set on the small side i guess but um they're out there and if we just use a sentence transformer that's trained on like casual conversation i'm pretty we're pretty confident that it'll have you know great performance on podcasts as well and you know there's there's always you know more performance you could probably eke out of you know fine tuning it on uh perfectly in domain data so that's definitely something we're thinking about so i actually haven't heard of this fischer data set and i'm sorry about that i've seen uh like dialogue gpt i've studied things like the mena chatbot the facebook's internet augmented dialogue generation i'm not really uh too caught up with the chatbot data sets do you mind telling me a little more about the fischer data set and what kind of uh benchmarks are being used for chat bots sure uh well yeah fischer is um not really a chatbot data set per se it's it's a very old data set it's like from that the linguistic data consortium um like 95 or something but um yeah it was just like some like exposition that people went to about speech and i think some people like set up a little booth where two people would go in and record like a few minutes of dialogue together over about like random topics yeah i've read some really funny papers about how they set up these data collection pipelines for uh the chatbot data sets where they have uh they they call it like wizards of uh wizards of turns it's something with wizards in the thing and it's about how you have these randomly assigned contexts and the way that they try to crowd source worker annotations to build up these data sets i think is is really interesting what do you think about just say doing a web scrape of reddit conversations as a as a data set compared to these ideas of uh having like a scripted character that you read into and then you record podcasts on this platform to create these kinds of data sets yeah that's actually a very interesting um sort of distinction between what people how people talk online and how people talk in person um we've actually like built some like basic like reddit scrapers previously um or i have rather but um i guess online just online discourse is uh much different than in person discourse i mean just in terms of text it's completely different people do like emoticons um they like try to write out things that can't really be referred aloud i don't know if that made any sense but yeah they use abbreviations yeah i've been so interested in that topic as well and i love this like um idea of having say style transfer as like a sequence of sequence machine translation problem and where you say transfer from a casual style to a formal style to and like they can learn it unsupervised and the first paper that i read this paper where they translate from python to javascript with neural machine translation completely unsupervised and i i couldn't really i like really couldn't believe that works so if they can translate from python to javascript i'm certain they can transcript from casual styles of speaking to more formal styles of speaking kind of do you think that like that idea of style transfer from the reddit conversation style could transfer in an unsupervised way into the um into say a real conversation like we're having now and then and then do you think that style transfer algorithm could then be used as say data augmentation to kind of uh smoothen out that like interpolation line where you take your data and you have like levels of how formal it is does that sound like something that might be able to bootstrap the reddit data into podcast transcriptions yeah that's that's really interesting um i imagine i mean so for the python to javascript style transfer i imagine they must have had some data set of like code that is equivalent in both python and javascript i'm just assuming i don't did they did you yeah they have this so they do this round trip consistency thing it's it's similar to cycle gan how you go from zebra to horses and then back to zebra so you structure the loss with how well it can go back to the zebra from the intermediate horse and then and then i think you also have uh you probably also have some kind of grounding loss like in the pics depicts paper where they go from the photograph of the shoe to the sketch of the shoe they also do like a like a l1 mean squared error or over just all the pixels to structure it as well but generally that kind of round trip consistency idea if i translate to french and then i take the artifact of the french translation and go back to english with some other model so for some reason that seems to work without having some kind of shortcut in the representation wait so it it guesses the transformation to the other one and then tries to transform it back so is there like i assume there's like loss but from like each step it gets accounted for yeah yeah generally in like the psycho gan framework you would criticize how well it's made the so it's going from horse to zebra back to horse you do the in the gan kind of framework you do the real fake on the zebra and then you also do the real fake on the final horse that comes back out of the intermediate zebra translation and that's that kind of way of thinking about it seems to work really well for not only python to javascript or casual style of talking to formal style of talking but also for question context answer triples where you generate some kind of potential question and then you use and then you mask out the uh the answer or the context and then you use your generated question to generate a new answer and then you see if that matched the original tuple and they they can use that to uh to generate that's how they say use generative models to produce uh synthetic data for question answering data sets okay interesting and and this all involves having like a data set of like the zebra and the horse right yeah what's so interesting about it to me is how casually you can label these domains like you don't need to annotate the data so severely you just kind of say this is python this is javascript or like this kind of domain annotation for self-supervised or like uns it's like self-supervised i wouldn't say unsupervised machine translation because to me that just means like looking for structures it doesn't mean like optimization but like that kind of free annotation or like easy at scale annotation you get from self-supervised learning where all you have to do is just label the domains like this comes from books this comes from scientific papers this is wikipedia right in the way that you can just do that to annotate these domains and you end up annotating like a massive amount of data pretty easily compared to say the manual this is the answer within this context for question answering labeling or that kind of thing okay cool yeah that's that's very interesting i mean yeah as long as you just have to label the the domain of the the data and not it doesn't have to be like a one-to-one representation then yeah i imagine this could also work for style transfer on text as well um yeah i mean the only thing different i'd say is that like text is sort of a linear thing um or like a sequential thing rather um instead of like an image i guess in the case of the zebra horse but um yeah that's a very interesting idea as as i mean i'm usually a proponent of being very hesitant to include another model in the pipeline of training a model um so yeah i mean this would be essentially that so if we want to retrain the uh like transcription model or like the embedding model we would need some in domain data of like podcast transcriptions um and style transferring reddit data is sort of a lossy step in that sense um that being said it's pretty easy to get a hell of a lot of you both out of the main data and in domain data um you know from reddit or some other form and then from you know some in domain data which i wanted to say books right there but really i guess that doesn't like scratch like the in domain um vibe of like just like podcast discussions so yeah i mean if there i mean if there is uh if we there is a good data source for in domain like podcast discussions then and if if it's enough to actually like train the style transfer then i imagine that also would just be useful to train with by itself um or to fine-tune like an embedding model that would fit into bb8 yeah i love thinking about that concept of transfer domains and maybe like domain and affinity how well transferring from one kind of say reddit or books or imdb movie reviews is going to transfer into some kind of other language domain i think that's one of the most interesting topics and i also think you hit a really interesting point that idea of if you use a generative model in your data augmentation pipeline you might have compounding errors where the bias of the generative model just runs away and makes the discriminative model even more biased and i guess maybe i've been like interested in things like in images how we have say variational auto encoders where you have that loss on uh diversity and trying to cover a lot of the data space and tr and hopefully avoid like a mode collapse in gans and then using that to augment your data would be horrible because it's just you're just collapsing your discriminative model on that kind of mode that your gan has over fit to or or like um i don't know if you call that overfitting that mode collapse scan thing but that idea the generative model just doesn't cover a diverse enough space the data distribution that using it as an augmentation model would just not be good at all so one other question i had is um what about using say youtube where you could probably get thousands of podcasts on youtube and and uh and just kind of download it that way well yeah so i mean there is youtube dl which is a wonderful tool that i think is like not totally kosher anymore i think youtube doesn't like it i think like try to take it down last year i believe but um yeah i mean that's something i've thought about a lot i mean there's a lot of like really great like even like long form podcast content on youtube um with video as well as audio um which is sort of an interesting aspect given you know we record video podcasts um but um yeah i mean youtube dl they support just like taking the audio from youtube videos i think you cannot even get the captions from youtube videos using youtube dl as well so it's a very interesting data source i would say um given its um you know full band in the sense that it's you know good quality audio good quality video and transcriptions via whatever transcription system google uses there yeah i've been really interested in this idea of like um say using copyrighted data for machine learning and i recently even read a blog post about it's something like the authors guild versus google and they sued google over copyright infringement because they're using the books and yeah and the recommendation engine so i mean i i do think this is going to be one of the most interesting topics as we develop these applications because this data is how the models are built and then it's it's kind of hard to get your own data so it's an interesting thing i don't know if uh like youtube has those kind of licenses when you upload a video like i know from uploading my own videos on youtube i don't think i'm signing any kind of license that says you can't use it for training whatever video generation model of me you want with it or whatever thing like that but um maybe if you're using the captions maybe you're using their like google's speech recognition text model or something like that yeah that whole kind of thing is pretty interesting to me thinking about like the copyrighted uh data and whether you can use that to train these models have you thought about like as you're developing zencaster in these tools have you thought about open sourcing one of these data sets that you're building i mean yes i mean i i personally love open source um i would love to be able to open source some of the models we're working on but also some of them do rely on like user data that we have of course they i mean we like scrub it scrub with the user data we use we like anonymize it as much we can um we don't um we don't like copy it onto our local machines so but i don't know it's a very weird issue the issue of you know can you release a model that is trained on private data because like you can if you try an audit like a machine learning model and like figure out if it's using private data that's like basically impossible there's zero way to do that unless there's some like build logs built into it or something where you have like file names but yeah i've seen i saw a paper on extracting training data from language models that is um i i think unrealistic the way that they just kind of prompt it and generate the emails and stuff i i can't imagine any company would be that careless with how they encode and publish the language models but so quickly i do want to get more into say federated learning and this idea of how you're how you're using data without putting it in a centralized database and this difference further between say copyrighted data and then private data which are two very different things in my opinion but one idea i think is really interesting maybe we could talk about a bit one other interesting thing about uh data privacy that i think is really interesting is recent developments in data set distillation so similar to knowledge distillation where the idea is to label data by using a teacher model and then the student model fits the labels of that the teacher model has labeled the way that dataset distillation works is that you distill the information from the entire training set into a into a compressed representation where say you only have 50 images that make up the 50 000 images of c410 such that you train on the 50 images and retain the full performance of the original data set and they optimize that by doing the same kind of gradient ascent technique all the way putting gradients all the way back into the input similar to say adversarial noise maps or heat map activations where you look at say what part of the image is they're looking at classified as an elephant or whatever and so that kind of optimization technique would allow you to maybe retain these data sets so with federated learning it seems to me like you send the model weights to the local machine you update the model and then you send the new model weights back but that's doesn't sound great for if something went wrong if you say continual learning catastrophic forgetting trying to study all these things so i think maybe this idea of compressing the data set and then sending this compressed data set back that i think would probably be impossible to invert back to the original data and then you at least have like some kind of sequence of the data sets that i think is a more useful artifact than just the sequence of model weights that have been updated on the local federated learning kind of way of thinking about this okay yeah that's a very interesting idea like a compressed data set like it's almost something i like it sounds too good to be true um like so there's like a you mentioned there's like a 50 000 image data set that got compressed like 50 images are they like are they images within that data set that are just selected from like to be a diverse set of the original data set or yeah i can well no they're um they're optimized directly and you use the original data you have and the performance on evaluated on that as the learning signal and so they're really there there are two techniques that i'd say are worth listeners looking into the first of which is generative teaching networks where you have the outer inner optimization loop where you generate you start off by generating something random like again and then you use the signal for how well the model trained on that data set performs on it so it's like a proximal policy optimization where all you have is the reward signal the final accuracy and then there's this other paper from google and i'm not going to talk about it too much because i don't really understand these techniques but it's it's like the neural tangent kernel and the infinite width kind of strategy and that's that's something else that seems to work but i don't really understand it okay so that's interesting so it's basically selects like 50 images that seem to train the model in the best direction no it um it optimizes them directly from random noise so they they all start off with 50 random noise images and then like gradient ascent back to the input space so similar to like you can stack it all up as a big picture of say like a giant tensor of 50 by 32 by 32x3 and then you can send gradients all the way back into that thing okay wow i am very curious what those images look like yeah well they're mostly random noise in the end there's still random noise with some with some kind of patterns but it's yeah i mean i guess it is just like essentially just like encoding like binary information in those images instead of like actual like human processable images yeah the idea of having zero one in each of the i don't know that's such a compression i i doubt that there's still probably 32-bit uh or like the you know 255 so in addition to the topic of fine-tuning and thinking about the data sets that we're using i wanted to ask about like your search pipeline and your retrieval pipeline and what kind of different models you're using you mentioned earlier that you started off by building a tf idf retriever we've got the sentence transformers retriever and then we've eat has the kind of symbolic annotations that you could use like i imagine in this case a symbolic annotation would be just who's speaking so if if you and i have a extremely long conversation maybe it's like a group of ten of us and we have like a three hour conversation maybe when we're searching through it we would want to annotate it by things that connor said and put that kind of filter on to it when we're going back into the database or imagine collecting like a like we record 10 of these conversations in a row which i think is really exciting for things like say we have a paper reading group i've seen these kind of things assembled where we have like 20 people in a paper reading group and you'd imagine putting together 10 sessions and then searching through that so maybe you'd want the symbolic filter through who's speaking or the date for just all conversations so what does your kind of search architecture look like for using these different information retrieval components say as someone okay so as someone sort of unfamiliar with neurosymbolic um ai is that like could you give me like a a 10 second crash course on what that means oh yeah so it it's not even like crazy neurosymbolic ai just the idea of um using the filters using a symbolic filter to facilitate the vector search so uh before you search through all your data say you're looking through scientific papers where you might have 60 million papers and you only want to see uh what was published in the symbolic filter in icml 2021 right so now you're only searching through now you're only doing your vector distance search within that sub population of your data set that matches the symbolic filter so i think there's more to the general category of neurosymbolic ai and i love things like uh discrete representations and causal inference and and high level ideas like system one system two thinking fast and slow and all those kinds of ideas but in this most basic case of a neurosymbolic system we just mean kind of using the symbolic filters to uh reduce the complexity of the of the space before we do this vector dot product comparison okay so is this a like distinct from so how is this distinct from just like a near a near text query for example um right with the near text say you've got the 60 million papers the icml filter would probably reduce it down to i think five thousand papers obviously that i hope that number's right but like it would take it from 60 million to 5000 and then you would do then you could just you wouldn't even need an approximate nearest neighbor algorithm anymore like face or um hmsw at that point you could just brute force it okay so you basically give some like keywords like whatever the name of that conference is and it's able to pick out um yeah and what's super cool about that is you can either use keywords and you can use keywords in say um in text or you can match it within the data space itself but in images or audio that might not that analog might not translate so well but what we va does which is really interesting is you annotate the metadata so you can use you don't have to necessarily get the keyword from the data itself it could be a meta tag on it like the domain it was sort or what or what time you you got that data or any kind of tag you might want to put on it like a sql databases you're handling maybe like high velocity data that you have constantly updating things okay so so symbolic is basic is symbolic just like introducing sort of like more classical like database functionality into vector search yeah well i guess i guess you could get pretty like into all that kind of symbolic filte all that kind of logic and maybe like relational algebra to speed up the retrieval but yeah for now i think all we're doing is just in the set i mean yeah i mean i use like symbolic filters like all the time in our data set we use we have um unfortunately since wv8 doesn't have a cursor api yet um they have yeah we have a transcripts by um like a a date time stamp and we can search through like um each one by day that way um and we sort of like make a cursory api by like returning all the transcripts from each day over time so a cursor api is um something that traverses a database based on the uh like the time encoding um not necessarily the time encoding it's just by something like in in mongodb it's like if you do like a find query um it traverses by like the natural insert order um which is basically i think it's just the the actual id but like sorted like the object id like the the hex object it just goes through those um i believe lexigraphically um but yeah that's like one thing that v8 doesn't have yet that we're working around but how about the um just the get graphql how's can you tell me a little more about that because i'm not too familiar with the say the limitations of vba's graphql api yeah i mean the graphql is is pretty pretty great um i always heard good things about it but hadn't used it until i started trying out with it um but yeah the wv8 console um it's yeah it's uh basically it's some website that we gave host that you can like connect it to your local mission local web instance and build queries and it remembers your history so it's super super nice to like be able to just like log into that and have all the queries you've run uh recently um but the graphql is great um it has nice aggregate functions it looks like in the latest version of wev8 we actually actually added the ability to aggregate over references which is pretty big um for me at least so like for example if we have um some text that has like a list of like entities that appear within the text and those entities are their own different class and they're linked to the original text with um a cross reference you can just aggregate over um those um entities and get a total count of entities that way which is pretty pretty cool um but yeah it's the graphql interface is is pretty amazing i actually watched um bob's uh did a talk at like a graphql conference which is really interesting um something i'm kind of interested in myself but yeah it's good it's great yeah i've been uh studying a lot of bob's talks on graphql and laura hamm has a lot of great talks as well on data science with graphql and showing uh how they use this graphql api and to me i guess it's really extensible the idea that you start collecting data and you flexibly want to update the schema and then also flexibly want to update what's being returned from the thing as well as what you're talking about with the cross referencing ability and so just quickly before i also want to talk about the semi console a little more because i think that's an awesome feature of it and the gui of getting to have the client to play around with your database i know all like the sql things have clients where you can query your data set and get used to that but it's a lot of fun to see it with a vector with a deep learning search engine i think is super cool to see that client ui so i definitely want to talk about that but um before getting too off topic i wanted to talk a little more about the cross referencing and say so i was sold on the idea of when i first started learning about graphql that the reason graphql is better than rest apis is because you only need to make one query to the uh to the to just the api i say to get to to traverse through all the different kinds of tables you might have in the sense of in graphql and json we think of it as class property compared to whereas in relational databases we think of it as tables and we'd be using keys to join the keys and traverse it so is that is that really a big selling point for you that you don't have to make five api requests you just make one api request and that kind of difference between graphql and rest apis yeah it's it's it's quite amazing for me um personally um i i mean i enjoy it it's basically like a nosql database but with like extra functionality to do like those reference references within like a single query um which is really powerful because like i've been using like mongodb and i'd have to like look for one document you know find the the related field key for like another document and like do like three separate queries to get where i need to go um which is hassle but yeah no this is great you just you pick out a cross reference you place it onto whatever class you want to read it with i mean it has to be like a single class a certain class but yeah that's incredibly powerful for me and expressive um it just sort of feels like the future you know it's you don't waste any time yeah yeah it seems um very intuitive also easier to communicate and share uh what it's doing and how to use it so to say compared to say sql where you need to like pass an interview to to know how to query it whereas i think the graphql thing has a is a quicker learning curve i don't i still can't really form graphql or sql queries um without looking up like half the things i write it's it's rough especially joins graph well it's real dead simple yeah you just say what you want you get a json object it's magic yeah i remember taking those classes where you do like the uh running time complexity of like inner joy and outer joy and all that kind of stuff on this topic of kind of like the software engineering tooling and um i want to ask about like um your experience with say docker containers and the helm chart and if you could just kind of explain all that to people and what it takes to take your wev88 instance and then deploy it and make it accessible through something like aws or google cloud sure yeah so i mean we've it has like really great um uh deployment uh manifests i guess um in both the docker compose that they have which is definitely the way to do it if you just want to stand up quickly and then also the helm charts and the helm charts um [Music] are basically just like kubernetes um instructions for um like a kubernetes kubernetes cluster to actually deploy i think um i think it's like on aws it's like eks that's like their kubernetes service elastic kubernetes service i want to say i don't know it's probably wrong but yeah so uh you can just apply the helm chart and as long as you have enough you know uh pods provisions it'll just pick up all those pods and get them deployed and let them talk to each other um i know for us we also had to add just like a really basic ingress um but yeah i mean that's something like we we should do on our side anyway because it's sort of specific to our network but um yeah i mean it really is as simple as just like setting up a kubernetes cluster and do like cube control apply and the help and that's it so it's just like one click from when you click on uh like get your docker compose file and then just one click on aws so like how many how many steps is there in the middle of that for all for the docker compose that's just you know you go to the website and you say what your configuration should be it gives you a curl command to just download the compose and then docker compose up and there you go it's deployed it's crazy and the the helm chart um yeah i mean so we did so the helm chart right now is not like built into that like really nice installation interface on the ev8 documentation so uh what we did was we basically just forked the helm chart picked what version we wanted and then made some configurations for what we want so we like disabled some of the modules we didn't use we pers we also personally disable auto schema just to keep the data a bit more rigid because it's being written to you by a few different services so having error messages is pretty useful there um yeah i mean once you have the helm chart and it's configured you just apply it and bam it's just like that you can get some monitoring from i think aws their kubernetes cluster has monitoring built in which is nice um and you can zoom into like any of the logs on each pod so yeah super cool and so one other kind of uh meta question about these things is um how has your experience been using the wev8 slack to me it seems like slack is going to play i guess like slack and also obviously like raising issues within github when you have issues with these newer kind of open source tools do you think the slack chat has been you know like pretty well organized to me it seems like a pretty vibrant community as i've been kind of exploring different open source software slack chats and seeing how well they're uh answering questions that people face and do you think like uh recently we interviewed michael westner building katie and we've integrated katie into we v8 and uh and so it's going to be able to do question answering within these slack chats which i think is particularly the idea of duplicate question detection i think could be really useful for slack chats so do you have any thinking about just how the slack chat question answering kind of system could be improved yeah i mean yeah that duplicate question thing is really interesting because i mean the thing about a slack is that a lot of people don't really consider like searching through the old conversations to find previous answers so a lot of stuff gets answered twice if katie helps with that that'd be really wonderful um but yeah i mean i think in general the slack channel is a really useful resource um for me like i've sort of learned to like search through old messages first um but like a lot of like the basic stuff the basic problems i ran into i could solve with a simple search there um yeah i mean just talking about some stuff on the slack channel like has like created some bug tickets that got fixed in the latest release um which is really amazing which means it's just a great turnaround just like being able to like talk to like is it at etienne is that okay i hope i'm pronouncing that correct yeah being able to talk to etienne um and bob too of course um it's really really wonderful they're always like pretty they're on top of all the questions i ask um i do wish sometimes that i would have made an issue on github instead of uh asking on slack because then i would get the github cloud of having more issues on my little like four axis diagram on my github profile but i guess with just getting what i want yeah i want to get like a little little semi-technologies badge my page that'd be nice that's a really interesting thing that topic around i mean we're going to get a little off going talking about this but that topic around say these question answering systems and it kind of comes back to talking about copyrighted data and data privacy how you contributing your question answer kind of builds the technology and i think i don't know if what i think about these like ideas of dividing up companies open source products into tokens and then distributing the tokens based on your contribution to things like that but that kind of idea of rewarding people who contribute to the platform because the platform or contribute to the product really through their data and their conversations and their uh like user testimonials so to say yeah that's that's interesting um i mean it's something we've also like been considering on at zencaster too um sort of how to incentivize people to sort of help with the machine learning tasks um you know we have i mean we've hired like some contractors to like help like annotate some datas a few times um but yeah it's it's it's it's a weird thing motivating people to do things without like without money i guess um because i mean like people get training data in a lot of different ways like say like netflix they have like their like star system or they did i don't know i think it's like thumbs up thumbs down now yeah i don't know it's just a very like arbitrary thing that like people don't put much thought into but that sort of trains like an entire like monster of a recommendation system um and i don't know that might i think honestly that's like something they consider when they actually design what this looks like they want people to sort of not think about it too much um which i guess is like a natural thing to do you want like the lowest barrier of entry um it's actually getting the data but it's interesting like um there's like how your liking and even just like your watch time on the videos or reading time on articles how that trains some super recommendation algorithm and then there's things like i've been looking into training a language model on the keras code examples contribution where uh contributing to that gives you like a reputational boost because you've written this tutorial similar to like a github contribution so you get more you have more of a built-in kind of feedback than just clicking like and training a recommendation algorithm that way but yeah i think this kind of topic around uh data uh ownership and and then the resulting machine learning products is definitely uh one of the most interesting kind of topics so kind of wrapping up around uh search and thinking about searching through podcasts do you think say the clip model for image text do you think that there'll be any use for say image search within podcasts or do you think that's probably or maybe even audio where i don't know maybe because i imagine even audio you always have the text transcription do you see any kind of multimodal whether it's images or audio or using the stack of image audio uh text in video to maybe search through that kind of way or do you think all the useful applications are probably just searching through text yeah so that's um that's something we've we've you know we have a lot of uh multimedia data um or like podcasting in itself is sort of like a multimedia format um and yeah we are just using text right now but the clip model is really interesting um to consider the ability to sort of search through video with text like sort of describe what's in the image um i think the only reason we haven't like sort of made more of an effort to like explore that is that you know podcasts tend to be you know a talking head in a random background so i don't imagine there'd be much sort of video information in there besides like general like descriptions of the person talking i believe but yeah i don't know maybe if you've ever thought about like um treating say like a commentary on a sports game as a podcast where you kind of have that uh dual entertainment factor of you have the two people talking to annotate and then you have the basketball game or whatever it is and then maybe you wanna so maybe in that kind of case you want to search through like a dunk and you give it like an image of a dot maybe that kind of setting i'm not sure i don't know that's just a really funny idea to me like searching for a dunk on a on a podcast um but yeah actually yeah that's a really interesting concept i mean um it almost sounds like a gold mine of training data for a clip model like that but but yeah i mean for for us we don't um have um the ability to like link um podcasts with like other media at the moment uh but yeah if we did that'd be a really really interesting way to to actually search through that i mean we have tons of like sports podcasts of course but um and if we did have access to those like sports you know videos we could in theory try and like figure out which podcast goes to which sports event it's like live casted or like maybe like a after game kind of debrief yeah maybe even not just sports but just like uh video games like uh call of duty and all that kind of obviously there's such a vibrant gaming community around that and and you could imagine you have an image of say a reward a player receives in the game and you can and you can use that to find out exactly when it happened although you could you probably don't need deep learning search you could probably just do an exact match if you had something like that like say when you've like won the game you're on like a plus 5 000 flashes on the screen and now you want to index where that happened in the game maybe some idea like that i don't think about commenting and having kind of podcast and then also live content and i'm trying to imagine some way in which an image search would aid in that yeah if if this was like twitch where like you had like large like streams of like video game content with like audio that'd be very interesting to try out also it's it's a bit trippy to think about um doing like clip like video and audio embedding search on a virtual world created by like a few developers at like a development studio i don't know it's it seems very it seems like you would have to train a clip model on like the video game to get good results because it is not the real world yeah that's why we always have to talk about off-the-shelf models and fine-tuning that topic i think is because i think there's a lot of breakthroughs in off-the-shelf models like especially the way that prompting seems to guide language models to to have that generalization and access information that you didn't know it contained because you didn't know how to ask it if it contained it so maybe i think prompting is a really interesting argument for off-the-shelf models but that idea of fine-tuning in domain i think it's interesting i mean this idea i think most people who've studied machine learning are thinking oh it definitely needs to be fine-tuned in domain because that idea of being able to be flexible to any kind of data distribution is just like violates your training sort of and you don't think that that kind of thing is possible but i do think it's trending towards that way which is definitely exciting yeah i mean i mean i always think about how you know as humans we can understand what's happening in like just about anything like we have context about like you say if we say that we are trying to process um like some media as just as like the clip to vec model is you know we can like look at like a podcast video and like describe what's happening like very easily um we can also look at like a video game and describe what's happening very easily um so i mean i take that as proof as there is um the possibility of like generating a model like that generalizes like as well as humans um you know i think like the architecture is sort of the limiting factor we just got to like figure out how to actually set up like a neural network to mimic the neural pathways we have um and on the scale we have i don't really i don't know like the number i'm sure there's some research into like how many like um how many equivalent like um parameters we have in our brain versus like some like large tone network yeah i think it's i mean i'm i don't want to say a number and then be wrong but i remember lex friedman made a video on this where it's about comparison comparing uh gbt3 with our brain and i think we have something like on the scale of a trillion parameters they say i don't know if that's right but more than gpt three i think yeah but to anyone interested lex fredman has made a good video that uh goes into that kind of idea of the size comparison between gpg3 and and human brains but i think there's like you said the architecture design there's still like a missing piece for probably several missing pieces but there's definitely still more to whether it's a neural architecture search or i think ideas like trying to go past a global back propagation and things like cellular automata and a local heavy and learning these kinds of ideas i think will be interesting too but yeah that kind of neural architecture searches definitely seems like something that will have a huge impact and and it's interesting because things like say the division transformers have had such a massive impact and then all these designs now as they're trying to figure out how to go beyond 512 tokens and the sparse transformer long former winformer all these kinds of ideas that seem really exciting but i find it amazing that um that a automated neural architecture search like nas net or the evolve transformer still hasn't uh say taken off and isn't really leading the new architectures have you thought about neural architecture search and that kind of area of research and any thoughts on whether that will take us to like the next level so to say so by neural architecture search are you talking about like the models that like try to more closely mimic like the human brain more so like where you um you identify some building blocks some kind of like computational primitives that you might have whether it's a depth-wise separable convolution multi-head self-attention multi-layer perceptron average pooling along some axis like or like a batch normalization or different activation layers and you try to encode that in like a zero one like genotype and then you say use evolution where you mutate those genotypes and try to render some architecture that achieves oh okay okay so it's like evolutionary architecture that's really interesting um yeah i mean i think sort of like a combination of i think that is sort of the next step because i mean if you think about the human brain we have like a bunch of different like lobes that like perform different functions that they seem to be specialized in so um and they all work in tandem i assume i'm not neuroscientist but um you know if if we can somehow train a a grand neural network that can actually decide oh this architecture isn't really performing as well as we'd like to um and you know replace it with another one or like decrease its size or something like that um [Music] i mean it sounds like a pain to efficiently train with all those different sort of architectures floating around but i think that should that that should work i mean i can't imagine why it wouldn't and i imagine it would perform better than a lot of the existing models if large enough yet i think the current trend of scaling is uh the mixture of experts thing where it's not exactly like different parts of your lobes have different say uh structural forms but they have different specializations so to say and then you have the kind of meta-learned learned it's like the way the mixture of experts work is you have an attention layer that basically says which which pathways to send the computation to so it's you know very wide rather than very deep say and you have the attention that routes along the width so that you don't have the dents because these models are gigantic like the latest um and they call it glam or some title like that from google is like a trillion parameters because they can scale that kind of thing up like crazy and then you have lessons like how sparsity works you have infinite with networks it seems like scaling width-wise is promising as well so definitely a lot of interesting ideas and i really enjoyed talking about all these ideas with you and i'm really excited to see the evolution of zencaster did a good job recording our podcast now and we'll search through the transcript and i do i think that there are so many exciting applications of search through podcasts we mentioned a couple of things like say having a paper reading group where you have 20 people in discussion and you meet for say 12 weeks a row in a row and now you have kind of a something that is worthwhile to search through and things like say commentary on events i think could be something that you also end up with these really interesting data sets so i'm really excited to see how zencaster integrates search and how search will hit the kind of podcasting uh medium and what kind of ideas can emerge and i think even just like chunking up the podcast into chapters it could be a really interesting way to annotate it too and just just like extracting the salient point yeah yeah it was wonderful talking today um thank you for having me um and yeah i'm excited to see how we viet grows and how we can integrate it more with zencaster i think it's [Music] i think it's gonna be great [Music] ", "type": "Video", "name": "Weaviate Podcast #6: How Zencastr searches through their podcast transcriptions with Weaviate", "path": "", "link": "https://www.youtube.com/watch?v=npdJAxDDtmY", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}