{"text": "Tuana Celik, a Developer Advocate at Deepset, presented many exciting ideas around Question Answering! We began with her ... \nhey everyone thank you so much forchecking out another episode of the wevva podcast today i'm here withtuwanachilik a developer advocate atdeepset working on things like haystackand she's created an incredible huggingface spaces demo of a game of thronesquestion answering bot and i just reallywanted to talk to her to understand thisfurther understand deep said haystackand the things that they're working onso tawana thank you so much for joiningthe we va podcastthank you very much for having meso to kind of kick things off could youtell me about the game of thrones uhdemonstration uh what went into it themotivation kind of these kind of thesekinds of thingssure so the game of thronesdemo is really a demo to display anextractive qa pipeline that you can veryeasily set up with haystackso it's a very simple uh demo it uses inmemory document store and the whole ideais really to to to show people how youcan set something up super quickly andyeah i've uh hugging face space has beenaround for not really long but stillquite a while um and i really wanted myto get my hands on it sothis is what i came up withso there are a few things i would unpackwith that but first could you tell meabout the the role of haystack in it thekind of the nodes and the pipelines andand what that looks like for thissure so i'll start off by talking a bitabout deep set so deep set is thecompany and we have a open source nlpframework and that is haystackand what haystack really is it's aopen source um and the whole idea is tobe able to quickly build an end-to-endyou know production-ready nlpapplication so that could be questionanswering that could be summarizationtranslation a whole bunch of thingsum and it comes on the it's based on theidea of nodes and pipelines soit gives you the flexibility to sort ofuh switch and sort of construct your ownpipeline based on the task you want toachieve so for me in this scenario i hadan in-memory document store and then ihad a retriever which would then basedon the query you know give the readermodel or the question answering modelumthe most relevant documents and then aquestion answering model um but yeah sohaystack is based on this idea of beingable to sort of um like lego blocksbuild something up from scratch based onwhat you want to doyeah that's super exciting and i lovethat kind of retrieve thenre-decomposition uh can you tell me alittle bit more about the in-memorydocument store and how that might differfrom other kind of uhsort of like back-end retrieval storeoptions that are available and i meanwith the plug-in with we've a we canimagine with having the yeah like avector database is the back end can youmaybe explain for the difference betweenthe in-memory thing and other options soin-memory document is basically what itsays on the tin it's an in-memorydocument store so there's nothing fancygoing on um and it was the easiest toset up just to get going with but youcan imagine a scenario where you havelots and lots of documents like likeresiding somewhere whether that be we v8or open search or elasticsearch anyother document store and then based onwhat those are you can also have moreflexibility to then work with maybevector representations of text data andwhich is going to makefinding similar documents to the queryyou ask a lot faster maybe soum it's really abouthow efficient you want your system to beand where your documents were in thefirst placeum so in this example i didn't want tobe bothered with connecting up anythingelse so i just went with an in-memorydocument store um but yeah you canimagine a scenario where i think wealready even um we recorded a demo withlaura from semi technologies yourcolleague actually where we use uh wev8um document store with embeddings so uhwith vector search um with a questionanswering pipelineyes so i really want to get back intothe details of the question answeringpipelines and uh the different retrievalmodels the extractive question answeringthese questions but really quickly couldyou could you tell me about hugging facespaces and uh kind of when you first sawit did it like what kind ofdid it capture attention right away likehow do you see this kind of hugging facespaces platformhugging face spaces is i i quite like iti really like it actually because it's ait's a really nice way to prototype anddemodemo applications to peopleum so i yeah i've i've quite enjoyed itactually there are two options right nowas far as i know so you can build anapplication with streamlet or gradio soi built this one with streamlit it wasvery you know quite quick to get goingwith to be honest um and yeah it's it'sa very nice way for you to be able toshow code to people where it's not justpure code and they can already startinteracting with it and you know inspirepeople to like this is what you canachieve inx many lines of code um so i'm i'mreally enjoying it uh and yeah so it'snice to show people prototypes ofcertain very likeumisolated tasks so in this one we aredoing question answering hopefully and ialready have a pdf summarizer one whichi'm not super happy with the performanceof but it still can it's it's great tobe able to have an application givingpeople the idea of what you couldachieve yeahyeah i think the way that it lets youvisualize it and then have uh userinterface input output and yeah gradiostreamlin the way that they'vesimplified creating a front end formachine learning people i think is justsuper powerful and exciting and it alsokind of has like a content platformflare to it where it has like theuh the news feed trending this week itkind of helps people discover otherapplications and maybe like integrate itwith science and publications and youyou know you please cite my work youlink to your paper with your demo and ithink overall it's creating like morecommunity more engagement with the deeplearning research crowd but sotransitioning from face spaces which ilove and i'm happy that we're bothexcited about this and i you know i i'mreally excited about the future ofhugging face bases uh can we dive deeperinto the question answering part of thisand i know um deepset has a robertaquestion answering model that's onhugging face uh the model hub and it'sone of the most popular questionanswering models i think out therecan you tell me more about the questionanswering model that's used in the gameof thrones demonstration and uhgenerally your thoughts on uh questionansweringuh yes so we do have quite a fewquestion answering models by deep set onhugging face actually so roberta basssquad 2 is one of themost popular question answering modelsbut we also have what we now calldistilledversions of these models which are waylighterhence a lot faster to work with as welland still performrelatively um you know comparably goodto the original the teacher model let'ssayand yeah so these models are generallythey aretrained on squad data sets so questionanswer pairsum and the nice thing about this iswhere i feel like a lot of people don'trealize this is howwell pre-trained models can actuallygeneralize tounseen domains so we often get thequestionuh with from people who want to sort ofstart diving into question answering sohow long did it take for you to trainyour data train your model on this let'ssay they see the game of thrones one onthe game of thrones data set and theanswer to that is actually i didn'ttrain a model at all i use a pre-trainedmodel question answering model and thesemodels are trained tolook search for answers which is why werefer to question answering as a searchtask they are trained to search foranswersin unseen data so that was how i wasable to just take a pre-trained modeloff the shelfand use an in-memory document store putall my game of thrones documents thereand basically told the model this is allthe documents i havethis is what the retriever says are themost relevant to the question that wasasked to meumnow now look for the answer in thesedocuments um so yeahyeah that's incredible the the abilityof these pre-trained models to dosomething like that i i never would haveguessed like i wouldn't have suspectedthat a question answering model would beable to adapt to all the domains likethis but it looks like this retrievethen read decompositionhas offered a flexibility that is reallyexciting and and so the retrieval modelalso doesn't need to be fine-tuned rightlike it's a sentence transformer trainedon like wikipedia and it's a huggingface model that part also doesn't needto be fine-tuned or specializedyeah i did not fine-tune the thesentence similarity model eitherum i can imagine you know scenarios thatmaybe you might um can't really think ofone off the top of your head becausewhat the retriever model is really doingis or let's say what the retrievercomponent of the pipeline is doing isit's getting a questionand it's saying okay i'm going to use asentence similarity model to to decidewhich documents in the document storeare the most relevant for the readermodel to even bother looking throughbecause the the challenge we have hereand this is where you guys get in thisis where vector and certain yeah vectoroptimized database is getting is thatreader models at the end of the day theyare limited by inputso um imagine you're telling a readermodel to list let's say go throughhundreds of documents that's going totake a lot of time for it to decide whatthe most you know what the the bestanswer to the question waswhereas if you were to have a retrieverbefore that you can say okay you don'thave to bother with that 100 you canbother with the first top five top threeum which the sentence similarity or thethe um yeah the retriever sets uh stephas decided is the most relevantyeah it seems like that uh like goingbeyond 512 input tokens or maybe 1024things like sparse transformers do yousee that as being a big limitation ofthis retrieve than read way of settingup the task or do you think we usuallycan fit enough information in that 512token input windowthat you know it's not that big of aproblem with it with a strong retrieversayumin practice it seems that we're doingquite well with a retriever readerpipeline again there are different typesof retrievers and this this isn't justabout the trend themodel you decide the sentence similaritymodel you decide to use it's also abouthow efficient the document store you'reusing isso uh document store like what you guyshave like we've eatwhere it's optimized for vectorsumis probably going to perform way betterfor vector search than another one sothere's a lot to consider not just notjust the models you useyes i i'm kind ofi'm thinking about this is inspiring mythinking about say the differencebetween large language models like gbt3of course and like the thinking aroundin context learning and then thecomparison with the retrieve then readdecomposition where uh it seems like theretrieval model could be you know apretty small model like say 100 millionparameters in the sentence transformerseems to be like around the range ofmost of these and thenthe question answering i think robertabase is like 300 million right so youreallycut down the size of these models andachieve a sum of performancedo you think a lot about that kind oflarge language models versus thisretrieved and read decompositionwe actually use them both in commoncombination combinations right wordright yeah so umi i already mentioned briefly distilledmodels but yeah like i said we have umso for roberta bass squad 2 for examplewe have a distilled model called tinyroberta squad 2. um this is a waysmaller model don't quote me on anyparameter size i'm not the most savvyabout this type of stuff but it is adistilled version of roberta bass squad2 i believewhich is way lighter than roberta-basedsquad 2.umand we know that it performs not as wellbut nearly as well so it becomes aquestion of give and takeand we when we do use tiny revertersquad two we then also use it incombination with a retrieverreader pipeline um so what yeahwe you can combine a lot of steps thatthat is going to make your wholequestion answering pipeline let's sayway faster rather than picking one orthe other um to achieve a applicationthat is sort of seamless and nice tointeract withyeah i think that's a fascinating ideathat you could have the knowledgedistillation go from say like a 100billion parameter language model all theway down into the question answeringmodel and have interfaces for thatknowledge distillation i think that's areally interesting kind of area toexplore furtheranother thing i'm curious about is say ithink right now the way that we mostlythink abouttraining ortaking retrieve and reader models isthat we train them separately we haveour sentence transformer it's trained tocontrast neighboring paragraphs ornext sentences heuristics used todetermine positives for that kind ofcontrastive similarity task and then wetrain our question answering model likeseparately what do you think about likethe idea of joint training where youhave like gradients the gradients fromthe question answering go back into theretrieval model do you think that'smaybe like unnecessary complexity thatkind of idea i think they use it i thinkthis gets into the to a branch of nlpthat i don't consider myself to have themost knowledge aboutbut i can imagine a scenario wheremaybetrainingboth of them with an understanding ofwhat domain it's going to be used in umhand in hand might make sense so i canimagine so we even though i say thesequestion answering models generalizequite well there are obviouslysituations where you will want toconsider training or let's not even saytraining maybe fine tuning of questionanswering model to your domain i canimagine that you would probably want todo that for a sentence similarity modelas well so if you decide you want tolet's saydo a question answering for i think oneof the ones that came up was biomedicaldatawould it make sense to alsoin line with that decide you're gonnause a retriever so maybe you should havea retriever with a sentence similaritymodel that's been trained for thatpurposei don't know i think it might it mightmake sense but don't don't quote me onthisyeah i think the the scientific text andwe recently published our we've apodcast on scientific literature miningand i think what kyle was saying is thatthe the vocabulary is so different theterms are so different that you havesuch a domain shift that it it's hardfor the wikipedia train books corpustrained models to get over there so soon the topic of domain adaptation domaintransfer in nlpdo you think there are some quick toolswe can use to get a quick sense of whatkind of degradation we might expectmaybe likesome kind of i've heard terms likesemantic drift where maybe you look atcertain tokens that have like a totallydifferent meaning in some other corpusor maybe you look at like the tokenizermismatch likecertain words are being mapped to theunknown token and you just loseinformationdo you think there's maybe a way to geta quick sense of domain adaptation aswe're like building yeahi wouldas a haystack person i would say why nottry fine tuning a model to your data andthenevaluating your model um again you canyou can do a lot of these via withhaystack tooso i would saygo for it have an evaluation set and seehow it performs for youon this topic of fine-tuning uh likesentence transformers so is fine-tuningsomething that's built into the haystacksuite sort of is can you tell me moreabout how that works yeah so you canyeah you can umyou can download a model straight fromhugging face haystack has a hugging faceumintegration um and then say okay nowcontinue training for x-men epochs withmy my data set where where which is atthis locationum it's actually i think it's on thehaystack documentation it's asurprisingly easy process i did it theother dayis that different from um so is that fortraining the retriever models or thereader models this is this is for thereader model that i'm talking about thisis for the question answering model ithink that's the most common um placewhere you would want to fine-tune amodelso is that fine-tuning it with theretrieval attached to it so it kind ofgets used to havingthis is just so the reader model likelike you mentioned before is a totallyseparateentity to the to the retriever so theonly thing that the reader really caresabout is i've got this context this bitof text somewhere and i've just receiveda question and i'm going to look throughthis bit of text to find an answer forit whether it gets that bit of text froma retriever or whether you input it viathe hugging face inference api forexample it doesn't caredo you think maybe there's something toadapting the question answering modelstoi mean i think like with squad rightit's already trained with thecontext like what is the atomic numberof oxygen it gets a paragraph fromwikipedia as a part of the input do youthink maybefine-tuning it with the kind of noisyretrieval end where you know it mightnot it might give you that paragraphfrom wikipedia but it's also going togive you somethingabout maybe like i don't know somethingabout oxygen for underwater i don't knowlike some like it might give you noisycontext do you think maybe you shouldfine-tune that reader toeven if you're not putting gradientsback to the retrieval but like to getthe reader used tolike i have this kind of noisymy sidekick here is kind of going togive me these funny results and maybe ican get used to it a little bit but thethe point of of the reader model isn'tto know the answers to to something thatthe point of a reader model is really tofind or search for the answers forsomething soum it really is a matter of how how wellcan youhow well can you deduce the answer froma given bit of contextlet's sayso i wouldn't seeas the point is to be able to lookthrough unseen datathe only the only thing that it has tomanage is to be able to be as good aspossible at looking for thethe right answer i think that's the bestway to summarize it it's like i i umdescribed this uh the other dayum as the following so the questionanswering modelis a thinga thing anything that has been trainedto do just that look for answersum and what the retriever is doing orwhat providing a bit of context and thehugging faceinference api is doing is literally justproviding itwith the playgroundthat it should do that in oror you wanted to do that insoso this idea of um question answering issearch i think that's very interestingand i i think kind of all these languagetasks are kind of like very similar tome like it seems like question answeringcould encapsulate sort of the setup forany of these uh tasks if you phrase youphrase any task into a question like uhyou know it would say like factverification you'd say do these claimdoes this evidence support this claimright and now it's question answeringso do you think kind of all tasks couldbe mapped into question answering ormaybesay language modeling alsothat's an interesting question and idon't have a straight answer for it butit isn't it's an interesting way tothink about itumthe reason why well actually one thingthat i should point out is that sincethe beginning of our chat we've reallybeen talking about question answering inthe context of extractive questionanswering we haven't really been talkingabout generative question answering thereason why we call question answeringsearch is yes simple becausethe idea is that an extractive qa modellooks for the answer which means that itdoesn't generate the answer it alsomeans that you have it can only answeruh a question if the answer exists inthe playground you've provided it withnow whethereverything else can be described as aquestion answering taskthat's a tricky onebecausethen i'm thinking about a lot of othera lot of other tasks in nlp in generalsummarization is a question answering idon't think so um[Music]but yeahyeah i'm really glad you brought that upthe extractive verse abstractive thingandand um maybe like as kind of a quickthing i think maybe theextractive models are much more likeready for these kind of productionsystems like you know haystack we va aswe build these tools that we're tryingto push into like a more like a softwareengineering crowd i feel like theextractive models i feel like they makea lot more sense you can debug them alot easierbut then the abstract and models are soexciting and they have kind of hacks onthem like maybe like the beam searchdecoding orrecently a paper called rank gen theythere's like kind of like strategies tomake them more interpretable sort of butlike in your thinking like do you thinkextractive question extractive modelsgenerally with classification labelscompared to this idea where like you'regenerating each prediction has a set of30 000 possible things and you unrollthat sequence like 50 times do you thinkthat's maybe too grandiose for like theproduction systems that say a lot ofthese like open source deep learningmeet software engineering kind ofcompanies arepushing out therei would say so umfor a very simple reasonum the the training process of agenerative bear in mind i am not an nlpengineer i've really been learning alongthe waybut this is my understandingan extractive qa model is trained verydifferently to what a generationgenerative qa model would be trained asum andthen the question of oh how long did ittake for you to train your model toanswer these questions that actuallybecomes very valid for for generative qamodelsbecausethat this this time around the modelisn't you're not giving the model datafor it to search through a generativemodel doesn't need context to searchthrough a generative model is generatinganswers from the things it's beentrained for now you can imagine in ascenario where you're a company let'ssay and you have all these bunches ofdocuments text data lying about and youwant to provide that via a questionansweringapplication on your websiteyou have two options you either go withaextractiveversion and you use a model either youuse it pre-trained off the shelf orfine-tune it to that data you have maybeif if necessarybut then you just provide this and youprovide the data for it to search foranswers in a document store and thatdata can grow you still use that samemodel if you're going for generativewhere there isn't a concept of searchingthrough context for it to still be validin let's say five months time when youhave all this new information you'regoing to have to think about growing andgrowing and growing that model so in areal life scenario where you just want asimple question answering applicationsomewhere i i from my experience so fari would say an extractive qa model isgoing to be a lot makes a lot more sensenow obviously i don't know maybein the near future hopefully we get alotmore a lot cooler things out there withgenerative models and everyone startsusing it but this is my understanding sofaryeah that reminded me of when uh maltfrom haystack was also on the wvapodcast he gave me three reasons tofavor retrieve then read that youreminded me of and the first of whichbeing that it's easier to update themodels and i think that's such acompelling point likeyou need to update the language model tosay like you want to change a fact orsomething like that or you're trying tojust continue the auto regressivemodeling task and itmight not forget it it's hard to say ifit did right and so that particularpoint of the ability to updatedefinitely sounds very compelling andthen the two of the things he mentionedwas the interpretability if you see thecontext the reader model is using topredict so that can help you outexactly that's another thing and that'sactually a really good point becausewith an extractive qa model you ask aquestion and you're not only getting thebest answer the the model can come upwith you're also getting information ofwhere that answer was from so in a casewhere let's say for documentation whereyou have a question you want to askand you get the answer but you want toread more about it now you know whatdocument contains that answer and whatyou can go and learn more about itactuallyfor this type ofscenario then evenyou can even imagine hownot having the answer but the relevantdocuments become very compelling toorightyeah yeah yeah it's super interestingthe way that you can uh see it of courseand yeah like i think they try to dothings likeuh they might do like some kind ofsignature in the training data to try tosee what produced the generativelanguage model's output like i thinkthere's research like that where theytry something like that butsurely it can't be as straightforward asthis is with the retrieve than read kindof thingyeah and so then the third thing he hadsaid was that it's i think somethingthat we've talked about a few times withdistillation and sort of is just thatit's more like economical when youdecompose a task you don't need to storeall the data and the parameters of thebig language model and and then yeah youcan have like 100 million parameterretriever 300 million parameter readerrather than gigantic model that doeseverything so i think that was reallygreat conversation and topics aroundquestion answering and kind of the nexttopic i really want to ask you about isthedeveloper advocate role and creatingcontent and things like that issomething that i really care about a lotand i'm really curious to see hear youropinions on on this whole process aswell of kind of communicating whatyou're doing the different mediums ofdoing sooh man that's such a big topic umsoi started uh developer advocacy in 2020actually so unfortunately when i startedfor the first year in a bit we were inum pandemicand we had no live events etc so frommoving from now on moving forward um oneof my main things is really going to belike doing this maybe doing it live aswellum doing community calls and events butmy take on developer advocacy is thatthere there are certain levels of itthere's there's one there's one thing ofbeing um trying tolike educate developers on the toolyou're a developer advocate for and theway i've been doing that is like i saidi'm not an nlp engineer i come from adeep learning background this is more incomputer vision maybe butumit means that i have now entered a realmwhere i'm learning a lot every day so wemay listen back to this podcast andthere's something wrong i said and ilearnedwhat's what was supposed to be the rightanswer for example somy aim is going to be and i've starteddoing that i think i should be doingmore isteach the community or like tell of thecommunity about what i've learned alongthe way sothere's an aspect ofeducating people inspiring people toto startusing certain tools or getting intocertain areasum and there's also the other aspect ofmakingthe tool that they're using morecompelling for them to use for exampleumi feel like there's so many layers ofdeveloper advocacy you can do so muchwith it it's a socialum role it's ait's a technical roleumbut it's also because of that extremelyflexible and very prone to toto be a role where you can use yourcreativity a lotyeah there's so many different ways ofpackaging up the lessons you learn abouthow to use the software and trying tomake it the mo in the most digestibleformat for your users or developerspeople interested in seeing what youhave so i've been kind of thinking aboutlike the flow of how these thingsconnect likeuh one thing i really like is the kerascode examples the deep learningframework and they have like computervision natural language processing andthat kind of organization of theexamples and and they implement thingsand it's very likei think it's a very great way to getstarted so i've been curious about howlike maybe we've ate examples we haveour github repository how we could kindof get into that kind of organizationwhere we have uhuse cases like uh e-commerce or uharchive like scientific literaturemining or legal or like the differentkinds of like real world things and thenmaybe the applications like uh searchquestion answering fact verificationlike kind of sorting starting to lookinto the nuance of the tasks and thatkind of thingso i've been kind of thinking about howyou can maybe go from like github toyoutube to medium sub stack like howthese different kinds of like platformsflow together and i think hugging facespaces is alsoemerging as one of thesecore content platforms for developeradvocates do you like do you thinkyou'll view hug or maybe even alreadyview hugging face spaces assimilar to github like in terms of thecontent platform for showing the codelike what would you put more emphasis onithink i i view hugging face as a veryimportantmedium for developer advocacy especiallyin the world ofai and machine learning and nlpfor us for example at deep set huggingfaces reallythe the medium where we share our modelswe've trained with the nlp community andfrom now on also give them examples ofhow these models can be used withhaystackgithub is where we have ourframework host oursource code for uh the haystackframework hostedum but i feel like hugging face yes andit is going to be more and more for meuh verylike it's it's such an opportunity to toget to the people whoare every day interested in thisparticularbranch of research who are interested inquestion answering or summarization oranything to do with nlp who wouldbenefit from using haystack or deep setmodels but don't necessarily know oftheir existence yet but now with huggingfaceand the whole community vibe let's sayaround itum it's it's going to it is already avery important umdomain for me to get to those peopleand one other question i want to askabout the uh the game of thrones demo isso you have you have such a tangiblehere's how you do something useful withhaystack and you mentioned the idea ofmaybe having a live audience how wouldyou think about designing that kind oflive presentation would it be very uhlikewhat kind of data sets do you all haveoror let's look at the game of thronesthing and really try to understand thatactually it's a very good question andon time as well because in a week's timei'll be doing this uh in london at odscalongside you actually but i thinkyou're doing it um online aren't youyeah unfortunatelyso i'm actually going to be using twodemos there um one is a it's a differentapplication it's not the game of thronesone um and it's alive haystack demo question answeringdemo aboutworld countries and cities andinformation about them for exampleum and i don't think i have to focus onone demo over the otheror like focus on like the game ofthrones data set specifically it's moreaboutdisplaying how semantic search works andhow that's different different tokeyword search for example so any livedemo application where i can sort ofportray to peoplehowi just asked the question to thisapplication and clearly it's returned ananswer but you can already see from theanswer that it got an understanding ofwhat the context was it had a semanticunderstanding of what the sentence theanswer was in wasum so it's more about that it's not muchabout the exact datayeah and it seems like it seems likesuch a generalizable uh pipeline theretrieve then read kind of thing tooi think with with the we've a thing andwhat caused me to think a littledifferently about it is where we havethis kind of like class property schemasetup where you can use cross referencesto say achieveuh like maybe it's like uh scientificpaper has author then you also have anembeddings for author like you have anembedding set for the papers and anembedding set for the authors and youcan do like multimodal yeah like kind oflike hierarchical like uh take advantageof the structure of documents with thatkind of thing so i think thatintegration with structure is what makesit a little tough to just say uh likehere's the template that will work foryour data set i think it might require alittle more like okay well what kind ofrelations what kind of like semanticontology sort of that kind of thinkingcan we do in your data and then how canwe use that to build better vectorindexesi think that's maybe one thing thatmakes it hard to think about for me uhthink about like a generalizable way tosay here is we've and here's how to useit for everything kind of like so butyeah uh so one other topic i want tokind of come into is some of the thingsthat are exciting you about nlp and thethings that you're working onoh manum that's a that's a big questionum i'm really mostly excited about to behonest with you a lot about what we'vereally already talked aboutis and it's a challenge for me so i'myeah like i said i'm learning along thewayumto methe interesting part right nowis um explaining to people who are notlike people like me maybe evenwho maybe i'm a bit further than thatright now but who are not nlp savvynecessarily but have been a part of thiswhole nlp and ai hypeand they are starting towant to understand and learn how thiswhole thing worksso to me the most exciting part rightnow is taking a step back and givingpeople the big picturewhy do we have this retriever readerpipeline in the first place what is theqa model actually doingand it's not it's not that easyexplaining this to explaining this topeople who areuh interested um it's really fun butit's not an easy task um so right nowthat's that's the main thing i'mfocusing onis it so explaining it from scra from sois it explaining the retriever readerversus thelet's build a massive end-to-end modelkind of angle of it or here's what aquestion answering it model is to beginwithboth of them so because here's what aquestion answering model to begin withis a nice leeway to explain why aretrieverwhy a retriever step might be veryusefulthe other thing by the way that is quiteexciting is umlong-form question answering which i'mstill wrapping my head around which is agenerative question question answeringumatechnique whereyou you get basically long-form answersand i haven't got my hands on it yet butwe have it available in haystack so thisis another thing i'll try soonyeah that the generative stuff is isreally exciting i definitely thinkthere's like going to be a lot ofopportunity with it and uh quickly thisis some you quickly mentioned that youcame from more the computer visionbackground and i also kind of had thatbackground so maybe i wanted to ask youabout uh retrieved and reading computervision does it make sense in thatcontext where maybe for imageclassification we're going to do a k nsearch and then similarly kind of reasonabout the label given the additionalcontexti'm now imaginingfind thea stack of images where you say find thei don't know red ball and you have aretriever that gives you the first fiveimages that that a ball might exist inum yes so computer vision was um i didmy computer science bachelors andmasters at the university of bristol andmy master's thesis was around computervision um so that's howmy background is in computer vision umsuper coolso do you um like are you excited aboutsay clip multimodal embeddings withimage text and kind of seeing if thisretrieve then read thing also kind ofworks in the multimodal embeddingproblems like maybevisual question answeringvision language navigation trying toremember the image text like imagecaptioning those kinds of thingsi think we actually have a few people umworking onstuff similar to what you're describingbut don't quote me for itum yeah so maybe one dayyeahyeah it's super cool seeing the umlike all the different data domains andhow i think languagelike language to me seems to be and isaw a really interesting paper wherethey were able to turn hyper parameteroptimization into a language problem andi so i think lang in my view language isthe most powerful interface maybe that'sa hot take but i think like language youcan specify most tasks in language oruse language touh specify the task you're trying toachieve and put it into the spacesomehow to get a better representationof thingsyes i do think like i i do i'm of quitea firm believer that we're going tostart interacting with machines ingeneral in a very different way to whatwe've been doing so far and i thinklanguage is going to be one of the topthingsit is already like even even nowevery every single interaction we havewith our machineswith our browsershas somelanguage aspect natural language aspectto it um and on the on the topic oflanguage and quest and imageshow cool is it now we can like describein humanlanguagewhat's art we want and we have modelsthat can do that how insane is thatyeah the the the dolly thing has beenliketruly insane to me what do you thinkabout dollyi mean i'm just having fun with iti'm just having fun with it honestlyum but i i'm really curious as like howfar we're gonna go with this like wherewherewhere's our peak when it comes to thissort of thingyeah i'm i'm actually a little afraid ofthe video models once you take dolly andthen itdoes videos and it can like generate allsorts of videos with that superrealistic nature to it that sounds to melike a little bit like well that'spretty crazy deep fake that kind ofthingum do you think there's i saw this onecriticism from gary marcus about likecompositionality in dolly whereuh that prompted something like a redcube stacked on top of a blue spherenext to a yellow cylinder is someprompt like that and it's unable tochain together these symbolic componentslike every time i think like i thinkbecause i've seen people also on twitterput together crazy prompts like that andshow look it did do it so i think it'smore so like well it just doesn't do itevery timeso i think maybe this also yes all rightum i was just going to say isyeah i mean it's a it's a generativemodel isn't it soumbutmy view of it would be i'm now curiouscould you send this over to me afterthis podcast recording um about thecriticism umbuti'm i like looking at this sort of stuffa bit like a pollyanna view it's likethis is just the beginning obviously notnot everything's gonna work perfectlybut isn't it insane and quite reallycool that it can do you know these threethings for example anyway you canimagine that in the future it's thosethree things are going to be those 30thingsyeah that's my exactthe thoughts on it too that it can dothese three things and i think that'skind of like the state of deep learninggenerally right now and and i thinkextractive question answering to comeback to that is more solidly correctcompared to the generative models wherethe generative models and deep learningthey're not it's not like correct like asymbolicalgorithm like traditional kind ofsoftware is it's likeyou know it it might amaze you or itmight like totally fail it seems to bethe way to look at these things likeyep and i've also kind of seen that asi'm exploring the retrieval models i'vekind of seen some cases that look likethat too where if you look at thedistribution of whether it returns thecorrect rank it's either one it got theexact thing or it's like 10 plus like itcompletely missed it and then and thenyou'll have like low density and twothree four five for like evaluating therank order so i think that kind of thingif it's either super correct and amazesyou or it's completely wrong is maybeone way to think aboutthe failure of deep learning modelsso anyways i think we covered a reallygreat set of topics is there anythingmaybe that we missed out on haystack anduhand question answering and any generaltopicah no i don't think so but i was justgonna ask you the last thing you saidwas this about the retriever stepyeah like um looking i'm i'm reallyinterested in trying toyou know get my hands up into theresearch of it and doing things like uhlike themean reciprocal rankrecall these kind of metrics tolook into the performance of theretrieval models and really i want totry to answer this question ofoff-the-shelf model versus fine-tunedmodel and explore that forscientific papers i think the beirbenchmark is one of the best ones outthere that has umi think it's likeyeah differentdifferent information sources likewikipedia books maybe legal documentsand then different downstream tasks aswell that are attached to them sort oflike wikipedia has feverand i know i'm going all over the placeif people listen to this but there'salso like um knowledge intensive taskswhere it's a bunch of downstream tasksthat are built on top of wikipedia sothis is a way of testing these retrievedand read pipelines for different kindsof downstream tasks using the same kindof information sourcebut yes um i'm really curious in justseeing the uh retrieval performance indifferent domains so the beir benchmarkis like the dream thing that i wouldlike to get to just a side note aboutthe retriever step being super accuratelike the top one working out or the youknow it's all down the number 10number 10 document that it retrieved ina question answering pipeline reallyif it was a top one life would be supereasy because then you know the answer isin that one rightumbut the point of that retrieval step inquestion answering is thatsomewherein the topx that's thequestion answeringpipeline wanted you to retrieve modelretrieve documents from somewhere inthose topxthe answer exists so it doesn't yeahdoesn't really matter that it's thefirst one the third one the tenth oneit's just how well is it in putting itin that top xd what do you think about re-ranking thelist based on the uh the certainty ofthe answer classification soit might be ranked position seven butthe question answering is like i'm 96sure that this is the answer whereaswith the one through six it was like 40you know so like you can kind of correctthe retrieval with the re-ranking of theumwe actually have a i think either atutorial or a demo or some some sort ofum demonstration of re-ranking documentsbased on which onecould be maybe the most relevant to thequery that was asked and so that'sdefinitely a use caseum but uhin the context of uma question answering modelumit's just received for for the questionanswering model the only thing it caresaboutisn't the rank it's totally about thisis the amount of the or the amount ofcontext i'm going to look through itmight decide that the best answer to thequestionwasat in the first document that wasretrieved it might decide it's in theseventh document it retrieved it reallydoesn'tcare about the rank that it was giventhe context ini think that's bringing us to anotherinteresting point of whether you want tostack all of the retrieved evidence asinput so it's like one big input or ifyou want to run the question answeringto each thing and i think there's liketheuh the fusion in decoder is whatfacebook calls their model of umseparately taking each of the retrievefacts and then putting it in that likeencoder decoder transformer kind ofarchitecture where uh you don't have theattention over all the facts at oncethat kind of difference where uhand by doing that you can have like areally massive list of facts compared toif you need to do the cross attention onthe facts yeah this is where we get intoa whole othera whole other topic based on um thecapacity the input capacity that acertain model hasyeahwell thank you so much tuwana i reallyenjoyed this weeva podcast and i learnedso much i think this is such an excitingtopic to retrieve and read pipelinesthinking about uh developer advocacy howto create kind of artifacts todemonstrate what we're learning thedifferent mediums like hugging facegithub youtube podcasting and all thesekinds of things so thank you so much forcoming on the vba podcast thank you verymuch thank you for having me this wasfun", "type": "Video", "name": "Tuana Celik on Question Answering with Haystack - Weaviate Podcast #20", "path": "", "link": "https://www.youtube.com/watch?v=kJtixgqdlHQ", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}