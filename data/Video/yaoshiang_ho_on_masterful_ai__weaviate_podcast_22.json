{"text": "Thank you so much for watching the 22nd Weaviate Podcast with Yaoshiang Ho! Yaoshiang is a Co-Founder of Masterful AI, ... \nhey everyone thank you so much for checking out the wva podcast today is going to be another super exciting episode we have yao shang ho from masterful ai uh masterful ai from my first impression of it looks like this super interesting model training and deployment computer vision software and i'm super excited to just dive into the details and get right into it so uh yash and thank thank you so much for coming on the podcast and can you tell us about masterful ai yeah absolutely glad to be here thanks for having me so you asked about you know what we are but let me just let's set the stage a little bit so you kind of understand the problem so obviously we know kind of deep learning became a big thing in the mid 2010s alex net kind of groundbreaking performance google acquires the company behind alexnet gets jeffrey hinton as part of that deal and a lot of use cases have been kind of been cracked with it including then deep learning also applying to nlp not just cv um so then kind of looking at the tooling a little bit um like back in the day you had to access gpu through shaders thank god at least there's like cuda now and then tensorflow and pytorch built on top of it and cloud providers offering hardware really easily but but all these platforms are still really really hard to use like you have to be a deep learning expert and as you know it takes years to become a deep learning expert so you know if i would describe the problem today it's sort of like there's no equivalent of a mysql database or nodejs much less a stripe or shopify there just aren't these like high-level services and platforms to solve all your problems yeah there's tensorflow but it's like it's basically like having to program all of those libraries i just mentioned in c plus plus yourself just to get an application going so so let me answer your question you know really directly masterful uh is a platform to build computer vision models and it simultaneously has a very simple interface and internally it has the most advanced algorithms so kind of think of like when you go to aws and fire up a terabyte of s3 storage it's so simple for you like three four clicks but you know behind the scenes there's all those machinery going on there's all this you know hardware being procured you know connections being made um the advanced algorithms inside are what makes it like a really ready to deliver production-ready models and i would contrast that with some other automl platforms um that are out there in computer vision so it's got a metal learning engine it's not an off-the-shelf one it's one that we built just for our uh engine and what that does is it takes away a lot of the hyper parameter tuning that the developer has to do and that's what delivers on that promise that you pass in the data and you get them all it's not you passing the data run 500 experiments track them all and just sit and act like a human grid search algorithm for you know like five weeks which is kind of how you do it today um it also delivers the best regularization policy um regularization means um uh basically means getting the most information or training the most accurate model from your labeled training data and then on top of that it has a semi supervised learning engine which means your model will get even more accurate data and get even more accurate from the unlabeled data that you pass in so that means images with no labels that technology is available today i think you saw our technical report so you saw some of the algorithms that we've kind of adapted including like noisy student uh barlow's twins um and and some clr um so that's that's what the platform is today and and you know the problem is trying to solve for for developers so so with our webv8 audience i think the sim clr and barlow twins implementations are just super relevant to what we're doing because that's where we get these embedding models that produce vector representations that optimize for semantic similarity but um breaking down the things you said uh you know we have and then also the the meta learner the hyper parameter sweeping and reminiscent of things like weights and biases and determined ai and all that kind of uh user interface for your model training stuff but so i'd love to kind of break these things down and go into them one by one and i know the web listeners probably want to go into barlow twin sim clear first but can we actually start with noisy student because i think you have some really interesting perspectives on the noisy student implementation in masterful yeah so notice student is kind of conceptually drawn from a well-known idea of knowledge distillation which is like one teaching one model generates soft labels from unlabeled data and then there's a noise function and then a student model gets trained as well and that's kind of like the basic theory and i think where to contextualize it where noisy student is arguably advanced and different from uh some clr and bar less to sort of unsupervised training techniques is um noisy student i think was one of the first to prove that he worked at very high cardinality data um whereas some of the other techniques were more optimized for lower cardinality data sets so for example a lot of times like if you look at the original sim clr paper they'll use like one percent of the image net labels and so you're using a much smaller cardinality like in the on the order of 10 000 labeled examples whereas noisy student was kind of trying to prove that they could do it at very large cardinalities so they did full imagenet 1.3 million images and then used google's proprietary jft 300 data set with 300 million unlabeled images to then further show that you can actually improve on on imagenet supervised training without necessarily adding additional model architecture size although that helps as well but you know i think one of the things i want to point out since your audience probably understands some unsupervised uh pre-training techniques like cmcr a little bit better there is a very strong similarity between these and it's not often thought about but in both cases you are training two models kind of at the same time you are noising the inputs between those two pairs of models and you are trying to get them to match up with each other so although noisy student often is thought of as a teacher and a student ultimately you are trying to get two models to match just like barlow's just like sim clr and then the loss function in noisy student is obviously kind of you know a weighted cross entropy whereas in sim clr it's contrastive and you know barlows has their exotic loss which kind of um you know adds its own kind of uh variance covariance and um sorry i forget the third part of barlow's last function but ultimately you are trying to get two two parallel paths to take the same image noise them and then see if they can ultimately match so they're not as different as you might think is when you kind of lay them out side by side yes yeah i love that perspective of the similarity between teacher student knowledge distillation and then siamese copies of the same network as we're trying to learn representations and then maybe just a quick i do think most listeners probably are aware of this general paradigm of contrastive learning but the idea is that you have a siamese copy of the network say this is your burp model and this is also that same burp model and they run it and then they get the vector embedding vector embedding and they'll be uh you you want them to align positive pairs and make it dissimilar to negative pairs so that's kind of the background behind that idea so just to sort of lay the foundation a little bit in case we just dove right into it a little bit sorry about that that was my fault with the way that i set that up but but let me um so the thing so noisy student for one this idea of high cardinality such an interesting insight i'd i'd never made that connection before that that i i guess i wasn't aware of the history of it that they were trying to show that knowledge distillation can work for saying a thousand label set i could you take me a little bit through that before we move on yeah um less about the number of labels about the more about the number of examples so you know as you know in computer vision the classic data set 1.3 million uh examples in imagenet um so again just to repeat like the sim clr often they'll take one percent of the labels and treat the other 99 of the images as your unlabeled set and so the point there is they say hey off of just you know 10 000 labeled images we can get really really good accuracy but we have to use you know 1.2 million unlabeled or draw from that pool of 1.2 million unlabeled images whereas with noisy student they said we're going to use all you know 1.3 million labels from imagenet and then we're going to take 300 million more images to use as the unlabeled dataset and prove that we can still improve the accuracy of the model um there's a talk that a guy named godarus gave and i'll um send you the link to it so you can append it in the show notes where he kind of contextualizes just kind of where these kind of ideas were kind of fitting in with each other and why they kind of were solving slightly different problems i remember when i was reading simclr i'm like why why does some clr use this really tiny data set and why does nobody just didn't use the full one you know and there is a little bit of a history and they were trying to solve slightly different you know slightly different nuances of the of the problem of ssl hmm wow yeah thank you i think that's a really interesting angle to it that i hadn't thought about kind of the direction i was i was thinking we were going to go in is talking about the the noising used in noisy student and how we have the augmentation and as i was going through the masterful documentation i saw this thing this phrase augmentation clustering i read that i need to know what that is yeah yeah well let me tell you just give you a very brief history here so i mean um within uh with a noisy student there's three types of noise that happen there's just classic data augmentation which by itself is extremely powerful and sufficient to make it work and they add two other forms of regularization that you know i'm sure everyone's aware of one is dropout which is classic and maybe one another one that maybe is not as used as much as a stochastic depth um but the point is um that to get these techniques to work you have to have the right regularization technique you cannot just throw in i'll just do a couple image blurs and a zoom and a darkening and i'm sure that'll work like these techniques are very fragile so in a sense when you talk about noisy student training people get excited more by the student because it's kind of architecture based but that you could argue the noise is at least as important um to the idea of noisy student training and what we found um in the lineage of let's just take data augmentation is that um there's a very strong sense of overfitting to imagenet and imagenet images all have a similarity that says that they're all taken by human camera by humans holding a camera whereas a lot of other applications don't have these attributes geospatial in manufacturing sometimes the cameras are very fixed sometimes they're very precise sometimes you don't a satellite doesn't go up and down or i guess some do but a lot of satellites stay in you know geostationary orbit or at least at the same altitude like they have the same resolution no matter if you've got a camera pointing at a manufacturing line like that is it that that camera is not going to move around so that's some of the explanation i think of why a lot of the classic ideas of how to uh build a good regularization policy in my opinion are just kind of over overfit to imagenet so kind of the lineage that kind of we study and we mentioned this is kind of the auto augment and then randog and auto augment is a very very deep search the problem with the depth of that search is it's 15 000 runs in their paper full training runs to figure out the correct policy so that's obviously you know just intractable for any like normal developer and then randog which i think is still considered state of the art because i think some clr used it um tried to reduce that search space by clustering they didn't use this term but if you kind of think about it that's what they did and the search space reduces down to about on the order of 100 searching the number of uh transforms and the magnitude of those transforms but the insight that we had is that um the way that they clustered those transform magnitudes was just on a scale of 0 through 30 and they would basically test all the transforms at level 1 all at level 2 all at level 3 etc and then all at level 30. so you know they didn't specify the search algorithm specifically but their point was in the paper um the search space is small enough to simply grid search so presumably that's the algorithm they use 100x is still way too long for any practical person i mean if you're renting you know aws machine for 24 bucks an hour like you know good luck telling your boss i'm going to need 100x more time you know like instead of 2 000 i'm going to need 200 000 like you know forget that that just that's not going to happen so what our insight was and it's kind of based on um a very very simplified version of uh an idea from gans which is for inception distance is the distance of something really can give you insights into how much you're perturbing the image so maybe darkening or brightening an image by one click help affects it very little on the darkness transform but maybe on the rotation transform it's extremely destructive and so that means that instead of just saying i'm going to search click one of darkness and click one of rotation what i really should be doing is searching click one of rotation and click 50 of darkness because they're more equivalent when you look at the distance that the image generates so how do we calculate that distance i mean you can have you know your choice right you can do very exotic distances you can do l2 of the feature maps uh another thing you could do is just you know um calculate the you know difference in the scalar losses when you rotate an image in forward prop uh versus darken so that's how we kind of we run forward prop to try to understand how much all these transforms affect the um affect the model perturb the model from some idea of a distance from the unperturbed images the non-noised images once you have that then you can very quickly cluster like five big clusters and then grid search five clusters or three clusters instead of grid searching um 100 clusters so that's a little bit of a nutshell and um i apologize we haven't quite published uh that algorithm in detail but uh that's the kind of heart of it is instead of clustering just based on some heuristic of you know magnitude we actually impute these magnitudes based on how much it perturbs um a model's uh performance on specific data wow that is super that is so interesting and i maybe i do want to kind of just get a couple of terms so just yes because you're so knowledgeable and i really just want to dive in but i also want to sort of lay the ground groundwork for people listening um so again a noisy student the teacher labels the data for the student network to learn from we're going to augment that data for the student to learn from so we have data augmentations like rotation blurring cropping flipping increasing making it more blue than usual like there's a massive set of image data augmentations that are label preserving so then came along the algorithm rand augment uh rand augment has two hyper parameters n and m n is uh how many to apply in sequence and then m is the magnitude to because each one has an associated magnitude with say rotation you're either rotating at 60 degrees negative 60 degrees or say 30 degrees right so so the the clustering yao shang is describing is uh n and m they do they define a cluster in this space of the massive configuration of the n and the m that you could have uh so then for shea inception distance was this thing used in gans that's like um you you gans are a generative model before diffusion models kind of stole the show where it's like real fake loss and you say how do i say that these generated images are uh quality images so what you do is you have the vector space of the generated images and then you have the vector distance of these generated images here to your original images and so that's that kind of foundation so and so now i think where we are is is we're looking at the vector distance between augmented images and the original images to define that of augmented images and and so this is super relevant for uh so noisy student of course so you're saying which augmentations should we apply to the image for the teacher to label for the student but also for our general framework of training contrastive learning models that produce like the vector embedding models that go into wev8 is we have to have this algorithm where we start with an image or any data point if we're well let's we'll keep it to images because that's that's like most of the label preserving data augmentation research has been done on so we start with an image and then we're going to transform it into the positive image and then align those vector embeddings that's the training objective okay so i hope that was like a decent little like kind of uh primer to it so now let's dive a little more into this idea of okay so we have so we're augmenting images and then we have semantic clusters that come from the augmentations so is that i mean do do augmentations really like throw off the semantic representations so much i know that there's like an issue with robustness with these models where say um you take your test set and if you rotate all the images suddenly you might go from like 90 percent to like 60 percent yeah so yeah in your experiments these augmentations they create like very different clusters uh i'm not sure how to answer that i mean the purpose of these clusters was to cluster all of our transforms by the distance or by how much it perturbed the model so i think what's interesting is maybe i'll answer it this way if you look at a lot of research when they're classifying imagenet they're just going to use a human prior and do a mirror for example but if they're classifying uh mnist or svhn they're like oh i shouldn't do that because numbers shouldn't be horizontally mirrored um and indeed what we find is if you you know if you if you if you rotate the num new numeral by 180 degrees or you uh mirror it you know models are just going to get really confused by them uh and so you you see that reflected in the amount of distance that gets come that comes out when you measure you know the intermediate future maps or the final value so that immediately tells you that's a that's a fairly destructive transformations and you know it usually gets excluded very quickly by the search algorithm so i'm not sure if that's exactly what you're asking but yes we we do find that this approach kind of automatically figures out um these ideas i think another thing that maybe happens in geospatial a little bit is because the objects are often very small i mean the order of you know single-digit double-digit pixels as opposed to imagenet um this is my hypothesis i haven't done the study to prove this but if you took if you rotated the image of me by five degrees you know you'd say oh that's still me no problem but when you take something that's like three pixels by eight pixels and you rotate it you're starting to get aliasing issues right the old computer graphics idea you know you're losing resolution you're adding blur to it and so i you know what we've also found is a lot of times in geospatial images these mild rotations are end up being very destructive in a sense the more mild the worse it is because the more aliasing you induce a 45 degree rotation is much better from a than a two degree rotation i'm not gonna i'm not gonna prove that i don't have a paper to justify it we haven't done all the studies but i'm just kind of giving you a flavor of some of the things that we've kind of observed as we try to explain why our algorithm has discovered you know certain made certain decisions about the right policy for you know in this case data augmentation and applies to regularization as a whole of course it's kind of reminding me of this idea of like um so out of distribution detection and so using the vector representations of images to determine that uh we've so yeah so we're up in a satellite and we've got uh i think you look at like crops or something i'm not an expert on this kind of thing but and it's rotated such that you really have it's you've corrupted the label it's no longer label preserving so from the vector representations of the images before and after augmentation does that give us enough signal to do out of distribution detection uh would i i don't know if maybe i should have said out of distribution i should have said just like label corrupting yeah this idea that you've lost the information and you can tell that purely from the vector space and thus you have this heuristic to say let's actually not have the teacher label that for the student and noisy student is that how it comes or or when we're training simclair models let's not make these positives because that augmentation is no longer a positive yeah we we we we don't um we don't use this technique to take things out of distribution and adjust the label as much as we just treat it as a non-label preserving not sufficiently labeled preserving transform and we don't use it typically in um uh you know these regularization policies you kind of have one policy and so you run it and the goal is that one policy will sufficiently perturb the data so it's not identical but it's still within distributions it still you know feels like natural data to the model um if you and i eyeball it usually it looks like you know relatively normal um but again it's it's that's just trying to explain what's kind of happening inside um but what's happening inside is is um uh it's it's easy to define kind of mathematically but it's hard for me to uh to truly tell you you know why why it's working these are just sort of you know my interpretations of why i've seen these policies come out very differently for for very different types of uh data sets i think the broader point is that you have to be adaptive to the data set and it kind of goes back to the point i was saying that a lot of research is overfed to imagenet and imagenet you know people think it's hard there are a lot of things that make it really easy actually it's a balanced data set for one thing there's very high entropy in this data set um it's it's it's it's it's all handheld uh cameras taking it so there's all these attributes about it that i think research is probably kind of like over fit to and you know when you try to apply any of these techniques out to production data sets in the real world you just see how it just it just doesn't work nearly as as well as it does on you know imagenet or cypher10 or something like that could we maybe stay i kind of want to explore one more idea in the space of using the vector representations and how they change and i'd like to kind of dive into this idea of active learning and i think it's maybe relevant to the masterful model training suite and yeah learning is basically this idea where let's forget randomly sampling the next batch let's try to intentionally find the next batch of data that would be the best one to train on and i've seen papers like focusing on the biggest losers where you say you know take your batch of 64. this one had a huge loss and so maybe you do like a vector search with something like we v8 to uh go get more like that because that's like clearly like a weak spot in our data space right have any thoughts on that kind of idea yeah absolutely so i'll um you know it's on our product backlog to um let me step back a little bit because a lot of our unlabeled data comes from video feeds um the problem some of our customers have is well video if you treat it as a sequence of frames of images you got a lot of it you get to millions and billions of frames really quickly so the question becomes i can't train on all of that so my naive algorithm my null case would just be randomly sample it or uniformly sample it i can't sample all of it but like can't we do a little bit better and so indeed we have done some research internally on using exactly that active learning ideas to say well what can i pick off from these you know millions and millions of frames you know can i find like the one percent that's going to help my model learn the most and i think one of the first algorithms like a lot of us go towards is great we'll just pick the one the model's wrong on that's got to be the one you know to fix um and sometimes that's right but sometimes that doesn't help because sometimes what ends up happening is you force your model to get really good at these really rare examples and at the expense it forgets all the simple examples so like you may have solved that one percent really hard problem but the 99 simple ones you know you just degraded that performance by 50 because your model was all this energy so it's like it's not as easy as you know as i think we we we think when we first you know start this problem and um uh so um some of our internal research in this area is um probably less oriented towards the vectors although we have ideas on on that as well um but a little more kind of going back towards the gan idea the generative adversarial network is to say you know can i train um can i can i again typically is thought of as generating synthetic images and the training is that there's a generator that generates the image and the discriminator that tries to decide if that's a real image or a fake image and they fight each other in a so-called mini max competition so you train one and you train the other sort of um the analogy people give is a counterfeiter and a policeman so you train the counterfeiter by having the policeman tell you you know is this a counterfeit or not and then you freeze the counterfeiter and you have the policeman now try to train itself to get better by telling it the reality of whether this is a counterfeit or not so these two kind of compete with each other to get better and so without giving too much away we have a similar idea that the generator is uh you know not really generating synthetic images but just in this case has full knowledge of whether it is um a label data labeled image or an unlabeled image and the discriminator tries to figure that out and if we can get to the point where um we can trick the discriminator into thinking that uh an unlabeled data image is labeled then maybe it's gonna think maybe that gives us an algorithm to pick uh the most useful unlabeled images the ones that don't look like labeled images in other words the ones that are not going to the ones that are actually gonna uh sorry let me start over uh if we can train a discriminator using this gan thinking but not a true can again in the classic formulation then we may be able to use that discriminator to pick out the most valuable unlabeled images that can teach the model something without being so hard that it's going to destroy uh what the models learn but not being so easy that you're just wasting your time you know back propping on really simple images so that's sort of some of some of the heart of how we've been kind of thinking about it um so apologies it didn't go into the vector space but you know our thinking there is a little more inspired by you know sort of the discriminator of again yeah i think um i had something i was going to ask you and then i and then i think i gained a better understanding of of this idea and um so so purely from an unlabeled image the discriminator would be able to tell if it's been labeled because you also have a classifier that's been trained on a labeled set like how does it connect the information yeah um the uh the discriminator could have information about uh would have information about the vectors that come out of a trained model um either the final prediction or the intermittent feature map so those would be the vectors or the embeddings and based on that its job is to predict you know is this data point labeled or not and uh you kind of you kind of use that to figure out these are the ones that are too easy so don't send it to them these are the ones that are too hard and here's ones that are right on the bubble where there's still some learning value that's not going to trigger you know forgetting of some kind so yeah again a little more inspired by the kind of gan discriminator concept um uh than necessarily um sort of vector distances although we have other ideas on you know using vector distances in other um you know active learning type ideas where uh similar problem you know just can i label can i pick can i be more intelligently pick the data to spend my money to label as opposed to just randomly sampling you know data points i'm seeing from inference or or using human judgment to you know find you know visually go through the data and look for clusters is there some way to try to automate that yeah the way that i originally described active learning is i remember a paper called mind your outliers that is a setting visual question answering data sets and that exact idea of which one was the biggest loss and they find that like you know you cannot it's impossible to answer and that's why it is so so yeah being a little more clever about that uh so so semi-supervised learning and as i went through the masterful documentation that you know jumped out to me is looking like one of the big focuses of masterful is on the summers obviously we talked about uh noisy student where in addition to having the image that you it's not just like you have the label data set and you take the image apply noise to it teacher labels that student learns it it's also like you have unlabeled data as a part of this kind of thing and and that's the idea of semi-supervised learning you have label data big unlabeled data set so i and i'm sorry if these names have you heard of the ada match the kind of uh that kind of approach where i think the key thing is they also they have a loss function at the label layer and then also in an intermediate representation of the teacher got it got it kind of so i guess kind of the question is uh like what what are the uh what's the landscape of semi-supervised learning look like what are kind of some of the cutting edge ideas yeah yeah um so you're describing you know some of them uh which is um kind of stepping back um when you take the pair of models and you ask them to kind of match up you get to pick whatever kind of layer to try to match up and i'm not as i'm not obviously on a to match but sounds like you're trying to get intermediate layers to match up as well as the final layer you can argue that you know in noisy student you're getting the final final layer to match up which is a post postsoftmax um so it looks like labels but the fact that this label is almost not as important as the fact that they just had to pick some feature map um some output of of of a convolution and and relu and in a lot of other models like barlows then they stack on a whole bunch of other layers which they end up throwing away and then you just use an intermediate uh layer so almost think of like you know you got resin at 50 and then you force it to do and just throw away the final layer so you have just resonant 45 and you just find that the intermediate layer is more generalizable in some way whereas the final layers get too task specific um towards the pretextual task that you're using pretextual means you've created a task for these models that is on the surface not actually solving your problem but through solving that pretextual problem you end up solving the problem you actually care about which is to learn better representations which will get you a more kind of accurate model um uh so i think um one of the most interesting things is i think when you look at semi-supervised learning um uh although as i mentioned noisy student and then the unsupervised pre-training techniques like barlows and simclr look very different they actually have a lot of similarities ultimately but you know sending that similarity aside and looking just at the idea of pre-training this isn't the first paragraph of this mclr paper they talked about um contrastive as one basic idea but the other one being generative and so generative you know they actually cite you know good fellows gand paper um so could you train again uh and then use the representation in a gan to act as a technique for building an unsupervised feature extractor and the conclusion broadly was no those representations waste too much energy trying to get pixel perfect images and so they aren't generally very good but you know kind of the latest latest right now is uh you know transformers and uh feature reconstruction kind of taking over where you just uh you know take an image and then you literally add white noise to the pixels of an image and you ask again to kind of reconstruct that and you know that apparently has turned out to be a very interesting and powerful technique for the model to learn representations uh inside and become a very good feature extractor and you know i think it goes back to um you know some older research that did not beat state of the art but it is i think very well known at this point one was to colorize images so if you have a black if you have a color image turn it to black and white and then train your model it's not exactly an auto encoder it's kind of an auto encoder at that point to colorize the image and then um godars i think his first name is spiros don't want to butcher it i think around 2017 or 2018 he did a classic paper it was just sort of a proof of concept just rotating the image in 90 degrees 180 degrees and 270 degrees and then your pretextual task in that case was to predict the amount of rotation of the image and you know they just found that oh you these these these models um did learn an interesting representation inside it may not have been better than supervised training on imagenet but it did learn something and that was just really cool at the time so then the next question was when does it get better than supervised and that's where you know some clr kind of uh was such a breakthrough in that you know for for one percent of labels it actually does train better than than a supervised technique um so sorry you asked a more direct question and i wondered a little bit but it's such an exciting area and yeah it just always frustrated me that you need labels because images have information so it's not a feature that you need a label that's a bug i mean you know there's there's nothing that holds there should be nothing that holds a model back from learning from images you know without labels uh so this this whole field is just you know super exciting yeah and you're so knowledgeable about it that it inspires so many different like ideas that listen to you and there's so many like questions i there's so many directions like we could go in but so the question i'd want to follow on with is so this idea where um it can uh as i kind of want to pivot into say cycle gans and the image to image translation thing yeah to kind of build up this idea the idea that you classify the rotation angle so you're rotating say that's a 45 degree rotated airplane yeah the cyclogan image to image translation idea would be like you rotate the airplane and then it's like unrotated rotate yeah yeah so so i've been thinking about this idea about like could this help with distribution shift where uh the distribution shift happens kind of and it can sort of map it back to the original thing yeah absolutely does that idea make sense yeah i mean cycle gans were um i mean the cycle part was just a consistency to try to prove that you know if you can go from horse to zebra back to horse very effectively then the cycle was kind of completed and so it was a way to kind of enforce the goal but the real task was not the cycle part but um you know a technique called you know image to image uh translation which says like you know if i have a um you know a horse and a pose can i then generate a zebra in that pose and can i then generate a dog in that pose and um so i i'd suggest people look at you know the series of papers that xunhuang i believe study the cornell under belongi um generated and his first idea was neural style transfer which we all know just just by looking at the layers but then adding adding the same ideas but with adversarial losses so the concept from gans then allowed these more like really even more powerful techniques um we've researched that area quite a bit particularly for domain adaptation as you described we looked at one scenario where um you know there are two major satellite providers and although visually you look at the images they look the same um if you train a classifier just on one it won't do very well on the other it'll be like two-thirds of the performance so you know could you use some of these techniques to to do an unsupervised image to image translation um and um what we found was and it may go back to that first paragraph of the sim clr paper which is when you well maybe it doesn't i'm not sure about that but um it seems like a lot of energy is put into visually appealing translations but it kind of doesn't really fool the model that you're trying to train so we were not able to successfully perform domain adaptation uh using these techniques and i think if i had to hypothesize i think at least in part again image that images are often very large objects very well framed and in the geospatial applications that we were applying you know you've got much smaller objects and so it was much a much more difficult challenge if you have a picture of a horse that's taking up 75 of the pixels and you translate that into a dog i'm suspecting that'll probably be useful um and more believable um but uh this is probably i think the more um i'll go out on a limb i don't think this is backed up by research but i think my observation of just a generative adversarial id of adversarially trained models in general is that they seem to work better for these very large well cropped images as opposed to lots of fine grained images so in the applications we've tried it you know we were not able to um to to to get success i do think there are some startups in the space of you know combining um 3d generated graphics but then using gans for more textures which presumably maybe that's where a little bit stronger so you can imagine having a 3d game engine generate some street level imagery and then use again to kind of texturize it into rainy or night time scenes maybe that adds more diversity to the data i can imagine something like that working um haven't done the research myself yeah i've always been excited about this idea that we can just use kind of data augmentation and maybe even like the way that cyclogan enables a more flexible interface to cover a bigger distribution our data and it seems to me like deep learning can always fit to the training set almost so it's like if you just blow up the training set with these priors about semantic preservation sort of that you can just do anything sort of with it it's always been the guy and think about it i wanted to kind of dive uh reference let me just pause you right there and just mention that you know what it seems like these uh adversarially trained approaches um the distribution actually gets narrower not more diverse so i mean if you just if you just fire up style again and you look at the faces that come out of it you're like oh my god these images look fantastic they look so realistic you know but then like um if you just you know even the paper they admit themselves you know part of the reason they look so good is they're sampling kind of the very narrow parts of the distribution when you push that latent the the the embedding push it to really wide it just gets really kind of like broken so there's actually a shortage of diversity not too much diversity but not enough within and that's why it looks so good it's maybe maybe and again i'm going to go out on limb i don't know if this is true but maybe that's why they look so good as they get really good at you know delivering 100 000 faces you know really effectively but they can't really represent the full you know diversity of you know six billion you know different faces um i think um the the the github for uh that paper that she wants a co-author on foon it few shot unsupervised image image translations it's called it has an image of a dog rotating and all these other breeds of dogs rotating the same way you're like wow they've really transferred that over but look closely all the noses are the same size that's just a little trick you just stare at it along enough you start noticing these little tricks like oh did it really do a full translation all species breeds of dogs or is it just believable to me and you that these 10 dogs look similar but it actually doesn't really represent anything close to the full diversity of dog breeds much less dog poses much less dog poses in different environments so maybe that's one thing i'll just go on a limb and say how about i call that future work hypothesis rather than conclusion or trend yeah well that inspires me thinking about like um i recently dove into a paper called augmax where they flip it to its friendly adversarial training and they also do this uh like tsne space clustering where they show you that uh certain generative techniques they exactly as you describe they just make it super dense on the yeah yeah yeah nice i hadn't read about that but obviously you know i'm familiar with t sneeze and i love actually you know our logo is actually inspired by a t-sne like when we were designing the logo it should be blobby like a t-sne so that's exactly why our logo is what it is oh yeah i see some cool art art where it's like um sorry i'm gesturing like a t-shirt i haven't seen this on a t-shirt yet but i've seen like uh the the data set used to train hugging faces bloom model i've seen that put into an embedding space and it's like colored by languages these kind of ts yeah yeah yeah yeah a the tensorflow embedding has an open source tool you can you know just just cluster mnist and all the dots will fly around you can use other clustering techniques i think umap and uh but yeah the idea that you know these these things are meant to be blobby this is the real world this is not a tabular world like if we're a tabular company we'd have a grid right like a good tabular world where everything's neat and organized but no this is messy this is deep learning this is this is the real world yeah and um we had so i guess uh it's like um so we certainly could continue this conversation for a while i think there's quite a few ideas in this whole thing but i i do want to kind of step out a little bit for the sake of keeping our podcast a little dense and and maybe kind of asking about like how you see the we vva vector search technology playing with masterful and it kind of coming back into like the tools and the software yeah absolutely absolutely so i mean just stepping back to you know our goal is to provide more high level tools for developers so they can access all the power of deep learning without you know spending two years becoming you know deep learning experts and reading all the papers you've read right like like what what person building a back-end app was gonna read all the papers on relational theory and relational algebra and relational calculus like you don't have to do that anymore right you just you learn enough sql to do what you need to do and you got all this power that's sitting behind the surface that you don't need to know about anymore i mean that is the vision we're trying to get to here um so we currently support classification object detection and semantic segmentation and obviously there's a whole bunch more computer vision tasks that we want to support um and there's something unique about uh similarity search you know whether you're applying it for computer vision or for uh text which is um in classification and and the other tasks i mentioned once you have a trained model you're ready to go to inference put it up in you know tensorflow serving or you know onyx runtime put it on a kubernetes cluster or you can use you know easier packages like you know google vertex uh endpoints you're ready to inferent you're ready to run inference you know integrate with your app in similarity search you are not done once the model is trained because now you've generated all these embeddings and you've got you know a million of them well now we've got to search over all of them so to deliver a complete product to our users obviously we need to partner um with a company like like we v8 because you've got to put those embeddings into the database to then allow you know users to do that reverse lookup you know indexing down through those embeddings and then rediscover like which faces is is this or what building or what room is this embedding represent um so um our our initial i think uh thought on working together just as a proof of concept would just be to use a classifier that's that's trained using you know our techniques of supervised and semi-supervised techniques taking an intermediate and betting and just seeing is that sort of a good enough mvp uh embedding just uh just to kind of prove the point that you know we can build an end-to-end product that involves training and the vector search database um and so once we can prove that then um uh you know it gives us you know uh uh more confidence that when we do kind of uh do a full on hardcore similarity search uh support that we'll already have all the integration set up so we can deliver a complete solution for our customers so again they don't have to understand hierarchical small navigable worlds they don't have to you know understand uh randog they don't have to understand uh sim clr they can just say hey i got these two tools that'll solve that for me put in the training data out comes the train model run the embedding put the embeddings in your database and now i've got a way to look up any image by running inference get the embedding look it up in the database and i can figure out what is what's the nearest you know image or person or thing that this image represents so uh so i'm excited to uh to pursue that and you know we've already started some of our uh some of our uh proof of concept work on this yeah that's super interesting i i love i mean it perfectly said that the combination between the model training the end-to-end and then having the database where you stored the representations that have been learned and then the way that the these hmsw structures you don't want to do a dot product with a million images so the way they play together is so interesting and i think that's also like kind of the interpretability of it like when you have a object detection inference you can be like it looks like this i've seen this kind of thing before sort of do you like that kind of interpretability play of it and sorry actually let me i think this is a good chance to kind of we talk about retrieve then read pipelines all the time in natural language processing because in natural language processing it's so intuitive you have data sets like squad where you say what is the atomic number of oxygen so naturally if you retrieve the context of wikipedia going oxygen you know the atomic number eight and then you just can classify it there so with nlp it makes so much sense but can i ask you about this retrieve then read uh for computer vision tasks like classification detection and segmentation to just start there yeah well i would i would i would uh frame that as i you know currently would think of similarity search as a separate computer vision task to support um uh in addition to our you know binary classification multi-label classification single label captioning object detection semantic segmentation um other computer vision tasks we want to support over time would be you know key point detection instance segmentation uh pose estimation uh object tracking um but what's a little bit unique about uh this is uh you know we've built in um some unsupervised pre-training techniques which um you know i think as you know are even superior methods to generate embeddings um but it's also superior to a user because they don't have to label their data to get the similarity search necessarily um the labels will help it's yet another data point so um i'm not sure if i've answered your question but i would just frame it as another computer vision task for us to support and then like i said as a proof of concept we thought could we just take a classifier train using traditional techniques and use that embedding uh as a similarity just as a proof of concept to to get our products to work together with with the support of tasks that we have today and like i said that already uh seems to be a positive yeah super cool and yeah i think with um like self-supervised similarity learning is so sophisticated in computer vision with again simclear and barlow twins the data augmentation interface is so strong that you get this search thing works extremely well without labels and i think that's just incredibly interesting i can ask like broadly um what drew your interest to computer vision if you kind of tell the story of yeah yeah um it goes back a long way uh so it goes back to just being a gamer in high school and uh you know you some people like me get excited about you know can i render my own computer graphics at some point um and so there's that you know i went and started my computer science program in an undergrad at berkeley you know i i put down computer graphics that's a specialty for it and you know once you render something then you want to like know what you rendered and i remember like hacking z buffers just to try to figure out you know basically you would call it segmentation today if you could access the you know the z buffer that you know frustums and all that stuff and um i had a professor john forsythe who wrote an image classifier for horses and that basically ended my interest in computer graphics and computer vision in the 90s because it was just so heuristics based it was just all these like hand crafted ideas if we just throw this and this and this and wait all these little like hand-crafted rules then maybe we can figure out what a horse is and i said that is just so not elegant um and i ended up going to databases ironically enough having a chat with it with a guy from a database company because databases are applied algorithms you know b-plus trees are just you know wide red black trees it was just such an elegant space and you know indexes no no question my favorite part and so it was a really real really enjoyable for me to to learn about uh hnsws and just seeing you know an index applied to a graph instead of you know a vector or a heap it's uh it's been it's been nice anyways um so i was out of computer vision you know from the 90s until about five years ago uh so my co-founder sam wookie our cto he stayed in computer vision he's founded a startup he ended up selling to the nfl to track you know player movements on the field pre-deep learning and when deep learning came out i didn't know anything about it uh but he was following all this stuff he's like oh my god deep learning can correct all the computer vision problems that like i thought were uncrackable at the time so he left microsoft and joined google just so he could kind of join the mecca of deep learning and and kind of you know learn learn the learn the secrets uh you know so um uh you know sam sam um uh sam said to me all the stuff is working take the andrewing coursera course on ml to get up to speed take the enduring deep learning specialization and i hadn't programmed in 15 years and i did and first of all i realized i enjoyed programming but second again or still and then secondly i was just amazed by the power of deep learning like holy cow from all those heuristics from that john forsythe and i'm i'm not denigrating his papers just that was a state of the art at the time but it just wasn't for me but there's just an elegance to to just having machines train themselves uh and so so i was hooked so that's sort of my return my first and second uh love affair with uh with with deep learning and computer vision uh that's the magic of deep learning the whole input output it's amazing it's absolutely amazing it's amazing yeah i had a sim the uh story of uh sam in the nfl thing i i the first computer vision project i dove into is i was i had full basketball games and i was trying to extract like made baskets and i think once like kind of coming back to your point about imagenet and being object centric and you know i i got started studying like c far 10 i think and yeah once you get into like a real problem you really appreciate these things like uh class imbalance or uh just the challenge of labeling data and why these semi supervised like once you get a real project you'll really appreciate the semi-supervised learning thing just because of the cost of labeling the data and all that cool stuff uh so one other question i really want to ask you about is is your experience at masterful ai can you tell me about the inception of masterful and all that kind of stuff absolutely well i mean the beginning of this story is just exactly where my last story you know trailed off which is you know sam got me super excited about deep learning um he was at the mecca you know at google but you know he started telling me some of the um the ugly truth about deep learning um which is huge models huge compute budgets you know uh huge labeling budgets and you know sam instinctively was just like this is cool but like how do we make this work for you know just just you know maybe people companies with two deep learning engineers or maybe just a general you know software engineer like a lot of our friends who you know don't want to spend two years learning all this stuff and that was the basic instinct that he had and um and he thought the technology was there uh to achieve that um which is you know all the stuff in our platform and then on the flip side um the technology may have been there and it may have been his instinct but i wanted to see if there was really a market demand for it so um i linked up with another friend of mine uh who i respect a lot tom reichard who's now my third co-founder um he uh had been a venture capitalist uh or was then still a venture capitalist had been one for about 10 years so i really trusted his insight in kind of what the market was and um in particular he like he wasn't one of those guys investing in social you know media networks he was he was you know investing in deep tech so he invested in a lot of ai companies and he said yeah i've seen lots of startups trying to commercialize deep learning and it's incredibly hard to get the models going so kind of between my you know inspiration and the space from sam and his kind of idea that the technology was ready to go and then tom saying yeah the demand is there we said well if both of those things are there then you know it's the right time for us to kind of build a startup to to kind of bridge that gap and so that's the birth of mass tools so i've got two just you know absolutely amazing co-founders and tom and sam and uh and we barked on this uh this journey can i ask about without sort of saying like could you describe the competition in the space could i ask about sort of the the category that you think that masterful falls into in some adjacent categories yeah absolutely um the most narrow and closest adjacent to us would be um the automl providers and um you know we live up to the same promise as automl feed and data and get back a train model where we've been a little reluctant to brand ourselves there is um we philosophically are are are not in the same place as as most of the existing auto mal providers um they are very neural architecture oriented whereas our hypothesis fundamentally is more data centric we think it's a lot less interesting to try to fine-tune the perfect model size and it's a lot more interesting to understand you know how to get the most out of those architectures so in our in our command line interface for example you can you can select the right architecture we think you should pick the architecture based on your inference latency you know throughput you know cost requirements which you can predict just from you know the architecture so if you're on an edgy device you may need a certain device if you if you have maximum accuracy then pick efficient at b7 you're going to get like 99.9 of all the improvements that you can from architecture just by doing that the interesting thing is how do you manipulate the data through regularization how do you add unlabeled data so so i would say that they are broadly our competition we have a very different philosophy um and not to plug my product but you know we put out the blog reports showing how we beat like like google vertex automl on a whole series of data sets um because i think we take a more holistic approach and a more data centric approach um to the problem um kind of expanding the ring a little bit um thinking about less as just sort of data in and model out think of this as dollars in and more accuracy out you might say that we also kind of kind of maybe you might imagine we kind of bump up up against like active learning a little bit you know again this idea of you know can i be more efficient with my labeling spend but the good news is we don't really compete in that space as much as these are complementary techniques um people can easily have both um so um that kind of contextualizes kind of where we fit um in the space and kind of what what other platforms somebody might be you know comparing us to depending on you know what profile they're in awesome well yeah shane thank you so much for this i'm so grateful for you taking your time to do this and i'm so lucky to have like such a information heavy podcast in our web podcast collections is really amazing so thank you so much i've super enjoyed it i didn't realize we're gonna go like this technical but uh but i'm glad we did because it's um it's just so inspirational you know and sometimes it's hard to it's hard to describe to you know my parents like why this stuff is just so exciting but um i mean the breakthroughs are just it's um it's it's a perfect moment where there's a ton of commercial opportunities so there's a lot of money to fund this research and there truly is like just groundbreaking stuff coming out i mean it's it's deep learning is going to be way more important than the internet uh you know in my in my humble opinion um because it touches real world things it touches you know where people really are out there you know driving and thinking you know looking know agriculture and manufacturing it's just um i'm super excited about a space yeah awesome ", "type": "Video", "name": "yaoshiang_ho_on_masterful_ai__weaviate_podcast_22", "path": "", "link": "https://www.youtube.com/watch?v=dHhQjlrLu9k", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}