{"text": "Introduction and demo by Bob van Luijt during FOSDEM 2020 about the Weaviate Vector Database. More info: ... \nyes thank you so much well thank you forhaving meso it's indeed it's a loss of the day soit's like outside and I saw like a lotof people already drinking beers andstuff so it's like I hope that there'sstill people in the room so thank youfor listening so this is like yeah lastyear my my colleague HN was here any hepresents for the first time we've yetand what we're trying to do and we'vedone a lot in the year so today I wannaI wanna show you actually an update andway more things about we v8 and a lot ofthings have changed also the things thatwe've changed into so quickly agendawhat I want to talk about so first ofall it's about what it is because theywork up you know we're new we're new onthe scene so yeah so what is it secondlyI want to talk about what has changedsince last year or presented here andthis year I want to talk a little bit ofthe about technology and last but notleast of course I want to give it demo Iwant to show it in action all right sothat's the that's what I'm gonna do sofirst about the about we've yet so wehit it an over soared smart growththat's what it is and what we mean withthat well the fact that it's open sourcethat is the usual suspects right sosource lives on get upbut you can use that using docker dockercompose and communities so we have saidall out of the box for youit is smart because of something whichwe call the confectionery and if younever heard about the context unarythat's fine because we invented it andI'm about to explain to you what it isand what it does and I want to use I'mgonna use one buzzword sorry for thatbut what I what is it that seriousit's it a serious notion so sometimespeople talk about like the AI firstarchitectures so like can we buildsystems that have like these machinemodels built into them that we can buildnew solutions and ask you new ideas andthat's what we try to solve withsemantic models so what makes it in ourcase what we like to call smart is thebuilding semantic model and the graph iswell I don't have to in this room Idon't have to you know share to peoplewhat the graph is but the we we'vechosen to use graph QL and the reasonwhy I've done that is because we see ofcourse there's like a lot of graphexperts you know you sparkle or sorryfor those kind of things but sometimeswhen we have to fail poor to find alittle bit more difficult we see thatthey really like to work with graph QLso we really embrace graph GL and it'scompletely 100% graph QL based so that'show we define what our smart graph isand we're the smarter if you can dothree thingsfirst thing is semantic search so whatyou can do and that's also what I willdemo to you what we know fromtraditional search if I may call it likethat we search for keywords right so ifwe write about the company Apple weactually need to search for Apple but inrefute you don't necessarily have to soif you add company within an apple butyou search for example what's thecompany the business related to theiPhone it will still find your dataobject company with the name app we alsocan do based on that it's automaticclassification so we can automaticallymake edges in the graph based on thesemantics of your that object and lastbut not least we can do knowledge viewpresentation and this is often what'salways also refer that's like nowadaysit's like it's the knowledge graph butit's like I wouldn't say we'renecessarily in notes graph but tea youcan create those similar representationsso those are the three things the threeuse cases that we can help you with sosomething important to share a base onwhat we did last year so the best way toexplain is a little bit on time and whatI mean with that is like that we saw ofcourse a lot of data bases in the pastthere were more relational base right sojust you know I wrote column structurework on structures and tables and thenof course we got these graph databasesso for example had the people from naioare in room so they made a beautifulbeautiful database like that and wechose a year back to store ourinformation with Yun'sand what we did was that we had thatsemantic element that but I will explainthe confectionary note I will explainwhat that is we had that as a featurebut last year we decided to just doubledown on that semantic element so we gotcompletely rid of the implementationyarn\u00eds and we created basicallyeverything ourselvesand we that means that we now only havethat context nary to store thatinformation in and we actually this is acrown so we actually figured out likethis is actually we're really happy thatwe made the decision because now wecould really bring something newyou know to need to stay somethingdifferent and a different way to handleyour data objects and work with them sowe store everything in a semantic spacethat's what we do and now if you go likewow semantic space I'm gonna explainwhat we mean with that so but just keepthinking when I talk about semanticspace I'm talking about the context inER so imagine it like this if you go toa I'm compliance as you can see let'ssay if you go to a grocery store and youhave shopping list and on that shoppinglist you might have four items a bananawashing powder you're looking for anapple and a piece of bread so if you goin the supermarket then you find abanana you know that an apple is gonnabe closer by than the bread or atwashing powder and if you move towardsthe bread you know that you're actuallygetting closer to the washing powder I'mmoving more away from the fruit that'show we store data in the space and sothat's the metaphor to actually imprintthe problem that we solve and we do thatto something that we that we call theconfectionary and I'm gonna give you alittle bit of background where we'recoming from and what's different toother solutions that around here so ifyou go all the way back again time tothe to the 1950s there was like a famousquote and I said like a word can becharacterized by the company it keepsand but it basically means that youwould say that the word Paris would bemore closely related to France then itwould be to Holland for example or theUS and New York would be more closelyrelated to the US then for example Spainand it basically went for all words anda lot of research was done there andthen we jumped for it and then with thewhole machinery boom we saw there was alot of work being done with with basedon machine learning and these wereembeddings and then we also sort of usand it was like we got first word tofact got gloss and nowadays had whatthey call in the academic realm whichthey call a state of the Arts Cobert butif we now put on our engineering hatwe really fell in love with glove andwhy did we fall in love it laughsbecause beard has multiplerepresentations of a word in a data setbut glove doesn't cloth has one vectorrepresentation for every word and thecritique that often got was that well ifyou for example aim Apple Apple can meanof course fruit but it can also mean thecompany and but we wanted to solve theproblem in a different way but it'sengineers we were very happy with thisbecause now we could index it we couldindex those words in a storage mechanismso if you start a we if yet you canimagine it like this it's an empty spacethat you start you choose a language nota programming language but a spokenlanguage so let's say for exampleEnglish and the space is filled with allthose words so for example if you findApple you find nearby fruit you findnearby company and you might also findiPhone and these representations that westore they have a 600d mention 'l vpresentation which is very fancy but ithas to do with compression that kind ofstuff and the thing that we did is thisif you store a data object like this sofor example the class company with thename apple founded in 1976 etc and inwhen i demo i will show you how thelexie works but if you sell informationwhat we've here does that it creates astring of the words and the conceptsthat are in there and it takes thoseconcepts and it funnels them down howdoes it do that it says like it takesthe lydian distance it combines thatwith e it's this sounds very fancy butwith the logarithmic function basedamong the occurrence of the word so forexample company might occur less oftenthan the word Apple so then we say theword company is more important and thenwe even work with word boosting so thatyou can say certain words are importantin my data object but what we do withthat is that we create our search ourfirst object in our graph and it getsits own factory presentation so now ifwe have an empty v8 with those words inthat and we store our firstdata objects that you can see therethere it is it lifts that's where itlives in the vector space that's what wemean so now if we query for let's say acompany an iPhone it can look in thenearby space and find that that objectso now without even having I phone intothat object we can still find it andthat's the thing that we created then wethought okay this is this is our thingright this is our golden goose eggbecause now we have different ways ofcreating graphs and actually query trulyso this example for example might looksomething like this so if I have a dataset with companies and this would be mygraph QL query where I say that yet giveme things which are companies and I'mgonna have their names but explore themby iPhone I might forget this resultApple and as you see iPhone it's nowherein the dead object and if I have thatsame data set and we say well a littlebit more abstract organize thesecompanies on the concept of Redmont Imight get back Microsoft and that's howwe structure our graph so and so that'sbasically what what we view this and asa developer vd8 comes with a fewfeatures so the first things that's thatconfectionery comes all out of the boxyou don't have to do your training youdon't have to set it up whatever itcomes all out of the box or basically alot of the container I should say addingdata happens to the HTTP API but callingdata to the graph QL API it's completelycontainerized so you can run it whereveryou want and because we use that vectorspace it's very very scalable you canscale that space very very tremendouslybig so I think the the biggest one thatwe ever try it was a few was a fewbillions it gets pretty big andsomething that we have in the pipelineand maybe I can show you that next yearbut is that we also can create peer topeer networks of v v8 so that we canpoint to semantic elements in differentgraphs so that we don't have to agreeany more on our ontologies or on ourschemas but that's like that's that's inthe meso a little bit about graph QL so EV sowhen I demoed it so how we structure ouryou graph here this is the the UX if youwill of our graph QL so we have a getfunctions first the other one is anaggregate function but I going to talkabout the gasp function we have asemantic kind which we make adistinction between singing selectionsnouns and verbs they have the class theclauses the property a property mighthave reference and then the propertyitself and what you can do you can havethese semantic filters on top of that sothere here you see for example theexplore a filter and they can search forconcepts but you can even move away fromconcepts of move towards concepts and Iwill demo that to you in a bit demo sonow we get to the demo so now you mightwant to said well how does it actuallylook so what I did is that I spun up a adocker container if you want to do ityourselfif you go to our websites and withouttechnology and then you simply click wev8 and then you find all thedocumentation the installation gives youjust a we v8 but what I'm going to demoto you is this news publications thatare set and if you click that onethere's just one simple docker composecomment that you can run that you canplay you know around with it yourself sothere's a meta endpoint which I'm justrunning just to make sure yes so thatit's running well so let's first look ata schema so this is an example of schemaso here you see I have the classpublication and I have I have the namenames or the name of the publication butfor example as I said the headquarter Glocation which is our geo coordinateshas articles etc etcetera this is how westructure the schema this key that'simportant because we will see that backwhen we use graphical to query and thethings that we actually store look likethis so for example you see an articlethe article has an ID a beacon is areference in our graph but why do wecall it beacon because we do it in spaceso it's a beacon in the space and thenhere you see for example a summary ofthe article the title of the article andthe URL where the article actually comesfrom so that's how it's structured sowe've created a simpleGUI that you can use to actually youknow look through and search to the tothe graph so you can visualize stuff butI wanna I want to dive into the thegraphic your queries so let me show yousimple query so if I say get thinniesand I want to have I wanna have apublication and I want to see the namethis would be valid query that you seerise in folk Financial Times Wyatt NewYork and economist etc now what I nowcan do is that I can say well I want toI want to explore for the concepts let'ssay business and I'm gonna limit it tothree results just that it's easierreadable so I know as I do the samequery but explore based on the conceptsof business so if I now run the squareyou see Financial Times The New YorkTimes International times etc but yousee the word business the meditech it'snowhere there that's what comes from theconfectionary or if I would say fashionand I would run it then you see itstarts for example with VOC so that'show we've structured it and how it worksbut that's not not only going for likesmall strings but also for a lot oflarger text objects so for example if Iwould say get things article and I wouldsay show me the title of the article andwe run the query so now you see allthese articles about a variety of topicsyou see brexit so you can see when wewhen we actually pull to that end butit's just a variety of topics in herebut if I now say well I want to havethose articles I'm gonna use the explorefunction again and again I say well Iwant to explore for the concepts let'ssay music and I'm gonna limit theresults again just for the sake ofreadabilityso same query but now based on thearticles so if a run that's you say likeyou see fair enough the first one hasthe word musical but then it's aboutGwen Stefani and then it's about JohnLennon so you see the word music is notnecessarily in there but it organizedlike that automatically so now even ifyou want to filter further in this inthis graph what you can do is that youcan say well the question we have actualhow do we do pagination because if youhave the 600 dimensional space whatwould be the next page so what we'vedone is this so we said well we canactually for example move towards aconcept so I can say well move towardsthe concept for example of The Beatlesso I guess you already know what willhappen if I do that so and I give it acertain force the force is how stronglyyou want to push towards this conceptinside the vector space so let's sayit's a little bit arbitrary but let'ssay 85% so if I do that now you see thatJohn Lennon the article with John Lennoncomes first and if I say like I'm morelike a stones person I hate the Beatlesthen of course you can also do move awayfrom the concept of the Beatles samequery and we see John Lennon is done andnow the question is like ok so whatmakes it so like now the traditionalgraph people are like yeah but I haven'tseem to do the graph in action yet sothat's just very simple because youcould say for example has authors and wecan say on author the author is a nameso this is how we structure the graph sonow you see huh so you see the graphobject here that says first the title ofthe article and if it has authors and onactually the authors that are related tothis and them to this articleI think if time allows it there's onemore second I still quickly show a morething yesso the Isle is wrong within which Icompletely forgot because there'sanother problem that we also try tosolve for this and I want to show youthat going back to the publication's soquickly going back to the publication'sthings publication and then I say thename of the publication when when youglanced over this you might have noticedthat we have all these publications butwe have the International New York Timesthe New York Times Company and the NewYork Times in there three times which isa problem because of course you want torepresent concepts but in a database wehave the same concept represented threetimes so we have something for thatwhich we can do is that we can say whatwe want togroup concepts together so let me sayI'm gonna do the type merge themtogether I've gotta give a four so howbig do we need to look in the vectorspace before merging and then I can saywell do that with 5% so if I now runthis query you'll see that it mergedtogether the International New YorkTimes New York Times Company and the NewYork Times but if I now do a graph queryso I say it has articles on article thetitle of the article it even now mergestogether those different articles fromthose different publications into thesame concept so that's what we havethat's how it worksOh way more features the automaticclassification I didn't even get to showyou that but you can play around with itbecause well we're at first M so thissoftware is open source you can playaround with it you can set it upyourself you can create your own graphsof cementec graphs I should say that'smy story in a nutshell thank you all forlistening and if you like it and if yougo to our website then I just have onequestion so this is our website if yougo here that you can sign up to ournewsletter cylinder or you can click onthe get up star button if you want todon't promote a little bit so that'sabout this is our website thank you verymuch for listening[Applause]why are you sure about it there's agreat question so let me let me startwith the first answer to the firstquestion so and sorry if I went to openit quickly too quickly because it's thiswhole everything where that tolls alsoon the website in detail but that'swhat's happening here we always know usethe same algorithm so if you would havea data object with a longer sentencelike for example the summary or thetitle the articles that you've seen itapplies the same algorithm so it sayslike first I take all these individualwords then I find this center positionbetween those words then I weighed thembased on the occurrence so certain wordsare seen as more important than other soit weighs it towards that and then wehave this optional word boosting thatyou can say in this specific case thiswas very important so move more towardsthat that's how we create those vectorposition so regardless if you'requerying or if you're adding data that'show we do it so that's why we alsobecame agnostic about the fact thatloves about single words and because welearned that if we saw with the firstprototype we did way back what's verysimple we say okay I have the word Appleshow me what's nearby and then as youwould expect as glove does it says likewell iPhone iPhone but I also foundfruit and then we did something verysimple which okay now go and sit in thecenter between Apple and iPhone show meagain what happens and then you seeactually that that successful and if Iif I may I can actually quickly showthat yeah I'm the lost okay so like shipI should have waited Elliott said goahead and thank you so there's aconfectionary endpoint where you can saywell concepts so if I now literally dowhat I just said so if I would say Applethen you see your Apple iTunes Googlepreview on and now of course in thisexample we don't see of course the fruitbut let's say if I would do Apple andnot based on the company but on fruit soI concatenate them Apple and fruit yousee how it now starts to get better andbetter in those results so and that'show it the algorithm works so you canplay around with this also yourself onthis on this end point and the otherquestionyeah sure so we were of course we alsoliked half we were also a business so wehave like so the core is like opensource but we built like a shell aroundthat so we currently have six companiesusing this on a large scale in a varietyof industries also retail oil and gasall those kind of things and thesegraphs get pretty big and especially ifyou scale the communities cluster itsfast which is something I'm now I couldsay that we fence li-like architecteddebt but it was actually something wegot for free because we just the Dynamois just vectors only vectors and that'sof course very fast to scale and tosearch through so the answer thatquestion is yes I don't know and I lovethe idea so we're definitely going totry that out I don't know I don't it's agreat idea we haven't we haven't triedthat yet so what we currently do is thatwe just have we've hidden in a languageso darts French English of course but wehaven't tried that yet so if you don'tmind then it would be fantastic to or ofcourse you can try this yourself or wecan do it togetherbut that would be fantastic it's a it'sa great idea and I don't know thank youI hope you all enjoyed it enjoy Bostontomorrow and show the evening of coursethank you[Applause]", "type": "Video", "name": "Weaviate Vector Database @ FOSDEM 2020", "path": "", "link": "https://www.youtube.com/watch?v=3NfcAF4qm2k", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}