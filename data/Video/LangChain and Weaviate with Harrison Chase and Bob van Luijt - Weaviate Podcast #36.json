{"text": "Hey everyone! Thank you so much for watching the 36th episode of the Weaviate podcast! This episode continues on the ... \nhey everyone I'm super excited about this week podcast we have Lang chain founder Harrison Chase and we've yet CEO and co-founder Bob Van light so guys thanks so much for joining the podcast thanks man looking forward to this conversation thanks for having me guys really excited to be here awesome well I'm just so excited to dive into it Harrison could we start with the story of like the origin story of Lang chain like how you created this yeah I mean it started really uh kind of innocently and innocuously I was just chatting with the so you know everyone's into language models these days everyone's building applications and and so when I first started chatting with folks and this is back in like um September October people were still into it not to the extent they are today but there was a lot of people playing around and messing around and building applications um and I chatted with a bunch of them at meetups I had some friends um building stuff Sam Whitmore uh shout out to her was building an awesome application and so really got to like know them and chat with them about their use cases um and basically just saw like a few common kind of like abstractions things that were happening factored those out put those in a python package very kind of like just engineer you know Factor stuff out put it in there and put it out there um put it out to the world and and got uh got a positive response and so kept on adding stuff in kept on refactoring as you know the the abstractions that I thought I first saw were of course wrong and so those kind of like evolved over time um but yeah basically just started off by chatting with a bunch of folks and pulling out comments distractions into a python package brilliant maybe so we could start with kind of maybe the first abstraction that helped me understand link chain was kind of the sequential chains like you know you have the output of the language model is the input to the next call to the language model could maybe start with describing the sequential chains yeah absolutely I think so the the idea of chaining is basically breaking up calls to language models and in multiple steps um and not only call us the language models but to other things actually and so I think actually most of the examples that I first added were like language model and then other thing and then language model and basically chaining the results of them together in a way where you can you you did some iterative stuff and so to make this concrete with an example one of one of the things that I first added was based on a paper by offer press self ask with search so basically you'd ask a question and and the examples in this paper were kind of like multi-hop questions so it wasn't just like a simple answer and these are cases where language models are known to kind of like not perform Super well um and so the self ask with search Paradigm is you first kind of like think about what intermediate questions you need to answer so I think like uh the the the example used in the paper is like what is the hometown of the current U.S open champion and so like if we're if we're thinking about that intuitively like what would we do as humans first we'd think about who who the current U.S open Champion is um and then would figure out where his hometown is and so basically the idea of training was was to break those down into exactly those steps so the language model should hopefully know that it first needs to think about who the current U.S open Champion is figure that out and then it needs to think about um where he's from and and figure that out and uh you could do that with uh you could do that with a single pass of a language model the the thing that's um the thing that's interesting and the thing that caught my attention was basically the idea of combining the language model with Google search because if you're asking about the current US Open Champion the language model who's trained on data up to 2021 or something like that wouldn't know the information and so basically using language model to ask the intermediate question you then pass that question to a search API you get back a result and then you basically keep on going from there and so the the idea of chaining that really stuck out and this is a little bit different than the sequential chains that you brought up but I think it's uh I think it's more interesting to be honest um so I think it's the idea of chaining language models to determine to figure out what information they need to look up getting that information from a separate more more accurate more reliable more up-to-date source and then plugging that back in and letting it continue on its merry way oh yeah incredible and uh Bob and I recorded our generate module podcast and Bob you could hop in more on sort of just how you're seeing this kind of you know language model calling the search database and you know from Google search to webiate search yeah it's so it's so funny that you bring it up because that's exactly what I wanted to say because the um there were like these two things sometimes you have like these multiple things coming together right so one is exactly when you started with Lang Chan I was looking into like so what is he doing and it's like oh I got it and then there was also that article that I was like when people say like okay hey we can hook this up to Google search I was like you can also hook it up to a vector search engine right and do this stuff with your own data so that was why I was so super you know I was super excited about uh you know about what you're working on um because I was like you know this is like just a beautiful Synergy of all these things coming together where we really use natural language to or create a change or have the database for like you know large amounts of um uh data and that's just I had the exact same I was looking at the exact same thing and when I saw hooking it up to Google search and I don't remember Conor I think you shared that first article with me but when I saw that I was like yes that's what we can do and then everything came together and and then you started with Lang chain and we were looking at what we could do there it was just it was just super you know super exciting so um I had the exact same thing and and to your point Bob like I actually think it's I think it's more interesting to hook it up to your weeviate database or your own internal data because that's just more like unique to you and that's more distinct to you like anyone can hook it up to Google Search right um anyone can hook it up to Wolfram Alpha but like your your documents your data that are in your vector database your reviate database connecting it with that like that I think allows you to build like a differentiated and unique kind of like chat bot or question answering or whatever kind of like experience exactly and I think why it's interesting to do that with like new types of of vector databases like um we've hit or for that matter also uh Wolfram right so it is that it has this natural language interface so and and when I recorded the the podcast with uh with together with Conor so while we're recording we have not released the module yet but when this podcast sees the light of day it people will know about it right so the um is that we're moving towards a um how should I say to to a way of interacting with the database not with like precise queries and precise answers like you need to have a keyword that matches in your database versus a result that you're getting from the database that must be stored in the database but we can just you know we can just have free associations with our database and we can even generate different results based on the tasks or the prompts that we're giving to our data and I think that's just that that this is super exciting I'm just just talking about it and thinking about it just makes me like super exciting for the coming year it's gonna be it's gonna be a cool year yeah I think like this natural language layer on top of like whether it's whether it's a vector database or a structured SQL database or a mongodatabase any of that I think that's really like powerful and really interesting because it allows like people who don't know SQL or don't know how to write graphql to interact with these databases and so um yeah I'm really excited to see this kind of like unlock a lot of you know new people using using this stuff across a variety of these cases yeah that's something that I that I thought was so exciting when I first saw lion chain is is how the language model can control the database like you just said with the writing the graphql queries out or writing the SQL queries writing graph database queries uh so can we talk about that kind of uh orchestration layer where you say tell the language model hey you have like in weviate land you have this class you have this class you have this class this class contains like weba blog posts this documentation this is the code base and and so it like chooses which information source to Traverse maybe you can also talk about like SQL or like the Google search API or just other apis like all sorts of external data sources and how that's interface in line J yeah I think um I'll maybe start with just a SQL database example because I think that's maybe one that that most people are familiar with but it's it's the the ideas are common across other types of databases whether structured or unstructured or vector or anything like that um and I'll start with like a simple example but I think uh we'll see pretty quickly that it starts to get pretty complex okay so at the most like simple example um the pipeline is basically taking a natural language query you then generate relevant SQL and I'll come back to that in a little bit but you generate relevance equal you then execute that SQL against the database you get back the result and then you you pass that to the language model if desired and kind of like return like a human interpretable answer and so um yeah like I think the example that we have in the documentation is like you know how many employees um exist and so it like does like you know uh it gets like the count of the number of rows in the employee table and then returns something to the user like there are nine employees or something like that um touching on the first part like generating the relevant SQL query as you were getting at there's a lot of information that you actually need to put in to kind of uh do that correctly so you need to you need to kind of like put in information about all the tables that exist and then not only the tables but the columns as well um and and if there are relationships between those columns you need to input those as well and so the prompt that you're constructing starts to get pretty complicated already and so um you know a lot of the the value that lane chain provides is ways to like easily construct these prompts and then also like just like see what's going on like this is a complicated prompt I'm already describing it I already probably lost half the audience and so like I'm making it like really easy to see like what's going on and what it actually looks like um okay so that's like the simple use case but there are a lot of like complexities so what if there are just like too many tables and too many rows and or sorry too many columns in those tables to put into the prompts because there are there are context kind of like window length what do you do then um and so and so one another chain that we have in LinkedIn is basically a chain that first select relevant tables and then puts only those tables in the prompt and so kind of like a simple idea but basically the idea of like filtering down um then so that's around like constructing the query then after like executing the query what if the query is like incorrect syntax what if it's kind of like references tables that don't exist um what do you do then so I think there's there's a few things that um to be honest aren't in link chain yet they might be by the time this gets released I think they're on our roadmap one is the concept of kind of like output parsing or validators or guardrail so like validating that things are valid uh SQL syntax and stuff like that um another idea is basically the idea of kind of like if it tries and it fails just like passing that back to the language model and letting it learn from its mistakes there's a really good example of this working um from Riley Good Side um with uh not with SQL but with like a python example where he asked it to do a math problem it tries to use like the math module but it's not imported and so if you allow it to kind of like correct its errors it will see like import error math not recognized and then it will kind of like fix it the next time around and so I think like things like that for for SQL for any type of database querying will be will be important as well um yeah so I I think that's an overview of kind of like what's in LinkedIn currently and like stuff that we want to add because yeah it gets a little complex a little quickly just out of out of curiosity to build on top of that price ID um uh because the focus now is mostly that you have like um language right Lang chain how do you what do you think when it comes to like these uh multi-model kind of models right so how what do you think will happen there and how will link potentially evolve with these kinds of uh models but or do you have any thoughts on that yeah yeah no that's a really good question I think um okay so at at the simple uh the simple answer is there's basically kind of like Integrations we can already do and they're actually already rprs for this um to kind of like hook up kind of like multimodal things on either end of the language model so for example like taking audio file transcribe it generate generate some stuff um generate some text um that's you know done by kind of like converting it into the text domain so um yeah so in that's in that sense it's somewhat multimodal but it's not like fully multimodal another example of this kind of like quasi thing is like hooking it up to an image API or something so so there is a PR open to basically have a chain that first like uh takes in uh you know like because all the image prompts are like really convoluted like that's like that's like Wizardry right there so take in kind of like a simpler query generate a more convoluted prompt pass it to dolly or something get back an image um so in that sense um I think there's already there there will shortly be kind of like multimodal support in link chain more and more involved than that honestly I I don't know um at the moment um I think I know there have been a lot of uh more kind of like uh first class multimodal models coming out and so I think we need to look a little bit more carefully about how to integrate those yeah so the the thinking is like you would have an image captioning model that parses the image into text and then it just easily flows into the text interface of the judge Epsom most like yeah that's that's kind of how we're imagining things uh that would be easiest for the current kind of like construction of LinkedIn for sure and so I think that's what we'll do first yeah I think that's such an exciting emerging thing of the um you know like the the multimodal chat gbt style large Vision language model I think that is definitely on the horizon I would if I had to take a guess I'd say that's within three months yeah I hope so I'm excited to play around with it I think um I think character AI has done some like multimodal stuff as well I think they have some multimodal chat Bots um so yeah excited to see it more widespread oh I just really say yeah I saw it like deep mines Flamingo at nerups is is that already yeah so I just it's like by matter of like a thought experiment so the other day I was thinking about the following right so what I noticed and it's something we we are doing also with wifiate and maybe that's also happening for like what you're working on Harrison is that um I was thinking we somehow we tried to capture these things in like a traditional way of a software engineer right so we say okay how do we write the technology to you know and then we get stuff back and we get this complex change in your case or in our case like how will we represent the data I must if we just think about like two three years from now could it be the case that we're just interact with like software completely different because of these LMS and what we're doing today so to give you a very pragmatic example the first time I interacted with a generative model so quite some time ago I needed to get used to the fact that I was not writing software so that I just could use natural language to give it an assignment that felt so unnatural to me right so I'm just curious actually both of you how you guys see this because I'm I just I have like a feeling that we're still trying to shoehorn this into a traditional way of writing software but that maybe in like a year two years from now that's just a completely different Paradigm in how we create technology and or how we interact with the database or how we change things together I'm I'm just curious what you guys think about that if you just you know kind of dream into the future what this might look like yeah I I think um I mean it's a really good question right like that's like I think that you ask for all these things um is kind of like what people are talking about I don't have any I don't have any great answers I think like one thing that um I mean kind of like in the vein of like and this is a bit more on the like developer side of things but on like the vein of like multimodal stuff like I think like you know right now a lot of the stuff that we're like when we're talking about ingesting data right for like PDFs like right now we're transforming a PDF into like some text and then maybe like embedding that text or something like that or transforming yeah doing like a caption on image and embedding that caption or something like that I think a lot of stuff will move towards just being like talking to each other in like embedding space basically um and translate in between that um but yeah that that's kind of a little bit separate than what you were asking I don't yeah I don't um I mean okay so here's what here's what I'll say around like the ux of actual products I think they'll start to be like look at copilot like why does copilot like work as kind of like a product um it works because it's in a situation where you don't need 100 accuracy um there's a human in the loop um and uh yeah it's a bit it's it's kind of like working with you and so I think like you know the the common thing for products has been like co-pilot for X co-pilot for x and I think like the important part is like it's people are still getting used to working with these language models the language models still aren't at like 99 accuracy or even close to that so you need kind of this human in the loop and this like situations where it's okay to have lower accuracy I think we'll start I think as language models get better and we get more familiar we'll start to go we'll start to see more applications where there's less human in the loop and they're more automated um and they're in situations where you can have like higher accuracy uh or or sorry yeah where where where yeah where you can get a higher accuracy where you need higher accuracy and it starts to unlock that um so yeah you know I don't have any great insights as to what the future will look like but I think that is like one kind of like natural progression that that we'll see we'll see yeah exactly because I I was also thinking about this and I I was trying to think back of like the how we could like the origins of it and I remember that I had like this 50 moments when I was at a conference and people were talking about the semantic web and that I was like that some people in the audience were like annoyed because we humans couldn't agree on stuff right so how we were structuring the data and that this whole thing of these language models not even large language but just language models in general right back then it was just Cloud pasta Etc that people started to think about what if we don't have to do that anymore and I think if you now for example fast forward to today so for example let's say that uh Conor and and I that were like we're working on a piece of code right and I make a mistakes hey Connor it's not what what mistake didn't make and Connor looks at it says oh you forgot like a summer column or something that I could imagine that we're moving towards the space where you have you're almost running like pseudo code but at the model says Oh no I got you I know what you want to do I'll just execute it right I I see where you're going and I would not be surprised that that would just get to a to a natural language state of of programming which of course will come with with certain rules and and and certain um uh ux elements um I I was I I was listening to a podcast from from uh um the open uh what's the name the open source data podcast from Sam ramji and he was interviewing bandorka and then Ben also said like how will we get the rigor of software engineering into working with these language models and I find that it's super interesting because if we're if we and then we like uh they had a Humanity we'll figure out how to do that then it will become easier for people to interact with these models but also write technology software right so you just describe what you want and the model helps you to deploy it to develop it I would not be surprised that we will see things based on also what you're working on uh that that data will be like uh deploy a complex uh infrastructure by just describing it in natural language and then the model figures out oh then I need to spin up a pot here and I need to do this over there and those kind of things right or or design that you just have visual design you just describe it and it was like okay gotcha and it just figures that out so I I really hope that it also lowers the barrier for even more people to just write so far and just create cool things and and I would not be surprising like a year two years now we just we're slowly moving towards well a natural language way of describing what we want the machines to do and then they just in combination with this model they just figure it out and just do it yeah and I think there's been a lot of talk around like you know prompt engineering like it is prompt engineering here to last like my take is it will kind of like converge to like what you're describing like just as you would tell a human how to do something you'll have to tell the language model how to do something and so this to your point around this natural language interface for doing something um like yeah I think that's pretty realistic that that we can get there no yeah that kind of like um I I think I really like the example that Harrison brought earlier quickly I want to come back to that about like with the SQL code how you check if it's correct kind of and it makes you like I'm so excited about this idea of like the language model would write code to run experiments like run the experiments and then analyze the experiments could maybe kind of Step into the technical and like how will it this look how will it check the correctness and then flow in the length chain uh syntax yeah and I mean I mean copilot had a really cool demo of this at one of the Microsoft conferences where they asked it to write a problem and then it kind of like it it they ran it you could see the output it realized that the output it wasn't even that I don't think it ran into any like compile issues or something it ran successfully but the output was just like nonsensical and so it like logically reasoned that there must be a mistake somewhere in like the program um so I I don't know how I actually don't know how kind of like co-pilot did that but I would imagine the way that um I think like uh I think the best way I think hard coding like oh check if error check this I think that is uh is is doomed for failure because I think that's just you're gonna run into too many educations and so I think actually the the right way to go about this and the way that people will go about this in in the future is basically letting the basically telling the language model that it can take these actions it can write code it can run code it can observe output of code or I guess in the case of SQL you know it can write SQL it can run SQL it can observe the output of SQL it can um check SQL query or something like that and then basically let it determine like what actions it should take so first you should write some code then it should run the code then it needs to look at the output and and then determines okay do I need to like edit the code do I need to run it again do I can I return it to the user and so basically like yeah I think the control flow will kind of like be determined by the language model but then it just has access to all these like tools or actions whatever you want to call them where it can do certain things but it's not hard it's not hard it's like it's not hard coded that like you do this then you do this then you do this I just think that will not enable the types of like um yeah the types of experiences that are needed for these more kind of like complex workflows yeah because I think what Bob was saying earlier with the idea of you spin up a pod and it knows how to monitor the production app and like I haven't learned to I haven't didn't know too much about this until I joined weave and got under the curtain and saw other things that happen and it's like that idea of you know the language model keeping the software running with all the errors that come up that is just amazing I think yeah exactly and to build on top of that so I'm just curious to see if it will go to a world where so as you're describing it Harrison I think that will of course be like the the short term not even tomorrow like right now like we we describe something what we see with gold fighter we describe a problem it outputs SQL and then it gets whatever it needs from the database right but I wouldn't be surprised if we're gonna skip that middle Step at some point so let's take we've hit as an example there's no reason why we've had couldn't have like a SQL interface we we don't have that but you could theoretically have that that if you just have like the situation where it's like okay we have like this is how the embeddings are encoded on disk or in memory and those kind of things and then the model just knows where to get it and um that we get just new systems that we just interact with them based on that it just knows where the information is and it just figures that out by just telling it in the prompt right so it literally prompt engineering like this is how the database is structured this is how we store the vectors and and the data in in memory or on disk do your thing right and I I wouldn't be surprised if you see I mean not tomorrow but like I would be surprised if very soon we will see the first maybe research or blog post and people experimenting with this so just to cut out the middleman basically where the middleman is the um um the the interface language like right like SQL or those kind of because the problem is that these um these interfaces they force us to be extremely hyper structured which is understandable and which is great for many use cases but what if you can just skip that all together because the way that we're now interfacing with each other right I I'm not I'm not formatting my questions or my remarks in a certain way then I go like okay I hope that Conor can parse what I'm saying right now right I just use language and then and then it just you know and I I really hope that that is where we where we can go based on these these embedding based and language model based approaches that's I mean it's a little bit further in the future but I I really hope that we can get there and you know with all these tools and what everybody's working on yeah so Harrison can I ask about some of the prompts that you like the most like maybe like let's think step by step the idea of um you are the writer you are the editor you are the reviewer right like like have any of these prompts really stood out to you as being really interesting yeah I I think the ones that are most interesting to me are the ones that um are good are basically good at interacting with arbitrary external kind of like tools um and so what I mean by that is like um so okay interacting with like external tools and thinking about how to like reason with tools there's a really good paper called react which is like synthesizing reasoning and acting and basically it combines kind of like some of the um Chain of Thought stuff which is where you kind of like ask the language model to kind of like think slowly about what it you basically tell it to think um which sounds silly um but you tell it to think um and then and then acting is like uh determining kind of like what action to take and what and and what action and what input to that kind of like action or tool to be um and so combining them it it kind of it you know I think the paper showed some pretty nice gains and a bunch of benchmarks and stuff like that um so that so that type of prompting is really interesting to me because I think having the language model act as kind of like a conductor that can interact with things is is very powerful and so any type of prompting that enables that I really like now taking like the zero shot kind of like thing to that so the original paper kind of like hard-coded a bunch of few shot examples so they gave it a bunch of videos I think like the original in the original paper one of the examples was interacting with like the Wikipedia client and so it could do like a search it could do like a lookup I think those were the two actions it could do it gave a bunch of examples of uh doing those actions and then it learned from those examples um and so to the point earlier about like prompt engineering kind of going the way of you just tell it what to do like I think the way that it will be in the future is you won't necessarily have to give it those examples you can just tell it like this is what you can do and so the zero shot way of interacting with tools is exactly that there's no examples you just kind of say Hey you have access to this tool it's called search it takes as input a string and what it does is it look and you should use it for like current events and stuff like that and then you you do that with however many tools you have and it kind of just like learns to use them in a zero shot way um and and the the the react prompting is is really powerful for this because it lets like think about like you know all the information it has like the tool name and the tool description and stuff like that and I find this really cool and I think like um yeah one it like works surprisingly well just out of the box um and then the other like really fun thing is that if it's not working it's kind of easy to tweak by just like changing the description of the tool so one of the common questions I get asked is like how can I like you know it's using this tool incorrectly or like how can I get it to use this tool in this specific scenario and it sounds stupid but the answer is you just tell it to like you just tell the language model like use this when you have a question about math use this when you have a question about music and and it's generally pretty good at like observing that and following those instructions um and so that type of like zero shot react prompting is that's that's by far the most interesting stuff to me I think very exciting very exciting I just I just want to highlight one more time that when you said like you just tell it too that is just how far we got like in like just think about two years ago right it's that is just that that one sentence there you just tell it that was like how exciting that is I I love that yep yeah all in in all the papers um you know a lot of the papers that came out in this were from like a I mean yeah even like a year ago when you needed few shot examples and you couldn't just tell it to and you had to do this like really elaborate prompt engineering and now you just kind of like tell it to and there's still like again like I don't think this I don't think this means that there's no need for prompt engineering like you still have to like figure out how to like you still have to construct The Prompt in a certain way and you still have to tell it like what scenarios it's good for and stuff like that um but it's a lot easier too and it's just like it's more similar just chatting with a human and instructing the human yeah I think maybe playing the devil's advocate here I think kind of the you just tell it too is maybe like I think the whole thing about prompt engineering is that there is a little bit of an art to telling it right like and so I'm really curious about this intersection between like you could sample many decodings from the large language model that temperature thing being like the key knob that you can slide maybe firstly just set the stage what do you think about that temperature parameter on the large language models I think um if I'm so I think there's two different like ways to use language models I would say one is like leveraging their like reasoning ability um and their ability to like you know yeah take ambiguity and and this and decipher and decide what to do and this comes into a lot of the action stuff and then the other is generating text right generating Pros generating poems so for the latter for like generating text yeah crank the temperature up like get some like funky responses go for it but for like the part where you want to like reason about things I almost always set like temperature to zero and just because you want the single kind of like you want the best most accurate response yeah that's it's interesting that you say that because the what we're also seeing with the database right so and and um Conor and I coincidentally earlier today had a conversation about that that what I really believe is like you have like the the reasoning aspect of the model and then in our case also the database right so appreciate that you can say okay we're gonna literally in front engineering you tell it like we're gonna give you information in this format and you must base your answer on the information and we're giving it and if you can't tell us you can't and then temperature all the way to zero you know what I mean and and I and I find it very interesting because what that means is that you're kind of cutting out and I'm recording it the knowledge that is in the model but you keep it to the language understanding like as if you have like a human who just understands everything you're saying but has no knowledge and has like an old set of you know encyclopedias and then you go like okay what's the distance from Earth to the Moon and okay let I understand the question let me find that in the in the encyclopedia and uh I find that that I find that very intriguing and I'm curious I have a question for you about this Harrison so um they're like two camps right that we see like one is like we're gonna keep fine-tuning the model until it has all the knowledge right for whatever use case we have or we're gonna just keep that separation that we say like these um we have the model and we just feed it with um uh with data coming from a data source I'm just I'm just curious where um uh I mean I have strong opinions on this but I'm very curious where you sit on uh on this uh no this this this this ain't you know what your angle is what your take is on these these approaches yeah I mean I I think there's a honestly I think there's like a place in time for both um I don't think that one's kind of like strictly better than the other I think the one that's more interesting to me is kind of like the one that you combine with external data I think um but like you know if we think about if we think about that language model is kind of like acting as a decision maker or a router one of the things that it could route to is a larger language model that is trained on everything and it could call that like when it needs to um but yeah I think like um I think the idea of well okay so yeah the idea of fine-tuning on everything and always trying to make sure the language model is up to date I'm not very bullish on that I do think there's a place for like general purpose language models but as I said I'd rather use that as like a tool rather than trying to like have that be the sole source of everything um and and then yeah like combining data with language models I think there's yeah there's I mean even today you can do that with prompt engineering there's some like really interesting papers on like um kind of like ways to do it at a more kind of like um more native to the model so where you're actually attending over embeddings of documents that you pull so like retro style approaches um I don't think there are any great publicly available models for that yet but that that might be something that comes out in the next year or two that would be really interesting um and then yeah the the idea of having like um the language model act as a router one thing that I I like I would love to have like a fine-tuned model that's just really good at zero shot kind of like routing and reasoning um and I'm actually chatting with a few Folks by this so I don't know when this is coming out so it's possible that when this comes out we'll we'll have some like uh yeah we'll we'll have already shared some stuff about that but like I think I think creating that like small fine-tuned model really good at zero shot reasoning would be really really cool um and so yeah that's kind of the that's the type of fine tuning that I'm interested in I'm not interested in kind of like the fine-tuning to make it kind of like up to date with every possible information that's under the sun I'm more interested in kind of like the fine tuning to have it do routing to other sources of information yeah I like that very much so I'm I I have a very similar uh point of view so I would not be surprised if we're gonna see these kind of models that are just that indeed are like good at like general purpose things like an llm for medical terminology in LM for legal terminology in LM for engineering those kind of things but that it is very good at just processing basically these One-Shot approach right that you mentioned that it just it understands like for like other terms of the understands it can help you reason what kind of data that it needs to get from in this case a database or any data source that you can tell it's I'm it's interesting to say that because I have a very similar point of view yeah yeah I think I'm I I used to really like this idea of like you would maybe have some kind of classifier that classifies the model in the hugging face model Hub that's the best for this task but uh so so what will this all these large language models the lawyer the biomedical language model how do you see like how will that be organized first of all I don't think anyone knows which makes this so exciting um I think um yeah I mean like I don't know right now the the private models are way better than the open source models like um I think it's yeah like you know GPT gpt3 is way better than than the best open source models probably flon T5 um anthropic when they come out with their model it'll probably be pretty good um I think there you know it's like um I I it's possible that there it's possible there will be a better open source model release it's possible that to your point around like having a model specifically for medical or specifically for law maybe you can get that by fine-tuning font T5 and it's and it's less about kind of like you know maybe for like smaller more narrow things like that actually definitely for small for smaller narrow things you can probably get just as good performance by fine tuning font T5 or something like that first like it kind of just depends on like what small and narrow is right like is law like small and narrow I would argue it's still kind of like big and and and and out there but it's definitely smaller than like every piece of knowledge on the Sun so um yeah I don't know I mean I guess my current guess would be that they'll be like the the private models will be the best general purpose ones the open source models will be fine for small smaller tasks and I think probably over time like the like the the open source models will start catching up and at the same time the private models will keep on getting better and better so I think they'll always be this Gap it will just become like you know what's in this Gap is is the real value in this Gap or is the real value now doable with like smaller fine-tuned models and and I don't I don't I don't know I don't know how fast those will move up a lot a lot of questions yeah actually now when you when you put it like that I'll take back what I said before so just I hope you can cut that out to comment I'm joking but the but the um it just let one model it's just one model to uh to roll them all and then maybe a few different providers with different you know for the education I don't know but uh um yeah so this is what I meant like what I said it's like it's like this traditional mindset right so you know it's like you have different programming languages for different tasks but like why not why not one one model that does it does make sense yeah I think the counter argument to that though would be like right now the inference is kind of expensive like compared to if I train like an extractive QA model on my documentation like it you know I could even sparsify the question answering model it maybe is only like 80 million parameters and so it's just super fast that would be kind of my counter argument to like the smaller fine-tuned models for certain tasks like in the search pipeline like the re-rank all right like I see a lot of tweets that are like oh you can use the large language model to re-rank and I'm like what a waste of the large but like to re-rank it's like kind of a I I think the Paradigm and and there's a good quote from uh Stan from dust and so I'm going to shamelessly steal it here but he has a great quote that's like um no gpus before pmf which is basically like no gpus no fine-tuning models before pmf product Market fit before you know what you're doing and I think like um the power of large language models is that they're so good at just like arbitrary tasks and you can get them to do like basically whatever and so experimenting with like different different pipelines or different things that you're doing is really it's it's so much easier than before and so I think the common thing will basically be like you know experiment with the best models then when you actually get something that's like working and you want to scale it up not even that's like what like when it's working and it's just for you it's probably fine but when you want to like scale it up then you need to worry about like fine-tuning models and so I think it's also kind of like a progression as well like you know if I if I was building an application I would never start with a fine I would never start with fine-tuning no I'd start with the the language model and then when I or I'd start with uh the biggest language model gpt3 or something like that and then when um yeah when when when I get to the point where I have enough confidence that I need to scale it up then I'd start thinking about that and I think that'll be like a pretty common workflow and I'm I've already seen a few people in yeah I've already seen a few people like doing that it's um it's not that crazy right so so I just want to say that's very that's very interesting and especially with what you said with cmap right with product marks because it's like there's this Tipping Point of life with the models being the general first models are good enough to solve the task right so it just and we're just we're hitting that Tipping Point I found it fascinating to what we saw the last few weeks the discussions on Twitter and what have you and in newspapers I was just sitting on the sides eating popcorn while I was watching it right of seeing like core research but also like product people working in this space how they look at this very differently right so it's like uh and I think I think um that is for from me in my profession one of the breakthroughs of 10 GPT was not only using the model to do it but also the packaging like the it's it's so simple if you think about it but it's so well executed and um and that kind of ties into what you were saying uh Harrison that it's it is it where to just we're just past the Tipping Point that where you can build amazing stuff like for example uh taking Lang chain right and solve your problem or store your data and weed it and factorize it just and just start searching and doing whatever you want to do that just that is just that has changed in the last year as opposed to a year before and that is also why I said earlier like I'll take back my what I says like maybe it's just gonna be like one model right or just a handful of models that are just good enough yeah awesome so I kind of want to Pivot topics and um you know and ask Harrison a question about sort of the virality of Lang chain and the open source engagement has been something that's really been impressive and so I just want to ask kind of like like how like the question like how did you do this how did you start the fire like obviously you're a really smart guy and the idea sticks like crazy but yeah like what are your thoughts on that yeah I mean I think like um I think like uh honestly I mean one it's just been like so much fun and I think like that's the thing with like all these things I think Bob was saying like it's super easy to like create these things like it's just so much fun and people have fun building these things and I have fun building these things and so it's just like I I don't know there's um yeah like I I think there's just a there's there's a lot of like energy and aspect of fun and just like building things and exploring and so I don't think like um yeah like I I think um yeah like like yeah I don't think I I haven't done anything like in particular it's just been like a lot of fun I think like yeah I mean like the you know the the the two things that I try to focus on besides just having fun although they are like very related are like build like actually useful things um and then like be nice as well like I think like and that gets back into like having fun right like I think like there really is just like everyone's like this is a great time to be hacking on like ideas and being exploring ideas right this is a phenomenal point in like uh you know yeah like I'm not that old but like this is by far the best point in my lifetime for like exploring these types of ideas and so and I think a lot of people realize that and I think a lot of people are out there doing that and so just like trying to build things that are actually useful for them and then like you know like be nice and having fun with people as they're doing it like I think you know I yeah no no that's that's all I really got so yeah that's incredible and I think just like that yeah exactly like the contagiousness of sharing um this prompt sequence created this is yeah and the it is really truly amazing um so I'm kind of curious to see like the Lang chain Hub uh so what are kind of the directions that you're thinking of taking Lane china yeah I mean I think like the to your point around basically like sharing this prompt sequence sharing that's exactly what link chain Hub is is designed to do um I think we put something out really quickly in like a GitHub repo where it's just a collection of kind of like chains and agents and prompts that that you can easily load and prompts you can't really use by themselves but like chains and agents you can just easily load and use by themselves um and I think the despite it being a terrible ux as it like you know GitHub repos are great don't get me wrong but they're not meant for like sharing and discoverability and stuff um and so um like within a particular GitHub repo so despite that terrible ux like I think we saw a lot of people really interested in it and so I think that's something that where you know and I think like like why are people interested in this because like there there's just so many different things that you can do with them and and so like having them like and and again going back to like the just like hacking and building and playing around like there's definitely that type of like atmosphere and a big part of that is like sharing your work and and so they're like not just Lane changes open source but a lot of other really awesome projects built on top of it are open source and so like the idea of like sharing um I think people really enjoy and like in in lynching Hub is really just aimed at making it um easy to share your your change your agents your prompts stuff like that yeah so maybe uh yeah like the the open source thing is really fascinating like the um how has kind of the maintenance been of it as because I understand how like how many are you the sole maintainer or how does that kind of thing happen yeah so there's one other person working on it full time with me the maintenance costs have definitely gone up I'm not gonna lie I think there's like yeah I think there's like 110 GitHub issues and like 43 open PRS so gotta catch up on that a little bit for sure um but on honestly like I think it's been um but like but but it's been like fun like it's it's like all the things that people are adding are like um you know like people are adding like different uh different kinds of like embedding modules different types of of model providers like there's like that I hadn't even heard about but like now I'm like okay I gotta go check that out like different types of like um Vector stories different types of chains different types of of tools to like interact with and so I think like um even though there's a lot of issues I mean and someone like opened up a PR for like some of like the image stuff right so it's like um I don't know like even though there's a bunch of stuff um and and there's a little bit of a backlog which I need to I need to fix right after this um but uh yeah it hasn't been like too laborious to kind of like keep up with or anything like that awesome well fantastic Harrison I'll let you get back to it uh I thought this was such an incredible such an incredible podcast I mean yeah the the sequential change is the first thing that just helped me instantly get it so if listeners are out there wondering checking my chain out for the first time you know the sequential chain of how you chain together multiple language model calls one to the weeviate vector database and yeah I think it's just such a fascinating library and seeing it emerge so Harrison thank you so much for your time and joining the weba podcast no thank you guys for having me and yeah just uh I don't know if people listening to this realize but like we V8 and you in particular Connor reached out super early on in the link chain Journey like I don't like it was it was uh yeah like I uh yeah that was a while ago and it's been awesome to kind of like work with you guys and and kind of like I think we both see the world in like similar ways and and there's a lot like I think Vector stores and are like for a lot of applications are like a critical enabling piece and so it's been awesome to to work with you guys and and chat about stuff and so yeah thanks for thanks for having me on always always happy to be like yeah thank you it's super exciting and you know we're you know here to help each other because it's just a whole new ecosystem that's like it's like a supernova like you know thanks to these LMS and like this whole new ecosystem and would just I I think we we need to help each other right to to reach our goals and this is just yeah this is super exciting ", "type": "Video", "name": "LangChain and Weaviate with Harrison Chase and Bob van Luijt - Weaviate Podcast #36", "path": "", "link": "https://www.youtube.com/watch?v=lhby7Ql7hbk", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}