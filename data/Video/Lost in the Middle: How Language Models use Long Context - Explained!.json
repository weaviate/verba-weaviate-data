{"text": "Hey everyone! Thank you so much for watching this explanation of \"Lost in the Middle: How Language Models use Long Context\" ... \nhey everyone thank you so much forwatching Wii vid on YouTube this videowill explore a super exciting new studyinto retrieval augmented generationtitled lost in the middle how languagemodels use long context lost in themiddle is used to describe this U-shapedcurve where we're exploring the impactof search result quality on the languagemodel's ability to reason across thesearch results that have been put intothe input so we asked a question likewhat is ref devec what is the atomicnumber of oxygen what are cross encoderranking models and we take that queryand then we hit our search engine getsearch results and then we pass theoriginal question plus the searchresults to the language model so therethis has been something that's justsuper popular in application Buildershackers but there hasn't been too muchexperimental findings I just you knowthe nuances of how exactly you're goingto append search results into the inputlike this so this is a super excitingpaper that has this interesting u-shapedfinding that is asking this question ofwhere is the relevant information so ifwe return you know 20 documents is theanswer in the first stock document the5th 10th 20th so on and then also whatis the impact of how many documentswe're going to return to then pass thelanguage model you know say we give 5 1020 and so on so this is a really funpaper to read I hope you enjoy thispaper summary video really exciting tosee more research more understanding ofretrieval augmented generation soquickly before diving into theexperiments to give a quick backgroundon retrieval augment to generation wehave questions that we want to ask alanguage model like about our particulardata whether we have you know some kindof particular software we're buildinglike in the case of weviate and reftubecor whether it's your legal contract yourparticular data to analyze your email soon so we wanna instead of having thelanguage model just be able to answerquestions about our data we're going toretrieve from a search engine and thenput the information into the input tothen help the language model regionacross it so whereas it would say whatis ref to vac and it says hey I have noidea what that is if you then give itthe context where you you know you'veretrieved this text Chunk from thevector database and it says wevia 1.16introduced of the back module blah blahand then it's able to reason about whatref to back is so this is broadly thesetup where we're going to retrieveinformation to put into the input tohelp answer the question so the keyquestion the authors are exploring inthis paper is how does search qualityimpact generation in this initialexample this is an example of the Oraclecontext where you have just the perfectcontext for answering the question whatis ref to vac you know it gives you theperfect definition of it but in the realworld retrieval systems are a littlemessier than this we can't guaranteethat the first result is going to be youknow the perfect answer to the questionso the question is then like what doesthat error how does that error thenimpact the generation level so you knowsay the breath say this particularinformation about reptavec was the thirdsearch result and the first was you knowI don't know something about crossencoders the second was something aboutreplication or so on and so what is theimpact of the needed information notbeing at the first but at the thirdtenth so on and you know as you saw lostin the middle the more it gets to themiddle the less the the language modelis able to reason across it so thenthere are two key experimental controlsto study this phenomenon how manydocuments are retrieved in the input andthen where in the retrieved documents isthe answer so they're going to befirstly they're going to control this toshow exactly the impact of it by usingthe natural questions data set and moredetails that we'll get into but the keyfinding being that when the you knowwhen you have this Oracle type of thingwhere you have the perfect answer it'sright there you have really highperformance and way better than theclosed book just language modelanswering the question and this is onpretty general questions so you know youimagine if it's reasoning about youremails or whatever obviously the closedbook would have no way of reasoningacross youremails or whatever so you know if it'sin the first position we get massivelysuccessful result but then if it's inthe fifth or the 10th in the case of thetenth it actually performs worse thannot having any retrieved contacts sojust a super interesting finding oddthing about retrieve augments generationreally emphasizes the value of goodsearch quality but let's get a littlefurther into the experimental detailsbefore coming back into that so let'sdive a little further into theexperimental setup so as you can seehere you know you ask a question likewho got the first Nobel Prize in physicsand then you have the relevant documentso in this case the correct answer is inthe second position so you see how youretrieve you know three documentsanswers in a second so the firstquestion is about how many are we goingto be retrieving so you see thedifference between retrieving threecompared to retrieving you know five inthis case and then the next question iscontrolling where the relevantinformation is going to be and again Ithink another detail about this paperthat I really like is that they'rethey're controlling the experiment theyhave the ground truth and you know it'snot about like just the messiness ofreal world data although we will seethey did an experiment on that as wellwhich is really interesting but you knowthey're gonna they have the answer andso they're going to be controlling whereit is one two three four so on andthat's how we were able to study this sohere's the first interesting set ofresults we have the impact of the numberof documents and the position of theanswer exploring uh six different modelsuh anthropic Cloud anthropic cloud with100K context gbt 3.5 turbo and then gbt3.5 Turbo with 16k uh input lengths theMPT 30 billion instruct model andlongchat 13 billion parameters with 16000 input links so the most interestingthing is seeing the kind of lost in themiddle thing probably in the middle yousee that when you only have 10 you youkind of see it with the five but the 20really shows it well as is a 30. soanyway so you see this Con this kind ofconcept of like if the answer is the10th document out of 20 you get wayworse search results compared to if it'slike one or two as you see towards theleft and I mean it's amazing to see thatif it's one like the impact of perfectsearch would be just massive forretrieval augmented generation yeah I Imaybe say maybe pause this if you wantto look through it a little further forme just kind of seeing the U shape andseeing it come from 10 20 30. so thenext really interesting thing exploredis the impact of the number of documentsso again these are you know gbt 3.5Turbo with 16k input lengths Cloud 1.3with a hundred thousand input length soyou know these are pretty uh longcontext window models so you know onequestion would be can we just you knowgive it a ton of search results becausethen you know we recall at 30 becomes alot easier than recall at one recall atfive but what they're showing is that inthe current state of the models you knowthe more you give it the lower theaccuracy and this is um averaged acrosseach of the positions so you know whenyou see this plot this point for uh 10that's the average of when the relevantdocument is one two three four acrossthe entire question answering data setyou know 30s so you get idea so the twokey takeaways from these experiments aremodel performance is highest whenrelevant information occurs at thebeginning or interestingly end of itsinput context you saw that u-shape thatcomes back up at the end I think that'skind of interesting that like as it'sattending if it's at the very end itgoes back up so that's just one of thosekind of quirky things about theselanguage models and deep learning sothen the second thing is modelperformance substantially decreases asinput contacts grow longer soyou know we're not going to kind ofsolve this problem by you know in thecurrent state we're not going to solvethis problem by just giving it moresearch results and then hoping that byyou know increasing the window we getthe relevant Target that way sort ofalso in the context of the final resultsI think it's really interesting to justlook at this table of the closed bookversus the Oracle the Oracle being youknow the information is search resultnumber one so this is the case where wehave we've solved search and we getperfect search results but you see thisdifference like with the MBT 30 billioninstruct going from 31.5 to 81.9 orTurbo 56.1 to 88.3 just a huge potentialin kind of improving search and thenwhat that would mean for the theImprovement on this and it's worthnoting that this this data set naturalquestions will get a little more into itin a second but you know this is likegeneral knowledge if this was about youknow again it maybe set it to death butlike your particular Discord chat orslack or emails or whatever then there'slike no chance at all at the Close bookbutyou know you still could fine-tune thelanguage model on your information andthen I think you'd be in a similar kindof Camp as this so you know this kind ofI think even with fine-tuning on yourdomain you still would want thisretrieval and then you know I mean we'llsee as these experiments continue toevolve and it's also interesting butjust this table right here you see theinput the impact answering these generalquestions the having the first searchresult is just an enormous Improvementso one other interesting detail withtheir experimental results found in theappendix are some experiments with gpt4so gpt4 is you know the pink one that'sat the you know performing better thanall these other models and you stilleven see that U-shaped curve with gbt4so I think the deal I read a little bitabout in the appendix is that they doyou know you see it in the in the titleof the plot is that they do 500questions to sample so I think it hasn'tbeen as fully tested as the other modelsmaybe details with the API access Ithink gbt4 is still kind of a newerthing at the time of recording thisvideo so you know just you still see theu-shape Curve even in like the mostpowerful I you know I don't want to pickfavorites but I think I think gbt4 iswidely regarded as one of the world'smost powerful language models at thetime of this recording so as mentionedearlier the current results we've beenlooking at have been in a reallycontrolled experiment where the authorshave the ground truth answer and they'reexplicitly moving it you know one twothree four but now let's see what thislooks like in a more realisticuncontrolled retrieval augmentedgeneration setup so the next experimentis going to be using the naturalquestions open data set which containshistorical queries issued to the Googlesearch engine and then human annotatedanswers extracted from Wikipedia but inthis case instead of you knowcontrolling the system by explicitlymoving where the answer is we're justgoing to look at what happens with thecontriver embeddings retrieval system socontriver is one of these you know denseembedding models it's been fine-tuned onMs Marco and so we're going to beretrieving with that model uh to thenyou know have the input and so it's alittle more you know uncontrolled in thesense that you know where the correctanswer is could be like eight threeseven like whatever the performance ofthe retrieval model is so this is theresult that we find from that soI think it's important to firstly startwith what this uh orange curve at thetop is this is the recall so uh so asyou retrieve fifth it makes a ton ofsense that like recall at 50 is reallyhigh you've retrieved 50 documents sorecall is like is the correct answer inone of the retrieved documents or is itfive it looks like it's a little under70. so then what you're seeingunderneath that is the performance ofthe models when these documents are thengiven as input so you see that you knowfrom 5 10 20 the performance saturatesit doesn't continue to improve and someof the models you know see like thisdrop from 30 to 40 in the MPT and and soon so it's a pretty it's a it's moremessy because it's more uncontrolledbecause you could have um you knowexamples where the input or where theanswer just isn't in the context at allso that's what kind of makes itdifferent from the original uhexperiments okay so now that we'veexplored this u-shaped impact of wherethe relevant information is and how muchinformation we give to the languagemodel the authors then want to explorekey value retrieval to answer thequestion of to what extent and canlanguage models even retrieve from theirinput context so what that means in thiscase is we're going to be giving it keyvalue pairs of randomly generated uuidsso like if you're using weeviate youknow that this is the ID that you giveto each of your data objects so you havethis key value and then so the task thenbecomes key you know 9f for a so andthen to Output so you're trying tomeasure just how well it can copy what'sin the input so similarly to thedocuments retrieved in questionanswering we can also vary where the keyvalue pair is going to be in the listlike say it's the sixth key value pairand then as well as how many key valuesthere are so you know this these arejust showing kind of that differencebetween you know you have a lot of themor you have a few of them and then howyou can control where where the query isgoing to be and then where it is in thecontext so here are the results of thekey value experiments so shown on thefar left we have again this thing thisU-shaped curve where when the key is inthe 25th or 50th it doesn't perform aswell as when it's the first key or thelast key kind of well the last key is alittle funkier with this and alsointerestingly though we do see that someof the models are really good at thiswhereas others not as much so maybe thisis another kind of test for you knowwhich of these language models you'regoing to be using but you know we see aswe you know in in effort of trying tobreak it this model the brown one thegbt 3.5 turbo 16000 once we scale thatup to 300 key value Pairs and then weput the key to be reasoning about in the200th position then we get to around 45it looks like so so you know it it seemsto do a better job at this than the thanthe other task which is prettyinteresting I think being able to justcopy that there's a lot of like learningto copy with lstms and recurrent Networkwork that kind of this these kind ofexperiments have been you know testedbefore but anyway so these are theresults of again finding some U-shapedcurve in this kind of experiment as wellto further investigate these details theauthors are also going to explore theimpact of architecture comparing decoderonly models with encoder decoderexperiments with query awarecontextualization and the effectiveinstruction tuning so to start off withencoder decoder versus decoder only anencoder decoder Transformers you firstencode all the input into a vector andthen you would put that Vector into thedecoder that then attends over thatVector encoded by the encoder so itmakes sense to hypothesize that you knowwhen you're encoding everything into avector maybe it has the you know the conthe context of looking at the entireretrieve search results and that wayit's able to see that even if it's inthe 10th position you know as we've beenseeing with this U-shaped curve it canstill attend over when it's decodingonly but so they're going to compare theflan T5 XXL model with the flan ul2 I'mnot super familiar with these models Iput a little bit with the flan T5 serieswhen we were you know looking at youknow this before the like the chat gbtthing came out we were exploring thesekind of models pretty heavily but soanyway so the interesting thing here isum you know they don't really find thatthis encoder decoder decoder only thingimpacts that too much but another reallyinteresting nugget that I think isreally interesting for prompt design isthis query aware contextualization sowhen you're decoding only the model onlylooks into sorry only looks at the pastif it doesn't like attend to the Futuretokens so if you're saying you knowplease answer this question based on thecontext and then you don't give it thequestion yet then it's just kind of likeforming this representation of what it'slooking at without any without thequestion to actually like help kind ofguide its reasoning through the searchresults so they're showing that a prettybig Improvement by just putting thequestion in the beginning of the decoderonly modelsso that way it can you know look at whatis reasoning about as it's building upthe representation so I like this quoteparticularly with that key valueexperiment gbt 3.5 Turbo with the 16kinputs with query awarecontextualization achieves perfectperformance when evaluated with 300 keyvalue pairs in contrast without it itachieves the lowest performance of 45.6so the difference between you know whatis the value of the key and then thatkey like 9f 4A B2 if you remember thatthe difference between giving thatbefore you then show the key valuescompared to you see the key values andthen you ask this question like thatkind of thing so I think that's prettyinteresting for the prompt design forretrieval augments generation so thenext thing is this really interestingthing oftrying to highlight maybe the bias ofinstruction tuning so you know thinkingthat with instruction tuning that youknow maybe it's bias towards the answerbeing right there because that's kind ofhow the instruction tuning might looklike so they compare the MPT 30 billionthe language model with the instructionfine-tune checkpoint so I think therewas a lot of confounding here becausethe language model is uh you know not astuned as the instruction modelsgenerally are and that's why I think yousee the accuracy is so much lower withthe just the pure language model but Ithink what is really interesting isyou're seeing the impact of Mosaic mlopen sourcing the language model as wellas the instruction tune this is one suchexperiment I think there are so manyexperiments that we can do by exploringthe different fine-tuning routes fromthat MPT 30 billion base model so I Ididn't really take anything awaypersonally from this but I just I loveseeing this kind of like MPT 30 billionversus the instruction checkpoint andhow that's been open source I I thinkthat's all super interesting so finallyone last thing the authors present isthe c real position effect the U-shapedcurve we observe in this work has aconnection in Psychology known as theserial position effect that states thatin free association recall of elementsfrom a list humans tend to best rememberthe first and last elements of the listso you know maybe that there's somethingto thatcomparing how language models recallthings as well as how humans do so toconclude here are some of my Reflectionsfrom reading this paper lost in themiddle how language models use longcontext the first of which is theimportance of search so kind of you knowbefore The Branding of vector databasesthis was referred as a vector searchengines or vector search databases thiskind of looking into informationretrieval literature following alongwith you know what search practitionersare doing and the practices behindsearch has always been core to wevia andjust as an example we get 120 the latestrelease at the time of recording thispodcast uh sorry video introduces crossencoder re-rankers Auto cut and newhybrid search rank Fusion now you'veseen like the you know how you cancombine bm25 with Vector search andleviate so search has always been firstclass Citizen and we V8 alongside thatVector index and the database scalingand all that kind of stuff soquickly let me take you through thesethree new the three like latest featuresjust to show you a little further how weV8 helps you get that better search thatavoids that u-shape and hopefully getsyou that Oracle context so here we arein the weeviate documentation to look atAuto cut re-rankers and new hybrid rankFusion so I think autocut is probablythe most interesting one for this paperwe've seen this other paper called uhlarge language models are easilydistracted by irrelevant context andthat's all kind of motivated you knoweven just human Searchers want this kindof thing where basically the idea is ifwe see a jump in Vector distance so wesearch something like what is reftubecand the first text Chunk has the vectordistance0.1899 then 0.1901 0.191 and then wehave a jump 0.21 autocut is a newfeature in weeviate that's going to lookat these uh slopes and so cut it thatway so you know it interpolates a linebetween the vector distances if you youknow plot them on an axis of just Xbeing where the search result is andthen y being the distance youinterpolate that line and you look forvery steep slope changes in the distanceand so you'll cut that kind of thing sowhen you're searching weevier you havenear text Concepts you know this animalsand movies that was turned into a vectorand then if you pass in Auto cut onethat means the first steep jump detectedit will cut the search results and thisis a great way to only pass in the topthree results the language modelsprobably the best solution foryou know this kind of uh the findingsbehind lost in the middle so anotherinteresting thing then is um you knowre-ranking search results so we alsohave the re-ranker modules that wereintroduced in uh 1.20 so we have re-rankour cohere as well as re-rank ourTransformers so re-rank our Transformersbetter for say local development youknow uh the hugging face sentenceTransformer libraries they've opensourced cross encoder models crossencoder models sorry to set the stage alittle more they take as input the queryand then each candidate document so sayyou have five you know five potentialsearch results it would take the queryand then that first result the query andthat second result and then output ahigher capacity score so you're tradingoff speed for a more accurate rankingand as we've seen with lost in themiddle that more accurate ranking couldend up being super impactful forretrieval augmented generation so withweavate 1.20 we have you know two waysof doing this you can either use thelocal sentence Transformers or you canget an API key and use the cohere modelso cohere their you know off offeringcross encoder ranking models as aservice as an API soyou know just really interesting stufffor the you know search quality and sofinally with hybrid search you know weV8 has bm25 built in and Vector searchso you can combine the uh the scoringsof documents of these two algorithms andso when we first released this we justhad rank Fusion where you're justlooking like you know it's okay it'sranked third in the bm25 and it's rankedsecond in Vector search so just combineit based score it based on the rankswhereas now we have an actual uh scoreFusion so score Fusion you're likelooking at the particular score thatcame out of bm25 or similarly to autocutyou're looking at that like distancethat came out of the vector search soyou know more ways of how we combinethese results and overall deliver thatyou know hopefully that Oracle uh searchcontext the second key takeaway for meis I want to see this testing otherkinds of context so right now what wesaw is this setup where you know k outof the end results are relevant but Ithink in this whole you know argument ofhow do large language models use longcontext I I've been just super inspiredby the Llama index idea where you'reretrieving from different indexes so youknow in we V8 we separate data objectskind of like categories of objects intoclasses so you have blog posts podcasttranscriptions a code repository or likeWikipedia so I think the interestingthing in Long context is to retrievefrom each of these sources so you knowagain I ask you what is reftubec youmight want to retrieve Snippets fromblog posts you know Wikipedia entry orlike say a knowledge graph that has likeyou know these tuples to giveinformation about reftubec as well aspodcast transcripts like how is reftubecused in conversation and then maybe likecode Snippets of seeing it in action insome way but this idea of retrievingfrom different sources I think that'salso a huge part of this conversationand not just you know only k out of theend results are relevant but you know ofcourse that's a huge part of it and thenyou have the compounding effective asyou retrieve from these other thingsthere could be irrelevant things as welland that's why I think this wholere-rankers autocut hybrid brand Fusionit's also exciting of trying to you knowretrieve from all these sources and makesure that you get that number one resultis a good result so then finallyyou know I think a lot of these you knowwe have these extractive questionanswering where you're you what is theatomic number of oxygen eight and Ithink generally it'll be moreinteresting to explore the impact of youknow where the relevant information ishow much is retrieved in abstractive oryou know summarization where you have towrite these like long responses tobizarre questions that you can't justkind of like give a factual answerquickly too so it makes the controllingof the experiment much more difficultbut you know it's just kind of theevolution of this space you knowevaluatingGenerations broadly is quite difficultbut I think you know it just that iswhat I would predict is the evolution ofthese kind of experiments thank you somuch for watching this video explanationof Lost in the middle how large languagemodels use long context I thought thiswas such an interesting paper so cool tosee these experimental findings on theimpact of where the relevant informationis in the retrieval augmented generationsetup as well as how many documents aregoing to be then handed to the languagemodel I love seeing the key valueretrieval the expiration acrossdifferent models from the gbt models thecloud models you know MPT and longchatit was overall just a really cool paperto read so if you enjoy these papersummary videos please subscribe toweviate on YouTube it really helps youknow encourage us to keep making contentlike this if you're curious about weaveyet you can check it out on weeviate.ioOpen Source on GitHub weviate webgateand please follow us on Twitter atweeviate underscore IO thank you so muchfor watching", "type": "Video", "name": "Lost in the Middle: How Language Models use Long Context - Explained!", "path": "", "link": "https://www.youtube.com/watch?v=Kf3LeaUGwlg", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}