{"text": "Hey everyone! Thank you so much for watching the 56th Weaviate podcast with Etienne Dilocker on the 1.20 Release! Check out ... \nthank you so much for watching the weviapodcast I'm super excited to welcome weV8 CTO and co-founder Eddie and dilockerfor the weeviate 1.20 release podcastthis is another packed release Abatewith all sorts of cool thingsmulti-tenancy PQ rescoring Auto cutre-rankers a new hybrid uh rank Fusionalgorithm and some Cloud monitoringmetrics every time we do these releasepodcasts it's always so much fun Ialways learn so much so Eddie andfirstly thank you so much for joiningthe podcastthanks so much for having me same for meI love talking about these things I lovethe the great questions that you alwaysprepare and I'm very very excited aswell to talk about this oneyeah amazing yeah I love just like thebreadth of it going through all thedifferent topics in weaviate and sostarting off with I think just a superexciting topic diving into the databasething multi-tenancy can you tell us uhmaybe just to begin in the highest levelabstraction like the overview of whatmulti-tenancy is yeah yeah what it isand sort of why why you need it even inthe first place so multi-tenancy uh forus and I always feel like I need to sortof because when people hearmulti-tenancy they think of cloudoperations sort of share resources andCloud operations and of course we have acloud service so multi-tenancy for us isnot about how we run the VB accountservice yes you can use multi-tenancy onthe cloud service but it's actuallyabout multi-tenancy for you the user solet's say you have an application yourapplication has separate users and theyhave somehow data that needs to beseparated from one way or another so forexample let's say you build an app andyour app allows you to index documentsthat you have on your hard drive somaybe just sort of install something onyour let's say something like Dropbox orso you want to search to all your yourdocuments you only want to be able tosearch through them yourself youdefinitely don't want other users whohappen to be using Dropbox to be able tosearch through your documents so that'skind of the the idea of scoping that toto individual tenants so you as aDropbox user in this case would be atenant or it could be that multipleusers maybe instead of Dropbox it wouldbe a workspace on a notion or anatlassian Confluence or something so inthis in this setup basically a group ofusers could be attended so it doesn'tnecessarily have to be an individualuser but it needs to be some kind ofisolation unit basically and this isthis is so far this is not even like atechnical requirement that's essentiallyjust an application Level how do youcurate what do you have access torequirement uh but then where this getssuper interesting for Vector search isthat it kind of mixes with the technicalrequirements and it's almost like likeit's sort of it perfectly aligns becausein in Vector search we have the problemthat we need to somehow figure out howto sort of limit the vector space andwe've talked about H and SW the sort ofthe indexing graph in the past and nowthink of this whole graph that containsmaybe a billion vectors but thesebillion vectors are spread out over amillion tenants now you would have tosort of assuming there was nomulti-tenancy you would have toessentially cut that graph into a verysmall chunk that only contains about athousand objects each though for amillion tenants have a total of billionobjects that's only a thousand each uhchances are this graph becomes either itbecomes disconnected or you have totravel a lot through that graph withoutsort of hitting notes basically thatyou're not allowed to hit or you're notsupposed to hit so this single graph andfilter kind of approach is at best theworkaround like you can we have this inmediate we have this flat surge cutoffbasically where if the the filterbecomes too specific you actually do aflat search so this would be one way towork around it but then um you sort oflose the whole benefits of of uh thefast paid or the high throughput and lowlatency kind of search that you expectfrom hnsw so there is a need to tobasically also do this kind ofseparation from a technical level andthis is where we said okay enoughworkarounds another work around thatthat users have semi-successfully usedin the past was to separate this on aclass level because in deviate a classis already a an isolation unit so youcould sort of uhsay per tenant you create one class andthey would all have all these classeswould have an identical schema becausethey're it's all the same use casebasically you just copy it forindividual users so your schema keeps ongrowing and growing and growing and likeeach schema update made the whole thingslower and slower and this workedokayish for maybe two to three maybefive thousand tenants and one workaroundand then really we're talking about workround after work round uh another onewas then you could turn off graphqlbecause part of the the part ofrebuilding the schema part of what tookso much time was rebuilding graphql soyou could turn off graphql only use grpcthat would make it scale a bit fartherbut we were really in the territory oflike this is this is not a long-termsolution this is this is happening fromone workaround to another so we said wereally want a dedicated multi-tenancysolution where the apis support tenantswhere uh the the architecture under thehood supports a lot of tenants and mostimportantly where this also somehowscales linearly and we can talk a bitmore about about scalingyeah it's so interesting the um Iremember when I first heard the questionabout doing this kind of thing I hearduh you know I was at the Meetup in NewYork City and someone said you guyssupport our back role-based accesscontrol and you know the time I'mthinking also that you could just havethat filter through your class where youif you have like you know a documentclass and you have content you have userand you know user edian user Connor toonly look at like Conor's emails insteadof Connor seeing edian's emails and sohearing about the limitation of that asyou know if you connect to hsw graph thefilter it might not be connected stillso you need to modify hsw itself um soas you were giving that explanation itreally helped something click for me isthe the difference between just kind ofnaively using multiple classes in weaveas we understand it you know like I havean Eddie in class I have a Conor class Ihave a John class but so can you tell mea little more about the design ofmulti-tenancy and how you have nativemulti-tenancy and I think really to uhseparate these two things hopefully it'snot a selfish question from myunderstanding but this the differencebetween just creating a bunch of classesand you know maybe at 1.19 compared tothe native multi-tenancy in 1.20absolutely this is definitely not aselfish question I think our viewers andlisteners will will absolutelyappreciate that as well so um thethis class-based workaround kind of workbecause one thing that a class alreadydoes is it creates something like someseparate space somewhere uh basically inthe class and internally in V8 this is achart and within one class you couldhave anywhere between so so this is inthe traditional mode withoutmulti-tenancy you could have anywherebetween one and any number of shards andA Shard is essentially you can think ofeverything that's in the databasescontained in that one chart and wheneverthere are two shards and you want tomaybe query across two shards then underthe hood this is split into two querieseach Shard does their part on their ownand then somehow aggregated againum so this is why this workaround kindof work because by creating 10 classesyou also under the hood created 10chartsum it was kind of doing a lot ofoverhead for essentially you just wantedto end up with 10 charts but you couldkind of do it with with creating 10classes the multi-tenancy feature thenative multi-tenancy feature in thesimplest terms you can think of it it'sa single class but within that class wecreate one chart per tenant so shardsare now no longer this static thing butthey're completely Dynamic like you canadd them on the Fly you can delete themon the Fly andum the the kind ofcluster association with a chart stillholds true so I need to need to explaina bit more for this probably so in a vv8cluster let's say you have a cluster ofthree notes let's make it simple likemulti-10 so you can go hundreds of notesbut let's keep it simple three notes andlet's say that in our class example uhyou would have each class would justhave one shardum A Shard is something that that can'tbe split further basically like if youwant to split it you need you need morecharts so in that old setup this chartwith the specific configuration wouldlive on one of those exactly three notesso one way of Distributing this aroundthe the setup of it would be if you have30 tenons or sorry in the old set of 30classes I mean you would have 30 tensbut you would model them with 30 classeseach node could hold 10 of thosebasicallyand thenthat's that that would sort of be evenlydistributedum but you'd have very little littlecontrol now with multi-tenancy we keepthat idea of having one chartum but the shards have become much morelightweight so you have essentiallywe've run a couple of load tests and umwe could in one example this depends abit on on sort of what you actually whatkind of properties you have in yourclass and these kind of thingsum and essentially the bottleneck isjust the file descriptor limit that thatLinux systems have and one test we couldreach 70 000 charts per note and pernode is now where this this sort of veryinteresting part comes in because youcan just scale this by adding more andmore notes to your cluster soum you would have a single class thatclass potentially spans the entirecluster but a single tenant still isisolated to one node so this this andand to make this a bit more complex wecould then also add replication becausethen you can make sure that this nodedoesn't become a single point of failurebut for now let's just ignore ignorereplication so you could have you couldstart with a three node cluster let'ssay your tenons are really huge huge andyou would only fit 10 per pernote then you could fill up to 30tenants on that cluster now if you wantto onboard more users you just add a newnote and per note you would again haveroughly 10 tenants capacity and inreality it's more like 10 000 but forfor our example thatthat um yeah makes it easier to toreason aboutand then vv8 under the hood make surethat you hit the right note so of courseas a user you don't know where thatstuff is scheduled but will be able tosay like with every and this is the theonly real API change for multi-tenancyis that now you have to specify yourtenant key so you don't have to use afilter basicallyum in your let's say you do a a get neartext search then you just have anadditional property that's the tenantand then you should specify that and vv8uses that under the hood to figure outsort of where in that three or five orfive hundred node cluster where does thetenant actually liveso then um so so I'm curious now aboutkind of maybe the design decisionsbehind the tenant key and kind of thethe reduction of the sharding the sorrylike taking down the size of each Shardmaybe we could step more into thetechnical details behind what it meansto have a Shard be lighter weight anddynamicyeah yeah yeah soum the the idea of splittingby Shard or by class or by index typefor multi-tenancy that is not new I knowfor example that um for for if you needto be gdpr compliant and traditionalsearch engines such as elasticsearch youwould also try to use that same patternwhere you would create an index I thinkit's called an elasticsearch per 10 andthen to have that strict isolation sothat is kind of where also this isessentially our class-based workaroundum but this always yeah sort of comeswith with limits so what we said is weneed to make the chart more lightweightand lightweight making it morelightweight is essentially sort of a anumbrella term for all the kind of thingsthat kept us fromum from having sort of a lot of chartson the on the uh on OneNote one thingfor example is asynchronous processes sosince A Shard is its own contained unitin the past we would have lots of asyncprocesses so so for example for uh theagents W index for maintenance thatwould be an async process for everyproperty that's stored in an LSM storeone async process would be to switch uhfrom ment table to segment so when a memtable is flushed that's essentially thememory storage is then to disk then youhave compactions in the background soyou have all these kind of sort ofbackground processes that tend to berelatively lightweight but let's say youhave 10 per of them per Shard and nowyou have 10 000 charts because you have10 000 tenants now all of a sudden youwould have a hundred thousand of thosebackup processes and in our very firsttest before adapting anything we couldsee that essentially all CPU time wasnow spent on just idle backgroundprocesses doing nothing and then to makethis even worsesome of those doing nothing kind ofprocesses uh for example the ones thatwould uh check for whether aLSM segments need to be compacted theydid that using a discrete so a veryinnocent sort of simple discrete of heywhat is the state on disk right now butnow this happens a hundred thousandtimes in parallel and now you hit yourdisk with all these unnecessary readsjust to find out that you don't have todo anything so very simple changes suchas sort of yeah making sure can we cachesome of that information can we checkless frequency less frequently if therehasn't been a change in that much timeuh can we yeah so if all these thesekind of kind of can we combine uhinternally I think this thing is calledthe cycle manager and there's onediscussion that we have like instead ofhaving 10 per chart could we maybe havejust one or could we have less than onebecause we take that outside of thechart so all these kind of optimizationsum that enable us to run more chartsunder hood I'm essentially making themthem more lightweightum another thing is also the memoryfootprint so in our very first testum we had a surprisingly large memoryfootprint for an idle chart don'tremember what it was but I think it wassomething around five megabytes or sowhere in a single class single chartsetup this would just never like youwouldn't even notice that that like youhave one class with one chart and nowyou have five megabyte of memory usagenot a lot but now again times a hundredthousand or times ten thousand all of asudden you have this like 50 gigabyte ofidle memory for or not idle memory 50gigabyte of use memory for what isessentially idle classes so that thatwas another optimization again sort of avery simple optimization which is to tomake this more more lightweight justlook at what kind of memory how are weallocating memory how are we doing thisdynamically are we sort of are we a bittoo optimistic about where the the chartis going to grow to and just sort ofsetting more reasonable defaults makingsure it can still grow so there'sessentially no no negative user impacteverything can still grow but just thedefaults are more reasonable and morealigned for for having many of them andthat's kind of what we mean by by makingthem more lightweightamazing that's such a clear explanationof it and um yeah it's it's reallyinteresting hearing about thesebackground processesum you know hearing about the Compactand merge I don't know too much aboutthe LSM myself but that explanation Ican understand how there would be youknow background processes that check onthe database and that kind of thing soum yes you mentioned the um you knowseeing the five megabytes of overheadper class and you know kind of theinsights that you gain by testing it Ithink that transitions nicely into thisnext question that I'm uh how have youbeen testing multi-tenancy and I guessit's kind of I think like some like whatare the lessons from it like it soundslike with the megabyte thing you youknow even though you have such a deepunderstanding of weeviate internals youstill learn from your tests yeah thatmakes I'm curious like how this feedbackprocess of testing it how exactly it'stested and then what has beenilluminated from the tests oh yeahabsolutely this this feedback cycle goessort of Beyond just the the test it'salso it's user feedback like I think thethe whole journey of getting intomulti-tenancy started out with userfeedback saying like hey I'm trying toapply these workarounds but now I'mseeing large memory footprint or now I'mseeing seeingstuff slowing down or emptying my diskbeing hit even though I'm not queryingand these kind of things and and thatwas I think in multi titanically thatwas the first thing that we that evenmade us aware of hey there is a need forsomething new there's a need for for arevolutionary change basically not justnot just extending the the workaroundsand thenum I think they're in testing you couldsay that there were two major faces sothe first one was initially when westarted out this was in proposal phaseand we just wanted to say like is this aviable idea we have the proposal out onbecause we do this out in the open ofcourse we have that out on on GitHub andjust it was sort of a mix of this is whywe think it's technically feasible thisis what it would provide and uh just aska couple of of users for feedback and ifthat would solve their cases and thatwas overwhelmingly positive so that wasgreat so then we just did a very simpleload test essentially the the old setupwhich because we knew that the shark isgoing to be a Shard yes we're going tomake it more lightweight but essentiallywe can already create a chart so I thinkthe first test we use the classworkaround but then we also increase thenumber of shards per class because ourend goal was having as many shards aspossible and then we we um I don'tremember the exact numbers but let's sayit was something like 50 shards perclass and then we just kept on addingclass after class after class and atsome point we would say maybe hit 10 000and we would say like oh all of a suddenthe next query is now failing I'm goingto investigate like why why are thequeries failing what is what is going onandthat that would be part CPU profiles forexample seeing what do CPUs spend timeon that is when that whole backgroundcycle sort of thing became apparentmemory profiles just to see like whereis the memory actually used right nowum simple queries or sending querieswhere we said like okay this query inisolation should be fast but now in thislarge setup it's slow and then sort ofinvestigating working backward from thatwhere where it is that that was theinitial phase where we sawokay what we have right now is not agood sort of not not the final solutionbut we're well aware of what theseproblems are and how we can fix themsort of with the final solution so thatwas essentially proof of concept thatwe're on the right path and that wasthat was sort of it it failedsuccessfully like it failed in theplaces where we expected it to failum but also it proved that if we fixthese kind of hurdles if we get rid ofthose hurdles it would essentially workand that was before we rode any kind ofline of production code so that wasbasically back in you could say in the1.19 release cycle we kind of preparedfor the 1.20 really cycle where we'vebuilt multi-tenancy uh then came sort ofa classical implementation phase ofcourse Implement implementing that haslots of tests and everything and wewould sort of constantly try to evaluatebut also sometimes you just need to waitfor it to be sort of like for to reach acertain level of majority that you cando sort of these theseum Black Box end-to-end tests where sortof in unit tests and integration testsand and even to some degrees end to endtests you always have some kind ofknowledge of the internals but withthese kind of end to end reallyend-to-end black boxum uh sort of API level tests you justdon't know anything about the system youonly get to use the same functionalitythat your user would use and then youjust try this just try to replicate howwould a user use it and and see where itgoes and there was a point I think abouttwo weeks or so before before therelease uh when red run from our team Iasked him sort of like hey what what canwe do right now to to sort of help yougive you confidence in the releasebecause he took a lead basically on thewhole Cloud orchestration side ofmulti-tenancy and he said please breakit for me try to break it and then wegot together and we tried to break itand only we found a couple of there wasno no uh no fundamental issue but we didfind a couple of things and um it wasgreat that we found them because I thinksome of them like some of them were abit on the edge case side that wouldhave probably taken a few weeks forusers to run into them some of them werea bit more obvious we really could findthem out right away and thenum we built thismore or less elaborate I would say loadtest setup where he just kept onimporting kept on querying measuring allthe the metrics like what so so the thelinear scaling was extremely importantfor us we wanted to make sure that if wehave say a I'm trying to use the actualnumbers that we used but I think westarted with a three node cluster and weaim for 10 000 uh tenants per node sowe'd start with a three node clusterwith a total of 30 000 notes and then wesaid hey if we turn this into a ninenode cluster and instead of 30 000 wewould do 90 000 attendance then we stillhave that same ratio of ten thousand pernode so everything should scale the sameis that the case and then we tried andwe could see and this was this was likeone of those moments where it's like yeswe've reached linear scaling where wecould see that the the nine node clusteressentially is three times the threenode cluster and that is that is veryvery comforting because then you knowokay now most likely the 12-minutecluster is also going to be four timesas large as the three note cluster andand all these kind of things and thatgave us the the confidence that hey thethese claims that we make about if youwant to extend it just add more notes sothis is really true and there's noadditional overhead for sayum for for import time or somethingbecause the the orchestration needs tohappen in the cluster is minimal uh thenode that owns the chart basically doesall the work so by adding more nodes toyour cluster you're not just are youadding a more more space in a sense likemore disk space and more memory butyou're also adding more compute powerwhich can take off the load so um a verysimple way of scaling import throughputessentially just using a larger clusternow is the the second testing phasewhereum sorry that gave us the releaseconfidence and this is something ofcourse like you would do testing andautomated and in manual and explorativeand all these kind of stress testingchaos testing you do this before anyrelease but I thinkfor for this release this was the mostamount of testing we've ever done sothis is the I would say the mostconfident we've ever been about afeature and this is mainly because weknew we have stakeholders that werereally waiting for this and they'rereally saying like hey this is sort ofnot having multi-tenancy or not having amulti-tenancy solution that scales tomillions of tenants is what's keeping usfrom from reaching the next level withV8 or maybe for others keeping us fromfrom using vv8 in the first place so wereally wanted to make sure that while weknew that there was a limit with theprevious solution like now we want toconfidently say these limits are goneyou can use it for the kind of scale youwant and now essentially the the onlylimit that there are two limits one isthe number of file descriptors so thatvaries a bit and that is is local to onenote soum the safe estimate is around 50 000tenants per node most likely you'regoing to run out of um out of uh otherresources before thatum so that's the the one limitum so so upwards of fifty thousand pernode and the other is just resources soresources is something that um that thatit's just Vector search in general hasnothing to do with multi-tenancy andthat is also something that's easy toincrease you just add more notes sothese are the the two theoretical limitsfor number of tenants and both are veryeasy to overcome because both linearlyscale with the number of notes in aclusteryeah it's so interesting hearing aboutthe uh like try to break it test I thinkthat's been one of the like one of myfavorite stories following along withweaviate has been like you know thesphere tests and you know blog postsabout that and you know trying to get abillion nodes into eviate and that wholelike load testing thing has always beenso interesting like I remember it's kindof like a theme of all of AI like youknow like with language models it's likehow big of a language model can youtrain it's like oh we train a 50 billionit's like hot so impressive and this iskind of like our analog of that is likehow many vectors can we put into a intoone index and I think also kind ofseeing it you know across indexes andyou know one kind of system across thenodes is so interesting um I have aquick kind of clarifying questionselfishly for me hopefully there is alistener with it too so when you'resharding a classso I understand that each class you knowit's the um you know it's the documentclass and I have a million documents init and I have one vector index so when IShard this you mentioned like searchingeach chart separately and thenaggregating the results somehow can youmaybe take me a little through furtherhow you Shard a class so here we reallyneed toum separate multi-tenancy from singletenant cases because in in multi-tenancywe use the these shards as isolationunits because you only want to searchthrough one like in in multi-tenancylike your query is for a specific tenantso we'd say there are 100 charts eachcorresponding to one Tenon we would pickexactly one of those hundred we wouldhave that chart serve the request returnit to the user nothing nothing changedso this is kind of the theumlike a small portion of the data on thenote is queried in isolation in theentire query is completely sort ofself-contained in that chart if youShard a so so for that case it doesn'tmatter how many charts you have likeeven if you have a million chartsbecause you're always hitting exactlyone there's never any overhead for fornumber of shards like whether you havewhether you query one out of ten or oneout of 100 or one of out of 100 000 it'salways the same you're always queryingone uh but in a single tenant case themotivation for sharding is different soin a single tenant case you don't havethe tenant key so the results that youexpect are the entire Vector space sonow your motivation for sharding is kindof the uh the the other way around likeyou don't want to have as many charts aspossible you want to have as few shortsas possible so for this the question isbasically why do you want to so if youif it's better to have fewer shards whydo you want to have shards at all andthis is uh where where sort ofumthe the scale of a single index comescomes into place with respect to thehardware that is scheduled on so thatwas a very very complex way ofessentially saying like you can only fitso much on one machine and if you needmore than that one machine you need toShard it across two or three or four sothat is the motivation in let's say wehave an index of a billion objects andeach machine could only fit 250 millionthen you would chart that across fourmachinesand now if the query comes in all fourmachines would say hey okay I'll giveyou the top 10 results out of my 250million so you would now and end upessentially with four lists of uh each10 top objects now you have to aggregatethat list again that's super easy to dobecause you have let's say the distancemetric so essentially you just Resortthat list of 40 and cut off the thebottom 30 again so you remember you havethe the top 10 remainingthat is easy to do but from a sort ofscaling out perspective you need fournodes to serve your query whereas in themulti-tenancy case because only a singlenode hits sort of owns the the data foryour one tenant also only that onetenant needs to answer it which meansall the other nodes and all the otherCPUs on that node are idle to serveother tenants basicallyso when you when you have the hsw graphis there anything to how you partitionso imagine like on layer zero I havelike these are clustered and so we'regoing to go Shard together is thereanythinguh the the chart is at a higher level sothe the hnsw index would be fullycontained within them and it wouldn'teven know that there are other chartslike the the shards you can think ofthis like class Shard and then in thechart you would have Vector index uhregular index Etcso the the agent's double Unix doesn'teven know that that said this issomething I think that that would beinteresting for for future research tosee like could we Shard instead ofsharding by sort of an application Levelattribute so right now we use a hash onthe ID you could also potentially Shardby Vector proximity so that would be thecase where sorry if a bit similar to howIVF based uh indexes worker you havethese kind of buckets so you could sayif I know that my data set is going tohave 10 chartscould I try to sort of pre-select one ofthose 10 shards based on proximity andmake sure that the vector sort of endsup in in The Shard that's closest to itproblem is what we know from from IVFbased indexes is with amulti-dimensional distance it's not aseasy like in a sort of it's not a binarydecision it belongs into bucket one ormaybe along so in bucket ten so in IVFwhat you need to do is you still need toquery multiple buckets like the let'ssay out of 100 buckets you still need toquery the the top 25 closes so that's alike interesting for for for research umbut currently that's that's notsomething so currently the the chart isat a higher level in a in a um singletenant sharded setup and the vectorindex within that chart wouldn't evenknow that other shards existyeah it's really fascinating thinkingabout the you know like the vectorindexes and then how you distributeVector indexes across Cloud computersacross the world and this is all reallyexciting I think kind of we'retransitioning into this like you knowfuture improvements Future Works uhtopic broadly I I hopefully we didn't gotoo into sharding particularly but likeback into the multi-tenancy like do yousee you know what are some things thatare top of mind now that rolling it outyeah there's one aspect that I haven'teven mentioned at all yet that I'm superexcited aboutum what what we wanted to achieve with1.20 was we wanted to have a stable APIwe wanted to have uh sort of the mainfunctionality which is scale so anyonewho was blocked by not being able to toonboard uh or or to to get started withVBA because of lack of multi-tenancyfeatures we wanted to say like you canget started with 1.20 but this is notwhere multi-tenancy ends basically thisis basically just the beginning and oneone big sort of potential forimprovement that we have is costreduction because something that if youhave a multi-tenancy case most likelynot all of your tenants are active atthe same time if they are all yourtenants would need to sort of Be Activeand this is I'm using this fuzzy term ofactive versus versus inactive becausethat's that sort of yeah just just as anabstraction we can go into what thatthat means as wellum but if we say that we have activetenants who are potentially expensivelike let's say they need a lot of memorythey need a lot of compute and we wouldadd inactive tenants which are notexpensive because they don't need memoryor orthey would need the memory at the momentthat they become active but in theirinactive State they would use fewerresources now what that allows us to dois potentially size vv8 cluster for thenumber of active tenants as opposed tofor the number of total tenants so ifyou have let's saya hundred thousand tenants and you wouldsay that you're in for a bill for ahundred thousand tenants is one thousanddollars if you know that only tenpercent of your tenants are active atthe same time and you have a way todeactivate those other ninety percentyou could reduce your infer cost from athousand dollars to a hundred dollarsand this is where where it gets reallyinteresting because that's the the casesgrow like in Vector search right nowalmost all cases are always sized fordata set like it's always the firstquestions like how large is your data Isaid what's your dimensionality how manyobjects do you haveum maybe can you use compression whatare your query requirements Etc but kindof the cost is linear to the number ofobjects but with multi-tenancy in theability and this is sort of what'swhat's coming next the the ability todeactivate and then there's there's morethings sort of in the pipeline um Beyondjust deactivating them but just withdeactivating them we would not need anymemory for for those tenants anymore soif you can say that even if it's just 50or so they're inactive you wouldessentially have twice the number oftens or could host twice the number oftenants on the same hardware and this iswhere I see a ton of potential for foreven more cost saving becauserealistically there's always this likelong tail distribution where you have afew tenons that have a lot of queriesand a lot of tenants that maybe queryonce per minute or once per hour even ormaybe just once per day and you canpotentially serve them for much muchcheaper hardware and that makes it wayeasier to to reduce operating cost withmulti-tenancyhmm yeah that is it's really interestinghearing the hearing the uh yeah like theprioritization of certain resources andso on and I I just kind of was havingthis Epiphany about how this could helplike if you know if I'm building amachine learning app with weavi and I'mcollecting user data I'm thinking aboutyou know I just kind of unlocked mybrain how much this multi-tenancyenables like you know just as a randomexample our last movie a podcast waswith Alexa gordich from ordis and youknow if I'm collecting the user data ofeveryone interacting with his uh YouTubechat bot I I have this new multi-tensityway of collecting that data and feedingit into my machine Learning System andyeah just incredibly exciting as I amunderstanding it better from hearing youspeaking about it yeah also makes iteasier to get the data out for aspecific tenant because essentially likea like a list all query is now specificto attendance so you're now listing allthe data for for that tenant soyeah I guess like when I was firstum thinking about this topic ofmulti-tenancy I hadn't understood thejust users with the same kind of classwell I hope I'm making this like I Ithought about it like say I want to haveall these different classes withdifferent properties like for whateverreason Conor has a age property butEddie and I don't know you don't haveyou just have like a HomeTown like youknow what I'm trying to say it's likedifferent properties or like differentuh I'm agelessjust a good example yeah like I I guessI was thinking like you know especiallywith the vectorization configuration Ithink maybe you know like differentclasses would vectorize a differentproperty and stuff like that but yeah Ithink just you know having you knowmillions of users for the same kind ofclass that application makes a ton ofsense and yeah it's just really reallyexciting stuff uh awesome so I thinkthat's kind of a Roundup ofmulti-tenancy edian's written thisincredible blog post in addition withthe Alice Corp and Bob and car yeah likethe examplesand uh so you know release post uhrelease notes all this cool stuff soawesome so pivoting to our next topic umproduct quantization I think this wasfirst released in 118 or 119 uh you knowa super clever Vector compressiontechnique I think the history of we V8you first started exploring a vectorcompression with binary passageretrieval where you're like hashing youknow positive negative of each VectorDimension but the problem with that uhproduct binary passage retrieval I thinkis you really need to train with it it'snot really something that you can dowith vectors that just come out of thebox from whatever modelbut this product quantization thingwhere you uh cluster the values and thenrepresent them with the centroid IDs youcan apply that to any kind of vector andso it's really Universal way to compressvectors and just overall really excitingway to kind of reduce the memoryrequired in these Vector indexes um itcould maybe start broadly on like whereis your head currently at with productquantizationyeah yeah as you said we we initiallyreleased this uh I think two or soreleases ago one 1.18 and we put this wewe slapped this experimental label on itand that was not because we didn't trustour implementation that was because wesaid we're hey we're Gathering feedbackwe want to see how do users use thatwhat can we potentially do to improve itand and we were well aware that thiscould come maybe with with some APIchanges that in the sort of regularsemantic versioning API guarantees wecouldn't uphold so they said like heythis is experimental this is probablygoing to change maybe not but this iskind of the the evaluation phase at thesame time we don't want to keep it fromusers here you are here please please douse it be aware that yeah either itwould change or maybe you don't use itin production yet unless XYZ and thatalso in turn meant that at some pointthat the experimental phase needs to endand it needs to become generallyavailable and that is now with 1.20 andthat's the the one big change just ingeneral we incorporated all thatfeedbackum of sort of maybe it wouldn't work asexpected in some cases maybe there areways to to do things better like that isthat is one part of of uh where we're atwith PQ right now but then the otherpart is also we've improved it we'veadded what we call re-scoring and thisis a very simple change that has amassive impact so in general PQ is alossful compression so you reduce thenumber of let's say standard example youhaveum a 768 dimensional vectorwith PQ you would represent fourdimensions or this configurable thenlet's say for example you represent fourdimensions with a single byte so nowwhat you have is previously for thosefour dimensions you use float32s so thatwas 16 bytes and now you not just do youreduce this or from float 32 into just asingle byte but also you pack multipleDimensions into a single byte so in thissimple example you can reduce the memoryfootprint for 786 Dimensionsum by a factor of 16. so that's that'sextremelysort of yeah that's an extremeImprovement but at the same time if youif you do that you're recall would dropconsiderably likely you would you wouldyeah your memory requirement is is afraction of what it was before but alsoyour result quality is bad nowso what we said is well but we're stillusing PQ in combination with this H andSW index and in hnsw essentially youhave a a top K Heap so you have like atemporary list of what you think is thecurrent results and while traversing thegraph you sort of keepum improving upon that list and thefinal result is essentially the resultthat we pass to a user so that's a verysimplified explanation of of how searchresults are gathered within hmsw andthat means that if we use that thatcompressed PQ distance to make adecision of whether we wanted somethingbasically whether we want to add it tothe list or whether we want to dropsomething because the list has a fixedsize so whenever we add somethingsomething that's a worse result needs todrop off and if we do that based on thecompressed distance oftentimes thatdecision is wrong and then this thissort of small compression error wouldaccumulate more and more and more and inthe end you would have a lot less recallbut if instead if occasionally weactually load the real Vector from diskand use the real Vector to make thatthat decision so this is the rescoringpart we actually take the the originalVector from disk so now you're trading abit of performance you have a discretenow where previously you wouldn't have adiscrete but in exchange you now get waymore accurate results and I think wehave a blog post coming up in a week orso or maybe two where we have some somegraphs in there and it's super excitingbecause in some casesthe uh recall QPS trade-off sort of thethese these curves were top into theright is typically best it's actuallynot any worse than it is withoutcompressed so it's like you you don'tlose anything that's not true for everycase like in some cases there is still abit of a trade-off where you canessentially say like am I optimizingmore for efficient storage or am Ioptimizing more for for efficientquerying but in in many cases it'sactually not a draft anymore you justhave to turn it on and this is this isin turn a bit sort of depends a bit onhow much memory you have available ingeneral because just of the way thatoperating systems work like a disk hitonly makes it to disk if that thatportion of the disk hasn't been cachedin memory before if there is memoryavailable the operating systems and it'sthe the page cache will start cachingportions of the disk so what istechnically a disk read might actuallynot even be a discrete if you still havesome memory available so you get theselike buffers zones in in which theperformance is still great even thoughthe memoryum usage has dropped a lot and this isthe second feature for PQ that we saidlike okay once we go for Generalavailability we want to want to havethat in and now we're at a point wherepreviously it was likeuse PQ sparingly in some cases becauseit reduces your Recall now it's morelike why would you use PQ pleaseeveryone go out and use it it's awesomeyeah I think when I first heard PQrescoring I thought of it in there-ranking way that we'll talk aboutnext where it's like you know you getthe top 100 results with the compresseddistances and then you just kind ofbring the full Precision to just re-rankI didn't realize how uh deep into thehsw traversal how say you know I I amstill like Curious so you're likeexploring the neighbors of the centernode and you're going to kind of likeResort the candidate I think maybe umyeah I don't know I think it's maybeoutside of the scope of the podcast butlike just yeah understanding that hswhas this like candidate list and dynamicnearest neighbors and you like rescoreit to have a better search like deepinto the traversal rather than just atthe very end I think very important yeahyeah that's like the Elegance of it so Ihave kind of two questions I want to askyou about PQ the first is maybe aclarification of the in-memory on diskpart of PQ soI'm curious if this is the correctunderstanding so you know we we load say200 vectors into Eva and now we triggerPQ we fit the k-mean centroids so fromthen on do we a new Vector comes intoweviate do we just send the full uh fullPrecision Vector to disk and then thecompressed Vector that goes to memory isexactly exactly yeah so we always retainthe full Vector basically on on disklike you store what the user provideswhich is the the the real Vector so tospeak but we also compress it and thenonly keep the compressed one in memoryadditionally we also need to store thecompressed one in memory so if the theserver restarts you don't want torecompress everything so we we storeboth actually but exactly as you sayinitially we keep the the original ondisk and keep the compressed in memoryfascinating I remember one of the mytakeaways from when we met with Matthiasdouzay from meta who's one of thepioneers of product quantization wasthis um like online k-means kind ofproblem and thinking about like you knowhow many vectors do you need to load into compute the centroids and then howdoes this scale with incremental updatesum like I guess the question would belike what are you kind of learning aboutthat problem uhyeah you you need a certain so so it'svery hard to pick an exact numberbecause you need your data orthat subset of your data needs tosomehow be representative of the entiredata set so in in the ideal case let'ssay you would have five clusters in yourdata naturally then those five clustersand then you want to train on tenpercent then those five clusters need tobe represented in your originaluma ten percent if they're not forwhatever reason like if let's say thefifth cluster didn't even exist and youonly have four clusters for trainingthen you're training on something wherethe fifth cluster doesn't exist yet sothat doesn't mean the fifth clustercan't be assigned anywhere but it meansthe the sort of trained model isn't asgood as it would beum uh if if that had print part of itthe good news is that there's a certainsize where it gets more and more likelythat the the distribution of the dataset sort of doesn't change of coursethat's no guarantee like it couldcompletely changeum but often what you see so for exampleon onum these a n Benchmark data sets thatare typically between 1 and 10 million Ithink we typically didn't see anyImprovement uh when using more than ahundred thousand vectors for forum for training sothat that's still a very manageableamount I think it takes 10 seconds or soto train with with a hundred thousandvectorsum yeah sort of a good a good trade-offyeah fascinating Yeah a hundred thousandjust like getting a sense of that issomething that I think is a reallyinteresting research question whenyou're you know going into the detailsand yeah I think this whole like clusteranalysis is definitely a lot to it Iremember you know earlier we did apodcast on Bert topic and we werediscussing like what would clusteranalysis and weba look like I think it'sstarting to see I don't want to giveaway I know that we have somethingplanned but like um you know we'reseeing Partners Integrations othertechnology companies building furtherinto this like embedding spacevisualization topic modeling clusteranalysis and you know definitely peoplelistening to be on the lookout becausethere's going to be some really coolstuff around that coming soon onembedding drift detection and all thiscool stuffum yeah awesome so I think that's agreat coverage of the PQ rescoring Ithink probably for me the biggesttakeaway is um you know understandingHow Deeply that goes into the hswtraversal and what you said I think is ahuge point about originally you werethinking I'm trading off memory foraccuracy but now that you know you havethis restore ring there's no trade-offsso just use it that's kind of a funnytakeaway yeah yeah for for some cases Imean there there is it's engineering sothere's always trade-offs and it's kindof like not seeing our users data it'shard to make predictions but we'retrying to model real life data usagewith with as many different data sets aspossible and I'm really surprised to seesort of how good it is in in some caseslike yes in some cases there is still abit of a trade-off but in others it'sjust notoh yeah yeah I think um having all thesediverse data sets is such an interestingthing of machine learning generally likethe very is still kind of no free lunchin a way like uh yeahawesome so I think kind of we have atrio of new features I'd like to kind ofcluster into one kind of category whichis kind of like we V8 being feature Richfor search you know like vector searchobviously like search relevance is ahuge topic and I kind of just to give aquick tldr of each of these things uhthe first of which is re-ranking whereso as I mentioned with my originalunderstanding of PQ re-scoringre-ranking is this idea where you takeyour top say k 100 results from Vectorsearch and then you're going to takeeach of those candidates in the queryand pass each of those as input to ahigh capacity scoring model so where dothese scoring models come from sentenceTransformers has six of them that areopen source and so we've built thedocker image where you know similar totext to back Transformers you you havethis like local image if you wanted tospin it up in your laptop host ityourself and all that good stuff we havethe you know the cross encoders thathave been trained from sentenceTransformers are up there and then alsoreally interestingly we have are-ranking endpoint from cohere andobviously our friendship with Millsrhymers you've seen Eddie and thoserhymers together um you know this is aan API for a re-ranking model and Ithink that's quite fascinating I don'tthink there's another another companydoing that and it's pretty interestingto see that API and you know it comesinto like weeviate's module system howwe have these um this orchestration ofAPI requests I think I think it's quitefascinating maybe actually adding wecould take a pause and I I really wantedto ask you this question about how um Ithink an interesting thing about we v8'smodule system is how golang handlesthese uh like concurrent HTTP requestsand maybe talk a bit about like um whatgoes behind that how golang is optimizedforscaling this kind of you knoworchestration to model inference becauseI think it's such a fascinating part ifwe get in general go is a language asyou say sort of easy simple concurrencyis one of the building blocks of why youwould use go so go has channels and ithas a simple synchronization Primitivessuch as mutexes and and these kind ofthings that you need for easyconcurrency it has a race detector whichis also a super important feature to tofigure out like are you doing yourconcurrency safely are there any kind ofrisks so in general sort of go is a goodI would say it it makes it relativelyeasy and thenwhat that allows us to do is if a userjust gives us sort of a a bulk datathingy like like a batch of data objectsessentially and our job is to sayvectorize them or or also re-rank themI'm going to pass them to to the modelif we would do that sequentially thatwould also mean that whatever thelatency is for one of those requestswill be bounded by that butthe kind of cool thing about theseserverless applications is that theytend to scale way better than that so ifyou hit them concurrently that allowsyou to to sort of not have to wait forone thing to be finished but just domultiple of them at a time that saidagain there is a bit of a trade-offbecauseum often these models have rate limitsso I think something that that oftenpops up in our support forum is forexample with open AI there the thedefault rate limit is I don't know whatit is but something like I think maybe60 per second or so that that has a 600per minute so if you want to vectorizemore than that you could potentially hitthe the rate limit so also concurrencyis not a free lunch here but it it helpsto at least sort of max out whatever thethe model provider can can use and thisis the same for for if you self-hostyour your modelum if you don't concurrently dosomething and it doesn't necessarilymean that it has to happen with withconcurrent HTTP requests that could alsomean that you bolt this up into a singlerequest and then maybe the concurrencyhappens on the model side but in ingeneral you want to sort of not haveidle capacity around like like if thereare either gpus or CPUs that are usedfor model inference you want to makesure that they're they're maxed down andthat's one of the motivations for usingconcurrency in in that case and thensort of taking that back to to golangit's a language that makes concurrentprogramming rather easily I know thatthat some of our team members sometimeshate go for it because it's it's notperfect in any way but no no language isum but it's it'syeah a very good sort of fundamentalconcurrency kind of modelyeah fascinating I think I also like theum you know like the Gen like ourgenerate module with openai palm thecode here I the way that particularlythe way that it paralyzes doing thesingle result call I find a ton of usein that where you want to put a promptof each of your search results and havethat all be parallelized it's so muchfaster than looping through there yeahyou know I think it's pretty cool sookay cool so on this topic again ofsearch results so we have re-ranking nowwhere you can get better search resultsand you know kind of building it rightinto weviate is why we took this tangentinto the more technical thing you don'thave to have you know manage some otherservice of your own to do re-rankingit's built natively into alleviate andyou know taking that search resultslices and either way so the next coolthing is auto cut so autocut has thishigh level motivation of you know howmany search results are relevant if yousir you know if you search for I I'vestolen Bob's uh landmarks in Francething I use this all the time now so ifyou search landmarks in front and youhave the Eiffel Tower in Paris and youhave some other things and you know sayyou only have like eight thing eightlandmarks in France and your searchresult cutting off that nine and onlyshowing eight resultsthis is incredibly interesting for thelanguage model retrieve blog went togeneration thing as well because thelanguage model can be quite thrown offby these irrelevant results so like Idid see something about Spain it mightwrite about that because it's in it saysbased on the search results and it'slike well it's there so you know so Ithink the interesting thing aboutautocut also is understanding uh howthat's done so it's like the um you knowit's like calculating the delta in theslope from the point so you take each ofthe distances and then you have tointerpolate a line from there and thenyou measure the steepest change in theslope so what we've got users will seeis a hyper parameter on which of thesteepest slopes because you know youhave like two to three as each reverseof the thing so it's like how extremelydo you want to be cutting it and thenthe last thing is you know one of ourfavorite topics is this kind of uh rankFusion algorithm weeviate has built-inBM 25 as well as Vector search so whatthat means is you end up with two listsand so you know the first time we didthis we were just doing you know justcombining it based on the ranks so youknow if you're like first ranked and BM25 and like fourth rank in Vector searchthat ends up you know just purely basedon the ranks and now we have like arelative score Fusion so yeah I thinkall these things um just all this new uhbenefits in search I think the questionI'd like to ask again is just kind ofwhere your head's at on yeah like thesearch stuff I think you know maybeBerlin buzzwords there was somediscussions on the search side of thingsI've been to the haystack conferencepersonally and seen that kind of likeyou know world around search relevanceyeah to to me this is a someone who'swho's very use case driven this is forme extremely important that we think ofthese these use cases and search is abig one it's not the only one there'srecommendations there's classificationsthere's called the of course the whole agenerative AI part but generative searchthen also as a part in thereum and for me it's I I like being ableto give our users something that theycan use end to end where they can saylike hey I want to spin up bb-8 I wantto have the data management part so youwant to have the whole database I wantto have the vector search obviouslybecause that's the whole reason of ofusing bv8 as opposed to say traditionalsearch engine uh the Hybrid part that'ssuch a big one just the the bm25 likenot having to spin up something elsethat that does the pm25 for you butbeing able to to do thatum and then all these new additions andand re-ranking and auto cut these arejust sort of the most recent ones but Ithink there will be way more down theline and a essentially if it makes yoursearch experience better then it'ssomething that we could potentiallyextend deviate within because of themodule system we're very flexible andeven adding sort of things that thatessentially the module system allows usto add things that not everyone needswithout loading VBA because if you don'tneed the module just don't turn it on soum this this kind of yeah everythingthat makes searcheasier to use better and I guess in away also more accessible because there'slike information res informationretrieval is like this whole researchtopic out there and there's there'sspecialized conferences you justmentioned Berlin Bus words uh which aresuper fun for us but for someone whojust wants better search maybe they'renot maybe they don't want to get intothe nitty-gritty details maybe they justwant to have better search and therewe're we're always trying to find thatbalance of making it easy for someonewho's new to the space without sort oftaking away the flexibility that someonewho's more experienced needs and I thinkthese these new re-ranking modules andthen AutoCAD these are great examplesbecause if you have let's say you haveyour own complex ranking pipeline that'sway more complex than something that youcould fit into evade just don't use there-ranking module but for someone elsewho's already using bb8 maybe doesn'thave a complex pipeline says like heythere's something that I can just turnon with a single flag and it makes mysearch experience better by all means doit that's that's what we're here foryeah yeah yeah I I think there are somany interesting tricks from theinformation retrieval uh Community wecan get out of to get better search Ithink um I think with re-ranking tomaybe stay on it a little more I thinkmaybe some of the search zealots wouldbe a little bit saying you know I stillcan't do symbolic re-ranking you knowlike if you're searching for productsyou might want to have the price and therelevancy these as features that youwould put into re-ranking uh so you'veseen we've had a podcast with meta rankwith Roman and Siva where you know we'rethinking about this thing as well it'llinterface very similar to thisre-ranking with coherent sentenceTransformers we'll just send theproperty to the API as well and andit'll look super similar there's also apretty interesting discussion aroundlike multi-valued vector search like Ihave a title Vector as well as adocument Vector so you know we'relooking into all these things as well aslike the the large language modelre-ranking thing is quite interestingwhere you prompt it with kind of thesymbolic rule so like please boost it ifit has the if it's recent and then youand then you give it the result likeJson dictionaries and that does workpretty interestingly I think it's alsoreally exciting for Integrations withlike llama index and Lang chain aboutand Samantha Colonel and Pat make a listof these things but I'll just leave itthere about how you can you know justget better search results by just takingthese things on and yeah I think alsomaybe just before uh graduating to ourlast topic I also just want to thankDirk for this Dirk did an incredible jobwith you know the rank Fusion and theauto cut I remember like the other guywas trying to figure it out myself andit was such a headache trying to figureout how to do the um uh get the go thego numb uh thing to do the slopeinterfellation and I had showed it toDirk and he was just like the next daylike oh yeah here's the notebook to testit I was like that's so fast I've turnedit around so really impressive stuff anduh yeah it's awesome so kind of our lasttopic is the um the cloud monitoringmetrics with the Prometheus and I'mcurious you know I imagine as you do wementioned like the trying to break yourtesting with the multi-tenancy I'mcurious like where your head's at withthese you know how you you knowbasically observe this massive thingrunning in the cloud rightyeah it almost feels like one of thoseboring features compared to all theexciting features that we just talkedabout but I think this is this is what'sso important to to pay attention to theboring stuff becauseeven something like multi-tenancy I meanwe're super excited about it but in theend you could argue like from a from aAI perspective this is a very boringfeature like it's not a New Concept butthese are the kind of features that youneed to run reliably in production forme anytime we we add something like thatand we we sort of expand on on theobservability stack like this is asimple changeum essentially it allows you to to trackyour success query rates like previouslyyou could only monitor I think latencybut now you also get just a ratio ofwhat queries have succeeded versus whatqueries have failed and and the failurereason so was this a user initiatedfailure so in HTTP status codes thiswould be like a 400 uh plus a statuscode or is this did something go wrongin vb8 so a 500 or so so internal servererror and these kind of things and theones from from out thereum so so adding the the monitoring forthis to get better observabilitybasically on these kind of thingsit's very boring but it's very essentialand I I love seeing bva grow up and andhave our users ask for these boringthings because in the end boring isproduction and production makes ourcustomers money and that makes us happysoalways always a place in my heart forthe boring featuresyeah I think well yeah I think um likejust for me learning about like datadogand learning about these companies thathave built around that kind of thing isit's really fast yeah I mean it's it'sto me it sounds like kind of likeInsurance on your uh thing that you knowlike your data is up in the cloud it's asuper abstract concept of like a littlebit of it is in Virginia a little bit ofit is in Germany so yeah yeah oh that isyeah data Doc is a massive company sothat just goes to show like how muchhow much need there is or how much howmuch room there is for for these kind ofobservability topics exactly as you sayit's like insurance it'searly warning it's just sort of lookinginto that the the more complex that yoursystem growsyou can only see so much from theoutside and sort of getting theseinsights and just seeing what's going oncanhelp sort of in in hindsight for fordebugging cases like what was going onbut ideally before something happenslike early warning systems like simplestthing uh monitoring for for memory usageor something like you see something goesup nicely in a linear line and you knowthat there is a certain limit with yourcapacityif it says 30 maybe that's fine 50 maybethat's fine if it's approaching eightypercent maybe you want to changesomething about your infrastructure soyeah these kind of operational thingsthat you need umit's it'sfor me it's it's just such a positivesign of like hey people people do needthem which means they're doing seriousstuff with bb8 so I'm very happy toalways ask our team to prioritize somesome observability featuresyeah well eddian awesome thank you somuch for another release podcastmulti-tenancy PQ with rescoringre-ranking autocut new hybrid rankfusion and then new updates to thePrometheus Cloud monitoring all soexciting um and also you know you canjoin the community slack if you needhelp with any of these things everyone'salways around and checking that all thetime and uh yeah thanks so much forlisteningthanks for having me had a blast thankyou", "type": "Video", "name": "Etienne Dilocker on Weaviate 1.20 - Weaviate Podcast #56!", "path": "", "link": "https://www.youtube.com/watch?v=xk28RMhRy1U", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}