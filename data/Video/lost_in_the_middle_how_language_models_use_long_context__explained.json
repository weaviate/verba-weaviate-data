{"text": "Hey everyone! Thank you so much for watching this explanation of \"Lost in the Middle: How Language Models use Long Context\" ... \nhey everyone thank you so much for watching Wii vid on YouTube this video will explore a super exciting new study into retrieval augmented generation titled lost in the middle how language models use long context lost in the middle is used to describe this U-shaped curve where we're exploring the impact of search result quality on the language model's ability to reason across the search results that have been put into the input so we asked a question like what is ref devec what is the atomic number of oxygen what are cross encoder ranking models and we take that query and then we hit our search engine get search results and then we pass the original question plus the search results to the language model so there this has been something that's just super popular in application Builders hackers but there hasn't been too much experimental findings I just you know the nuances of how exactly you're going to append search results into the input like this so this is a super exciting paper that has this interesting u-shaped finding that is asking this question of where is the relevant information so if we return you know 20 documents is the answer in the first stock document the 5th 10th 20th so on and then also what is the impact of how many documents we're going to return to then pass the language model you know say we give 5 10 20 and so on so this is a really fun paper to read I hope you enjoy this paper summary video really exciting to see more research more understanding of retrieval augmented generation so quickly before diving into the experiments to give a quick background on retrieval augment to generation we have questions that we want to ask a language model like about our particular data whether we have you know some kind of particular software we're building like in the case of weviate and reftubec or whether it's your legal contract your particular data to analyze your email so on so we wanna instead of having the language model just be able to answer questions about our data we're going to retrieve from a search engine and then put the information into the input to then help the language model region across it so whereas it would say what is ref to vac and it says hey I have no idea what that is if you then give it the context where you you know you've retrieved this text Chunk from the vector database and it says wevia 1.16 introduced of the back module blah blah and then it's able to reason about what ref to back is so this is broadly the setup where we're going to retrieve information to put into the input to help answer the question so the key question the authors are exploring in this paper is how does search quality impact generation in this initial example this is an example of the Oracle context where you have just the perfect context for answering the question what is ref to vac you know it gives you the perfect definition of it but in the real world retrieval systems are a little messier than this we can't guarantee that the first result is going to be you know the perfect answer to the question so the question is then like what does that error how does that error then impact the generation level so you know say the breath say this particular information about reptavec was the third search result and the first was you know I don't know something about cross encoders the second was something about replication or so on and so what is the impact of the needed information not being at the first but at the third tenth so on and you know as you saw lost in the middle the more it gets to the middle the less the the language model is able to reason across it so then there are two key experimental controls to study this phenomenon how many documents are retrieved in the input and then where in the retrieved documents is the answer so they're going to be firstly they're going to control this to show exactly the impact of it by using the natural questions data set and more details that we'll get into but the key finding being that when the you know when you have this Oracle type of thing where you have the perfect answer it's right there you have really high performance and way better than the closed book just language model answering the question and this is on pretty general questions so you know you imagine if it's reasoning about your emails or whatever obviously the closed book would have no way of reasoning across your emails or whatever so you know if it's in the first position we get massively successful result but then if it's in the fifth or the 10th in the case of the tenth it actually performs worse than not having any retrieved contacts so just a super interesting finding odd thing about retrieve augments generation really emphasizes the value of good search quality but let's get a little further into the experimental details before coming back into that so let's dive a little further into the experimental setup so as you can see here you know you ask a question like who got the first Nobel Prize in physics and then you have the relevant document so in this case the correct answer is in the second position so you see how you retrieve you know three documents answers in a second so the first question is about how many are we going to be retrieving so you see the difference between retrieving three compared to retrieving you know five in this case and then the next question is controlling where the relevant information is going to be and again I think another detail about this paper that I really like is that they're they're controlling the experiment they have the ground truth and you know it's not about like just the messiness of real world data although we will see they did an experiment on that as well which is really interesting but you know they're gonna they have the answer and so they're going to be controlling where it is one two three four so on and that's how we were able to study this so here's the first interesting set of results we have the impact of the number of documents and the position of the answer exploring uh six different models uh anthropic Cloud anthropic cloud with 100K context gbt 3.5 turbo and then gbt 3.5 Turbo with 16k uh input lengths the MPT 30 billion instruct model and longchat 13 billion parameters with 16 000 input links so the most interesting thing is seeing the kind of lost in the middle thing probably in the middle you see that when you only have 10 you you kind of see it with the five but the 20 really shows it well as is a 30. so anyway so you see this Con this kind of concept of like if the answer is the 10th document out of 20 you get way worse search results compared to if it's like one or two as you see towards the left and I mean it's amazing to see that if it's one like the impact of perfect search would be just massive for retrieval augmented generation yeah I I maybe say maybe pause this if you want to look through it a little further for me just kind of seeing the U shape and seeing it come from 10 20 30. so the next really interesting thing explored is the impact of the number of documents so again these are you know gbt 3.5 Turbo with 16k input lengths Cloud 1.3 with a hundred thousand input length so you know these are pretty uh long context window models so you know one question would be can we just you know give it a ton of search results because then you know we recall at 30 becomes a lot easier than recall at one recall at five but what they're showing is that in the current state of the models you know the more you give it the lower the accuracy and this is um averaged across each of the positions so you know when you see this plot this point for uh 10 that's the average of when the relevant document is one two three four across the entire question answering data set you know 30s so you get idea so the two key takeaways from these experiments are model performance is highest when relevant information occurs at the beginning or interestingly end of its input context you saw that u-shape that comes back up at the end I think that's kind of interesting that like as it's attending if it's at the very end it goes back up so that's just one of those kind of quirky things about these language models and deep learning so then the second thing is model performance substantially decreases as input contacts grow longer so you know we're not going to kind of solve this problem by you know in the current state we're not going to solve this problem by just giving it more search results and then hoping that by you know increasing the window we get the relevant Target that way sort of also in the context of the final results I think it's really interesting to just look at this table of the closed book versus the Oracle the Oracle being you know the information is search result number one so this is the case where we have we've solved search and we get perfect search results but you see this difference like with the MBT 30 billion instruct going from 31.5 to 81.9 or Turbo 56.1 to 88.3 just a huge potential in kind of improving search and then what that would mean for the the Improvement on this and it's worth noting that this this data set natural questions will get a little more into it in a second but you know this is like general knowledge if this was about you know again it maybe set it to death but like your particular Discord chat or slack or emails or whatever then there's like no chance at all at the Close book but you know you still could fine-tune the language model on your information and then I think you'd be in a similar kind of Camp as this so you know this kind of I think even with fine-tuning on your domain you still would want this retrieval and then you know I mean we'll see as these experiments continue to evolve and it's also interesting but just this table right here you see the input the impact answering these general questions the having the first search result is just an enormous Improvement so one other interesting detail with their experimental results found in the appendix are some experiments with gpt4 so gpt4 is you know the pink one that's at the you know performing better than all these other models and you still even see that U-shaped curve with gbt4 so I think the deal I read a little bit about in the appendix is that they do you know you see it in the in the title of the plot is that they do 500 questions to sample so I think it hasn't been as fully tested as the other models maybe details with the API access I think gbt4 is still kind of a newer thing at the time of recording this video so you know just you still see the u-shape Curve even in like the most powerful I you know I don't want to pick favorites but I think I think gbt4 is widely regarded as one of the world's most powerful language models at the time of this recording so as mentioned earlier the current results we've been looking at have been in a really controlled experiment where the authors have the ground truth answer and they're explicitly moving it you know one two three four but now let's see what this looks like in a more realistic uncontrolled retrieval augmented generation setup so the next experiment is going to be using the natural questions open data set which contains historical queries issued to the Google search engine and then human annotated answers extracted from Wikipedia but in this case instead of you know controlling the system by explicitly moving where the answer is we're just going to look at what happens with the contriver embeddings retrieval system so contriver is one of these you know dense embedding models it's been fine-tuned on Ms Marco and so we're going to be retrieving with that model uh to then you know have the input and so it's a little more you know uncontrolled in the sense that you know where the correct answer is could be like eight three seven like whatever the performance of the retrieval model is so this is the result that we find from that so I think it's important to firstly start with what this uh orange curve at the top is this is the recall so uh so as you retrieve fifth it makes a ton of sense that like recall at 50 is really high you've retrieved 50 documents so recall is like is the correct answer in one of the retrieved documents or is it five it looks like it's a little under 70. so then what you're seeing underneath that is the performance of the models when these documents are then given as input so you see that you know from 5 10 20 the performance saturates it doesn't continue to improve and some of the models you know see like this drop from 30 to 40 in the MPT and and so on so it's a pretty it's a it's more messy because it's more uncontrolled because you could have um you know examples where the input or where the answer just isn't in the context at all so that's what kind of makes it different from the original uh experiments okay so now that we've explored this u-shaped impact of where the relevant information is and how much information we give to the language model the authors then want to explore key value retrieval to answer the question of to what extent and can language models even retrieve from their input context so what that means in this case is we're going to be giving it key value pairs of randomly generated uuids so like if you're using weeviate you know that this is the ID that you give to each of your data objects so you have this key value and then so the task then becomes key you know 9f for a so and then to Output so you're trying to measure just how well it can copy what's in the input so similarly to the documents retrieved in question answering we can also vary where the key value pair is going to be in the list like say it's the sixth key value pair and then as well as how many key values there are so you know this these are just showing kind of that difference between you know you have a lot of them or you have a few of them and then how you can control where where the query is going to be and then where it is in the context so here are the results of the key value experiments so shown on the far left we have again this thing this U-shaped curve where when the key is in the 25th or 50th it doesn't perform as well as when it's the first key or the last key kind of well the last key is a little funkier with this and also interestingly though we do see that some of the models are really good at this whereas others not as much so maybe this is another kind of test for you know which of these language models you're going to be using but you know we see as we you know in in effort of trying to break it this model the brown one the gbt 3.5 turbo 16000 once we scale that up to 300 key value Pairs and then we put the key to be reasoning about in the 200th position then we get to around 45 it looks like so so you know it it seems to do a better job at this than the than the other task which is pretty interesting I think being able to just copy that there's a lot of like learning to copy with lstms and recurrent Network work that kind of this these kind of experiments have been you know tested before but anyway so these are the results of again finding some U-shaped curve in this kind of experiment as well to further investigate these details the authors are also going to explore the impact of architecture comparing decoder only models with encoder decoder experiments with query aware contextualization and the effective instruction tuning so to start off with encoder decoder versus decoder only an encoder decoder Transformers you first encode all the input into a vector and then you would put that Vector into the decoder that then attends over that Vector encoded by the encoder so it makes sense to hypothesize that you know when you're encoding everything into a vector maybe it has the you know the con the context of looking at the entire retrieve search results and that way it's able to see that even if it's in the 10th position you know as we've been seeing with this U-shaped curve it can still attend over when it's decoding only but so they're going to compare the flan T5 XXL model with the flan ul2 I'm not super familiar with these models I put a little bit with the flan T5 series when we were you know looking at you know this before the like the chat gbt thing came out we were exploring these kind of models pretty heavily but so anyway so the interesting thing here is um you know they don't really find that this encoder decoder decoder only thing impacts that too much but another really interesting nugget that I think is really interesting for prompt design is this query aware contextualization so when you're decoding only the model only looks into sorry only looks at the past if it doesn't like attend to the Future tokens so if you're saying you know please answer this question based on the context and then you don't give it the question yet then it's just kind of like forming this representation of what it's looking at without any without the question to actually like help kind of guide its reasoning through the search results so they're showing that a pretty big Improvement by just putting the question in the beginning of the decoder only models so that way it can you know look at what is reasoning about as it's building up the representation so I like this quote particularly with that key value experiment gbt 3.5 Turbo with the 16k inputs with query aware contextualization achieves perfect performance when evaluated with 300 key value pairs in contrast without it it achieves the lowest performance of 45.6 so the difference between you know what is the value of the key and then that key like 9f 4A B2 if you remember that the difference between giving that before you then show the key values compared to you see the key values and then you ask this question like that kind of thing so I think that's pretty interesting for the prompt design for retrieval augments generation so the next thing is this really interesting thing of trying to highlight maybe the bias of instruction tuning so you know thinking that with instruction tuning that you know maybe it's bias towards the answer being right there because that's kind of how the instruction tuning might look like so they compare the MPT 30 billion the language model with the instruction fine-tune checkpoint so I think there was a lot of confounding here because the language model is uh you know not as tuned as the instruction models generally are and that's why I think you see the accuracy is so much lower with the just the pure language model but I think what is really interesting is you're seeing the impact of Mosaic ml open sourcing the language model as well as the instruction tune this is one such experiment I think there are so many experiments that we can do by exploring the different fine-tuning routes from that MPT 30 billion base model so I I didn't really take anything away personally from this but I just I love seeing this kind of like MPT 30 billion versus the instruction checkpoint and how that's been open source I I think that's all super interesting so finally one last thing the authors present is the c real position effect the U-shaped curve we observe in this work has a connection in Psychology known as the serial position effect that states that in free association recall of elements from a list humans tend to best remember the first and last elements of the list so you know maybe that there's something to that comparing how language models recall things as well as how humans do so to conclude here are some of my Reflections from reading this paper lost in the middle how language models use long context the first of which is the importance of search so kind of you know before The Branding of vector databases this was referred as a vector search engines or vector search databases this kind of looking into information retrieval literature following along with you know what search practitioners are doing and the practices behind search has always been core to wevia and just as an example we get 120 the latest release at the time of recording this podcast uh sorry video introduces cross encoder re-rankers Auto cut and new hybrid search rank Fusion now you've seen like the you know how you can combine bm25 with Vector search and leviate so search has always been first class Citizen and we V8 alongside that Vector index and the database scaling and all that kind of stuff so quickly let me take you through these three new the three like latest features just to show you a little further how we V8 helps you get that better search that avoids that u-shape and hopefully gets you that Oracle context so here we are in the weeviate documentation to look at Auto cut re-rankers and new hybrid rank Fusion so I think autocut is probably the most interesting one for this paper we've seen this other paper called uh large language models are easily distracted by irrelevant context and that's all kind of motivated you know even just human Searchers want this kind of thing where basically the idea is if we see a jump in Vector distance so we search something like what is reftubec and the first text Chunk has the vector distance 0.1899 then 0.1901 0.191 and then we have a jump 0.21 autocut is a new feature in weeviate that's going to look at these uh slopes and so cut it that way so you know it interpolates a line between the vector distances if you you know plot them on an axis of just X being where the search result is and then y being the distance you interpolate that line and you look for very steep slope changes in the distance and so you'll cut that kind of thing so when you're searching weevier you have near text Concepts you know this animals and movies that was turned into a vector and then if you pass in Auto cut one that means the first steep jump detected it will cut the search results and this is a great way to only pass in the top three results the language models probably the best solution for you know this kind of uh the findings behind lost in the middle so another interesting thing then is um you know re-ranking search results so we also have the re-ranker modules that were introduced in uh 1.20 so we have re-rank our cohere as well as re-rank our Transformers so re-rank our Transformers better for say local development you know uh the hugging face sentence Transformer libraries they've open sourced cross encoder models cross encoder models sorry to set the stage a little more they take as input the query and then each candidate document so say you have five you know five potential search results it would take the query and then that first result the query and that second result and then output a higher capacity score so you're trading off speed for a more accurate ranking and as we've seen with lost in the middle that more accurate ranking could end up being super impactful for retrieval augmented generation so with weavate 1.20 we have you know two ways of doing this you can either use the local sentence Transformers or you can get an API key and use the cohere model so cohere their you know off offering cross encoder ranking models as a service as an API so you know just really interesting stuff for the you know search quality and so finally with hybrid search you know we V8 has bm25 built in and Vector search so you can combine the uh the scorings of documents of these two algorithms and so when we first released this we just had rank Fusion where you're just looking like you know it's okay it's ranked third in the bm25 and it's ranked second in Vector search so just combine it based score it based on the ranks whereas now we have an actual uh score Fusion so score Fusion you're like looking at the particular score that came out of bm25 or similarly to autocut you're looking at that like distance that came out of the vector search so you know more ways of how we combine these results and overall deliver that you know hopefully that Oracle uh search context the second key takeaway for me is I want to see this testing other kinds of context so right now what we saw is this setup where you know k out of the end results are relevant but I think in this whole you know argument of how do large language models use long context I I've been just super inspired by the Llama index idea where you're retrieving from different indexes so you know in we V8 we separate data objects kind of like categories of objects into classes so you have blog posts podcast transcriptions a code repository or like Wikipedia so I think the interesting thing in Long context is to retrieve from each of these sources so you know again I ask you what is reftubec you might want to retrieve Snippets from blog posts you know Wikipedia entry or like say a knowledge graph that has like you know these tuples to give information about reftubec as well as podcast transcripts like how is reftubec used in conversation and then maybe like code Snippets of seeing it in action in some way but this idea of retrieving from different sources I think that's also a huge part of this conversation and not just you know only k out of the end results are relevant but you know of course that's a huge part of it and then you have the compounding effective as you retrieve from these other things there could be irrelevant things as well and that's why I think this whole re-rankers autocut hybrid brand Fusion it's also exciting of trying to you know retrieve from all these sources and make sure that you get that number one result is a good result so then finally you know I think a lot of these you know we have these extractive question answering where you're you what is the atomic number of oxygen eight and I think generally it'll be more interesting to explore the impact of you know where the relevant information is how much is retrieved in abstractive or you know summarization where you have to write these like long responses to bizarre questions that you can't just kind of like give a factual answer quickly too so it makes the controlling of the experiment much more difficult but you know it's just kind of the evolution of this space you know evaluating Generations broadly is quite difficult but I think you know it just that is what I would predict is the evolution of these kind of experiments thank you so much for watching this video explanation of Lost in the middle how large language models use long context I thought this was such an interesting paper so cool to see these experimental findings on the impact of where the relevant information is in the retrieval augmented generation setup as well as how many documents are going to be then handed to the language model I love seeing the key value retrieval the expiration across different models from the gbt models the cloud models you know MPT and longchat it was overall just a really cool paper to read so if you enjoy these paper summary videos please subscribe to weviate on YouTube it really helps you know encourage us to keep making content like this if you're curious about weave yet you can check it out on weeviate.io Open Source on GitHub weviate webgate and please follow us on Twitter at weeviate underscore IO thank you so much for watching ", "type": "Video", "name": "lost_in_the_middle_how_language_models_use_long_context__explained", "path": "", "link": "https://www.youtube.com/watch?v=Kf3LeaUGwlg", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}