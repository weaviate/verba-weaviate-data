{"text": "Useful links: Generative Search docs: ... \nforeign [Music] air I believe this is the fifth episode so we are just one away from half a year celebration I think we should get a cake or something next time why not this should be really really cool so for today um uh first of all I want to say to everyone thank you for coming and and spending your time to watch and listen to everything that we want to share and and thank you for being part of this community if you're watching it live or if you're watching it later on please give us a like it always helps with the videos being more reachable and accessible and you'll come up more often so maybe we can get more people coming over and then watching with us participating asking questions Etc and speaking of questions please ask questions as we go so I will be more than happy to answer any reasonable questions yeah well there's some questions that will probably moderate out or something but in general if you ask good questions we're more than happy to ask any questions you may have so today we have few topics that we would like to cover um I'll quickly I'd like to quickly cover the topic of meetups because we started doing more meetups around vv8 and we are visible in many different locations so I'd like to talk about that later we also kicked up Google summer of code campaign for this year so I'd like to talk about this as well we will talk about search in viviate this is something that many of you asked us for many many times and it's working progress we're not there yet but we'd like to show you what we have so far and and that will be a pretty cool topic and then also we'd like to talk to you about a pipe API proposal this is really cool topic of something that we are in in process of making we are not ready just yet but you'll be really great for you to learn all about it and then also share your opinion your feedback and of course I am living The Best For Last uh we've had this huge announcement yesterday which is uh really great to see actually we had pretty good response from all over the world uh so we'd like to talk to you about the generative module and how it works there will be demos there will be conversation there'll be questions hopefully a lot of questions from everyone involved in here so these are the topics and I hope that you all enjoy watching it as we share what we have to share so since I I have the floor so I'm gonna do the the annoying thing and then kind of like continue talking to you and then the first thing I want to talk to you is a meetups so basically this month we are going to have a couple of meetups so we'll have one Meetup in Toronto and that'll be run by Zen and then we'll have another Meetup in Boston which will be run by Erica and Connor and I actually decided to jump on the plane and then come to both those meetups so if you anywhere near those locations uh definitely come and join us so just a quick reminder for Boston it's going to be on February the 21st uh while the Toronto one you'll be a few days earlier so that's February 16th so we're talking yeah next week Thursday and then two weeks from now should be pretty cool and we also have meetups in other locations so if you have a curious what locations uh for we have for meetups and where you can meet us and see us then you can go to wivid IO and then at the very bottom we have all the locations so you can see Amsterdam New York San Francisco so we have links for each of those currently we don't necessarily have scheduled Meetup just yet for those additional locations although a little Barry is telling me that Dan is planning something for one of the last couple of weeks of March to do in San Francisco so yeah if you're in San Francisco or nearby then definitely could be a cool option for you and then I also want to do something early March probably in Amsterdam so that should be pretty cool if you feel like there's a location that we definitely not covering where you feel like oh there's a few of you that like the meet up to happen give us a shout there's many options maybe you could run it and then we provide you support or maybe we could help organize it ourselves uh let's see how it goes but if you feel like there should be a meet-up I'm more than happy to hear all about it next topic I want to quickly cover is Google summer of code so I'm not sure if any of you is new to it but the whole idea is that Google open up this really cool project where various companies especially open source companies can join in and then propose various project ideas and then very students can apply to participate and then the way it works is basically as this like you could go and review some of the project ideas that we have so you know we could maybe build a new demo with a jupyter notebook or if you know about Django or any CMS that could do the Vivid integration that could be a pretty cool topic we are looking also for a way to Implement code examples into our documentation that are interactive so basically ones that people could go and execute in the browser but also make changes to them and then ideally it would be great to have them you know in Python in JavaScript but also like go and and Java that would be pretty cool um we also would like to see like a an important tool so it's something that JP showed a few months ago that we had a proof of concept we just didn't get to building it just yet but if you'd like to be part of this that would be really cool and maybe you could help us build a new vb8 module there is all sort of different ideas and and options like maybe there could be another like uh image to vect type a module using Ray uh why not that would be pretty cool um we also looking to make some improvements to the Vivid client so if you are interested in helping us make our either python Java Juice or go they don't have to be all of them uh clients then give us a shout and perhaps this could be a great project for collaboration and if you have cool ideas that I didn't cover right here there is also an option for like go and propose your own idea right and then in order to apply and here's how it works usually uh the first step is you go and apply for a project and we have two places where you have to apply for one there's this Google form that we've created so in here you can go and then provide like your details your level of Education where you base and then the next page is basically you say which project you want to participate in and then provide a bit of background as to why you think you'll be the best person for it and then also separately you need to go and then submit to directly to the gsoc page so kind of have to do it in two places and then based on that once we have those projects approved we can then work over the summer so usually projects kick in like mid-june and then they run through July and August project should be complete and everybody will be happy so yeah I share we've shared the links to the gsoc page so if you go to vividio Google summer slash G sub 23 the link is in the the YouTube channel so you can find it there and same thing for for the meetups we've shared the links as well so yeah this is basically the announcement iHeart and um Erica you first you suck separate from an internship I guess if someone has a question on internships that we make yeah it's yeah exactly it's not so it's not exactly an internship but it's very similar if you think of it so it's something that is organized by Google or by the Google summer of code program we also last year did Google summer of docs which is why we have docusarus today for for our website but so it's not officially like an internship internship but it's very similar it's like we could argue over semantics right like maybe you know semantic says will probably match them both but they're not exactly the same um but it's definitely like yeah it's like a single Project based uh yeah single product based activity and the way we did it last year as well and it would work the same way this year for every project there will be a mentor or a couple of mentors attached and then usually if let's say you Connery were like one of the students or you work right like each of you would meet with like a mentor for your project for one hour a week and then you would be a like you know discussing what needs to be done what are the next steps uh maybe review the code or maybe review the ideas and then it kind of like goes on and on until the project is complete or Runs Out of Time and that's kind of the idea yeah yeah I was just gonna add there's a potential to do really impactful work uh Sebastian was telling me that some of the modules that we use and the customers use were actually initially initiated by uh g-soc students right which modules were those of us yeah so and oh my God I think the the open AI module that we have uh it actually was built by by student and there was a second one and I'm just going blank but yeah two two of the modules that we have in Vivid were created by a student so they collaborated with Martin um yeah our senior engineer and that was pretty awesome and also if you look at the Vivid examples so some of the Vivid examples in there were created by uh by a gsoc student so definitely there's a big big impact and and here's the thing um the the impact it that the project may have depends on the quality of the output right if at the end of let's say that import tool is at the stage of like this is a pretty solid uh proof of concept but it needs more work then that's what it will be right it will be it could be become a basis for the next level of the project well if it's something that's like actually this is a fully blown fully tested everything like an import tool um that should be ready like we'll probably just like uh say hey this is the author uh and then everyone can use it and launch it right so definitely there's a lot of opportunity for an impactful work and also it's a it's a cool way to um raise your profile so if you're looking for a job after that and they're like okay so what you've done you could show like some University projects maybe or you could go like I feel this you know thousands of people from the viviate uh Community or tens of thousands are using it on a daily basis you go like oh that's pretty cool right so yeah any more questions no not on mine all right so I think next we should talk about search in widget and I know Dan you want it to really share with everyone uh what's what's been happening in that space in that space yes hi everyone my name is Dan tuscalesco and I'm a developer advocate for bb8 on the US West Coast and this is my first time on we be there so thank you Sebastian hey welcome welcome it's great to have you here what I would like to share on my screen is this um new search feature that we are working on so this is the graphql console for it we don't have a UI yet and this is how you can run queries so for example if you want to learn how to import data you run a query like this and it gives you the content of that augmentation page and its URL so let's try a different query for example you want to know how to search by concept and I made some purpose there because um our research polarized that and it returns this content page with a near text filter and again a blog post that explains how to use it so this is going to be released at some point in the future it's going to have a convenient search interface and we're looking forward to get your feedback on this that's so cool but what's that Meme I put a search in your search so you could search while you search I just I'm a big fan of the movie Inception and this is something like that so I'm curious about how how is getting the data like uh is it like a like a web scraper where you get the mark down and chunk it up like what went into that um we'll actually grab the HTML that's generated so this might not be optimal right now and the rows are not quite correct at the moment but it's all work in progress and um yeah we essentially feed the HTML Pages import them as data objects very simple schema and then there's a search that runs against the content super cool yeah and I think this is still a little bit of work in progress in general but hopefully we'll be able to add that officially to to our Vivid search page it's a sore topic so I'm excited to see the work on it so it's um something that we must have on the documentation yeah and I mean a big part of like why we haven't done it before um it's also it's not the challenge is not how easy it is to implement with it with that search I think that the challenge that we looked at was that we would be judged based on the quality of results even though that necessarily with it as we did as such we are not responsible for creating uh those vectorizer modules right we are the ones that we that that use those right to to provide the results so if for some reason the module that we use doesn't help you answer exactly the question we still would be judged on that whether it's uh our responsibility to make that model respond to those questions or not that's right so we need to increase our confidence scores here and perhaps I'll train on our specific document domain yeah yeah yeah I think maybe to preview what we're talking about later I think when you add the generate to this and it's retrieving from the documentation you say like please write me the code for the new uh say when we add audio to VEC please write me the audiovec code and it could generate The Code by retrieving from this information Source just crazy yeah that'll be chargpt in your browser I mean yeah that that would be pretty easy to add right so um and then as we'll probably talk about it towards the end of the session um changing that will be just pretty straightforward thing so pretty cool so unless we have any other questions we can move on to another topic having said that to your question earlier than Parker comes to the rescue and I think it is actually text tobacco here and this some Transformers were the GSA contributions so it's pretty cool um if if you haven't heard of the summary summarization Transformer because I think Erica we talked just before this session and and you're like oh right I wasn't aware of this this is actually a really cool module uh FYI there's like one where you can run any query and then pass it to summarizer and then it can summarize uh the content of that so it could be something that's like 10 paragraphs long piece of content and then you could summarize in like a short paragraph or a couple of sentences so um that's kind of cool and I see yeah you are just searching for a summarize filter that's pretty cool yeah all right thanks Dan for this uh demo so in the next segment we have who do we have who's next raise your hand I think we have Connor talking about the pipe API right corner awesome super cool let's get into it yeah the floor is yours uh Zoom I share my screener yeah let's do it awesome super cool so I'm so excited to be presenting this pipe API proposal so this is something we've been working on about oh sorry about search pipelines so what are search pipelines so usually what happens is we start off with a query like uh what is reftubec or how do I use the summarize filter and first we use some kind of information retrieval like retrieval method like hybrid search or you know near text near Vector near object and then what we might want to add in the search pipeline is re-ranking with say a cross encoder or maybe this recommendation cases where you have these more particular learning to rank models they use these like a more complex feature engineering but you have some kind of re-ranking phase and then maybe we hand that off to the generate module so what this API is about is about letting weva users kind of decompose these steps for how they want to form these search pipelines within webiate so here's an example of the API proposal as it is now so at the top level pipe is going to be equivalent to get aggregate or explore with the key difference being you put kind of the chain of the search filters in as the argument to pipe so you start off with the query how do I use VBS pipe API and then you have the this could be a query to Dan's uh documentation search and then you have the stages so stage one so stage one would be the name of stage one and then it has Associated parameters and then at the end you index the class that you want to be retrieving from so similar to kind of like git and aggregate how you have the class and then the properties that you want to return and the underscore additional so so let's kind of Step a little more into this one search pipeline flow of retrieve re-rank and then generate so what you would have is you would have a query like what is reftubec and first you hit that with a hybrid search and then you can also chain together the where filter so Parker came on the uh podcast to talk to talk about rough the back replication and other things in the 1.17 release so we have this where speaker equals Parker Duckworth searching through the podcast Clips then what we'll do is we'll re-rank the uh these outputs from the hybrid search with say a high capacity cross encoder that is uh it's lower but it's more like fine grained how it scores the search results so now once we've so we get this kind of like coarse grain search result from our retrieval method then we you know clean it up with the re-ranking and now our top five that we hand off to the generative module is like you know some really good search results really high quality what is ref to vet content so we give that and you know we can do say write a tweet and this is German this was an earlier iteration of the uh generate arguments but you get the idea of how you pass in the different search filters then you say you want to search through podcast clips and you want to see the speaker in the content but it's just a rough idea so here are some of the search Primitives right now that are in weba that we're thinking of chaining together so you have the spell check you have you know Vector search with near Vector near object you have bm25 hybrid search and then you have the symbolic filters of where and then you have so right now we have a symbolic ranking which is sort we can also have the kind of the neural re-rank and we're also thinking about say like taking move two and taking that kind of vector ranking out of the argument to near text and also making that kind of a primitive that you could put in the pipe and then we also have say extractive QA summarize and generate as things that you could plug into the pipe API so now I want to talk to you about something that I think is really exciting and more more something that's kind of on the frontier and that's the flow of retrieve generate and then rank so the first idea behind this is so these generative models like large language models they're stochastic models so you every time you query it it'll give you like a slightly different output because it has this Randomness to it and that's also controlled with the temperature parameter as you look at like the uh you know like the open AI DaVinci if you go to the playground stuff you see that you can slide that temperature which is how random it's going to be so what you can do is you could sample many potential outputs from the generative model and then re-rank those outputs so one example of this is by using reward models and this is what's used in say deepmind Sparrow model so here's an example let's say we have a a therapy app so John I can't believe it someone took my parking spot that I always use when I come here and Bob is our uh is our app like hi Bob from the from the other thing so Bob says well it's not like you own the spot it's first come first serve and John says but I always use that spot it's convenient and close to the entrance so now we send this to our language model and it generates several candidate things to say back to John I don't have time for this just deal with it and get over it I'm sorry that happened to you and so on so we have eight different potential Generations so now what we're going to do is we're going to take a sentiment classifier as one example of reward model and that sentiment classifier is going to assign a positive or negative score to each of these potential outputs so you see I don't have time for this just deal with it and get over it that has a negative 2.5817 score I'm sorry that happened to you but it's important to remember so that has a high positive score of 3.5607 so then what you'll do is you'll rank these outputs based on the positive score and in the end this number seven I understand how you feel but maybe you can use this as an opportunity that's what ends up getting sent to the user so you've generated all these candidates and then you've re-ranked them with some kind of classifier and this is so this is one way to just kind of they use this in like sparrow and papers like that just to kind of like filter the quality of it so they have all these things like oh is it reinforcing a stereotype or you know the harmful thing of these large language models that scale plugging them into making it like the next Google that kind of thing but you can also use this for uh say say you're going to be using a cheaper language model like you know the da Vinci one I think is like two cents per thousand tokens generated and I'll tell you about how much money I spent on the demo later on but like maybe you want to use a lower capacity model and you sample a ton of outputs from it because it's cheaper and then you can re-rank those outputs like this so it's like another interesting thing of what we can kind of achieve with the pipeline so then let's talk about another way of re-ranking Canada Generations which is personalizing them or recommending them so earlier we presented ref to VEC which is a way of representing a user based on what they like with a vector so you can do this online online recommendation case so what we can do is we can similarly generate a bunch of candidates and then we can re-rank them to personalize the output to use so say we go into our stable diffusion model and Erica comes with some shoes that she wants and it generates all these shoes and then it and it learns to re-rank the candidate Generations based on you know which shoes Eric has been interacting with so to kind of come back to this this is sort of the early thinking behind the pipe API design you know you put these different stages in like this kind of syntax and so we'd really appreciate it if you want to join us in this discussion on GitHub so if you go to github.com issues and this is issue 2560 so we would really love it if you would join this conversation let us know what you think about this pipe API design perfect this has been great and by the way we've included the link in the description of the video so don't necessarily type it yourself but that's the link definitely uh for all of you to come in and yeah we'd love to hear your feedback your ideas um I I know we talked about it Connor before and and I really like what that what you could do with pipeline but it's like it was a bit of an eye-opener when you showed the example with this like sentiment uh example search like generate the results and then score them based on the sentiment and in a way because there's no like one way of doing it because you could actually say like hey I want to uh respond with a super polite answer or I want to be really nasty or you go I want to find a happy medium I don't want to be overly polite but they want to be rude either so that that gives power to you like when you write a query to to make it exactly how it should be it's it's pretty awesome and and I like how there's so many possibilities right because you could check it for like is does this use inclusive language or it could it be you know culturally inappropriate etc etc and and I know even like there's some startups that help rewrite job descriptions so that they are more inclusive because there's like for example certain language that is very helpful for people like me middle-aged white guy right but everybody else feels like oh I don't like this content so I think this kind of use cases could be achieved with this pipeline it's like it's amazing so I like it I like it a lot if my therapist outfitted that first response I would fire it I guess it because I don't know gender but um also where would you include the positive sentiment I guess if you can go back to the oh okay so I meant to uh so in this kind of part I'm more so saying like okay here's what we can here's what we can do with pipe sort of As We Lay the foundation from it I think sort of this is the thing I would recommend looking at first this is sort of the next step for pipe because um when you're going to be doing generations with the da Vinci model you're going to be spending a bit of money on the on the generation so it makes a lot of sense to me that you would want to re-rank your search results so the trade-off with re-ranking is that it's a little slower it's also maybe a little harder to maintain because now you have another container with the with another Transformer in it that's going to be scoring the query documents and giving that high capacity score but this I think is like the next step of massively added value and then with this I with um sorry with this idea of the reward model ranking I think we could introduce something like a you know classifiers as another we V8 module is you know maybe that's another module to build for the Google summer of code yeah hey yeah yeah that's cool that it would be implemented and we need so instead of having to Define it yourself be like positive class negative I don't know who would want to use the negative but well it depends you may want to just like put a lot of pressure on somebody right just like uh like in negotiations when you want to be really tough you shouldn't need to be nice right oh we should have like an alpha score like we have for hybrid it's like positive negative like right in the middle there we go modul API is being designed yeah that's lovely it's lovely is there like any time frame of like for by when we want to hear hear this feedback or what's what's there could we have a discussion maybe on slack or should we just all focus on this GitHub issue what's what's your preference I think the GitHub issue is a great place to discuss if you go there now you'll see uh edians left some initial comments and so like if if you sort of look at what Ed Ian's done that's sort of the perfect example of I guess how these GitHub issue conversations happen edians like the the master of the repository I'd say and it's still something new for me to be learning honestly this open source the way that the issues are open is just really fascinating I'd recommend checking this out even if just it's to see what these kind of Open Source issues look like foreign it's pretty amazing cool cool let me check because there might be some uh questions some questions about the Google summer of code in the chat yeah so I I could probably do that so like suppose I'm into interested in a particular project from the gsoc list what should be the next steps so NG edits so Angie I think that the best step would be you could so first of all join this slack Community for for wibit which I'm pretty sure you already are part of and then in there we have a g-soc channel um if if you have an idea on working on some of those projects you can ping us there like Drop say like hey I'm interested in this project what what should we next is we could maybe have a conversations if you already have an idea of how would you want to approach it or if you could come up like hey if I had three months I would do this in first Monday second this third or some sort of anything to kind of like just get going and that'll be probably a good thing and and honestly like this step is just go and then apply uh to uh through the two links that I shared earlier so you'll be pretty good like that and on a different font of like talking about uh get up and at the end you know I think it's like if you see say his name three times you actually so like I think here he is so we have at the end I was like yeah definitely like this is something that we truly believe in right like we are not a GitHub we are not an open source company just for marketing like or this is not a marketing strategy it's literally how we want to run our business how we want to communicate because this is the best way for for all of us to do the best and and I always like kind of like use this example it and that's part of like diversity you know the power of diversity and power of like having all these opinions if you have like five people coming from Harvard and you give them a tough problem they will give you a really good solution right but if you take people from like five different completely different places they'll give you five different solutions and quite possibly a couple of them will be better than the that single one from Harvard right so that diversity of ideas and things that you may see or may have missed um yeah it's just so important to to always appreciate so yeah that's just my little few cents cool so I don't see any more questions for now I hope uh everyone is enjoying it and by the way like for those that you listen like drop us a message from of like where you uh listening from and then just just say hi in general like that that's enough not all interactions have to be questions so uh and if you say something nice I'll show you as well on the on the screen as a banner so why not cool so like I said we're leaving the final topic uh the best topic for the last so that's not to say that the other topics were important but I think we are super excited about the genitive search uh project oh well there will be a demo so I think I would pass the floor to maybe then then you have like some opening ideas and you want to discuss yeah we can talk about well first we'll talk about what generative search is a little bit later on but I wanted to talk about gpt3 what chat GPT is and what are some of the shortcomings of chat GPT um I don't have to tell you what chat gbt is it's the fastest growing app service it was the it reached 100 million uh daily active users I think it was a monthly or daily I can't remember now but in two months the next closest is Tick Tock which got there in I believe nine months um so everybody knows what chat GPT is but uh one of the things that is super interesting that's coming up on Twitter even before while we were working on this module in the background I was looking over Twitter and people were like oh this would be cool if we could have a personalized version of this right what if we could have uh our internal uh organization docs trained on chat EBT so that it could become a automated uh HR chat bot or an automated um I don't know like a customer service chat bot where it knows what products I have on my Ecommerce store which ones are on sale which ones are on clearance and it could answer all of these questions um not just in a robotic way but kind of taking your own data and then having it uh having the power of the generative module of chat GPT right getting it to creatively answer and realistically answer questions with your own data and so that that really is the utility of the uh generative open AI module that we announced this week yesterday in fact and so the main idea is the power of chat GPT the creativity of chat CPT everything we like about chat gbt but customize to your own data and the applications are really Limitless um so we wanted to talk about that a little bit there are some questions that I had and instead of getting on a call with Connor one-on-one and kind of uh talking about and expanding my understanding I thought that we could take advantage of my ignorance and for for the benefit of the wider audience we could ask those questions here but um Connor if you'd like to add in details to that yeah I think coming off of what you just said I what I love about this idea of how we v8's integrated chat GBC is so I used to think that you were going to need to fine-tune the language model on your data so you know you'd have to have this 175 billion parameter model and there are these advances in like sparse fine-tuning that meant that you know it was getting more cost effective but still I mean you have to do some gradient descent optimization on your data and that to me seemed really ambitious but now what we have is this General reasoning ability that can you just provide the data with search and then it can reason across the search results and it and it has this like local memory kind of thing that you can do where you you can prompt it to like kind of store intermediate uh outputs to overcome that 4096 uh context length and that I think is just so powerful that kind of local memory in in addition to the external memory I think that you don't need to fine-tune the language model and that I think is the big I I honestly am so surprised that this that it ended up working out like this yeah I thought I thought we would have to do something similar and I think I I was talking I was looking at this um press release from Microsoft actually yesterday and they might eventually go down that route for like an Enterprise version of chat CBT where you have the ability to fine tune but if you use the new module that we released the results with just providing context and then grounding your prompt or task in that context are pretty fascinating it gets pretty much the entire job done yeah it's really interesting maybe Erica you could talk about your early experience with just Chad gbt first thoughts on it and oh yeah I think the main point I wanted to cover was extractive versus generative um question answering so chatbots aren't new but what is near aesthetic you're able to give it like this open-ended prompt um so like you said you're able to ground the question in the content that you just gave it and I'm going to show a demo and give an example on this but I mean it's just fascinating like instead of just like a quick short answer of just extracting it from the paragraph that you gave at that it can like generate a paragraph a tweet a Facebook ad it's like fascinating that it even knows what a tweet is and it's pretty accurate um which is really cool so that's just one thing I want to cover but also like just for like an HR or chatbot like that kind of the ability for it to extract information and then summarize all of it is really cool is that what generative searches this is one of the questions I had what's the difference between generative search and search so I know in search you can ask a question and then it'll give you like 100 pages of resources that you'll have to peruse through to get the answer right what is generative search yeah well if I'll take I'll start this off and then and think about like um yeah like I take it as I think generative search started as an academic term to refer to you would have the language models just general like same with the stochastic sampling that you re-rank with the therapy app you'd use the language models to generate candidate answers and that would be like your search is like you get the information from the language model but I think now what the term is evolving to mean is just like that generative layer at the end of search so you search and then you give it to the generative model that's how I'm thinking that that term is going to evolve it yeah so I feel it's it's an evolving term at this point there was a a press conference from the CEO of Microsoft yesterday that I was watching and he I think he shared a similar concept he didn't call it genitive search he called it AI backed search and the and the whole point there is instead of giving you the 10 most relevant links it'll give those 10 most relevant links to the uh the large language model and the large language model interprets and produces a relevant answer regarding that content and then it cites those uh sources in the in the at the bottom there right which is pretty interesting because now you can with search but before search we had to go through and look through lots and lots of resources this is relevant this is not relevant the initial concept of search made that easy now feeding that into AI makes the next part of our job easier so he was saying how it's not that it's going to do your job for you it's just going to take the results and interpret them and give you a potential first draft that you can then improve on you accept this don't accept this which is pretty interesting there's also a bit of philosophical debates when it comes to um image search for example generative search for Samurai sub gate Golden Gate Bridge during fog in the spring at 8am there probably is a picture of that but thatly could also generate one that would be near indistinguishable from reality so as a user does it matter to get a real picture or generate one at some point you won't be able to tell the difference yeah and I know that both Google and Microsoft are are working on stuff like this where if you're searching for images they're not just going to bring you images that are sourced from somewhere on the web now if they think that they can create a better image from a model like dally or the open source version of uh similar to that stable diffusion you'll actually be able to see fake and real images which is pretty interesting and the same thing could happen for music audio Google just came out with this model called music LM you can describe what type of music what you want to generate and it'll create like 30 second five minute clips of it for you and so it's not just question answering text to text or text to image it's all sorts of modalities that are coming up now it's so exciting to be in the space yeah I think as a user if you ever give me like a response that is like automated generated that is not like a bite person or something I would always want to have a flag something that lets me know this was generated because uh because I think there's a an element of trust when I get something this is real versus generated and and I actually in the past wrote an article about whether chatbot should pretend to be people or whether you should always be aware because in my opinion if you ever talk to a chatbot and then you find out like six comments later that this wasn't a real person while you were like super polite and everything um and by the way you should always be polite even to chatbots um like you you would feel like you never know right but you you you you would feel cheated right you would go like oh I built this trust build this relationship I ex like spend this energy and then it's like it's just a machine right like it's like what the hell so I think I'd want to know the truth you know like if I'm in The Matrix tell me yeah sorry really quick um when using Chachi BTO we start with please I don't know why but when I had to start paying for it I was like ah let me excuse like a minute please because I don't want it to use that as a token now that it's your service so so you know this technique like when you're dating somebody for the first time you know what kind of people they are based on how they talk to their chat Bots right to Tinder and then it gives you a personality score yeah well I wanted to actually relay this back to Eric anyways because what Sebastian was saying about truthfulness and I know Erica has built this demo that she's going to show us soon about where you you know there's like this kind of SEO content where you know you generate it to hit the keywords so what do you think like should websites that do this like kind of just like brute force a ton of const content that comes out of language models what do you think is the future of that I think having the source is extremely important like Sebastian said like building that trust but also knowing where it comes from um I had asked chat to BT a few questions and I was like huh like is this taking is the content that it's given me from Wikipedia is it from someone's personal blog post like can I use this can I relay it um so I think like a track of these sources is very important um yeah whether it's generated or real I don't think that matters as long as the source is there and it's clear with that the other thing that's super cool to think about that is if you're taking Chachi BT and not just asking a general knowledge questions or publicly available questions that can be answered with public publicly available information but more proprietary information you don't really want it to hallucinate details like if I ask a HR chat bot when's the next company social or when the next holiday is or something like that I don't want it to give me a fake answer that's that's stochastically optimized on on previously pre on my training set I wanted to give me real facts right it can flourish it by telling me Oh you might want to book like a flight or something two days in advance for this but I don't want it to give me fake information um how do you limit hallucination in a in a like a customizer private application of chat GPT can you limit it or sounds scary yeah it's a really great question well it kind of comes back to when Erica brought up the extractive QA versus generative models as well like I think the extractive classified the answer in the context I think that still has a ton of value especially because it's I mean it's way cheaper it's super fast like if you're putting this into your website do you want to pay however many I mean like I don't know like you'd have to do the math of like how many viewers you have any questions right but like the extractive QA model where it's classifying the answer in the context I think that will still have a lot of value also with the temperature like with the Q a opening how we have that it's like reducing the randomness I think is important but also still generating the like of balance like yeah just setting temperature to zero and yeah so can you explain what happens if you set temperature to zero it takes away it takes away the stochasticity completely or that's a good question I don't know if it takes it away completely but yeah that's the scale more deterministic towards zero ah okay right yeah I did test it out and I had temperatures set um really high I don't know I didn't check it and it um I think my question was on what is Rapture fact and it outputted a Labrador Retriever as popular in America I was like I didn't even mention a dog like what's happening keep that one internal so it was confident hey don't don't you know why they are related come on it's so obvious maybe it knows something I don't I guess I don't know maybe they're both super enthusiastic and full of energy and can't help themselves when there's food around I know Sebastian was touching on this as well but how much would it cost let's say I wanted to just try this out as a hobby uh this module that we released how much would it cost me um yeah so right now you're paying about two cents per thousand tokens generated and when I do my demo I'm going to start off by let's go into the playground let's see how much how many tokens we're looking at realistically but um yeah I mean My Demo cost me fifteen dollars so yeah you have like a 15 certificate like I know my stuff you know I've invested time sweat and money fifteen dollars at two cents per thousand prompts that's a lot of problems yeah I think you're being charged for the prompt too what I suppose it makes sense maybe the like the way that they batch the inputs or something like that I'm really not sure but yeah that's interesting would it be a good time to actually then jump into like the the within integration because I know we have a couple of demos and then because then we could also use it as a context maybe and that could be relevant like all the temperature and kind of results um because I know Erica you have demo right should I share your screen because I think that'll be pretty cool like this is super cool stuff right like I'm gonna go quiet now floor is yours so I'll start with the docker file um so I we you have to use the latest version of bb8 which is 117.3 or a newer version um you can also run this in WCS I mean you can use the free sandbox for 30 days um it is already implemented in WCS you can do that as well um but for this demo I'm using I'm running it locally um so one thing to note is in the enable modules you have to use the generative openai and you have to include this with the vectorizer that you are using in this demo I'm using text spec openai Sebastian has other ideas but you can also use any vectorizer cohere hiding face and the other one Sebastian yeah pretty much any any vectorizer that that you have that we have you could use it with with open AI uh with this like the generative open AI because this is the really cool thing um the generative uh module is not used for a query right so like you search for uh for your results and then once you have the results you pass it on to the the genjive module and go like hey I got this result what can you do about it so technically you could actually just do a scalar search without any vectors still retrieve results like you could say like give me the results where title equals this or like this that's not a vector search and yet you could retrieve some results and opacity to a generative module to kind of go like now create a response for it right so that's that's what I'm kind of saying like as much as your demo Erica uses to open AI based modules they could you could use anything so the kind of stuff that if you're already building with V8 um you can just that at General open eye to it and then boom you have an extra option possibilities are unless right now so many so many possibilities and then this is where you will add your openai key I've hidden my secret key so I don't get charged a lot of money um but you can also import at import time as an import query time um instead of in the docker file um so it is up to you all right I will show my data um so I'm using the podcast on uh from Connor Parker and edian on the 117 release um so you can see we have three property properties we have speaker content and then the podcast number um Conor has a GitHub repository on um I think you have like 15 maybe already complete yeah 15 out of the 35 but I'll get it done and I guess I could omit pod number because it is only I'm only using the 31 but just so you know if you clone the repository uh this will change all right now I'm going to go over to my schema um so this is standard the real the cool part about the generative module is there really is nothing different about it um for the schema so just like to have any instance running with me that you need to have a schema um so here I'm just pointing out that I have my class the Pod clip which is podcast and then just my three properties which is content speaker pod number and I'm using the text to back open AI module all right and then uploading is standard it's what we've all seen and if you aren't familiar with it if you have a great quick start tutorial end to end is what I think it's called and you can reference that it's extremely helpful and Sebastian and JP have worked very hard on it all right and now on to the fun part of the console all right so here we have the query so we have pod clip and I'm using hybrid search um so my query is what is rectifac and I'm setting my output as 0.5 so it's doing the mixture of hybrid oh sorry of dense and sparse a search and I'm limiting it to one um just because that's all that is needed and I'm output I'm retrieving the content from that which again is where they're speaking about reftubec um and by they I mean it's either Connor park or onion and then uh this is the new part is additional so have generate single result and I'm giving it a prompt so my question is what is reftubec and I'm asking it oh no I didn't includes please see I wasn't lying um answer the question based on the content that is given from up here you don't have to be polite and then I'm just going to Output the single result and I included the question what is reftubec because I found that it always included it and the the generative module always had a question so I just thought it'd be easier to Define it here and then just the answer um so you can see uh that it answers what brought to back centroid is ETC but the really cool part is that it is condensing this content bit into like a few sentences any questions anything small Point anything out yeah so that um parameter that you have like that content thing what what is that doing exactly so that's a good note content is just a property of pod clip so I don't want people to think that content is um like you have in your data you have to have content it could be um like ratings or that's about it or review I guess so if if I had a property review this is where I would replace it here and then also here so that is something to keep in mind maybe we could try adding speaker above content and then in the prompt uh please answer what speaker setup said about content would that be fun for everyone you you can do that speaker yeah so maybe yeah uh please answer what are you speaker oh yeah in the curly brackets or maybe instead of the just was so then it would be like please answer what Parker Duckworth said about content I said you could embed the speaker property can you yeah so so it's putting in the speaker result into that prompt template will there be a should I oh let's run it enter what uh actually yeah yeah cool see what happens let's see oh this is going to be cool oh Parker was speaking about it but yeah yeah maybe like please include something about who said it yeah I think that I think you've done a great job of just showing that you can do the two properties yeah yeah that's a cool idea maybe I'll get it to work while we talk about something else or not yeah I think it already works all right so this is uh so in the generate module we have added single result but we also have grouped result and what this is doing is well what is happening there we go um we have added group result um so instead of a prompt it is being assigned a task um so here I'm also I want I'm using Vector search sorry about that to get to return where they are speaking about hybrid search and I'm setting my certainty to 0.75 just so it's um as accurate as possible right um I'm limiting it to three let's see I think this would be in the history I just wanna let me just show hold on sorry on the Fly I guess maybe one extra curly bracket all right so here's the content where um speaker sorry all right so here's the content where Connor is speaking about hybrid search then we also have Parker this is hit where he's speaking about it and then also edian right so if I add in the uh generate the group result it's going to consider these three results oh no here we go and it's being tasked with combining the results which are which are the three that I just showed and um condensing it into one paragraph on explaining what hybrid search is oh no oh wait I know that worked all right so here we have hybrid search is a combination of keyword scoring and such as tfid yet it does a great job how amazing is that and again it's just um outputting content because that is what is in my schema and not because you have to use content but yeah and that's September hey this is pretty cool I had a question about this limit what if so from what I understand we're using we V8 to filter the context and then provide the prompt and the context to the open AI API um is there a limit to how much context we could provide yes I think Connor had some trouble um like if it is too long I think open AI times up I believe yeah I have a little recommendation for that on the fly if you under if you go to error so Byron added error above group drizzle and maybe let's go ahead and put limit to 30 to break it on purpose here yeah nailed it 30. yeah so if you so it'll tell you you know hey you try to give it uh in this case okay yeah also I feel like if you're kick report let me just limit it to three it also kind of it doesn't help so why not limit one limit three I think in the first one I had limit one yeah I'm just here yeah I think it's really cool like um if I can add on this limit thing well this whole kind of like so we've yet to implement it with asynchronous calls so the open AI so like if you want to do uh summarize all the podcast Clips then you it can do it will do all those calls for all the podcast Clips like in parallel and I thought that was really nice like I was previously I was looping through it in Python and you know I'd be sitting there for like five minutes waiting for all the calls to get back I mean that's the part of the beauty of developer experience in viviate right like it's all about making making your job a lot easier than uh it needs to be right um I think there's one thing Erica in the first demo for the single search right or single result because at the moment you're limited to one but um if the limit was five then basically what we would do is run the query and then for each response you'll get one generated answer right so definitely don't necessarily run it on like if you have 10 000 objects on like all ten thousand because there'll be a lot of tokens um so it's good definitely to to limit and now we because yeah this will perform five parallel uh requests but you don't need to worry about like doing it one by one just like Corner set so um this is cool because so now we should get five answers right okay let's check it out look at it go so if we had type uh operator now then we could take these answers and then maybe do some sort of analysis of each answer and then rescore that and reorder or like just provide like the best answer right yeah yeah amazing yeah because he was like are you still talking about reftube or are you talking about uh Labradors you know like right like maybe there could be like a field uh some sort of modular can gen double check if this is a true answer or if it's a hallucinated nonsense or star yeah brilliant and it lives in weavia so nicely I think the yeah yeah amazing very nice well done cool demo Erica yeah awesome job here Christ thank you so um what should we cover next Conor did you also have a demo that you wanted to show or yeah my demo will be quick it builds on Erica's demo using the same data set and the same idea and yeah if I could share my screen um so yeah there's one there's one kind of one more thing I want to add to this which is um that was my screenshot all right so first let's let's go into the open AI playground and let's see how much 4096 tokens is all right so if everyone sees my screen here uh this is the playground for open AI so so this is a segment uh from this is the uh transcript of Bob's demo on our new podcast announcing the generate module so this is 622 tokens this blob of tech so let's you know just keep going what's a token is is a water token or what makes a token awesome question so so token is you is like it's like a part of a word so it usually be like three characters together so so you the tokenizers they look at the statistics of the commonly co-occurring characters and then they try to compress the groups of characters so you have like wordpiece tokenizers by pair encoding they're like all these different techniques but uh yeah I I think with the new era you can just drop it into open Ai and see how your tokenized account but I'd also recommend if you are curious about this of seeing um you know hugging face has the auto tokenizers Library so if you want to play around a tokenizers that's a great resource for it okay so so no that's a quick question on that yeah um so what about stemming and stop or do you think that could improve or search results are that's an interesting question with with like how you would remove stop words for handoffs to the generate module I think it's probably best not to do that because it probably uses all these little language Clues so like stop words would remove uh like the and yeah things like that and I think those little language Clues do help a lot that's the kind of like the templating thing with the speaker said content on date those little words in there that you actually like help it a little bit Okay cool so so this is 4 400 tokens so if we just kind of you know it's it's a good bit of text that chat gbt these gbt models can take in so you know one two three four you know it can take in you know roughly seven of Bob's demo transcripts no that's like the most hand wavy measurement ever but like just to get a sense of how much you can take in so so it's it's pretty good at taking in you know it can take in pretty long context but you know you can't put an entire code base in there you can't put an entire book in there right so you we still need these clever strategies and and that's the reason why I think we've yet is just so exciting and like this positioning of the language model and the vector databases is so exciting because chat gbt offers this reasoning but then it needs help with the memory because you know set six paragraphs or so it's it's like a lot but it's not like unlimited right so there are two things that I want to talk about and the first of which is breaking down a task into subtasks and the other of which is top level indexing so then and then first I want to thank our friends at langchain and gbt index uh Harrison and Jerry both they're both going to be on the podcast we've been publishing those podcasts soon and they've done incredible work on pioneering the tools in this kind of space of decomposing tasks into chains of language model calls and also the top level index Construction so let's talk about how we're going to break down the task I'm sorry this is I was like rushing to get this done before so I hope this like it's not as neat as Erica's with the docker file and all that but you'll see the idea hey the history is happening live here in you know here live you know that's important yeah I'm still finishing it now but the um okay so we're going to Loop through each of the podcast clips and the prompt is going to be please write a summary of this podcast clip so that that is stored and so here's the next thing we can write data back to eviate from the language model so the language model is going to generate a short summary and it's going to write that in weviate so now you have pod clip has short summary short summary a new Eva class and so so then we're going to aggregate the short summaries into a group summary so so you know this again so we're trying to get away from that 4096 token length if I take all these short summaries and just try to put them into GPT and say summarize this it goes error this you've tried to give me like 12 000 tokens so we group them into group summaries and then we're going to summarize the whole podcast with the summary of the group summaries and then what we can do so this last thing is What's called the top level index so now we can search through the summaries so so let's say I want to say uh you know what's the most similar podcast to podcast number 35 you know generative search with leviate I now have a Vector representation of these podcasts which are these like wildly complex objects so you know say you have scientific paper and it has passage has passage like now you have a top level index you have some Vector representation of the whole paper and so with ref to VEC we were looking at you average the embeddings of these passages but now we have this new controller with the ability of the large language model to summarize that gives us another way of forming that Vector representation at the top level so so I mean My Demo isn't uh super organized but oh sorry so you can just see the uh on the uh so you can see the uh the summary so so in the end all pause summary oh yeah oh no oh yeah all right so in the end you can see the uh so the All pod summary so if we want to say uh so let's say we take this is the podcast with or let me say so this is a podcast with Bob so the most recent one that just came out uh sorry ah well let me copy and paste this all right let's do that much so if we just add here a near text and Concepts this one nope sorry about that wait so anyway so that kind of idea of oh sorry about that okay let me just start eating let me just say good all-pod summary uh let's do new attacks sorry everyone for the delay on this all right let's just say concepts we're going that's part of live coding right like um sometimes you may say character it's hard to find Okay cool so okay so what we have here is the so it's searching through the summaries of all the podcasts so we say you know it's saying uh the podcast with Jonathan Franco where he was talking about uh Mosaic ML and we did also talk about uh the self-ask prompting and all that so so you can see how you like now you have this top level index so so this is the recent podcast of Bob and this is the podcast with uh Bob Chris and Marco where we talked about uh facts and how they're using large language models in search so so yeah so you have this top level index that comes from breaking up the task into first summarize the podcast Clips uh then summarize the um you know the summaries and then form the overall summary so it's how you decompose the language model calls and and achieve the memory that lets you overcome the 4096 tokens cool so that's all I got nice so yeah cool so that's basically yeah the generated answers already so I guess yeah because if you keep uh if you already have the answers right like you don't have to regenerate them over and over again right that's the idea yeah Coco hey I I've got a question and back to like Erica's demo originally right um or if we were to do like um a generative search on this could you make it uh to respond in another language like say like hey given the context of this what is this explain in Spanish or polish or German is that possible I did I did see Bob's demo had um write the tweet in Dutch I can't read Dutch but I took his word for it and he says I know Dutch so maybe we'll go back from summary of summaries back to a summary in Dutch you want you may want to limit it to like two three right in Spanish yes foreign now who speaks Spanish here Erica speaks a little bit of Spanish oh Don you and I when we're in Italy we tried that communicating in Spanish right like maybe you can figure out the Spanish yeah but yeah result in the uh first return I think it helps to pick it up like this why not just please summarize it in Spanish and then end with content please summarize this please write the summary oops sorry okay stop yo so you definitely passed you're definitely passing the the chatbot politeness communication test yeah it's a proof that Conor is a nice person in real life too okay I'm sorry I don't know why it's um I think kind of the way you format it um well I guess we'll just have to figure it out another time or something and let's see what happens what if you don't do please write in Spanish like is there just the syntax issue or yeah oh I got it working okay that's good Erica nice message to Erica no worries that's part of like live coding right so uh let's do it oh I think answer in Spanish can't be translated so we have some like Miami Spanish going on thank you today that is epic nice nice so impressive so impressive cool I guess um that's most of the stuff we have to cover right for today and um any final questions any any takes anything from anyone here in the audience yeah if you ask any question when this is not running live I won't be able to retrospectively go into my time machine and answer it so I'm sorry for that my time machine it's out of battery at the moment that's cool and we promise to give you real answers we're not going to use uh machines just yet or do we I'll do it well we promise we won't let you we won't be caught yeah definitely all right uh Team uh thanks for for this amazing session thank you all for watching and listening and and then please let us know your feedback your ideas uh just just to summarize if you want to join any of the meetups that we have like like I said at the beginning we are in Amsterdam we're in New York we're in Boston we're in Toronto and San Francisco and specifically in the next couple of weeks we will be in Boston Toronto so definitely join us there um if you're a student and you want or you want to contribute to vv8 we are kicking off Google summer of cold uh the links are in the description so you should definitely do that if you're interested in the new pipe operator is it an operator well we can figure out the new pipe API um there's a GitHub issue go read and see what others are saying provide feedback give us your ideas because we want to work on all of it in in the open and then any feedback will make it just better Dan shared with you as well for how we work on our search capabilities that we will use for the documentation so that you should be able to search throughout docs through our blogs and through the podcast that Connor is currently transcribing so this should be pretty awesome and of course like the the thing uh the cherry on top of the cake like with the generative module like we're so super proud and as you could see it's pretty amazing and um and it works really really well well there's a little caveat you need to get your syntax right and your brackets right uh but once you have the syntax right everything works just perfectly um so yeah that was uh that was it's been a pleasure and uh thank you all for listening thank you team for joining and have a great day evening morning thanks everybody thank you very much see you soon bye bye ", "type": "Video", "name": "Weaviate Air \u2013 Episode #5", "path": "", "link": "https://www.youtube.com/watch?v=a_yfl4jIXFQ", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}