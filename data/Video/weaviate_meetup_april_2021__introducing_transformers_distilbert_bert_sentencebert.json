{"text": "documentation: https://weaviate.io/developers/weaviate Github: https://github.com/weaviate/weaviate. \ni have a few slides just for the meet up just to you know to talk through the meet up and for those who um uh are new to eva they're just a few things that i want to go over with i don't want to go into detail too much into detail into that because that's of course something that's done in the past normally i also was numbering the meetups but my my colleague lara who took over the meetups unfortunately couldn't make it today so i have absolutely no idea where we are number wise so i just call it the meetup and as you might have seen is that um we're now releasing more and more into eva then we try to schedule a meetup every now and then to share these uh these updates with you um these videos always also will be placed on youtube so if you want to see something back you can do that as well so let's quickly start why we why we are here so i also i always would like to go back to reiterate the problem that we try to solve and and um and what we aim to do so um uh weave it is a is a vector search engine and um a nice way to to to understand the concept is is based on the way that the data is represented so for example um this type of wine and if you use a traditional search engine and you would search for wine for for seafood it wouldn't match with something that would be good with fish because it wouldn't necessarily make that relation between seafood and fish um but what you can do actually with vector search engine and we'll we'll go through that in in the examples and and the vectorization how that works um you can actually find these these enters because the rather than doing a keyword match based on in this case wine or for seafood you do a similarity search so you search in a vector space for something that is related to wine for seafood so now you can find different data objects and you can relate a data object different and that is the it's at the core the the the the core use case for we've hate and what vpa tries to solve um you can of course already do that with with machine learning models but the power of review it sits in the fact that it's a database a data engine so if you would have thousands or hundreds of thousands of millions of wines for example in your database and you want to quickly search through them that's the reason why you would work with we've gate although we're also getting feedback from people saying that they just find the ease of use handy because it's just you know as you will see in a bit uh it's it that's easy to to to get up and running so the core idea came actually from uh from google from google search so all right so the question that we wanted to answer was like okay if we ask these kind types of questions on google search um in the form of an abstract question and the answer that is generating how was it able to find that answer um how uh how would you be able to determine your own data and how can you do this as fast you know as possible so this was more the the overarching goal that we had can we make something um uh can we make a search engine that people can use themselves on their own data regardless what if what you know what kind of data you have or what kind of project that you're that you're running but this was like the core idea can we can we build something similar um so as i just mentioned so how can you do that in an easy fast secure scalable way and what we mean with easy in this context is that we strongly believe in the in the ux element of these kinds of technologies because in the end if you are deep in in in these kinds of of technologies then then you probably understand a lot what's happening but we actually noticed that there are a lot of people that they need to build something they need to have a solution fast that needs to be easy for them to get up and running and it's easy um it needs to be easy for them to add data search for data and those kind of things um fast i guess uh speaks for itself and so that he um it's nice if you can um uh get to these kind of results uh but if it takes a few seconds to get to an answer that's very unfortunate so the other day somebody showed me a demo of something they built and they were looking to evade and they were able to solve the problem but it took up to five to six seconds to actually generate a response what we mean with secure in this context is that we've had is not a sas service i mean it exists as a sas service but it's also the packages just or the the software just available through an open source core so you can run it from wherever you want and i can assume that scalable speaks for itself um so we've hit as cloud native or as one of my colleagues alike always likes to say it's it's kubernetes kubernetes native so that basically means that the um what we need at the lowest level is kubernetes and that nowadays almost runs everywhere um maybe one of our most important usbs if you will in we've eight is the fact that it's modular that's something we're going to look at also today um so the version one release early this year and made wv8 not only uh completely stand alone but also modular and that means it becomes very easy for us to to add plug and playable modules to to alleviate so regardless of what these what these modules do right so they can for example they can vectorize data but they can also add functionality to we evade or those kind of things and it's very easy to integrate in the in the vc8 ecosystem um we have a focus on on real time or near real time if you will uh so my colleague chen i believe is also on call he wrote a nice article on towards data science about that that we are able to get to sub 50 milliseconds results and last but not least and we feed this is a vector search engine and what's important to say here is that um one of the goals that we have with vivid is not to only be able to store vectors and quickly search through vectors but also to store the data object itself so are our oh somebody wants to join now i have to click the accept link oh and they're gone okay sorry so our goal is um uh um not to only store the vectors and search for them but more also store the data object that somebody really gets the feeling that they're dealing with a with a a normal database like what they're used to but that they get all these additional features that we can uh do based on the on similarity searches and and classifications and as we like to say it's built to scale your machine learning models and what we mean with that is that if you um let's say you use a um well a transformer model for example that we're going to look at today um it's great to use it on a document or to get inside of documents or a handful of documents um however if you want to scale that to thousands or hundred thousands or even more documents that's difficult that's hard and as i like to say it's like nobody nowadays builds a database anymore right if you have an application i mean it's not you're not also going to build you know your own database you pick something off the shelf that you know that that fits your needs and we believe that we've yet will um well you know we'll serve in a new need and that is like or storing data that gets vectorized by machinery models or um searching through your own factors or if you want to do that however you want to want to structure them and especially thanks to the to the machine learning boom for those who don't know a lot of models came available to actually factorize content so this so we're now going to look at text but very soon you also see that we will start to release modules related to images and those kind of things that you can mix and and and and match them um so the two core features that we have and we've hit are as you like this so called the search and and classification those are on the highest level so search basically means that you search for something in the vector space and that what we also will go through in the in the demo and classification is actually the other way around so um weave it as a as a graph like data model so that means if you have multiple objects in your in your um your graph you can automatically make relations or ask with yet to automatically make relations between certain um nodes in your graph if you will or between data objects uh so we have the modules as i mentioned so we have nlp models modules transform modules you can also make custom modules um i will go through that in a bit the goal is that it's completely end to end so it is a complete solution for for any industry right it's like a like you would use a normal database you're agnostic about the about the use case that you have it's more the problem that you need to solve we have an open core so that means that the core of vp8 is is open source and the focus is on it being scalable and fast and there's more ux ux elements um one thing that i want to say about the um about the architecture but maybe i don't know hm are you still on the call yes yes you want to say you want to say a few words about this because this is your core um you know your day-to-day job so maybe you want to say a few words about this uh yeah sure so uh on the architecture so bob mentioned something in the beginning that um vvn is not just about storing and searching vectors but also about storing and searching the objects itself and this is something that is reflected in our architecture right in the in the core basically so we try to make sure that we store the data alongside the vector positions and what that means is that sort of if you scale um for example if you scale horizontally in a cluster that um sort of on a single request you will always get what you need sort of very nearby without having to make a lot of network requests for example so let's say you're searching for something like the um the 10 or latest 10 related wines and then you very quickly get 10 ids back but then you have to do 10 follow-up requests to resolve those ids to objects then these 10 follow-up requests could easily take more time than the search itself and there in our architecture we're making sure that sort of the vector index which is the core is stored alongside the object storage and there's also an inverted index so an inverted index essentially means for um yes sort of the the classic search case structured search case um which you can combine in v8 so for example if you have a query such as give me uh the wines related to or that that match well with fish which have less than 12.5 percent alcohol or something so this is something that's super structured and you could combine these and so if that's in the core of one chart um yeah then if we if we vectorize data this is a sort of a very simple um a very simple microservice pattern here so in this case vvat itself runs entirely on cpus so that's cheap commodity hardware um but these machine learning models typically require gpus so um in this case you can sort of split that out scale it independently and uh just have your your sort of cost intensive parts on uh on a separate machine okay can you click to the next one yeah one sec somebody's joining yep uh oh i don't i don't have a next one i only had this one in here okay okay cool cool i'm going to improvise the next one yes so um what you what you see right here is sort of the the current state of the architecture and what we're currently working on um is sort of turning deviate which right now is confined to a single node into a fully uh distributed database so right now um we can easily cover um sort of data sets of 1 to 10 million maybe even 100 million if you're okay with sort of yeah the machine running a bit slow here and there slow in relative terms so just meaning you can't get maximum performance and what we're working on right now is basically making this horizontally scalable and this is one of our key goals in the architecture and that was always from the beginning so if we we always knew that we wouldn't do any kind of patterns that wouldn't lead to or that wouldn't work with scalability so um yeah sort of half a year from now basically uh vba is done a fully distributed database and you can scale sort of for whatever needs you have so one of the typical examples would be high availability so you just don't want your machine to go down um because i don't know it's maybe serving a user-facing site so you need high high availability or maybe to get a higher throughput because you produce more data than a single vba machine could import in a day for example or all these kinds of kinds of um yeah scaling use cases that you can configure to your needs cool thank you very much for sharing so this is as i as i mentioned this is my my last slide because i i want to get to more a show rather than tell um a situation uh but of course i wanted to also give a little bit of of context for you all so um what i always like to do is that i like to start from the perspective of our documentation because the documentation is the hence the heart where you start to learn uh about we've gate and how you can how you can use it um the um uh one big thing that has changed with um uh with the new setup is actually the installation pattern so as as we as i mentioned earlier like the goal for us is to make it as simple as possible for you to start working with all these models and all these out-of-the-box setups and and what we use at the lowest level is or for the for the production environment we talk about kubernetes but if you develop you can just do that on with with docker so one new thing that we have uh is the um the customizer so if you just go to the installation you can customize your your weave yet setup so of course i'm gonna choose the latest version uh here you already see the media types so what's um you can set you can select none so if you only want to use vva to store vectors and search through vectors but if you want to use specific modules then to factorize you can do that as well what is interesting here is that you see your images coming soon we will have more coming soon but an interesting um thing that i already want to share with you is that in the future we'll be able to mix them so you will be able to say well this data object i want to factorize based on its text and this data object i want to vectorize based on its on the images for the image that it represents but you can still connect them in the same graph space if you will so i'm going to go for text for now we currently have two types so the first one that we started with was what we call the contextionary which is based on cloth and fast text and now we also have the f transformers available the um of course if we gonna add in the future we're gonna add um the images then you will get even more of course modules available um just interesting to know is that one of the things that we learned from using it in practice is that if you have more classification use cases so that you have data that you want we've had to classify for you then the contextionary works very well uh so the based on glove and fast text and if you want to do more with text or if you want to search through text then the the transformers work better so it depends a little bit on the use case that you have but this is also something that you can play around with so it's very easy to spin up one we've gate to the other and just you know play around with um oh somebody else is joining we go with um you can play around with these with these with these modules easily what's important to mention here as well you just have to make minor tweaks and i will talk about that in a bit to um uh to just shift between modules while you're developing and trying to figure out how to use we've hit the last but not least oh no two more steps so one of you you you pick the model uh that you want so i'm just gonna go for the first in this case like this one this is for enterprise users which we don't need now so it's disabled and then you select your runtime so you say like door compose and so we also have kubernetes and what this outputs is very nice one single curl command that contains very long url as you can see with exactly your um your configuration so if you would look just at that url you see the configuration that we just made is represented in this docker file so now just running docker compose up is enough to um to start working with the uh with the environment the next tiny tiny comment from my side sorry for the interruption uh just on the kubernetes part um the the part that's missing here is just uh kubernetes in the customizer um so vva itself already works with kubernetes just right now you would have to sort of manually uh adapt your your we have a helm chart and you can adapt the values yaml so you can use it on kubernetes already just not uh configure it with the customizer thank you um the next step um um the next thing that you need to do with we've hit that we did is a schema so so a graph like schema um so we get a lot of questions about schema so my colleague lara is also working on a few extra tutorials to understand um at the schema but actually the using the schema somebody sent something here let's see thanks well we'll look at that um we'll continue so the um the schema so lara is working on a few tutorials also to learn how to work with the schemas but the very simple concept is with the with schema is that you define the class property structure that we've yet has so um we spun up what we've had already in the cloud just to give you a demo so we've hit is completely api based and i can explain the schema actually from that perspective so the schema this is a very simple schema because it only contains one node uh we don't make any graph relations here because we wanna really focus on showing the the model in action but it does nicely show you how the the schema works so all the schemas have a have a class so they can have multiple classes so actually let me increase it a little bit and have multiple classes hence it's an array and then here you see for example the first class that is a paragraph a few things that i want to highlight here the first thing is that you have a module config so you need to explain which um module you want to use to vectorize the data and that has to do what i said in the past like in the future we will be you will be able to use different vectorizers for different um classes so now we have a class paragraph where we want to use the text to vect module but maybe in the future we also will have images that would be part of that paragraph and then you would say well i have the image to vec module then of course we have properties because a class has properties so in this case this class has two types of properties so one is the title one is the content and those are both both text objects and here you can see we can even give more in-depth configurations for the schema so in this case we're saying that we do not want to vectorize the property name for example something you're going to learn on the website that's all in the documentation but that you can see how that schema is is defined and if we now go to the graphql interface that we use to query data you will actually see this coming back so if i now say get and i would say paragraph so i start with the from the perspective of the class which always starts with a with a capitalized first character and then here you see i have title and format so these are paragraph title and content which i use during querying and these are defined here in the um in the schema um creating a schema updating a schema many ways how you can do that you can do that directly through through curl so yeah so you can just do api um calls but we have the python and we have python libraries javascript library go library and we have i believe java libraries in the making um so we will have more and more of these kinds of libraries available in the in the future so what i'm showing you today is completely through the api but you can make all the requests for all these libraries as well if you like um one thing that i want to do first is that i want to before we start searching um with we've yet i want to look at the data objects so one of the things that we can do in we've heard is that we can see like okay show me just dump all the all the um uh dump all the all the data that you have so here you see for example we just have objects as you will see in a bit we have about a million objects in here so this is just the first object that it encounters this object has an idea it has content and it has it as a title uh so what we can do is like we can take that idea and then go just one step deeper and we can retrieve that that note this is if you look at with it from a graph perspective this is one uh a note in your graph so it is a note of the class paragraph a paragraph it is a properties content and title then no references in here so it's not referring to any other notes this is just a very simple standalone note but what is interesting here is one of the things that we always say is that every data object in the vp is represented uh by by the um by vector so these these these nodes are if you will they're floating around in space and there's actually a way how we can see that so we can do uh additional and then is i'm doing this from my head not additional it is include i think oh include sorry includes yeah but it's a demo right so sometimes stuff goes wrong so it's not it's not it's included so i can say well include the vector here so now you see here so this paragraph about independence day is represented by this vector and we could do this for all the data objects that we have and so this also you can how you can envision this um these these um all these nodes are floating around in hyperspace and the coordinates where they float around are these represented in this in this vector so if you now go to the graphql interface so what we often say is um you use the restful api to or well what i often say is like use the restful api to add data and and the graphql api to retrieve data or if you use one of the client libraries then you're basically under the hood you're that's what you're using um but you can use the graphql um api to retrieve data so if i now say well this square is very simple query this base is like okay get me all the paragraphs and show me the title and the content so if i run the square it just shows me random paragraphs and their titles and their content issue and as you can see the first one here that returns is also independence day which it's kind that the query is kind of similar right to the one that i just showed for the restful endpoints oh let me not forget to also show you um how many uh that we're dealing with so what we have at the at the core is like we have we have uh three root functions so get that's one that that's the one that we'll be using we also have explore um that's something we're just gonna pause for later but we also have aggregate and aggregate is actually used to um uh you know to get insights about about your v8 so you can say for example meta and then i can say count so with this query basically says like okay aggregate all the paragraph information um and the meta information is that i want to see it's just that i want you to count all the objects so as you can see here this results in just under a million uh wikipedia paragraphs just so you know what the size is that we're dealing with during this demo so let me get rid of this again let's go back to the previous query um one of the things where it becomes interesting is of course is that we say like okay we now we want we want to leverage these factor representations we want to do something with them and so one of the things that we can do is that in um in this specific case and for this specific demo we use the transformer module and and what what the the transformer module is specifically good at is dealing with longer questions or those kind of things so how we do that in we fit is that we use the near text [Music] function we say we search for concepts and we can ask for example we can say verbs used in the french kitchen i believe if you have seen the video that i made i use the same i use the same query i think uh certainty so let's say oh let's not use the certainty yet let's search just start with limit first and let's say for example the first three results so what this query says is i do the exact same as you just did before but now vectorize the query herbs used in the french kitchen take that vector representation of that query and search to the vector space what kind of data objects are the most closely related to this query and show me the first three results so if i now run this query you see we get back titles from in this case french cuisine so we see here the herbs and seasonings and french regional cuisine etc now it's important to bear in mind here is that the lower the further down the answer is the further it's removed from our from our curry and there are two interesting things here to look at so one is the uh the order and and the the reason that um we either got to this order is actually something you can see so we can say we can add a certain p as an additional property so what it shows here is like what the so what it shows here is the um uh the distance from the um from the query to the to the result and as you can see here so this one french cuisine it's a it's close by because it's like you said to 88 and this set to 84 so it's it's further removed um from the from the query and here it's even 81 percent so that means that if i would increase this number so let me set it to five at that then the further i go down you will see that the further the lower the the percentage um drops now one of the things you can do with vivid as well is that if you want to use that in a for use case then you might say well i don't necessarily want to limit it to five results but i also want to set a a minimal certainty so you can say well i want to have a minimal certainty of let's say at 85. so i now run this query it basically says like show me all the results uh that are higher than 85 certainty and if there are more than five um results then you know then then limit those two to five so for now around this query you see that it's um the answer is returned in um uh there's just one answer return that you that you can use in your results and it still has that additional certainty here so that is in a in a in a nutshell with how that weave it works um uh one of the things that we're currently also working on with these modules is that we say like we kind of move closer to for example a question answering that it also goes into the content to try to find the answer in there as well and oh yeah the last thing that i forgot to mention that is very important that is of course that's the speed that we're dealing with so this server i'm i'm running this demo from from amsterdam this server is spun up in the in in the center of the us so the moment when i when i hit this um this search button uh the time that it took to send back the results based on the the million data objects first had to travel to the us we had to process the data and it had to send it back and based on our own benchmarks we see that these for this specific demo it takes about 100 milliseconds to travel to the us about 35 milliseconds to get you know to capture the results from the ev8 and to send it back so one of the things that you will notice that if you're building a solution um yourself and you run it locally or you're running um your queries on the same machine um as where you're working on then it will be even even faster um so that's an important thing to mention um here as well um let's see if there's something more that i want to say about this well the only thing that i want to say about this that's the um that we look very much forward to people starting to work with these with these with these uh vectorizer modules i'm also curious if people are interested in custom modules we are interested to learn if people this is a little bit more into the nitty-gritty but if people are interested in different uh index plugins we are interested in learning if people need different client libraries and we're interested in learning if people need different um crop operations for um working with the with your data so coincidentally somebody um created an issue today on our github asking for specific support so we just look forward for people trying it out work with it and um you know share with us their learnings and and what they think that we can do better or where we can where we can improve so that was what i wanted to show to you today uh everything that i've shown you is something you can run yourself so um let's quickly so are you still on the call uh hm yep do you know by heart what um uh bird model is being used for this um for this demo uh yeah in this case this is one of the sentence transformer models and the one uh that is based on the microsoft uh data set uh i think it's called ms marco and essentially this particular so they call the asymmetrics uh semantic search so that means that particular model is trained on matching short queries and question queries to potentially long answers so it was trained on i think being search queries just from from usage data knowing that i don't know users who ask like who is the ceo of google or something and then if they don't click on the first result but on the second result and that's like a usage metrics uh metric that tells the model um that this is a potentially a better result and it was trained on on these kind of um yeah these kind of data sets yeah thanks thanks for mentioning so so so um uh where i wanted to get inside so it's very easy for you to get this same setup up and running important thing to mention i'm aware that there are people on this uh on this school who are also interested in maybe working with their own transformer models uh what you can do there and i'm going here from the top of my head so if you go to the the modules the text effect modules you can go to the option custom built with a private or local model and there's explained how you can build your own private or local model the only difference there is that what you've seen in the configurator is a setup where we v8 um takes everything from docker hop just out of the box you literally only run docker compose up and and you're good to go so we've hit runs uh here you need to take that one extra step where you need to build that that module yourself with the with the um transformer module that you want to use also if you will be using that there's somebody turning again yeah so also if you um if you want if you're working with that we also would love to learn what your experience is if it's easy enough for you to use i think that here the the importance sits more in the um in the documentation rather than in the software and what i mean with that is that if the if you know if it's clear for you how to go uh how to make the changes uh from a documentation point of view um then it runs because using all our tests and and also for certain um customers we use these custom models so um and we know that it works but we also would love to get feedback from people just try it out directly from the perspective of the of the documentation so that leads me to also looking at the time because i've been speaking way too long so to also stop the uh stop the recording and ask ", "type": "Video", "name": "weaviate_meetup_april_2021__introducing_transformers_distilbert_bert_sentencebert", "path": "", "link": "https://www.youtube.com/watch?v=DCQWqMecdlA", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}