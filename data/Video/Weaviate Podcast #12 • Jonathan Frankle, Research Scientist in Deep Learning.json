{"text": "Please check out Composer from MosaicML! https://github.com/mosaicml/composer Jonathan Frankle is the Chief Scientist at ... \n[Music]thank you so much for checking out thewva podcast we've had a lot of greatguests in the wv8 podcast but i'm superexcited about our next guest we havejonathan frankel from mosaic ml jonathanis one of my favorite researchscientists and deep learning to followalong with since the debut of his paperalongside michael carbon his advisor atmit the lottery ticket hypothesis thispaper i won an iclr best paper and itwas just an incredible finding about theability to apply these sparse mass ontothese sub-networks and this kind of ideaof pruning and how you can kind ofachieve this training of the sparse subnetwork from scratch is theoreticallypossible and now jonathan has continuedthis research developing all sorts ofthings related to efficiency andefficient deep learning algorithmsexploring the benefits of things likemasking with pruning and all these kindsof things that jonathan will explain toyou better than i will in this quickintro but i'm so excited to learn moreabout jonathan's latest company uhmosaic ml and the product composer andfor our weev8 community this is superinteresting because you know they'remaking it more efficient faster cheaperto train models and then we get ourvector embeddings that we can searchthrough for semantic similarity and allsorts of things so jonathan thank you somuch for coming on the wba podcast anduh can you tell us about mosaic and mland maybe what i've watched in my quickintro no no this is great sothe story of mosaic ml goes back almosttwo years nowand really it started with an email fromour now ceo naveen rao to me and myadvisor saying hey nice lottery ticketpaper i want to do a start up based onthatum and my first reaction anybody'sreaction who knows the lottery ticketpaper well should be are you insaneinteresting science not useful um andyou know i'm willing to admit that youknow this work is not practically usefulit was meant to be an exploration intounderstanding deep learning butin order to use lottery ticketpractically you would need two things tobe truenumber one is you need some magic way tofind sparse networks early in trainingthe work is all about showing that thesesparse networks exist but it doesn'tshow an efficient way to find them andpeople have been working on this problemin the literature but in my mind therehasn't been a ton of progress to becompletely honest and i can say moreabout my thoughts on that in a momentthe other piece is that suppose youfound one of these sub-networksyou need to train efficiently when youhave a sparse network and you know forthose who aren't familiarjust because a network has fewerparameters doesn't mean it trains fasterthe network will hypothetically requirefewer compu fewer calculations fewerfloating point operations to be able totrain but that doesn't make it go fastbecause our hardware can't actually takeadvantage of the topology of the networkgpus and modern tpus and hardware likethat can't take advantage of sparsityvery effectively it's used to processingyou know contiguous matrices or tensorsor vectorsso i kind of looked at this email youknow getting back to the topic i lookedat this email from davina went is henuts like this is a pretty smart guyum but we got to talking you know myadvisor and i sat down and talked to himand you know hanlin tang his colleaguefromfrom his nirvana days also joined nowour cto um and we came to the followingconclusionthere's a lot of opportunity to improveneural network training by making thetraining algorithm more efficientand what i mean by algorithm is thefollowing because it's worth clarifyinghereso there are a lot of ways that you canmake training more efficient one is youcan build a faster chip like nvidia justreleased the h100 it's a lot faster thanthe a100 which is a lot faster than thev100 that makes training more efficientand cheaperum another thing you can do is writebetter compilers you can take the sameset of operations and run them moreefficiently you know this is these arethings like cu dnn and kernels and youknow triton and that sort of thingbut i think the interesting observationhere is all of this is taking the exactsame math the exact same way of trainingevery operation that you're doing andjust finding ways to run it faster youcan you know spin the gpu faster you canhave you know faster tensor cores youcan you know run pick the rightoperations to run it faster but it'sstill the same math you'll get exactlythe same weights in the network assumingyou know non-determinism doesn't kick inon the gpu or things like thatandthe lottery ticket work is interestingbecause it says actually we can changethe math we can get you a differentnetwork than you would have gottenotherwise but it's just as good andhypothetically in our fantasy land wherewe can accelerate sparsity it would havebeen more efficient to get there we canchange the algorithm to make it moreefficientand that's what mosaic is built on istrying to look at ways that change themath that fundamentally change thetraining algorithm to make things moreefficient and i'll give you two reasonswhy this you know makes a lot of senseto me intuitivelyone is that you know think about themath behind deep learningthere's nothing correct about this it'snot like we're doing the right thing andmosaic is doing the wrong thing but youknow making it workwe're using sgd which is a convexoptimization strategy in a non-convexlandscape if you want to get thatnetwork to train you need like all theright hyperparameters just the right wayif you set the learning rate a littlebit too high or low bad performance youneed batch norm or layer norm or pickyour norm you know youjust like we're living in this tiny tinyspace of networks that work wellit's minuscule it's like you know youyou step off of this tiny little islandof stability and you fall into lava andyour network doesn't work very well soyou know there's nothing correct aboutwhat we're doing so we shouldn't beafraid of changing the maththe other reason why i think i thinkthis makes sense islook at this tiny space of networks wedon't know how to describe this tinyspace theoretically i can't give you aproof or a statement or a set ofassumptions to describe itand i think of this kind of the analogyi like to use is biologyso imagine we have the laws of physicsthink of that as all the neural networksthat are out there we can describe themall think about biology in you know thereal world this is one emergent propertyof physics we are just physics this isone outcome of the laws of physics if igave you the laws of physics you wouldnever predict the biology here on earthpartially because it's one very smallpossibility in the range of a hugenumber of possibilitiespartially because it's so complexthat from first principles it would behard to predicti look at the neural networks we trainin practice kind of metaphorically likethis biology it's these are highlycomplex systems that have emerged fromvery simple dynamics like physics butit's one specific thing that might haveemerged and we can imagine all sorts ofpossibilities in theory we have tocontend with all these but in practicewe only deal with a tiny portion of themand when we study biology umyou know wewe don't go down to the physicsimmediately we look at motifs andsystems and behaviors and mechanisms andall these higher level abstractionsthings that you know we wouldn'tnecessarily think of from firstprinciples in physics and so i think ofthis in the same way as deep learning idon't care what might happen i don'tcare what you can prove i care aboutwhat is happening and how to describe iteven if the descriptions aren't perfecti don't need a proof i just need goodempiricismand then you can think of what we'redoing is almost like pharmaceuticalswe're designing some kind of chemical orsome kind of intervention that takesadvantage of processes we understand toget a different behavior out of thesystemand so we design a lot of theseinterventions the lottery ticket is oneof those your body chemistry is nolonger functioning the way it was beforebut ideally you'll get to a similarplace or you'll be you know just ashealthy as you were before but you knowideally we can make that outcome happenin some desirable wayand you know there i'll continue thismetaphor for one more moment one of thebig challenges is not just you know okaydesigning one pharmaceutical you need alot of pharmaceuticals to treatdifferent conditions to addressdifferent aspectsand you know lottery ticket you canthink of as one pharmaceutical at mosaicwe have you know i think 25 and countingright now and that encountering isbecause we've got a lot more in thepipeline that you'll expect shortly umand the last thing i'll say here is if ihad to do 25 pharmaceuticals youwouldn't take them all you wouldn't justpop all the pills into your mouth andsay ah i will be better nowthese drugs can interact in reallydangerous waysand so a lot of the magic of our work istrying to understand the science of howdo these things interact what's going onunder the hood in these networks thatcauses things to interact negativelysome of this is systems optimizationsome of this isyou know just two methods seem to overregularize or what have you but there'sa science here that we're starting touncover and that helps us build recipesso i'll stop there i'm rambling butreally mosaic improving trainingalgorithmically i think this is a newlayer of the stackand i think this composes really nicelywith what lots of other folks are doingat other layers of the stack hardwarecompilers all the way up to good youknow ux and mlabs so you know that'swhere we arewow this is absolutely fascinating andfirstly uh to preface this i hope thatthat story of the email ends up in thesestartup books one day i get that ideathatyou get an email and that's the start ofmosaic ml that should be one of thosebooks but um yeah i'd love to the 25 andcounting the composer the interactioneffects of things like uhuh ghost batch normalization stochasticdepth and maybe the og mix ran dataaugmentation strategy sobefore coming back later on i think i'dlove to dive more into the design ofcomposer and it's kind of api and how itintegrates into most of the existingworkflows of people working with deeplearning but i think it would be greatto transition right now into some of themethods of composer and you mentionedthe interaction effects of these methodsand the new science is just absolutelymind-blowing such a fascinating thingbut um so could we kind of come throughthis list a little bit and maybe couldwe start off with umso i kind of as i went through the earlydocumentation on method on themethodology and the composer i kind ofchunked it up into categories of dataaugmentation normalization what i callmodel augmentation and then labelaugmentation and optimization so maybelet me start off with model augmentationin which i groupedthings likeswarunning average of the model weightsstochastic depth replaces specific layerwith a stochastic version that randomlydrops a layer of samples during trainingcan you tell me more about these kind ofthings like layer freezing uh squeezeexcite and how these kind of things cometogetherdefinitely sothat i think you've actually done areally good job chunking these off i'mgonna have to steal this because we'rewe're struggling with this right nowthis is one of the challenges ofbuilding the science is kind of you knowi look at something like let's startwith stochastic depth because i thinkthat's an interesting one so you canthink of stochastic depth as kind of afancy version of dropout what it does isin a residual network you can skip alayer and the network still works youcan just kind of chop it out but youhave those residual connections that gothat route around it soin a normal feed-forward network if youcut out a layer like the network'sdisconnected and it makes no sense in aresidual network you can drop layers andso the idea in stochastic depth is youcan do this two ways one is that youjust for your mini batch you drop someof the layers and you just skip them onevery forward pass the other is that youdo this example-wise soeach example kind of takes a differentjourney through the network and on eachlayer you only run through some of theexamplesso stochastic depth has a complicatedstory on the one hand it shouldhypothetically make things moreefficient because you're not using thewhole network on any given passso this should be you know beneficial tospeedyou might think of it as a model changein that way but it's a regularizationtechnique as wellso the reason why i'm picking on thisone is it kind of some of these methodsreally defy categorization or fall intomore than one category which is whatmakes the science so interesting andalso so difficult and stochastic depthhas an interesting story so there aretwo things i'll tell you about this oneis that you know the paper that came outwith it makes a very compelling casethat you know this leads to speed upbut we actually tried it using the samemethods from the paper and we didn't seeany speed up in fact we saw worseperformancewhat it turns out happened was the paperhad some pretty terrible bass linesyou know they trained a c410 network forhundreds of epics usually you know 160epics is enough i think they weretraining for 500 epics in the context ofthat 500 epics the regularization reallydoes help you and you can get betteraccuracy and therefore the appearance offaster training time but you know whenyou look at normal trainingregimes this doesn't actually help atall and in fact we found it to behurtful so this is a good chance for meto take a step back and ask what does itmean for a method to be good what doesit mean to have a positive interventionand so here's the way we look at it atmosaic you know there are a lot of gamesyou can play in the research literatureto make something look goodone is that you can you know just have abad bass lineyou can like train your bass line fortoo long way past the point where it'snot helping your accuracy and then showoh i made things fasterby you know showing that it doesn't takeas many iterations but the baselinedidn't eitheryou knowanother technique is that you can gothrough and say wellhypothetically i reduced the number ofsteps it took for you to reach a certainaccuracywhat i'm not going to tell you is eachstep takes five times as long and i'mpointing at the sam paper here thesharpness aware minimization where thisis a huge problem each step takes twiceas longand you can train in fewer steps but itslows things down and we had to mitigatethisin the case of stochastic depth thereare two issues here one is the baselinesweren't good in the paper um the otheris thatjust because you can drop examplesdoesn't mean you can get speed up fromthat going back to that sparsity examplewith lottery ticket you know if you dropthe examples on each layer you've gotthis weird shape tensor you have toreshape the tensor run it through thelayer and then reshape it back and thenended up slowing things down to thepoint where the method didn't actuallyhelp from a speed-up perspectivelandon uh who were our researchscientists who worked really hard to getthis method up and running tried hisdarndest to find a way to get some speedout of that and struggledbut it also has a positiveregularization effect so especially forlonger training runs it is stillbeneficial so these two failure modeshere one is you know bad bass lines andwe've seen this a lot the other is thesehypothetical fantasy land speed ups likelottery ticket is a hypothetical fantasyland speed up at mosaic we care abouttime and cost those are very unforgivingmetricsum you know well clock time doesn't careabout flops and it doesn't care aboutnumber of steps it just cares how longit takes to get to a certain accuracyand so stochastic depth does helpbut you know i hope there are twoteachable moments there for people whoare thinking about this spaceso if i can unpack just uh a quick thingabout the short and long training timesay 160 epochs to uh to maybe test a newalgorithm compared to maybeuh you're training the new gbtx on yourdata set and you basically want to runit for as long as you can right like howdo you think about these two resumes ofshort and long training timesso the way that i think about this is interms of pareto curvesthe the entire view of the world atmosaic is about pareto curves so i don'tthere's no right answer to the amount oftraining time really it comes down towhat is your budget and what do you careabout and maybe you don't have enoughmoney to train gpt3 forevermaybe you know you domaybe you're a big tech company andyou've got a bunch of new datayou need to get a model deployed for adrecommendation as quickly as possible soyou don't have time to train it forweekseven if you have all the gpus in theworld and sothe way that we think about this is wetrain the model for all differentamounts of timefor any given model our baseline is notone baseline we take our baseline and wetrain it for 10 fps and 20 epics and 30epics or we actually double you knoweach time but we say we don't know theright amount of time to train so we willcreate a pareto curve where we look atthe trade-off betweennumber of seconds it took or dollars ittook versus accuracy if you train forlonger generally you get higher accuracythat's not true in general you know youmay eventually start overfitting orsomething like that but by and largelet's just take that assumption for themoment so you get this beautiful curveand a speedup method is valuableif it moves that curve up into the leftso if it moves that curve you get abetter trade-off at all these points orat least at many of these differentnumber of training steps and you know soyou can think about a couple scenarioslet me give you a method that takes theexact same amount of time to train butgets you higher accuracy like you know aregularization method like mix up whichtakes two examples and interpolatesbetween them and then runs that throughthe networkmix-up doesn't really take anyadditional time but it gives you betteraccuracyon the one hand that means you know forthe same amount of time or the samebudget you were putting in you getbetter accuracy on the other hand youcould just train for less time or fewersteps and get the same accuracy you hadbefore accuracy can be traded off forspeed these two quantities areinterchangeable that's really the keyidea that a pareto curve shows youand so things that regularize andimprove accuracy are speed ups for themost part some of them like stochasticdepth actually you know hurt you onshort runs and benefit you on long runsso there is you know there's a littlemore nuance to it there but you know byand large we think about this in termsof pareto curse and that means that badbaseline is just a point that's way farout on the right but it's way past thepoint of diminishing returns and so youknow it's kind of a silly point thatisn't even it may not even be on thepareto frontier as far as we'reconcerned because it may be that you cantrain for a lot less time and getexactly the same accuracy which meansit's not on the pareto frontier and sowe would just ignore that point as youknow non-relevantthat's such an interesting way ofthinking about it and yeah i saw a lotof fredo curves as i was looking aroundthe mosaic website and getting caught upand so uh so yeah i love that way ofthinking about how to think about thecontribution of these methods but if wekind of step a little uh back into modelaugmentation and um you mentionedstochastic depth works for resnets butif you have a just a sequential networkwhere you know say node a goes to b goesthis c goes to d and you can't drop apath because then what are you going todo so it's kind of specific to thearchitectures and then so another thingis um you've implemented a squeezeexcite and i love squeeze netall that research the squeeze bert whenthey generalized it from computer visionto nlp as well and uh there's also uhalibi replace attention with alibi andthat those sparse transform all thatattention stuff obviously yep soexciting so how does umand before we really get into composerand the interface and how to use it howdoes that um how do those layersintegrate with the the way that you'vedefined your model codeso the way that they integrate is we dothis thing called model surgery which isa module that we've written it's part ofour library which basically allows youto do find and replace within a modelgraphand this is really important for thesekinds of model modifications i want youto be able to start with standard code iwant you to start with torch vision orstart with you know hugging face andthen you can use our methods to replacethe modules that you want in huggingface you can replace the attentionblocks with alibi attention which youknowdoesn't you know doesn't cost youanything in the speed side and gets youmuch better you know short to longsequence generalization which means youcan train on shorter sequences which isgreat for efficiency or squeeze excitewhich is kind of this interesting animalit gets you much better performance andresonance but it it makes the modelbigger so it can lead to memory issuesand it can and it slows down inferenceso it's actually a trade-off there interms of squeeze excite we don't includesqueeze excited on some of our topbenchmarks because the one golden rulei've had at mosaic so far is don't makeinference worse i don't want people tohave to say well you made training moreefficient but i have to trade offinferencei want people to be able to say you knowmosaic is just strictly better usingcomposers strictly better these methodsdon't hurt me they only help me no sideeffects or at least no side effects andinference so squeeze excite is greatit's kind of a basic form of attentionbetween channels of a convolutionalnetwork that's the way that i thinkabout thisand so i you know those kinds of methodsare tricky because if you modify thenetwork you're getting into this trickyterritory where you may change the costof inference blur pool another examplewhich which uses anti-aliasing on downsampling same idea thankfully slow downfor blur pool is very smallfor squeeze excite there's anappreciable slowdown but it's so helpfultraining becomes faster inference that'sa trade-offone other thing i'm curious about is umsay you're trying to maximize how muchmodel slash gradient you can fit on onegpu is that uh say is gradientcheckpointing the best way of doing thatright nowum yeah i think so so we have deep speedintegration so we can use deep speedzero which does great in offloading umso it'll actually you know or activationoffloading so it'll send activations offof the gpu onto the cpu and then pullthem back in so that you can basicallycram an arbitrarily large model intoyour gpu memory if you'd like to um sowe we have integration with deep speed ithink zero one and zero two um so youknow you can go ahead and use thosetoday our friends at deep speed havebeen really helpful to us again this isthe nice thing about being a new layerin the stack you can kind of collaboratewith everyone i could pretend i'm stillan academic because you know i like tothink we don't really have anycompetitors just friendsso you know deep speed is available andyou can do this you know it does costyou something but that's something canbe worth it if you're able to cram abigger model on thereyeah so also on the friends topici really love when i was looking throughthe composer examples the integrationwith hugging face and how hugging facehas done so much abstraction on themodel code layer that it's like as muchas auto model for language modeling automodel for sequence classification is howeasily you can interface thesepre-trained weights and all thecomplexity behind the model definitioncode is completely abstracted away andyou just point at the task i can tellyou a little more about the integrationbetween composer and then hugging facesit's just like a tren does the modelsurgery super interesting and and thenit just has like a new training layer toit yeah so what we're really doing isyou can bring in your model pick yourfavorite model and just bring it inbring it into composer and you can useit with our methods in our trainer infact there kind of there are two levelsto this one is that for all of ourmethods or for most of our methods atleast we have what we call thefunctional api which is where you canuse these methods in any trainer youwant in any training loop you want totake these supply torch lightning goahead um now there's a reason we builtcomposerwe built composer because it's reallyhard to deal with some of these methodsand get them to intervene at the righttimes you need a lot of if statements inyour training loop if you want to dothis right and we there wasn't really aframework out there that provided thefaculties we needed by inventing a newlevel of the stackyou know we neededhooks and tools that nobody had reallyconceptualized before because you knownobody had thought about this use casebeforeso we needed to design a trainer forthat reason and you know we started offin pi torch lighting but it reallydidn't have the capabilities we neededto actually integrate all these methodsand you know getting back to thatcomposition of methods question thereason why we call it composergetting all the methods to interleaveproperly and run at the right times ifyou want to use more than like twomethodsisincredibly difficult if you're doing itin your own code so composer takes careof all that work for you it you knoweach method can have we have events thathappen at different points of thetraining loop each method you know canget triggered by events at differenttimes it can modify the state of thetraining loop we make sure all themethods in or leave in the right orderif you know there's one operation whereall the methods go in you know webasically have a stack you know thefirst method is the last method outbecause you know methods need to cleanup when they're doneand those sorts of details are reallyreally hard to replicate in a place likepytorch lightning umi don't say we took a lot of inspirationfrom our friends at fastai we really owethem an enormous debt of gratitude notonly for the inspiration on a lot of theideas that we're using but also on theinspiration for the trainer where we usetwo-way callbacks where the callbackscan modify the training state so ourtraining loop is very clean and then youwrite these callback functions that canyou knowthat can take root at any time duringthe training process based on theseevents and can modify the training statethat allows you to inject arbitrary codeinto your training loop without gettingthis wildly complicated training loopfull of if statementsso again thank you to our friends atfast ai we really owe them in thatrespect but you can bring your own modelto composer if you want toand you know hugging face has a bunch offantastic models we're not going to redothat work that's you know great workthat our friends at hugging face did andyou know people already use those modelswe use those models sowe had integration for that you know umour friend ross whiteman who producesthe tim library of computer visionmodels we have integration there as wellso you can bring in your model from timand use that we're working onintegration with several other modelzoos that are out there because quitefrankly we don't want to build thesemodels from scratch that's not our corecompetency that's not what we should bedoing anyway andeven if we did you'll want to use themodel for that's industrial strengthalready you wouldn't want to use ourcomposer version soyou know we have them there if you wantto use them we have some basic models webuilt in or bring your own and you knowour method should be compatible and ifthey're not it's pretty easy to modifythe model surgery to fit the new modelthat you brought in if you know thelayers have different names or what haveyouit that's such a fascinating engineeringdesign and the two-way callbacks the wayit lets you interface with thesedifferent things one other thing i'mstill kind of trying to piece togetherwith my own knowledge is how the dataaugmentation flows with it how you havesay the data loader and then becauseusually i kind of think of it as likeimport your data set define how you'regoing to be sampling batches from itdefine your model code and then theoptimization logic so how do you kind oftie together that optimization logicwith the data loading partso the you know we use the the standardpi torch data loader right now um andwe're workingone of actually the big things thatwe've been working on a lot of streamingdata loading because at the end of theday you know you don't want to have tokeep your data locally especially forbig data sets and so we want to makesure that experience is seamless we'reusing the web data set project um andwe'll be moving to the new pi torch dataloader that's coming out soon um butwith respect to the data augmentationsthis is a part of the pipeline i'm lessfamiliar with but my understanding is wehave you know it's just another eventis that you knowon the data loading we can you know runan augmentation at that point justanother place in the pipeline tointervene so it's just another placewhere callback can show up and modifythe datacan you tell me a little bit more abouthow you think about which operationshappen on the cpu and which operationshappen on the gpu becausethis is a messy messy question sothis is this is a less of a problem fornlp where the data is smaller and youdon't really augment the data typicallythis is actually a pretty big problemfor computer vision where we do a lot ofdata augmentation especially in you knowthese mod these modern self-supervisedmodels where we actually do a crazyamount of data augmentation much morethan we would do in a standard you knowsupervised vision settingum the problem with data augmentation isyou gotta run it somewhere and it takescompute to run these imagetransformationsyou have two choices you can either runthem on the cpu using you know standardlibraries you can run them on the gpuusing nvidia dollyif you run them on the gpu you're givingup some of your gpu throughput and youknow in our experiments you gave up 10or 15 of your a100 to do dataaugmentation there shouldn't end upbeing worth it for us on the other handif you run it on the cpu you can end upbottlenecked on the cpuit can take you so long to do the dataaugmentation on the cpu that you'reactually not using your gpu fullyyour gpu is way more expensive than yourcpu so you never want to be in thatsituation and what we found is thatthere are a lot of really interestingpapers out there that use you know thesekinds of crazy data augmentation methodsum and advertise like you know you cantrain in fewer steps yes but you'rebottlenecked on cpueven you know the a100s on aws are veryclose to cpu bottleneck on rest at 50 onimagenet as it stands because the a100sare so fast and quite frankly amazonselected a pretty crappy cpu to go withthem all things considered on our owninternal clusteryou know whichyou know stay tuned on whether otherpeople might be able to access that atsome point soon we picked cpusspecifically for this purpose to makesure that they were powerful enough tokeep up with the augmentation so theproblem here is that you know our bestnumber resnet 101 at about 3.5 x speedup on those awsa 100s that is dataloader bottleneckedthat is actually bottlenecked on cpudata augmentation we could go fasterwe've broken aws'sassumptions about the nature of theinstance they should build in theworkload by speeding things up so muchon the gputhat now the cpu has become thebottleneck and you know now we'reworking with our friends at mit whowrote the ffcv library to get thatintegrated because that's a way ofbasically doing a lot lower leveloperations and the cpu to speed thingsup um and you know we'll have thatintegrated i think it's actually alreadyintegrated on cfr10 and should be up onimagenet soon which should alleviatethat bottleneck and you'll see all ournumbers on imagenet go up a bunch moreby virtue of that so it's you really gotto think whole system on this stuffthat's so interesting and i see that asa lot of these methods i think they haveto do some kind of offline stylecomputation like maybe it's anexponential moving average of theweights and it has that kind ofbackground computation of doing theexponential moving average or thestochastic weight averaging similarly ithink the um the sam sharpness awareminimize i think it also has some kindofoptimization computation it doeswhat are you oh sorry oh yeah yeah forsam you have to basically do thebackward propagation step twiceum which you know can get prettyexpensiveso i'm also really curious if you if youthink about these like pretty kind ofambitious meta-learning loops where youhave sort of like a teacher student youhave like a teacher inference so maybethings like uhyou know meta pseudo or even maybe kindof coming more into the things that arealready in composer we have og mix whereaugmix has what augments is for peoplewho don't know is you take your originalimage you form three augmented versionsof it by randomly sampling threeconfigurations from rand augment thenyou do a pixel-wise averaging of that toget the final augmented image and thethe next extension they did was augmaxwhere they add an adversarial controllerinto the weighting of thosepixel wise averages so you're startingto add more kind of optimization inthese little things how do you see thatkind of idea of adding optimization notjust in the gradients of the classifierbut also in saythose kind ofmeta layers of these kind of algorithmsplaying into the composer design as welli think it starts to necessitate adifferentcomputational architecture at that pointbecause the augmentation stuff hasgotten so expensiveso we don't use augmeix we have itimplemented augmix we have found noscenario where we can get around thatcpu bottleneck for aquavix you have todo nine image augmentationsyour cpu bottlenecked across the boardrandogman we found works decently if youhave you know if you have a good enoughcpu to gpu ratio that can either meanyou slow things down on the gpu you usemethods that you know make things slowerbut reduce the number of steps oryou know again it's a balancing act wegot to make sure that you know our gpuand cpu throughput are balanced or youjust use a bigger model which you knowtakes longer to train or you have abetter cpu like we have in our owninternal cluster that you know thatallows you to use rand augmentproductively another interestingscenario where this starts to helpif you look at the aws t4s not the youknow not the 100s but the t4s they'repretty wimpy gpus they're built forinference so they're about a third asfast as a v100 but aws way overprovisioned the cpusso actually on a t4 you can use augmixand randognet just fineso in fact in scenarios where you wantto do a ton of augmentation and youdon't really care about how long the jobtakes just cost the t4s may actually bea much better bet than the v100s ora100sbut you know augmix we haven't reallyfound any other scenarios where it makessense randogman it makes sense incertain scenarios especially for longtraining runsbut by and large we you know matthewlevitt who implemented these methods forus you know i think was prettyfrustrated at the end of the day thatyou know they didn't make sense simplybecause they were slowing down each stepand they were taking you know thebottleneck was no longer on our mostexpensive component on our gpu it was onthe cpu so you know great methods veryuseful but when i look at some of theseself-supervised learning papers and isee the amount of augmentation they'redoing i kind of take a step back and goi wonder if they're fully utilizingtheir gpuum but in terms of how to do this goingforward or you start to get into theseadversarial scenarios you need anaccelerator just to do your augmentationlike and i think nvidia i haven't lookedtoo much of the h100 set up if i recallcorrectly they have kind of thiscoprocessor h100 that does networkingdata loading and preparation to send itto the real a100sor the real h100s so you know i thinknvidia's got in the memo that havingthese co-processors is necessary youneed a gpu just to prepare your workloadfor your gpu then you get into questionsof you know is that the best use of yourresources or should you just drop it alltogether um you know that's a complexsystems optimization problem butthis really changes the architecture andi will say that makes a lot of sense forvision i think it makes a lot less sensefor nlp where data is just smaller interms of number of bytes and we don't dodata augmentation so this does beg thequestion do you need two differenthardware setups for vision and for nlpwhich complicates the hell out ofeverythingyeah and i think maybe in nlp too you umyou don't really like repeat throughyour training data as much like ifyou're going through uh yeah like webtext you probably will never even makeit through the whole data set with mostof the training so it's like whyaugmented to begin with and maybe justquickly i wanted to see what you thinkabout maybe like offline dataaugmentation where you kind ofstart storing so it's not stochasticanymore it's deterministic and you storetheaugmented data set there's a papercalled the deep bootstrap frameworkwhere they sample from a ddpm generativemodel and they uh they store the dataset it's cfar five million it'savailable on github so you can you knowyou have this offlineuh in that case it's sampled from agenerative model but you can imagineapplying rand augment and then storingit do you think that would bea useful direction going forwardi think the data sets end up getting toobig you know for imagenet you'rerepeating through the data set let's say90 times conservatively that used to bethe standard back in 2015. in the pastcouple years i've seen this creep upwardto 180 and then to 300 then to 600 timesthrough the data set umimagenet is already uncompressedprobably about you know 400 gigabytesi'm guessingnow you have to do that 100 times overand you're talking you know what is that40 terabytes of dataum so then you get into questions of isit worth the storage costs and is itworth the network bandwidth costs tostream this data you know over yourcluster that can become its own issueespecially if you're streaming the datafrom you know one cloud region toanother from one part of the cloud toanother usually for data sets likeimagenet you can cache them locally orat least they automatically get cachedin memory if you have about a terabyteof memoryyou you obviously can't do that for adata set of this size so i think thiscan work for cfaronce you get beyond cfar i don't thinkit makes a ton of sense and really atthe end of the day you start thinkingyou know in terms of storage costs it'scheaper to have a co-processor that willdo augmentation or to just buy somereally big cpus than it is to pay forthat extra storageso one more idea i want to run by youbefore coming back to the efficientmachine learning is what if maybe westored the vector representations ofimagenet so we had something like somekind of api offering that producesvectors and then we saw those vectorsand then we starthaving complex networks from vectorrepresentations or maybe it could belike the vq vae where it's still like amatrix but it's like you know how theymap it into the discrete codes butthere's some kind of compressedrepresentation and maybe we could havestorage and things like a vectordatabasethat would store those things no i thinkthis is a this is a great idea and insome sense that's almost where we are asa community you can think of simclearand moco or you know whatever the latestgreatest method is you can tell them ayear out of date on this topic umyou know that is what we're gettingessentially is you know if we were tojust train a network on top of thosevector representations you know that'swhat you do when you fine tune sim clearmoco assuming you're just fine-tuningthe head and not the actual network ifyou fine-tune the network obviously it'smore complicated um one of the biggestchallenges there is again augmentationwise yeah you've got to actually performall this augmentation at some point buti guess to your point if we have asmaller representation of the image thenwe can do all that augmentation offlineand i've also always wondered i'm suresomeone's written a paper about this canyou find you know ways of basicallydoing an augmentation to a vector thatthat is equivalent to an augmentationyou knowof the image itself and you know so thatyou don't even have to store the imageso yeah i think this is a yet anotherplace where these vector stylerepresentations are incredibly valuableso you know i'm all ears on that thebiggest challenge is still you've got toget that simclear moco style networkand what i'm finding and chatting with alot of folks in the community isthey want to train their own network ontheir data because they have a hugeamount of dataself-supervised learning unlocks thisgigantic amount of data that mostcompanies and just many individuals havefloating around and everybody nobodywants to use gpt they all want their owngpt nobody wants to use a pre-trainedsim clear moco they all want their ownand you know people often ask meif you know we only do fuchsia or zeroshot learning what's the point oftraining you know what's the point ofmosaicif people are using you know thesevector representations and that's itum my response is you know so fari have seen no evidence that people aretraining less um the h100 would not becoming out if people were training lessso as far as i'm concerned as long asthere's training there's mosaic andpeople seem hungry to have their ownbert model pre-trained on their own taskor you know fine-tuned quote-unquote ontheir own data but they have more datathan the burt was pre-trained on tobegin with anyway sorumors of the death of training havebeen greatly exaggerated i'll say thatmuchyeah i think that's one of the mostfascinating topics in our current stageof deep learning is this can we just useprompting where we just have these maybediscrete prompts or predefined templatesto get large language models to dowhatever we want train on infinite dataand but even even then you would havelike a continuous prompt tuning usuallywhere you put the prompt into a latentspace and you still want to putgradients through it and those papersseem to be the ones that are moresuccessful anyways and so kind of andthen quickly before coming back toprompting but and then the idea of yeahlike one can one model produce in vectorsearch when we talk about embedding apisit's like can one embedding api produceembeddings for every kind of applicationwhether it's e-commerce biomedicalliterature mining you know news reviewsand it's like there's a paper calleddon't stop pre-training which shows thatyou know that's not really the casefine-tuning on the domain it's going tobe useful so coming back to promptingand this idea of still putting gradientsthrough continuous things that you putin the middle of the networks uh what doyou think about and there's papers likevnl adapter where they only tune fourpercent of clip they don't fine tune thewhole architecture what do you thinkabout that kind of masking in thearchitectures to you know put theselittle layersthat you updatei think this is a great idea i've seenthis you know i wrote this paper ontraining only batch normwhere i started with the randomlyinitialized network um and thought itwas cool that you could still get goodperformance just by training the batchnorm parameters which you know forreference don't even they're not reallyreal parameters they're kind of scalingfactors and bias factors for randomfeatures in the network i thought it wascool that you can train even okay withthis let alone train somewhat decently iwon't claim you can train well that's abridge too far for that paperbut you know in writing that paper idiscovered this is an old idea mostpeople aren't starting with randomlyinitialized networks but there's thisgreat paper called k for the price ofone that shows okay you just you knowcreate new sets of batch stormparameters for each downstream task andkeep all the other weights fixed so ithink this is a pretty profound ideaand it's a nice spin on the idea thatwe've always had of transfer learningwhere you just replace the head of thenetwork and keep everything else itlooks like the right answer is actuallyyou know having trainable featuresscattered throughout the network is theway to goso you know i think this is a deepprofound idea the biggest challengebeing as soon as you scatter theseparameters it's not as efficient as justtraining the head because you have toyou know forward prop and back propthrough all these other layers in thenetwork there's a cost to doing thisbut you know given how much better itworks i'm sold you know i think this isa fantastic idea it's i won't givecredit to any one person for this ideabecause you know i'm sure this idea goesback into the 80s and before if wereally were to dig and find evidence ofit just as all good ideas in deeplearning do but it's an idea that seemsto keep coming back and there's a reasonit keeps coming backyeah that paper training bash normanonly batch norm that just was like howcan that work how it like similar tothings like uh layer sharing i can'tbelieve layer sharing works either thesethings that kind of defy sort of like mycore like principles of how these kindof things work and so yeah mentioningthat um you still have to kind of trainthis whole thing and so is that kind ofstill was bottlenecking bringing all theall the way of the spar sub networkmasking back to life kind ofnot really i think this is i'm guessingif you were to sit down with a couplefolks from industry and i have noinsider knowledge on thisi'd guess that you'd hear that if folkshave kind of a multitask setting they'reusing this especially i'm guessing onmobile devices things like this are inuseit's a great way toyou know to efficiently store manydifferent networksin a small amount of storage soi would i would hypothesize to you thatthis may be running on your phone oryour computer right now and you justdon't know itand i don't know it right now so it's agood enough idea i'm sure it's inpractical usedo you like the vowels of googlesomewhere who's doing itdo you like these like mixture ofexperts models do those kind of thingsexcite youthey do excite me i think they excite mefor a couple reasons one is that youknow it's a way to train bigger modelswithout increasing the cost of inferenceyou're kind of you know you get moreparameters for free in some sense it'snot quite free you have to manage allthese experts and you have to use theright ones at the right times butassuming you know ideal costs and notthinking too much about how do you loadthe right experts at the right times andall that good stuffit's basically keeping the cost ofinference fixed as you increase theparameter count that's that's a hugebrilliant ideaand the other piece is you know it'ssparsity i like sparsity and you knowthat's what i'm known for sparsity showsup in all different forms of neuralnetworks and as i mentioned weightsparsity which i've studied probably youknow in i would say it's the mostinteresting part scientifically but oneof the least useful parts of sparsitymixture of experts is really interestingto me scientifically but moreimportantly it's incredibly usefulpractically so i'm a believer i thinkthe biggest challenge with mixture ofexperts is there's some threshold youcross where you need a sufficientlylarge network before mixture of expertstarts to make senseumand the question we should ask ourselvesis is that threshold going to increaseover time or decrease over time will weget better at training non-mixture ofexpert models to the point where youknowreally you know mixture of expertsaren't useful until 100 billionparameters aren't useful until atrillion parameters um or actually amixture of experts going to scale downto the point where they start beinguseful all the way down i think this isa big question for us as a fieldi'll point to one trend as context forthis i look at vision transformers thisis something that you know i still trainresnets i'm still a believer inconvolutional networks visiontransformers are popular and i thinkthey're really excitingum there have been a bunch of papers inthe past few months showing thatactually if we just give all theresonance the same tricks we gavedivision transformerswe can you know resonance work to largerand larger scales than we would expectotherwise and do better and better umthe threshold where it makes sense toswitch from a resnet to a visiontransformer is getting higherum and you know if that trend continuesi kind of wonderwill vision transformers always be nicheto some extent because we'll keepgetting better and better at trainingour cnnsi'm not entirely sure butyou know it's always a question to watchwhere do those thresholds go do they goup in which case cnns may be the futuredo they go down which case visiontransformers may take over same thingfor mixture of experts does thatthreshold go down and do we you know dowe have a small mixture of experts thatare more useful than the equivalentsmall models or do we get better andbetter about training those you knowquote unquote small models as in smallerthan 10 billion parameters to the pointwhere a mixture of experts you know onlymakes sense at the absolute largestscalesyou know open question but always worthkeeping those trends in mindyeah that's fascinating the idea ofbringing the mixture of experts down iyeah i mean it kind of it kind of seemslike the like the spar sub network'sidea it's very similar to it right inkind of the purest form of itand um so i kind of did want to ask youa little more about attention andconvolution just quickly um i've reallybeen liking attention for say uh likefeature like latent space fusion andparticularly with like multi-modal imagetext models where you you kind ofcompress it into bottleneck all the waysuch that you have a vectorrepresentation of the image you have avector representation for the text andobviously we like to search through thevectors with our dot product but thenwhen you want to fuse that you can havethat attentiondo you like that kind of attention layerfor that thing andwhen you think about generalizing it doyou think convolutionsmaybe like an mlp layer to fuse them oryou know you could just concatenate themalong the i think with some computervision architectures you just kind ofconcatenate a tensor of features alongthe channel axis and then justconvolve away with it rightso i think they you know i'll give youthe boring answer which is what i likehas nothing to do with it um at the endof the day it's what worksand you know i'm a believer in thescience and we should do the science andunderstand what works best forapplication we should look at thetrade-offs in performance um you know idon't have enough experience withmultimodal models to tell you what theright answer is i will say cnns arestill pretty darn good for you knowbasic computer vision tasksbut you know when it comes time tocombine features from multiple differentrepresentations or multiple differentyou know kinds of features fromdifferent kinds of backbone networks itwould surprise me if convolutions workwell in that setting because really theyare a great fit for dealing with imageactivation maps and not so much fordealing with you know arbitrary tensorsalthough there were a lot of cnn basednlp models in the pre-transformer daysso i would expect that you knowattention or a feed-forward layer wouldwork better as to which one and in whatcontext um you know i leave that up toyou to go and do the research and findout for yourself and report back to meand let me know i know there's been alot of push back and forth of like mixermodelsand you know just kind of non-attentionbut attention looking not attentiontransformer style models versusattentive models at the end of the daywe can do the science and find out forourselves and you know i like whateverworks best you do the work let me knowthat's a great answer transitioning intowhat i wanted to ask you next is um youhave such a clear thinking about askingscientific questions i really admirethat i think the way that you're able topose these questions identify theinterventions confounders that are inthese experiments is is you know veryprofound skill and i want to ask you howyour experience has been with yourscience and then starting a startup howhas the science and the startup creationplayed togetherthe startup will not succeed as abusiness unless we do good scienceand that is somewhat by design um youknow ii don't want we have a big research teamat mosaic it's about a third of thecompany right now and it will you knowit may not always be that large becausewe got to build a business but it willstill be a significant research team andit's going to grow the company will growfaster than the research team you knowthe research team may be growing by 50or more by the end of the year also youknow mosaicml.com jobs come you knowcome apply for a job we're growing buti don't want us to be a google brain ora fair where this is a funthis is a fun diversion for an executivewho's already made all their money andjust wants to do something interestingyou know no pointing fingers buti want to make sure that we're doingscience that is essential to thebusinessto get these speed-up methods to workwell we have to intervene into trainingvery carefully and we have to understandwhat we're doing and why the why makesus more efficient in terms of developingthese speed-up methods the why is goodfor the businesswe are certainly in this wonderfulscenario right now mosaic where thescience and the business are aligned wecan't have one without the otherandso we you know we have to we have to dogreat science we have to be rigorous wehave to turn over every leaf we have tobe even more rigorous i would argue thanyou have to be in academia because we'renot just trying to sneak some graphsthrough the publication processand you know sneak some claims throughthis has to actually workand it's you know anybody who's writtena paper knows going from getting a graphin a paper published to getting it toactually work is really hard and reallyscary and often we say we'll leave thatto the practitioners well here we arewe're the practitioners and also thescientists and we have to make thishappensoyou know i think the science we're doingat mosaic is as good if not better thanthe kind of science that would be donein academia and it's impacting the realworld so that makes it all the moredifficult i will also say when it comesto scientific communication as imentioned beforeyou know we need to basically putprescription labels on each of thesemethods and describe everything aboutthem it looks like you've seen ourmethod cards which our researchersworked really hard on and i spent mostof the past month on i'm just revisingand cleaning up all of themto tell you how does this work what doesit do what are the trade-offs what arethe warnings associated with this whatside effects might it havebecause you need to know you won't trustme to say like i did something crazy toyour model and i won't tell you what itis we have to earn your trustand so we have to communicate all thisinformation a publication is not themost efficient way to do so butscientific communication is stillparamount for our research team so asfar as i'm concerned science andbusiness completely aligned and i'mreally grateful for that kind ofopportunity because you know i think itgives the researchers so much more of asense of meaning than you'd have if youwere just writing papers all dayyeah it's so great that's so interestingto hear and um yeah the the startup andthe science the way it plays is sointeresting and i didn't want to quicklycome back to um the yeah i love themention of the interaction effectbetween these different methods and youmentioned that at the beginning of thepodcast it was just like mind blown forme thinking about that do you see thatas the way that you're going toempirically collect these data these theexperiments to see uh how doesstochastic depth mix with augments andhow does ghost batch normalizationinteract withsome kind of other strong regularizationsuch that your regularization is goingto be you know ruining the training isthis kind of similar to hyper parametertuning where you might use things likemaybe osha where you have uh differentresource allocation these kind ofstrategies totest all these configurations how do youplan on exploring the combinatorics ofthat space it's sort of like hyperperimeter tuning except that you knowi'm not sure the space has the nicesmooth shapes that you want for hyperparameter tuning where you know as youincrease the learning rate accuracy getsbetter and then worse for examplei don't think the space necessarily hasthose dynamics because adding a methodor not adding a method is a discretetransformation in a lot of cases youcan't just kind of slowly phase in amethod in a way that would makehyperparameter tuning work much betterand you know these methods again havethese weird crazy interactions that areyou know not smooth in the way thathyperparameter tuning would expectso really i think the way that we'regoing to do this is a couple of waysone is we're getting intuitions for theway that these methods might interactfor how you you know i mentioned beforebalancing the training process if i'vegot something that you know requiresextra augmentation on the cpui want something that will actually slowdown each step on the gpuideally that is traded off for trainingfor fewer steps something again like samlike sharpness aware minimizationbut sam if it can reduce the number ofsteps and not slow things down too muchthat can make room for a betteraugmentation technique and you hit thisvirtuous cycle so it's it's a systemsoptimization question in that respectthere are also things like theregularization budget where you know wehave a lot of different regularizationmethods we could use we should use theones that basically impose the leastoverhead for the best improvement inaccuracy we developed this methodin-house called call out it's a lot likecutout which is where you take an imageand just cover up a square patch of itexcept instead of covering up a squarepatch we eliminate rows and columns fromthe image so that you can actually lowerthe resolutionit's like cut out but faster because nowyou actually can take advantage of thosemissing pixelsso these are the sorts of tricks thatyou know come in handyand you know these are the ways ofthinking about it but there's a sciencehere and by improving that science we'llget better about coming up with theserecipes not only ourselves but ideallyautomatically for customersum you know and then on top of thatonce we have a good recipe for aparticular vertical like say resnet50 onimagenet our hypothesis is this shouldwork reasonably well for other relatedmodels like densenet or for other imageclassification tasks so there's a way ofkind oftaking advantage of all of thisbut it's right now it's kind of it ismore alchemy than science and we'rewe're trying to develop a science in acompletely new area for the record ithink this is a great academic problemand i would encourage folks in theacademic world this is one of severalbig new exciting academic problems iwish people were working on right nowthat i've discovered only through kindof being pressed into the real world andhaving to deal with some of theseconstraints that i can usually avoid asan academicand umso it's super interesting i mean theidea of lowering the resolution allthese efficiency gains in general maybecan enable say video processing wherewe've been bottlenecked by having tohave these absolutely massive data setsto try to have high resolution highframes per second video butreally what i wanted to ask you aboutnext is um have you been like how haveyougrasped your intuition with the deepdouble descent phenomenon and thingslike rocking where they show that youthey leave it running i think they evensay that their experiment they had justleft it running accidentally and whathappened is it was like overfitted thenlike two days later suddenly it like haslearned generalizable representationsomehow has how do you uh wrap yourunderstanding arounddouble descent and these kind of thingsdouble descentbends my mind a little bityou knowi there's a paper that i'm working onwith you know a student in in my oldresearch group where we're looking atthe relationship between sparsity anddouble descent to try to understand youknow maybeyou know is the double descentphenomenon really just you know having alot of parameters means you find a smallsubset of parameters that actually workwell it just gives you more options butyou're still learning a low dimensionalrepresentationso this is something we're talking aboutand thinking about to some extent thedouble descent phenomenon doesn't reallymatter in practice and i'll go out on alimb and say that for a few reasonsfirst of all we're often not trainingthese models to the point where they canmemorize the data often the modelscannot memorize the data gpt3 can'tmemorize the training set it's nowherenear big enough resonant 50 can'tmemorize imagenet it's not big enoughso you know on that hand we'reyou know we're not really in that regimenecessarily double descend is maybe anexaggeration of a regime the other partis that in order to get those twodescents in order to get like that sharppeak in between you have to work for ityou have to like the deep double descentpaper had to use random labels to getthat to show up that wasn't somethingthat just shows up naturally it looksalmost like a smooth curve with kind ofmaybe a little bend in itbutyou wouldn't really tell this phenomenonis happening so it does i think beg thequestion coming back to where i startedat the beginning of this conversation towhat extent is this a natural phenomenonand to what extent is this an artificialphenomenon that you know we canwe can suss out to use to understandwhat happens naturally but you knowthere's always that question of when yousee an artificial phenomenon how muchdoes it have to do with practice and howmuch does it have to do with the naturalworld um you know i i think deep doubledescent does have a lot to do with thenatural world perhaps i don't understandwhy yet and we don't understand why yetbut it's always worth asking thatquestion how well does our model or howwell does oursetting of inquiry actually relate toyou know the settings we care about andin deep double descent ii think we've oversold a little bit justhow natural these settings are they'renot that naturalat the momentthat's an eye-opening take away from mebecause yeah i've just come to acceptdeep double descent as something to beaware of but yeah i actually haven'treally encountered it in the experimentsi've been running itit hasn't really been something that hascome up with me but so the next topicthat i'm super excited to get into is isee mosaic ml as almost being likeelectric cars in the way that this isgoing to help with climate science andsay the damage that is done by trainingthings like gpd3 and if if we justcontinue this arms race of you know justrun as much as you can to you know buildyour ai businesses compared to thesekind of efficiency algorithms which arereducing that kind ofuh footprint on this can you tell usabout your mission on helping withclimate science with this with thesealgorithmsyeah so a big part of at least mypersonal motivation here is the energyusage aspectum you know just as these models cost alot to train and they take a lot of timeto train you know time is money and timeis co2now we haven't looked into co2 emissionsthat much at mosaic simply because irecognize it's easy to do a bad job ofco2 emissions calculations these datacenters are often using renewable energyor near energy sources that are less co2intensive these data centers may usedifferent mixes at different times ofthe dayso the amount of nuance you need to beable to tell people yes doing usingthese methods reduced your co2 emissionsis actuallyreally difficultand i you know this is the scientist inme i don't want to do it unless i can doit right so if there's someone out therewho has expertise in this topic or isexcited about looking to work on this iwant to hire someone to basicallyspearhead this but i know i'm not theexpert and i'm not going to read acouple papers and become the expertsomeone has to have studied this for along time to really know exactly what todo here so you know i don't want to dothis until we can do it right but bythat same token just if you look atsheer energy usage you know time ismoney and time is energy and we knowthat at least from an energy usageperspective if you can train moreefficiently you can reduce the energyusage and i look at the h-100s i youknow i had a rather uh cynical tweetyesterday that i will try to explainwhich is you know the h-100s are prettyremarkablethey're 6x faster at least if you lookat the sxm highest bin part one that weprobably won't see for another year butum you know 2x of that is from floatingpoint eight so kind of you know a littlebit of cheating in some senseas you start working your way down that2.5 x or 3x faster that you getis by doubling the poweryou know you're going from 350 or 400watts to 700 watts on the sxm parts onthe highest endummoore's law is deadum you know dinard scaling is deadthe idea that every two years you get anew part that is the same cost as it wasbefore but is twice as fast for the sameenergy that's deadum we got a new part for twice theenergy that is about three times as fastand you know jury's out on the price buti expect it'll be pretty expensive atfirstso you know plot that trend outand you know maybe the next part we getwill be 2x the power 2x the price and 2xof speed i don't know how much furtherwe can keep pushing this withoutthrowing more power at the problemand so we've just hit our limits themodels are getting bigger what is it thestat is like 4x per year larger um inthe past couple years um hardware hasgotten let's see we've got two years umwe doubled our power output umand we you know 2.5 x or 3x star computeso 50 improvement overtwo years um call it 25 per year uh 25per year in terms of hardware efficiencyper watt um and probably efficiency perdollarum4xor 400increase in model sizes and model youknow model fluffsyou can do the math and see there's apretty huge gap thereand we just need more compute and morepower to keep up with these models it'snot like the good old days of the 90s orthe 2000s where you know what was thesaying you know um you know intelintel makes and then microsoft takes youintel doubles the amount of compute fora dollar microsoft makes software twiceas resource consumingwe're we're not in those days anymorewe're just consuming resources wayfaster than we can keep up with it andhardware is not going to do itall the fancy architectures in the worldthat folks are coming up with are veryexciting and video is still the fastestmost efficient ship in town despite allthe interesting hardware architecturescoming out from startups and you can seewhat's happening to nvidia sowhat's going to close that gap eitherit's just going to be out of reachincredibly expensive and incredibly badfor the world or we have to change thealgorithmthat's where i think mosaic comes in ithink this isthis is the future of how we're going tobe able to take that 400 increase peryear and make sure that it's at the samecostand i think that was one of the uh thecomments on your tweet was uh if youhave it people are just going to runtwice as many uh hyper parameterconfigurations they mightat the end of the day people will spendtheir budgetsum but budgets are not going to increase400 per year so we can talk about energywe can talk about you know time butlet's focus on cost at the end of theday somebody's going to sayyou need to spend how much money totrain this model you spent four timesless last year what are you talkingaboutum and if budgets don't scale that's theend of the day if people want to usetheir budgets to do more you know ifpeople get their budgets 4x and want todo hyperparameter tuning you know tosome extent more power to them i can'tstop them and you knowmosaic will do better as a business ifthey decide to do thatbutyou know i like to hope at least youknowat some point some executive's gonna saywhat the hell are you doingwhy are you spending all of our moneytraining this one model and at thatpoint you know we're going to have to dosomething differentsuper cool jonathan thank you so muchfor uh for your time and for coming onthe podcast and discussing all thesejust amazing things i think the workthat you're doing mosaic ml everyone outthere should really try the composerit's such an easy thing to add on toyour existing workflows and testing outthese methods is so incredible so thankyou jonathan so much for telling thestorythank you so much for having me i'llgive two quick shout outs actually threequick shout outs at the very end one isi'm just the messenger this is the workof a huge team of researchers engineersum you know everybody at mosaic so it'sreally nice that i get to show up andbrag about 30 people's worth of workum you know i really am the manager herei have not written a line of code sincei got to mosaic so credit where creditis due to the incredible team um andi'll say you know if you want to jointhat incredible team we are hiring likecrazy so mosaicuml.com jobs and the lastthing i'll say is you know check outcomposer give it a star um you know ipromised everyone that you know i try toget us to a thousand stars by the end ofmarch and the end of march is coming upso you know a star would be great umcomposer.dev you can take a look thankyou so much for having methank you and uh the composer to give ita star i'll be linked in the descriptionof the video first thing so go clickthat and do that right nowthank you[Music]", "type": "Video", "name": "Weaviate Podcast #12 \u2022 Jonathan Frankle, Research Scientist in Deep Learning", "path": "", "link": "https://www.youtube.com/watch?v=ZiBkspwrICA", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}