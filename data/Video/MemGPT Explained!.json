{"text": "Thank you so much for watching our paper summary video on MemGPT! MemGPT is a super exciting new work bridging together ... \nhey everyone thank you so much forwatching this pay-per-view video of mgptmgpt is a super exciting new researchpaper that's bridging together Conceptsin operating systems with large languagemodel applications and this is probablythe super exciting novel thing aboutthis is this reframing of theperspective around retrieval augmentedgeneration as well as kind of llm tooluse into this perspective of thinkingabout the llm as the processor thekernel behind the operating system thatswipes in its own memory similar to pagereplacement and all these kind of butquickly before we dive further into thedetails of mgpt the general setup isthat large language models have a limiton how much text they can process asinput so say you're just chatting backand forth with one of these largelanguage models you can generally get apretty large amount of messages so saygbt 4 with 8,000 tokens as the maximuminput you can trade 140 messages ofabout 50 tokens back and forth beforethe chatbot is no longer able to uhreason about conversations you had inthe past so you know say you fired onchat gbt 2 months ago and told it yourbirthday was October 11th then twomonths later it no longer remembers whenyour birthday was so this is kind offraming it in this chatbot perspectiveof how many chats you can have back andforth with one of these popular largelanguage models if you also think aboutuh chat with your docs you then alsohave to have uh the specific informationin the context window which also eats upsome of these tokens and thus how manymessages you can trade back and forthbefore it can reference old messages byjust looking in the context window soretrieval augmented generation hasemerged as a solution for this is asuper popular technique where you usevector eddings and search queries toonly populate the input with relevantinformation so here's an example withoutrag this use this example of time yousay what is rtoc a feature in weate andchbt doesn't know what it is but if youinstead you know say please ground youranswer in the following information andthen you ask it what is refc now itknows how to answer the question so whenall the user wants to do is know whatrefc is we get away with this prettysimple rag setup where we can justretrieve some kind of information to usein our context window and then answerthe question so say we only need 200 to300 tokens in this particular examplebut now let's imagine a longconversation where the user isrepeatedly asking questions about how touse refc for their particularapplication and we have to balance me ummanaging the memory of this conversationhistory as well as lookups from ourretrieval database so the big idea inmgbt and this is super exciting is whatif the large language model was aware ofits own input window limitation and tookactions accordingly so they're extendingthe large language model with the tooluse of knowing when to add retrievalresults or say things that it learnsfrom the conversation into its workingmemory so let's step into the overallarchitecture of mgpt so mgpt is theoperating system for rag applicationsbridging together memory with tool useand this particular focus on managingthe main context memory so at the heartof the oper in system we have the largelanguage model processor then we havethe virtual context so the main contextis what's currently in the input for thelarge language model to make itsprediction what's in the externalcontext is say our Vector database orour store of data where we can uh swapin and out with the main contexts likepage replacement and operating systemsto get the relevant context that we needto complete a certain task so this isthe the memory part of it and then thememory is managed with functions andTool use so tool use is a super excitingidea where we conect Lang languagemodels to things like say calculators ormaybe a weather API if you want to askit the question what's the weather inBoston right now it needs to send thatexternal request rather than relying onthe information stored in its parametersor particularly in the weather APIexample it's unlikely that you'rekeeping your vector database fresh withwith that particular information so youprobably have these kind of externalservices that work into this picture aswell but more interestingly are thesefunctions around reading and writingmemory so reading memory is where youyou know read from the vector databasewe say retrieval a search query and thenthere's also ideas around pagination andquery rewriting in this mgpc paper butwe'll get to that later but this is theidea of where you read memory topotentially add to the main context byusing these uh operating systemfunctions like a pen to the workingcontext we'll get into that later aswell but also interestingly we havewrite memory so as you're having thisconversation history as I'm talking tomy chat gbt for a long time the mgbt hasthese functions around you know Connorjust told me his birthday is October 11so let me add that to to my uh mystorage then we also have this idea ofinterrupts and events so events are whattriggers mgpt to start doing someprocessing so whether this could be assimple as I come to Chad gbt and I saywhat is rtoc and then that triggers theevent that starts all this processing orsay I upload a document or say there's asystem message like hey your contextwindow is at uh 3,500 tokens let's let'syou know start trimming it down or youhave things like a timer like every fiveminutes maybe it trims down or somethinglike this so you have this kind of ideaaround interrupts from operating systemswhere maybe you have some kind ofasynchronous processing like to thelarge language model you say uh researchhow selfrag works selfrag is a papersimilar to mgbc that came out around thesame time so say you're doing thatasynchronously while you continue thechat and then it interrupts and says heyI finished this uh research reportshould I work it into the conversationor what should I do with this so this isthe general overview of how mgpt worksso I took this quote from I don't knowif it's an ex quote but just toreference that I got this from outsideof the paper this was Charles Packer onthe amazing run llm podcast withProfessor Joseph Gonzalez and he framesthis idea of mgbt as an agent that knowshow to use memory management tools and Ithink that's just a perfect way todescribe it a super exciting directionfor this whole field of retrievalaugmented generation is endowing thelarge language model with the option towrite to the database as well or in thisparticular case read WR to itsparticular main context so you've alwayshad this idea of read from the SaveVector database and then just kind ofblindly put it into the main context butnow you kind of have this layer in theMiddle where you have search results andthen you're saying what from the searchresults am I going to put into my maincontext and then kind of keep there as Icontinue the conversation so I thinkthis quote is just so powerful an agentthat knows how to use memory managementtools so the general idea here is thatwe're building an operating system forlarge language models and a quickquestion I'd ask the audien is what youthink about framing Frameworks like Langchain or llama index in this kind of wayis thinking about them asoperating systems for large languagemodels that orchestrate the connectionbetween language models and tools andVector databases and memory but sodiving a little further Andre karpathyhas also written a really interestingtweet on this kind of llms and operatingsystems and this is quite an interestingidea I think of uh taking that next stepin the thinking of not just llms anddatabases but having a whole operatingsystem that orchestrates this kind ofthing so from Andre karpathy with manypuzzle pieces I assume dropping recentlya more complete picture is emerging ofllms not as a chatbot but the kernelprocess of a new operating system sowhat we're seeing in mgbt is the llm isthe kernel process that's saying I needto change my memory I need to use thistool or I need to respond to this eventso it's the kernel and the operatingsystem that just that is you knoworchestrating how to manage its memoryand use tools for example today itorchestrates input and output acrossmodalities text audio Vision codeinterpreter ability to write and runprograms browser internet access orembeddings databases for files andinternal memory storage and retrieval sosome examples of the different kind ofuh functions it can it can do or memoryit can access a lot of computingConcepts carry over currently we havesingle-threaded execution running at 10Herz tokens per second and enjoy lookingat the assembly level execution tracesstream by Concepts from computersecurity carryover with attacks defensesand emerging vulnerability so that thatI think there's just so much informationso much interesting stuff to explore inthat in that little text alone this ideaof you know you know we have singlethreaded where we just have one llmprocess you know where most of I know atleast I'm doing this is logging the llmin my terminal and just kind of watchingit Go by because you know you're payingfor the tokens but this kind of we SEthings like uh maybe Lang Smith is agood example from Lang chain ofvisualizing these complex prompt chainsfor uh understanding these executiontraces but I think this kind of likeparallelism you know if you have likeconcurrency and llm tasks like againthat example of like uh research selfRag and then it's doing This research inthe background and then it says hey Ifinished like there's probably just somuch opportunity from that and sofinishing up I also like the nearestneighbor analogy of operating systembecause the industry is starting toshape up similarly Windows uh OS X andLinux GPT Palm cloud and llama anoperating system comes with default appsbut has an app store you say the chatgbt Marketplace and kind of how codeinterpreter is plugged in with coh hereyou have coral and so yeah we'redefinitely seeing that kind of app storearound the language model and then mostapps can be adapted to multipleplatforms again the API say wv8 generatemodule showing how you can also take thethe model out of the app store tldrlooking at llms as chat Bots is the sameas looking at early computers ascalculators we're seeing an emergence ofa whole new Computing Paradigm and it isvery early so this is a super excitingtweet in this whole context of uh llmsand operating systems so let's dive intoa little more particularly what makesthe the mgbt and operating system whichis particularly this kind of memorymanagement as a large language modeltool so what they're adding in mgbt isthe working context append replace thesekind of functions so you have this kindof conversation where you say hello Chadwelcome I'm excited to embark on thisjourney with you as a PhD in computerscience blah blah blah and then the usersays you know this information of mybirthday is October 11th and my favoritecake is this chocolate lava my favoritecake so this is an example of how ittakes the conversation history and it'sdeciding what is important to put in itscontext so similarlyuh it might have a system message thatsays warning the conversation historywill soon reach its maximum lengthyou're making the language model awareof its own token limitation say Hey youknow you've got 3500 tokens we're goingto need to compress this so then it willcompress it as following it takes theconversation history and it depends thiskey personality trait enjoys high-speedAdrenaline Rush activities like FormulaOne racing and intense gaming sessionsin csgo so it's doing this to compressits context window so similarly withsearch we say search will quickly getinto recall versus archival storage andhow they differentiate that in the paperbut you're having this conversation whatwas the artist you mentioned you couldget into so the user is asking about youknow conversation history so you're thenlooking into your you know your externalstorage your vector database where youmight have two kinds of vector databasesyou can have all sorts of kinds ofvector databases but for now you havethe storage of the event history likeall the conversations between you andyour chatbot as well as say like generalinformation like if you're chatting withyour docs this is the conversation we'vehad these are the docs so two twodatabases so say you're searching theconversation history about music fromthere it recovers you know you'retalking about you like Taylor Swift soit adds that to the working contextsimilarly we also have replace so thisis a super exciting idea where you havenew information so you used to say youknow I was super into horror moviesrecommend me horror movies and now yourtastes have evolved and so now you'reinto romantic comedy so it replaces thein context with I watch horror movies toromantic comedies so I think anotherreally interesting quot from the paperis to think about this as the the mgbtis documenting its progress on the taskby writing to its own working memory soit's doing a long task like documentanalysis it's only writing to itsworking memory say important things thatit's going to help it further so it'skind of like this idea of distilling TheCore Concepts into you know what'shelping you with your research orwhatever you're doing so let's dive alittle further into types of context sothese types of context are the explicitlabels of the parts of the input windowto the large language model and I thinkthis kind of Separation this explicitthing of these are the systeminstructions this is the conversationhistory this is the working context Ithink there's just so much moreopportunity to explore that kind ofseparation of the parts of the inputwindow so say you have kind of this uhsay you're in these multi-agentFrameworks like autogen where you havethis uh information about your personalike I am a software engineer I likegoang and things like this and then youalso have say a highle description oflike what you're working on as well asthis kind of retrieval context and maybemore immediate recent conversationalcontext with the other softwareengineers and that multi-agent frameworkbut this kind of separation of thecategories of the input window and howyou think about retrieving andaugmenting each particular part of it sowe start off with system instructions sosystem instructions you know these arelike the pre- prompt is like you are ahelpful assistant but now that you are ahelpful assistant thing has beenextended with the the descriptions ofthe tools that you have access to so insay open AI funks you get this like Jsondictionary that tells you what afunction does and how to format theinput output Arguments for sending arequest to that API so if it says heythis is a calculator you can use it toadd multiply numbers together this wouldbe how you would uh trigger a request tothecalculator the conversational context isthen a first in first out cue of recentevent history and then an interestingthing is this recursive summarization soone of in my opinion one of the mostinteresting ideas of Lang chain in llamaindex has been this idea of uh where youwhere you have this kind of recursivesummarization so if you have docu youhave too many documents that you can fitinto one input window you'll you'll havesome kind of prompt like pleasesummarize the following documents you'llreceive them one at a time as well as asummary so far and then so you keep thislocal summary and you just keep loopingthrough the documents updating thesummary and so on so you do this to theend of the conversation history so saythese are my last 10 you know back andforth of the chatbot and these are myfirst 200 messages these would berecursively summarized and justsomething that can fit in the input andthen you have this working context sothis working context is where is thememory scratch pad for where the LMprocessor is reading say what it's justretrieved and it's looking through thesearch results and it's saying okay Iwant to take that and put it in myworking context or you know similarlylooking through its conversationalhistory so these this is what'scurrently in the in the memory to thelanguage model what's currently incontext so then we also have thisexternal storage if we'll you know goall the way back to our picture of thiswhole thing we have the main contextthat's of system instructionsconversational context and then workingmemory and then we have our externalcontext which is where our Vectordatabase comes into the picture sowithin external storage the authors arelooking at two different kinds ofstorage so recall storage this is justlike the Raw event log so say it's chathistory or document processing just theraw you know what happened I uploadedthis document I asked you this questionabout the document the system ended upsending this answer so that I then sentthe next question which was this or justthe um the archival storage which is thegeneral read right store of sayWikipedia or you know your docs in thechat with docs classic example so theseare kind of the two kinds of storagethat we might retrieve from so theauthors also present three differentways of querying these external databaseit can be time based where say you justget the most recent events which makes aton of sense for conversation historybut then say you have informationstorages then you have text search andembedding based search so the next keypart to this and this is probably thekey part is this self-directed editingand retrieval so again we saw withinthat function schema that goes into thesystem instructions are going to be howyou do this working context out of penworking context out of replace thefunctions for how you will take saysearch results and then you work thatinto the working context or update theworking context so then we have thecontrol flow so events are going totrigger the start of this process so theyou know user message system messageuser interactions and then we have thiskind of function chaining for how we'remanaging retrieval results which is alsoa really interesting uh part of thispaper that I think of as marrying memgbt with web gbt but with this also kindof interesting rephrasing of the queryso with web gbt you had search actionsso you wouldn't just retrieve and thenjust take the top five results and justput those into the into the input youwould instead say uh let me see the nextpage of search results because you knowthis is how humans use things likeGoogle search as we you know scrollthrough the results to try to find thething that we're looking for so mgbt isalso adding this thing where uh inaddition it might look through say youknow three pages of results and thenreflect on uh let me actually try adifferent query this query is notspecific enough and I'm learning that bylooking at the results that come fromthis query so that's a prettyinteresting additional layer to this wesay have things like uh queryreformulation where you pass the queryto the large language model and promptit like here's a query uh could youplease reformulate it to get bettersearch results when passed to a searchengine and things like this but makingit more meta with this kind ofself-correcting where it sends thisquery sees some results and then learnsfrom that to reformulate the query so asa quick recap of the core ideas we havethe main context that has these explicitseparations of the part of the contextfrom the system instructions to theconversational context or the eventhistory as well as the working contextand I think this that part of it is justsuper exciting the continued explorationof how we explicitly separate thecontext especially as we get longer andlonger context models then we have theexternal context which has the recallstorage and archival storage or say twodifferent ways of thinking about itcould be two different classes in a we8vector database instance but twodifferent sources to retrieveinformation from then we have thisself-directed editing which is you knowhow we're augmenting our work in contextwith things from the event log or sayour retrieval and then we also havethese search actions through theretrieval like paging through searchresults or say formulating a new querythen we have this control flow andfunction chaining understanding that wehave system events that kick off themgbt process as well as say the functionchaining involved in paging throughsearch results and reformating the queryso kind of opening up the framework tobe extended in the future now let's diveinto sorry now let's dive into some ofthe experiments in mgbt so they startoff with the the general questions ofdoes mgbt improve consistency andengaging this soconsistency does mgbt leverage itsmemory to improve conversationconsistency can it remember relevantfacts preferences and events from pastinteractions to main coherence this isthe whole idea of you know two monthsago you told chat gbt that uh yourbirthday is October 11th and now you'retalking to it later and it's going toremind you of your birthday or thingslike this so then engaging this doesmgbc produce more engaging dialogue bytaking advantage of its memory does itspontaneously incorporate long-rangeuser information to personalize messagesso to evaluate this they use themulti-session chat data set so themulti-session chat data set is generatedby human labelers who they're given aprompt to play a particular Persona andthen they have five SE five chatsessions and each chat session has about12 messages so this is the data set isyou play a role like hey I am into to uhgaming and horror movies and things likethis and then you chat with anotherhuman who similarly has like a Personacard so then the data set is augmentedto add a single question answer pair atthe six session that will do some kindof long range reference to somethingsaid earlier to see if it was able touse this uh particular way of organizingits memory to recall the particularthing about who it's speaking with sothese are the results of uh jointlyhaving the Rouge score which is like thesome some type of engram overlap I'm notsure the exact details of but you knowthis kind of engram overlap between thatground truth answer and then what thelanguage model produce as well as thisaccuracy which is this llm self evalthing where you give gbt for uh thequestion answer and then the gold answerand you say you know how was this answeris it accurate did it f is it closeenough to the uh the gold answer so thenanother task they tested wasconversation opener so this was about uhseeing how well it can open theconversation with something engaging Basso this is measuring that engaging thiswhereas this measures a consistency sothis is you know the the it has the goldPersona of the user and then it has thisuh human Baseline of what was said soI'm setting the elsat I want to be anattorney blah blah blah and then hasthis particular preferences around Ilove coffee and I love tea so then theseare three different kinds of responsesfrom mgbt whether it's using the work incontext and the recall storage to saywork in the uh the tea and the coffeething into the opening response versusjust something generic like you know heyit's a pleasure to talk to you let's youknow let's talk so so using this kind ofstuff to have a more engaging opener andand they similarly measure this byhaving the similarity in conversationopeners between humans and then the mgbtwith these different contexts so here'sanother Super exciting detail to mgbt isovercoming this lost in the middleproblem so lost in the middle is afamous paper that shows that uh inretrieval augmented generation thelanguage model tends to only attend tothe first search result or the lastsearch result so if you have theinformation you need in the middle soreturn 10 search results to put in theinput and the thing you need is atposition five the language model is notreally able to parse the search resultsand find it in position five so becausemgbt does this kind of paging of thesearch results and only adding relevantinformation back to its working memoryyou have this kind of flat line of itdoesn't matter how many documents youretrieve it's going to have the sameperformance because it's parsing throughthe search results to add it to itsworking memory and they also introducedthis new task of nested key valueretrieval so in Lost in the middle uhone of the tasks is you have key valuedictionaries so it's like U ID key u IDvalue and it would say like what's thevalue for94071 FF right and so then it has tolook up the value so now you're kind ofdoing this chaining where it's storingthe the nested values and so it's it'stesting this multi-hop questionanswering where you're the you know theintuition what it would eventually beused for is you have questions like didAristotle use a laptop you break that upinto when did Aristotle live when werelaptops invented answer each separatelyand then merge it together so this iskind of testing that ability to mergetogether facts awesome so that's a recapof the core ideas of mgbt and some ofthe experiments in the paper now let'sdive into some of the future workdirections outlined in the paper as wellas some of my personal takeaways and howI think mgbt will impact the entirespace of retrieval augmented generationso starting off the authors mentionapplying mgbt to other domains withmassive or unbounded context theyexplore using mgpt for chat Bots as aswell as document analysis in this casereproducing the natural questionsexperiment from Lost in the- middle andshowing how mgpc can help with that theyalso discuss integrating differentmemory tier Technologies like databasesor caches I think that is a superinteresting one where you have differentkinds of memories and you have differentlatencies for the different types ofmemory and so that is a superinteresting topic then further improvingcontrol flow and memory managementpolicies just I think just furtherunderstanding the action space and howto describe the tool of me of memorymanaging your memory to the llm and thenfine-tuning an open source model for MGPT tool use so in the experimentsthey're prompting mostly gbt 4 gbt 3.5to do this what would it take to getllama 2 to achieve the same kind of toolfollowing for uh memory management so Iwant to kind of outline that a littlemore because I think that's a prettyexciting future Direction so wegenerally are looking at this idea wherewe're using gbt 4 to create trainingdata then we use knowledge distillationto train a smaller model on the labeledexample from gbt 4 and this way youcompress it into the smaller models orsay the open source models so this iskind of you know we're there are allsorts of tasks that you can kind ofcompress this way and I think that's oneof the most exciting directions for thefield right now is we're using languagemodels for all sorts of things wheneveryou have a prompt for something you havea task that you could then generatelabeled examples with GPT 4 and thencompress it down into a model into sayllama 2 with the 7 billion parametersit's going to be cheaper to serve andyou know you run it well it's it's stillin the air if that's going to be cheaperto serve because you know open AI theythey have all sorts of you knowinfrastructure behind how they servetheir API so it's interesting stillexactly the cost difference between youserving your llama 27b compared to thegbt uh 4 3.5 turbo all these kind ofstuff so there's also kind of the ideathat say F 1.5 billion parameter thattextbooks are all you need paper there'sthis idea of you can maybe keep goingand keep compressing it and if we get tothe point of sayyou only need a 300 million parametertransformer for your task and say neuralmagic is exploring uh like sparsifyingthese models to run them on CPUs so thatcould be really interesting in terms ofjust you know the cost of running theseinference and if llm inference cost getssuper fast and super cheap that unlocksall sorts of kind of new use cases sohere are some of my personal takeawaysfrom this so firstly for me it wasreally interesting to see this kind ofexplicit uh when to active retrievalthing and compare that with Flare whichis the active retrieval augment togeneration technique so with Flare whatyou do is you're sampling the nextsentence and you multiply out the logprobabilities of the tokens for thatnext sentence if that's below a certainthreshold you'll do another retrievalbecause it's saying that basically thatnext sentence it wasn't grounded infacts you need to retrieve moreinformation to help have a better nextuh sentence generated compared to thiskind of M GPT where you have this likeexplicit you know I I don't know what II don't have what I need to write thisnext sentence so let me go retrieve andupdate my working context and so thiskind of whether you just want to decodeit from the probabilities of thelanguage models or you want to have thiskind of memory management activeretrieval as an explicit tool that ityou know calls with functions the nextbig thing and something that they talkabout in the paper as well is kind ofthis latency of mgbt if every time youchat with your uh you know your chatbody it has to do all these steps thenthat's going to be really slow andthat'll be problem so there' probably belike a a layer on top of this where it'slike kind of a quick answer compared tothis whole like operating system thingand that's kind of related also to theyou know the compressing the model andtrying to make all this run faster thethird point for me that I think isreally interesting is the differencebetween this kind of uh web resultpaging or reranking so similar to webgbt mgbt is going to like scroll throughsearch results and so I'm curious whatpeople generally think about thisdifference between like next pageprevious page actions compared to justapplying a reranking model which is ahigh-capacity model that generally takesin the query in each document and thengives it a higher capacity ranking scorelike matching score and then Resorts thelist from say the course grain retrievalor we're now seeing these kind ofranking models that would take in likeyou know 10 documents as input and thenrerank them by kind of looking acrossthe documents in addition to just a sortof query and one candidate document at atime kind of setup so a lot ofinteresting things happen with rankingand it makes me I'm not sure I'm superbullish on this kind of paging conceptbecause I think reranking is alreadykind of you know know the the betterversion of that so uh then in the paperthey also have these kind of umperspectives on training longer contextmodels sort of framing that the purposeof this uh paper is is hey we're youknow we're never going to get modelsthat can uh process like a 100,000tokens so we're going to need these kindof memory management techniques and sowhat I've kind of learned about thisespecially in the weeva podcast withofier press is that it's not just kindof the quadratic attention you can kindof have like gradient checkpointing andthings like Alibi attention to getaround sort of the computationalcomplexity behind uh scaling the inputlength the real problem is sort of theuh training data and there's not a lotof good um training data that'snaturally like 100,000 context length sothat's more so the interesting thing iswhere do you get this data from so I'vehad these really interestingconversations with Owen kgve who's thefounder of sci-fi and so this will belinked in the description it's a GitHubproject where you're creating synthetictextbooks and I think this kind ofsynthetic data it could be the answer tohow do we create the training data forthese super long context models so someother takeaways is I think this kind ofparallel asynchronous concurrentprocessing is going to be a superinteresting direction for the evolutionof this so you know say you're havingthis conversation and you we talkingabout mgbt and you also want to kick offthis research on how does self ragmanage rag selfrag is like another paperthat came out it's like this is like thedog food as I'm doing this I'm thinkingit would be interesting if I could alsohave a knowledge of that paper soimagining it's kicking off that asyncresearch task and then interrupt hefinished the report or I've written itto the database all these kinds ofthings so then actually let me come backto the gorilla thing but so seven wouldbe uh this idea of the use of databasesin caches so a really interesting thingand I you know I'm not an expert on thisbut I learned so much from listening touh Eddie and present how multi-tenancywas architected in we8 at the AIconference and some future directionslike you you have all these kinds of Umemory with computers right you havelike the memory cache you have like L1L2 and then you have like RAM and thenyou have ssds you know hard disk andthen you maybe have like cold cloudstorage so there's like all thesedifferent kinds of ways of having memoryand maybe the llm operating system canmore intelligently kind of cache memoryand use like the physical storage soit's definitely not something I'm anexpert on but it it definitely I can seekind of you know how that could be anopportunity so then let's talk aboutgorilla and so shashir Patel is one ofthe authors of this paper this comesfrom the same lab as the gorilla llm sothat's kind of what drew me to thiscurious to see if there was a connectionwith the gorilla llm so I think theangle here is allocating as few tokensas possible with the gorillas sothinking about mgbt and Guerilla theidea is to describe the tool in as fewtokens as possible because the wholeidea of this is you know we need to beefficient with our token allocationsimilar to lead to like memoryallocation is token allocation so if wecan just describe our tools like youknow in the case of the we8 gorilla youhave access to a vector database API youcan perform different kinds of searchessuch as bm25 vector or hybrid you canadd reranking or you can add filters soyou just have this succinct naturallanguage description then it can do thenatural language instruction of thesearch it wants to execute with you knowmore sophisticated searches rather thanjust uh search music right you can useall the apis of we v8's graphql API andthen it can translate this naturallanguage instruction into the graphqlwith the gorilla under the hood somultiple language models also in theintermediate say the parser step let sayin this parser step you have another uhlanguage model a gorilla that is Hformulating the tool requests into theparticular API so thank you so much forwatching this explanation of mgpt Ireally hope you enjoyed it I'd be morethan happy to answer any questions ordiscuss any ideas you had about thecontent you know explored in this videoif you want to connect with mepersonally I prefer to manageCommunications on X at C30 if you wantto learn more about wv8 you can checkout we8 iio or if you want to join theWEA community on slack so thank you somuch for watching and I hope you foundthis useful", "type": "Video", "name": "MemGPT Explained!", "path": "", "link": "https://www.youtube.com/watch?v=nQmZmFERmrg", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}