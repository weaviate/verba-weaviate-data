{"text": "Private GPT: https://github.com/hsm207/privateGPT HowTo Search https://weaviate.io/developers/weaviate/search Weaviate ... \nforeign [Music] air this is this is pretty amazing we've been going through this every month for the past eight months and we're super excited to have you all you know watching and listening to us so today I have Erica Dan and Zen joining me and we are hoping for an amazing episode as always so like the usual like um if you like the video whether you're watching it live or you're watching it after uh give us a like that always always helps us you know like maybe you put the Vivid air on the map more and make it more visible um also thank you for watching thank you for taking the time and and see that to see the things that we have to share uh with you um and the cool thing is for those of you that join us live you know come and ask questions as we go right this is the benefit I think the episode seven was one of my favorite because there are so many great questions um that like we run out of time even like there were so many great discussions that we wanted to get into uh so this is great so um yeah so thank you for uh watching and listening to us so we have a pretty packed agenda for today and there's a bunch of topics that we'd want to disc uh discover discuss go over in general um the first one is running llms privately so Zen is very keen on this topic and he has some cool things prepared even maybe a small demo maybe maybe slow demo but demo nevertheless exactly um then we will We would like to talk to you about ingesting PDFs to deviate so that's something Erica uh you feel dearly about since you published a blog post about it yesterday so that should be a good topic close to my heart very close to your heart indeed uh the topic that is close to uh Dan's heart is uh with it Forum it's something that we are launching today maybe some of you discover it randomly maybe not uh but we are often officially launching it today during this V8 air which is pretty amazing um next I'd like to talk to you about something that I feel dearly and something close to my heart is some development experience improvements that we're planning around the python client so there's a proposal and we need your feedback and the python line is just going to be the first one that we want to do there'll be other ones too uh so yeah if this is something that you find interesting definitely bear with us for that um then we'll have done one more time to talk to us about the new how to search segment in the documentation I think this is pretty awesome this uh definitely worth uh waiting for and like learning you probably already saw it but it's in case you haven't done we'll show you more all about it and then I will also like to talk about reviewed recipes so this is something that Erica is cooking for us pun intended and so this is this is going to be great for sure so that's the intro this is the Canada things we want to do and I will open the floor to Zen who talk to us about running llms locally all right again thanks Sebastian should I share your screen by the way uh no you know that I wanted to kind of lay the groundwork so about two weeks ago our team was in um in Boston for the open data science conference and I think the the first most asked question we got was what are vector databases and then immediately after that uh people were interested in this interface of vector databases and large language models where you could give a large language model not just a prompt but a a prompt with retrieve documents from a vector database and there's a lot of people uh that were coming up and they were from Fairly large companies and they wanted to know how they could run large language models with Vector databases privately whether that's on their own cloud compute whether that's on-prem that their main concern was they didn't want employees to use a large language model with private company data and so over the last week since then we've seen a lot of large companies have banned the use of public language models where you're calling an API sending data back and then getting the generated response um so this is something that's fairly important that I think is front and center for um for a lot of for a lot of big companies so I wanted to talk about this topic uh I'm working on a blog post around this as well and there's a couple more things that are cooking up around this but the main drive here is everyone wants to do this idea of retrieval augmented generation and that's a fancy word for just saying I want to run a vector database with a query and then I want to take the retrieve documents from that query so let's say I'm interested in uh the recent uh PGA Golf Tournament I want to retrieve documents that are related to that that are stored in my Vector database and then I want to say tell me who over performed based on uh based on expectations or something like this so it'll read through the documentation it'll retrieve everything that's PGA related that's the vector databases job and then it will send that with a prompt to the large language model and typically how we've been doing this is by calling it API and then we send the data as well as our prompt to the large language model that's uh that's remotely deployed and then we get the response back um the whole conversation was around how could we bring that large language model locally on-prem so that we can run it privately so this entire ecosystem of the vector database the the client side the people the employees that are within the company using the vector database as well as the large language model that's generating responses as a result of your retrieved context all of that should be private whether on-prem or running on uh on your own Cloud solution and so this is a really fascinating topic because now you have to think about you can't take advantage of the uh the huge compute that companies are offering when you call an API they they have optimized instances clusters to run that run the inference for you and then send you the response back really quickly you have to think about how you run it locally uh you have to set up that infrastructure so I think this might only be relevant for the really big players who are able to maintain a a very large on-prem cluster where they can actually do inference and model training and fine-tuning uh locally but I wanted to kind of talk about this because I think people are really interested in it so what we did is Sebastian if you share my screen now so what we did is we created a a quick demo with private GPT which is a which is a repository that's uh pretty popular and it essentially accomplishes this uh this topic of running a vector database as well as running a large language model privately locally on your on your laptop and that laptop part is fairly important later on as you'll see so the the actual public repo uh is uh is over here you can have a look at this one so what we did is we forked this and then we got it to work with weeviate as a as a vector database and so you can go through it's it's pretty straightforward all you have to do is uh spin up a local instance of of leviate so through Docker and then you can go and uh ingest your data so here what I've done is in this particular case I took so Eva is hiring quite a bit right now I took I scraped all the job postings and I put them into one uh one text file and these are all the postings that are that are live right now and what the demo does for private GPT it goes through and it chunks it automatically so it'll take 500 characters and it'll automatically chunk it and then there's some repetition so it'll repeat it'll include this and it'll include another 500 characters this will be one separate document and then you'll have another separate document so on and so forth and then when you run uh so when you run private GPT so let me figure here if you go here uh all you need to do is let me exit out of this all you need to do is execute the private gbt python script and then it'll uh it'll start up so for this for this demo I'm using the Llama C plus plus model uh that's that's available publicly and that one it's fairly slow to run on my on my MacBook this is one of the other problems because everything is running locally on your computer the vector database is running on my machine and also the the model is running on my machine right now so technically I could run this demo without any internet connection or anything like that the only reason I'm connected to the internet is because I'm I'm talking to you folks so you can see here that the vector store is locally deployed you can now query this right so here if you're interested in um what uh rules is uh you V8 hiring for doing four things and sending this as a query to the vector database it retrieves four documents that are that have the highest uh similarity and then it takes those retrieved documents send them sends them in as context to llama C plus plus and it answers this question using those uh using those documents as information so it's doing retrieval based question answering which is implemented in Lang Chain by the way so we could talk a little bit more about that later on um but this you see here what's happening this is the problem right I can I could give the entire presentation while this is going on um so if you're interested in this actually what you should do is you need to have an ec2 instance from Amazon and then you can kind of juice up uh the the specs on the computer that you're renting out but with my M1 Mac which is fairly powerful it takes a really long time to generate anything and we can probably get through this entire session without us seeing tokens that are printed so what I did is in prep for this I ran this beforehand so I ran a bunch of questions so I asked what rules is the core engineering team hiring for and then these tokens were generated one by one and then it also tells you the answer is based on these Source documents these four Source documents and then this particular question that I asked here what roles is leviate hiring for um it tells you everything it also actually uh hallucinates a bit here as well so there's more prompt engineering that can go into this like that but we're not really hiring for a developer Relations Specialist uh actually yeah so it yeah and if we do I should probably know about that right yes it's not it's working I can guarantee you it's just running the inference and generating the generating the tokens words one by one cool I mean and and here here's the thing because uh we talk about as well and you said that there's already been some improvements to like the the original private GPT Library uh and it's just like a matter of like eat updating like this is an early days thing so the power itself that your maybe your laptop has is one issue but like the other issue is like there's still early days but the idea is if you for example cannot afford to send your sensitive information outside but still you get something like you know like a GPT like uh request and then generate response based on your own data the future is there right like and we could already start playing with it right so this is this is mind-blowing that you can actually do it yourself privately yeah for sure that's definitely where the future is going um internally we we've been uh discussing having more support around this whole stack of the vector database as well as the large language model deployed oh there it is okay there it goes all right it's coming up yeah yeah so uh we'll uh we'll talk more about this soon around how you can make it easier so that you don't have to go to external repositories or play around with demos like this you'll be able to take your own uh llms that you've trained or fine-tuned on your own data and then very easily connect them uh through a module to uh to eviate so yeah that's something that's in the works this would be really cool for uh companies that are in healthcare but also Finance so this reminds me of the team too that participated in the odsc hackathon where they're sending um what was it like the symptoms or what the previous existing conditions of patients so it'd be cool if this can kind of like help the doctor um diagnosing but also treating patients so this is really cool yeah that's yeah I think all the regulated Fields where you have to be careful with uh information um uh this would be very useful for it and I think epic is already working with uh with companies to implement these llms and I can guarantee you they have some sort of clause in their agreement where the llm has to be deployed locally on-prem or on their own servers um yeah nice is amazing and like so what are the like do we know anything about like the parameters around these llms or like how big are they and yeah so this is uh this is a seven billion parameter large language model so it's not it's not a when large language models are concerned it's a bit of a mini llm um and even that it takes a while to run on my run on my computer so we can actually I can actually show you the the one that's uh number C plus plus this is the one that I'm that I'm running on here and so it's a fairly popular open source uh version of uh of llama the Llama model from meta um the cool thing with this will be that we're going to have new and newer and newer open source models and then eventually people are going to bring them in fine tune them on their own private data and then not open source them so that there needs to be support going forward for this stack of llm vector database all on-prem all all private nothing ever leaves the confines of your uh of your of your server and even yesterday not yesterday but I think a couple of days ago I was reading a paper over the weekend around um another open source model um the paper is called less is more Lima and uh that was another open source model from uh Facebook with 65 billion parameters uh and that that tends to outperform some of the state-of-the-art models that we have um so the field the direction is going in I think having this private stack where everything is uh everything is locally or privately deployed I think is fairly important nice nice and um we have uh Nikon saying great you know he definitely enjoyed uh your presentation of on private LMS or like local Ryan LMS locally so uh thanks uh um so yeah and just a quick reminder for anyone that arrived a bit late um feel free to ask questions drop us you know just hi say where you messaging us from and uh actually speaking of the devil we have a question so let's see if we can handle it how much how much amount of data used here to fine tune this 17 parameter model like how many dogs to get standard results yeah so I didn't do any fine tuning of this model actually this is purely powered through retrieval from a vector database and then question answering based on that retrieval as context so let me bring this guy up so you can see here I asked the question what roles is leviate hiring for and this is the generated answer from llama C plus plus and it at and it bases this answer off of these retrieved four documents right so the document that I showed earlier is just automatically chunked and you can see here that the answer is based off of these four retrieved documents so there's no fine-tuning here it's purely retrieval based generation if you had fine-tuned if you take your own data and you fine-tune it the answer is I can guarantee would be even better um that's that's another really interesting question maybe we can talk about this in the future where how do you weigh fine-tuning versus retrieval augmented generation where you just have a vector database that you're keeping up to date the minute a data point comes in you put it in um we actually have some content planned around that as well so I don't want to spoil that but this is a this is another topic that I'm super excited about yeah cool cool thanks uh for for sharing this yep um got it thanks yeah thank you thank you um so um and then similarly on the topic that you were just talking like about you know possibly like you load the documents all the chunking and everything I think Erica is walking right into this very nicely around like ingesting um unstructured data and then more importantly around um PDFs in this case what a smooth transition thank you very nice huh I improvise you could hear that yeah so day to day uh data comes in different data sources and the main question is how can you take this data and import it into wheat gate um so yesterday if you wouldn't mind sharing my screen and I wrote this blog post um adjusting PDFs into webiate and in this demo we are using unstructured which is an open source company working at The Cutting Edge of PDF processing and other data sources and then extracting the content from these files and then uh converting it into a machine readable format so one thing about PDFs is like to humans it makes sense but it hasn't there hasn't been a case where you know our computers can understand it and then extract content from that so that's really what unstructured is doing um so in this blog post um we talk about Optical Care character recognition uh process and what this is so it converts different types of visual documents into a machine readable format and there are models like layout LM V3 and donut and what's special about these two models is that it leverages both text and visual information by using a multimodal Transformer so it's visually looking at the PDF and then it's extracting the content from it um so what unstructured has maybe I should go on the website is they have these bricks um yeah so they have three bricks uh for document pre-processing so partitioning cleaning and staging um so in the demo that shoot remade um I believe we have a link to it in the bio right um we're using the partition brick to extract information from uh to uh research papers that are publicly available um so we're using uh or they call it an element um when you are taking um so like a title and abstract and an introduction of research papers like the standard structure and they call that elements um so what we're doing is extracting the content within an element um so I have a notebook you can follow it in the blog posts um but also I just decided to run it myself just to confirm that it works uh so in here we're just connecting to a rebate client and we're using open AI that you can use whichever a third-party provider that you'd like and then we are defining our schema so we have the document and then we have the source which is the source of the text and then we also have the abstract because we're really just focusing on the abstract part and this is where we're going to ask questions um so I'm importing just one paper I'm using the partition of brick and now you can see it's successfully extracted the abstract so this paper is about house price classification um this is it the problem with this is actually that the paper is in a two column format um so it kind of over ran into the introduction part of it rather than just extracting the abstract so there is a GitHub issue on actually um fixing this and improving it so it's just giving it a thumbs up all right so once we have the abstract um we're gonna import this into deviate and then now we're at the fun part where you can query this uh research paper along with the occurring the abstract so this is just a simple bm25 search where we're extracting or we're finding the paper that is related to house prices which is the paper that we uploaded and you can see that it's citing the source of paper too but that's like just a simple example just to see that we successfully got it working but the cool part is is that now you can use generative search um so the prompt is to summarize the following academic abstract and a one-liner for a layperson um so let's kind of simplify this abstract which as you could see previously was extremely long I just want a one-liner like how can I sell this to someone that maybe isn't familiar with the paper um so then you can see that using machine learning models trained with instruction and unstructured data it's like it's successfully um summarize the paper and of one liner but the main issue with this is that it's two columns so it kind of grabbed the abstract effect along with the introduction so it's not completely accurate but uh it works and that's the best part is that you're able to chat with your PDFs and the unstructured also supports PowerPoint documents and a whole bunch of different kind um so I'd recommend checking out the blog post along with the demo and you can see the different kind of sources that they support guys this is really cool I mean like I I guess this is like one of the big problems where um people have like vast amount of data right and information and then sometimes taking it out and then putting into Vivid that's usually the true like the harder part than running a vivid query often right and so so I I yeah I do like this that we have like some recommendation and that structure helps us with this chunking and setting things up even if it you know messes up a little bit with double columns that's part of the game you know that's how we learn and evolve there's also two things um so there is a GitHub issue on supporting on creating a bb8 brick and unstructured um so you can go to the bottom you can give this a thumbs up oh nice and Connor commented on it but then we also have six thumbs up so that's pretty good they're very uh Community Driven so the more likes one issue has the more likely it is to get developed so maybe we should share a link for it uh in the description later on we could add it after the after the video um and uh guess what uh Thomas hacker thinks he is a great work on the blog post amazing use case thanks so much and uh Philip is saying hello hi Philip questions um does it so this is different from the OCR application you were talking about right technically I could oh it's the same it's just like the process of it but what unstructured has is different models that have implemented this um so there's actually a benchmark this one um where the task is to uh see how well it's extracting the content from PDFs and then seeing how well um it's doing that um so yeah uh sorry unstructured has uh different models that you can use it's like layout on V3 and donut are the best performing models right now with this Benchmark so is why I highlighted it okay because it uses yes okay um you can see it all in heaven face actually and they have a great blog post as well in OCR okay I see it looks like we have a keen contributor as well by the way so uh I just wanted to give nikuja a shout out you know if you contribute like that would be amazing That'll be amazing maybe socks t-shirts Anyway said you had another box oh yeah is it always the case that regardless of what type of input file you have whether PowerPoint uh whether PDF it's always this OCR module and then it extracts the data from your input and then it and then it puts it into like a Json right is that okay I see yep and then also in the notebook from chicory he starts off with just the basics which I which I think is really cool so she's using that first partition brick extracting the first uh so this is like the title and then you have the authors but then it also has the abstract and then what's cool is that you can literally just take this uh data export it and then just Spectra is that import it into repeat so you don't have to do the like end to end kind of demo that I um that's at the end of this notebook but you could just stop here and important to be the infect Transit so many options okay yeah that's super cool because I didn't I didn't know that it was all OCR driven they were just doing optical character recognition I thought they were actually reading the file and extracting the content and then organizing it based on here's here's the title here's the abstract here's paragraph two this one but that's super cool okay yeah that's awesome thanks for clarifying things you're muted special damn it uh yes I was just saying that we don't have that many questions and uh that was a test if anyone is paying attention to me just kidding um so um I guess I would recommend everyone to read through this blog post and uh a little birdie told me that this is not the final ingestion of unstructured data type blog posts that we will have so there will be more uh just uh wink wink uh like uh let us know um and then I would like to pass on them the the room the microphone to Dan who will talk to us about something that's that we're also very excited about and that's uh with a forum hey Dan what can you tell us hey everyone good to see you again after uh quite a while so today we are happy and proud to introduce the vb8 form which um I've been cooking for a while and you can find it at forum.va.io I'm going to see a bunch of topics there um so first let me go a bit over the reasons why we have the Forum in the first place when everybody is on slack already so the biggest problem is that slack isn't indexed by search engines so if you have a question about vv8 and you search for it on a search engine maybe you're going to find a slack overflow post but nothing from Slack also if you want to join slack and check out stuff you need to sign up while on the Forum anybody can visit so you can go there right now without having to create an account and slack is great for quick conversations asking about um something very easy but conversations tend to get lost over time there's lots of messages they are fleeting and our team also has a hard time keeping tabs on what we've addressed and what's still unsolved while on the Forum you can Mark answers as accepted and can easily keep track of what conversations still need attention and then another problem for our team and also for you as a user is that when you ask something you want slack you don't know if it's been already asked before but on the Forum if you type a question the Forum will suggest similar topics so actually let me demonstrate that the form engine we use is called this course and since they have a lot of topics I'm going to try to search for something let's say we want to build a bot that lets you search by voice so I'm typing that question there and you can see that it already found some similar topics here and you can maybe already find your answer so in that regard this will prevent a lot of FAQs and maybe you won't even need to ask a question because already it has been answered so let's see how this um works with the longer threads if if something on the Forum is hotly debated and has a lot of posts like this example here on this course has 300 posts um on slack you need to go for all of those to figure out what the current status is you need to ask people but the Forum has a very useful feature called summarize this topic and now it reduced everything to only 60 or so posts and it also can summarize using AI let me go back to original View and this is yeah at its finals going through 320 something Forum posts and telling you what they're all about doing to click the summarize button ah right yeah that could be improved I'm not sure that last one is displayed there but there we go one paragraph summary kind of like talking to a PDF but here it's a very long form thread so um we would like to welcome you on our forum and to that end we've created this monthly introductions thread and would like to get to know you and you can also get to know other users in the community but also a bit more difficult on slack but here everybody has profiles you can see their bio and the link on social media handles and so on so please join the Forum and uh with love for you to introduce yourself in this thread we are going to run it monthly help new users meet others in the community also want to add rate and actually it's already a question request to deviate I would like to hear feedback from external users on what they think about this Integrations and how they're currently dealing with PDF documents yeah thanks for posting that Erica well we are just launching the forums so we are very glad to get more content and more users there and I just give you a heart thank you so am I we have a bunch of categories you can see here in my left sidebar most likely new users will be interested in a support category but if you've built something we'd love for you to Showcase it so um shukri mention him earlier he also built this demo with fixie and we posted about it and don't worry too much about where to post things the general category is um catch all and we're going to categorize it later if it doesn't fit but um you're not sure if it's a question that can have a very specific answer that's best asked on stack overflow so if there's one best answers like overflow is the place for it if there's something you want to discuss in more detail please post on the Forum and if it's something real quick feel free to post on slack and over time we hope to migrate from slot to The Forum because the Forum is much better suited to become this permanent easy researchable repository of knowledge yeah absolutely to your point uh done I think we will be encouraging over time more and more for more people to actually come to the Forum to ask questions to get the answers especially by the fact that like we see so many questions that have been asked over and over again um and then we basically losing that wealth of uh knowledge that's that's in the slack and sometimes you know people answer this question in a really nice way and then maybe the next time you answer the question maybe you don't update the time to give the right answer and funny enough like in our in engineering Martin who is very active on the slag and answers questions he actually has a document with like ready-made answers for some of their questions would you see Larry's right but he should he shouldn't need to do that I basically now marching can provide like the best possible answer once on the Forum and it should be so much easier for everyone to find and you know what I also say that like slack is great for those Communications but it's not so great for finding the info so it's not necessarily people's fault that they didn't go and search for if oh has somebody asked this right if the search is not so helpful but in this case you may start typing your question and then you have that problem like oh wow like there is uh somebody already you know like answered this so without even thinking to search you might be able to get a prompt with a with an answer or a post so this is pretty awesome and if you give good answers you'll get lots of likes you have this extensive profile page with um your top links must reply to and so on it's kind of a good way to build credibility in the community reputation nice yeah and sweet Lana doesn't know it yet like our in-house like our head of design uh but that they may be asking her to create like special t-shirts designs and everything for like the most active members on the Forum and that's definitely going to be easy for us to to see who should be rewarded by cool t-shirt right so should be cool nice do I hear any questions from the audience or hear or see nothing at the moment uh any final words anyone else here yes no I love the form looking forward to being active on it this is a I think marched was the OG he created the OG Forum in the notebook and then and then Dad kind of modernized marches for him so thank you Dad very nice Before Before I Let You Go with the next thing done um look at this like uh we have Thomas who definitely needs a t-shirt um well Thomas there will be an easy way like a help our community with like answered questions or even comment and like help us make it active um would definitely appreciate it you know t-shirts can be arranged super easy to sign up you can use GitHub or email password so we hope to see you there yeah and there's also I know you told me done earlier that we are working on uh like a Google authentication and then from what you explained to me if you use the same email for email password us your your Google account then we will be able to make that connection right that's right so we're going to add the Twitter and Google logins and if they refer to your email account that you use for both then you can sign up right now with that email and later link your Twitter or Google account very nice thank you for this presentation that was really good and I'm super excited about the Forum so uh hopefully we'll get to see more people uh coming over and then using the Forum with us having said that I'm going to take this segue because I think um I already kind of like I'm going to be one of the first users as well for the Forum uh and there's something that like I mentioned right at the start that I I find very important and it's something that I want to work on in within the company so we would like to improve how people use our clients like especially and I'll be like starting with a python client so I even created a post on the on the Forum that kind of like highlights some of the high level of like ideas that we wanted to do but but in in general the idea is like hey how can we help you maybe with uh working in to begin with python then probably JavaScript typescript next and then all the clients will follow but like how can we help you like in Python to like develop your code faster how can we help you build with it faster uh and then kind of like and then I think the first focus is going to be like how can we help you maybe create and manage the collection it's a complete like do your Vivid Vector configuration maybe some crowd operations around data how can we help you make like search to data in an easier way so I definitely recommend uh checking out this post uh but uh but I'd like to also walk you through it in terms of the some of the ideas that we had and then questions you can drop them here during the uh the podcast here but also you could drop them on the Forum later that would be definitely more very very helpful so here's the first thing that um I kind of want to introduce like the uh the concept of collections so that in general like in if you look at the nosql world uh like just like in SQL you have tables you know as keyword you have data collections I think in Vivid we used to call them classes but I would like to start calling them Collections and that that might be a little shift at some point um but the whole idea is that we want to create this API that's super easy to work with right like so if you wanted to create a new collection it should be something as easy as like hey client.collection.create right and then just provide a name and that will create a collection like default configuration you want to say specify which vectorizer you want to use no problem we would like to have like this Vector config where you can specify the vectorizer so in this case maybe I want to use open AI or it could be hugging face or which or cohero whichever other model you want to use and then if the cool thing about maybe using this Vector config is that that it should it is like a class right in this case so if you start typing in your IDE you will start getting prompts from ideas saying like oh so here are the possible parameters that you can pass in maybe you can pass in the name or you can specify which model you want to do etc etc the other cool thing that I like would like to introduce is what if instead of specifying which properties to be vectorized uh when you define the properties what if we could just do it as part of the vector configuration so you could say like hey I want to use cohere with multilingual model and I want to vectorize the title and description right so you don't even need to think about the data schema but you can actually say like my data might have like 20 different properties but these are the two that matter to me and that could be pretty cool in this case and then separately if you do want to provide the data schema itself I would call it like the properties so you could create like maybe an array of properties and then again would have this property object that will give you prompts in your IDE so you could say like okay I could give a name description data type etc etc and then you could create again a new collection with this configuration it's pretty cool I I think that from me reading this should be a lot easier to kind of like follow and read your code and also like typing all of that will be way way easier um the other thing if if you want to like get information about how is it how's your collection configured uh you should be able to for example go like hey I want to go to the collection and that get the configuration for articles and then you could for example print your vector config or properties Etc or separately uh I mean additional options we could either say dot get configuration or we could have dot configuration.get right that'll be some something that I'd like to hear feedback maybe from the community which approach works or maybe there's a third one that we haven't seen yet um so it'll be definitely cool to hear from you um the other thing is if you wanted if you'd like to update a configuration it should be as straightforward as just saying like hey I want to take collection update configuration and then maybe you can pass the vector config or properties and then and send that update um you don't want to use a configuration anymore sorry a collection more you should be able to just dot collection dot delete past the name boom done right and then that should be super straightforward to work with um then the next thing is like around data operations and we have more examples around but I intentionally don't uh we don't have like a extensively all the possible examples but if you wanted to operate on the data right uh what you should be able to do is just say Okay I want to go to collection and get this data object which is kind of like something that just uh creates an object that connects you to this specific collection right so so this data object then I can say for example hey I want to go and insert an object with like a full bar right or I could insert a whole bunch of objects so you could actually pass in an array of maybe four objects in one go and look how elegant this this is right like it's actually pretty straightforward you have this data and then you just insert objects into it straight away so I hope this is going to be appealing to you um if you just want to get a bunch of objects uh you should be able to do something as simple as oh this should be they should just say data but you should say like data.get five objects right or maybe you could get the data with a wear filter and then again that that should be straightforward and then updating an object could be something like hey I want to update the data by ID by or uid and then you could pass in the uid and the object that you want to update um dealing an object could be as simple as data.delete and then pass in the uid OR data.delete and then you pass in the where filter and and then that could kind of that would automatically delete all the objects where the price is greater than 100. um should be kind of cool and then for search I want to have actually a separate post for it but just to what uh with your appetites here uh for for the next meal um the idea is that we could reuse the same data object and then you could do something like for semantic search like what we call at the moment near text maybe we could do something like data dot text search then you pass in the concept properties that you want to return and then the limit so I was like say I want to want only one 10 objects so in this case we would be going away from the the Builder pattern that we do at the moment into something that should be more readable and then similarly we could have something like data dot image search or data dot hybrid search right and then each of those would have their own parameters and of course there will be like a more complex one that it would be like a catch-all sort of approach um but yeah like I'll be I'd love to hear your feedback about it uh because there's definitely something that I would like to introduce and then we are at the moment in the face of uh Discovery around what are the good options what people may may find useful um and it could be around the syntax but also ability you like to use objects like this right where if you want to specify filter then we should be able to tell you that oh when you use filter you could use property operator value the the IDE should be able to help you but also the operator instead of a string like it used to be so far it could be an enumerator right so we could pass in an enum so you will never get it wrong like the number of times I have to go like oh yeah I want to run a wear filter and I was like okay so how how did we call like uh less than right is it less than or is it fewer than or what what is the term right is it all lowercase it uppercase you don't worry we just have like maybe operator dot right and then you see all the options and I think that's pretty cool so yeah that's me uh in terms of this topic I love where this is going uh looks like somebody got some inspiration from mongodb in some places uh maybe maybe yeah like um well worked a few years so you know it helps I'm curious if um well what do you think of um portability of code from the graphql syntax because a lot of users play with uh the console to consider queries and then um how easy would it be to copy that query paste it into your client and with minimal modifications arrive at client code yeah I think that that's the thing I'm trying to go away from because just because we use graphql underneath uh first of all does it mean that when you write it your code in python or JavaScript that you should be aware that there is graphql underneath so in this case I'm trying to create something for python developers that they can think python first that's the idea however if you do have your graphql query ready and you just want to you know take it and run it we could for have like a data dot row graphql search right and then in this case you could just pass into the raw string that the problem with graphql is that you don't get any of the support and and type validation on thing when when you run it directly in python or when you run it directly in JavaScript and just that just becomes very hard so uh yes the intention is to actually pretty much hide the fact that there's graphql on the underneath it's not that we are ashamed of it or nothing at all it's just you shouldn't need to worry about it you as a patent developer or JavaScript developer all you want to do is like I want to delete an object or I want to query and run a you know a similarity set you shouldn't need to worry about how we structure it ourselves right and that also cover the separation between the rest API and graphql because graphql is read only and the rest API is separate formulations yeah exactly right so in In This Moment For example when we have data.delete like you don't need to worry is is a graphql there or is it a rest which one do I call it doesn't matter to you right like you want to call Dot get that's a rest call you want to call uh near text that's a graphql usually right so your head was kind of like spinning am I doing this or am I doing that in this case you don't need to worry about it that's our problem if and if one day for some reason we decided to to move away from graphql completely and have something else instead um yeah it might be a hint like I'm not stating anything just now but there is a possibility and we have a question um is there a plan to add this kind of validation when using client dot query dot raw um I'm not sure if there's a validation around that graphql structure I think that's like a really difficult problem uh I don't know um I would have to ask Dirk who's actually mostly working with the client um so the so the raw thing for now I I want to stay away from for now but I want to create something that allows you to very quickly write code like um and I'm also doing it on a selfish point of view like sometimes I go to events and I speak in in front of large audiences and I like to you know whip up my IDE and kind of like start writing code I want to be able to just write super fast code uh that that is super easy to read where I don't even have to explain what I'm writing people should be able to like see it and understand oops he's yeah yeah this is that last point I wanted to I wanted to highlight because it's I think it's super important because right now um uh what I so coming uh coming from a background in Python development and this uh right now the proposal is only for the python client but that what we want to do is make the clients feel more uh more native to the language that they're written for so if you're a python developer you should be able to use the python client and it should behave the way your other python libraries and modules behave so if you're a data scientist that's using scikit-learn and all these other uh python libraries we want we want the python review client to behave similarly so you don't have to go out and learn something outside of your comfort zone it should just be it should be second nature as a as a python developer to you and then same thing for for all the other clients and you don't need to worry about Lake Sebastian said that graphql is actually what's happening underneath hood or I need to like uh write in strings that that are then easy to turn into graphql we should take care of that on the back end and I think that is going to um help improve the developer experience significantly like the time to writing your first query the time to inserting your first object into Eva I think I'm super excited about this yeah absolutely excitement yeah yeah so I will probably do it in like a small phases too to begin with it's not going to be a big change to straight away um and one thing that about doing things like client dot collection because there is no collection namespace at the moment so the idea is that we could do it side by side so this change is not going to be a breaking change for anyone what you do so far will continue to work um but then once we figure it out and and get approval from the community in general we will over time deprecate the the old approach and then use this as a new one but easily for at least six months you will be able to do both uh side by side and I'm sure the new developer experience you'll be won over by by the new approach and that that's not going to be much of a fight the other things that we can also introduce is like in python or most languages you have the four in loops uh what if we could have like something like four four in data dot iterate right and then in this case we could uh automatically give you a system that you could create an iterator that uses the cursor API for you and then you could iterate to all of your objects uh with with just like one line of code right that would be really cool um I just realized because I'm not sure if I answered Thomas's question very well but um by the way like thank you for asking like the question I want to just come back to it and I would definitely take it as as a feedback as a challenge kind of like hey can we address that so just because I didn't have a good answer for it yet that doesn't mean that this question wasn't valid it's absolutely valid uh and we'll definitely look into it because if there is some uh log maybe an opportunity to validate your raw queries from python why not maybe we could uh maybe we could do something so we'll give it a go I just don't have the answer myself cool cool so um yeah let's let's hope to to have some help from from the community on the Forum I will have like a nice discussion around the unstructured and the clients and then we can make it work foreign s um so next one I have done to talk to us about the new how to search pages right so done take it away I think you still mute it hello everyone it's been quite a while since I last spoke to you in the meantime we've launched a bunch of uh how to pages which we aim to be task oriented so the first category is how to search and uh here on our documentation site you can start with the search basics and it's very pointed examples so how to get something directly this is the code how to limit in return objects how to paginate and so on and for similarity search probably the most often used feature you just have the code with near text so these pages are um probably going to become your first hit when you look for how to do something specific we create so far we've covered search almost exhaustively last night we published the generative search and a lot of this work has been done by my colleague JP who unfortunately is in Australia and cannot attend the movie there and um these days we are working also on how to manage data we have a couple articles here and I'm working on cross references and that should be ready before the end of this week so um again we'd like to invite you to check out the new how to's before that we had the install and configure and now we have managed data and especially in search which is probably going to be your most referenced section on our documentation yeah I really love the new search how to search Pages uh honestly it's very similar to what I was talking about the python client uh where at the moment if you want to know like oh how do I do bm25 search especially if you're new to evade uh you sort of have to go to like our reference API to then go to graphql and then suddenly like you have to know where things are well in this case we just say like hey you want to run a bm25 search here's a page here's all you need here's what you need to know um and then and I think um over time this is going to take over uh a lot of the con content from those reference Pages like let's face it those reference apis should be references not where you go and learn everything so uh yeah definitely love it and um there'll be quite a few more coming up for sure and then also when we have the new apis with the new collections uh I think we'll cover it here too Erica you unmuted yourself for a brief second oh yeah I thought uh we're kind of tight on time and I want to show the recipes really quick yeah let's do it let's do it so um thank you for for uh presenting this uh so Erica tell us about recipes yeah um so this is also complimenting the work that Dan and JP have done with um how to uh so if you don't mind sharing my screen this is with inspiration from the openai cookbooks uh we're calling it the weeviate recipes so we're chefing it up and the idea is to give you an easy way to take this code and then just run it yourself um so starting off with a similarity search um right now it's organized in a way where each third-party provider has their own folder but here is just one example of the running similarity search with open AI embedding so first installing the dependencies um configuring it so this is where you would put in your openai API key a link to your BBA instance and then authorization if you have that set um this is an example of a schema so the data that I'm used seeing is the same exact one that den and JP are using so the Jeopardy questions which is just 10 objects but it just gets the point across and gets you started fairly quickly so showing you how to create the schema and where you specify the vectorizer um especially I feel like sometimes this might be overlooked you are providing your API key but then when you uh you forget to include it in your schema so the goal is for people to just take this notebook and then replicated um themselves and how to import the data and then um since we're doing similarity search I've shared uh three operators that we have in movie for text objects which is near text near object and near Vector so you can literally just copy and paste this and take things out as you need but yeah and one thing is like I said each provider has their own so right now I have cohere as well this is a work in progress this will be done by the end of the week but if you you would like to see specific features we can have this discussion in the Forum um so I can kind of know which area to focus on next so this week is similarity search but I can do all types of things maybe next generative search Hybrid search the list goes on so I'm looking forward to feedback on this and thank you said for the emojis that's hilarious very nice yeah I think that the idea is that with those recipes this could be a place where people can come like hey I have like a specific thing in mind I don't necessarily need to read a ton of text explaining what's going on I just want to read go through the code and there'll be like a brief explanations and I think that I'll be good and python is again we'll start with python but I do already have plans around like uh JavaScript typescript examples to also show people like how to do it in the in those worlds yeah it probably shows that I have a JavaScript background right so I'm biased but obviously in our world python is like one of the most used languages this is amazing I'm super excited and um and I guess and we're open to contributions as well if somebody thinks that there's a recipe that we need a pull request could would be welcome um so be cool we'll be cool um here's okay so I have a question that is unrelated to anything we're talking about that comes from the audience uh so somebody asked is it possible to group by multiple attributes in well I assuming um So my answer is yes it is possible uh yeah you could just provide multiple as part of it but that could also be a good question on on the Forum uh so if you find like more people are asking this like uh we could post it there and then we could give you an answer unfortunately I can't share links on YouTube otherwise I would have pointed you in the right direction uh but uh yeah I can definitely say it's possible so thank you for the question but yes now we can come back to to the recipe so anyone like uh any any ideas anyone on what would you like to cook or help Erica cook buy multiple properties I mean yeah like so it could be like a cook recipes around various aggregations right I think it would also be really cool to have um like popular Frameworks and then recipes on like for example what I showed earlier with uh with the private GPT stuff uh retrieval question answering or retrieval augmented uh generation how do you how do you use a framework like land chain with leviate as a vector store to do retrieval augmented generation or how do you use it to implement Chain of Thought and then like if I wanted to do that for myself I could just start off with the recipe and then 80 of the work is done and then I could modify small lines of code here or there I think that would be super awesome yeah I agree nice nice perfect um all right in this case unless there are any final words who will do the waving things we're like we and uh and thank you all for watching thank you for spending this time we really appreciate uh you spending an hour with us I hope you enjoyed watching uh what would be hard to share there's always so much to share uh and and we already are we already have plans for the next release will bring more people I think the next one is going to be pretty crowded which is making me super excited uh so this is going to be great but until then thank you for your time thank you for watching have a great day evening morning whatever it is and see you next time bye everybody bye thanks thank you ", "type": "Video", "name": "Weaviate Air \u2013 Episode #8 - Forum, Recipes, Demos", "path": "", "link": "https://www.youtube.com/watch?v=mzEq9an0NAU", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}