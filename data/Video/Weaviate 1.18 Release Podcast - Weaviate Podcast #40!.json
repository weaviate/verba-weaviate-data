{"text": "Thank you so much for tuning in to another Weaviate Release podcast! Weaviate 1.18 is packed with new features! Bitmap ... \nhey everyone thank you so much for watching another weeviate release podcast I'm super excited about Ouija 1.18 the big theme of this is being speed memory savings some ux improvements like adding filters to hybrid search uh so I'm super excited to welcome we've a CTO and co-founder Eddie and dylocker to explain some of the new updates to Eva 1.18 hey Connor thanks for having me awesome so we have such an exciting list from bitmap indexing and update to hsw with adding product quantization and then the wand algorithm for bm25 and hybrid search so can we kick things off by diving into bitmap indexing and what's new about that yeah sure bitmap indexing is is one of those things that the first time we saw it was almost too good to be true um I think at some point I I tweeted out where we had like a thousand times a speed Improvement on filter query and that was not just some marketing stuff that was really we had a filter in a specific combination on 100 million uh data set where a filter could take uh I think something like five seconds and now that same filter would take five milliseconds so it's really a 1 000 times Improvement and the idea is basically we use for for filtering we use the inverted index and the inverted index is basically the the ground truth of what is allowed or disallowed and then we pass that we call internally we call it an allow list we pass that allow list to the hnsw index and say like hey these are the X number of objects that you are allowed to to use in your search basically um and the problem there was if a filter suddenly matches the entire database on 100 million objects then you would have or you would spend so much time building up that filter from the inverted index that all of a sudden like even if the vector search for for a filter that matches the entire database the vector search doesn't really care right it's like it's almost the same as an unfiltered one or it is the same as non-filtered one if it matches literally everything in the database but that mere fact of building up the filter that took so much time and then we thought okay what what's going on what what is uh happening with our current implementation and what could we improve and we came across roaring bitmaps which is basically a a perfect sort of fit because bitmaps or bit sets in general are made exactly for this like is something contained yes no and uh it works with with basically with increasing IDs which is exactly what we have because with the inverted index already so perfect fit only problem there was um that sort of if you store your your inverted index in a specific way and then you convert it into a roaring bitmap which is what some other databases do then you still have that cost of storing it in a in a different way basically you still have that retrieval cost and we said hey there's this one thing and and that's that's another tweet that I put out where at some point I said like we made that crazy decision to basically build everything from scratch in nvidate and and that is one of the points where it really really pays off right now because we just put those roaring bitmaps straight into the storage layer like in our LSM store itself and that means we we don't have to do any kind of um and he kind of retrieve then turn into a bitmap so so so for example um if you you could do it that way if you had um say a classical storage engine that would just Store The Roaring bitmap then you could still sort of retrieve the Roaring bitmap and you could do fast intersections like for an and or or operations once you have the bitmap that would still be fast but you'd always have that that cost up front but because we could build it straight into the LSM Store that cost is completely gone and that gives us latencies in the in the single digit milliseconds for that that Filter part so I think on the the 100 million was um our evaluation case because there before we ran into those five second latencies and then on that same case now a simple filter that matches everything is something like a millisecond or maybe two and then even if you have like some complex operations like like range queries or or merging different filters this and that or that like with multiple operands that's all super fast right now which is is incredibly cool um one more thing to say about this um of course this is this is a very different way of storing and handling data in VBA in 1.18 than it was in 1.17 so listeners might now think wow what do I have to do like is that is that going to break my my entire VBA instance the good news is no it's not so if you don't do anything if you just upgrade to 1.18 it will just fall back to what it's already what it's already used to but then you also won't have the speed benefits but we also have a migration option which you can basically run one time you just start a bb8 with an option say like hey migrate my my index is to Roaring bitmap indexes and then it will run through your objects once basically with the first time it starts up and vv8 will be in in read-only mode during that phase because of course when it rebuilds the entire index then you can't have rights coming in but you can still use it normally it's it's only read-only and once that is complete a readout only mode is gone and you have the benefits of super fast filters in VBA 1.18 with your data that you imported in in 1.17 . yeah that's extremely interesting and yeah that detail is super cool that you don't that you can just kind of update it and um yeah so maybe kind of coming back into the application and motivating a little further I remember like reading your article about pre-filled about pre-filtered uh Vector search and 50 milliseconds and this kind of thing and I always thought initially this idea of adding symbolic filters to the vector search was such an interesting idea learning more about how you use this allow list structure with the hsw like how technically that's implemented so maybe kind of talking a bit about the applications and the tests a little more um I really like these things like when we interviewed Sam bean from you.com he described this idea of like a vertically integrated search engine so say you want to search through just Reddit or just Wikipedia that's one case where you then would put this filter on the vector search and maybe if we could just talk through some more examples of filtered Vector search and what that looks like yeah that that's a great example um one that I always think of because I think where where users are most used to filters is e-commerce like if you order something from a web shop you would let's say you would filter the price range because you want your object to match a specific price range or you would filter it by a specific uh product attributes so if you're looking for a new uh VBA t-shirt it has to be navy blue and then you put navy blue on it and then you'll you'll get the the right one um not not for me I only wear black shirts I guess um no but but these are these are the kind of applications where users are so used to applying filters maybe without even being aware of it because it's just it's just natural and then yeah as you said I really like this idea of having a massive data set and sort of narrowing it down on the Fly you're saying well now I just want to filter by yeah only only search through through Reddit for example and um by having that that sort of big Vector space and having the ability to set those filters on the Fly you also don't necessarily need to know up front right because you could also say if say a vector search engine wouldn't support filtering you could just say okay I'm going to split it up up front I'm just going to here is my collection for Reddit here's my collection for uh I don't know the next thing um and then you search them one at a time but that only works if you can predict entirely what these kind of combinations are but if you can dynamically sort of combine these filters and that is also something that that that vv8 already supports but that will be way faster with bitmap indexing just because it's so cheap such a cheap operation to do like an intersection on a on two bitmaps basically so you can do that dynamically so it gives you sort of more uh flexibility later on like yes there are cases where you maybe want to optimize something by splitting something up up front like I'm not not saying that those don't exist like in those hyper optimized cases for when you use case but in the general just throw something at it let VBA deal with it and figure out what you want to do later without being sort of blocked by a previous decision that's that's nice and that's going to be much faster now than it was before yeah that example of uh chaining together the wear filters is so interesting like uh Navy like a t-shirt navy blue uh price between 20 and 40 dollars I'm not sure how much a sure it costs but but we're using my shirt but yeah so another question I got uh when I was at the New York Meetup we get Meetup was about role-based access control uh would this enable that interesting interesting uh question yeah so so yes and no so um there if you want to do or it depends a bit sort of where you do your role-based access course so maybe for for people who aren't aware role-based access control is basically a very fine-grained Access Control mechanism where different users have different roles and then one role grants you rights to specific things so basically by default you have no rights to do anything but then you could have right it could have specific roles and basically through those roles you inherit permissions to to access something whatever that something is that could be something in in the context of of the application could be something in the context of a vv8 basically a specific object so if you do set such a property on your vv8 object and then in your application you would make sure that you um that you set those filters to match the specific either the specific roles or the specific permissions you could definitely do that with with those filters the sort of downs are maybe not downside is just so if it's a it's an engineering decision and it has trade-offs and basically what you need to do then is make sure that the user can never find a path where they can basically skip that kind of filter because like if if for example there was a way that they could all of a sudden have an unfiltered search and they would they would search across um basically everyone's data which is of course what you want to prevent so um depending on your kind of security requirements it may also make sense to have the the role-based access control directly in the database and not sort of on top of it but that that that's basically that that you need to decide that from a use case to use case perspective perspective of how you would want to integrate this this by the way also a question that also comes up um every once in a while and I've been meaning to write a blog post about it but I haven't yet is multi-tenancy because I guess sort of the the role-based access control is very similar to to the multi-tenancy case where you would have um say your users are grouped by something and then how do you represent those user groups or their companies or whatever it is that Associates them to one thing how do you group that in bb8 and typically um what we do there is we do it on a on a class basis because that sort of gives you and and that's why I'm bringing this up as opposed to doing it in a sort of Monolithic large class and then set individual filters if you have it per class then you have that separation like the the way that a class is created in mediate it's very it's like everything is isolated basically it's it's a separate folder on disk it's separate files on disk and there's basically no way that that one tenant could influence another attendant but that said multi-tenancy is also not the exact same thing as a sort of fine-grained access control so but but these topics overlap so I thought that was that would be worth mentioning as well at that point yeah that's extremely interesting so I think that's a great coverage of uh bitmap indexing um you know beginning with the technical details and then you know talking about the applications the filtered search and I yeah just I think it's important for people to know that you can filter the vector search it's not just Vector search you can also add these symbolic properties add things to your data like you would with any kind of data management system and you can achieve faster Vector search by filtering with the properties and that's probably one of the most exciting things that we've in my opinion I love this kind of filter Vector search thing uh so so then stepping into the meat of the vector search the vector index so H and swpq what does product quantization add to the table yeah such a such a cool big topic and I'm so happy that we we finally have the uh the first release so I think we we teased something late last year already when when all of us were together in Italy and this was one of our our presentations that we had there um where the overarching goal is basically to reduce the operating cost which is mainly driven by by memory consumption so sort of goes hand in hand like if you want to look at it from Tech perspective we want to reduce memory usage if you want to look from the business perspective we want to reduce operating costs but really it's the same thing um that was driven by by sort of okay how can we get the the usage down basically without without sort of any making any sacrifices I mean what you could do is turn off the vector index like that would that would reduce your memory index but it would also make it kind of pointless to to use VBA um so so yeah how can we give the same kind of uh uh quality to the user while making it a bit cheaper to run and there we have basically this this two-step approach to it in hnswpq is that the first step the two step or the second step starting with the the sort of longer goal is a fully disk based solution so disk based solution basically means what's on disk doesn't have to be in memory so therefore it's it's cheaper and ssds are fast and if you optimize it for ssds there are certain ways where you can you can make benefits off those disks and still have a very similar um experience that said there's also sort of like you you can you can never run any kind of program without memory right like if every little thing was loaded from disk the minute or the second or the millisecond or nanosecond that it was used it would become very very slow so you can basically put parts to disk and you keep some parts in memory and put some parts to disk and the kind of idea in any disk based system so so Abdel for example published this comparison of vamana which is what what Microsoft's disk and uses which technically is quite similar to hnsw because it's also graph based single layer graph instead of multi-layered graph but it's still still very very similar so basically with all of these systems you need to keep something in memory and and the idea is you keep sort of something small and cheap in memory and have the large inexpensive stuff put to to disk and now to loop back sort of what is the the small and cheap stuff that you can keep in memory and what you find in all of these these ideas is compressed vectors and this is where product quantization comes in so we had matiz Do's on on the podcast uh who which is basically just div dive into into product quantization so he explained this way better than than I could and then I could in in like 30 seconds but the general idea is that that product quantization is a form of compression it's lost full compression so it's not lossless compressions Lost full compression and you can tune it um in in the simplest case you would have um a single float 32 Dimension which was which originally in the continuous Vector is four bytes would be turned into a single byte so we you'd have a four to one compression but then you can you can basically compress it further by having a multiple multiple Dimensions per segment so you could get to like eight to one sixty to one thirty two to one um and this comes at the cost of dropping recall so as with anything there's some sort of a sweet spot which is if you if you want to have the exact same kind of quality as you have with hnsw right now then The Sweet Spot is probably four to one compression um but if you say well I care less about recall I care about running billions of vectors cheaply then maybe your personal sweet spot is at eight to one or sixteen to one or thirty two to one and where abdell is currently working on a blog post um illustrating these these kind of kind of trade-offs anyway long story short um compression is a vital part of any disk based system and we we thought well we we we're currently developing this disk based system and it has a certain timeline but can we create value for the user sooner than that and then we thought like well hnswpq is basically half of this picnics like the 50 of this skin and it has the compression it still has the the uh the vector index and the only difference really is that the the vector index is still in memory and not on disk but the vectors if they are compressed four to one then fictional example let's say your current memory usage is 10 gigabytes of those 10 gigabytes two is the vector index and eight are the the vector embeddings then um and now I'm glad I chose round numbers that are easy to calculate then if you compress those eight gigabytes of vectors uh by four to one and then they would turn into two so basically your um you still have the two gigabytes for the vector index plus the two gigabytes for the compressed vectors so instead of 10 gigabytes overall you now have four gigabytes so you basically had a sixty percent reduction in memory usage and four to one is such a low compression that you're probably not even going to notice the the the um the drop in recall so it's like almost almost free almost for free you've got a 60 reduction in cost and then said you can drive it even further yeah that's incredible and ABDO has done such an incredible job with this blog post I'm not sure if this is uh the blog post is published when we're publishing this but I highly recommend people checking this out uh this blog post really helped me firstly understand the memory requirements of hsw uh the blog post describes the calculations of uh with hnsw you have this Max connections parameter and then you have to store this number of parameter uh connections and with like n16 and describing the details of what that creates an overhead as well as the compressed Vector as your calculation with the eight gigabytes two gigabytes and that reduction um yeah and then I really liked what you said about like if your goal is just to run a billion vectors compared to a million with the recall trade-offs uh basically you have this decision of whether how you want to group the vectors so like if this is a 32 dimensional Vector you you know segments of two right and so that's that's how whether you increase that you'll compress it more but at the cost of lower recall and it comes back to that I think a n benchmarks thing you did with the you know showing the recall of approximate nearest Neighbors yeah I think all that is just so interesting um something I'm really curious about is explaining further the um the K means and how that's used to Cluster the vectors and kind of what the overhead and the thinking is behind that yeah so for for the compression algorithm basically to reduce the amount of data you need to sort of build groups you you just uh said that example of taking two dimensions and putting them into into like groups of two so then the questions becomes what identifies a group like how do you pick your groups because you could pick them very very bad basically where the distance between two points in a single group would be very large and then if you put all of your your groups served on top of each other in a graph all your groups would suddenly be identical and then you'd have the problem that that well what's the point of having this many groups if they're all the same so basically you need to to pick your groups for and without going too deep into what uh how product quantization works under the hood but basically you need to distribute your data in groups that have no overlap and where something that's within a group is actually similar because if you if you use that for approximation um then yeah if if it's dissimilar then basically you you that's why you recall drops and k-means is a very simple clustering algorithm to that basically does exactly that so if you imagine all your your vectors like a 2d graph plot it somewhere there are naturally going to be clusters and then k-means is just an algorithm that identifies those clusters and it's it's by no means perfect but um it has like a an exit condition basically where it stops running if it can't improve further um and then you have ways to to um sort of run it over and over again to to find like the the the best possible clusters and that is essentially what's used in PQ these kind of kind of it's like the I would say the initialization in PQ is is this K means uh or these K means clusters because these clusters then represent the individual segments in in um in product quantization and uh then something that that I think that will also be uh mentioned in uh abdel's blog post or has been mentioned depending on whether it's been published already um you don't need you don't necessarily need all your vectors to produce good PQ produce a good PQ code book or good uh PQ clusters so in the example that he runs in his blog post I think he uses 20 of the data and um then it runs under the assumption that the remaining 80 of the data is distributed in a similar way as the initial 20 and like this this assumption becomes true depending on the size that data set and of course if you if you think of um yeah if you think of your your data set in real life topics if your first 20 happen to be about um navy blue vv8 t-shirts and then your remaining 80 percenter all of a sudden about a car parts or something then that distribution that that assumption doesn't doesn't hold true but typically that's not the case like typically you have like a slice of your data that's somewhat distributed and then if you add a like a light blue t-shirt then it makes sense that that you would find the existing group for for the light blue t-shirt and then if you add a new uh I don't know headlight then the headlight the LED headlight would be close to the the whatever incandescent headlight that's already in the not sure if they're still used anymore but uh yeah it kind of runs under the assumption that um these clusters still make sense and typically they do which is nice yeah that that notion of like distribution shift and how it applies to Vector index structures are super interesting I've read this paper Ood disk a n that yeah touches on this problem and I definitely think that'll be something that uh to keep an eye on I I found that find the k-means thing to be so interesting uh like for example we were comparing k-means with umap on our Bert topic podcast and in the case of Bert topic you're trying to Cluster Vector spaces to understand the similarity between clusters so uh so k-means has a bias on trying to evenly distribute the space with the centroids which makes a ton of sense for product quantization because you're trying to compress the vectors with the centroid IDs so it works great for that whereas umap is better for like if you want to have some odd shape where Martin uses this example where it's like a circle with a dot in the middle and K means would just try to like cut that up rather than um you know clustering the circle around the dot so it's very interesting to think of the trade-offs between the clustering algorithms is very cool to see the k-means implementation in go Lang and leviate and seeing the clustering coming as a part of the vector uh analysis stuff as well so so awesome so I think that's so exciting the opportunities with reducing the memory requirements of vector indexes and yeah all that making it more accessible to create billion scale maybe if we like dream a bit about this like how much how much can these kind of savings help people with you know scaling up their clusters or the things they imagine doing but limited with the cost and thus needing optimizations like this yeah yeah I I think the the biggest uh sort of enabler or this is going to be a big enabler for cases that don't necessarily have a lot of traffic but have large data sets so if you think of the other data let's say someone's building a new type of search engine and that would hit millions of requests per second they would probably probably not mind that the setup is relatively expensive because just to be able to serve millions of requests per second you need large infrastructure anyway so it doesn't matter if the memory requirement is a bit higher because you have I don't know tens or hundreds of CPUs anyway so that kind of goes hand in hand but for those cases that are maybe and I think we also find them a lot in the sort of more analytical space and maybe on the research side where you have a massive data set but your requirement is not necessarily to get like real-time information if the query takes a bit longer you do it async that's fine your your goal is basically to get that information and then you can't justify having a cluster that costs like multiple thousands of dollars a month then you need a cheaper way and and in that case you're also probably very happy to to trade off performance because well you don't have that many concurrent queries and that is I think these cases are going to be enabled because you can easily just say like okay here 50 60 70 cost reduction and that's only the first step right like with um if we get the the go for for the full blown disc based solution then the reduction will be even further but already um I think we we can enable these these kind of cases where vv8 maybe right now would be sort of a nice solution but just isn't viable and then it becomes more viable for for these yeah that's super exciting uh super cool so if we're pivoting topics a bit uh we have two more uh like kind of update things then coming back into a speed Improvement another algorithm to talk about um could you explain the cursor API and what that adds to aviate yeah so we have I think in in this release and the other one we haven't talked about yet but in this release we had the two highest or most requested feature requests from our community and uh the second one that we're going to talk about later um is now the the highest the most upvoted one but before that was the case this one the cursor API was the the most upvoted one um and the the cursor API is if you think about it it's a super super simple construct but it's so uh useful so before the cursor API existed if a user used vv8 as their primary database um then at some point they they might have a requirement could be for for auditing purposes could be for something completely different but basically the requirement is how do I go or the question that they would ask is how do I get all of that data out of vv8 again and then in the past we would say well that's kind of like maybe it isn't designed to get your your data out maybe it is designed to to get your data in and then search your data and you can like if you find the right search queries you can find everything that's that's in that uh database but there was basically no no way to get it out and then um people would try to use like the pagination feature as a workaround but the pagination is is built in a way where basically each each higher page becomes more expensive than the last so you could do that to a certain degree but then typically after 10 000 elements you would run into some sort of an error message where V8 would tell you um you've reached your I think it's called query maximum results and you could increase that but there's also warning that that comes with higher costs and that you shouldn't necessarily do that and now if you want to get a million or 10 million or 100 million objects out of basically there there just was no way and then you could build a workaround where you would every 10 000 object you would assign a a field to a unique value so you could set like a a page property on your object like a fictional page and then every 10 000 you increase the page and then you could set a filter give me page seven or something and then you could get those 10 000 that would match the the fictional page seven but that's of course super complex so all of that throw it out of the window don't have to do it anymore now we have the cursor API the cursor API is a super super simple construct you basically start somewhere with an ID and if you don't know where to start then you just start with nothing basically with an mtid and you say give me a thousand objects from the database and then from that list a thousand objects they are in a specific guaranteed order and the last one you can take the last ID and then you can just take that ID for the next page so to speak to bb8 and say give me the results after this ID and then you get the next ten thousand and the cool thing is that this this query has a constant cost so it doesn't matter if you've already retrieved 500 million objects and now you want the the I don't know how how many if page basically to get the number next object it always starts a new sort of retrieval process at that point that you specify and now you can in constant time basically get every object out of your database which makes it we've added the backups feature which allows you to get every object out if you want to get it back into Eva basically but now you can also if you want to pipe it into a different system like you could you could use that to stream your data to something let's say you would have a specific or let's say you want to train a model Based on data that's in bb8 then how would you get all the data out of it you get well you can do that now with the cursor API and you could either do it in one go and basically just export it and put it maybe in a parquet file and use that or you could yeah do it on the Fly and sort of go through the cursor and do something with it yeah that case of training the model is what inspired me to click on the thumbs up on the on the roadmap thing and uh yeah it's very interesting like getting the data out like imagine like just for the case of you maybe just want to have your data somewhere else again just because it's your data and you want to have it as secure as possible and as effective as many times in as many ways as you can think of I guess but um this kind of idea of getting the data out for the machine learning models I think that's so interesting I I also think you could use we V8 to retrieve the next batch of training data kind of similar to Mosaic ml released this streaming data set thing and I think we could be used in a similar way to get batches of data for training so so yeah all of that I think is extremely interesting um so if we also come in now uh pivoting topics again um so adding the filters to hybrid search uh maybe first we could start with just like what it is and then I'm very curious what the technical challenges behind this were yeah so this this is the other one that was most requested so in in 1.17 we um released um or we had bm25 support before already but I think it was considered experimental and we removed that flag and then we said like here it's General availability in in 1.17 um but it did not support setting filters yet so it's exactly the the same kind of use case as you as we talked about before on unfiltered Vector search so same we can use the same example of the navy blue t-shirt or combine it with price tags and everything um and you couldn't do that on a bm25 search and in turn because a hybrid search is both a bm25 and a vector surge that also meant you couldn't do it on a hybrid search so in one at 17 we have this awesome new feature um of hybrid search which um we we see that it can improve the results running it over over a specific data set benchmarks to enable it you see that you can get a higher ndct score and higher recall and everything which is super awesome but now you're missing that kind of other functionality that Vivid already has for for Vector search so it was only a matter of time and um this was accelerated by I think our community of voting that ticket like crazy and it it really became the the most upvoted ticket um very very quickly and um yeah that's that's in Viva right now so you can now nothing changes from a search or if you let me put it this way if you've used a filter in vb8 before you already know how to use the new feature like it's literally the same wear filter and the only thing that's different now compared to 1.17 is that that error message goes away that tells you uh filtered search is not supported yet um and then now you can you can do it uh from an implementation perspective um filter filtered bm25 search is actually not that difficult because we already have the inverted index which we use for for Vector search which by the way also means that that these features that happen in parallel like if you set a filter that's uh that's using the bitmap index that we talked about before then all of a sudden these will also now be much much faster so there the good thing is that these two parts are a bit decoupled so that wasn't necessarily A a big change to get them working into vv8 however we've also completely Rewritten the way that we do bm25 scoring because in 1.17 the initial release was a um sort of a let's call it a primitive approach where we'd literally score every term that was matched um but now we've introduced the wand algorithm which is um yeah another way to to improve the retrieval a bit and then that tied in nicely with the filters because we just did that that all in once super cool and um yeah awesome so yeah that's very interesting how the like I was very curious like how the allow list kind of concept generalized to the bm25 scoring with the inverted index and the other connection to the bitmap indexing that's also interesting um so yeah so maybe stepping now into the wand algorithm um I I had to give Dimitri Khan credit for he's the first one who kind of taught me about this kind of idea where he's like oh you could probably have some way of uh sorting the terms so when in bm25 you have a query like how to catch an Alaskan pollock I like this car and you like um you know you start scoring how to catch with the matching and you can probably you know have some efficient way of scoring the documents like these have matched so many words so far that it's like highly unlikely that the match would be high enough so I don't know if that's the exact algorithm behind Wanda could you explain like uh what what the idea is with one scoring yeah yeah that's exactly the idea so you have in a bm25 index or in a in a let's say in a in full text search and this is something that traditional search engines have been been uh or have provided for for quite some time already now you can also get that in in viviate in full text search you have specific keywords so so in very simple terms TF IDF or bm25 basically rewards terms that are rare So in the how to catch Alaskan pollock uh the the um how to is probably super super common and doesn't really add any any value uh to the the um the search query but Alaskan already sort of narrows it down quite a bit so ideally in your search and so so maybe to to step one step back scoring everything that's in that term has a certain cost right so if we because we use the inverted index that means the inverted index will tell us every single document that contains a specific term so for for Alaskan that's very few documents compared to overall unless your entire database is about Alaska and I guess you know that that wouldn't um be true anymore but but then you you'd have another query in that term that would be or if your entire database is about phishing then yeah same thing so so basically there's always going to be frequent and rare and ideally you want to give more weight so to speak but I'm not talking about weight in the sense of of um changing the calculation because the calculation like the score is going to be the same but more weight in the sense of I want to tackle this first because it adds more more value and then maybe I can skip something else so that's why it's also called Dynamic pruning algorithms because you you want to prune the stuff basically that that you don't need so if in a classical inverted list or inverted index a kind of scenario the the database doesn't know upfront which terms which right like it doesn't have a sort of semantic understanding and it does with hybrid search because of the whole Vector search part um but the the full text part doesn't um so so now you have these like how which matches so many documents then you have Alaska I'm just going to narrow the query down for for example say it's just going to be how Alaska now doesn't make sense anymore of it makes it easier to explain um that then um yeah basically you have these mini matches for for um for how in these few matches for Alaska and you know how much each term can potentially contribute to it based on uh the the bm25 scoring so basically there's a maximum and this is from this tfidf calculation it's also part of bm25 there's a maximum contribution that each term uh can make so now if you've let's say you've not had one but you've already started scoring 20 documents maybe um and your your top K so basically the user defined limit is let's say it's 20. so you already have 20. that means you now have 20 scores already so now you know basically that let's say the bm25 score of the the worst of those 20 objects that you have is 17 made up number now you know that if an object if a new object cannot reach score 17 you can discard it and how would you find out if a new object can can reach 17 is basically you look look at the maximum contribution of each term so let's say the word how and this this gets nicer if you have a longer example let's just say I'm going to stick with this how we'll ask him let's say the term how has a maximum contribution of three and the term Alaskan has a maximum contribution of I don't know 15. then if those two are combined that's 18 that's good but if they're not combined if you only have how you can no longer reach your threshold so basically what that tells you and this is everything the one algorithm does it tells you skip every object for how that doesn't also match Alaskan and and this means that if you have the these like sort of relatively dense list which is the one with how which is dense because there's so many matches and then you have the sparse list for Alaskan which is rare at some point those lists are going to overlap and basically it allows you to to sort of move that pointer ahead to the next point where they're overlap where you have the the minimum possible score to reach and then um same same goes on basically let's say you managed to skip 5 000 matches for how because only then you would find the next ID that matches both how in Alaska and then after that there's another 5000 which only match how can do the same thing again in this way you can reduce the amount of scoring drastically like in the most extreme case you could reduce it to the number of scores for the word Alaskan and almost sort of ignore the word how but of course if the if if the difference is not that extreme then you still you don't want to completely ignore it you just want to sort of yeah not score it if you know that it's mathematically impossible to reach a higher score and then you want to want to skip it and and that is what makes that algorithm super cool because in the context of vector search we deal with approximate algorithms a lot but this is not an approximate one this is really a just a smarter way of scoring basically in the the score in the end is going to be exactly the same yeah that is so interesting to hear about this sparse scoring algorithm and maybe uh talk a little more about as far as vectors but just quickly like I've plugged in this wand test into the beer test with natural questions and for me evaluating natural questions is 2.6 million documents and this time is how long it takes to evaluate the ndcg of 3500 queries and the difference before and after wand is like five and a half hours to like 14 minutes and this lets you yeah I missed so I was like wow this is pretty exciting just anytime you see that kind of thing it's similar back to when you had the Thousand speed up with filters yeah those things are always like the Eureka thing to see but yeah I think it is very it's so interesting like um I did a little research into one and I saw the paper from 2003 that uh proposed this and I think it's so interesting to see it uh still being used and I also saw this other idea of splade sparse vectors or used deep learning models to uh to create sparse vectors so the idea being uh you have how to catch an Alaskan pollock and you put the language modeling head on each of the tokens and then you get a distribution over the vocabulary that comes out of the language model so I think Pollock would probably be the best one because maybe like in addition to Pollock the language model might have thought it was like salmon that would make some sense right so it would um so you'd have this sparse distribution and it would be the same kind of scoring building blocks as far as I understand for uh how you would index those that particular kind of sparse vector and then you know retrieve it efficiently but yeah the whole thing hearing about the thresholding algorithm it's all so fascinating um so super cool so yeah I think uh coming out of the some of the speed things and now something that is into the database thing uh so what's new about replication yeah yeah so I think today um like uh depending on on who the audience is either you have to wait a very long time to to uh to get to the parts that are interested in or they're they're going to learn something completely new because we we have so many like this is such a diverse kind of release like we have we provide value in so many different areas like the the bitmap indexing I guess was sort of also more on the database side but maybe easier easier to to grasp and uh yeah the the um PQ is sort of completely in the in the nitty-gritty uh um Vector indexing thing kind of side but yeah coming back to sort of the the good old uh distributed system kind of um uh um yeah replication topic so we released replication in 1.17 and and something that we made very clear in the beginning is basically this is the feature set of what replication can do and this is if you if this is sort of good enough for you then use it this is what we're going to do in the next couple of releases and if you have a certain point where you say okay I need this particular feature then you can you can sort of start using it that specific point it's just going to get better and better over again so sort of the the idea is to release this in in chunks or in increments um where it constantly gets better and you don't have to sort of wait over and over again because 1.17 probably covered 80 of cases but then you have those remaining uh 20 basically that take take much longer but also that there may be a requirement for it for some cases depending on what you want to do with with replication replication to me is is such a an interesting one because the the motivation of why someone needs replication in their database can go anywhere from they have a specific failure scenario that they need to protect and where they need to make sure that if a happens the database doesn't go down and then a database can still read or can still write into these kind of things all the way to they need to sign off some sort of a corporate sheet where there there's some sort of a compliance policy that they cannot use a system that has a single point of failure so everything needs to be replicated and we're really seeing we're seeing all of those we're seeing like people who who need replication for the actual value of replication we're seeing people who just need replication so they can take some sort of box and say like okay vvn has has replication um and yeah depending on on um where you sit on that that line I think different different points and different features add more more value um that said that was all these sort of the long introduction of how we release replication in in smaller chunks but what's new in replication in this release is we now now have tunable consistency for every single input or I think almost all endpoints and and this is um so tunable consistencies is modeled after Cassandra which in our opinion like basically we looked at okay what's out there in the space and what behaves in the kind of way that we want VBA to behave and then Cassandra with its hyper scale kind of capabilities with a very good good role model for for this basically so tunable consistency is something that's modeled after Cassandra where basically you as the user can make that kind of trade-off how consistent does this read or write have to be and the the most extreme case would be all where you say like you have your database replicated three times and then you make a a write with consistency level all that means that request is only going to be successful if every single one of those three nodes acknowledge your your right and if one doesn't then the request fails but then you could also have the Other Extreme which would be consistency level one where the idea is that only a single node has to acknowledge right that doesn't mean that only that node owns the data like the data is still going to be replicated to the other nodes but let's say node 2 failed and now it can only be replicated to node well let's say you're hitting Node 1 and it can be replicated to Note 3 that and node 2 is currently down this would still be allowed with with a consistency level of one so basically in this case your your request would still succeed and that can be desirable because this gives you a kind of scenario where you can still write even if a note is down and why would a node be down well could be because it actually like Hardware failure or something is broken but it could also be something that's that's planned where it's down for a very short time for example because you're doing a rolling update so you could say like I want to upgrade vb8 from 1.17 to 1.18 um then each node would be in this this rolling restart kind of fashion would be down for a very short time and if in that short time period something happens now you have a full control do you win strong consistency then you you you probably right with consistency all which means this request will fail and you have to retry once it's back up again or you could do it with one where basically it will be replicated later or you could do it with a quorum which were the idea behind a quorum is basically that that it's it's a fancy term for a majority basically so a majority of the notes has to be present and then if you use the majority both during writing and reading then you always have to guarantee that that it has to match because if a majority of the nodes have received an update and have confirmed receiving an update then when you read with the majority there's mathematically no way to get the the old stale data so these are the kind of um trade-offs that you can do and and really configure this this for your for your needs and then there's the the second new part which is read repairs which to me is like the first time I used it I was blown away by how simple it is and how effective it is so if we're sticking with this kind of situation where a specific node was down during an update and you accepted it because you said okay my the consistency level is say just Quorum or maybe even one then how do you deal with it when the node gets back like because at some point that that node is going to be out of date right and if then you you read with just consistency level one and there's very good motivation to read with just one because that means your throughput can go up like if you if basically every data is always replicated and you read with just one then now you have not one node that can serve all the data but now we have three nodes and you could also add a fourth and a fifth and so on so there's there's very good motivation to read with with consistency level one but now what happens if that node is is out of date like is it if you happen to hit the note that didn't receive the update you would be serving stale data and you can prevent this by setting a a higher consistency level of course but also this is where the eventual consistency of of um a VBA basically you know that that whole design comes in if you don't want to read immediately with the high consistency level because it has a higher cost and lower throughput you can rely on the fact that eventually this will be consistent and one way to repair the data and this is new in 1.18 is the read repair in the read repair basically says if you query the object with a um let's say with a quorum consistency level and VV it realizes that let's say three nodes and two are queried and one of them is out of date viviate will just repair it and VV it knows exactly based on on timestamps and based on other kind of metrics of what happened DV8 knows who of those two nodes is basically is wrong who who missed the update and then if you query it with that read repair by the time that your query is returned the data is already repaired so you won't even notice like you can query exactly the note that had the stale data but as part of the query and that's why it's called a read repair basically as part of reading the data VB had already repaired it in the background you you'll receive the right request and that was the moment when I tried this out for the first time like I tried to to specifically get my my database in an inconsistent state by actively killing a node then making an update making sure that the node missed it we started it um try to query it and and forgot to to set the the one consistency and then default it I think to to either Quorum or all and immediately it was already repaired by the Moon that I I got the response and I was like wow this is exactly how it should be like this is this is so cool because um yeah if you really want to fine tune you've got all the ways but also if you just want your data not to be out of sync then let's just read it just just let it repair itself yeah it's amazing and I really liked how when we first started talking about um replication you had like the um the Black Friday e-commerce example and understanding these kind of uh trade-offs with read write and when you need certain levels of consistency yeah that read repair thing is so interesting I think this would be also a great transition of you've recently given this amazing lecture on our journey to build a vector database and go and this is published on YouTube if people are interested in checking this out um so a question I have with uh replication these kind of things is um you can touch on what features of the golang programming language help build things like this like distributed database systems yeah um that's nice because my my uh talk was basically about what what features of go um are hindering us from building the best possible database no that's not true because it also it also has solution for all of those challenges um features in in go that really work well is I think the main two things that that um come to my mind right now is one is dealing with concurrency because especially if you have these kind of kind of background operations you may have to spawn like in in go you don't spawn a new threat but you spawn a go routine a go routine is basically like a lightweight threat but then you can have because it's so lightweight you can have way more than than actual physical Hardware threats and then the go runtime will just manage it for you and it gives you lots of tools to basically make sure because the moment that you have concurrent processes there's always a risk of data races and the whole tool set and go both during development as well as for for how the language is structured really makes it easy to write threat safe or or sort of safely concurrent code um so that's that's one of the parts that that um really really helped while there um the second is the strong standard Library so this is something that depending on on so I way back before I I started writing go um I used node.js because that was sort of the thing at the time and and I also did some front end work and then some point moved into back-end work and then it was yeah it was node.js and in node.js you have the exact same opposite the exact opposite sorry um so so basically anything that you do you have to import a package there was this I think it was called the left pad Fiasco at some point where um a day a package that would pad a string on the left side which is like one line in any language even in JavaScript that's just one line that was an external package and then this particular package had a security vulnerable or I think it wasn't a security vulnerability in this case I think it was just deleted I think it was just the author decided to delete this this package uh but the problem was that thousands upon thousands of packages or or a node uh programs and packages that were used everywhere in production dependent on that one package and all of a sudden all the bills started failing um so coming from node like way way back in the day like I was an early adopter of go so that I don't even remember when that when that was but it was really like like way back coming from node to go seeing how strong the standard library and go is and not needing these kind of third-party dependencies and and these these kind of third-party packages that was such a such a both an eye-opener that you can do it just with the standard library but also such an enabler because there's all of a sudden you don't have to learn framework after framework after framework anymore you just know that the the standard library of go and it is it is somewhat sort of limited but over time you you yeah you'll feel like you know all of it that's probably not true it's probably way bigger than what people typically use but um it gives you a nice yeah sort of the the com commonly use things you just know how to use them and don't have to look stuff up and just by either by just typing it or relying maybe on your your code editor IDE to do some autocompleting you can build basically an entire database and that's that's not even a fictional example because that's basically what we we do in in V8 and to to loop back in the beginning when we talked about bitmaps we said that we took that crazy decision to um to build everything from scratch and not have many external dependencies and that works super super well with go because I think that's also the philosophy and go that the language is so powerful on its own that you don't need these third-party tools but just with the language provides is there and very practical example in replication because we're we're dealing with the distributed systems where nodes have to talk to each other there's so much Network traffic going on and then there's multiple ways of how to do that um but basically being able to do all of that just with tools that the standard Library provides is is super powerful and really he keeps the iteration time down so those would be my my two favorite points about go um for a feature such as replication yeah that's that's so interesting it's so interesting like going into the golang and yeah I mean the whole end to end as we start off by talking about the bitmap the P hsw like vector search deep learning and then coming into the features of golang It's amazing And um maybe one other detail on your talk that I was curious about is um see as you mentioned you you're talking about the hindrances of going in them and one thing was the um the stack versus the Heat memory escaping to the Heap and and I know we talked about the garbage collector uh maybe talk a little more about like the state of I know the go mem limit and all these kind of things and like uh how that's been because I understand that it's getting better and yeah yeah let me try to answer that from the perspective of a vv8 user amazing because I think the vb8 user has two interests well or basically in in V8 but in turn then also in in go uh one is the application shouldn't just randomly run out of memory like that's that's basically the worst case you you sized everything correctly you plan for everything and then all of a sudden vbh just goes bam out of memory kill and the other is you want high performance and and these these things are closely related because um performance in go often is a topic of of memory management which may not be super obvious because if you think of performance you think like the bottleneck is your CPU right like if your CPU does this many things per cycle uh then it's it's fast and if you can reduce the amount of things that it has to do per cycle or I don't know use a smarter way of doing it like in the talk I talked about simd for example where you could do eight operations for the cost of same operation that is what you immediately think about but in a garbage collected language like go often it's it's not even it's not CPU but or it is technically still CPU bound but because of something that you do with memory and that is where where memory allocations come in and um the idea between or the split between the stack and the Heap the stack is basically represents your function Stacks so as you move through functions you move through that stack and you have variables with a very very short life cycle they they are as you start calling the function basically the variables are created or are avoiding the word allocated right now but they're they're assigned basically and then when the function is over those variables can just go away so they have a very fixed lifetime whereas the Heap is everything and that's why go uses the term escaping which I kind of kind of like because it escapes that clear function stack and now and this could be for example because it's shared between different function calls or because the lifestyle cycle is meant to be longer than these individual functions called so basically something escapes and this escaping makes it way more costly so both the process of escaping because now the the go runtime Master like okay this is this needs to be tracked basically and this needs to we need to find some space somewhere on the Heap um but also then it needs to be deallocated again because it's it's it's like now now that it's escape the function stack it's no longer in this kind of predictable life cycle of when can we free it and if you couldn't or if you would ever lose that kind of Association of how it could be free then you'd essentially have a memory leak you'd have memory that's assigned somewhere that you don't need anymore that's still around and and then your your memory starts ballooning and you don't know why and that's that's um yeah there are some things you definitely want to avoid and then to avoid this basically go use the garbage collector it's a very good garbage collector but nevertheless something that's allocated on the Heap is always slower than something that's that's allocated on the stack so this is the the kind of perform performance a background in this kind of setting but then I also said you don't want your application to just randomly run out of memory and this was the the biggest change in go 1.19 I'm almost getting confused with the versions of like bb8s at 1.18 right now goes ago is actually at 1.20 right now at some point we have to start releasing a releasing V8 faster than goes so we can hire her numbers which I think that the bb-8 release Cycles are way faster so that's only a matter of time um but yeah in go 1.19 uh the go runtime introduced this go mem limit thing that we have a blog post about um also and go mem limit does something that's that's actually is one of those things that are super simple in in hindsight or once you know and it's super simple go mem limit just says this is the limit of memory that you have available in the garbage collector will try to respect that limit and now the question is like Why didn't it before that like why wouldn't the garbage collector do that before and before the garbage collector had um a tuning parameter that was relative to the current Heap so basically in the default was a hundred percent which would mean that if you have a machine with 64 gigabyte of memories and you currently have 33 gigabytes of memory allocated the next hundred percent increase would be to 66 gigabytes bam out of memory so so now you're at like 51 usage and just because of the way that the garbage collector would try to allocate or try to um The Collector doesn't allocate memory but it would just delay running again so basically now would say like okay I'm gonna run again once the memory is at 66 gigabytes but your machine only has 64. so that that doesn't work and all that go mem limit does is basically give the garbage collector that kind of hint of where the the limit is and go call this a soft limit because it's really it's just a hint to tune the garbage collector um to to run faster and now the garbage collector basically knows okay I'm at 33 gigabyte my limit is 64. so I'm not going to wait till 66 but maybe I'm going to run at 60 already and all of a sudden like you you made use of that memory that that memory is there to yeah to serve whatever purpose you want whether it's a purposeful Heap allocation so for example in bb-8 uh the vectors or with agents wpq also the compressed vectors they stay on the Heap because that's what we want right we want them to stay around um but something that we do for example a filter that's used as part of a part of a single call stack that doesn't stick around so we don't want that that on the heat but either way no matter whether it's intended Heap or unintended Heap now with go mem limit vv8 can can or or any Go app basically can respect that and make sure that it doesn't quote unquote accidentally run out of memory again because that's really what it was like it it was it just set the wrong target basically and that's that's prevented with that and I think at some point we also added a a feature into vb8 specifically that marks your objects as read-only if if that if you get close to that threshold so I think it's it's getting more and more like as vv8 improves and as as go improves it's getting more and more difficult to actively manage to run out of memory which is awesome for a user because that essentially means it's it's more stable and and harder to kill and if you still manage to kill it then you have replication and then all is good because that that one note death doesn't matter because it's replicated so yeah it's so amazing it's so amazing like the depths of the languages that build databases the features of databases and then the whole search technology more broadly and uh so awesome Eddie and thank you so much for another weeviate release podcast it's very inspiring for me working on Wi-Fi to hear the depths of your knowledge with all these things and I'm sure it inspires people who are using levator already or are interested in checking out alleviate and I think we've 1.18 is packed with so many awesome features with this bitmap indexing to have these filters just as these like thousand times faster is obviously like just so incredible the wand the way that speeds up bm25 and sparse search I've seen that myself it's incredible and hswpq all these things replication and that's just so exciting so Eddie thank you so much thank you for for having me and also big thank you of course to the entire team that that actually built this I am just here basically summarizing everything that that we've done as a team in this this print but of course we have these individual experts on on all of these topics and and then that that allows us as you just said to have a a release with so many different topics that provide value in this is this is really cool and I'm super happy um with our team and super proud of our team so thank you everyone both both inside bb8 but also of course outside or Community for for all the input and for telling us um what the kind of features you need what kind of features you benefit the most from and and yeah any kind of feedback like use vv8 and tell us whether you like it also tell us if you don't like it so we can improve it but please also tell that tell us if you do like it because that's that's awesome to hear um so it's so motivating both for for me and I think everyone on the team to hear the success stories with bb-8 and this kind of there was a tweet the other day where uh sort of Father and Son uh pairing did some experiment with with V8 and just reading that and reading the actual stories this is so cool to to yeah change people's lives one at a time with reviate yeah that was an awesome tweet be yeah the whole the moment of like the father-son hacking with Ouija amazing awesome thanks everyone for watching thank you ", "type": "Video", "name": "Weaviate 1.18 Release Podcast - Weaviate Podcast #40!", "path": "", "link": "https://www.youtube.com/watch?v=Q7f2JeuMN7E", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}