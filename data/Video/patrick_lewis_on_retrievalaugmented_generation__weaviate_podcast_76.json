{"text": "Hey everyone, I am SUPER excited to present our 76th Weaviate Podcast featuring Patrick Lewis, an NLP Research Scientist at ... \nhey everyone thank you so much for watching another episode of the weeva podcast I'm super excited to welcome Patrick Lewis to the weeva podcast Patrick Lewis is I would like to say the inventor of rag is something we'll talk about in a second he's the lead author of the original retrieval augmented generation paper and Patrick has done so much amazing work in the space of natural language processing broadly I want to say because the the coverage of all your papers is just so extensive from DPR to rag to Kilt and then you know even task Weare retrieval with instructions is so many exciting ideas Patrick thank you so much for joining the wva podcast thank you very much for having me yeah I'm excited to excited to be here awesome so could we dive into this kind of like origin of retrieval augmented generation I know it goes back as early as like 2019 that you've been thinking about this um I mean yeah so so I think thank you very much for the for the for the intro saying inventor of rag I think I I'll backpedal a bit um so I mean yeah like like the kind of idea of of of uh getting computers to speak to corpora of text uh is super old um and it goes back to like almost as old as kind of computers so there's kind of some evidence of it from from the 60s and you kind of have like question answering systems um and people kind of yeah working on these systems that have the the kind of DNA of uh a model or some kind of computer program um trying to access information that's stored in in an index or a database and then use that to achieve a kind of task or a goal and usually it's framed around question answering and that's changed pretty recently actually um you know there's been other other use uses for this over time but like you know until very recently the really really big dominant way that you'd study this is in question answering and yeah no it goes back really really far and the kind of continuous levels of of improvement um there even like you know uh in terms of the intent behind systems there are like kind of question answering systems from from from the 90s like ask geves was kind of the intent behind ask geves was to try and answer your question and not find you a document so you can find your own um answer and then if you look at the the architecture of like you know some aspects of how the original IBM Watson the Jeopardy model uh worked it had you know similar kind of conceptual underpinnings to this and then throughout the kind of uh 2000s and 2010s particularly the 2010s when neural models came along the kind of uh one conceptual shift is like okay you can move from having models that kind of pull out and juristically extract information from text for your use case to having a model that kind of reads it uh in a more uh connectionist uh kind of way um and yeah those kind of came along um a bunch of folks kind of looking at these machine readers initially these um machine comprehension machine reading models and then people hooked them up to uh retrieval databases and you had this kind of first big kind of conceptual open domain QA models guess they they were kind of referred to and yeah then it kind of accelerates from there I guess uh lots of people excited by bigger models neural models that kind of had their own uh had sort of a bit a much a much stronger kind of capability um and uh really interesting ways that you can kind of train these things end to end so all of these things were going on uh and have been going on um throughout the the year 2010s and four um there also connections to some kind of parts of semantic paing uh especially end to end has some kind of similarities uh and yeah like long story short like lots of people publishing lots of stuff uh but yeah over the kind of late 2010s and and and 2020s it a bunch of people kind of realized you could train a neural uh dense embedding uh retrieval model and a uh neural reader and get these really compelling kind of systems and you could maybe distantly supervise those things uh and yeah I guess the rag paper is a weird one because it's sort of uh very like very much along that kind of line of of the Zeitgeist it just was their the right time to kind of knock away a littleit of the conceptual blocker that you could have a generative model that did this so like a seekto seek model um input sequences output sequences uh and retrieve in the middle and just was sort of there at the right time the right kind of uh messaging and it seems to have captured the the kind of imagination of people but yeah I'm always very careful to try and credit you know the the continuous process of innovation which is like thousands of of people over over many many decades yeah I I think in addition to the paper you also did this lecture I think you were in your uh PhD at UCL and this lecture really explained it I because yeah I think that kind of I think you nailed it with that connectionist perspective I don't think a lot of people were thinking that you could separate uh retrieve and then read and so this kind of it was just like a really elegant way of forming it and so kind of the next question I want to ask you about like so so yeah so so people of running away with rag because we can just kind of plug in these like language models that have been trained for something else we can just kind of like aend rag onto them right and and so I thought this paper Atlas that you published was so profound with how it shows that with 11 billion parameters you can still do this kind of like reasoning capability with the retrieval augment augmentation so I think if we could if you don't mind talking more about Atlas because I I think that shows that this kind of uh rag has this kind of like economic potential yeah sure so um so in my mind like Yeah The Atlas paper came together with a bunch of people at sort of the right time like um people in London than I was working with um in the in the fair office you know we'd been looking at knowledge and language models and then um D Kier and Ethan the student he was working with uh we really excited by this paper called realm that came out and then we sort of Applied the combination of our ideas to make that first rag paper happened but it was quite naive we kind of stuck it together it worked and then we're like hey here we go this is cool that's published this and then um published it uh I got bored like I often do and then did some other things uh for a little bit and then working together with a colleague um from Farah and Paris goate isacar and and his supervisor edar gra they'd come up with this really compelling way of using a um different kind of architecture for reading uh with models called Fusion in decoder or FID and uh they got really really good empirical results and Atlas was effectively our uh revisiting of the rag ideas about two years or a year and a half maybe I think about two years later uh applying the uh really impressive powerful FID model plus other kind of like stronger dense retrieval models uh particularly Conta which was um unsupervised another thing that that goate and edar had done and kind of combined those ideas together so like the really really strong empirical results that came from uh the new models and the same kind of uh spirit of what rag was trying to do which is this endtoend learning the thing we didn't crack in rag um was how to pre-train these models and actually that design space is massive uh really really really big and the reason you kind of want to potentially pre-train a retrieval augmented model uh comes around the ideas of few shotting and zero shotting and instruction following and this sort of thing um you know otherwise if you have a bunch of training data and you have a well- defined task you might as well just you know find tune the thing end to end in a supervised way and then that's great um but if you have very very little data you don't have enough of your kind of data budget to spend doing that fine tuning so we were very interested in how to get these not only you know apply the updates and the improvements in in in models to the Paradigm of retrieval augmented generator models uh but also to see how well they could work in few shotting and even zero shotting uh situations how you can design them so that they do work well there and how that compares to your kind of now relatively uh standard run-of-the-mill large language model which is bigger uh but yeah um just without this retrieval augment a uh so that was kind of what we were doing in that paper was really kind of trying to deeply understand that design space how to pre-train these things how to do so without any supervision and really ablate that super super carefully we were trying to in some ways kind of take inspiration from the T5 paper which again was just kind of like hey here's this Paradigm here's the million ways you could do it uh and making a first St at doing as many of those Million Ways as you can um it's not our paper isn't quite as big or as profound as as the T5 paper there but like there are a lot of tables there that kind of assess all these different ways in which you can uh hook up a you know a dense um unsupervised uh retrieval system with one this powerful fusion and decoder architecture and how you can get them uh to kind of mesh together with pre-training uh objectives uh over lots of text and how far how far that gets you and yeah to what extent knowledge can be decoupled from reasoning and the yeah the kind of sample efficiency and the kind of I don't like the term but scaling laws um kind of work for this kind of system as well yeah amazing that yeah that T5 paper where they were like uh you know span BT mask at the end and and then it's like okay well now what masking probability just so many different things right and that kind of isolation to that so I have kind of two things I really want to get your take on which is uh the fusion in the import like two things of the importance of which is the importance of fusion in decoder because right now I think we're mostly just uh you know putting it in the input window and then hoping that works and then maybe we do one at a time I don't mean to give you to it one yeah let's let's hit fusion and deoda so um fusion and deoda is a has a number of like really useful things for retrieval augmentation in general um so it has this uh really nice property so it has a few nice properties one is that like it saves a lot of memory uh over simply concatenating a bunch of documents together in the input to a model you avoid some of the worst of the of the uh quadratic complexity of of Transformers uh and in a way you can kind of view fusion and decoder as a as a type of efficient attention mechanism um you could draw a you know attention Spar map and it would correspond to Fusion IND decoda pretty much um so it saves memory in one ways uh in other ways it's useful because you get this uh document invariance uh document positional invariant so uh all of the documents are viewed by the model as equal uh and you don't get any kind of biases that come through there and so that's a kind of nice intuitive way as like here are the set of documents observe them all uh then there's this nice deep cross attention between the query and the documents and that lets to very deep contextualization of the two this bidirectional encoding of the um uh cross encoding of of the documents and queries is probably probably fairly important and you don't necessarily get that b directionality if you throw it into like a big linear uh kind of Transformer um and yeah it's it's great as long as your output sequence isn't very long um and it works well with pre-trained seekto seek models uh my view now is subtly different um these are mainly kind of practical arguments uh but it's still a very good architecture uh where I'm working now we're not using it um mainly because of the invention of flash attention so flash attention and other kinds of like additional kind of model hacks uh have basically meant that it is no longer a uh it's no longer terrible news to have a long context window uh from a memory perspective it is a bit from a uh maybe from a performance perspective uh maybe from a latency perspective it's kind of worse um but it's no longer prohibitive to just like have a really long sequence and the other thing is that uh big standard large language models left to right uh GPT uh like all GPT like everyone's got one now llama Etc um there's so much R&D and so much ablation and so much care going into those and all of the kind of tooling that goes in there especially on in the open source uh it's very hard to bet against that model being better maybe not because it's inherently better but because people have worked very hard to make it as good as it can be and there's a smaller brains trust working on uh seek to seek models and fusion inota etc etc there's this whole space like a really exciting design space of models that are somewhere in between depending on how you kind of draw a sparse attention map so there's some really interesting research I poked for a little bit along with um an intern called Avy and and edar before I left uh fair that was looking at that and goate as well uh which is really exciting we didn't quite manage to make it work but like I think there's still a lot of interesting things in the in the space B of how to process um documents along with a input and produce an output in an efficient way with the right kind of attention Etc um for language models um but yeah from a practical perspective at the moment I would say if you want results uh Fork out for a long context model and just use a large language model um yeah yeah that's fascinating I I guess I always thought there was something to like putting the embeddings in the middle of the Transformer like in layer eight out of 12 or whatever compared to just putting it in the input but I guess you still embed it in the input so that argument's gone and Flash attention and we had opar press on the podcast to talk about Alibi attention so a little bit of you know we've been looking into this positional embeddings and how that works so I and yeah and maybe just one other nugget that I took out of what you're saying is there's that lost in the-middle paper that's talking about maybe and so the loss in the middle is saying like if the right information is in result five out of 10 then it can't attend to it absolutely yeah so Fusion in eodo has not does not have that problem it's actually invariant to that so like the result is is is mathematically identical depend it doesn't matter way you put it in there is that not that concept of of ordering which is really nice that is a great thing about Fusion in decoder I would say um maybe yeah that like that is a observation based paper um I'm sure I I don't look at the literature nearly as much as I used to or as much as I should um but I I believe there are ways to combat that empirically um some I'm aware of um where you don't see that kind of problem happening um so the the retrieval augmented models that I'm training at cooh here right now for example uh are almost invariant to document position and they are based on standard uh model um and you kind of have to you have to just prepare your training in a very careful way um but yeah like I think that's a a bias that kind of comes through mainly from the combination of ways these models tend to be trained whereby the beginning of General documents that you might need to you know complete the the end of the beginning is important and the last few words are important and in the middle just naturally is less conditionally important um but if you then find tune the model to make the middle more interesting then you you can kind of recover from that a bit uh but it is nice to have you know um inducted biases that mean you don't have to worry about that so that's why those kind of fusion and decoda models that can be can be really good for that there's also like other ways where you can make them very very very fast one of the the people I work with Co here now Sebastian hofstetter um has uh a really interesting paper from I guess it was a year and a bit ago now uh called fidl of basically working on on kind of slight changes to FID uh that make it really fast and very productionize um and that's very cool like that kind of thing is really interesting um yeah yeah amazing so we're about 15 minutes in the podcast i' already gotten a massive takeaway just in that insight and I have so many other topics to get into but so okay so yeah I think that's so interesting but so the next topic I really want to get your take on is rag end to end putting the gradients from the reader back into the embedding model and how you're currently seeing that um yeah so there's many different ways you can do it um the original rag paper was using a pretty much like initially taking inspiration from a paper called realm uh which uh came came out kind of as we s have started this project but not really got into the stride of it yet and um um they had some like very like simple straightforward marginalization based ways of kind of training a model um end to end SL jointly kind of depending on how you look at it but yeah kind of this nice little probabilistic model with a slight kind of fudge uh for a topk truncation as opposed to a proper uh marginalization and that's really nice and it works very well for the span based uh method um um but so the first thing we did was say okay we can train a model like this with a with a gener generation model um very easily but it's quite slow to uh decode from um so we thought of like other ways that we could do it uh effectively you kind of have a model that generates a long list of of words and you sort of need to marginalize across every single time step and that's really really slow so we're like okay let's not do that let's fudge fix the documents every time and um that's where you get one of the formulations from the original rag paper um the other thing you can do um yeah so that's kind of called rag token I think it is where you're kind of pretending to do that marginalization at every token step but you're keeping the documents fixed you're not doing a retrieval every token but you could do uh and then there's the second one called rag sequence uh where you do like one marginalization over the sequence and that's much harder to to run influence on um so those are the kind of things that we started out with and then um I think driven really by like my my colleague uh um goer and and edar together they kind of came up with um other ways of doing this that kind of were a little less um maybe like theoretically kind of off the shelf um but uh much more flexible these kind of distillation style approaches where you kind of say all right I want to train the uh generation model how do I do that well I just need some documents that come from a retrieval system as long as they're not total garbage um you'll learn something useful from them uh and so okay you need a retrieval system that gets you some documents you use that plus the input to try and train the model to produce your target output that's fine and then you need a way to train the retrieval system and they basically say okay well let's try and distill out relevant signals from the language model part of the system and that's the high level kind of thing they're saying we know the language model is capable of giving us some indication of which documents it likes and we should incentivize the retriever to rank those ones it likes higher and there's a lot of different formulations you can do there um and there's some really nice work I can't remember his name off the top of my head it's really bad about it but um really nice work for from um someone I have to look it up afterwards uh who um basically like wrote some some nice like theoretical uh work uh produced a new model he was working at at Deep minder on an internship and showed that they actually quite similar um and there's some math you can do to say that actually those things are more similar than it might seem um but yeah effectively now I'd say the the recipe is something like um have a language model that conditions quite strongly on documents use that to obtain signals with which to find tune a retriever and then iterate between the two you can also write like interesting em style algorithms to do the same sort of thing but usually it kind of comes down to hacking something together that kind of iterates between I'm going to train the retriever for a bit relevant signals from the generator and then I'm going to pull out lots of stuff from the Retriever and train the generator and kind of go in that that Loop um and so from this perspective you're no longer really training it end to end I prefer to think of it as jointly um so you're kind of training these two things at the same time but um and that's kind of nice because in some ways then you can you can almost train them in isolated processes uh and you don't have to maintain this one gradient flow through two models uh which can be can be pretty helpful for from an engineering perspective yeah that definitely makes a lot of sense to me that um you know not putting the gradient from the reader directly into the embedding would it would be easier if you kind of have the signal from the language model than contrastive learning with the embedding model and um uh so really quickly I have one more question about I want to talk about DPR as well and the kind of separate training of the question encoder with the passage encoder but really quick on this topic the so for me what makes end to end rag so exciting is the idea of just like one uh API that does it all for me right I don't have to you know like um like where would you say maybe coher is at with like um you know kind of like I find tunal language model and I get an embedding model as well yeah that's interesting um I'd say um so we're not in my team we're not directly working on this but there there are plans a foot um we we're actually almost in our team doing the opposite in a way we're trying to we're trying to get a a a retrieval augmented generation API where you hook up your retrieval system in the middle and we try and work with whatever you've got um so we want to be robust to maybe weaker retrieval systems or turn-based things and what we're doing under the hood there is is uh using rankers um so we're trying to basically be fa tolerant we're trying to also get the language model used to what to do with situations where a retrieval has failed um we're also thinking about query reformulation and other kind of like active styles of of of multihop uh retrieval these things aren't aren't available yet but it's kind of what we're working towards is something that will work with whatever you give it um because often like what people really want is to say I have this retrieval system it's been there forever I can't be bothered to W or rebuild it you know bad news for week um and they want to kind of yeah make it work you know and then eventually you know you can upsell um you know a really nice dense Vector uh database um yeah like that's that's what I spend a lot of my like our time kind of thinking about you like um working with with with what you come with um yeah like when it comes to those relevant signals that come from a language model and that uh say of like okay if you know what your system is supposed to do you have a a database of stuff um yeah if you have a you have a database of stuff I I actually it is interesting to to to see you know whether some direct annotated contrastive training with hard negatives and the kind of older style of supervised uh stuff will work empirically better than trying to get a language model to give you those relevant signals um I um I think it really depends on how hard you want to solve your task and how much like and and what I mean by that is like you obviously want to solve your task otherwise you wouldn't bother um but is this a task that you can anticipate before uh how long do you have to prepare and can you annotate and do you have the operational capacity to annotate a lot with high quality etc etc usually supervising something in domain is going to work best and so for like really really intense use cases that you kind of bus is critical for you and your kind of table Stakes um often the best thing to do is just directly supervise on that over the top of you know some pretty good unsupervised retrieval system but if that use case kind of materializes on the Fly then yeah you can't do that and maybe you can you know use your data more efficiently or you don't have the ability to annotate the middle bit of like put document pairs or document output pairs um and yeah these kind of endend systems can be like a good like a very good kind of option yeah it's super interesting and it also that also kind of answers the question I had on DPR so I kind of and I think it would be excellent to kind of transition the topics from here into um so kind of mentioning this kind of like fitting rag onto your database systems I find this one topic of like an SQL SQL or like a you know a search engine kind of to be extremely interesting where you're either like you know you ask the question what's the average age of country music singers and so you retrieve with an SQL query instead of like vector search yeah how do you think about that kind of like um you know like retrieval tool use yeah I think that's really compelling for the flexibility um so that's actually what um that's actually what we do right now in our team um we follow that kind of Paradigm so if you use coh his retrieval augmented API uh you'll find it kind of comes intoo hard when you can like hook hook it up so it goes end to end but really there's two calls to a language model going on there one which figures out what the search queries should be and soon more General tool API use should be and the other one which consumes the output of those things and they're just two large language model cools rather than a kind of dense retrieval call and a large language model call um so we kind of swapped out that like one style of of thing for another uh that's not to say that you don't need good retrieval systems and and the DS approval system is dead that still needs to be there and you know uh because sometimes you'll call a search engine sometimes you'll call SQL um but yeah kind of figuring out what to search uh is really important especially as you trying yeah you give higher level commands like I might say you know give me the pros and cons about buying a house at this time of year in the UK in this in this you know economic climate and that's not something I can I can I can like produce a query Vector to get documents for so I really like those things for the flexibility is really important you can write yeah pretty much arbitrary kind of list like lists of of queries to do you can also make some kind of higher level planning and transitioning more to this kind of style of Agents uh I feel is is is the way things are going just because it's just very very compelling but dense Vector search has a part to play Within there you could also take a step back and say well I want a model that can be trained end to end in order to generate tool use calls and that's a blackbox and that could be a retrieval augmented model internally that's pulling up a I don't know a database of of text of uh documentation all kinds of stuff like it could be the docs for how to use all the tools and that's too much in the context window or something so like you have these kind of black boxes and like kind of where you draw uh the the boundaries around them mean that like all kinds of things are possible um but yeah I would say you know having models that don't don't aren't aren't constrained to generate one query vector or one query Vector for by another but can do a very wide broad output of uh space of things is pretty important um the other thing to mention there is if you kind of think a little bit about how world is describing about how you train this this style of like how we do it in Atlas where um you're training this two hops of a model effectively you're pulling out documents from the retrieval system and you're putting that through the language model you're producing an output and you're saying how good is that uh and you're training um the models with that signal um that begins to look a great deal like good old RL with a two poop kind of episode uh so it's in an environment has the ability has sort of two models that need to collaborate it's almost like multi-agent RL in a way like collabor collaborative RL and um that is a really interesting way to kind of view what these tool use or Lang chain uh chains of of language model inferences doing is exactly the same thing you have a chain of models uh and together they are responsible producing a good output or not so you can kind of think of it like a model or a large amount of parameters that performs a few actions few hops and gets a signal at the end how well it did and RL kind of formulations feel like a very natural way of like actually how we should be training these things going forward and it's still end to end in a sense or jointly trained in a sense is that you have these two and they are together collaborating to achieve this goal um but a little bit less of this kind of like back propop through both models yeah well I've never thought of uh comparing a lang chain chain or like agents selecting tools as that kind of like next state action reward kind of transition reinforce learning and that that sounds incredibly compelling and it does sound a little difficult to maintain maybe but um I so so I know that you kind of already started touching on this but I really wanted to also get your thought I know it's like multihop but I think multihop is almost like overloaded whether you mean like choosing tools or you mean like breaking a question into multiple questions I know you have this really amazing paper called concurrent QA which is about uh public and private information retrieval I think that topic is just so powerful for like our interest that we VI it as well because we're curious about like you have your private search database as well as like Google maybe just if you don't mind just continuing on this kind of multihop perspective yeah for sure so I can give a kind of summary for the the readers there so this was this is what kind of uh pioneered by an intern I was working with and uh and um supervisor Jacob Khan uh this simran Aurora super talented like a real kind of um yeah like really productive uh driven uh person to work with really really great to work with uh and yeah she's very interested in security and personalization and personal data and um yeah we thought about these these question answering models these open domain QA models as we used to call them now we call them retrieval augmented generators um and yeah we were thinking about you know multihop stuff and what happens when uh one database is on on you know on device or or private and one is public and we're kind of constraining it you know in our in our view to two hops uh so you have this kind of question that comes along uh like you talked about you know maybe who is the father of my father is a very simple one first you need to know who is my father and then you find out the father is X and then you have to say who is the father of X these two hops uh and both of them require retrieval over a database and some of that is private information some of that's public um and so in that kind of Paradigm where you might need to be mixing information from the public sphere like pinging the internet versus the private sphere which is you know I don't know referencing your private documentation um this is where this kind of thing arises and we kind of think about what can we do and what can't we do and to what extent you know how how well can you build models that are privacy preserving um but what we mean by that is like understanding the risks of if you say um who is our like who is all um let me think about this um who is the father of our new CEO maybe we've just decided who our new CEO is um and so you first ping like who is our new CEO and that's not public information um but then you issue a query to the internet saying who is Patrick Lewis just been made CEO and anyone who happened to be like taking a look at what queries we were issuing to the internet could probably use that information to to to do things with and would find out some leakage of our of our private information so like we were thinking about these these hops so we think that like Public public is safe uh it's just public information you were just asking public stuff uh public private uh also safe private private safe nothing is going out um but there is an issue when you first ask stuff on your private database receive an answer and then uh then go and ask about uh public stuff so there's kind of like you know if there is anything other than a private call following a private database call then you have some private uh problems so yeah we were kind of like interested in defining these um sort of this privacy framework but then we're also interested in saying okay like there's that you know offering is is kind of trying to understand what I guess what you'd Now call this kind of before this became a thing how how L chains over um databases you know what what the Privacy kind of model should look like uh but then also saying okay we've got like an interesting empirical uh commercially relevant question uh like like modeling kind of Paradigm to work through which is maybe it's really useful to know to know who the father of R CEO is or there's more practical things that you could probably come up with um to say okay it is really compelling to try and take public information and cross reference that against private information um and produce models that do kind of can be hooked up to several uh kind of information sources some of them private some of them personal some of them public and what you can achieve by hooking up these different things and hop and the model hopping over the different ones rooting queries to different databases and so what we were doing there was building this this sort of set of of data to kind of study that problem how well you can navigate that um and how well you can get fuse information across these different sources uh yeah was kind of like two two offerings in that paper yeah that already just completely expanded my perspective I hadn't thought about the Privacy preserving angle of it I was just kind of thinking about like uh searching across multiple indexes like if I have one index of books and then one index of like podcasts and it's like one query for the books one query for the podcast and then combining the queries that way do do you also like that kind of just thinking about like um you know symbolically structuring your indexes with some kind of highle category and then sort of multihop maybe like that exact example of I search in you know papers as well as say like podcasts to try to like combine yeah yeah I like I like that so um I think we yeah consider like you know if for so before you used to have some you if you had this end to end kind of powerful QA system you'd have all these modules uh they'd look at your your query and they'd do some query intent and then there'd be a semantic paer and then for the combination of the query intent and the semantic P you would make you rooted to one database you'd execute that query and then you take those results and you do something else with another model now we're kind of looking at the the situation like Co here where uh you'd basically provide the entire schema uh to the model and hook it up and say here are the here's what you're working with uh root some queries uh or like root some API call and figure it out for yourself and kind of um yeah that kind of thing of saying like here here here is the stuff we can arrange the stuff in as you say kind of like ways which are convenient uh for the LM um and I think that's something we'll maybe pick up on in a minute it's like figure out the ways which are convenient for an LM to have information structured in a in a kind of yeah in a in a way you say right here is our database of papers here is our database of this and that um emails uh and here are the ways you can query for emails and they may be different to the way that you'd query for um papers um but you kind of show the LM and it's input all of the uh all of those things and then say okay now you're going to rout uh a we V8 API call uh to papers with the following Fields uh or maybe a date range or something um and it can kind of figure out what it should populate so it's kind of doing all of that work internally for you so it's doing that query intent it's doing that rooting and it's doing that semantic paing Allin one go um and producing that and sort of rooting that to your different like to the the things that you've got together in this big multi-index as it were kind of meta search engine has another kind of term for this um I wanted to quickly pick up on on on something that I think you're hinting at as well uh is how should we structure our multi-index for an LM to use and I don't have a good answer for that other than an observ that I see a shift happening maybe commercially um in that for a very long time we've built apis for programmers to use and maybe also you know like other kind of computer systems that talk to each other uh but we have never been building apis for large language models to use and the designs are subtly different I think and so there'll be a whole host of offerings I think of um companies that kind of Designing tools for llms effectively say like this is an optimal llm calculator or and so on like what should the it's not just sort of what the input signature looks like so the llm finds it easy to use and low you know low effort like doesn't get the paing or you doesn't doesn't produce an unpa ible input um but also what the output looks like you know how much context to conclude how much of a stack Trac or other things that would help it do Chain of Thought reasoning if it was to like read the output of that tool so I kind of see like a really interesting opportunity there for like uh in all sorts of domains building the tool the llm wants yeah yeah I'm I'm a huge fan of like the gorilla large language model Works which is kind of like um you use like self- instruct data prompting to like come up with a natural language command of when you would want to invoke an API and then you find tuna language model to pick the right API and I think with wv8 it's really fascinating because um that it's like in instead of doing the open AI function calling approach where you're going to try to describe the arguments to all the functions in like this dictionary if you instead have this like just natural language template that describes the tool and then it can do a natural language command and then you have this like uh compressed model that's like a parser in the middle I see it as like also being more efficient so so maybe if I could quickly get your opinion on that what do you think about like a you know a 1 billion parameter model that's just specialized and say uh text to SQL and then the the most powerful model just has a description of the kind of apis you can uh the kind of queries you can use with API uh sorry SQL and then send it to the parser um no I think that's that's also yeah like very uh compelling I think often my my sense on these things is um you've got to work the problem uh so it's like what are you trying to achieve and what is your cost tolerance and what is your latency tolerance and that sort of thing so like what's the best system it's the most powerful model being used in as many ways as possible uh uh from the perspective of like you know the quality uh what is the best system from something you want to ship or use or experiment with it's definitely not that um so yeah you know bringing models down to make them as small as possible such that they can achieve some part of the wider kind of goal is is really compelling and so the higher level uh Playbook that I see from a lot of places uh which is really compelling is take the most powerful model that you can get which is gp4 uh and use it to annotate some in domain data for you and you got some supervised data and then fine tune it for your task at hand and then you've got a smaller model probably a small llama and uh it is good and at no point have you touched a real annotator which is very very quick and efficient and easy to kind of Dev on um and that's just really compelling and you can you can repeat that Playbook every you want um I should be you know biging up coher models here so like C has some good yeah yeah amazing and yeah so I I fully agree with you on that I I also find that to be really really exciting um you mentioned another thing that really transitions into one of your papers I really want to get your thoughts on as well is what you what are you trying to achieve task whereare retrieval with instructions I love this kind of like you're searching like you know the core duplicate question data set where you're searching to find another question not necessarily the answer to the question so could you kind of tell the story this task absolutely this one's kind of near to my heart um it's funny because like we did the rag paper and then um I started saying like retrieval has two arguments uh there's the query and then there's the task context I can't remember what word I used I think I used context or something um or instruction or you know whatever whatever I was using and I was I was saying this quite early on and I was frustrating my advisor Sebastian rle um we had some debates about this and he was kind of like no that's kind of like just a redefinition of what the query is um and like relevance is more Global than that um anyway uh we kind of I was like there was two things and um I was really interested in that as as he was and we kind of um thought about it and then sh the idea for ages uh and then this was sort of before uh instruction following was really a thing um and it turns out that what we were talking about was effectively a really similar thing to a instruction following Paradigm um except it was an instruction following retrieval model the analogy I used to used to to argue about here was like I'd have you know I'd be in in a library for example and uh if I was using the computer system to search for what I was looking for I would use a query and if I was talking to the librarian I would say what I was doing what the information was for that I was retrieving for uh I would explain the task context I would you know and and then I would say what my query was um and so there's this whole sense of like you don't just retrieve in a vacuum you want to retrieve things again these aren't new ideas but it's kind of like it was a bit zy then anyway we we we didn't do anything thing for for ages and ages and ages and then um aari isai came and did an internship with our uh group and we kind of were like hey this is the time to to hit that uh and so yeah um that was kind of yeah one of the papers that was looking at that and trying to figure out you know how should we should do that and then like loads of papers came out at once all kind of doing the same thing so like thought about this in 2019 or 2020 and then when you do it in 2022 all the pap come out it was a a bit late but um yeah very compelling concept where yeah you want to basically have more fine grain control of what you're retrieving you want to tune your notion of what relevance is in a zero shot Way by explaining it um and that paper was basically about that saying like Okay I want to do retrieval but I'm doing it for the following reasons and you could Define that in instructions you could also Define that as few short examples of what a relevant example looked like um and that system is very useful it's a very very useful thing uh and it turns out what we found at the time was dense models really struggled um so a pure dense model at the time couldn't couldn't do this well rankers could do it relatively well uh but I'm pretty sure that we I don't think we did this but a large language model that was refining what the query should look like taking that task instruction would also do a really good job and I'm sure plenty of people have demonstrated that since yeah when I so when I was looking at I saw I I think the reranking thing works quite strong again reranking for people listening is uh you take the query and the candidate document and you apply a high capacity score to it so if you describe kind of the search intent as well as the query you can rerank it really accurately that way um but so so this kind of language model to reformulate the query I also find that to be really fascinating um but I'm also curious like um is there a way to put put the intent directly in the embedding or something like that to have no I think I think so I'm sure if that I tend to think of ways to do these things and I go to the Leger and find plenty of examples um you can imagine so I think what what I had wanted to do uh when I first started talking about this was have a um you know fairly normal dense embedding space for for the content and then a modifier like kind of extra Dimensions uh that you might concatenate along uh that would you can hook into for certain kind of yeah uh intent kind of modifiers um and so maybe you train two models one that embeds normally and one that embeds a kind of ranky style uh like signal uh that would be specifically trying to attack that like longtail relevance so you say you found your top depending on the size of your database your top 200 and actually you now need to map from your top 200 to your top five and that extra few Dimensions could be the difference um so this kind of I was thinking about this and I was it was a time I was really into in ensembles of dense retrievable models I have a paper that that no one read called boosted dense retriever um that that kind of looks at that and it's not a very practical system kind of an interesting one um that gets to that kind of notion so I think you definitely can embed these things into embeddings um I think the difficulty is uh is potentially figure out what like em like how to embed that is is challenging especially if it's in isolation so you kind of have this decomposability appap thing um that that people talk about maybe with like you know newer multiv Vector models the kind of Colbert Colbert style or whatever's come more recently probably you're much better at this because you need you might need a bit more of a rich interaction um depending on yeah yeah I don't know too much about the cobbert this might be like a dumb version of it but I can imagine the idea like a really popular thing with we8 is the hybrid search where you have like bm25 in Vector search and then you do like a rank Fusion or a score fusion and I can imagine having like eight different phrasings of the query to to search and then some kind of fusion of that also working kind of well yeah no for sure I think so um yeah just all all all kind of different variants of uh relevance being a kind of complex product of of like several different models not complex prod a product of of several different models some of which are kind of doing the good oldfashioned term-based relevant some of which are doing something a little bit more learned and nuanced and some doing something a bit more kind of instruction following based um yeah I think there's kind of ways to attack all of those things yeah yeah super cool so I want to kind of hit you with a couple quick ones and then you know an anchoring question being mindful all the time so uh I another paper this whole edit eval Pier kind of looking at the editing of text instead of just the generation I've always found out to be really fascinating do you mind giving a quick like um overview that yeah sure so this is kind of work actually driven by some of my my ex colleagues I guess at Facebook so credit goes to Teemo um and Jane uh who who kind of LED those two papers I guess respective respectively um but yeah really interesting stuff so it's kind of based on the observation is like humans don't write in the way that large language models do you ask a large language model to write a blog post and out will come the stream uh you know there it is kind of like thing um humans tend to write in a hierarchical way with a great deal of revision and editing in the middle um and and the intent there was to basically say okay let's let's let's mimic that process of like how how humans uh right so they'll write something they maybe write a plan uh so plan edit repeat Pier um and uh yeah you'll find a kind of say all right first lay out the structure of a document going to run like maybe the the titles or like the rough ideas behind each paragraph and then you're going to say go and uh fill out the first section and then have a look at how you did and be like oh I made a mistake uh so go and fix that mistake uh and then kind of repeatedly calling this language mod to iterate over this document and kind of grow it out and so Pier was a model that attacked that uh task and edit of Val as a suite that kind of evaluates that that kind of Paradigm um and looking at how different models can kind of do that and how kind of repeated application of maybe a smaller model can reach the individual kind of like one shot uh um or like one forward pass of a bigger model and kind of interesting um trade-offs that you might see there so um you how you might how you might do that and how you know maybe a language model would be much more effectively applied if it is iteratively kind of um drafting a document with you um and yeah we were we were kind of working towards a kind of demo of of something like a Google doc where the model would would put in um edit suggestions at the side like your kind of editor would like someone suggesting changes and the model would suggest changes and you would kind of together draft this thing so a model could play the role of um the planner uh the editor or um yeah the executor I guess the your executor of the plan um or human could play one of those roles and you kind of collectively would collaborate to produce this this document but yeah um those ones are kind of I I was helping a little bit but mostly Jane and Teo and uh Sebastian rle kind of um uh research lead of the group uh with the kind of main people driving that yeah amazing I I thought it was such an exciting paper and to me that's kind of like the genius of Lang chain is kind of like coming up with these plans sort of where the llm is like a just a compute unit in one part of it and I love that you mentioned that you could have a specialized model in checking for uh paraphrases and maybe a 50 million parameter model or like arbitrarily chosen I have no idea or like um this idea of um at one step you come up with like five continuations and ask for human input I know that's one of the langing chain things and that yeah all that's super exciting so um Patrick I've already I've already learned so much from the podcast there's so many exciting directions for the future already but let me ask this question that I always try to like end podcast with is like um you know what what on the horizon is exciting you the most um yeah that's that's that's a great question so um I think I am I'm interested in language agents as is you know a lot of people are but yeah effectively kind of an agent and within it is a language model kind of powering how it works but effectively you know models that have a a memory uh and go out and research things in the World and Achieve kind of intellectual effort for people uh and kind of will I think it's going to unlock very interesting things so there's like there's dangers here uh aware of those need to mitigate those but um kind of automated researches uh giving people the ability to kind of do clear-headed analysis research uh understand things build kind of uh grounded ATT tributal kind of um documents through the kind of process of of research which involves kind of formulating hypotheses going out acquiring evidence uh reformulating a hypothesis and at some point saying I have done my research I'm going to write up uh agents that can kind of um mimic that process is the kind of thing that I'm excited about uh for the next maybe year or two yeah well we also love that idea and we're calling it uh generative feedback loops to describe like the generative model feeding data back into the database and this kind of loop and so I'm curious do you like that term and if so could you please write another viral paper that it's funny it's quite similar to one of the ways to described this before we kind of started to work on it was an auto regressive research Loop so it's almost the same yeah know it's it's good um and I like the idea of a model I guess this is a slight tangent um managing its own context window which is effectively part of this process you say like it has an ex a memory it can write to it can take things out and if its memory maps to its context window uh you can kind of think of it as a model that kind of um looks after its own context and kind of has a working memory that it's in charge of which I think is also a really fascinating concept um but yeah like certainly models that write to a database and kind of build up this uh more long-term memory is is really cool uh for sure like kind of taking notes as it goes uh fascinating stuff yeah I've been fascinating that uh like that mem GPT paper where it's about like uh what do I add to i' I've done like a a search and now it's like what do I add to my working context as well as trying to blend that what do I write to my long-term context trying to get that right has been yeah yeah amazing really great yeah Patrick thank you so much for joining the podcast I can't tell you how much um you know your retrieval augment to generation paper back in 2019 I was still in in school and watching it just like profoundly changed my understanding of deep learning it's such an honor to have you on our podcast thank you so much thank you it was great to be here ", "type": "Video", "name": "patrick_lewis_on_retrievalaugmented_generation__weaviate_podcast_76", "path": "", "link": "https://www.youtube.com/watch?v=bHrYZQOEV_Q", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}