{"text": "Thank you so much for watching the 23rd episode of the Weaviate Podcast! This episode dives into a new technique for ... \nhey everyone thank you so much for checking out the wva podcast today i'm here with ori rahm the lead author of learning to retrieve passages without supervision i think this is such an exciting paper about self-supervised learning particularly for training these retrieval models and retribution retrieval models again with wva this is what we use to produce the embeddings of say text data particularly for this paper that we use to retrieve to find the nearest neighbor with our query and this new work is a super exciting way to train these models self-supervised meaning that we don't need to label data and this can work with just say you know wikipedia or archive or you just drop in a huge text corpus and it can bootstrap a loss function to optimize a deep learning model to produce high quality representations of data so enough of that for me ori can you explain what's new about this paper and the ideas behind spider recurring span retrieval and all these cool ideas yeah sure so uh first thanks for inviting me i'm super excited to be here um yeah so regarding spider so recently we released my colleagues and i from tel aviv university we released a paper on unsupervised learning of dance retrieval and basically the main idea of our training system is to use the notion of recurring spans which are engrams that appear more than once in a given context or in a given document for example on wikipedia um to uh to construct pseudo examples for uh for contrastive learning but in a totally unsupervised fashion um and uh yeah actually we were uh i guess we'll talk about it but yeah we were quite surprised by how far we can get uh dance requires like how good can they be without using any label examples during training at all um and yeah that's that's spider basically um yeah so can you so without kind of getting too much into the whole research story i really want to hear about your perspective around self-supervised retrieval models just so we can kind of stay on that topic a little more compare it with say supervisor retrieval models again the overlapping engrams yeah sure so um you know for the perspective side uh in the perspective side story um so i think that when we started this project um there were mainly either supervisor recruiters in the dense side of the retrieval and obviously unsupervised ones in the more lexical spar side of uh retrieval and yeah uh supervisor troopers were ma were really good for the for their data set the data said they were trained on but they but they started i mean uh we started to observe and papers started to come out and show that they're actually not that good for out of distribution and uh you know even in the same corpus but other types of questions uh then they fail quite uh quite miserably i'd say um and our intuition was that if we can uh if we can come up with a good signal for self-supervised retrieval uh then maybe we we will not overfit to specific distributions of questions and corpora and all that and and be able to be more robust and and yeah so i think that actually like during the project we we did observe this i mean we did confirm this hypothesis i'd say that um i mean we'll jump to the results i guess uh later but um spider for example is better than dpr uh in the you know in the zero shot setting without seeing any examples at all uh even you know in very close settings to where dpr was trained on for example if it was trained on natural questions and you evaluate it against trivia qa then in this setting spider an off-the-shelf retriever that didn't observe any uh um labeled example during its training is just better so um yeah so that's i think uh um that's a hypothesis that we confirmed and and also other papers like parallel uh to us i also showed very similar uh observations and um another uh and and from another uh perspective uh they're also these pre-training methods are also very crucial for the success of the um also in the supervised setting so for example if you take a spider and fine tune it you know to uh to specific data sets then you would just get uh better models for this data set and better transferable models that better transfer to other data sets as well so um yeah so we were really excited by this uh you know by this direction and we still believe [Music] a great believers i'd say yeah i think um so so from my understanding of the experiments we're training on wikipedia and we have these different question answering data sets so say we have natural questions trivia qa web questions these are mostly question answering data sets derived from wikipedia is that correct yeah yeah so dense passage retrieval which is the supervised learning baseline is trained on say labeled open domain question answering data sets so basically what the way that this works is the the data labeler will receive some context like say it's a paragraph from the wikipedia article about oxygen and they'll they'll come up with a question and an answer from that context and then the information retrieval task then becomes given the question can you retrieve that context so that's the training data for the supervised baseline so it sounds like the spider models trained on wikipedia with the self-supervised learning task are able to outperform in in the same uh data distribution sort of if like so dpr has the train test split and dpr trains on this train set of questions this test said they're both being evaluated on that same test set yeah so i feel like that's a huge result especially because you can keep going with the data with spider right like yeah yeah yeah yeah exactly so spider doesn't outperform dpr on the same data set that was trained on but it does beat it quite consistently in other data sets over the same corpus yeah so yeah yeah and i think that's just absolutely remarkable and um i think quickly before going a little more into say the spider algorithm recurring spanner treatment we're going to talk about the query transformation how this mimics bm25 lexical as well as this kind of uh you know neural network style fuzzy matching but uh yeah i kind of and i've lost my training thought a little bit i don't remember all these topics but i want to talk a little more about the the scaling of data because it's self-supervised so it's like you know we can we can grab a bunch a bunch of more data so going from wikipedia to say archive or out of legal contracts like yeah or just web scripts what happens as we scale this algorithm um that's a great question so actually i think about it quite a lot um so we didn't uh try that on other corpora mainly due to you know resources because we're in a university and all but um i believe that um it will work on other um domains as well especially on on domains like news and archive and stuff like that um yeah i i i don't think that there's nothing uh uh very we wikipedia-ish about uh this uh training paradigm um except for the part that maybe perhaps wikipedia has like more repetitive nature than maybe other domains but um i don't think that that's the case for news for example and many domains that require retrieval especially for open domain question answering so uh yeah that's a great question and we might um i mean we do think of uh going like on a larger scale and maybe training another spider on more data um but i i don't have a concrete answer to your uh to your question but i do believe that it will be pretty good on other domains as well yeah yeah i can from kind of the vva perspective i'm so excited about how this interface lets people say say you have like a blog or let's use the blog example and you have like 100 articles and you want a specific retrieval model for your blog and your specific language i love how these self-supervised learning algorithms let you just you know have a way to train it without labeling the data without retrieving the context deriving questions and however long that might take to get like a hundred thousand to push an answer yeah i think that would be a great time for us to dive into how recurring span retrieval works so yeah um okay i can i can tell you my head oh sorry yeah yeah go ahead go ahead and ask your question oh so i can tell you my my understanding of it so far is we look for these overlapping engrams within the same article and so i think that's another really interesting use of inherent structure in these in say documents and that's the thing about self-supervised learning is you're trying to exploit structure and labels that are already kind of in the data so you take an article like oxygen and then you use overlapping engrams so say they share the snippet uh atomic number is eight or you know some engram like that and you use that to form the positives and the negative is some you know some pat some passage 100 word chunk that doesn't have that engram and then there's a query transformation that's applied so 50 probability flip of a coin you either delete the engram from the positive or you keep it and that's kind of the idea behind uh having this sort of like fuzzy reference where you're saying uh i don't know who was blank like kind of referring to it in that kind of way so uh is that yeah exactly um yeah just one uh one minor i mean you describe it really well but uh the um the masking that the deletion of the span is is not from the positive passage but rather than uh in the queer the passages remain the same throughout our training and yeah the yeah the idea behind this masking is that you want to be able to model both lexical overlap which uh which is encouraged in the case where this uh engram does appear in the query and also appears in the positive passage and then we can say okay there's a term overlap between the query and the passage uh but when you mask the when you delete this span from the query um the pre-training test becomes more semantic in a way and more about the context of the of the query and uh yeah in a way we try to you know to model both this these complementary skills um that you know uh um one is uh attributed uh to dense recruiters and one is more attributed to sparse retrievers but eventually i believe that we want all of them to be just like in a single vector or maybe multiple vectors but in one model that is able to model uh this interaction both in a semantic way and in a lexical way between the query and the corpus so that's what we try to to model and the query transformation is mainly it is both about this and about um being more similar in distribution to questions or to queries of search engines or whatever because we don't want it to look like a passage like a long test if we want it to look like a relatively small window short window uh so we can control we basically control how long is the query we sample it from our uniform distribution between i think 5 and 30 tokens around the recurring span and then we just randomly either delete it or keep it in the query so yeah but the passages both the positive one and the negative one they remain the same throughout training so one more and a lot of ideas in that i can't wait to get back into say hybrid retrieval where we combine bm25 as well as dense retrieval models and get into that discussion one vector to rule them all compared to the multi vectors and nothing but i one more kind of detail to the algorithm i wanted to get behind is the span filtering so when you do the overlapping engrams you probably get a lot of like uh you know as the is right like conjunction or you know like phrases that don't have any real meaning and thus positives they would be useless so how do you you know process all the engrams yeah totally so yeah that's a great question um so we did want uh to to filter out spans that we don't think that uh bear any meaning um and but on the other side we didn't want this this filtering to be like a model dependent you know because many works uh uh that used salience fans in in all sorts of ways they mainly used they mainly did something like you know named entity recognition or something like that and then they took the the entities uh and and treated them as aliens fans but we wanted something that is uh totally unsupervised doesn't depend on a trained model in any way and thus are filtering included uh first uh removing any engram that is um that it only uh includes stop words so for example your example with as the and all these words they they don't cut they don't uh contribute to the um to this uh engram uh uh process and uh we didn't take any engrams that any any unigrams uh uh but with that uh we looked only on spans that are uh two that are by grams or longer than that um and i think that's it maybe we had some more filters i i don't recall but um these were the the main ones um because as you say i mean you can easily get with like uh of the uh in them and they will dominate the whole distribution of your uh stands and that's not what we want to achieve um so yeah but i do think that uh further that we can further improve these uh filters and and you know have and be more precision oriented maybe then recall oriented that that's a trade-off that that is really important and interesting to to explore but um we kind of stick to our initial filters and and kind of uh uh used them in a they were constant uh throughout our research so yeah can we get a little i want to get a little more details about these salient span masking how does that so you have another model that classifies some salient span that would be useful for positive what would be the thinking around that kind of algorithm yeah so maybe like in a general way salient span masking is used for example uh for training t5 for close book open domain question answering and but not in the context of uh recurring stands so these are kind of orthogonal things right i mean recurring spans are in a way uh like a self-supervised way to obtain salient spins i'd say um so in the context of spider for example uh you could think of uh of a process that uh you know looks only on the overlap of some salient span from a named entity recognizer for example and its overlap with recurring stands and only taking those as like a higher quality set of spans and be more precision oriented in that manner than reoriented but and it may even improve i i'm not sure because like any other self-supervised method it is very noisy but you know you use large batches and and in a very long training process and you just uh you hope that you know the the signal to noise ratios is just uh you know that it's uh that it's good enough and in our case i believe it was good enough but but then again uh uh maybe you can improve it even more and due to the fact that it's self-supervised and you can do it like on very large corpora being precision oriented doesn't necessarily um hurt you in any way maybe you can just look at what on a larger scale of data and be and be very picky on what on what spans you take from it uh and you can still train for very long and very large batches and maybe your signal would be even better really interesting so the ceiling span masking idea is like when we're picking that token we're going to mask out or even we're masking out a whole span in models like t5 you don't just replace a single mass token you have like five words that it's gonna you know decode so you so you try to find the phrase that would be you know the most information dense and almost sounds like an active learning kind of way where it's like what would be the most effective next training update yeah that's so interesting how the self-supervised uh overlapping engrams can bootstrap what's the most selling thing so i'm so excited to get into the training details could we could we start with uh you use the info nce loss right where you have the batch of negatives could we start with comparing info nce with say triplet loss yeah sure so um yeah so we were kind of inspired by the dpr framework uh in our uh in our training uh so what they did is simply take they took large batches of questions and each question had its positive passage which is labeled and the hard negative passage which came from a high-ranked bm25 passage uh which doesn't contain the answer and then basically their training looks like for each question you have like a multi-class classification problem which takes these two passages and all the other passages of the other questions in the batch and try to optimize the model such that the representation of the question will be similar in uh in the dot product sense to the positive testage and and far away from all the other all the other pathogen beddings so that's uh i mean this this framework was kind of reused in many settings in machine learning and some call it as you call it uh influenzae some call it uh the contrastive loss today because [Music] uh you know uh i guess uh since the simclear paper or or something um so yeah and and that's and that's very natural because in deep learning we we use the cross entropy loss all the time and this contrastive loss is basically just cross entropy such that you know every question defines a multi-class classification problem which if you could you you would want to do over the whole corpus but you can't because it's too costly so you just estimate the partition function from the from a sample of negatives from the batch and this hard negative that you um that you bring uh uh you know um the specific uh uh hard negative uh tries to um to be like the like like it's called the harder uh like making your task harder um because if you just sample from the corpus then you uh basically the test can get uh too easy and we also observed it in the paper we have ablations on that uh in the paper and they have it in the dpr form of as well um so yeah uh in comparison with the triplet loss so i think that it's just more natural in deep learning to use the something like the cross entropy and contrastive loss so people use it more often and we didn't compare these two because we just you know um we didn't try to apply the objectives but i do believe that uh they'll probably work better than the triplet losses uh but i i'm saying it only because that's you know the common wisdom of deep learning i guess and yeah so taking from so so this is the dpr framework and we wanted to and it and it was very um uh very effective so we try to think uh can we take this framework and replace those uh question passage pairs which which are expensive to collect and are very hard to collect in many domains and many scenarios and can we replace them with pseudo examples and and indeed our pre-training scheme looks very similar to dpr in that sense where uh as we discussed before we have uh this uh pseudo query uh which replaces the dpr question and we have this positive passage which replaces the positive the dpr positive passage and our hard negative is uh doesn't um we didn't uh fetch it using pm25 or something it's just from the same document and due to the fact that it's from the same document as the positive passage it makes the task a lot harder and a lot more semantic in a way where lexical clues on their own can't get you that far in terms of discriminating between these two so uh yeah i think in table four in our paper we show that removing the hard negative passage from the document and from the batch results in a in a much in a in a high significant drop in performance at least in the zero shot setting i'm not sure about the fine tuning setting but in this in the zero shot setting it's pretty significant it can get to like i think 6.7 points of uh recall k which is pretty uh significant so the presence of power negatives is extremely important for this contrastive learning framework both in the supervised setting and in a unsupervised [Music] yeah i think the hard negative is so interesting the the use of the inherent structure in the documents compared to these other kind of things where you know say the the positives are overlapping engrams and then the negative the negatives are just from other articles so it's like totally irrelevant it would be like two passages about oxygen and then it's like lebron james yeah eiffel tower without that structure yeah so so you will get something out of it but um this uh fine-tuning uh hard negative is so important for the semantics of all the of the model yeah that kind of like reasoning i had one more like kind of just nitpicking my understanding of info nce multi-class so to me what makes it not multi-class is you have to you do that like batch matrix multiple where you have all the dot products right so you have this big matrix that has all the uh i'm trying to do but you have all the dot product scores of everything in the batch yeah so then my question is so when you have this big matrix and in the paper you describe eight gpus scaling that out with the horizontally with the gpus how does that work how do you distribute this big batch matrix multiplication that then gets compressed then to multi-class classification oh yeah so yeah so the so the scaling part the the distributed part is mainly in the transformer part of it so um so we're doing like a distributed data parallel training where uh you have the same you have the your your model copied around uh across these eight gpus and then you divide the the batch to like eight parts and and uh both the questions and the passages are divided to these gpus and then the transformers are in parallel they um they compute the you know the representations and all and then the horror present all of the representations are gathered to the same gpu to gpu 0 and then this matrix multiplication happens only once in one gpu but it's very um it's very cheap compared to the to the transformers encoding your inputs so that's not such a big deal and it the matrix is uh something like it's actually its size is like your batch size times uh twice the bedside because each uh each question has both uh like a positive and a hard negative one so in our case it's like 1000 over 2 000 so it's not such a such a big matrix and uh yeah from what i recall uh it's it's really this step of multiplying the representations is quite cheap compared to [Music] the transformers calculations so what goes into the the code with that with eight transformers in parallel sync up to one node to do the batch and then get the loss and then send it back to each of the transformers how is the code what extra challenge comes with doing this um so it's actually not that i mean it's it's much simpler than what it sounds i guess um so torch has you know it's uh it's uh distributed um uh framework and basically you have this uh gather operation where you just you simply tell the gpus um fetch me please fetch the this tensor from all the gpus and i want it concatenated over some dimension and then you just get and you know the whole back pro and all that it's just implemented for you and you don't need to even um you know to worry about it so uh yeah i guess it's much simpler than than what it sounds and um um yeah that's it i guess so from your expertise i'm really curious to get your opinion on do you think the abstractions implemented in pytorch for distributed training for this particular idea is is it enough that it's easy enough to use compared to say needing some new model training software to say make you know more people able to do this kind of training um so i guess that uh like there are many papers that implemented contrastive learning in many ways for many for many tasks for many scenarios so um i think that you know the initial um effort that you need to put in order to um to set up your own framework is is so uh you know it's so small compared to even when where we were uh only like you know four or five years ago where you needed to implement your training pipeline from scratch for any task that you wanted to work on and today you have all these infrastructures like hugging phase transformers and hugging face data sets and many others and everything became much easier than what it was not so long ago so yeah so i believe that the effort is is not very um very significant and one of the so i'm really amazed by say the google collab gpu everyone's got a gpu now one gpu though yeah could these models be fine i know the the fine tuning experiments happen with say eight rtx's compared to eight a100s could people fine-tune these models for their data sets with one gpu yeah um that's a great question uh i think the answer is absolutely yes um so for example in our fine tuning experiments i'm not sure if we mention it in the paper we used only two quadro rdx gpus so even if you have only one you can lower the batch size worst case you can do some trick like maybe gradient checkpointing or something like that and maybe the training would be a bit longer but you'll get the same effect or just uh or just decrease the batch size and in the supervised setting it's not that uh crucial what is the batch size that you use it very far correctly in the dpr papers it's really a minor improvement from like 32 questions to 128 um so to your question definitely you can fine-tune such models on on a single gpu and also there are other tricks for example like just do weight sharing um from the question to the passage use the same weight in the question and passage encoder that this will um this will the model will be two times smaller but the results are are pretty much the same and um yeah so i think fine tuning is is really uh efficient and very cheap compared to pre-training where we had to use 8 800 gpus which are pretty expensive and other papers like the contributor for example also used uh you know they used i think even more gpus i don't remember exactly how many but during pre-training the um the size of the batch is much more uh has much more impact than at fine tuning time so uh so in terms of fine tuning you don't have to worry about the gpus but at pre-training i'd say that it's it's a lot more important to have this large scale both in terms of the data and in terms of the compute um yeah and but perhaps a way to overcome this issue is through something like moco or something like use a memory bank of passages and um just have like um kind of like an artificial uh batch in a way that's how i think of it and that's a lot more a lot cheaper than just calculating the batch um each time so yeah that's that's also another option to how to like um decrease the the compute that you that you use and probably get the same results i believe yeah it's super interesting and i suppose one detail on the fine-tuning thing is i'm imagining you know wev8 users who are using the vva search engine for say their specific text and their blogs do you see say for them use spyder as an off-the-shelf model similar to how we're using a lot of these mini lms in the sentence transformer library just kind of you know using that in the zero shot and then so if we think about fine tuning should they fine tune with the engrams the recurring span retrieval idea or should they just you know make their own little question answering data set using the labeling scheme we described where you you know pop up a context derive a question derive an answer and what would be the best way for that's a great question um so if they if their corpus is wikipedia then i definitely say just take spider off the show make sure that the format of the passages uh is exactly as as it was during the free training of spider which is 100 words and you'll definitely be okay and you'll have a pretty good retriever and maybe even use a hybrid retriever over uh like take a spider and vm 25 and you'll get a retriever which is better than both of them if you have the ability to even annotate uh as few as i don't know hundred examples then you probably get uh a significant boost in in the results um specifically for the for the distribution of your annotation uh example of your annotated examples um yeah so i think that's the that's the best recipe if you have another corpus if if you're not uh uh using wikipedia as your purpose then i would say that you can take spider and just fine tune it with repairing spanner people on your specific uh corpus and um make sure that your passages during pre-training are just formatted as uh specific exactly like you intend to use them uh at inference time and if that's the case then i believe that uh it will be a really effective way to get a good retriever to your domain and and again another boost will probably be obtainable by uh by using a hybrid approach with 1.5 yeah so now i want to dive into the hybrid quickly i do want to see say that yeah that query transformation id is really clever how you you know the window length makes it more like a query with search engines and i think that whole thing is super interesting idea but so can we talk about how we combine spider with bm25 what's the idea behind hybrid retrieval sure so i think okay so uh first we're not the first to show that using both of these of these models together result in in a better model that improves over both of them quite consistently so i think a few months before our paper was um a paper by uh uh some authors from uh university of uh uh um [Music] actually i'm not sure where they're from i think maybe waterloo but i'm not sure uh the jimmy liens left uh so um so they showed uh this uh phenomenon for dpr and they showed that when you use it uh in a in a specific way you get uh you get an improvement over dpr in contrast to what the dpr paper showed where they uh where the hybrid model was uh was sub-optimal compared to dpr itself um so but the only difference was that um they use the hyper parameter that scales the scores of bm25 and dpr and they find and they and they perform hyperparameter tuning for this hyper parameter on the deficit but in our case we didn't want to do that because uh we wanted to be in a zero shot setting where you don't assume you have any labeled examples so we just uh uh so we decided to uh assign this uh hyper parameter to the the value one and and uh that means that you basically sum uh some of the scores of spider and bm25 and uh their sum is now your hybrid model score and this approach uh is very uh is very effective turns out and it improves over both of them consistently and quite significantly so i'd say that if you have a retriever over your own domain um be sure to to use both of them if you want if you want to be if you if you care about your retriever be to be as good as you can then i definitely say use both of them and if you can annotate some examples of your own then it would even get better and then again use it in a hybrid fashion with bm25 and that's basically the best model uh that you can get today so i have a couple questions about how this works and first i want to get into so we we're just going to sum the bm25 score with the uh with the dot product or l2 distance cosine similarity from our dense model is there any maybe normalization of the scores that should go into this i also saw a devil in the details in the paper that say say a passage wasn't in either of the um bm25 or let's just say a dense you know i'm sure you know yeah yeah you just yeah that's an important detail so what we did is uh and we took it also from their paper is that obviously the set the top k uh retrieved results of both retrievers won't be the same so one approach you can take is simply recalculate the score for uh for the second retriever for for example if you have a passage that came from vm 25 and was it in the dpr set or spider set you can either explicitly calculate its score its dpr score but then you need to um you need to to feed it to the to dpr and all so instead of that we just used a very simple trick uh in which you just take the uh in the example that i just gave for that you have a bm25 passage um but it wasn't in the spider score in the spider set sorry then you just take the lowest uh set the lowest score in the spider set from the top k review results and you and you just assign this passage uh it's its spider score will be now this lowest score and that that's just good enough you don't you don't need to do any more than that but um it would be important to to test whether explicitly rescoring these candidates will will result in a better in a better ranking i'm not sure about it but it does require some more effort so this simple approach that i just described is is quite effective so so sort of the in you know maybe tabular data pre-processing for machine learning we do this min max scaling with each of the features because they have different ranges so you know that kind of x minus x max divided by x min something like that to keep it on zero one for the dead scores and the bm 25 scores that kind of normalization of the values of these things before we then have that linear addition without the you know useful um that's uh that's a great idea i'm sure we didn't think of it um yeah maybe it can work maybe even better than what we did we didn't try it but you do need to i mean bm25 has this property that it's um its scores are length dependent right uh as the query is longer and the passage is longer you will probably get higher scores while in dpr this is not the case you will always have pretty much the same uh the same scores because you just you have these vectors in the same space and just multiply them and they don't really depend on the length of your query or length of your passage so um basically you will need to think how you um how you normalize it in a way that is robust across different lengths of the query and that will work both for short queries and for long queries but uh i guess that it's somehow solvable i believe yeah that that the the query passage length i do that's such an interesting detail with how the bm25 works i've seen some variants like you know bm25 plus there's like little different ways of kicking this work can i and well quickly can i ask about the reciprocal rank fusion idea where we combine the scores not uh combine the ranks from bm25 and dpr or dens retrieval not by the scores but by the ranks sorry can you repeat the question ah sorry so so instead of doing a linear combination of the scores bm25s produced you know spiders produce we might look at the just the ranks solely you know yeah um yeah that's also an interesting idea we didn't try that but it may it may be very very effective i believe um i'm not sure how exactly like uh uh you would do that maybe as you said with their proton rank or something but um yeah i mean that sounds like a very nice idea to to check out yeah um yeah like how better uh i mean it's really interesting to to uh to do some sanity check of like how uh better can you get like how much of the you know the union of the two sets are you able to um to squeeze into the top k that actually is interesting for you uh given that you don't for example in open domain question answering you want the top 100 or top 50 or top 20 to be as good as possible and don't really care about the ranking inside them that's often the case so it's really interesting to check out whether how much room can you like uh given that you uh fixed the retrieved set from bm25 and uh and dpr uh uh how like uh how many of the um like how often does it happen that you have some good passages that are left out in the um when you do linear combination and are left out in the outside the top 20 and then you you understand whether you have whether you want to improve the combination or the hybrid model in a way like you described but i'm not sure whether there's a lot of room to improve in that aspect yeah i think the i think it can maybe like yeah like as we design these pipelines maybe you're doing well the idea that you've just presented say we want to have some diversity in the result it's not necessarily about make the number let's get the number one thing to be the most accurate thing we want to have some diversity if it's say getting pipelined into a question answering model so so i guess that kind of changes the downstream application case changes a bit how you want to uh com combine ranks you know is it a search application in which i want to give you the absolute best thing number one then yeah let's optimize it like crazy whereas if it's qa you might want automated qa2 you might want to yeah it's only relationship i totally agree it's it's really uh application dependent and that's why i believe that in in many um search scenarios you you want you want to like uh to run across encoder or re-ranker in order to to get the the best result in number one or number two and not in the top 20. like like the open domain case where you don't really mi you don't really care about the ranking inside the top 20 or 200 um so that's what that's something that a cross encoder uh or a re-ranker uh can can get you like uh uh can is still much stronger than just a simple retriever so so so that's normal so that's another reason why you can claim that it's not that important to optimize for for the top one of the retriever because if you um if you intend to run a re-ranker and in in any case then you don't really care about the ranking of its like the initial ranking of its uh inputs right yeah the cross encoder is potent that thing but it's expensive to do a bunch of pairwise classifications so we want to try to reduce that so on the subject of efficiency uh what goes like at wva we're super i like at least me personally i'm super aware of this h sw vector index structure for doing the retrieval and find the nearest neighbor with the retrieval i'm kind of curious with bm25 how do how do you have efficient search or is it's just a scoring metric could you tell me a little bit more about the speed of computing bm25 scores between query and then massive collection yeah that that's a great question so we kind of used uh uh pi sirini of the show for bm25 and from what i observed it seemed like it's a bit slower than dense retrieval but i'm not an expert to you know the efficiency um to the efficiency aspects of dance retrieval versus sparse retrieval i know that many people um i guess have like different opinions about that uh topic and in dance retrieval you also have um you can pretty much control the the efficiency of your um uh of your index by uh like in the so hnsw can uh um like you can do like a more course grade search or you can do exact search or you can do uh you can do all sorts of things and obviously doing exact search is is a bit uh is a bit slower but it's not that uh it's not that bad i guess like over wikipedia it takes i think something like um uh some hundreds of milliseconds uh for one query but then if you do uh an approximate search then you will get like uh just then it will be very very fast uh in terms of bm25 i'm not sure whether piscerini is like the most optimized library because i think it's a bit more research oriented than efficiency oriented i would want to say like uh i mean maybe maybe not maybe i'm wrong but uh that's that's how i see it so as far as i can tell and from what i remember it you have like its is something like um some dozens of queries per second something like that um yeah but specifically our work we we didn't try to optimize this aspect of query efficiency because our you know our test sets are relatively small so we didn't really care about that but there are many scenarios where you do care obviously like if you want to i don't know do retrieval in a very large scale or just have like a very quick uh response to a user uh but then i'm not sure what what is the best way to go in that sense yeah i think man i'm not an expert on the bm25 how exactly i know that i think it has something to do with inverted index i know there's things like elasticsearch that have built up the you know really how to do this efficiently and just hoping to get a little nugget and keep getting my understanding of yeah efficiency on that uh i believe that gpus can also accelerate the dance retrieval as well so we used cpus for our dance retrieval system but um so we didn't try to use gpus for them but uh i do think that i saw in some papers that if you uh if you do um store your index on gpus then you can get quite a boost and that makes sense and also maybe you can pair uh you can do like multiple queries in in parallel and just um and then you can scale it like on many gpus and and basically get to the whatever performance you want um yeah i think i can comment on this quickly and then maybe you know eddie and or someone from wva can clarify this is correct but with with hnsw we have a hierarchy graph where you're it's connected based on this it's a proximity graph that is layered so the gpu idea is you want to do a bunch of vector calculations at once so we can trip so we can traverse a big set of nodes in the graph this visualization is doing it for people but you know that's how that idea of i think gpu accelerated distance calculation also plays with the hierarchical structure of the nation sw proximity graph but um so all right thanks so much for these technical details i think this is you know such interesting so many interesting details to this could maybe step outside and could you describe to me sort of the meta around your research career putting this project together sort of your plans future work that whole kind of thing uh yeah sure um yeah so i think that this work connects to to some other papers that i um that i work i was part of sorry um i think i'm uh that i'm super excited by uh by unsupervised learning and self-supervised learning and future learning and basically learning from very few examples or not at all no examples at all and yeah i don't like to work on supervised tasks that much because it's it's too often you get very minor improvements from modeling [Music] um changes and in the self-supervised uh setting or flue shot setting then everything is amplified and um you can really see the signal much clearer and in a much more um yeah in a much more amplified way uh and that that's uh and that's why i really like it um and also there's i think there's more room to like creative ideas and how to use the structure like you said before the structure of language structure of images i don't know structure of whatever domain you're working on and rather than just fitting your data into uh into a model in a supervised way which is okay but uh um thinking of uh of what is the specific aspects of your task of your task that that you can leverage in a in an unsupervised way or in a self-supervised way uh really excites me personally um and i'd say that rather than that i also i'm really excited as as we all by language models and large language mods and whatever you can do with them which is quite a lot um uh so and now i'm super excited by the i'd say intersection of language models with retrieval and with dance representations and there's like a bunch of new work on how you do use large language models in order to rank or in order to produce good representations and it's uh it's very exciting and this whole direction of generative retrieval and stuff like that and it makes uh and it really makes sense because we now have very powerful generated models that we can use and um the question of how to use them for your task is very similar to what i said before like how do you how do you take advantage of the specific aspects of your task uh in a way that language models can can contribute to so i'd say that the intersection of uh unsupervised learning future learning retrieval and language models is basically what i uh what i'm most excited about awesome that's so exciting and well really quick i just want to get a little more on the on on how you see retrieval and language models yeah this a little more so do you see putting these large language models in the search pipeline so you know a few shot learning say we have a query intent classifier you provide a few examples of uh the query is hey i'm looking for a shirt that would match these shoes so do you want shoes to be returned that you'd say like it's a compliment compared to say a substitute or an exact search like queer intent consultation and with the language models you could provide a few demonstrations of this and then the language model can make future inferences so it so are you thinking about it from the angle of you put the language models into the search pipeline to enhance the search experience or the sort of wreck deep mind retro retrieval augments of language model inference where the retrieval is like here's the information you need and the language bundle is like all right i can just generate anything uh well that's a great question i i'm a great believer of both directions actually um so i i do i do like um the retrieval language models direction because i think that there's uh there's a limit to where you can get when you put the entire uh like your entire uh um you trust only the parameters of your model and indeed you get to i don't know 200 billion parameters but i'm not sure that we need to get there uh if if we use our data more correctly and that includes retrieving the relevant stuff to what you want to do and just have your model be conditioned on this relevant information and then on the the other direction that you said is also very uh exciting in my opinion and it's how you do like uh the opposite right how you use language models to improve a retrieval and i do think that it's um it's very promising but i'm not sure that it should be within the pipeline rather than use it as a supervision to train for example dancer quivers like um like recent work from facebook where they use the uh the re-ranking of t0 to supervise um to supervise uh actually it's unsuper it's an unsupervised model uh which uh yeah which and and it's only in a zero-shot manner so yeah i do think that we have so much knowledge in language models and and like distilling it into retrievers i think personally i think it's the future um and uh yeah so so both directions that you mentioned i think that they're very uh promising and i believe that they're the future of language models and the future of retrieval and that they'll be uh like interleaved yeah super interesting ori thank you so much for coming on the web podcast is i learned so much from this conversation i really enjoyed it and thank you so much thanks for inviting me connor ", "type": "Video", "name": "ori_ram_on_learning_to_retrieve_passages_without_supervision__weaviate_podcast_23", "path": "", "link": "https://www.youtube.com/watch?v=IWAxkHspjEM", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}