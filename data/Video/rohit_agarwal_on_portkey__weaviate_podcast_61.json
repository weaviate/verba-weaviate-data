{"text": "Hey everyone! Thank you so much for watching the 61st episode of the Weaviate Podcast! I am beyond excited to publish this one ... \nhey everyone thank you so much for watching the weba podcast I'm super excited to welcome Rohit Agarwal from Port key portkey is a super exciting company making it easier to use llm serve them in production and routing to different LMS saving costs all sorts of interesting details that we're about to dive into firstly Rohit thank you so much for joining the podcast excited to be here Connor looking forward to the discussion awesome so could we kick things off with kind of the founding vision of Port key sort of the problem that you you know set after to tackle absolutely so I think when building llm applications will last three years or so at multiple companies and as we built out those applications we realized that building out the first version is super easy you can do it as part of a hackathon and you get really excited because it's been 48 hours you've built something that's worthwhile it's valuable and llms and Vector DBS are like really powerful tools that you are disposing so you build it out and yeah you're now excited to show it to the world uh and the minute you start doing that or getting to production you realize that a lot of the engineering challenges that were solved in the traditional engineering world are probably not solved in the lln engineering world so think about uh you're so used to using data DOC for logging but how do you put these large chunks of strings in data dog it doesn't work similarly Vector databases behave a little different than regular databases so how you're monitoring them how do you make sure that your app stays reliable secure compliant a lot of these things I think the gold layers of devops have not been built for the llm world you know an example I often like to take is for every other API you get a success or an error so it's very deterministic but for an llm or even a vector database it's like it's all probability everything is between zero and one so every system sort of has to adapt to this new reality of probabilities and not deterministic outputs so that was sort of the background where we felt okay there needs to be an llm Ops company uh we had written down sort of a Manifesto back in December January on what this could look like but then we started building this out uh in early March is so many exciting points I like the you know data dog the monitoring thing is core topic to weave yet I want to stay like hello llm Ops versus ml Ops maybe uh separate the two a little more absolutely I think and it's very interesting I think they're they're very joint to some extent I've had conversations with people where they say that llmops is probably a subset of envelopes um I would disagree a little bit I think the difference core difference is that mlops focused a lot more on servers while llmops is more on Services uh for ML Ops you had core metrics like drift accuracy Etc whereas these terms are probably almost unheard of in the llm world unless you're building your own core foundational model where you'll worry about drift Etc 99 of the use cases you're using a deployed llm model that's pre-trained and steady to use so then you're more worried about using the apis what's the latency of these apis how accurate are these which is where you know stuff like evaluations output validations retry metrics or if these things come into play versus the amylopes metrics so I think it's a little bit different analog in in some places it can be extremely different which is where I think even think of the concept of test data and train data right for an llm a majority of the companies are not doing any testing or training at all if you just pick an API and just start at it so I think that's that therein lies the big difference between ml Ops and llmops yeah I think before we step further into maybe almost the more like academic topics around the whole you know LMS and machine learning I'm I'm just kind of really curious about like the state of the market kind of like you know yesterday I was you know Eric was looking into llama 2 from weviate and we've we've saw how you can serve it with replicate and there's like brev and you know I've known about like modal banana like kind of like serverless where you wrap kind of info and just like hug just like so many options to run models like well it's kind of the state of that like model inference API Market yeah I think it's amazing right till till you're back or till two years back you had to deploy everything on your own so people would rush towards you know a stage maker kind of a setup where you're deploying your own models you're testing it out and there's so much stuff to be done and everybody was busy for six to eight months something would come out you'd I trade and go forward from there but then I think with these uh inference API endpoints that came out it the game completely changed now most the companies are very happy with I'm just going to deploy or let somebody else manage my deployment how they speed up their gpus how they effectively utilize the entire core of the machine is not my expertise I'm not going to spend time doing it but what you can do is use these apis for you know solving my business problems and I think that's where we also see I mean Mosaic posted this very interesting stat right or there was a database I think databricks posted this stat that said llms are probably their fastest growing and fastest adoption segment across anything that they've seen before and that's probably because earlier you were constrained by you know a lot of data science machine learning training testing that needed to happen before business Logics could be introduced and now you're seeing business logic introduced on day one and then maybe some testing training happening a little Point down the line when you're really looking at accuracy or latency or cost so I think we digest a little bit from your question but uh it certainly is that inference API endpoints have made it super easy for a lot more companies to utilize Transformers as the technology and then because of that nobody wants to deploy these foundational models anymore but you do want that you know next level of fine-tuning or data privacy or compliance metrics and in those cases uh there's a there's a variety of companies out there that just offer these services at a very very affordable price I mean we work with players like banana model before and it's just so easy to work with them and they'll say hey we already have these models pre-deployed you can fine tune with us go live and it just becomes so easy deploying a model so it's almost like the llm world to a large extent is getting commoditized really really fast wherein I don't need to manage all of these gpus myself so in fact you know I see almost three layers of companies evolving one is companies that are providing bare metal gpus so you deploy a model you train it and that's for that's not for the faint-hearted that's where you've got the ml expertise you've got your data scientists in place and you're like okay fine I'm gonna do this on my own that's great then you have these next level of info companies which are like we're going to deploy all of the open source models or you tell us what you want us to do or fine tune and we'll do it at ourselves so it's it's more a managed model service you know dlfx does this obviously to some extent with MPT and others but and then there's this third category which is the open AI anthropic cohere which is like Hey we're doing end to end you just need to worry about your API key and everything else is managed so I think almost three layers depending on how deep do you want to go or how mature you are as an AI company uh companies tend to choose different layers yeah I definitely want to like later in the podcast take your temperature on the um like you know run a closed model the end to end versus fine-tuned language models and just like kind of wear your sentiments out on that one but I kind of want to stay on like kind of Port key the product first of all that was a really great tour I love the three levels of you know bare metal up to like this kind of like um running open source models or running uh models where you fine-tuned it but hey you don't need to worry about now deploying it I'm gonna take care of that I I really think that's a super interesting emergence of this Market but okay so something I really took away from portkey that I find is fascinating is um you say I have um you know I use gbt4 I use anthropics command nightly and I use cloud and I'm getting rate limited by open AI so I so now I can pivot how do you think about like managing multiple llms in uh in your apps absolutely so almost think of this as this so I end up drawing a lot of parallels from the traditional devops world so the traditional devops world has the concept of API gateways and load balancers now we've sort of implemented the same things here so you know think of an AI Gateway or an llm Gateway and that is connected and load balancing across maybe multiple keys or multiple accounts of open AI or even between open Ai and anthropic so this could be load balancing this could be fallbacks this could be retries this could even be Canary testing and portkey makes all of this possible wherein we are in the end connected by all of these different providers closed doors open source banana everybody else and the user just needs to call one endpoint and Define a configuration saying how do I want this call to Traverse and quad key can essentially orchestrate the entire call and make sure that you get you know the fastest call with the best accuracy which is also cost efficient so that's something that we do on our side but you get to choose whichever model you want at any point in time and portkey can manage the rest so quickly let me ask about um so so the kind of model inferences that we're routing are are you mostly seeing you know open AI cohere or maybe like you know Azure Bedrock kind of like some of the more like major Google uh like the major uh Cloud products like what what kind of um ensembles of apis do you tend to see organized this way yeah I think so it's been surprising to me the most common actually the most common isn't surprising it's open Ai and Azure because people are trying to go over the rate limits and Azure gives them the extra rate limits uh sometimes it's faster than open AI inference points so multiple accounts of open AI or open Ai and Azure are the most commonly load balanced system that we see on 4K today a lot of companies are not trying out anthropic uh as a fallback because they're saying you don't want to get when they're locked into only open AI we don't know how their model evolves so there are companies that are falling back to anthropic and also building those capabilities and I think just because portkey makes it super easy to do this you don't have to set up anything at your end and we're just doing uh you know we're falling back to multiple providers as well the other thing that's interesting and it's just begun to happen and we discussed this about players like banana Etc is as companies become a little more mature and they're serving a lot more calls and they have the data to fine-tune a foundational model for a very limited use case then they're saying hey let's try a load balance between open Ai and my fine tune model and evaluate the results which is we'll do an 80 20. again if I draw from the traditional engineering board you have these blue green deployments I want to use the fine-tune model but I have no idea if it works well enough so let's do an 80 20 test um I'm going to send 80 of my calls to open AI 20 to my fine tune model used evaluations and human feedback and then compare the performance and if it starts to do well then just start increasing the load until I get to 100 there yeah so that's that's something that you're seeing and that's actually a very interesting space for companies as they mature yeah that or yeah I think that's you painted the picture perfectly how you think about deploying a new model and how you're going to integrate it with your software definitely you know yeah it's all very clear I guess like um yeah maybe we could just kind of step into this broad like you know fine-tuning llms you know I I maybe just to tell a quick story I've been looking heavily into the gorilla models so this is where you like fine tune a language model to use a particular set of apis and so in our case we're fine-tuning language models to use the weeviate API so what this could result in is like an auto API where you would come to wevia and you say hey I want to make a bm25 search but you don't know the graphql syntax yet so this language model would produce the graphql for you and and so so this has been my experience with you know fine-tuning language models and so yeah so this kind of like you know it's getting so easy there's like a replica tutorial on llama too this is like how to find a tuna llm obviously Mosaic ml being Acquired and so it's like yeah this kind of General thinking on fine-tuning llms can you just maybe if we could even step out of just like what you're seeing in the market just like your sentiment on oh well yeah it's like the city has sentiment on the evolution of how common that's becoming to do yeah I think again it's almost the maturity curve of organizations adopting llms so as you you'll always tend to start with something that's very safe you know this is you know very well defined and then you sort of say okay now I want to extract the next level of efficiency so I think there are probably two three factors that play one is obviously maturity so as you go more mature you want to fine tune you want better results you want to outsmart the competition you want to build your modes so that's where fine tuning really becomes useful the second is I think I've seen companies which are really particular about data and privacy they want to only work with their own hosted foundational models so it's interesting that Azure today offers you know co-located servers I don't know what exactly they call it but they say we can deploy or we'll VPC pure the opening apis to your API and that's how we work so I think that's where companies are becoming more and more interested in fine tuning foundational models uh because I mean llamato is really good in terms of uh actual business use cases and how you can get started with it especially for smaller use cases like gorilla is a beautiful example I'd love a world right where you don't need to probably invoke or remember the API endpoint make mistakes in the body of the post request and then fail rather than just sending a text query saying that hey this is what I want and if something fails it just replies back in text and I can again reply so I think that can be a really natural conversation the same way maybe Conor and rohith are talking on a podcast can servers talk to each other using natural language and that becomes like the new API standard so that'll be fantastic and these use cases which are very limited can be done much much faster and using a fine tune llm so I think that's where people are now trying to figure out what are these small use cases pick them out and create uh fine-tune llms on top of it I think it would be it would be unjust not to mention but almost everybody is right now looking forward to open air releasing fine tuning capabilities for 3.5 in gpd4 that could be a major step forward for a lot of people because these models are already really good and if I am able to fine tune them on my own data and get them you know get all of its latent capabilities to my data that would just be fantastic so I think fine tuning is going to become I mean I am definitely looking forward to more and more use cases on fine tuning both on foundational models as well as on the the players that already are ruling the market yeah I have a couple interesting things with fine tuning that I've been thinking about the first well the the first one I I don't even know if it's worth continuing on the conversation but it's this idea like earlier I think maybe uh seven months ago I had Jonathan Franco on the podcast from Mosaic ML and he was explaining this idea of like continued language modeling on your data so you know say I want to do again this gorilla model that writes we V8 apis If instead of just going right to uh instruction tuning with the did you write the API correctly I also continued language modeling like a gigantic data set of like we VA documentations like so like that kind of intermediate step and so I do think a lot about is that needed and so that's like one topic but the second topic that I think is a little more interesting especially for weaviate and particularly retrieval augmented generation this is another nugget I took out of the gorilla paper is that they're um they're going to be doing retrieval aware fine tuning so what this would look like as you know most of these like uh how to fine-tune an llm with replicate is like just kind of give us the data right so you would you would retrieve and then you would put that in the data and then it's still uh instruction to so you would put yeah put the retrieval with the input and it's I think this kind of retrieval aware tuning it because you know especially for us maybe if I could ask you this broad question of how you see retrieval augmented generation because that's especially with weaving and Vector databases that's like the you know the hot Gold Line totally yeah I think no ID uh is definitely the flavor Everybody is using it's also it's amazing when it works right so I remember there was this blog post by Guan back in like late 2020 where he was talking about context stuffing to make your LM outputs a lot better and context stuffing earlier was examples or context or everything that you can give the llm because then you're constraining um the output that it produces and that was something a lot of companies and you know early content generation companies used quite a bit and then once the RDG paper came out uh I mean not the paper but the RG implementations came out it was just amazing how people are getting to use this for a variety of use cases right and um then so you have base RNG then you have these interesting Chain of Thought experiments on top of rig which is I've asked you a question let's do a Chain of Thought analysis let's reason let's classify and get to the right answer accurately so I think that entire chain that started to evolve um is obviously very very interesting um Rog itself has multiple flavors um somebody yesterday but we're talking about how internal company use cases versus external companies cases the rack can be very very different for internal use cases you need a lot more permissioning access management Control Management Etc versus for external use cases so if you're doing uh you know like a customer service spot then maybe it's lesser restrictions or permissioning but a larger set to search on so how do you do that using say hybrid search in review so those are very interesting Concepts that people are now um looking for uh interestingly data leakage was another thing we've been discussing with the customer very actively so they're worried about your leakage because now you have all of this so earlier the data is very very segmented in their database but as we know these embeddings to a vector database how do the embedding stay as segregated as their data here and what are the you know compliance requirements for that so those are all the things that people are now getting into as they understand Rag and then they're taking these rag implementations to Legal security to their instances Etc yeah yeah I can I oh I'm not sure I see it with embeddings all the way yet but I can I remember with the language models how you'd be like um you prompt it with like uh the email these like from Bob Van light to and then it's start yeah that could be a problem with language models yeah yeah and I mean yeah that multi-tenancy kind of thing you know rv8120 introduce a major revamp for that and people listening you know Eddie and Delacruz CTO he explains that and yeah that's something that um you know that's something I wouldn't have known about just academically looking at the papers you know that's something I think you got to go out there and see a little bit to understand what a problem it is but yeah it's all really interesting I guess um so I get then kind of on this topic of fine-tuning and yeah there's def there's you know compliance and it's huge issues from you know just sensitivity within business I think also like you know it's kind of related to like can gbt really be your doctor or like that kind of thing and um but kind of pivoting a little bit I want to come back to um this kind of like Port key and this kind of um you know just like the cost Savings of running inference and so I maybe we've talked about a load balancer that kind of like um you know is uh saying mostly from the perspective of like this is rate limited do you also like you know yeah I think people are aware that if you start paying it too fast it says hey stop it and so you you know you go but you also think of it of maybe like first ask the question to the cheaper language model see the answer that comes back and then say okay and send that to the user versus like no let's give that to the more expensive one yep absolutely I think we're starting to see those implementations now when people are uh and that is you know so llm call to the cheaper model evaluation and then decide if you want to send it forward or not uh a simpler implementation of this is also make the cheaper API call uh because I know that it works 80 of the times the user has the capability to regenerate especially for very low hanging use cases so where it doesn't really matter and the users totally fine regenerating the second call goes to the GPT 4 API so it's like a mixture of these two but definitely the LM call evaluation second LM call is usually a very interesting plate if you're starting to see some companies do very much and this is almost a 10x price difference so it becomes so much easier and you know even in terms of January testing a lot of people will make calls to both apis initially figure out the similarity between these apis so that you have at least a minimal level of confidence that this model performs at least 80 percent of the times and then I can start to expose that a little bit more yeah because I think well I think there's already going to be so much value I imagine you know with just routing it with rate limiting and then routing it for accuracy but then kind of you know dreaming into the academic thing and like one thing is there's this idea it's kind of related to gorilla where you have like gbt4 is kind of like the Master model and it would route to like in the gorilla sense it would route to a tool and then you have a smaller llm that uses that formats the request to the tool particularly like alleviate the graphql API or there's this other idea where it's like um you turn like the hugging face model Hub into like um you could think of each model as an app I don't know if I love this but like you know you're the image segmentation model so I go get my segmentation mask then I send that to the classifier stuff like this so do you think about that kind of like orchestration of models as being maybe like an a direction Port key would go in yeah I think um not right now uh we're mostly focused on production use cases where we've seen companies already go to production with it I think this is clearly in the realm of so you first have agents and tools which are yet to go to production in a really large way um from our perspective uh and then you you're thinking about models as Tools in agents I think that is clearly an area of Interest right now a lot of academic Pursuits happening in there but I have not really seen anybody deploy this in fact we've seen the reverse wherein you have a very uh simplistic prompt router which which is written before the llm which decides which llm should I send this call to or should does this need additional context so I need to send it uh to my Vector database to get the context from there and then push in the llm so I think that smaller model is actually more efficient also because more business most business use cases I would not like the added latency of you know the entire chain running and then I getting an answer so you'd want to drop off at the right point so you definitely don't want to introduce additional latencies in play and I know we'll talk about caching a little bit later but I think semantic cash is one of those pieces right which is how can I return an answer faster so I think latency is something that businesses really really care about so for anything that's asynchronous and you don't care you're gonna get a reply later then yes we can do this you know interesting routing across multiple llm calls and then get our output but for most other cases you want it as fast as possible so yeah yeah uh yeah I love kind of like I've been studying a lot of llama index and these query routings kind of like where you have one language model that takes the query and says you know is this my favorite example is like is this a vector search query or is this a SQL symbolic query so that kind of routing you don't need like uh 300 billion parameter model to to do that yeah I think that's really it'll be really fascinating how this how the um llm Frameworks evolve with the hosting of different models needed for different kind of routing or tool selection it'd definitely be a whole a huge thing but but yeah you mentioned it and let's get into it our debate topic when we met in Berkeley was um you know semantic caching I I admit when you know when we first started talking about it I was kind of cold on the topic I didn't really like because I kind of think that like the the Nuance of prompting is it's so particular but you know you explain to me about question answering and and yeah and so I do agree that this kind of semantic caching of LM calls could be really interesting could you maybe uh set the stage on the topic absolutely so I think caching has been a thing that's been long uh drawn we've seen caching use cases across llms very well but when we implemented caching so this is almost something that we learned across the way right so when we implemented caching for our users we saw a cache trade of approximately two percent and I mean it's no surprise is there because everybody's probably asking the same questions but they're all asking it in a very different way and the whole promise of llms is that there's no annoying what clicks anymore so there's no deterministic inputs which means caching becomes that much harder because it's not the same query anymore right but if you still look at it for customer support use cases employee support use cases ID support knowledge search 60 to 70 percent of all of an employee query or a customer queries are going to be similar they're going to be somebody would have asked them in the last seven days even if I take a very short window so how do you go about really making sure that I can serve responses faster if I've already seen that question so for example somebody asks does vv8 have a free developer plan or somebody else I'm a developer now these are both the same questions and the answer is whatever it is but then how do I go and make sure how I could either do it through a complicated llm call or if I can scrape the intent of the question from The Prompt coming in store it as an embedding in a vector database and then match every incoming query to my list of already answered queries then I can essentially build a very effective semantic cache so I think that's where we started saying okay this seems like a straightforward enough problem let's build semantic caching and let's go to let's try it out so I think the results were I think they were just amazing wherein we saw without any optimizations 20 of all of our queries would give us a 90 95 96 accuracy so and 20 of these queries would be 20x faster um I think the challenges and this is what uh I think milvis launched with GPD cash uh and which also I think which is also why you were skeptical and I remember discussing it then then I think the base semantic cash the base semantic cash implementation is probably just a theoretical uh foray but it doesn't solve use cases because in the five percent that it fails it's gonna give you a an extremely absurd answer and it's going to give it extremely fast so you will feel like you know it's coming from cash it's absolutely wrong and you've probably leaked some data because your question and answer are completely different that was a question asked by somebody else the context was very different and now I have leaked data so and this is where I think we started our journey of just understanding semantic cash how it works I think that's where we ended up implementing uh you know dismatch patch libraries um we take out relevant parts of the prompt which need to be cached because a majority of the prompt is similar so I don't need to Cache it so how do we take that out um there's a lot of post-processing involved then to make sure there's a quick eval check to see you know is the question and the answer relevant to each other and then there's a constant evaluation model running to say that before I introduce semantic caching let's what is the right Vector confidence to set so that I get a 99 or 99.5 so it's this entire production chain that builds you a very robust semantic cache um and that's where I think we've spent a lot of time just tuning these small little things to make sure the entire piece really works but now yes we are seeing customers when they start they start on day one with 15 to 20 percent of their responses being served from cash and in some cases we've seen it go as high as 60 percent of their responses which is like phenomenal for them most from a cost savings perspective as well as in terms of user experience I think you still have to get we're constantly working to keep getting accuracy better and better uh but I really think we've gotten it to a stage where it's super useful for Enterprise search use cases customer support reg yeah it just makes sense there yeah as first I think there were a ton of nuggets on the great details and I really enjoyed it I like the as you ended with the kind of well yeah this thinking of like the how do you check the failures case can you maybe calibrate the vector distance of the similarity from the query to the questions you have cached I find that to be very interesting at wevia we've looked into Auto cut where we we look at we plot like you know Vector distances on the on the y-axis and then X is like result and then you try to check a a slope to say only give you like three out of the 100 search results to try to like but but yeah just for the top one I don't see a good solution to that I mean calibrating your vector distance scores for your particular you know that I think like I'm going off topic but like there is like conformal prediction like that that seems to be like when you know like causal inference like how they're like these buzz like these emerging categories of research I see that that coming up a lot so it does seem like you know uh the research and uncertainty is catching up but um yeah maybe even just take a step back like I remember like with weavier when we were brainstorming how to add a site search to eviate I thought one thing that would be really interesting is just to kind of you know create a frequently asked questions and then just the embedding of the query to one of the frequently asked questions would be a way to do it but then you kind of mentioned like this cold start of like 15 to 20 evolving to 60 because it's like you might not have a comprehensive FAQ yet so you can start using your you know language model QA system until you build up this base and now this cache thing is really critical and yet it's really fascinating I mean I can imagine also like um you know you mentioned like data leakage like if I if I'm coming to and I tell you about my particular schema as I'm asking you this question about my bug and then you end up like giving away some questions I can imagine maybe a language model that takes that and then says and then says like please rewrite this to up to you know make it more abstract and then that result is what's saved in the cache yeah I think it's a supercharger for question answering systems and I think that like you know chat with yeah like the whole question answering part of that is just one of the biggest applications but I guess kind of my question then is um this kind of like embeddings for question answering there's also like embeddings for classification is like how do you see like when a task is ripe for just embeddings and just Vector search alone is the way to do it compared to when you're like no you need a generative model for this kind of task yeah I think for anything to do with classifications um classification topic modeling anything that has a deterministic answer it's a deterministic question you need a deterministic answer or at least you need options which are deterministic I think in those cases yes an embedding is probably the best solution unless so maybe let's let's do this right so you have deterministic input deterministic output embeddings um subjective input deterministic output may be functions so use Json form or functions or embedding so it can be both so you can have an llm call which returns Json or you can have an embedding you'll have to see which one works better subjective input subjective output then you need an llm call with a cache on top if you see a lot of these similar questions coming in which is where I feel rag use cases q a use cases search use cases these three are all subjective input subjective output which is where you need an llm call with the vector DB in place that's what I think but what what has been your learning from seeing all of these systems at bv8 so far well I think um one uh you know email and Finn set up uh Kappa AI with leviated it's really cool I think they just got into Y combinator and it you know they're you know going to the moon and it's really cool and it makes me think about how um a lot of people criticized uh apps that that were described as like a rapper around a gbt API but if you're a Kappa and you're collecting uh you know customer question answer for you know like we via slag is in the Llama index Discord and like so like you build up this data and now your remote is that uh data set that you could then cache for cost savings and that's kind of the light bulb that's going off in my head is I'm thinking like you don't need to create a moat necessarily by fine tuning with gradients with I think that's like a lot of what AI people would think is that he like you you know train train it really nicely and yeah but like having a having a really curated data set for retrieval augmented generation it's yeah yeah I definitely let me pass it back to you on that kind of like the debate between you get better performance by fine-tuning the language model or just by really having great data for your retrieval augmented generation stack yeah I think I would love to do like a like a proper Benchmark on this I honestly haven't seen one I think my general guess would be if it's a very repeatable use case um uh then maybe fine tunes might work out better so you know it's the same character length output it's gonna contain almost the same information the parameters might be different then I feel so fine tuning is great for constraining outputs right so you can keep improving accuracy up till the point and you can keep constraining your outputs as much as possible whereas and I think that's where fine tuning is really shines so if you're doing product descriptions just keep constraining keep fine tuning keep getting a great data set and then your product descriptions are gonna look great but the minute you're going for variance I think that's when you do need embeddings and Vector search um at least that's been my practical learning so far that fine tuning and these can still coexist in fact I would love to see an amazing lag use case where you fine-tuned a model and then you're passing in the right context and this always knows that I've got to be helpful this is my tone this is how I reply I've seen these questions I've seen these things before Etc so I think but definitely giving it the context in the prompt is more valuable than relying on its memory or the llm's memory because there's just so much stuff inside it that's very hard for it to prioritize which one to pick at what time yeah I um yeah there's definitely I think um with me with my sentiment on fine tuning I think like so that gorilla experiment I mentioned earlier is retrieval aware fine tune where you retrieve the API documentation and then you know you get like the schema the API for like how to do bm25 search and we get and then it would write bm25 query and so I guess for me so you mentioned fine tuning is like how you steer it into that narrow pocket of the space so it's like this is the thing that you do and yeah like if if I fine-tuned it on so on like you know I have 60 apis and we via search apis and it I've trained on 40 and then it can't generalize to the 20 or or what happens is I train it on the 60 and then 20 new ones come out and it can't generalize to the new ones as well as the you know gbt4 can that would firstly be kind of that would hurt my sentiment on fine tuning pretty pretty massively but I guess the other thing is like um I've been experimenting where I summarized these podcasts with like one of those summarization chains where it's like you will receive the clips one by one as well as the summary so far and then you know you use the localizer and so because it doesn't really deeply understand you know like um just what we're talking about right now right it'll it'll it'll usually just kind of collapse uh topics into like comma lists so like with that use case I think heavily that if I fine-tune the language model doing that language modeling thing we talked about where you continue language modeling it on weeviate you know just like as much information as I can find but I actually so sorry I kind of want to Pivot the the topics a bit back into kind of you know being at The Cutting Edge of you know with Port key and you know this kind of like you know this layer between the LMS and overall understanding that this is a new layer in the software cycle gets us closer to cheaper llm inference I want to kind of talk about two things that cheaper llm inference would unlock and sorry if this is a context switch but the first is this tree of thoughts thing where you could um go several Pathways you know it's aside from kind of the caching I suppose maybe you could cash paths but like this as as inference gets cheaper we'll be able to do this kind of thing and what do you think could be the potential of that yeah absolutely I think so a lot of the things that um you needed deep decision trees to decide can now be left on reasoning by the llms um I think it's going to be a function of cost as well as latency so you need to be able to process this fast enough because if your router is going to take three seconds then your infant's gonna take another three you've necessarily doubled your latency which which nobody wants so I think cost is definitely without it getting cheaper you definitely cannot route but then if I could do you know two queries on add a level cost and inference time that would just be up that would be mind blowing because then I would imagine a lot of software developers will just stop writing workflow code at all I mean why do you need to create workflows and software which are essentially just decision trees and these decisions can be taken extremely fast extremely cheap extremely accurately by an llm itself you know so a very interesting example for this is use and throw software I think that I feel is going to get on the rise because of this wherein could I just spin up a quick replied instance and half charge GPD write me some code that says hey I just want to create 50 PDFs with this text in it these are appraisal letters for all of my employees create these PDFs upload them to my Google Drive Link that's attached here ask me any questions and then if the llm could navigate the entire piece and give me the output which basically it's doing routing plus inference plus a little bit more then I think it's it's becomes really really interesting because you don't need workflows anymore you can just rely on the lln building the workflow and I think this is slightly different from Agents because agents are doing multiple inferences Gathering data points from those inferences and then using it out versus just saying that create the entire decision tree framing which is the workflow and the llm is then in control but I'd love to see a time that it comes possible I don't think we're anywhere there yet because you need extremely highly extremely fast inference and extremely cheap inference to even make that happen yeah I agree completely with that perspective I and earlier we made podcast Colin Harman had described that you know like Auto gbt it's yeah it's like the search of a workflow and it also kind of reminds me of demonstrate search predict from a marketab and the lab at Stanford where they give like a few input output examples and you're kind of like compiling the the workflow that would produce those input outputs and so yeah I I think all that kind of thinking and yeah it's fast I mean as the the cost of inference gets cheaper so this kind of brings so then the next thing I wanted to kind of talk about with as the inference gets cheaper is uh we've had this we've been trying to like evangelize this generative feedback loops or the ideas that you know you do some kind of generation and then you save it back into your vector database and so you know a simple example could be I get my new blog post and then I take my crms and I write a personalized hey why you should care about this blog post to everyone in my CRM and you know I and then you know I that kind of thinking that I save that back in a database so maybe first you let me take your gauge your interest in that idea broadly like that kind of thing of how people use well because we've already kind of talked about it with some it's actually semantic caching is kind of exactly this idea because you're answering the questions and saving it back but um so the the the big headline thing that really excites me is like you know like vector search is about scaling Vector distance calculations like people kind of troll it on Twitter by saying like hey I just implemented a vector database because you've like numpy Brute Force like 100 000 vectors but I think really the exciting thing is once you're talking about searching through like 10 million 100 million billion so on how big of a number can I say right now but like but um this kind of I feel like this kind of generative feedback loop can be the Evangelist for like where everybody is thinking about having data of that scale to search through like you know I I pointed to my blog post and it like writes it Compares parts of the blog post I don't know it does useful things such that it's like it creates this latent space what do you kind of think of that idea yeah no I think it's interesting I haven't given it a lot of thought honestly until now but I'm almost now imagining people build data Lakes before this just to store a bunch of data so that I can query it and I can use it for training or I could do so many things with it I think this is almost like the vector data lake or a vector link you have so much embeddings thrown in it that now I can do various different computations calculations and training on it so that's actually a very interesting concept exactly semantic caching is exactly that right so it's storing the outputs back in for a specific use case but then I can definitely think of evaluations as another use case um maybe some validations maybe fine-tuning at some point in time so these could be interesting things what are other use cases you guys seen for generative feedback loops well that um yeah well firstly I agree that I think the semantic caching might have to be the new Evangel the new like headline thing for this but the you know that that thing I mentioned initially of um you know you go through you you have like your newsletter and you then have a CRM or something and you're writing like personalized that's a pretty interesting thing and then um there's another thing that's particular to search which is like if you have like you know say I take our podcast and I start indexing this this is like my favorite data set to kind of like dog food the technology with is like um if I just take the raw podcast clip and then I pass that to a language model and I say please summarize the content and then I index the summaries I actually I get way better search results with this with the index because you know like this like because like um if it's like totally ruin the embedding sort of not totally ruin it but like it's not as good as the transformed and so that's been probably my favorite thing in the search case is you transform it for a better index there's there's yeah there's also like you extract structured data from the chunks and the chunking like chunking text of the vector databases that's quite a deep topic but yeah totally yeah I think kind of the headline here though is like the you know Technologies like portkey and this whole emerging you know llm inference is getting cheaper there are more options we've got this llama 2 model like is the latest one as we're recording now you know we had MPT 30 billion come before that and like it continues to get cheaper and cheaper so that's why I think this like tree of thoughts and maybe like generative feedback loops I think those things are currently blocked by the cost and yeah yeah 100 I think it's just the cost of inference as well as I think the resistance of pre-produced multiple language models today I think I'm we're definitely going to see over the next few months people start to use maybe two three ten twenty different models for different use cases and then there's some orchestration between them just to make sure you're getting the best you know on the cap theorem so cost accuracy performance you have the best on the cap theorem and then you've got this tiny router sitting in the middle making sure that the calls are being distributed to the right um llms there and that I think will overall enable companies to collect more data store more data obviously I'm guessing the way llm inference is getting cheaper I'm sure Vector searches are also going to get cheaper and as that happens people will definitely want to store a lot more um I think just search a lot more of these data sets as well yeah no I definitely I think kind of like Bob gave me this term like speculative speculative design Theory where we're like kind of going out way into the future but yeah I think just right now what you mentioned on managing like yeah 15 to 20 model inferences whether that's like gbt cloud or or how you're going to introduce models that you've fine-tuned I think that's absolutely like a super massive emerging space of this whole like AI market and so on so yeah awesome thank you so much for joining the podcast I think this is an incredible coverage of these topics I had so much fun uh picking your brain about these things and you've definitely changed my mind on semantic caching I think that you know it's totally flipped I think it's very useful and awesome I'm so excited to be following along with portkey and see how all these things grow absolutely it was great chatting with you I think as I was talking with you a lot of new ideas also popped into my head so I'm going to write about them now but thanks so much for inviting me Conor was amazing doing this video ", "type": "Video", "name": "rohit_agarwal_on_portkey__weaviate_podcast_61", "path": "", "link": "https://www.youtube.com/watch?v=GnyajCD1Vrs", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}