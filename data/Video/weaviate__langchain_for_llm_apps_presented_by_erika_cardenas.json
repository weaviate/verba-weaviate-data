{"text": "We are super excited to publish this overview of how LangChain and Weaviate work together from Erika Cardenas! Erika explains ... \nhey everyone I'm super excited to share this presentation on using line chain and leviate to build awesome applications using large language models let's begin with what Lang chain is just in case you aren't familiar um so this quote is taken right from their documentation and it states that large language models are emerging as a transformative technology enabling developers to build applications that they previously could not but using these large language models in isolation is often not enough to create a truly powerful app the real power comes when you are able to combine them with other sources of computation or knowledge and in this presentation we'll really see how using leviate helps the large language model the presentation goes in this order so first we'll start off with the llm chains and what this does is it enables it enables us to combine multiple inferences together so this is a step-by-step task um so in this presentation I have an example of asking the AI chat bot to use the previous output as and can take that into consideration for its next output next I'll go over combined documents so what this is is it solves the problem of The Limited token length so I will share four techniques that are implemented in link chain and I have a pres a visualization for each next is tool use and this connects llams to tools that can help execute it so for example connecting it to webiate or a calculator and again I have a a quick demonstration on that and then for chat Vector DB um in the beginning we'll go through the documentation in line chain and then we'll end with a demo on using line chain and mediate so make sure you stick to the end as you can guess from the name sequential chains execute their links in a sequential order so it takes one input slash output then it uses the output for the next step so let's look at an example of sequential chains and a conversation between a bot and I before we get into it I want to credit the originator of this example their name is jaguely on GitHub and in the blog post I credit the author so you can link right to the GitHub repository all right let's get on to the example all right so here I'm asking the bot what type of mammal lays the biggest eggs and the response is the biggest eggs laid by any mammal belong to the elephant and as you can see in the Emoji that's a little confusing there's a little bit of hallucination going on so what I'm going to do is ask it to it's like a fact Checker so I'm gonna say Okay given this statement how can you improve your answer so let's see um so here's a statement and it's the blue text that's why it is colored differently so given the above output make a list of the assumptions you made when producing the above statement so here it gave four assumptions that it used to generate the statement above so now I'm going to ask okay so for the list of assertions determine whether it is true or false and you can see that in the curly brackets and in the color text this is what it's using all right so let's see what it says um the first one is true so the elephant is a mammal mammals lay eggs is false and it also gave the um it also fact checked itself a little bit with most memos give birth to live young the third one eggs come in different sizes it's true and then the fourth one is false elephants do not lay eggs so all right given that um how would you answer the question which is which mammal like the biggest eggs um in here all right and then we can see that it has uh this question cannot be answered because elephants do not lay eggs and most mammals give birth to live young and there you go it has it right and that's kind of the sequential chaining that I was referencing earlier of um it kind of fact checking itself all right let's go over combined documents so what this does is it overcomes the limitation of the input token length um so in line chain there are four techniques that are implemented and I will go into greater detail but at a high level I'll be going over stuffing map reduce refine and map rebrink I just want to make a note that there is not one that is better than another performance wise it is just based on your application so make the decision as you wish okay the first technique is stuffing and this is the simplest method where you stuff all of the related data into the prompt as context and then you pass it to the language model next we have mapreduce so what this does is it involves an initial prompt um so what is a golden doodle and it is used on each chunk of data so we're going to the vector database we're grabbing the relevant documents and now we're going to um we're going to add in our initial prompt all right so then a different prompt so here we have a golden doodles and mix um we have three different responses and now the next prompt is to collect the answers and combine it into one sentence goes into the large language model and boom we have a golden doodle is a mix of a golden retriever and poodle okay next is refine this one is interesting because it has a local memory that is used to refine the output um so an example of this is for the large language model to summarize the search results one by one and use the summer summary generated so far to refine its output um so as you can see in the animation the green and blue document went into the large linkage model outputted the orange document and then on the next round it used the orange document and the remaining search results to Output the answer of a golden doodle is a mix of golden retriever and poodle alright so mapri rank is a method that involves running an initial prompt on each chunk of data and from here it will assign a score based on the certainty of its answer so here we have three seven and five and now we're going to rank it and then we're going to use similar to the stuffing method we're going to put that into the large language model to Output the response next we'll be going over tool use so this is equipping language models with tools like weeviate a code execute or even a calculator I am asking it to write python code for the bubble sort algorithm and I'm asking it to generate code I'm passing it through the python repo which is a code executor that is implemented in langchain and it outputted the response and then from here I'm asking the language model is based on the output is this python code correct and then it says yes now we'll get into chat Vector DB so linktrain has many pre-built chains so we're going to go through an example of using chat Vector DB of it pointing to our Vector database and we'll take a deeper dive into how this is implemented in light chain okay so now we're in the code base for the chat Vector database we have the base dot pi and prompts.pi all right let's start off with prompts all right so the template is that given the following conversation and a follow-up question rephrased a follow-up question to be a standalone question all right so we have the chat history and the follow-up input as a question and the Standalone question is using the prompt template and then the prompt template is using the following pieces of context to answer the question at the end um if you don't know the answer just say that you don't know don't try to make up an answer and this is um helping with the hallucination problem all right so it takes in the context and it has the question and then the prompt template so this will only output four search results and you'll see this in the base dot Pi file and you'll see that right here so in this um in this file you have the two main components of the uh using the chat history and using the query to the vector database now on to the fun part of using Lane chain and leviate together um so on my screen you can see that this is where we are connecting to our revate instance um just localhost 8080 and then in Vector store this is where you're specifying which class you want Lang chain to see and then along with the class the property um so I'm using the podcast search demo that Connor has created on GitHub um I can add a link to it and make sure that you star the repository but within pod clip which is just the transcriptions of I think almost 20 podcasts it has content so that's the conversation that Conor is having with whoever he has on the podcast this is where we have the QA so the chat Vector DB chain from llm We're specifying the openai model and then we're storing the chat history so just using the stuffing method that we went over previously it's really cool that this is so simple and it's what almost like 20 lines of code um just to get this started so let's get into it with going into the terminal and asking the large language model what were the features released in 117 first let's make sure that we are running the demo all right so let's ask a question what features were released in aviate 117. all right so replication and hybrid search as well as rough to back were released in 117. though reptavec is specific to Evie and that is a cool thing with having it the large language model connected to a vector database is that it's able to grab the um context that is relevant to your application so um I guess if I ask like a general large language model what is rough to back it'd be like I have no clue what that is but when we add in the context of the webiate podcast it understands it let's ask a follow-up question of explaining hybrid search what is hybrid search and there you go it's also able to explain what hybrid search is and that it was also implemented in mediate 117. um so it's just a quick demo and a few queries of using this but you are able to run this on your own and I recommend cloning the repository of the podcast search to get started using this I hope you enjoyed and happy chatting with your data bye ", "type": "Video", "name": "weaviate__langchain_for_llm_apps_presented_by_erika_cardenas", "path": "", "link": "https://www.youtube.com/watch?v=7AGj4Td5Lgw", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}