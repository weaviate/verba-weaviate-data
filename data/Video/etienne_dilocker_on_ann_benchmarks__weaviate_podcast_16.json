{"text": "ANN Benchmarks are a tool for evaluating the performance of in-memory approximate nearest neighbor algorithms. Etienne ... \n[Music] hey everyone thank you so much for watching the we va podcast we have such an exciting podcast today eddie and dilocker is coming on the show again to talk about a n benchmarks approximate nearest neighbor benchmarks and how you can find the optimal parameters for wev8's implementation of h sw so to take a quick step back before diving into it at the high level idea with the we v8 vector search engine we're looking at how we can encode objects into vector representations and then once we have these vector representations you say take your search query similarly embed it into a vector space and then you want to find the most similar vector and one of the parts about this that makes it so interesting is once you talk about billions of vectors and like an absolutely enormous amount of vectors to do that similarity search with and that's where this thing about approximate nearest neighbor benchmarks and algorithms like h and sw come into the picture because they let you do this in a reasonable amount of time find the most similar vector in a absolutely enormous collection of vectors if you're talking about you know like all the tweets on twitter or just like i'm sure that people out there can imagine absolutely enormous amounts of objects that you'd want to search through so eddie and thank you so much for uh coming back on the vva podcast and can you uh tell us about the a n benchmarks thanks for for having me it's been quite a while i think that i've been here uh so very happy to to be back uh yeah the benchmarks i think is probably the most requested well feature is not a feature i guess in vb8 but one of the most requested topics overall from from a multitude of directions so new users might be worried if i'm using vv8 can it handle the load that i would receive in in production uh users evaluating different vector search engines might be wondering okay this is how how quick or fast is it um and and all that we had before was basically this like very vague claim that you can do a a vector surge in in considerably less than 100 milliseconds and that's sort of if if we look at the benchmarks that we just released like i think maybe we were a bit too modest with that claim because i think that's that's typically the the search latencies are much better um but really benchmarking like it's such a complex topic and i think you can't put it in like a single statement of saying like yes you need to put some number somewhere on the website like you need to get people the impression okay uh speed and throughput is a concern for us and i think that was the the initial idea of just having something like that in our in our github uh readme um but yes the benchmark show right now it's it's such an interesting topic with different latencies and and how like latency and throughput the connection between those two um then of course data set sizes uh vector dimensions like all of these things have an influence on on um what you can see and yeah i think it's it's it's super exciting because vv8 is is fast i think we're we're sort of definitely in a very good state with our performance and why not be super super vocal about it why not show what we can do and not just make these like vague claims like yes it's fast but we're not going to tell you how fast okay so can we maybe step through the complexity of what makes each different data set so you know say you have uh a billion paragraphs from wikipedia that have been encoded compared to i don't know a billion images or like what what are the different things with respect to each of the data sets when you're trying to benchmark an approximate nearest neighbor search yeah uh yeah so if we if we look at um the current benchmarks what we see is sort of an and what i would have assumed before is size is the different the the most important factor like i would have assumed sort of without any context um maybe that let's say if you have something that's a hundred thousand objects and you have a million object you would think that the million object ones is considerably slower and if you brute force this which has a basically uh o of n complexity like linear complexity that would absolutely be true like a proof for a search through a million objects would be roughly ten times as slow as uh one through a hundred thousand objects uh but of course we're using a n uh uh indices like in our case h and sw and we've talked um at least twice i think one in once initially and once also with uh yuri on the show we've talked about hnsw and the whole point of like this this complex graph structure that hnsw is the whole point of it is really that we don't have that linear um kind of kind of a latency increase or or um yeah throughput decrease however you want to see it and um it should have been really obvious saying that now it should have been really obvious that data set size isn't the biggest factor but it's still it's nice to see it and it's it's in a sense um yeah it shouldn't be surprising but it still kind of is and it's nice to to see that so um that is i think the a very interesting learning uh also when applying this to different data sets um when users in the community tell us something about their data set i think the first thing that they would mention is how many objects it has and it's it's actually for performance it might not be that relevant it might be relevant for like resource planning and for for cost estimates and these kind of things but for performance what we see um you can generally get the highest throughput um two things have the the biggest impact one is the vector dimensionality and i think this also relates a bit to your question about let's say these are paragraphs from wikipedia or maybe these are images depending on the model used um different models use different dimensionality and traditionally i'm not sure if this is true anymore but traditionally we would see that like text based models have rather few dimensions so maybe like 384 on on something like uh center transformers um and image based ones would have very high dimensional analytics so 1024 dimensions for example and this would actually be the biggest impact uh um that you could have in on performance and if we were diving a bit into how hmsw works and and what's happening is the approximate uh part basically tells you uh the index does something smart so to speak that you don't have to evaluate the entire index it's a neighborhood graph so by using this a n model that that agents w has it basically tells you here is a smart way to only evaluate let's say five percent of the index because these five percent are close to uh to your search query um and what uh sorry if we if we do this we we can kind of see why it doesn't matter so much if we're if we're evaluating out of a hundred thousand objects or out of a million objects because this five percent of course is made up number like it could be that in in a million objects suddenly we're only evaluating maybe 0.5 however the dimensionality of the vector is something like if you've narrowed this down to making this up let's say a hundred thousand vector comparisons then you really see that the cost of doing a hundred thousand vector comparisons with a thousand and twenty four dimensions is higher than doing it with 384 um that said i'm not sure if this this sort of text is low dimensional and an image is high dimensional that's still true because for example we're seeing clip and then clip um it's the same vector space so so we're it seems to be very possible to to compress that uh image information in a in a relatively low dimensional vector space um but i think it is something to to think about just in general like if users or if users are using out of the box models then it's something to think about like do i really need the one that has such high dimensionality could i maybe achieve the same thing with with a lower dimensionality and same thing for if users are training their own models just think about think about the dimensionality like do you really need um maybe even 384 which seems to be like the sweet spot right now for text like how much performance drop would you have if you cut that in half because the the the sort of benefits that you would get from an engineering perspective is half the memory potentially half it's not entirely linear but potentially half the the latency like double the throughput of course it's not not yeah not linear in that aspect but still you're going to get better throughput and better uh latency so uh yeah that's maybe maybe a long answer to your question about different data sets like really think about the dimensionality of your data set and if you actually need it or if you can maybe get a free speed up by by reducing it yeah quickly the the idea of like how much can you compress into the vectors it kind of reminds me of arguments around like you know is intelligence compression like is being able to find the minimum description length of a program is that the ultimate way of uh having intelligence and and you know that 384 dimension thing is so interesting and uh so quickly i i did later in the podcast i want to go a little more into binary passage retrieval and i remember our broadcast with yuri malkov and hearing about product quantization and you know starting to get a taste of that and i maybe i want to quickly bring up again locality sensitive hashing and kind of how with the problems with that i guess it's not because it's not learnable is what i had taken away from you or you know we could uh rehash that out a little bit on the podcast a little more and then that maybe knowledge distillation to try to compress that but but kind of what i wanted to step back into is so i'm i'm a wee v8 user and and i'm coming to these benchmarks so is it the dimensionality of the vectors again the so it sounds like the approximate nearest neighbor data structure structures like you know hnsw and i really enjoyed uh eric bernardson has a blog post series on explaining how annoy works and i highly recommend that to listeners who are you know not who are still getting caught up as a way of understanding how the data structures divide the space but so for a new wev8 user coming to the new benchmarks webpage is that is the dimensionality of the vectors is that the key thing that they should look at to compare the benchmarks with their data set or is there more to it than that uh yes yes and no so um i think it's one of the learnings that i did um but i think that the great thing about this benchmark page is it's it tends to be it's basically just information and you can interpret in in whatever way you want and that's also why we use different data sets in the benchmark so that you could pick the one that represents your use case uh or closest and that that gives you i mean that can help you in different stages basically it could be uh prior to using vva to know what you could expect from vv8 but it could even be helpful in like a debugging scenario let's say um you're using vv8 and you're not getting the kind of performance that you would expect by picking the the data set that's closest to yours from the benchmark list you could say okay like they were using this specific machine they were using uh this many objects with this many vector dimensions and they were getting i don't know making this up 5 000 queries per second throughput i'm only getting 500 then you have a very good indication that somewhere in in that setup something is going wrong and then of course the super interesting question is like where is it going wrong but i think uh debugging something is much easier if you know that you can expect more from it than if you're if you're not sure like if you're saying like this kind of like of the gut feeling it's it's not as fast as it could be but what is reasonable and i think that is one of the the questions that we also really wanted to answer with those benchmarks um given these things so so two two questions basically given these parameters this is what you can expect and then the second question is by comparing the different data sets to one another changing these kind of parameters what what would change and and there yeah as i said before one of the biggest learnings for me was the that the vector dimensionality is a much more impactful than the data set size would it be useful to maybe look at like a histogram of distances between maybe you sample five queries and then you plot the histogram where say it's like a gaussian looking thing of you know what the average distances are to the data set would that kind of thing impact the hsw parameters uh yes you are uh raising something super interesting um because i i think i'm not sure if you're if you're referring to that but it matches perfectly that uh twitter threat that we saw from from niels reimers about a sort of distribution of vector of vector points in the space and he was doing that in in the context of using random vectors and if you use random vectors basically by definition they'll be evenly distributed there's like every there's there's no reason that there should be clusters or there's no reason that um yeah objects are relatively far from one another so you see that that distribution curve um where basically everything is close and the problem in a neighborhood graph is we i think we initially talked about this when we we talked about agents w like it it's based on that fact um that if you let's say you want to want to make a connection from uh someone in florida usa to frankfurt germany um then in the best case you might just only have to know one person but that person knows someone there so and this is i think the the the more common example is um you're just five or six hops or any person on earth is five or six connections away from from knowing barack obama or any other uh famous person in the world um and that only holds true if some of those distances are actually large like if if let's say i know my neighbor and my neighbor i mean physical neighbor and my neighbor knows someone else in the same city and they know someone else in the same city then i can do 10 hops and i'm not anywhere closer from frankfurt to to florida so it kind of depends on that that distribution of objects of having sort of long enough edges so to speak because it's a graph and that is the problem that if you do um if you use random vectors all distances are going to be equal so how do you sort of move in the right direction where it's closest to you but still far enough that you're actually jumping uh enough distance and there the the super interesting learning is that if you use an actual data set you kind of get that naturally like you're you're not engineering your data set to be sort of i don't know to have clusters and have large distances between clusters it's just something that happens like whatever data set you you take um if it's if it's images there's bound to be some clustering if it's text there's bound to be some some clustering um so simply by using like in a sense it's simpler than random because you feel like maybe i need to engineer this in a specific way where the distances are are under control but ironically just not doing that and just using actual data that somehow has a has a meaning on it gives you that perfect distribution that that works so well with these a n indices by the way also other ones like not not just agents w i think is a is a famous example one but if we think of um for example ivf so so the inverted file one um same idea basically you you start partitioning your data set into clusters and then ideally if your search query can be answered by a single cluster you can skip the others but exactly the same problem here like if you think of of a two-dimensional space where points are completely evenly distributed so you have like almost like a checkerboard pattern or something where the points are basically where the lines intersect how do you cluster that like is it viable to put that into one cluster that matches everything well yes but then you don't have the benefits of cluster if you do four clusters where do you draw the lines like every combination is basically equally as good whereas um if these natural like if you have this natural clustering uh that's where these these algorithms um really benefit from and yeah that was interesting to also to also see that like you can um if you run um with with random vectors as opposed to like keep all the the the data uh the same as on an actual data set and basically try to copy that with random vectors you can actually get the same kind of speed and throughput but at a very very bad recall and um then just using the actual because and and that's specific to agents w because h w at some point it just stops the search when it thinks it's it's covered it well enough and uh the fact is basically that well enough is just way way worse so i think something uh might be making this up but on on like one million objects of 120 a.d i think the difference that we could see something like 20 recall with the same parameters where you would get 97 recall within with an actual data set interesting so so to recover that would instead of random vectors where i guess like the random prior is that it's going to be normally distributed evenly spaced out if you had um you know algorithmically generated vectors that but the randomness came from like a gaussian prior or bimodal gaussian prior i'm now going to reveal that i don't remember too many probability distributions but i think like poisson there's like other distributions right like would that kind of way of algorithmically generating vectors recover the properties of real world data sets i i think yes like i i don't know why someone would go through that effort but i think like assuming that there is a way that you could randomly generate data that maybe even through training something that is trained to reproduce random data that sort of matches the distribution of something that exists i i think you could get there um but in the context of benchmarking i think it's much easier to just use a standard data set basically yeah and i'd really like to talk a little more about uh clustering i think so clustering is the key thing here right like the the just the representations of these vectors are clustered and so the algorithms exploit cluster structure in order to facilitate faster search and clustering is also kind of i'd read an interesting paper that something like learning to classify without labels where you basically assign semantic categories to clusters and you use that to classify uh d do you see that as um and i know also wev8 has a zero shot classification built built into it as well how do you see that like idea of um you know like maybe to provide a little more context is i'm in i'm looking into this idea of doing the twitter analytics with weaviate and so what i'm trying to do is combine like the usual descriptive statistics where you look at uh basically segmenting where you kind of try to like look at the view count and the engagement rate and you try to look at the the way that these two symbolic properties are distributed and now i'm trying to add this semantic component or you can cement you can segment by the semantic clusters to to do that kind of segmenting that data scientists kind of do so maybe this question i hope it isn't too broad but how do you think about that idea of clustering to form semantic categories and how that might differ from kind of the school of thought around symbolically labeling things and then fitting classifiers to them to form categories yeah yeah yeah i think it sounds like you're pitching a new custom module for vb8 so that's that's real good yes and i think um the only step really that we need because if we naturally have those clusters and we can discover those clusters those clusters basically have a a centroid point or a centroid vector that represents the cluster so really the only thing that we need to do is jump from that centroid to something that can sort of classify it in a way so if we think of the the contextionary module that we have which is very primitive kind of it's not a deep learning model or maybe there's deep learning involved in the training but in the way that we apply it it's essentially just a lookup list for words and and vectors and by having such a simple mechanism you can easily do it the other way around if you have like this might might be a viable option if you already have some data just calculate the centroid vector and just ask the contextionary assuming it's in the same vector space of course as the conjecture be like what are the nearest words that you know off in that in that space and then if we and that could be something like the label or or something such more complex and then if we think of deep learning models um and and you probably know this way better than i do but i could imagine that that's not too different from generator models right like if a if i don't know you have something like a text summarizer i i could imagine that you could tweak that in a way where you just give it an input vector and tell it to to i don't know create a label or something so basically text to vec in in the opposite way like back to text that should be doable right and yeah i've been i mean i'm pretty obsessed with this idea of prompting and how the t5 texas text transfer transformer shows that you can kind of unify all the tasks into language modeling and uh so yeah that idea of uh classifying it like zero shot by having the kind of prompting interface and so i've been really curious about uh recommendation and that kind of thing and maybe do you think that recommendation could be also put into that language modeling framework where the prompt is um you know say i'm recommending movies and so i would say uh the the user is eddie and dilocker he likes dogs and and wine say and you use that as the input representation like the prompt and then and then it like well so and then it the prompt then it goes like so what movies do you think eddie and like eddie and liked iron man like the history of the how they do the recommendation and then you just turn it into a language modeling problem where you generate the kind of next movie do you think that kind of thing would work or it's probably too grandiose of a way of formatting the task now that i i i think that could absolutely work because what i know or i sort of have to build this base on what i know is that um we see users using vector search as the first step of the recommendation pipeline where they use vector search basically just to generate candidates and then have some other logic to to um sort of base like let's say staying with the movie example um we're taking the last movies that i've watched and then we're now looking for for something that's in some way related and related i think doesn't necessarily have to mean like because this was a comedy this has to be a comedy but it could be could be anything and and then sort of to to have yeah these other inputs in there as well i think i could absolutely see that sort of as yeah something that you would either do through through re-ranking which is basically what our users are already doing by applying their ml logic on on something that's that's originally a vector search or maybe even by by building this into the model itself basically that yeah the similarity search wouldn't necessarily return something that's similar but would return something that yeah maybe is similar in the sense of the user likes this therefore they also like this so where the the vectors page with space would maybe not so much represent um the the in the case of movies like the movies but maybe the users where you then search for similar users or make these these these kind of jumps in that space yeah one paper i really liked is uh it's a doctor query and what they do is they run a a model that predicts a question given a document so it takes the document as input generates a question and then what it will put into the vector database is that pair of generated question and then the document so it's kind of like a way of adapting it to question answering because then you kind of have that like lexical similarity sort of with the style of asking a question so yeah i'm curious about that idea of adding prompts to queries maybe to to have it go into the space and and then maybe the particular application recommendation but maybe to step out a little bit i'm i'm very curious about re-ranking it looks like there's quite a few ways to think about re-ranking and i'd like to start with uh you know i know you implemented binary passage retrieval and really got into the details of that i'd like to talk about that idea of maybe a coarse grain stage one retrieval and then a more fine-grained stage two retrieval and whether just with binary passage retrieval as the coarse grain step and then maybe just re retaining the floating point values of say the top 5 000 retrieved uh candidates and then having it maybe like a brute force search within that second stage or some other thing how do you think about these like multi-stage refining pipelines maybe yeah yeah i think there's almost no way around them because if you so so bpr i think is or the motivation behind bpr is basically uh performance and or sort of memory costs by by uh originally searching through these compressed vectors um the goal is then to to yeah sort of have a smaller vector space and therefore smaller resource requirements um and then the re-ranking is only to get back to the original quality um but i think re-ranking has much more potential than just sort of uh yeah sort of improving upon the loss of of compression by producing something that's actually better and like in the simplest sense could be for example re-ranking uh based on keyword matching so so let's say you have a uh event so the typical problem here of these pre-trained for example sentence transformer models is they are great on general purpose data but the closer that you move to um domain-specific data that they haven't been trained on the worse they become one way of course is to fine-tune them but that's maybe not always an option so for example if someone who's not from the ml space just wants an out-of-the-box solution that works okayish then fine-tuning may not be an option because they've maybe never trained the model before and don't even want to want to get into that and then let's say we have a case where we have a model that's generally quite okay so let's say e-commerce you search for for a brand name and like winner apparel or something and then it might be really good to to make that connection from like winter apparel to let's say a jacket um but it's it's very bad at matching that to the brand name that was actually in there so let's say you have nike adidas whatever in the search query and then the model almost ignores that uh so if the original candidate selection was good enough then you could even re-rank this with something super primitive such as bm25 which is basically just keyword matching uh the the problem here is of course if those matches weren't in there like if you you can only ever re-rank basically what was already matched so i think you need to be in a situation where the matches were good enough but you maybe don't necessarily agree with the order so like the i don't know top 200 ones are good but you can really only recommend one out of those top 200 and you don't agree with with the order within those top 200 and i think that's where re-ranking really shines yeah that's a really interesting flow of um you have say the the wikipedia sentence trained sentence transformer that can do like pretty you know semantics similarity cover a pretty massive distribution of text and then once you get like the top 5000 now you have the bm25 lexical style re-ranking which what are the you know like because you want the exact term and we talk about things like serendipitous discovery with respect to how people really interpret what's returned from a vector search engine and that kind of fuzzy distance and that makes a lot of sense to me and i'm also thinking about this idea of say pairwise classifiers and personalization so we have um you know eddie and connor and we're recommending movies would we have you know we could re-rank this list by having some kind of label data set where we have pairs of movies eddie and chooses movie a then you know we show eddie in movie c eddie and chooses movie e and then you use the pairwise thing to do the re-ranking what do you think about that kind of approach to it yeah just i i really i think the the the great way of reranking is that you can re-rank on on anything really like if you have the initial list um anything basically that you can train something on i think that's that's you you can re-rank based on it and maybe even to go outside of the the space of um of machine learning itself it could also just be some simple business logic let's say to sort of stick with the e-commerce example again it might be that the shop is not necessarily interested in uh presenting the best match but maybe in what they have the highest margin on so it could be like that you put in these these completely different where we tell the the the um out of the box uh yeah trained on wikipedia data like how is that model supposed to know that if they sell let's say nike over adidas that they get a higher margin um but through re-ranking i think you can really sort of yeah put potentially in and whether this is this could be something as simple as if brand equals nike then plus two or something or it could be like this complex model that's basically learned that um yeah if we sell nike shoes in summer then um our bottom line sort of improves and um that deviates a bit from your original question about pairwise ranking um but i think it's sort of what i want to convey is basically that that anything goes yeah that's one of my favorite things about the vector search engine and we've eaten studying weave it is the way that it can plug in symbolic algorithms with the neural retrieval you have like symbolic ordering of how you then process the neural algorithm and i love the playing of those two algorithms and i think that's a really nice transition to what i wanted to talk to you about next and you know i recently had the opportunity it was such a great time to attend the knowledge graph conference in new york with laura and see laura's presentation on question answering watch the room reacting to and it was really just such an incredible experience to see and i really wanted to get your perspective on semantic web ontologies and understand how you think about these kind of things so so for for me i think this this ties very well into into the previous question because um like i i see myself and my role a bit more in enabling the platform to then build these kind of use cases on top so so i i don't think i'm an expert on knowledge graphs um but i might well be an expert on uh sort of providing the platform that you need to search through through knowledge graphs and um yeah i i i almost like that i i don't have a good answer on on the knowledge like i you you were part of the knowledge craft conference you can tell me probably a lot about what was going on there and i i wouldn't even know so i i really like this that that it's in a sense vv8 is so so general purpose so to speak like it's good at specific things but still it's so so general purpose that there's such a wide variety of like yeah from e-commerce to to knowledge graph to to move your recommendation it's like almost feels like the base of anything that you want to do in in that space in the future yeah the the class property design of wva where you can have these cross references between different classes i'm starting to see how kind of like i perceive as elegant this abstraction is that i've seen like in our we've eight slack uh someone had asked you know how do i encode multiple languages and bob had responded you could have a cross cross-reference that links between the two languages as different classes so i see that as a way to do say like multimodal image text where you can have a cross-reference to the other image as well as this kind of ontology relation style where you say uh connor is an employee of semi technologies uh is uh uh is a business category vector sergeant you know like these this kind of way of relation style i think it's such an interesting thing and then within each thing you have these uh h sw index structures within each class so kind of like a quick clarifying question so each class gets gets a approximate nearest neighbor data structure correct correct so that the class is essentially like a namespace or in in my sql world i think it's called databases so where you have the whole database and you have individual databases and um yeah in kubernetes you would call it a namespace um so yeah it's really the the unit of isolation basically um so so what so in one we've eight instance i could have say three h and sw indexes for the three very cool yeah i think that's such an interesting part of it and doing that kind of linking uh is really just one of the topics that is interesting to me the most right now definitely and um so so maybe transitioning from the uh knowledge graph thing and it's a bit jumping to a different topic but so i wanted to ask about we va custom modules and if you could tell me a bit more about what it takes to extend one of the modules with uh with a with it it seems like to me if you want to say point to uh another say hugging face model path you you look at this like python inference container and i'm sorry this question is kind of going around the place for our listeners but maybe could you start with the understanding of wev8 core and then like the python inference containers yes yes absolutely so vb8 core in itself is a vector search engine basically that means vva does not understand anything but vectors and the the objects that are attached to that vector so often called metadata i don't think we use that term but it's it's very commonly used in the space and and of course the whole object has a lot of complexity around it as well with different data types but essentially the the idea is that if you use vv8 without any modules vva doesn't actually understand how to go from let's say text or image or any of the those to to that so if you use for example the the near text in the api uh you need to provide a module that basically does that translation and um yeah one of the ways uh to use sort of a custom module i think there are several levels sort of off of abstraction or not not maybe of abstraction but of how deep in the stack do you want to go to create such a a custom module and as you already mentioned starting with something existing can be super helpful so for example starting with the we have the text effect transformers module and the point of that is basically this textvac uh translation so if you use this um all your objects that you import will be turned into or will be vectorized and the the vector will be stored in an index in the hmsw index um and the same is true at query time so essentially you can use vv8 like a traditional search engine that supports text now and of course if you add an image module it would be same for for image so what that does um this module is basically it extends vvate a tiny bit um in a sense that it tells vba okay now you will have a near text search operator for example because that's that's not part of like it's not that it's there in vv8 core but you can't use it and now you need a module it's just it's just not there if there's no module so this is sort of and and this is i think the the extreme strength of that module system that any new module could extend vba in any way that we can't even imagine yet and and um basically yeah you have this small piece of code in vv8 itself i guess or closely running in v8 core like it would from from an engineering perspective would run as part of the same like it would be a plug-in that runs as part of the same process that vba is but then what that small piece of code can do is it can just send requests to something else so here this you you mentioned the the pytorch inference container here the idea is that a vector search engine has very very different requirements from a from a resource and a resource and infrastructure perspective than let's say model inference so the idea is that you split that out so basically a micro service pattern we split that out and vva that that does vector search comparison which is most likely cpu bound um makes that network request to something else which could for example run a model that's running uh on a gpu like these deep learning models they tend to run very well on gpu or there are also efforts to optimize them for different cpus but typically they run well on on gpus so what you can do in that journey basically is i think three different parts to to come up with a custom module the first one would be you take that inference container that existing inference container that's built for uh for transformer models and you simply run it with a different model so you could let's say download a model from from hugging phase or you could have a locally compiled model and as long as and there there are instructions how to do that uh in our documentation as long as you get that new model inside of that container you can reuse everything there so in a sense you could call this a custom module already because it's something that wasn't there out of the box you have now your i don't know fine-tuned model there the second step is you could say the use case that i want to do is the same i still need text to vec but my model architecture is so different that i can't use the pie torch container anymore so for example uh something completely new comes up some some new way of of running a model or it could be um maybe not something new it could be like like onyx or these uh sort of optimized compiled models that simply have a different architecture what you can do in this case is say i want to keep the part in vva that controls the api because my api is still going to be from text to vec and i'm still using this near text um yeah near text search parameter basically but how that's done like the implementation is completely replaced and that would be sort of at the at the micro service level where we're now saying we're simply replacing one microservice with another and the only thing that needs to match is the api as long as this api contract between bb8 and um and uh the inference container is upheld you can entirely replace it you can do whatever you want and i think this is more commonly something that people would start calling a custom module but then if there's a third option so the third option is what if we want to for example support a completely different media type so let me think of one that we currently don't support so let's say audio to that you want to start with audio too like that means you can't just replace the inference container that does text to vec because of course the the interface is yeah it takes text and you now need a completely new api you need to to get um answer if that that audio part in it i don't know let's say for the sake of argument you would just upload an mp3 file or something like that um and for that you actually need to change the entire chain of the module so basically you need to change that part that extends the vba api so basically the plug-in that runs in the same process and then most likely you would still run with some sort of an inference container but that's that's actually not part of the module system that's that's basically just a convention that the existing modules use so you could um if it's not needed you could run it inside the plugin directly and never have that split out microservice or you could say probably a bad example for for audio to event but for the sake of argument let's say you need three steps and you need three different models let's say one i don't know one cleans the audio the next one uh sort of cuts it into chunks and then the third service actually vectorizes so made up pipeline but if that was your pipeline you could spin up three containers for example and that i think is really the strength of the module system that in the end it's just a tiny piece of of of plug-in code that extends the vv8 api and then can do whatever it wants with that api and that gives you these three levels basically of of hooking into it and integrating uh something custom into it yeah that's super i i think the extensibility of that is super interesting and i i do think there's a bit to impact and i i'm really impressed with the uh google summer of code initiative in the we vva projects that i'm seeing and the titles of them sound kind of like this thing like the idea of uh you know maybe just adding a new hug and face module and understanding how to do it or how to add audio to vec but if we could if you maybe take me through you know i want to add let's say i want to add a new tokenizer and i want to add a new hugging face module would i need to pull request text event transformers add in my two things and then uh and then push it back to the main uh github thing uh yeah yeah essentially if you want it so so the the uh sort of pushing it back point would mean um that you would release it for everyone um that's one option another option would also be that you have something private that you don't want to release back to the community so in either way i think you would uh so so for the for the first one where you want to release it you would definitely have to fork it issue a pull request and with the pull request is merged then it could be available for for everyone um if you do it privately uh there is sort of a smoother process for this where you um do this at the docker level where basically when you do a docker build i think there's one environment variable where you can put in the hugging phase uh a url string so um the the uh hugging phase bundles up the model yeah the models with their their tokenizer and everything like under under one string right that looks a bit like a like a github repo and yeah organization and repo and if you i think we have these uh docker files where you just put in exactly that string and if it exists and if it's accessible then it will download it and sort of build that in your container um so yeah if you if you don't plan on unpublishing it if you only plan on using it you can even take that stuff but if you want to publish it then the pull request would be the right yeah that's so interesting for me uh trying to get deeper into weaving it and trying to really understand it and for me i'm a little scared of things like uh docker files and go and i've been trying to just like you know roll up my sleeves and get into the details of it but it does feel pretty accessible the docker containers that i think there's a bit of a learning curve for it if you're totally nothing but i do think that it's pretty accessible and so great i think that was a really great uh recap of a bunch of different topics that i wanted to ask you about and maybe any closing thoughts on the approximate nearest neighbor benchmarks and changes to the website and helping people understand uh how to configure their h sw through the use of these benchmarks yeah i would say first of all check the benchmarks that's that's the most interesting thing like look at them be convinced that that bb-8 can match your performance goals and then share the benchmarks that's of course also super interesting for us to to spread the words um yeah and and uh everything in the benchmark oh yeah that is something that i actually haven't mentioned yet everything in there is open source um our intention is that that you can reproduce like we're very transparent about every single step that we do in there we we ran those uh on a single machine we ran bb-8 on a single machine in uh gcp that was i think a c2 standard 30 or something it's it's in the article um and we ran uh the benchmark script on a separate machine which was just sort of best practice for for benchmarking like you don't want your benchmark script and your server competing for the same resource and we split that out into onto two different machines um and uh the the repository to to import everything to run all the scripts uh to to compile everything down to uh the the graphs that we're showing in the article all of it is open source so if you think we made a mistake or if uh you think we we yeah in either direction basically um if you think we're we're you know doing something that that shows numbers that are too good or we're not optimizing something um it's open source everyone can contribute and i think that that would be my my key message there yeah super cool it seems so exciting developing all the benchmarks and having all the research and the science the reproducibility that comes out of having open source reproducible benchmarks so thank you so much eddie and i really enjoyed this vba podcast i'm learning so much about we've got i really love this topic and things we discussed like the class property referencing i think all this is just so interesting awesome thank you thanks for having me time went by very quickly it was a nice one thank you [Music] ", "type": "Video", "name": "etienne_dilocker_on_ann_benchmarks__weaviate_podcast_16", "path": "", "link": "https://www.youtube.com/watch?v=kG3ji89AFyQ", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}