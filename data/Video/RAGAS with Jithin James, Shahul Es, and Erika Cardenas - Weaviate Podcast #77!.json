{"text": "Hey everyone, thank you so much for watching the 77th Weaviate Podcast on RAGAS, featuring Jithin James, Shahul ES, and ... \nhey everyone thank you so much for watching another episode of the weeva podcast we have a super exciting one to dive into there's been so much exciting research around retrieval augmented generation everyone is building these super cool apps by plugging in their Vector databases and their large language models but now we have this problem that there are just so many options for how to construct these systems which embedding model should I choose do I need hybrid search do I need these Advanced query engines or even which language model should I use ragus is a super exciting new framework foret Ral augmented generation evaluation that's really taming this chaos and I think this is going to be such an exciting conversation jithan and shahul thank you so much for joining the podcast hey hey thanks a lot k andica for having us it's a pleasure awesome and also super excited to introduce my beautiful fiance Erica also developer Advocate at we8 who's going to be joining this podcast Erica thank you so much for joining yeah thank you for having me again awesome so uh jth and shahul could you tell the founding story story of ragas and what motivated you to build this yeah so we were actually playing around with uh like GPT and we're building a couple of applications and doing a bunch of consultings and that's when we saw that like with like when we coming from M background right and there we have the the practice of making a baseline having a test data set and having a couple of metries that we are trying to optimize for and that is the basis of all all the experiments that we learn that practice is not there when you're building LM applications and because under under the hood models are stochastic in nature the need for that is very real so that is why we thought okay we needed we needed like we need some way to actually quantify the performance of all these apps so that we can like we can be much more confident about the experiments that we are running and actually make sure that there are improvements happening so there was nothing like that right um right that we could use so we started off off with crafting a few of the existing metries existing Benchmark data like pair that was there and we Tred to craft it for our use cases we quickly reach the problem like problem that although these are valid data sets it doesn't the quality the performance of our applications on that data does not reflect the quality of that our users would experience so we wanted to build something that would be uh that will give that give the true reflection of the performance of our applications for the user so that was the initial problem that we had and that we was trying to solve and yeah trater on that will D us yeah amazing so I I think if we could um quickly if we could touch on kind of the the software stack partnership at a high level and then later on we'll dive further into like the infrastructure problem of tuning these search knobs but uh Eric if you could kind of explain you know the work that you've done with the Partnerships with say Lang chain llama index and unstructured and how we kind of see our relationship with ragas and these kind of uh Integrations yeah so it's so nice to meet the both of you and just like hear the backstory and congratulations on getting into YC that's a huge accomplishment and I'm excited to see where it goes and happy to be talking to you guys in like the early stage um so what I've done at we8 is I helped with the Integrations with L chain and um and llama index and then also working closely with our Integrations uh person Shri at we8 um for cre for creating a staging brick add in structured so how can you take the um chunks from your PDF documents that is handled on the instruction side and then fill it into your bb Vector database um but most recently I've helped with the Lang serve um adding the hybrid search Rag uh template to Lang chain and I've seen like great feedback from that and that's like a great way for people to get started and then similarly with uh llama index um we have like a few features that I don't want to share um that we're working on getting that integrated to um so I just love helping people build and like have these full flesh rag applications with these various Frameworks um and I definitely see like the opportunity and like the Partnerships that we can have with we and ragas um just with like the evaluation is so important and we'll jump into like how to do that and what exactly you're evaluating but yeah really excited for the partnership yeah thank you so much for that Eric and I think it's just wanted to just open the podcast by emphasizing how important that kind of uh collaboration is for us and how much we're going to be trying to you know keep I think it's so fascinating how you know you integrate like hybrid search or how to how we are going to interface our apis to let you tune the vector database and so let's TR let's dive into uh what particularly to tune and how you guys see that sure yeah so regarding Ras Ras currently is M focused on uh evaluating and improving uh R applications we started off with r because it is the most you know production ised llm application up until until date uh and while coming to R what we did was firstly uh when we started off there were no real established metrices to uh you know quantify the performan of RG systems uh the main two components being the Retriever and the generator the retriever we already know we had uh you know uh retriever recall precision Etc but again uh in an R settings it was difficult for people to go on and adate uh you know the correct chunks and everything so we had to come up with methods to quantify these without the you know uh the annotated extion and regarding the generator side we came up with metries such as faithfulness relevancy which can quantify the you know performance in the jour generated sale as well so this was initial starting we mainly focused on reference free because developers were facing issues with annotating you know all the samples they had with you know the the ground proof and then we also moved forward to do some things like answer correctness where we quantify the correctness of the answer to quantify the end to end performance as well so ragas currently offers two main things the isolated evaluation and the overall end to end pipeline evaluation interesting and I I did quickly see on the documentation that you were saying like there are existing ways to like um see the performance of these like theall Precision all these metrics but I saw that you guys were like there isn't enough like human annotation or like because we have like a lot of judgment we're like sure maybe the Precision is high but what about like just the wording of it doesn't make sense so like where did it fail and that like in each step so I think that's really interesting how you compare it yeah I I love that Eric I think that's the the what's now you mentioned the faithfulness and the relevance that you prompt the large language model to evaluate the retrieval quality I think that's the huge what's what's new about all this is in the past we had this like a labeling task of document relevancy right and it was like super tedious and you'd have to annotate like you know 10 documents for every one query and it's this notoriously difficult labeling test but actually even before we go further into the uh the the what's new and in this kind of llm evaluation thing I'm just really curious like um you know so so you can tune like the embedding model you can say hey do you need reranking or you know even maybe the indexing parameters is something we can dive in I'm just like maybe if you guys could tell me like how you rank the priorities of like what you should start tuning with your rag system right uh from what what we have seen from our customers and users tuning the retriever is the most important thing if you get the retriever correctly if you get the if you can get the correct chance for the for a given query correctly llm is most probably going to answer correctly 95% 90 95% of the time depending upon the quality of the llm as well so here mostly I'm referring to ANM of quality of 3.54 char 3.5 to 4 quality uh so there are now once you have get that done correctly you can move forward and to optimize for f things like Chun size where you don't really want to send you know tokens of really huge Chung size because it can one increase the latency two increase the cost three it can also lead to problems like lost in the Middle where the important chunks actually falls somewhere in the middle and llm ignore it due to the nature of find data or something so this is that will come later and and then if you have have to experiment with different llms once you have a basine then you want to shift to other llm or experiment with other llm you could always you know use matri like faithfulness to see how much you know how much change is happening just when you you know swap the LMS yeah and on the retriever side So like um I don't have a stack order of what all like POS what like on like what all are the most impactful but just with like what is easiest to do like one thing that have like we have seen like get a lot of like like get a lot of bank for buck with just how much effort there is like bring in a hybrid search like just having a vector search while it while it works on like some application is very hard to do a vector search for your data domain while just if you're doing hybrid search because there's also a component of like SP dat SP um signals also coming in that gives a massive lifting performance in fact that is like one of the questions right okay how can you actually tune your like Alpha score to so that you find the good the best balance between both of them but in in general getting a good hybrid search is one way having a good ranker is another way also there's a lot of easy wins to like easy wins at how you actually structure the data So based on um how you how like depending on use case how you make sure the data is much more accessible because right now um all the data is like people just like without doing any formatting they use the data as it is and there we find that it makes the retriever much like it makes it harder for the retriever to bring like correct size there's also there's subtle bugs where the the formatting of the data is wrong because of that the the even the llm kind of like extract the correct information all that part can be solved with good like basic data cleaning and yeah those are like three things in my mind yeah just adding one more point to it if you want to really if someone wants to really push the boundaries of rags to you know enable questions like that can even that can even query structural questions that involve structure for example if I am if I'm having a rag that has all the papers but I'm going and asking the interface to for a question like how give me the summary of last table in attention is all you paper so here the rag actually needs to identify the last table in the particular paper so if you just push the whole chunk together it is going to miss the structural information uh and which which can be very important for these kind of questions that reques structural information to identify the correction CH so all of these are you know very very new new things regarding drags and these are these are the uh aspects people who are trying to push the boundaries with rags tring to explore but these are you know very exciting uh experiments to run yeah I I think there's just there's so much scope to it it's I think that's why the opportunity for ragus is so you know compelling and um so I the the tuning of Chunk size I find that to be really interesting because then you kind of have like this end to- endend system like I think as unstructured new AI we're seeing all these players in the data ingestion layer grow and so is it's like um you're orchestrate you're kind of like orchestrating we8 as well as other things as well as the embedding like we8 has the embeddings in the modules but the chunk tuning but anyway so so yeah I can't it's there's so much like to the end to end and um the the adding structure to your uh schema I also find that topic to just be super fascinating I haven't seen too many people uh really experimenting with like personally I love this idea and maybe Erica you want to hop in on this because this is something that we talk about a lot is like um having you know like right now I think yeah people just put all their chunks into one big class and it's like you know documents like just content chunks whereas instead you could have like this is what I know about kubernetes this is what I know about rag this is what I know about embedding models and then the language model has this prompt or or you have some kind of like classifier that takes a query and routes it to one of the indexes um yeah Eric maybe if you want to hop in on on how we see that kind of like multi-index search and the structure of your vector database yeah um so for one of the episodes that I made um on index was covering their various quer engines um which is extremely interesting just because there are so many options and like the best one to use is really depend on your application your data how your data is structured it's like so there's too much to it I feel like that at some point it's overwhelming um so yeah I just wonder like how do you see would you would bragas make a recommendation on like which query engine to use like should you use the sub question query engine where like a very complex question is decomposed down into two questions or should you maybe use the SQL router qu engine just because like you do have like the SQL database and then also your vector database it's just like I feel like there's so much to cover in that um yeah it's like so complex but um I guess one question that I wanted to ask in addition to that one but I feel like this flows a little bit nicer is do you see ragus as like the full evaluator for the full pipeline but then when you really drill down to like the trunking the retrieval and then the large language model like the generation of the text like would you make recommendations and like evaluate each step or would you do it as a whole because I feel like if you do it as a whole it's like well your chunking was wrong but then in the next one use the wrong query engine it's like how do you see that with ragas right so our our vision of ragas is to build it as the open source standard for llm application evaluation and Improvement so this will cover racks and this will cover every aspect of racks like the the query uh query transformation chunking and theal phase and LM also so RX as itself as an Enterprise use case what we have seen is it's in most cases it's not purely rag it's bringing the whole value so for example in most cases there will be some small agent action or maybe there are some you know other other other small l actions that can complement the whole experience of the user so from building an application point of view rag is one of the most important component of of this application but again there will be other aspect as well for example agents there will be small kind of Agents there will be other you know application where rag is not clearly needed uh all these will come in so ragas will in in future will provide evaluation and Improvement um infrastructure for building such llm applications so that's where we are heading so my my question is I really want to kind of get to the bottom of this um multi-index kind of thinking and just um I see kind of two Dimensions to it where it's either you're adding structure to kind of vector search or you're also kind of orchestrating the existing infrastructure for what Enterprises might have with again this kind of like data mesh data Lakehouse thing where you have like all sorts of different Data Systems like maybe a you know reddis cache or like you're in your SQL snowflake or whatever you have all sorts of different tables and so just kind of that whole scope of uh searching through all these different indexes yeah so um like currently I don't I like we haven't thought about that problem a lot because even though like like the exactly what the exact problem that ER said was a Orin story like VI like all there's all these like different indexes we don't actually know what actually gives the best performance but we haven't actually did a lot like done a lot of research on okay which works better because what we have seen is that it depends on your distribution and like your use case so if like with a few clients who have like very simple like um retriever like um like very simple like data sources like a very simple like rack system works so you don't have you don't need a bunch of like complication and there like even like like some sort of basic tuning is like important I use cases I don't have a favorite depends on your like your use cases but hopefully like what we do is like okay even the retriever State we have like a couple of metrix that give you the Precision and recall of the data set and we also have a test generation that will make sure that okay it's much more easier for the user to actually build a test set that that you can actually test against with the ragas like scores so those are like like two things like those are like that's the approach that we have so we provide the tools but we haven't done like work with a lot of people that we are this is something that we're trying to do to actually figure out okay in this situation these are the indexes that actually work in this situation these are things that work but I can I think sh can provide like what are the metries that you currently have with for the retriever that people can use to actually evaluate this so you you like the listeners can actually try it out for themselves right so for retriever me we mainly focus on uh two things retriever precision and retri recall and here the Precision mostly the recall is the normal recall that we uh everyone knows I mean how much recall but the recall here is calculated using the annotated answer not the annotated chunk because chunk annotation is very hard uh so that's retrieval recall and then we have retrieval Precision which is also rank sensitive because people are trying to use uh you know uh things like uh rerank everyone mostly in Enterprise use mostly everyone uses re rankers and they would want to mainly know the difference between the you know the Precision if they are using re ranker and if you have not reason re ranker so and this difference mostly comes up when in in the ranking of the Chun because in after using reer what we see mostly is the chunks that is that the chunks mainly go up in the ranking and without this ranker the chunks will be somewhere down the lane which can also C LM to ignore these important CHS so these are things that we focus on Retriever and regarding uh how we facilitate this thing is that okay what we saw with developers is that developers have systems in which they are they have data coming in from different sources and to build test data sets for testing all this dynamically is a human can be very you know time consuming and it's not you know an attractive job either to sit and build all these questions and you know and answers so what we facilitate there is that if you have a data source you can connect that data source to ragas and ragas has a paradigm through which what we are aiming to do is to create questions that are that you know are very much similar to what you what what is seen in production so in if you see in production you will have questions that are of very diverse nature not that and and this is what we are trying to synthetically replicate using llms under the hood plus some paradigms that we have uh that we have developed yeah amazing so there's a couple things in that and I I really want to explore the ranking thing a little further Eric I think we wrote Our blog post on ranking models do you remember when that was I think that was like April maybe yeah earlier this year yeah but um so to me the question is um you know there's there's this tradeoff of um you know how many to retrieve than to rank like if you if you have a high-capacity ranker model it's slower but more accurate and so maybe you only need to rerank like the top 10 with that compared to like a faster like a you know faster ranker that you retrieve 100 and then rerank with the faster one so hopefully I've painted this picture where you have the how many to retrieve and then and then you the reranking the tradeoff is accuracy for Speed generally so I mean there tuning that kind of thing is such an interesting question because it's like um you're kind of tuning like which re ranker to use as well as how many to retrieve um maybe there's this question of is training your like you mentioned um generating the synthetic queries and we're going to really dive into that in a second because I think that's kind of the huge what's new about all this and but I think you you can also use those synthetic queries to train your own ranker model you could train your own embedding model that way as well but but I don't then you have to reind you have this like shallow layer but I think training the ranker is a little easier for now so so you could add that in the loop but yeah how yeah so maybe my my question in addition to kind of the whole topic and you know asking to comment on the whole topic broadly but my question would be like how important is that idea of the like uh tuning the ranker capacity and how many to retrieve to then rerank like where would you put that in your priority list it it is a some some it will be certainly in the you know high priority list because reer again can you know there is there is because of the nature of BU encoders which are used for the retrieval purpose it has a limitation there so what what ranga brings into the picture is the ability of the Cross encod models to take these retri CHS maybe top 100 and figure out which top 10 results or chunks are the ones that is the best so so that's that's a picture of rerank and that is why I think ranking is very very important and because there are uh you know open source developments in embedding uh you know embedding space you can actually train your ranker you know if you have you know you can even synthetically create data and train your ranker and this can really really improve your pipeline as a whole without much cost it might be like even for fine tuning plus you know doing some synthetic data generation this this experiment will cost in my my estimate less than $1,000 so but the impact of this is really really high on the quality of r as well so this is something of really high priority and something that I would really you know looking forward to more people doing it in the future so that you know I I think I think in general people are you know trying to find tune open source llm side rather than trying to fine tune the embedding side which is much the embedding s in the rag is the one which you can easily replace especially the ranker and then also get you know much uh much better output as well so that that is something like a lwh hanging food people are currently not much focused on but I think it people should I think people building rack should focus much on that yeah that's one aspect that I haven't really heard people um suggest but yeah like you said the impact is very high and yeah just awesome it's a great suggestion and I can only imagine what else you guys have coming yeah I love I think um fine-tuning the llms is um it's kind of easy right because you just kind of like language model your text and then the instruction following it's it it really looks like like I've I've been playing with fine tuning language models for this gorilla project and so I have some I've run the notebooks with the PF library and done that a few times and you know the instruction tuning is also just really simple to set up your data so I I mean you could kind of use the same framework for training your own ranker where you just kind of uh language model it and then you have the relevance score as like the last token that you're predicting at the end of the candidate document relevance score and so yeah I I do think just tuning rankers is something to really evangelize further but then I also am I'm really interested in this kind of orchestration of ragus and weate because I think uh training rankers might be out of the scope for both of us kind of and we're looking for that that other part that we say Hey you know we think that we need a new ranker here's the data you know and and so you would have that orchestration of like a a model training component into also kind of the evaluation stack with like the chunker and better uh Vector database so yeah it's such an interesting end to- end stack uh cool so um maybe we could transition into because I think this topic will touch on everything we've already to started talking about but this kind of like infrastructure problem of uh testing multiple like vector indexes or multiple uh index search index configurations because like maybe to set the table like if you want to compare the open AI model with the coher model with sentence Transformers you've now created three indexes so your you know your cost is going to be you know three not three times because they're not all the same Dimension sizes we could you know nerd out about the details about exactly how much each index would cost but you have this kind of orchestration problem of managing all these experiments you're running yeah um running them yeah but the first like so I was like um trying to figure that out the first aspect is obviously getting a good test set right that that will ideally be a reflection of the user the like what I would see in production and that's like the first major goal in fact like even the clients that we talk to that's the first thing that we actually try to facilitate so that that asset is good and we have a good representation of what like we have a good of what the like like what you're trying to optimize for other than like otherwise like okay raas like so when you like if you're trying to validate Ras Right Now the default what we the L that you're trying to use is open Ai and you can actually connect with any other like LM that you want all of them like all of the possible ents are supported but we like do most of the experience with open a uh and one thing that we do is we try to like break down the problem into much smaller T so if you are actually trying to figure out the faithfulness uh of your um of the different um the indexes that you have we actually break break it down into two steps which is okay the out of out of the generated answer what are the number of statements that are there and from the given context and these number of statements is how many of these statements are supported from can be derived from the context and that is how we actually score so we use so for from for Ra's point of view you need access to a data set uh you need a good data set you need LM that can is able to perform it and then you are like able to scale it so like what what we do is give the data set into your pipeline and get make sure that you get the the input query that is given the retri contact that you got back the generated answer that is there and if there is a ground truth in the the answer you use that and these four Fields you give them back into ragas and ragas will give you the scores you can choose you can choose the metrix that you want so the metrix will like if you want to evaluate if you want to compare between like open a and L 2 or something like that you can okay you can you want to that's the generator part so you can use metrics that focus on the generator part but if you want to compare like s Transformer with open a edings and VVS hybrid search all three things like that you can use um uh the retriever metrix that will be much more relevant and score that so um that is the point of view like that is how you would like set up the infra for a to do the evaluation yeah just St if you if you if you are any of the people hearing are using Frameworks like Lama index or L chain you can easily do it without preparing data by just plugging Yas into Yas into the evaluation and uh evaluation frame because we have Integrations with this framework so it will be much easier to use it if you are using these Frameworks I also I I love that ragas easily has these Integrations with linkchain and llama index um but another thing as like covering the visual aspect of it of like the full Pipeline and like where the problem is and when Connor was like oh would you like to join the ragas podcast he was like explaining to me what it is we were just like chatting about it and I was like wait so this is like the weights and biases of rag and I think that's like such a great comparison and it's cool that you guys come from this machine learning background because you understand like what weight to biases is and how it benefits people setting up these experiments um so this is like definitely something that we need in the rag world and application is it's like visual aspect of like seeing the evaluation of your app and I know it's really neat I don't know if you guys also agree that it's a way to bias this comparison that was why Epiphany yeah maybe um yeah so maybe one more thing about the weights and biases analogy that I like a lot is I see in weights and biases you have like the call back manag so you're training each of these deep learning models each one generally needs at least one GPU and so you know you run it for a while and and then you have like this orchestrator node so you have this like two infrastructure problems where you have the man managing of the gpus and then you have the managing of the central thing that's like how's all that going and so I see kind of index building is kind of a similar problem where um you also need to like wait like if I'm going to test coher vectors versus open AI vectors for 10 million vectors or like some particular configuration of product quantization or hnsw I'm need to like wait a little while before the index is fully built and I can fully test it and it's kind of because it because you know I I understand that like there's kind of like this full stack where you can evaluate kind of the question answering part and you're just kind of looking at the answer the like the answer that comes out and you say is this a hallucinated answer and then also maybe the language model is what tells you if it hallucinated and that's quite novel but there's also like this building of the index's middle layer which is where I see like the that's why where I kind of see the super exciting problem and so I think kind of the weights and biases another thing that we are very curious about is like um you know I think something that appeals to us a lot about ragas is the uh the code like as we opened up n mentioned like her experience with integrating our apis Into You know open source python libraries is the code the abstractions are great I'm curious like what your future plans are maybe building like a vis visual GUI like something that would really explain this tuning process yeah so um as like so one thing like at the future plans currently we want to like uh make improvements on the test generation part and the next part will be actually trying to answer the question of how can I actually use the user feedback that is coming in after you have actually deployed uh into the into like the into production and complete this Loop and make this a continual learning framework for pipel and that is the direction that we like want to go we want to make sure that this is a continuous Contin learning framework for R pipelines and then possibly LM application that's the direction that we possibly going but like um we the current one shortcoming that all of our users are saying that there's no way to actually like visualize all of like the results that are there that is something that we are interested in but then um currently there are also tools like um like there are Lang Lang Smith there are also tools like and bias that Pro like and you can also have your play Old NP jupyter notebooks so um so like what we expect the like users would do we want to give them the flexibility to use that the tools that they are actually using right now and then try to figure out it's it's current that is a bit of tricky because that's tricky because then the US it's the user responsibility to leverage the uh leverage the information best and get the insights that they want so yeah we want to like we will provide hopefully some more documentations and outs on that but that's our general like theme there we we are giving you the uses the tools and the flexibility uh to use that the tools that they already have to like tackle the visualization part but we want to go we want to be the continue learning framework for R pipelines so that because like all of these are combined okay you have something you have a test set and you have evaluation or you have a setup that is working when you're developing it but you when you go into production you get the real user data and like a lot of lot of like developers actually are still not sure of how can actually make the maximum use of this everyone is like like using like simple metrix like thumbs up thumbs out there are lot of there are many more metric that you can actually track and then all this come back into the development process to build a better test set that okay you figure out these These are situations where the users are not happy now in the test set I I should make sure that these cases are covered and then I I add them Tes it then you run them with the ragas metrix and see that okay for these experiments we see that the retriever is not performing very well and the recall of retri is not good because that is somewhere the the measurement is not good okay so how can I improve that then I maybe tweak something that in my application to improve that and then I go back put it back into production and see the results so that is the end goal that we want to achieve so um with that uh maybe um I'm not like super familiar with Lang Smith unfortunately I understand it's like you're visualizing like a promp so I understand Lang chain is like chains where you have like five language model calls in sequence and so you're kind of visualizing what happens at each step to see like you know like if I if I have like this uh editor prompt where it first parses each paragraph to like rephrase this and then later on it will merge them or something like that um so maybe if you could help me understand further like um yeah because I I think this whole thing can be so complex where you have like just retrieval and it's like Precision recall or you have like these langing chain llama index type of like agentic I want to call it where you have like five or so language model calls sort of on top of it uh maybe if you could just further explain kind of like what the relationship between Langs Smith and ragas yeah so um So currently like Langs Smith and Lang fuse and all the other like LM or absor llm observability tools uh give you uh like a like view of the avability of what is actually happening inside your application so suppose if I have a complicated rack system where I have some software for some sort of query transformation where I have I have like multihop questions so I'll give an example so suppose I have a multihop and a query transformation inside my rack I have a first query that is coming in and then I'll ask the LM to actually break this down into like smaller queries or figure out okay what are the what are the queries that I need to run that to in order to actually come come to the answer so that's a transformation process and suppose it generates like three three additional queries and then you give and you run those queries with your rack systems and you have to log in log okay what are the chunks that I got for each query and then how did I actually use that uh to fa bring out the answer and so it's like a trace span of the all the internals of LM application and it is exposing it in an ice UI so as a developer it's much more easier to reason about like what's actually happen happening in maap so with dras how how we we do have what we what we provide is that people can okay now there are all these visibility tracing tools that developers can use what is lacking is a workflow and metrics to you know run you know make this all you know work at scale so that that under that spine is what ragas is ragas provides and people for example now can use ragas metri with L spit where whatever visibility tool they use but the metrics or the you know the underlying things like test generation the you know making sense out of feedback all these things will be provided by n yeah so like I can give you an example so we working on this one blog post with Lang fuse where so Lang fuse what you can do with you can export like suppose you have like traces you are logging all the traces into l fuse you can export off like the whole trace or a part of a trace and you can ask ra to score um score like with the matri here one thing is that you can only score matrices that are reference free we can like dive into that if you want but you can like use the metrix like reference free matrixes to score all these like different all these different logs and if you if you find that any like in any of these like traces the performance is not good you can click through the UI to figure out what happened internally so that is a production like online monitoring um setup that you can actually do with Raa Studio yeah I think that vision of um visualizing the multihop question answering where I love the um did Thomas Edison use a laptop example of multi-hop question answering where you break it into you know when did Thomas Edison live when were laptops invented so I you you really paint the story clear where I have this sub question query engine in our llama index investigation and you break it out into these two questions and then you ragus you look at the you know the retrieval here so I have a couple of kind of futur looking questions um shahul you mentioned the kind of metrics and I think I think this whole like using the language model to evaluate the search results I know some people there like you're using noise to evaluate noise like some people have this kind of you know adversarial uh take on this kind of perspective but uh to me kind of a vision for ragas that is kind of excite is quite exciting is like um fine-tuning a smaller language model just for the sake of uh faithfulness evaluation are you guys thinking about this right so exactly so this this is One Direction that we are thinking about so so we have you know we we can think about in two ways either you know we can have smaller models for evaluation and or we can have the smaller models for just for gaining insights from production data points so the the these are things that will uh you know uh will hopefully we'll do in the future that you know we'll have these custom models that can do these kind of tasks specific tasks that can enhance the visibility of you know the the LM application and how it's doing and not only offl but how it's doing in production as well so yeah I think it's completely possible and with the you know with the ADV advancement of Open Source with you know new and better cheaper Foundation models coming in these kind of TS can be very well Achi achieved with smaller models as as very similar accuracy of these huge models I guess another like futur looking thing touching on what you just covered is what do you think of having agents that are assigned with testing the faithfulness um I know it's like you can find two in the llm but what are your thoughts on agents because I feel like right now everyone's like it's too early for agents like just like wait a second but I don't know what do you think for the future do you see that as a possibility sure sure as I said uh the the you know value and and the total experience of creating an LM application comes with not only racks but also agents and with other compon but but other things that that is possible with llms right so again if you have some specific aspects like agent or any agent Behavior it should be tested that it should be tested on evaluation at scale so it is something that AAS will tackle in the future we are also figuring out you know it's these are like open questions how to actually evaluate or test agents because this is these are very new uh you know very new you know capabilities of llms so again yeah so in future we will hopefully tackle that as well and include that in the ragas tack so that you know it can ragas can be the place for llm application evaluation and continue to improv it yeah I I I have a question on this I really want to dive in um so I've been super interested in the gorilla llms fine-tuning a model to use we v8's query apis and I'm super curious about this idea of using tuning apis as well so maybe to set the stage a little more so weeva has this graphql search API where you can combine things like a filter with hybrid search and then the particular Alpha parameter so on and I imagine like with uh like agents and agents that tune you know rag systems is that ragas has you know these apis and you know maybe if also we could you know so like I don't think you could describe all the apis of ragas into an open AI Json functions like I think it's you know I I I think that's only for very simple functions like get the current weather I don't think it can the agent can control ragas just with the Json of the you know of the apis but yeah so I do think this is an extremely future looking question is agents that are like using ragus to tune a rag system I'm just curious if you guys are interested in that yeah like Andre kapati like I guess like two years three years years back like told this like project like Operation Vacation where the whole idea is that okay you have you have M like you have your Monitoring Solutions that is monitoring like your your performance of application and then you use that data coming out to actually improve your bottle also so as a like as a data engineer you don't have to just have to oversee the if it's everything is working like properly and that's the future that we also hope like ideally okay you have set you set up and everything like naturally happens because there's data everywhere that's coming in it's like it's definitely possible it's not it's just that the yeah it's just just the process of experimenting it and trying to figure out because people have already done it so yeah that is something that we are all super excited about having an agent that actually optimizes po yeah yeah I I love that question um like this whole kind of like meta AI building AI recursively self- automation all that those kind of topics so yeah one more kind of anchoring uh future looking question I wanted to get some more perspectives on is um is this long context rag thing and Erica maybe if you want to kick it off I know you you told me a ton of new stuff about this just this morning we can just continue that well I guess this isn't super forward-looking because opening I just released GPT for Turbo with the 128k context window so it it's Pres um I just wonder how you guys see like the evaluation with such like a large context window um Greg ceron I hope I said his last name right um just covered an experiment of testing this long context window and seeing how well that it performed and turned out like you still have obviously the loss in the-middle problem um with the output and just like the performance of it um but yeah so what are your thoughts on these large context windows are they the future should we still like have these chunk sizes and like the structure of our data um yeah what do you think right so regarding yeah regarding larger context and the lost in the middle problem I think the Lost in the middle problem is something that can be sold because I I think it it might it it occurs partially due to the nature of maybe the fine tuning data because if you see if you even if you find for longer context length what happen is that what what happens is that the model does not really need to use the context somewhere in the middle to you know predict the next token at the you know at the end maybe so that that so that so I have find tuned you know ton of models and this this behavior is something that I've observed when F tuning for larger context because if you ensure the data quality while find uning and if the data has such dependencies it will naturally allay its attention score values and Vector values so that it will use the you know it will it will identify information anywhere in the you know context window so surely I think in very near future that will be sold but the the case of longer context windows again now we know you know with the row pending endings we know that how to you know enlarge context windows up to 128k I am pretty sure that even open has followed very similar you know ways to enlarge their you know enlarge their context lengths uh but again uh I I am not sure what is the use of context length after 32 or 50k window because I haven't seen anybody use it or I haven't really seen any application where that is very very usable and scalable uh so yeah I'm skeptical towards the uh the enlargement of Contex window Beyond a particular point because I don't at this point I don't see any use of for it after you know 100K or even less interesting yeah I mean that's a great point I think it's like in the beginning stages of testing it out and like are these large long context Windows even valuable and like at what point should you cut it off um yeah and I think um earlier we talked about multi-index tuning and like having multiple indexes and I think that's also quite related to Long cont text rag because um I really like the example of like with we8 features if someone has an app and they're saying you know I need to add uh can I add recommendation from can I add whatever wva offers for recommendation to my app and then it retrieves like 50% from your codebase 50% from the wva documentation and then like stacks these up into like a 50k context window and and I also really I think with the rag it's it's quite interesting to like you know like I think a lot of us as we talk about like recall and precision metrics we're mostly talking like recall at 10 when you're then putting it into question answering but if you could do recall at 200 now now you don't need to be as worried about the um the re I think the ranker thing might lose some value if if if you only need recall at 200 because that was the setup to begin with yeah yeah I do think that's an interesting idea of having like take the we documentation and then also the blogs or like any other information that could help build like how you can include bv8 in your application or something like that um just like maybe you take like 150 um objects like relevant information from the documentation and then like 50 from the blogs it's like how do you wait that if you are going like the multi-index rout it's like similar to the hybrid search problem it's like how much should you weigh Alpha so that you have like the best performance like should it be closer to the keyword or should it be closer to Vector soar it's like yeah just like a mesh of opportunity yeah I think that was just the perfect yeah like um you know now tuning how much to search from each of the indexes relating that to tuning the alpha and how much to weight bm25 in Vector search Eric I loved how you compared ragas to the weights and biases for rag I don't know if you guys are gonna I don't know that'll be too adversarial of a slogan but yeah Jan and shahul thank you so much for joining us on the WEA podcast I thought this is such a fun discussion I'm so excited about the future of this technology and we're going to be you know sending messages about ideas that we have to help contribute to this and congratulations on why combinator also it sounds like such an exciting journey and thanks again yeah thanks thanks car for having us it's like like it's like ex exact like exciting times actually like to be be be able to like working like work on like all these are like like opening up opportunities that we could like we were even a year back we were like you wouldn't even imagine so like exciting times to live ", "type": "Video", "name": "RAGAS with Jithin James, Shahul Es, and Erika Cardenas - Weaviate Podcast #77!", "path": "", "link": "https://www.youtube.com/watch?v=C-UQwvO8Koc", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}