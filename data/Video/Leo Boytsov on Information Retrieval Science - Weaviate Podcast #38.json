{"text": "Hey everyone! Thank you so much for watching the 38th episode of the Weaviate podcast! This episode features Leo Boytsov, ... \nhey everyone thanks so much for watchingthe wevia podcast I'm super excited towelcome Leo boy stuff Leo is currently asenior research scientist at AWS Labs uhprior to this he's written adissertation at Carnegie MellonUniversity titled efficient and accuratenon-metric k n surge with applicationsof text matching I started studyingLeo's work after seeing some of histweets that brought a ton of knowledgeto the discussion around deep learningfor search technology and finally Leo'sco-authored an incredible new work inpairs light that I'm just super superexcited to dive into so firstly Leothanks so much for joining the podcastoh thank you very much for inviting meum it's great to be hereawesome so I think maybe setting thestage I really want to start with thisin pairs light work I think this is sointeresting and maybe this background ofas we think about the intersection oflarge language models and searchtechnology I think most of us are sortof going in this direction of you sortof retrieve and read you use the searchresults to uh to put into the context ofthe language multi Granite generationbut this other idea of using thelanguage model to generate training datafor the you know for the search models Ithink is so interesting so could youmaybe start by describing your interestin thisum yeah um if you don't mind a littlebit ofum I would like to start with a littlebit like historic uh background uh withuh retrieval maybe I I should say acouple of words about retrieval ingeneral and also aboutum the difficulty of applying uh machinelearning to retrieval and that has beenlike historically so um the caseof it what we usually what what we didbefore all the machine learning we hadwe indexed uh text using uh invertedfiles which basically is your you knowbook uhbook catalog for every keyword you youwill get like Pages where this this wordoccurs and uh the the biggest Advancewas the basically the bm25 function thatwouldum tell you uh the score of of a pageuh given how many times the the certainkeywords appearin in a page and that was a step bigstep forward compared to the previousscoring function compared to Booleansearch where you would have to specifyall the words that need to appear andthat would experts would do so and thatuh being qualify function was really abig standard for quite a while and itwas difficult to beat even with machinelearning techniques because whathappened next like people figured outthat bm25 is a goodum good signal but there can be otherones and then they umstarted to use machine learning uhapproaches such as boosted regressiontees to combine those and now theso-called learning to rankum and that um in in somewhatregretfully in many cases it actuallythat didn't help much and all like theuh whereas NLP was advancing at you knowat you know rather rapid Pace in ininformation retrieval it was for a longtime it was not possible to be Beyond 25or like slightum you know slightly boosted Beyond 25.and and one reason and and there isactually a real reason for this in myopinion is that in NLP uh like sayconsider something like a classificationtask in NLP you look at the page andthere are like certainattributes of that page that indicativefor example of the sentiment right andthey're only typically or often they areonly attributes that are specific tothat page but when you're doingretrieval you need to look at uh atfeatures thatum that are query document features thepairwise features and those featuresneed to be sufficiently generic so theit's it's often just not enough to lookat oh oh this is like a trigram orbigram that that appears in the documentor in the queryit's often not enoughunlike NLP so in in general there's likequery documentum similarity that has been historicallymore difficult for machine learningapproaches and another thing is I Iguess that search is also much lessprecise and much much noisier so it'sharder to come up with this you knowthere is bigger long tail so it's harderto uh come up with with somesome features that work for this in ingeneral and again that this is really uhdifferent from a lot of NLP tasks uh saylike uh question answering factoidquestion answering the rock use uh evenbefore neural network uh these systemswere able to achieve a human levelperformance like for example with ABMWatson system that did human Championyeahum I hope that's not not a very longintroduction but historically uh it wasdifficultuh to beat simple bass lines and when uhdeep learning started toyou knowum to become very very popular inum information table it's fair to saythat until we got birdwe didn't get any really goodImprovement at least not in all thedomainsum over Beyond 25 and only with Bertwhich is relatively recent uhdevelopment we were able to beatum bm25 like decisivelyyeah super interesting I think with mypath in deep learning I also rememberlike when I started deep learning thenatural language processing was mostlylike uh one-dimensional convolutionalmodels but like before the Transformersthe Transformers just like busted openlike the Deep learning for naturallanguage processing so so could wecontinue on the cross encoders and thelearning to rank um just kind of maybealso just the beginning of it being thatthe cross encoders are high capacityclassifiers that take his input thequery in the document and then output ascore and so we use that score tore-rank it and you know the vectorsearch will give us like a thousand oryou know however many results to beginwith and then we'll re-rank that to geta higher quality list uh so can you sortof describe like cross encoders and thenI know learning to rank and like the umusually you have like clicks to likeuser data like XG boost models so it'slike a little different right tothinking between cross encoders and likemost of the XG boost style learning torank stuff that's been developeduh yeah absolutely uh so first of allum so when we say learning to rankum although learning to rank is a verygeneric uh very generic uh term butoftentimes when people say learning torank they mean uh specifically that oldstyle learning to rank when you havefeaturesit can be pairwise features like uh thebm25 scores it can also beum query specific or document specificfeatures such as like query type or likedocument quality score but these arefeatures so in the end you get a longVector of features and then you usesomething like boosted regression treesor it can be actually as simple as alinear functionbut that'scrucially on like on top of featuresthat are pre-computed somehow engineeredand with theumwith the invention of this strong neuralnetwork models such as Transformer basecross encoders and buy encoders we wereable to replace most of those at leastum most of those features with just thisuh similarity score that comes from theneural model and it's also learning torank of course but it'sum somewhat different from the theclassic learning to rankso the difference is um you you say thatbm25 score maybe also like bm25f scoreas well as BM 25 score for the multipleproperties of some kind of object aswell as the vector distance score anduse those additional features in there-ranking model compared to maybe theselike content only neural models thatjust is like query document morefine-grained kind of matching is thatlike kind of an accurate description ofthe sort of Distinction of ituh yeah I think it's a good one and it'sstill possible to plug in Bird scoreinto the bigger model to but when ourimpression that they say for for the thesay query document similarity onlypeople would use more sophisticatedfeature-based model to achievebirth-like performance but now you canreplace uh those with bird uh and infact in terms of the query documentsimilarity what I also used in the pastand that's relatively unknown umsimilarity function is the so-called ummodel oneuh and it is semantic in terms of in thesense that it can assign scores betweenquery document pairs that are not exactmatchand in some certain cases in particularfor question answering uh for uh if youretrieve document for a questionanswering system it gives really bigboost on top of Boom 25 which actually afew people know but this is the case andum it can improve BMW 24 upon 25 but youwould still need to do some fusion likesimple learning to rankuh and you can replace it now usingbirth and typically get much betterresultsso I guess kind of the ending onreplacing it with bird is sort of whereI where I'm hoping this is headed isthat these cross encoders can just sortof again like the query document isinput and kind of overcome needing to doall this feature Engineering in thelearning to rank that maybe just thecross encoders can re-rank by themselvesjust based on the based on a morefine-grained query documentrepresentation and maybe we could ummaybe we could come back to the in Parislight and how you how you train crossencoders for custom domains maybe talkabout the zero shot versus the in domaincross encoder in sort of that sort ofDirectionso uh yeah uh um sure so the uh as Isaid it can uh the the the cross encodercan largely replace uh some pre-neuralMachinery that was using the combinationof features and for the cross encoderacross encoder and there are two typesof neural models that we're using nowbut they are both Transformer based butbasically cross encoder is a little bitmore accurate and it um you concatenatethe queries and documents and pass itthrough theirmulti-layer a relatively largeTransformer model which basicallyproduces your scoreusing a projection layer uh uhadditional projection layer and youtrain this model so these models arepretty accurateum there is also a buying color model uhwhere you encode queries and documentsseparately and produce uh embeddingswhich you can compare using cosinesimilarity those models are somewhatless accurate uh but they permit afaster retrieval because you can pickComputing beddings during the next timeso these are two types of models and interms of the um so those are great butthere is one problem so typically uhwell that's the perennial probably weneed training data for these modelsthese models are relatively especiallycross encoders they're relatively dataefficient but not like it's not like youcan train them on five examples we havea paper where we uh evaluated the fewshort properties for a number ofCollections and yeah it's more likeum hundreds uh well if these aresparsely charged queries you need a fewthousand maybe hundreds with um denselyjudged queries with a lot of relevancejudgments per query thenum you can do few of them it's still alot of data right and another problem isthatum they are trained for a specific typeof document you called domain forexample question answering overWikipedia and you try to transfer it toanother domain say legal documents andoftentimes it just doesn't work so andwe it's not like a big discovery thatthat was expected we uh we showed thatthat was the case uh in in factconcurrently with that uh uh now famousbeer paperuh then they did it also for they umthey did of course much more becausethey showed that also for buying codersand what they showed that in fact crossencoders transfer beta thenuh thenthen buying quartersumand I perhaps should clarify what wemean by transferring better Wars so inthe main mismatch and big problem withthe main mismatch is that it's not justperformance goes down it goes down somuch that this wonderful neural modelsworking so well in the main they becomeworse than bm25 out of the main which iskind of ridiculous right it go it throwsus back uh 15 years agosoyeah I think that in domain out ofdomain there's definitely a lot to thatthat we can unpack um but I guess kindof like this idea of having an in-domainmodel for your particular applicationlike um I kind of wonder about the zeroshot and whether you need that like Ithink it's a great thing to get startedbut once you have some data set and youhave some queries for whether you'rebuilding like a finance search app or aTwitter search app or like I'm workingon the webiate podcast search app soI'll search through this transcriptright so like I I'm thinking like um youknow having I could use something likein pairs to generate my data set andthen train my cross encoder on that dataset and then I can you know re-rankwithin domain and it kind of makes mewonder and it feels accessible to mebecause you know like these um crossencoders are like text classifiers maybeyou can shed some more insight onto thedifficulties of training cross encodersbut to me it seems like a you know notthe most complicated deep learningoptimization task to train a textclassifieruh oh yeah uh training cross encodersum I I love to train cross encoders andI hate to train buying coders I I didthe uh the cross encoders the um so thecross encoders versus buy encoder youprobably should have uh said talked morea little bit about the advantages anddisadvantages of both types of models sofor cross encoders they are really easyto train but they are slow unless you'reusing them in their ranking mode thegood news is that in many domains bm35already generates a list of candidatesthat is pretty good and these models canbe some of these models can be prettycheap to run with buying coders uhthey're difficult to train they convergereally poorlyum and they uh but they get theadvantage is that you can potentially uhyou know make ritual faster because youdon'tmaybe need that re-ranking step or makeit less necessaryuh so yeah in terms of the but bothmodels require a lot of data to trainum and what was ex it was difficultbefore this like this data typicallyneeds to be manually created and onceagain anotherum another difference between the NLPdomain and uh uh information tabledomain so if you tell somebody like picka random NLP person and tell them thatyou have difficulty annotating 1000queries then I was like oh like what'swhat's the big deal with a thousandqueries one thousand examples basicallythat's kind of the light bulb that goeson in the mind of uh your typical NLPresearcher but no no no you have toannotate the query what does it entailyou say for each query you run somecandidate generator or use a bunch ofretrieval systems that produce resultsthe modern area they need to be diverseand then find some a bunch of documentsand ideally you get like hundreds ofthose some of those are going to berelevant some of those gonna benon-relevant and then an assessor ormaybe a couple of assessors those arepeople who are judging um would go andread these documents at least brieflyand for the 100 documentso this is a normal salutation effort soI it's even if it's like really sparselyjudged it's still like one thousandyou need to read a few thousanddocuments to not take one thousand quizit's much more expensive so yeah thedata is a huge problem and zero shotdoesn't always work and moreover what'sinteresting when the zero shot isworking well and that that result comesfromum from the great work of RodrigoNigeria uh and some of his quarters thatsome of the models they do transferreally well but they typically need tobe large and not just large hugeso something like 3 billion or 10billion parameter models we are notgoing to put those in production anytimesoon maybe not even within a couple ofdecades so that that's that's a big bigissueum so ideally we wanna train models thatare small so how do we get data and uhexcept entertaining it manually and thenalthough there there was some work onself-supervised learning in the IRdomain but again things I would rememberthings are more difficult than they arethe main it was much less uh for examplethe contributor paper there was much andbefore that we had embeddings which aretypically much worse than being 25 likeaveraged in bearings um they are worseand unless that's a very specific somedomains in some domains they do thesethings do work well but say not for MsMarco not for legal ritualum and yeah so those Excel supervisedapproach training approaches theyum don't work well and there wasrecently there were a couple of excitingpapers one coming uh from rodigo and hisgroup called in pars and another waslike slightly preceding work UPR andshortly after that those propagatedpaper as a follow-up to eat parse andthese over three three papers thatshowed Oh you can use a large languagemodel to either generate uh trainingdata fromsuccessfully or you can useum llm directly as a pre-trained llm asa re-ranker and that was the UPR paperthat um we also cite so so and obviouslyin person propagator they are much moreumpractical in the sense that they insteadof re-ranking using a large model theyare basically using promptingprompt engineering to distill theinformation from the large languagemodel to smaller ranker or retail modeland that makes things much morepracticalyeah and I think there's lots of there'sdefinitely a lot to unpack I think thethe distilling part I'm very interestedin that and kind of different from theknowledge installation where you use thelarge model to label the data and thenyou learn those labels the syntheticdata generation thing is a differentcase where you just generate the data todistill it like in the data space ratherthan just like the pseudo labeling labelspace but uh quickly it is you there'sone detail you mentioned that I reallywanted to unpack a little more and aboutthe relevance judgments for trainingcross encoders and I know we have likethis ndcg metric where we say label therelevance of documents and say like ascale like a three level scale like youknow this is a three this is a threethis is a three this is a two this isone one one one zero zero right so umlike how important is that label likemultiple relevance labeling havinglabeling multiple relevant documents perquery for the sake of training thesecross encodersoh umthat's more than one questionumso first of all it's uh it's great pointthat the distillation through promptingthat was uh used in in-personpromptegator is so much different fromthe classic distillation through caledversions that we typically do uhand I would want to address this uhquestion first because uh it's I I thinkit's still it's there is work showingthat it may still be possible to do themore standard uh distillation as wellso the um the UPR paperthe umI I hope I not mispronounced the namelike by session at all uh what theyoriginally showed that they can re-rankusing a big modeland uh theuh they have a follow-up paper wherethey show that you can actually usescale diversions to distill thatKnowledge from the main model to the uhto a smaller rankerso that seems to be possible too uh butanother uh yeah uh but what the thepromptigator and imparts foundapparently so it's easier to use promptsto actually uh use prompting as as ageneration of synthetic examples andsome sort of proxy for thisinstallation of the knowledge well thatsaid I think what's what's important Imean these are sort of technicalitiesbut what's important I think the oneimportantum piece of information one importantInsight here is thatumwe have something to distill and that'sa really good question how this happensso I remember I mentioned before thatum IR is a difficult domain for machinelearningbecause well for many reasons in thisquery document similarity features areharder to umto somehow for ML and before uh beforeall those exciting developments therewas a lot ofum work on unsupervised uh like dog todeck style word to vaxton embeddings andthey don't work welland there were and one reason why theydidn't work well because they weretrained inself-supervised fashion without directsupervision and we know that it didn'twork well for IR moreover it workedmostly mostly didn't work well and itworked really poorly so then Bertappeared at first people tried when Bertappeared bird if people probably peopleare familiar that with bird bird canthey provide embeddings for text as wellalthough this embeddings well a lot ofpeople including actually Nils who wason the podcast they quickly noticed thatthose embeddings are not good for thetable for something like sentenceretrieval yes you can use it but butquery to documentary doesn't work soself-supervision doesn't workuh then you would need a ton ofsupervised data and buying coder whichis like really tricky to trainnow we have models that uh a trainedusing self-supervisionand now they work for ritual sosomething changed and and that thatwasn't present beforeso and we don't know exactly what is itlike in scale of the data is it what uhis it the way how these models aretrainedso yeah so that's um and that's why uhin in that's why for us in in partslight paperwhich is in fact basically reproductionpaper which is very incremental withrespect to that much more fundamentalwork but we try to answer a couple of uhimportant questions such as can you makeit even more practical and anotherquestion isum is it really the scale just the scaleof that next talking prediction trainingthat makes the model learn somethingabout IR as well which didn't happenbefore and to do so we need to takeum we need to usea purely open source model like Bloombecause we can trust that they train itlike this way and didn't find unit onany IR data sets or even something likeIrish which cannot be we cannot be sureif you take gpt3 do you know how it wastrained no I don't knowyeah I love those details of the paperlike the um the bloom seven billion themaybe the GPT J model so these opensource language models to generate thetraining data which are also a littlemore accessible because I think likewith the gbt3 da Vinci like at thepricing of recording this podcast it'slike going to be probably like a centper you know generation sort of aroundthat so you know 5 000 scaling it so itmakes it seems a lot uh very useful tobe testing if the smaller generativemodels can produce good synthetic dataas well and then also the smallerrankers because in the end you use the30 million parameter uh mini LMarchitecture based rank or model so Imaybe could we touch a little more aboutthe inference requirements of therankers because uh well I guess thesejust just inference Topic in general thebecause like the rankers you know if youhave like a three billion parameter rankor you mentioned the log profit thelanguage model to get the score tore-rank that either in you're gonna bewaiting waiting for the re-rankingforeveroh yeahum yeah so so basically uh like ourpaper although it did get some positivefeedback uh it's it's really uh it'sreally very incremental reproductionpaper but what Iso so it was uh in fact not my main workproject it was a sidekickum I supervise the team of Masterstudents and and CMU and I was doingthings jointly with them not justsupervising them so they implemented infact some key component how to generatethese synthetic queries and then I tookthis and um I I ran I ran theseexperiments most of the experiments andand yeah uh it's originally the questionwaswas that yeah gpt3 is somewhat expensiveso can we make it cheaper by using theopen source model but that's not not allthis story uh that'sum this is the part where uh Rodrigo noguerra's team with Bonifacio and allthat they scooped us a little bitbecause they showed in the follow-uppaper yes you can replace gpt3 but whatthey didn't try to do they didn't try toreplace the huge model with somethingthat is uh really much more practicaland that's something that what we did sothe cross encoders theyum although they can have reasonablerequirements they can be applied on theinference times they can be applied onlyto umto like small subsets of documents so ineven with the I don't remember inferencetimes for a bigger model but even forlike 30 millionparameter models on just with Generalpytorch without much optimization it'ssomething likeumI I think a few thousand documents persecond on the modern GPU which whichmeans you can't afford the ranking morethan like a couple hundreds documentsreceived Say by pm45and it's actually much worse if you trythe model that is 300 million parametersyou multiply it by 510 and another 10multiplier if you take the models thatuh Rodrigo guerra's team was using withexhibition parameters probably also likefive timesanother five times slower or somethinglike that uhyeah I think it'll be really great likeuh sort of like blog post content aroundweediators getting these numbers out oflike how long it takes to re-rank athousand 100 documents with with eachparameter size uh Transformer basedcross encoder uh yeah it's superinteresting because the the 30 millionright it could re-rank it super fast andand uh else maybe because I think likepeople are used to like I'd say like sub30 50 millisecond Vector search and bm25search so uh because I I just like yousaid to rank a few thousand documents ina second with a cross encoder becauseI'm just curious if I heard that detailright because with my experiments it'sbeen to re-rank like a thousanddocuments and probably looking at likeyou know 10 to 30 secondsuh Which models do you use I it'susually I'm using the sentenceTransformers cross encoders and uh yeahmaybe like 80 million parameter scalejust like uh testing it but uh yeahmaybe just for because I think I'mmissing a huge part of my knowledge nowhow are you uh optimizing the crossencoders for inferenceum I I didn't and I I think it may be uhdepended a little bit on the GPU thatyou're usingum but for the uh for the for the 30million parameter model that I I wasusing I I did measure the times and it'sfor 100 documents it's something like0.3 secondfor a hundredum yeah so the you can rank one thousandin some cases you probably would need todo so and it's going to beum more expensive but I think that inmany uh in many cases people deal withsmaller collections we are ranking 100is enough and in fact that was one thingthat we changed between the when we didreproduction but that one thing that wechanged between in-parse paper and ourreproduction is that they were rankingwith thousand documents we were ranking100 and with somewhat uh with somewhatbiggest modellike having like 400 million parameterswill basically match the numbers on mostcollections we get in the neighborhoodof day numbers while we're ranking only100 documents so this already shaves offuh an order of magnitudesoum and also another thing that goingfromuh 400 million to 30 million fundamentalmodels is also much more uh efficientalthough the results are not as good soit's um an open research questionsomewhat open the situation how to makeit even more effective butumwhat I should have mentioned before Ithink it's it's a step forward it's notdefinitely not a solution of the problemuh what is a step forward in particularif you look at the paper over the kingParts they use two models one is as bigas our biggest Model A little bit andthe the same same order of magnitudeum 300 200 to 300 million parameters andwith that model they do not beat bm25 atall just except for Ms Marcos parse uhand the track so only on one collectionthey can do it and we take a 30 millionparameter model and we were able toconsistently outperform bm45 on all thecollections they were using so it'sdefinitely step forwardum yeah soum people are in pretty sure people willoptimize this recipe even further andwe'll get someyeah I think like um because I haven'tdone things like even just like you knowmaybe just the Onyx optimizations themaybe compiling into the Nvidia Tritonserver and we're also really excitedabout neural magic and the sparseinference acceleration all the kind ofthings we can do to make here and fasterand uh I also think kind of this topicum like I think as we're thinking aboutthis retrieval augmented large languagemodel thing you might be willing to waita little longer for the re-ranking alsobecause you know you're going to pay forall these Generations so you know ifyou're searching likelet's say I'm simulating my interviewwith Leo and and I'm uh and I want and Ium and I'm and I want to like practicemy podcast and I'm talking about inpairs light and I want it to retrievesome passage and I have this big Corpuslike I'm willing to wait you know twomore seconds for it to get that topresult that will help supplement thelanguage model to ground the context andthen pretend to be Leo in my use of thelanguage model so yeah so maybe we couldtalk about just this General space ofretrieval augments that large languagemodels sort of pivoting topics a bitum I'm sorry I'm a little lost do youwant me to touch what kind of theydeveloped the topic of efficiency alittle bit more oryeah if you're interested in doing thatfor sure I I yeah sorry that was kind ofit's like a Loosely tied togethertransition but yeah this it's like Isigned I kind of see like the value ofre-ranking is increasing with the largelanguage modeloh I see uh well first of allumthe the ranking can be relatively cheapso that's that's againum as you mentioned people sometimespeople can wait if you're doing anEnterprise search and something withinone second is probably uh a possibilityum so uh 30 million parameter model isdefinitely practicaland um but you probably need still needto run it on some gpus deviceso something like I think there may beM1 uh hybrid CPU GPUprobably would be requiredum or like pretty multi-core CPU um sothat's that's one thing but the secondthing of course uh incan be possible again uh combine it withthe retrievers as well I we didn't trainretrieversbecause unlike in parse paper sorrypropagated paper who trained retrieversand that can be if you train retrieverwhat happens with the retrieverretriever is like buying color model itcan be pretty accurate on its own yesevery ranker adds something on top ofthat but usually like 10 15 so if youhaveum aretriever you can in Factory rank fewercandidate recordsso instead of say 100 you'd rank on thelink30. so that's or instead of 1000 youwould rank 100 so that makes us a littlebit moreumuh yeah more feasible and withretrievers you can invest more into likeuse bigger models because you kind ofshifted to the indexing timeyeah so that's something to try uh andwith four four fouryeah so um yeah so that's basically uhyou can wait a little bit longer you canuse buy encoders you can use slightlyfaster Computing devices and you can usea more optimized um environmentand I'm not sure about the neural magicbecause I haven't uh played with itmyself but uh I know that it's like onsip on on the there are cheap gpus nowlike what things that are like that arecomparable in speed total Titan and theyhave some likethey're very cheap and andwhy not use them if they even cheaperthan maybe than CPU why not use them forinferenceyeah I think um just yeah on the neuralmagic side of like it's like thiscombination of things with likesparsifying the models lower Precisiontaking advantage of like the CPU cachehierarchy uh if people are curious thatwe have an interview with Michael Goinwho explains all the details of that uhbut uh yeah uh so yeah super cool and Ithought that trade-off you mentioned wasso interesting of the um of like how youwant to think about putting yourresources towards optimizing the buyencoder like vector embedding modelswith the cross encoder re-ranker modelsand how if you have really good buyencoders and you would only need tore-rank 30 and then you could have ahigher capacity re-ranker like threebillion parameters because you're onlyre-ranking 30 or or this other kind ofthing where you're like we're going tothrow it all into the learning the rankkind of thinking where you're like let'sget like a thousand from bm25 a thousandfrom Vector search and just into this 30million parameter ranker which is likethe engine to fix it I that trade-off issuper powerful I thinkyeahum absolutely so you can in fact as Isaid uh the like old style IR can stillget like in the in the case of QAcollection I'm I'm guilty I haven'tpublished that paper I should uh thatshowing that actually really can getit's not only like Factory tool thatthat can be helpful and uh yeah in theadvantage although there is some workpublished on that but the games that Iget there they are actually not that bigI think you canum can you can I should publish a morecompelling paper basically saying thatumyou can use bm25 it rank just likereally a lot of uh bm45 entries usingold style Tech or like modernized oldstyle tag which runs on CPU it runs fastand then it gives still you like almostalmost like close to that by encoderlike style quality and then you can useyour ranker and that said I'm not likeparticularly attached to rankers incells we hadum I think it was more for the paperitself it was more important to showthat you can't replace gpt3 and not onlythat uh we got thingsfor their uh for the in-person paperauthors who big thanks uh twin Partspaper authors who uh released all theirgenerated queries so we were able todirectly compare regenerate sort of thesame using the same documents but usingonly the generation was done using blueand gptj I think and we were able tocompare against the quality and we couldsee that yes we we get results that arebetter thanGPT 3 resultsum so that was an important question andanswer the second question would we beable somehow get models that are muchsmallerum that was that was and again we gotscooped in part in on both of theseresearch questions by propagator paperor buying parse version two paper uh butI do think there is value to ourfindings as well and it's also nice tohave like independent confirmation of ofideasthat's for practitioners it's probablyvery useful as aas a signalyeah that I love that idea of opensourcing uh the synthetic data sets likethe um you know I love hug and facemodel Hub is they mentioned like theopen source generative models being ahuge part of this and um like opensourcing the generated data set the thedata set Hub papers with data also haslike the collection of data sets yeahI've been so interested in wanting toparticipate in this kind of thinkingaround the science like we are workingon like the beer data sets and so I'mplanning on you know putting these inGoogle buckets and then you can restoreweave it instances that have like thebm25 search the hnsw index already butthat like publishing of the like gbt3generated synthetic data set to expandlike NF Corpus or you know the beer datasystem this kind of idea I find it sointeresting so so in the podcast I liketo kind of like really transition topicsum can you tell me about like your sortof your origin story of how you came into be working on search and thenparticularly this non-metric spaceoh yeahumyeah so that that's a little bit thethis to my story is uh a little bitunusual if not to say it it's quiteunusual becauseum I'm not like your typical uh not yourtypical researcher and uh so I I wasworking in the industry for quite awhileum and then I I got my PhD basicallyvery recentlyum mid-careerbut one important reason why thishappened because the the country fromwhich I originated itit stopped to exist and all these signsthat that wasn't that country to isbasicallythere was no funding there was no wayyou can go and get uh doing research andalthough the fundamental education likeremained strong they're the only likeopportunities were like go and you knowwork on financial systems databases andthings like thatum but I got bored by this prettyquickly and um talking about how I canautomate this stuff like green creationand writing of SQL queries yeah thisthis stuff is pretty repetitive soprobably a lot of that can be automateduh but anyways so I got of course I wasinterested more like in research and atthe moment that was like basically lateuhit was like 30 no it was a little bitmore than 20 years ago the the only likeAI of whichum it was basic research and searchengines there was a big topic that we wehad Google that you knowum and that's how I got interested inretrieval algorithms and I was uh doingthat that stuff part-time and at somepoint like I talked with my familyuh that theyum and they basically supported mydecision to go get like a formal degreebecause unfortunately in our societyit's really hard if you don't come fromnowhere you're not being treatedseriously in yeah you will not just getlike good quarters yeah it's uh somethere are some stories of peoplegoing to I don't know some engineeringjob and then they emerge as researchersthere is one interesting case of uhofum the author of Chain of Thoughtprompting that's an interesting storythey're interesting but they're rarevery rare cases that happens sobasically yeah um and that's how I gotinterested in retrievalumnot so I I didn't plan to work onretrieval while in my PhD programbutI joined a labum whose Pi like Eric nyberg wasparticipating in that IBM Watson projectand for me that was like super flashyand super AI beating human Champions butafter like reading and learning aboutthe topic what we quickly realized thatoh IBM Watson system is actually is notis is pretty simpleit is basically a retrieval augmentedmodel what we have now retrievalaugmented NLP that's what it was theretrieval uh retrieval based QA systemand that's how I came back to work alittle bit more on the duo because Idecided that you can retrieve the youcan improve the the retrieval componentthat's that will retrieve the performimprove the performance of the wholesystem the okay systemand now answering your question a littlelong-winded explanation about how nometrics such came into place so the thenon-metric we have like inin the real world we have distance whichis euclidean distance and has a bunch ofnice properties and it has satisfiesthat actions of the metric space butthere are a bunch of similarity metricsso we don't normally call them distanceswho arewrong they not do not satisfy ourintuitions of the 3D world and they arenon-metric like they assigned similaritythe kale Divergence they're all but theycan still be useful because they're usedas asimilarity function to compare queriesand documents and so the idea wascoming from that Old uh featureengineering worldso you can compute that similarity youhave a data analyst that would computeuh that create this uh the featurescombine them and that would be somecombination that would not be nice andyou would need a special kind of systemthat would uh support retrieval usingthis similarity functionso sorry I'm still trying to wrap myhead around it so it's like a learnedfunction for the for the distancebecause that sounds a lot like the crossencoder to me uh I I know that soundslike really alien to people becausenowadaysin like recent years we made a hugeprogress in representation learning sobasically you can compute very goodrepresentations and compare them usingthe euclidean distanceor they can sign similarity betweennormalized representation is basicallythe euclidean distanceuh and that's all you need it's justeverything else like neural networkumjob to make this representationcomparable using a simple similaritymeasure but in the old days uh peoplewouldcreate multiple featuresand the idea was that ohso these multiple features there will becomplex function people werecontemplatingsome expensive similaritiessomething like Earth Mover distancewhich is really inefficient to computebut people like like there is a paperI think there that's using uh this earthmover distance to compute more thanbearings with some small gainsalthough Earth more resistance I thinkit's still metric but you want some somesystem that what it's it's a geogeneticmetric it's much harder to doretrievable with this generic metricbecause it's not euclidean so it's noteven like vector space distanceso there was that idea that all that'suh let's help analysts by buildingproviding them with tools that wouldsupport retrieval using thesecomplicated similarity measuresyeah so I remember studying like uhwasserstein Gan where they you use likethat Earth mover's distanceum do you mind explaining that from thebeginning what what the difference isbetween Earth movers distance and thisis like Optimal transport like what thedifference is between that and then justlike a KL Divergence between vectorsum I have to tell you that I don'tremember a very specific nuances of howEarth what was distances calculatedbutum as far as I rememberit is not it is not represented youcannot represented by something liketake Vector coordinates just likecompare them like in the euclideandistance you can't just you have a bunchof simple distances the euclideandistance the lp Matrixand the decay of the versions they'restill pretty simple because you justcompare uh pairwise you do pairwisecoordinate comparisons comparisons likeI quote unquote because you do someoperations like either you subtract themon square or I can kill the versionsit's a little bit more involved butbasically it is a pairwise operationswhich are often that you just cansimulate why are the inner productcomputation and then it's a simpleiteration in the vector spacebut in a more generic uh way you canconsider generic vectors not a vectorsby generic space that doesn't have anotion of thissimple vectorsor they are not compared so easilybut they still they would space inschool could be called something like ametric spaceandum it would have some actions inparticular metric space satisfies thetriangle inequality symmetryand you might be you might Envisionsearching in this space without usingvectors directly just distancecomputationsumyeah think about it like um I don't havelike a piece of paper on the desk butlike I crumble up a piece of paper andthat's kind of how I think about likewhat a manifold is like a discretetopological structure where you can'tyou can't let or you know my hand rightnow is a manifold like I can't dodistance from here to here because thisdoesn't exist sort of a sort of likehave been thinking about that kind ofthing but uh yeah maybe I know it'sgetting a little distract a little offtopic but do you do you have an interestin things like category Theory and thesekind of topicsum not really familiar but I think thethe manifold example is interesting inthatum it's a good illustration that you canreason about this space in abstract wayslike you have some certain restrictionsyou can go from here to here but notfrom here from another point but at thesame time you don't reason in terms ofexplicitly in terms of coordinatesand that makes a huge difference interms of the search function becauseyour search function is going to berestricted the you have less informationabout the spaceand that's that's going to be bigrestrictions but what's interestingum that there wasn't that that uhsequence not sequence of uh there isthat uha class of algorithms that rely onbuilding neighborhood graphswhich surprisingly uh people realizedgradually that those algorithms are aresufficiently genericto provide a retrieval in in the casewhere you don't have those explicitVector space coordinates and not onlythat they can oftentimes beat those uhalgorithms that I designed uhspecifically for this knowing likehaving this coordinate system and accessto individual coordinatesand they can be so we have thatum yeah and once once I realized that Icould use this kind of algorithms and II I saw the research opportunity whichended uh in the that was a dead end to alarge degree unfortunately but it may beuseful in the future and we also helpeduh helped promote these approaches whichwere also super useful foruh distances like the euclidean distanceas well as for inner product search andcosine similarity basesyeah it's amazing I I think the wholelike graph representation learning whichis usually like let's try to get thegraph into a continuous space let's tryto get the graph into a euclidean spaceso we can like do all these things I Ialso find it so fascinating the the hswlike the the graph index that's pairedwith the way that you organize theeuclidean space distance measures do youthink uh yeah I I don't have likeanything super conquer I'm just kind oflike thinking out loud with this butyeah so interesting so can we also talkabout um so you developed this the flexNewark Library you've built the hsw liband uh nmslib uh can you tell me aboutyour experience developing a library toorganize your experiments andcommunicate your research this wayum yeah absolutely well first of all I'mwasn't like the only the the personworking on this and thenum the that with respect to that graphbased retrieval algorithms I I helped topromote it I was Icreated the first optimized version ofthat thatum to some degree was was reused butthen we of course take our mostefficient implementation was contributedby Yuri malkov uh but more generallyum that project was uh in terms oforganization of the the the the researchit's actually quite interesting thatthis project started as the PHD projectofum another person Billy naidan who alsocontributed quite a lot to itumand at some point like everybody's usingthe python version python biting hecreated the first python bitings youcan't it wouldn't have been possible todo anything useful without pythonbindingsum yeah but that that started as his PhDproject and we had no uh graphalgorithms at that point uh these werehis interest wasn't this retrieval andgeneric metric spaces so we did a littlebit of of work on this topic and it wasbasically two of usum and then we had a couple more peoplejoining our the project so it was likeyou know like sort of random alignmentof people on the interests and uhum and their contributions and I waskind of holding that um I'm a sleepproject togetherso uh doing actually a lot of uh thesoftware engineering job the uh some ofthe running some of the experiments onthe published uh a couple of papers ofthat non-metric searchand also maintaining it and it'sum yeah doing things like that basicallyeven like encouraging people to tocontribute although it wasn't a bigproject it was a small collaboration itwas a little bit it is actually a littlebit different with a flexnard becauseflex No Art isum the retrieval toolkit that uh isbasically separate from animal sleep andit's just incorporates all the the goodstuff like bm25 and Keenan and searchand that was solely at some point it waslike 100 my project and what I'm tryingto do I'm I work with people andwhatever we do jointly for theinformation tool we trying toum reuse this library and I also like Iit it actually started it it started asfrom it comes from my PhD this is so Iused itand I you know it's like I improved alittle bit and a little bit and then alittle bit more and thenum it becomes more useful with timeyeah it's really interesting and I Iwould think that like waviate could beused for a lot of this IR research aswell where you could you know you havethe you can build off the implementationof hsw product quantization like as disca n is developed and these kind ofthings and I guess like my thinking islike the library it adds value andsometimes they have like the data setsare built in so like some of these IRdata sets is really easy to load thedata because you don't have to clean itit's already clean and sort of a part ofthe library as well as these indexingalgorithms and sort of like the wholeranking flowyes I guess like um what do you thinkare the this might be too open-ended ofa question but like what are the mostimportant things for an informationretrieval research library to contributeto the science to you know be usedoh that that's uh that's that's a greatquestion and that's a very open-endedone I don't have definitive answers sothe uh I think the animal sleepdefinitely uh was useful and that's kindof funny because I I don't have a keycontribution that library and it's stilluh wouldn't have existed without me orlike the league and thenum one thing that it should be useful insome ways it should uh there should besome missing elements like an animalsleep we had methods that uh before usnobody was or like few peopleuh seriously considered them asumas you know something to be usedum that's like one missing missing pieceof science it'sanother thing is the ease of use and Ithink say within Amazon sleep we um wewere able to largely to a large degreeachieve this so first the Billy createdthis Python bitings and then I got helpfrom another person to make it like youknow deployment easy and installation onumcomputers that you don't have to compileyou'll get some pre-compiled binarydefinitely ease of use you just doPipeline and also on a bunch ofoperating systems ease of use definitelyit should be it should should be no painfor people to installum the it was great point aboutdownloading data sets and I personallyunderestimated this part for a while andflexnorth doesn't have this ability butI see what pi syrini is doing and Idefinitely want to appeal for that forthem from them I don't want to support alot of data setsum because it's like you know it'sdifficult but like basically I say Iwant to convert beer data sets and MsMarco and store it in the cloud sopeople when they use flex not and saybm45 indexes from flexnor they can justbasically run a command and then it willdownload things as wellso what else instead of like years ofuse it should it shouldn't it should beyeah somehow uh it should have somethere should be some missing is somemissing piece of science or technologythatthat people need and and last but notleast I think I should have said thatmaybe earlierum promotion makes a huge differenceyeah so people should know you shouldjustwriting something beautiful and puttingit out on GitHub nobody knows uhsometimes Word of Mouth works but youknow you compete with Google andFacebook and the others whenever likeyou get good results from someleaderboards you publish papers youthen peopleyou know get people interested so that'sI think an important component tooyeah super interesting I think to buildon the data sets part you've done somework on Long document uh benchmarks andI I think this is a very interestingtopic that hasn't really been coveredlike if we're gonna rank you know ifwe're going to search through entirescientific papers sort of can you sortof just like set the stage of of thedistinction and long document and sortof how this hasn't been studied as muchas you know Ms Marco natural questionslike that whole tour of the beer datasets yeah so that'sum that's that's a great question that'sactually one of the uh importantlimitations of the Transformer modelswhich are currently I feel is beingremoved now at least like mitigated butoriginally the the way the birth wascreated it had to um it still has if youuse that original birth model it has alimit on the number of input tokens itcould process and it's only 512. andit's not a whole lot right so and if youhave a long document you need toclassify it or you have to do somethingand there is a bunch of algorithms thatare doing some sort of clever chunkingaggregation of results andum that's uh that was something toBenchmark andum compare for the paperum that said I feel like the communityhas made a really great progress onremoving that limitationand with approaches like flash attentionI think we'll soon have uhTransformer models that will supportvery long inputs up to a few thousand afew thousand more than 10 000 tokingsum it's I think it's a matter of timebut it should happenyeah I think there's kind of like ummaybeum yeah so that that idea of like flashattention I've seen like staircaseattention I I went to neurops this yearand I saw a lot of the posters are likehere's our sparse Attention our take onSports attention right it's like thislinear attention kind of technique andand I certainly agree like things likethe Sarah Bros chip as being one likebig chips that get that like you put inmassive inputs to the Transformers thatI find that whole thing to be incrediblyinteresting but maybe if I could pitchyou this other idea on what we'rethinking with uh with a long documentrepresentation where like our so ourweva data model is like classes and thenlike references to other classes so wethink about it like article has passagepassage and so like article has passagepassage passage passage passage and thenum and so one way would be to representthe whole article you would have thevectors of each chunk and then you wouldaverage the vectors from the chunk torepresent the article another idea wouldbe maybe we like cluster the uh passagesand then we have like three centroidsthat represent the article and then anewer idea with chat gbt expensive butit could it's an idea is that you woulduh you would you would say like pleasesummarize these passages so like you yougo one by one through the passages andthen update the summary and then fromthat represents the article so what doyou think about that kind of like graphstructured way of representing likeabstract long document object with thesechunksoh yeah um there is definitely a bunchof way how to uh chunk data andaggregate it and represent in the thisphase I am I have to tell you that Ihave to I have to give a disclaimer thatI'm not particularlya knowledgeable person in in for thiskind of work uh in yeah basically I didwork in the domain muchuh but it's my yeah you can dointeresting things you can do embeddingsand then you can do something probablylike graph neural network based on topof that again how well it works I can'ttell uh butum I I can tell you that there aredefinitely efficiency issues becausewhat we see a lot of times that uhthings that work best are trained end toend meaning that like you you say youhave a graph you have like embedding foreach node you embed it and basicallyyour neural you feed a large chunk orlike the whole graph sort of into youlike the whole documentor the graph of the document into thatneural networkand then embedding the being althoughembeddings might be somewhat to becomputed warmed up but in the end youwant to run this end-to-end computegradients propagate them all the waythrough the network because if you don'tdo this your your representations youdon't know how good they are right likecoming back to that example to birdwhich is Trend in self-supervisedfashion but whose representations arenot good enough for retrieval so it canbe something like that and then you needand then fine-tune the whole uh thingagain and that's expensiveso the the modern accelerators theydon't have enough memory for longdocuments so that's that's onelimitations that I would I would thinkof and then you can be clever andsomehow split um but how well it works Icannot comment because I don't havefirst-hand experienceyeah that's a that's a such aninteresting topic because the graphneural networks to me like the bigappeal of it is how well it can handleuh like variable size inputs so like onearticle might have five passages thenext has like 30 and then the graphneural network I think like theend-to-end thingit's very like in the graph neuralnetwork sense to stay on that I think itwould be like you update the originallike you send gradients all the way backto the input would be maybe how youwould try to do it end to end and youget like new embeddings that way sort ofthat are synced up with like the contextof the graph but yeah but but then youlike I find it very hard to believe thatthere will be like zero shot graphneural networks that that like representAggregates of embeddings for longdocument representations like I thinkyou'd need to train this on yourparticular uh thingoh I I totally I totally agree with youthat weum the there is a relatively poortransferability of neural networks tonew domain and tasks and although uhopenai made some great progress on insome domains like with a clip modeluh but I think it's still you knowbecause we uhoftentimes it's still like reallydifficult in many practical situationsto do this transformation although therewill be some progress there was it'sit's an interesting topic and it's acomplicated one and I again I have toI can only I I should probably repeatthe disclaimer that I'mum that's that's not that's not my youknow uh it's not my primary expertisethis more structured objects regretfullyso I cannot tell you what works and whatwhat not what doesn't work in this uhspaceyeah I think it's a pretty Cutting Edgetopic from my understanding I know thereare definitely some graph neuralnetworks uh experts out there in theworld but um I think this is a good likenot leaving the graph neural networkthing in the past talking about the umthe in-domain out of domain so you knowwe're kind of talking about like thisidea that you if you want to have agraph neural network that can representlike your cross-reference schema youwould need to fine tune it in domain onyour problem and one of the bigarguments around out of domain in domainone of the key things to be mindful ofis robustness this topic of like robustgeneralization the out of domain modelsbeing really good at being robustrelative to in domain like there's thisgreat paper called wiseft where theytake the clip model off the shelf that'strained on something like Leon I don'tthink it's I don't think the clip dataset is open source as far as I'm awareuh and they fine tune it for particularimage Text data sets and they show thatlike you know like the out of domainmodel is more robust so uh lee I knowyou've done some work on awesomeness canyou sort of explain what robustnessrobust generalization isoh yeah sure so the uh well first of allwhen we talk about robustness we uhtypically separate uh it into naturalrobustness in adversarial robustness andwhat I worked more on was uh actuallyadversera Boston's although we did somework on the umnational uh natural generalization ofnatural robustness for the informationtool too but basically the the ID is isvery simple and I think you explain itit really well those very good examplesso you take it something that is trainedfor one type of data we call it domainand apply it to another type of datawhich is another domain and things don'twork as well and we call it distributionshift although there is no realdistribution and some people like ohlike what's distribution here but it's ait's a common terminology we somehowenvisioned a probabilistic datageneration process that generated datain one domain generated data in anotherdomain and in fact there may be somebecause how is sample is is a generativeprocess and then there is some somemismatch between properties and becauseour algorithms are statistical so ifthere is difference in statisticsbetween these domains it's sort ofinevitable that you would um you wouldget thisum differencesand with adversarial or business wewanna it's actually really I think it'sprobably going to beumit's not it's going to be a little biton off topic uh because with the reserverobustness we assume that a malicioususers can modify inputs in a way thatwould completelychange model prediction in a way thatyou want it it's a it's a very hugeproblem in in like academicallyand it's not there are no easy ways tosolve it apparently but I think it'sactually doesn't um it's not clear if ithas a big um practical impact becauseit's difficult to attack these models inpractice it's easy in the digital domainbut you never know the model you youcan't usually control the inputs at thesay pixel level or so it's difficultum and with text you don't have accessto gradients because that'sdiscrete discrete thing but I think thenatural robustness is is a much moreimportant topic for everybodyohthat's your robustness of thedistribution shift is something that weshould really care about at least in thenear futureawesome sorry there's a bit of a lag inthe connection but it's all backtogether yeah awesome yeah I I like thisexample of like it's like they have theself-driving car and it's like oh no anadversarial optimized stop sign likethat kind of like you look at like thethe brain of the network to optimizewith gradients some kind ofimperceptible thing yeah I also don't amnot super interested in that but I'minterested in the general thing of likeif uh like say you want to test therobustness of your Tech search system byuh generating paraphrases of the queriesand seeing if the paraphrase of thequery returns like far different searchresults than the original query or likethe image analog would be you have likean image and you rotate it or youhorizontally flip it increase thebrightness these kind of augmentationsand that produces like totally differentsearch results so how do you seerobustness uh impacting searchohum that's yeah that's definitely greatquestion and the I do see robustness uhbeing a huge topic for neural networksbecause uh it will frustrate users whodo not understand why exactly why theychange the inputs a little bit and thatwould break systems and I and I'm prettysure that all those systems arebreakable there is no easy solution whatuh that I am aware of and I think tothings that workuh somewhat reliablyin as was demonstrated it's it's anotheryou know talking about there is thatconcept of Peter peel and machinelearning that no matter how smart we arejust bigger models and and more datahelps us so I think in terms ofrobustness it's probablyum we need again the the Practical sortof the the only practical uh remediesthat we have now are train more genericmodels on more data that's what open AIdoing and train models that are biggerand of course both Solutions havepractical limitations too soyeah like one idea I'm sort ofinterested in like is um the bm25 vectorsearch Hybrid search is that more robustnow because I think of like the sort ofsymbolic lexical search being morerobust and then so like on that layerand running some experiments to try tolike as I'm still getting the beer datasets and we've almost finished and thenI'll be able to do these kind ofexperiments and then also I think withthe ranking layer like um maybe you canhave like you you use the same rankFusion you use in hybrid search to havesome kind of symbolic like clicks orsomething like that where you where youhave a sort as well as the you knowneural re-ranking in the new rank fusethose to maybe improve the robustness onsort of those two layersoh yeah absolutely so umso yeah so again uh I perhaps I shouldhave mentioned that that some sort ofensembling usually improves robustness 2and that's your classic result thatEnsemble can produce both variants andbiasuh in particular variants so someonesampling in particular like diverseapproaches using diverse approaches canwork well uh although the informationthat's again the example shows howunique the information in triple domainis so look we are like it's 2023 neuralnetwork Revolution after imagenet it's adecade in Inspirations he will we stilluse build 45 because we we have hybridsystems now we still need Bill 45. okayso that's difficult uh definitely tellsabout the uh the difficulty of theretrieval domain andso one problemum uh like providing a kind of morespecific answer to your question soFusion is definitely one way to go uhthen umthere is a problem with fusion thoughthe you you get bm35 and get resultsfrom build 45 you get results from across encoder or buy encoder and you getscores of course and their scores arenot comparable so how do you fuse themthat's yeah so like you can do thatround robin right thenum but the problem with round robin isthat if you have systems that are verydifferent in terms of the effectivenessyou may end up like injecting very badbm25 results into very good uh and theother way aroundum yeah uh but there are also otherSolutions uh probably we can come upwith the models that are somewhat morerobust in particular cross encoders aremore robust than buy encoders so youcan't to some degree if you're rank withthe cross encoder that's actually oneadvantage that I could have uh recalledwhen I asked me why I use cross encodersuh Crossing quarters are somehow morerobust likeum uh yeah so this that's what takuraand andquarters uh founded a beer paperthat's actually great and maybe thereare other models that areusing maybe lexicalClues and other signals kind of combineit together maybe more robustthan just when you look awesome colorstoumyeah I have to pick your brain a littlemore on the um on the rank fusion partbecause I've had so many conversationsabout this score based Fusion versusrank based Fusion that I'd really loveto get your take on this as well um soyeah so if you so like if alpha alpha isthe parameter that determines how youcombine the ranked list of bm25 andVector search and then like 0.5 meansequal contribution uh so so with tuningthe alpha parameter if we tune the alphaparameter of like say on beer like we'regoing here it is from zero to one pointone point two like is is that badmachine learning because you know you'rekind of you're kind of tuning a hyperparameter on the test setmaybe you could you start with yourthoughts on that idea oh it's a littlebit change of topics so there's maybe uhuh I I want to make it like um sorry Iwant to make such the stage for thisbecause I think we jumped to uh quicklyso as we talked before about differentsystems let's consider two just toproducing different kind of scores thatare incomparable between 25 and saycross encoder and one one approach tocombine those just do like Crown Robinuh like for one like basically mix themin the order of uharriving but another approach is tocompute the linear combination of thiscourse and it can be potentially betteruh but there are two issues one issue isthat you may lose some of the Beyond 25results of the good because they will bepushed down the total score is not goingto be but maybe it's not such a bigproblem but uh another problem is thatyou need to have that Alpha coefficientuh the fusion and how do we choose itwell ideally we choose that coefficienton the umtraining set but we have a beer data setwhich is does not does not have a properdevelopment set and people want to uhwant to claim say uhhigh scores on beer because thatattracts like basically as anadvertisement of the system and theirapproaches so if you are like I'll gonnabe very straightforward if you just finetune Alpha on thatfull beer data setum it's it's gonna be an upper Bound foryour system performance but you can'tclaim that it is achievablelike it may or may not be because youmay just overfeed it's not such a hugedata set right it may be maybe maybeyou're just overfitting if you you forexample uh you at some point you decideto double the number of entries in thebeer collectionimage data set imagine like we somehowfind a more track covet documents andqueries and expand it and it will stopworking it just don't know so it's notlike a good way to say this in generalis what I think would be fair to do isto do some sort of cross validation forexample uh divide each test set intofivecompute that Alpha on thatand then test on the remaining parts andthen it would uh would tell you like ohlike this is going to be moreum robust more fair estimate of how youcan perform on the data set in a fewshort settings say or something likethatyeah it's so interesting I I also hadthat like let's see what the upper boundwould be on a conditional Alpha wherefor each uh query we go through thewhole list and then see which oneperformed best and then like as a quicklike kind of hand wavy thing like on theNF Corpus I was seeing hits at one gofrom like 1400 to 1800 out of 3200 doingthat just is like a quick sense of whatyou can get from that but I that otheridea of like you've trained some modelor or a linear combination or someoptimization of the bm25 score and thevector search score or the cross encoderscore uh and then as you mentioned butthen you those scores could shift Iguess you normalize them with like it'slike the max BM 25 score over theminimum BM 25 score you know do thatkind of thing um yeah I think it's veryinteresting because like if you had likea like a query classifier that liketakes this query and classifies how toweight the bm25 and the neuro like howto weight the lexical neural methodsyeah I find that whole thing to be veryinteresting even though I don'tuh yeah it's it's I because I usuallythink of the Cross encoder as like astage one stage two thing where whateverthe cross encoder says is the output isthe final right list where yeahyeah so it's really interesting thiskind of high research thing and I thinkall these yeah it's such an interestingtopic um so anyways uh I think it's agreat coverage of topics that Leo do youhave anything else you want to maybe addI just want to quickly follow up on yourumon New York uhor the problems that you describedbecause that's something is indeed is aproblem with fusing this course yeah wecan compute the linear combination ofthis course but other like these scoresactually properly normalized there is apotentially huge variability acrossqueries we just don't know in in in inpossibly we don't want to use the thelinear model you want to use somethinglike gbrt right boosted regression treesand in fact that's uh one way to go youneed a lot of data for this uh that saidin practice for like small domains likeMs Marco I played a lot of this when Iespecially uh when I was active on thatleaderboard in theearly stages when I was uh and I traineda really strong uhPuri like classic IR model for this andI play for the MS Marco documentleaderboard and I played a lot with likefusing different scores from differentcomponents and I had two options to useone is linear fusion and another usingthis non-linear GB model and uh and Ionly in one case I was able to get uhsomewhatum noticeableperformance boost on top of the linearmodel so yeah so linear works wellum although with the caveat that that'swithin a single domainand once you go out of domain you're notonlythis coefficient maybe make efficientmaybe become like known invalid notappropriate and the audio scores maychange like a normalization confusionsmay need to change for another domainit's it's reallyum I I think robustness is going to belike really huge topic for the comingyears yes more data and moremore everything will help but it alsocomes with the efficiency cost andeventually would want to run thesemodels maybe only even on your phoneyeah so it's uh it would have to besuper efficiencyyeah yeah it inspires a lot of thoughton like if you have this yeah it's likea very tuned system for this domain ofdata uh because you not only have theembeddings that come from maybe aparticular like if that's if youfine-tune your embedding model on yourdomain and then you have the rank Fusionis a fine-tuned parameter for your datathe cross encoder maybe also isfine-tuned for your data and then thatdomain shift maybe like don't like uhdistribution shift detection I I'm notlike super caught up with those methodsbut maybe that kind of thing could helpuh like you know like maybe the idea ofyou in the vector space you cluster itin like hdb scan can like do a good jobof telling you like this is an outlierpoint in your vector space kind ofyou know that kind of thinking uh yeahjustoh yeahum uh that's that's Again The Good TheGood the good reminder that the there isalso the difference between more likekind of academic style ofumthe machine learning where you justcompute an average score and be done andthe data scientists who work more like areal problem they care much more aboutoutliers they care much more about thewhat sort of the what's in the data andwhat's like specific results for thespecific they dive much deeper and theycare about like the the whole Spectrumyeah but anyways so I I think it's adifficult problem uh I'm I I can't Idon't have aunfortunately I have to admit that mycrystal ball is not good enough I made alot of mistakes about what things aregoing to be in the future so that's whyI don't want to even make any likepredictions maybe the scale will do agood job for us although we will have tobe efficient uh although it may not domuch more beyond what we umwe see already but it's hard to predictbecause it all those things there is noTheory it's just like your intuition itcan be wrong mind was wrong many timesso I don't want to make this predictionabout things that are not uh clearbut I'm pretty sure that it's becomeit's not even like it's become veryclear that the the Deep learning the thelanguage modeling the generative modelsit's there is no slowdown there is nolike winter per se some may be goingdown in terms of like I don't knowfinances some layoffs no but it's notlike a real winter some a little chillbut that's a huge topic it's probablythe area to be in the only thing that Idon't want to be really narrow IRresearcher I think it's probablyum worthtrying to be kind of more generic andkeep an eye on several sub problemseveral sub-domains so that's that'sprobablyum what I I could recommend personallyasas a researcher and engineeryeah I think that's really great adviceas well again I think it can be kind ofoverwhelming trying to keep a tab onlike all the all the little subtopicsoh yeah uh somebody uh somebody use likeis quite like really really great uhterm for this it's uhit's breakthrough fatiguenothing less that breakthrough fatigueso in the old days like IBM where theychess playing machine IBM with the youknow beating human champions in Jeopardyand that happened with like you knowfive to ten years yeah and now like webeat humans and go we beat humans inthis and then like our pH systems aresuper human kinda and the chat GPT isdoing so much stuff oh it can't do mathyet but I don't know maybe like tomorrowit will be able to do math too so andhow soon will it happen you never knowand those things that not only just likegreat papers but all those uh superhumanclaims of superhuman performance theirdensity in time is just like really uhyeah you can get a fatigue from justfollowing all that stuff let aloneworking on on any problems and trying toimprove things yeah yeah I've had someinteresting time definitely interestingtime to be an AI and deep learningyeah for sure yeah I I feel theexcitement of like I think theBreakthrough fatigue is very interestingbecause I also like was like with thestable diffusion thing I don't know whatit was about me but I kind of just likewas like all right yeah it's cool butbut this new Chat gbt thing to me thisbreakthrough is like I love it it's sofun like it's so much fun to play withityeah it's definitely nothing like we sawbeforeum it's uh it's absolutely amazing inmany waysyeah awesome well Leo thank you so muchfor joining the wevia podcast I thinkthis was such a collection ofinformation and a tour from the you knowthe cross encoders the re-ranking thefusion the uh you know the non-metricspaces in Paris light of course and II'm so excited to play around with inParis light I think that generating datafor the custom Training of people'sparticular models even just benchmarkingon these data sets and facilitating withthat I think that idea is just massiveso thank you so much oh thank thank youoh thank you very much for inviting me Ireally enjoyed our conversation", "type": "Video", "name": "Leo Boytsov on Information Retrieval Science - Weaviate Podcast #38", "path": "", "link": "https://www.youtube.com/watch?v=X-VFkq6fHDY", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}