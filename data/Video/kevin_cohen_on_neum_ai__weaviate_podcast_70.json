{"text": "Hey everyone! Thank you so much for watching the 70th episode of the Weaviate podcast with Neum AI CTO and Co-Founder ... \nhey everyone before diving into the podcast I want to highlight this incredible article that new mayi has published that'll be linked in the description of this video the blog post is titled retrieval augmented generation at scale building a distributed system for synchronizing and ingesting billions of text embeddings this is a really great article going through all things data ingestion and particularly building this distributed messaging system for different parts of this process from connecting to different data sources to chunking and embedding it's a super interesting system and I learned so much from talking with Kevin so quickly I want to highlight uh this image particularly because about uh 16 minutes 16 and 1 12 minutes in the podcast we're going to reference uh this figure two in the article this particular diagram about uh how this messaging queue is structured with uh doing different things like the chunking and how you kind of like isolate each PDF say so overall this is such a nice article there are so many insights about U things like this celery task CU these uh distributed import benchmarks and uh looking at at things like the number of threads per machine and just all sorts of interesting insights so I just I highly highly highly recommend checking out this article and thank you so much for watching this podcast where we'll discuss the things in the article but yeah I just wanted to make sure it's known that this is linked in the description and it is just a fantastic article hey everyone thank you so much for watching another episode of the weeva podcast I'm super excited to welcome Kevin Cohen the CTO and co-founder of n AI there's so much exciting things happening in the space of say ETL for l M or data ingestion into Vector databases this whole flow of uh reading data connecting it to different sources like notion geub uh you know S3 buckets all these things then chunking metadata extraction EMB bedding so this is such an exciting space and I'm so excited to interview Kevin Kevin welcome to the podcast thank you Connor Thank you super excited to have you to to have me here and and honestly you bring this excitement that it uh it motivates everyone here that's that's talking to to really get excited as well so awesome so I think kind of staying on the exciting story can we get into this uh you know what motivated you to start new Ai and how has the journey been so far yeah for sure uh this is a this is a quite a long journey but I I'll summarize it uh me and my co-founder uh we're both ex Microsoft and uh we uh we applied to Y combinator uh a couple months ago and we got accepted with uh this idea that we had about building a restaurant but uh and it's completely different to what we're doing with neom but uh one the insights that we had what we were doing this restaurant bot uh playing with GPT and llms and and embeddings and all of these things is uh the the the notion of syncing the data from like Source into a some sort of vector storage where you have the embedding so that you could do retrieval augmented generation and all of these applications because what we were doing at the very beginning was sort of like very naive in which if uh the source data would change we would not reflect that into our um embeddings and Vector storage and so if you would ask a question to your to your restaurant bot like do you have this item on the menu the bot would answer yes but in fact you didn't have this because you know let's say that the context had changed from the menu and so we we got this insight into hey maybe we should actually focus on the tech and building a platform that um allows developers and people to actually make these applications easier because we understand what's going on with the data syncing the data pipelines the infrastructure so that dat needs to happen specifically for large scale and that's what we decided to you know focus on and and and the birth of NE just focusing on the on the uh platform side of things yeah it's amazing I I definitely want to come back to kind of the experience at YC combinator and I've seen a lot of AI companies and why comary we'll come back to that but I want to stay on the the restaurant example uh keeping the source of data in sync with the vector database I'm very curious in further understanding that because I understand you know we8 to be like a you know it I guess there like in databases there's like olap oltp like transactional processing versus analytical queries so I kind of understand uh the vector database to be sort of like a transactional processing thing like your restaurant perhaps would be doing crud updates directly to WEA versus say it has a notion template that it needs to keep in sync with WEA could maybe expand on that point a yeah yeah yeah yeah yeah so um the main thing with restaurants is that they don't want to uh get like new technology or new things on top of what they already have right so the restaurant really uh is based on like the point of sales system that they have and so there's tons of different point of SES systems but that that that's pretty much what what dictates how the data is stored in their systems and their databases and whatnot so if you're building an application on top of that for whatever needs you are you probably want to connect to the point of sale so that you can get the most upto-date information and so what we were doing naively as I was telling you is that we were basically getting the um connection to a point of sale uh let's say square right and we would be uh dumping whatever menu items they had one time and we would be chunking embedding uh and and and putting it into into a vector database and we would do you know your general rag application where you would ask hey do you have this item for a menu and things would work but as you can tell if the point of sale system would get updated because let's say they added a new item or they removed an item or they change the price or whatever it is then your your your your vector database or or or the place where your application is actually pulling the data from would need to have that context would need to know that something changed because again the source of Truth is the point of sale it's not a notion document is this either complicated system or easy to use like depending on the point of self that you use that that the restaurants have access to and so you want to make sure that the source and the destination which is what we like to call it are in sync so that if the your application is is is is is going and doing semantic search for example to a specific Vector database like we then that that information needs to be sync so that the that so that the the bot can respond accurately so that's what that that's pretty much the Insight that we had um because we naively just started with a simple basic idea and we figured well I mean this is definitely a problem is something like this changes every time yeah you've definitely sold me on the idea I think like one of the arguments around the development of we8 has been to have many different programming language clients so we has like you know addition to like Python and JavaScript there's also like Java and goang and we had Andre on the podcast who talked about like uh Ruby client for we8 and so I think that's one kind of argument on this like connect it with how restaurants or government database systems integrate with these things but I like what you're saying a lot where you know you have a square data like date Square operates like a database and then you sync it with the vector database and I can definitely see that argument so yeah so I think kind of Dove right into the data connectors topic our listeners we're doing this end to end of data ingestion and you I do think data connectors though is just one of the most exciting topics because I think about you know I I love thinking about rag for my personal note taking because selfishly I want to be you know leveraging these Technologies and I feel like just this idea of like connecting my notion with say the we8 GitHub issues with the weeva blogs with my Twitter feed this kind of like syncing of the sources sources it sounds so powerful can you tell me about just like how you see the opportunity there yeah for sure man I mean and and and look this was just the restaurant example that really sparked our interest there but um where we really want to go is um what other applications can actually be powered by this and and how are Enterprises even think about even thinking about this uh this type of problem and there's there's bunch of people doing rag as you probably know this uh and and really one of the key main things that we want to dive into is what are sort of sort of the the the business requirements that are happening at this level of synchronization because um the restaurant idea is actually true and it's actually a problem that we faced but um we want to try and also understand which is another topic that I haven't touched but we will touch later on the large scale sort of thing if you have millions and millions and millions of of records of data or unstructured data like your notion files although I doubt that you have millions of notion pages but uh it could be um so how do you actually go and and and chunk those efficiently and extract those and embed those and and we we'll talk about embedding scash and synchronize those and so really going into what are these large scale Enterprises um uh business requirements that will'll go and see more into in in how business are adopting Rag and and more of the Gen applications that's where we're trying to really focus on and and dive very deep yeah that is super sold it all makes a ton of sense and so I think okay so I think kind of this data connection argument is is pretty solid I don't think anyone would disagree with the importance for this so I think kind of moving then into our kind of endtoend data ingestion topic The Next Step would be this kind of like chunking and metadata extraction and I maybe also wanted to you know quickly get your thoughts on chunking in metadata extraction because I want to like I think kind of well sorry I think kind of connecting this with the connectors thing is very interesting because I imagine like my notion Pages for example the reason I like using notion personally is I love having this like recursive how I can like create a page within a page kind of so having the the metadata of like the title of the page I imagine with PDFs you the metadata is like that you know if your PDF has like a scientific paper it has like a title and so you trying to look for those kind of things I think it's such a deep topic so at the maybe we'll also come back to this at the end of the podcast it's such a deep thought let let me just give you like the the the two sentences that I have on this is that one of the things me and my co-founder have been thinking about is the whole topic of like extraction and chunking and smart pre-processing that I think uh you've also heard with with other people that you brought in into the podcast to me that is still a research topic like I don't think anybody has a chunking method that works for everything that doesn't exist because as you said like scientific paper chunking is way different than how you chunk Json or how you chunk financial documents or how you chunk any different things have it's very um requirement and business specific and so there's a bunch of things that that have been tried um uh in the industry and we've been thinking about some of these things simple things like hey we can allow the user to specify what metadata they want because they know their context or we will do a basic recursive chunking and and and basic metadata extraction based on some fields that you were saying like title page number whatever okay but then you can go and and I'm sure you've heard about this as well about the smart chunk and smart metadata extraction where you can maybe leverage llms or even on structure which I know that they they came to your podcast right and and how can um you correctly or smartly assign metadata and chunk based on the type of data that you're having and so we've also done a little bit of that over at neom but it's it's it's definitely such a large uh piece and and important piece as well but it it it's still in its very early stages in terms of like to me it's like a research uh topic so yeah I think it's so fascinating like you know learning from about like n and unstructured and seeing how this kind of Market is developing I could almost imagine these categories within data ingestion evolving further because the smart chunking thing makes me laugh because we were joking about this at the AI conference and Eddie and's like we should do a blog post on Smart chunking and I'm like well it definitely sounds better than dumb chunking like add smart to it I I don't know what we've all converged in industry to like smart chunking if you go to Twitter everyone's like oh yeah smart chunker it's like okay what do that what what did we have before dumb chunker that's upgrade yeah that's fantastic and I think kind of like there's like Nils ryers who's been one of the biggest like early pioneers of AI powered searches been explaining this concept of like multi- discourse how if you have a paragraph that has multiple topics like you know a lot of the times is I've been one of my favorite apps is um is searching through this podcast and something I've noticed a lot is when you're talking for a while you tend to change the topic in the middle of the talking and so that will cause a really funky embedding so I think that's one of the best opportunities for chunking is to extract that kind of like this is separated the topic and yeah exactly and so so yeah I think metadata is such a deep topic I may want to actually save that for later let's now d uh sorry dive into how you're thinking about uh embedding inference management yeah yeah that is this is another big piece and and maybe at the at at the end we'll uh um we we'll talk a little bit about like how how the whole end to endend uh Works in terms of like the whole infrastructure and and uh and talk a little bit about that but embeddings is definitely a huge piece here of course you want to do semantic search you want to store the data in the vector DB you want to have the embeddings but there's so many things that come into picture so stuff like which model you want to use you want to use an open source model you want to host it yourself you want to host it in replicate you want to host it in pH you want to just use open AI all of these things come into play um depending on the model that you choose you have different dimensions in your vector and so people at the very beginning might just go with oh let's just go with openi it's very easy to use and yeah you can use that openi has 1536 Dimensions with their Ada Z2 that's fine but when you're actually dealing with a scale and you want to insert all of this information in the vector database then the dimensions that you choose for your embedding models is crucial it can save actually huge costs if you don't use a model that has a lot of dimensions of course if you have less dimensions then maybe your accuracy is is is um not as great as the one that you have more Dimensions so as in software engineering it's all a trade-off but these are all the things that you need to really take into consideration when when you want to you know vectorize and and maintaining this this source and this destination in in sync and the other very important thing for embeddings is that um you know if if you're building like proof of concept of applications or like hobbyist applications where you know you have a couple of documents and you want to extract and you want to embed and you want to store it's virtually very easy but if you have a large scale data and you want to make sure that you embed and vectorize that and you put it into the destination you don't want to re-embed everything if you already embedded it what I mean by that is if you have you know gigabytes of data um that you want to put into the into into your you you want to vectorize you want to put into your vector database and you want to run that let's say every couple hours or every every whatever Cadence you want um you don't want to revalorize the things that didn't change so you need to have some sort of like embedding scash and this has again you know uh Lama index and L chain they've talked about these things as well but how do you properly determine whether something changed so that then you can figure out if you need to do the um calculation and use the compute of your model for for open AI or whatever you use and then and then store in Vector because chances are that if the data did not change you did you do not need to to to waste those compute resources or and that eventually URS towards dollars right and so you need to you need to have this concept of well I want to embed and I I need to put it into the vector DB but I need to make sure that I don't do the I don't do it blindly uh you do it with with with um conscience that okay um this thing's changed now I need to re-embed then I'm going to insert it insert it and so that's the whole piece of like the embeddings management and the embeddings cash that we've also played around with so um we can essentially you know save the costs to to the end user who wants to move the data yeah so I I want I think first i' say I think this is another category of that kind of um you know ingestion keeping the source and the destination in sync I love how you expl this kind of motivation it's quite novel and yeah I've seen this before like with um people who are doing like code search on their GitHub and when they do a new poll request they're saying hey should I rev vectorize now this function and that's probably one of the big opportunities is like LMS for code putting your big code base into a vector DB and the miscellaneous nuances that comes with that but um so yeah it was kind of so I I do I think I want to come back to this kind of Park the um you know how we keep the sources in sync and and learn more about the embedding cache but right now I want to get into kind of the end to end flow of new that you describe in this really excellent article about these these three different messaging cues that you orchestrate you present celery and how that works for having uh requests process documents and embed store can you maybe walk through the architecture of how you have these different distributed messaging cues yeah yeah for sure for sure maybe maybe it helps if people can see like the the diagram and the picture that that we have in that in that blog post where where where they can see how have everything uh links to but I'm you can put like a link to that um but look at the core of at the core of it Connor is really just a distributed system uh and in this case it's a rag pipeline that you need to do embeddings and store sure but at the very core is the problem of an engineering distributed system where you have lots of data you need to be able to do retry logic you need to be able to do parallelization you need to be able to do resource compute management and you need to be calling some API some different providers you need to use some CPU some compute um and then you know whatever end to end thing you want to do which in this case is chunk read um uh embed and then vectorize uh that's your end to end but uh at the core of it is this distributed system and this is pretty much what me and my cofounder have had experience with in the past uh in Microsoft and whatnot uh on building data infrastructure and data pipelines and data platform for distributed systems and so um what what we ended up saying is well what are the three main things that we need to do in order for for doing this rack Pipeline and and we separated it in and as you as you saw on the blog post into like three main topics which is there's a request that comes in um into our service uh where that request is going to be um quickly processed and figure out like which source this request is from for example S3 in in the case that we talked to the blog and then um we want to return to the user right away saying hey we received your request we're working on it because it's it's an a sync operation of course it's something that takes time and so then that um from from that spec from from that first step we go and we we we enter into the our distributed cues so that we can process multiple of these requests of course uh and and we send them to our next step which is hey now I'm going to be now that I know it's S3 and I have this many files then I want to process each of these files again naively you you don't need to build the distributed system if you don't want and you just Loop through everything and you process everything and like it's going to take an eternity and not a very efficient thing right so that's why you want to enter this distributed cue messaging uh distributed distributed message cues um and and you want to be able to paralyze and so hey now you are at the file level that you want to process for example in this specific X S3 example and so okay now I'm going to process each of the files each file might need to do some chunking in and of itself and so now you need to process each file each chunk and now you maybe want to say okay now I have each of these chunks of this file I'm going to send them to another processor again so that you can parallelize and and distribute the the the different tasks where the last essentially um cue that you have the last step here is okay I have this file from this location for this specific chunk now I'm going to just embed this thing and do the the vectorization like you do the vectorization and then you put it into the vector database and so it's it's a whole process of like how do you distribute all of this load in terms of the reading and the parallelization so that when you get to the embeddings it's just this unit of operation that you know you can just send to either an open source model or whatever you want to do for for vectorization and then insert it into the vector database and we'll talk into the the the vector database all of the we stuff that we did uh in a second but um essentially this is the whole flow and so in the end it's a distributed system and and because of that you also want to make sure that you have the appropriate logging and the appropriate dead letter Q mechanisms and reach TR logic and hey you had 100 files but you processed only 50 files what's going on with the other 50 what's the status of this 50 etc etc etc so you have like this State Management along across all of these tasks and and making sure that you can return to the user information as well as the logs so that you know if they want to know what's going on that they can so there's a lot of moving pieces and I know I talked a lot but it's it's really the core of like a distributed system where if you've had experience with this in the past you could build it but then you have now the the added it of the embedding cash and the embedding management and the vector DV nuances and and whatnot so um it's way easier if people see the image as well but hopefully if people heard me in this podcast then they can see that they can see the the image and and and understand what we're talking about but um yeah I'll add the a quick intro to the blog post and so people can jump back if they've already seen it I suppose but um yeah so I I really like this separation because I imagine with the process documents you know so you take a PDF and you chunk that into like 50 Atomic units that then are you know tasks in the embed store exactly and then you have this fail Logic for if one fails and maybe we need to go back to it yeah it's all super fascinating and I think uh kind of listening I think to have a better question to ask you next it would be important for me and maybe our audience hopefully our audience is curious as well to understand more about the embeddings cache if you could tell me a little more about just particularly how you see that yeah yeah yeah so at the I'll try to explain it in like the very simplest terms because I think that's that's what probably will resonate the most but you get the data and you want to vectorize the data and you want to get the embedding so that you can put them in the vector DV well what you want to do is be able to have this sort of cache so that for a particular text let's say for a particular um input that you're receiving let's say like the hash of this like like how how a typical cash Works let's say the hash of this uh text is stored alongs the um uh embeddings uh of whatever that text was so that on the next run whenever a new um unit of operation as you correctly put it comes in you can quickly check hey does the hash of this um of this text um uh has already uh been seen in our embedding cach because if it already has then you don't need to embed because you know that the embeddings are going to be the same and you just wasted the compute for that when you already have the data right and so chances are that you don't need to rebed um for that particular text because you already have that into the vector database and so this is crucially important when because one of the other things that we have at NE is that you can run like scheduled pipelines right so um maybe I didn't give this overview but uh in NE you what we've essentially done is a pipeline management and a pipeline platform for you to run this end to end applications for rag uh you can um trigger like a oneoff pipeline you can then schedule a pipeline or we have some sort of support as well for um uh listening for events for for for for specific sources so that you don't even need to worry about this schedule we just listen to whatever has changed like CDC kind of thing uh so there's there's some sources for example S3 can let you know if a file changed and then you can listen to that event and whatnot we could get into that uh later but the whole point is that you have all of these functionalities and if you're you're if a source does not have the CDC capability of you didn't use the the the the CDC uh capability for one of your pipelines and you're rerunning the scheduled one or you were testing or essentially you're running uh pipelines on the same Source then you want to make sure that you don't do overwork you want to make sure that hey this Source didn't change this source is still the same like let's not embed it let's just return very quickly and so that's the whole point of the embedding scash so that uh essentially you don't you don't waste the the the time and you don't waste the compute and the cost on on reeding something that has not changed yeah that that makes so much sense I I think it's really nice it with the whole theme of you know keep the source and the destination in sync I'm really starting to understand it better and yeah that yeah that cach yeah of course it makes a ton of sense if you're reinges your whole notion page and all not ex so I think now would be a great time to transition topics you know we've covered kind of the high level the concepts behind end to end data ingestion now we' started getting into the queue and how the qes and new and how that works and now I think would be this really interesting topic of the open- source wva you know WEA letting you customize the kubernetes and I remember the first time we we spoke Kevin you had a pretty technical issue to discuss can you can you tell us about your experience with kind of customizing vva yeah yeah for sure for sure yeah I had tons of questions and and really props to you guys for like two things like your your documentation is really good and then the support that you guys have um whenever we had a question is is is really top notot so I'm really grateful for that so uh in the end uh again uh what what we're trying to do with neom is a platform where you can bring your own Vector database whenever you want to run this pipeline right and so um for this customer that we were that we were doing this blog post for and and for some of the customiz session that we have whenever a customer doesn't want to bring a vector DB that we use essentially our default one um in this block po we were using um in this in this run we were using web and because it's open source we were able to you know deploy it on on our own kubernetes cluster and and be able to um properly configure uh and so there's a tons of things that um you can configure that people can read on the documentation and whatnot but very very very simple at a very high level is you have large scale data you probably want to parize how you ingest how you query the data into the vector database so very simple things like having a multinode cluster is super important and that's one of the things that we want to really do and and we really just has like you know kubernetes confix where you can say these are the number of notes that we want this is the memory that we want on each of these notes boom you know you deployed it and and and everything works out of the box uh and so at the very beginning we were having problems like well we have like six notes that we had in our kubernetes cluster but uh web was only um listening to One and so it was like as if we had one and so oh what's the problem there well we had an enable sharding Okay so we need to enable the sharding so that it goes and distributes the loads across the different nodes so it was a lot of like TR reading the dogs and talking to you guys and understanding but uh the whole point is if you have a multinode cluster that you can then configure with the right resources in terms of memory and CPU then you can effectively again parallelize how you're going to be doing all of these injections because I told you all of this about distributed cues and all of this data infrastructure that we did and we've par ized a lot on like M's end we have the tasks everything is retryable blah blah blah blah but if your ingestion is not like if your if your vector database here does not support any type of parallelization then you did all of this work and now it's all coming to this bottleneck that it doesn't work so why did you do all of this right so of course with we8 you know because you can also distribute it and it's based on you know for for for for the people that really know and understand and want to learn we it's based on like the Cassandra AR ecture so you have all of this um fa tolerant and and replication and charting strategy that you can uh that you can distribute the the the ingestion too and lastly and and and and we'll talk more if you have more questions but the other thing that that uh that was really important for us that we Grant some quick benchmarks that are there in the blog post if people want to see is well of course we want to take advantage of W's parallelization and at the very beginning we were naively saying well let's just paralize everything right and so uh what what we didn't understand is so we it has this support for every time you ingest you can specify the number of workers that you want to um be using whenever you ingest and so very naively we're like well let's set that to whatever High number we want so that everything is paralized and clearly this was wrong on several levels first you need to understand how many CPUs you have available in your kubernetes cluster if you have four CPUs available and you said your number of workers to be 100 well guess what things are going to blow up because you only have a certain number of threads that you can you know ingest this information and then the other important thing is let's say that you have I don't know 50 CPUs um but with NE we were already doing parallelization so that different threads can ingest into w at the same time so think about this if you have 50 threads on our side that are all ingesting into weat at the same time and each of your weat ingestion has 50 number of workers that you can paralyze then effectively you have 250 right like like so so or or even more like depending on the 50 times even more but what I'm trying to say here is that it's very important to understand the parallelization that you have before you ingest and the parallelization that you have at your ingestion we8 cluster per se so some of the benchmarks that we run and understanding this thing was well let's say that we have 50 uh available CPUs in devate then we have let's say um 25 threads in NE working concurrently basically we we we we try to come up with what what were these numbers and so if we have 25 then maybe we can set each of these threads numbers of workers to two two right so that if you have 25 threads and each of them have two as number of workers in we you effectively have 50 which is your maximum capacity that wva can handle so I know I know I talked a lot about like technical Det here but the whole point here was to understand what are your resource limitations on we8 how how how how high have you uh how vertically have you scaled and horizontally you've scaled each of your nodes uh into your weet cluster understanding that and then understanding how much parallelization you're doing at the extraction and embedding and and everything that happens before the ingestion so that you can then make sure that you maximize the the the resources that you're using we8 but you're not you don't go over it because if go over it then you start getting a lot of these batch connection errors and like I'm I'm I'm overwhelmed essentially the system is saying and so finding that balance uh running the apprpriate tests and and giving that customization to the user which is what we do in terms of how many CPUs you want to run how many threats you want to paralyze was very key for us so I know I talked a lot about that but um it's it was super important for us to understand these things yeah know that was really great and I just like I'm not going to pretend to be an expert on this I'm trying figur out as you know so so my question and I hope this isn't a dumb question but if so I'm glad that there's never D question there never Dum questions is um so I'm having a hard time understanding the parallelization of this kind of like you know uh process documents embed the documents and then and then with weate because my understanding would be that these would be like Network requests uh things but is it that because you're managing like one kubernetes cluster that's doing both newe and WEA ingestion on the in the same like infrastructure as code kind of set up that that's why you need to be more fine grained about this kind of interplay between the paralyzation of N and weate right right right right right so and and this is what I was telling before that um it's a great question like if you have a lot of parallelization before and you don't have any part like and and you just dump everything right into like a funnel that that doesn't allow any parallelization then you've effectively bottlenecked at at at at this stage um and and what's happening here is that if you have a lot of files that you want to chunk and embed what you really want is to maximize the resource the resources compute wise of how you do that because what you could do is you could have one file that you process that you chunk that you embed and that you store into we and you tell we hey do this as fast as possible use your maximum CPUs but then you have 99 files that are waiting also before you finish this thing and so what you want to make sure is that hey maybe you can process the files parall you can have different tasks where each of them are going to be chunking each of them and then you're going to be also parallelizing the embeddings because again like even even if you use open AI right open AI has this uh rate limit depending on the the the account setup that you have but let's say it's like a million tokens per minute or something like that right and so let's say that you used open AI we haven't we did talk a little bit about the dimensions and stuff let's say that you were using open Ai and you were just processing um one file at a time with different chunks so at any given time you're only using let's say 10,000 tokens it's like well you're not maximizing the throughput that you could get with open AI so chunk more things at the same time embed more things at the same time maximize that throughput to get maybe the 1 million tokens per minute for example so that you maximize the throughput there and then everything can be ingesting in we8 asynchronously um or or any other Vector DV but in this case was we8 and parallelize that part so it's a it's like a multi-step parallelization because you have different steps you have different tasks you need to process you need to chunk you need to embed then you need to ingest and so um yes it's this it's this fun and complicated interplay of like how do you paralyze the things that happen before you actually ingested into the vector database and how can you make make sure that whenever you ingest into into your vector D like we8 stuff is also getting paralyzed and maximizing the computer resources at your uh at your compute uh store layer so yeah well that's all really amazing and I think the kind of yeah yeah like I've heard of things like I've had some conversations with Michael going at neurom Magic where we talked about um I was always curious why there isn't more like um you know blog posts about just like here's how many embeddings we can do in like a minute right and so I I don't know too much about this kind of like batching uh like I've heard uh you know so like with uh when they're benchmarking model inference they usually would like uh group it by sequence length so they'll have like you know 384 sequence throughput on deep learning model inference and so I've heard a little bit about that kind of stuff and the idea of orchestrating that with how you do your chunking and then how you yeah all that does sound super no it is fascinating and look in the end is in the end is a compute optimization and compute resource usage that you want to do like you know when you submit a request to open AI people might think oh it's an API I mean yeah it's an API but in the behind the scenes there's some compute that is happening for for for the sequence Transformers to actually you know to to embed this data and so you know if you have one GPU then and well you can do so many texts at any given time before the GPU starts to blow up the same thing when you have like CPU parallelization on on on your computer that you're running multiple tasks and multiple things the more compute that you have the more polarization that you'll have but the more compute you have the more costly is going to be um and so this is also like that that fine balance that we didn't talk too much about um uh that that we had to do some research in terms of well these are the rate limits that open AI enforces um this is the cost of open AI this is dimensions of an of an AI what if we use something like replicate and and this is also a YC company where um you know they help you with the with the hosting of the models um and essentially do all of this parallelization that I was talking about but from a compute and and and inference embeddings model layer so that you know you can submit these API requests to replicate but you can also configure hey these are the maximum gpus that I want this is my cluster this is the things that I want and so again depending on how much you pay and depending on on on the cluster that you have um well you can maximize that throughput and and and um and get the embeddings in parallel before you go to the the vectory but yeah this is a fascinating thing Conor like it's a there's so many different layers and and different compute resource optimizations again it's at the core of like a distributed system where you need to be able to pay attention to the different parts that uh that you have and and how can you maximize and and parallelize everything um and and and the reason you want to maximize and parallelize stuff is well you want to be able to run this in a decent amount of time and not just a get a pipeline that embeds gigabytes of data in two months you know like well CH no like you want to finish this in you know a couple hours so uh so so so that's what you wanted was thing uh but it's always a trade so yeah I wonder if you know mentioning replicate I find that so interesting this kind of like um well I I think from the perspect the first perspective is just like you want to use this MP net model which is like half the dimension size of open AI so it's a little easier to use but there's also kind of this emerging thing of maybe you um you know train your own embedding model and that's like one idea but I think another thing that's pretty interesting with this is like with weeva one of the biggest topics in weeva is the development of multi-tenancy and multi-tenancy is about your users have users and you have the scheduling of like active inactive and maybe also there's something to like you know this user is particularly active they have a 100 million vectors in their we8 instance and so let's give them eight gpus for their embeddings make it extra fast whereas the inactive they maybe 20 of them share a gpus right right right right so yeah I think this kind of infrastructure is code and under you know as you've really helped me understand better the these different processes that could be running in parallel it's quite the system yeah yeah yeah so yeah so I think all that was just really fantastic and I think kind of transitioning into more open-ended how you see the future topics you mentioned like large scale and I'm really curious just like uh in your blog post you mentioned putting a billion vectors into weeva how do you see this kind of like billion scale plus Vector search yeah and uh I love the question but I always I always am SK not skeptical but like how I respond to this future looking questions because chances are that I'm going to be wrong on whatever I predict here uh but uh really like the the thing here is Conor is like we're very early and you probably you guys probably know this better than than than us but like we're very early in all of this like Rag and and gen applications that are coming at at at large scale and at Enterprise scale there's a lot of like Twitter chatter on like hey I built this thing and you know it's a couple PDFs and I use these Technologies and these things work and it's great but what we're really trying to focus on is where are these billion Vector opportunities like where are these large scale opportunities and we're fortunate enough that we're talking and and working with a couple of like um customers that have this sort of scale but I want to be also honest with you and it's like we're still yet to see lots of people that have this oh I have 10 billions I need to be able to have this uh mechanism to be able to store them it's like I want to talk to you if you have that that sort of scale but it's like we we're also a very early stage in terms of how how many players we're seeing that have this sort of scale and that have the sort of uh business requirements for large amounts of data and so it's not really a prediction to answer your question but I'm really hopeful um based on like history of how data engineering started like H how it has transcended in terms of like there's always going to be more data there's always going to be more needs to handle all of these data infrastructure data management so if history repeats the same itself in in in that same uh approach where on traditional datalytics then the same thing will happen here like of course we're just at the very uh basic layer right now where where the emerging technolog is coming but like can we really be at a place where we're position ourselves here to help the people that uh that are doing the the really large scale um requirements and and uh um you know be able to support them and and uh I'm actually very glad that you know for example in web8 um uh one of the reasons we're using we8 for for our uh for the pipeline run that we were sharing there in the blog post is because of the easy to use and and and being able to scale um horizontally and and handle all of this parallelization for such a large amounts of data and and there's some other stuff that we didn't even mention here and and stuff that maybe for a future podcast or or something that I also we didn't invest too much in but like stuff like the grpc client that you folks are working on which is greatly going to increase the uh speed of like how you ingest and product quantization and how if you have large amounts of data you can actually reduce the memory footprint that you want to have with trade-offs of course on like the inje time and your search latency and whatnot again everything is trade-offs but being able to to have all of these configurations for like large scale uh ingestion and and and data I think it's it's crucial and it's uh what we're really excited to keep working with and and finding this right set of people yeah I love that answer yeah I think that's all perfect just like kind of the the the assumption that all the kind of database workloads are going to move into also having this kind of like vector data Vector database native kind of thing AI native and um yeah I think all that just makes a ton of sense but I'm also kind of curious about like um you know what what kind of applications might be able to be built with billion scale plus Vector search that we've just never or let's say trillion a quadrillion let see how high I can count yeah yeah yeah yeah I would love to know from from your point of view as well like what what are your folks seeing because you are the store layer right so you probably have more customers other than than you know than than us that that actually did a billion scale Vector as well I don't know if you have trillion Vector scale honestly I don't know who has that much yet but like that probably will come um but yeah I'm also interested in in what are the sort of things that that that you're seeing because I think as we move away from like your typical chat bot experience um into into more sophisticated uses of semantic search um where you know you have knowledge graphs and and and really be able to do semantic search quickly over 100 billion vectors that you have in your store and you get a result in subc like this is this is the sort of thing that I think it's super interesting and um I don't know what are your thoughts in terms of what are some of the applications um that that can be powered with with this scale but uh it's definitely exciting where we're going um so yeah yeah yeah I think to quickly I really want to go to that knowledge graphs Topic in the metadata I think it's so related to ingestion I I say just concluding my thoughts on like what are the new applications available with quadrillion scale factor search yeah is I think we're headed to like if you seen that Rick and Morty are you aware of that show where they have like the interdimensional cable kind of thing and it's like you know it's like so we have this concept in we called generative feedback loops where it's like you put some of your data in it and then the llms or the generative image models multimo models will like do stuff with your data like create new data out of it and I think it's very related to like you know we have had this research called open-endedness there was you know work by Jeff Clon Kenneth Stanley Deep Mind has an open-endedness team and so that's where I think that quadrillion scale Vector search is headed but I would love to talk about knowledge graphs because I think so so knowledge graphs you know there's there's kind of a couple things with this there's like the you know thinking about graph databases and storing all your data as like entity relation entity and these tupal and fast indexing of these like it's like a more native way to have joins in database sense and then there's also kind of like Knowledge Graph like you have this Rich metadata so you can have like filtered Vector search so you know if you have like I'm searching for you know NBA players that play like Kevin Durant but but also were born before 1985 and grew up in the west coast of the United States so yeah yeah yeah yeah know that that's that's uh I don't know how to put it like we we've gotten so many questions around this learn like well I want to do semantic search but I also have like facts it's it's what we seeing people call it I also have like facts that are not really anything that you would want to semantically query on but rather like do your typical select where this equals this right and this is where the metadata is super important and hnsw algorithm right and and how can you put the right metadata so that you can effectively do the hybrid search where and you probably know this more honestly more in details but where you can do the priv the hybrid search where you have the your typical semantic search over your text semantically data but you have the filters that you can do based on facts based on dates based on names based on places all these things that you want to like filter on so that you can then have a more accurate um accurate answer and I think um one thing that we actually saw um uh for all the hate that L chain gets and and honestly I think L chain is great uh there's this thing in L chain called the self query retriever I think it's it's what it's called but it makes it super easy in in in in transforming a query based on some metadata that you want to pass and some metadata that you have in your vector database to really create a query that does this hybrid search where based on your natural language query some stuff is happening at the semantic level and some stuff is happening at the filterate level and get an accurate response and I think that's that's huge uh I haven't myself played too much with that um uh to be able to talk to you for like 20 minutes about it but it's something that I think it's super important and and why we're also so keen on whenever we extract data um at our preprocessing stuff that I was telling you making sure that a we support for metadata inputs that people can bring or we also come up with basic metadata extractions or smart uh chunking smart metadata extractions not dumb metadata extraction no but like really um so that you can appropriately have your vector information with your embeddings with all of the appropriate metadata that can be linked to that Vector so that you can do the the the hybrid search and whatnot so um again super fascinating stuff that um I don't think it's like a solved problem today on like how you do this thing and that's how you see so many players doing all of these different uh um approaches uh and and hopefully we'll evolve as well into how we do this better uh but yeah I I don't know what are your thoughts on like hi hbd search and and uh uh usages that you see within we8 from from the support channels and the customers that you have uh and some insights that you might have on this topic yeah well I I think that was great and I I I guess I was super bullish on this kind of extract all the metadata you can have some llm that just looks for anything that's symbolic and then maybe we use that in SQL style query but uh then I was talking to Nils ryers about this again who I consider sort of The Godfather of AI search from cooh here and and he was skeptical about this because it's hard to kind of unify metadata and I kind of now I'm now I've kind of come to understand and agree and I because to me it's like the only obvious kind of metadata to have uniformly across things would be like uh title content source is three properties of a generic kind of unstructured vector index content being the thing that's vectorized title is like a pretty uniform thing you'll find it in any kind of PDF blog post they'll all have some kind of title and then you can maybe do like like in the elastic search in the bm25 there was bm25f where you would keyword score with two properties and title and then the main content thing being the most common manifestation of that and then Source being a common filter where you you know where Source equals Twitter equals arive equals GitHub for that kind of not- taking dog food so I you know outside of that being so I I think a lot about this kind of what would be like the standard schema for Vector indexes and I think that kind of like title content source where content is the vectorized property and then maybe I think uh linking it to external tables that contain the symbolic SQL style and but I love how you brought up the self quering retriever from uh Lang chain and I think llama index and Lang chain they both really push this like um Innovation on llm query engines you know like having the llm do some intermediate do some stuff yep yeah and so yeah that's why I just think this whole like you know sort keeping I love this this the source and destination you you've just opened my eyes to that I'm not going to pretend like I had a good understanding of that before I spoke to you just now but like this this like ingesting all these data sources and then having these like llm query engines that can route queries across it and having this Rich structure I don't really think you know I I like the knowledge graph idea I think it's quite cool but I don't really think this kind of like entity relation entity native way of thinking but you know I do kind of agree with this idea of like extracting facts and maybe like having something that's like is just a fact rather than like a paragraph of atically WR yeah yeah yeah yeah know makes sense yeah I think you probably know this but the space is evolving right and and it it's eving so fast uh I don't know like every day I go into Twitter and I see a new parad paradigm that it's like okay maybe we should include this as part of like how how we extract metadata you know so yeah it's fascinating honestly building in this space at this stage I think we're lucky um to to be able to be a part of it and as long as we constantly iterate and we constantly improve and yeah things will fall through yeah I think my concluding thought also could be maybe that there's something to evolving your schema like maybe you can have some kind of exper like we're seeing a lot of um people are curious about how do we evaluate these systems and the most common thing is the llm eval where the language model is prompted with like here's the query here's the search results what how how are they and and then it kind of can self-tune itself that way and maybe you can also self-tune the the schema and the metad it could be quite deep I think it's quite complicated but maybe so so I have kind of a concluding question I think a lot of our listeners will be very interested in why combinator it's obviously like you know the NBA of startups how has your experience been with and and I think also this year especially is really relevant for our podcast because it seems like y combinator has definitely made a big push on AI deep learning LM based companies so I'm very curious like what your experience has been with it for sure for sure in terms of the AI the number of AI companies in in dis patch I don't have a number um maybe there's like public information but there were a lot of AI companies majority on like infrastructure developer tools and whatnot and and some of the like consumer side of things and and application Level um stuff but um it it has been an amazing experience man it it it it has been something that I've always looked uh looked up for I spent five and a half years in Microsoft and then one year in in circle the the creators of usdc for for those who are crypto uh knowledgeable uh and I've always wanted to you know start my own thing and and and be able to to to learn as I go and ship fast and whatnot and really me and mounder have known each other for like 10 years and it was my co-founder's idea on like hey let's just apply to W hater I'm like I mean sure I don't know like isn't super hard to get in like yeah okay whatever let's just apply and the funny thing is that look like we even applied with a different idea right but but we show this energy we thought this we show this motivation we show this commitment we showed this you know we're going to build we're gonna we're going to make it happen right and and you know from the learnings that we had from from that initial idea this is how NE was uh was created essentially but um it's it has been an amazing experience really uh these people that really help you push you it's really an acceler for that reason you're accelerating at a very very fast pace um for the first three months and then of course they tell you why comat doesn't stop uh here in September when you end like you need to keep getting your goals you need to keep delivering fast you need to keep shipping keep iterating and so I think the the network of of people that we met the the advice that we received and and the whole acceleration and and and being part part of this uh of of this wave of people that are innovating fast it's it's it's an amazing experience really an amazing amazing experience highly recommend for people who uh who who want to build startups and and create something uh and want to meet more people so amazing Kevin thank you so much for joining the weeva podcast just such an insightful discussion I'm so glad that our paths have crossed and I've gotten to learn these things from you I think you know all the you have so many great insights on this topic of ingesting data and I really love this keep the source and the destination in sync I think it's a super unique angle as well as your perspectives on kind of all the end to end stuff Kevin thank you so much for joining the wva podcast thank you so much Conor I appreciate it a lot man thank you awesome ", "type": "Video", "name": "kevin_cohen_on_neum_ai__weaviate_podcast_70", "path": "", "link": "https://www.youtube.com/watch?v=dWtqwt5cGjI", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}