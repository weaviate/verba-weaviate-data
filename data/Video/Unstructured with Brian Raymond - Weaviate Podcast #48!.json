{"text": "Hey everyone, thank you so much for watching the 48th episode of the Weaviate Podcast!! This is a SUPER exciting one, ... \nhey everyone thank you so much forwatching the wevia podcast I'm superexcited to welcome Brian Raymond thefounder and CEO of unstructuredunstructured is such an exciting companyfor uh I'll explain kind of the way thatI see the space of kind of how do youget day unstructured data like say PDFso these corporate data Lakes web pagesall this stuff kind of getting it intosystems like we V8 and so I'm so excitedto welcome Brian thank you so much forjoining the podcast thanks for having meexcited to be here awesome so could youmaybe kick it off I know I'm going togive it a course description but can youexplain what unstructured is in thefounding vision of it yeah absolutely soum so when we we embarked in uh on thiskind of Journey that we're on today withunstructured the vision wasum pre-processing is terrible nobodylikes it it takes forever but it's acritical step if you actually want toum deploy language models of any typeagainst data that's important to you andsoum I had uh the last four years before Istarted on structured also the a greatcompany called primer Ai and at primerwe were fine-tuning models orchestratingpipelines building applications on topof them doing all sorts of really coolstuff with Transformer based models andum and we we'd have these you knowfantastic applications and we'd showthem to customers and say I want that onmy data and then um we look at theirdata and we'd hold their head in ourhandsthis is going to be a huge amount ofwork and rejections and Python scriptsand OCR and everything just to geteverything into typically like a niceclean Json format right so that we canfeed it into an inference Pipeline andso the vision when we started out washey hugging face is you know explodingover here with um you know tens ofthousands of models incredible Communitywhat if we did something similar to theleft of hugging face and we made itcheap fast and easy for data scientiststo get through that data engineeringstep so they can consume more of thatand soum you know the you know initially itwas hey let's help folks build out thesebespoke pre-processing pipelines to takesay a non-disclosure agreement that's ina PDF you want to train a classifiermodel or something on it how do you getthat into Json or I got a bunch ofPowerPoints or CRM data or scraped HTMLhow do you get that to a point where youcan start interacting with it andum and so for the past 10 months or sowe've just been been in build modeum you know focused like a laser oningestion and pre-processing a littlebit of work on connectors but mainly onthat transformation that filetransformation work to get it into abase format that you can go use withmachine learningabsolutely fascinating it it really youknow captivating overview the thing I tokind of maybe to break that in sopre-processing is this like are youthinking about like in the I knowobviously there's like the PDFs to Evakind of angle that we're so excitedabout learning more about but I thinkalso you're hitting on likekind of like the ETL like when you are adata scientist and you take like a CSVand you look for like missing values therange of the distribution of yourfeatures is this the kind of thing thatyou think about like that kind ofpre-processing can you tell me moreabout the pre-processing yeah and it'sits own weird space here onpre-processing with files that containnatural language data and so we thinkabout it in in internally in terms oflike three steps okay the first thingyou want to dois you want to partition and extractthat the natural language data from aparticular file right and so if it's youknow a JPEG that's of a billboard or ifit's a Word document or a text file orwhatever you still want to detect youknow you want to detect like okay thisis the headline this is body text thisis an image captionum because you want to be thoughtfulabout down down the stream about whatyou put into your inference pipelineum you want to extract that data andthat's a whole process in and of itselfof doing that um and so Matt so onceyou've done that once you've partitionedand extracted that dataum the next step in our our world is iscleaning and how do you get reduce itdown to like a clean markdown file orJson so there's all sorts of artifactsthat that that that you know wind theirway in here so likesentence fragments and what weird whitespaces and beautiful characters and allsorts of just like little gremlins thatjust burn up time on on cleaning that upand so we have spent a lot of timethinking about that cleaning step and soonce you have that clean step as a datascientist your job's still not you'renot ready yet you gotta stage it andthere's a lot of work that is in thestaging still which is Last Mile thatburns that time so concatenationchunking tokenization creatingembeddings mapping Json schemas for youknow maybe you want to set up a labelingtask maybe you want to dump it a vectorDB maybe you want to send it toum a hugging face pipeline whatever thatmight bethere's a lot of that last mile kind ofstaging work that has to be done sopartition clean stage and then you'reready for ML and all of that today isstill completely manual and that's why80 percent of data scientists work inthis domain it's actually on thispre-processing stuff that they don'twant to be doing they want to be doingthe modeling and so we're trying to givethat time back to themyeah amazingdata science is always saying like allthe data cleaning the most painful partbut the most productive part and so yeahthe pro the pain point is so clear andthe problem being solved you know souseful um so I think kind of I want totake into this question of the why nowquestion sort of like the how largelanguage models are facilitatingunstructured in this technology what'swhat's now making it easierI think um what large language modelshave done is really increase the demandside of the equation the right hand sidewhereum there's a there's a couple attributesof them that that are really importantone of themis that um from a generation and I'm nottelling anyone anything they don'talready know but like they're just a loteasier to use for generative tasksum right than um than old you know kindof the olderumclass of models and so it's just thetime to value and the cost of value is alot lower and the second is that they'rea lot less fragile a lot less brittle toto the data that you're putting in theminto the problemif you're using like in context learningor whatever and sowhat that has what that means for us isthat our Persona has shifted a littlebit so we were in a world where youneeded to create these incrediblyaccurate pre-processing pipelinesbecause if there's any noise or shift inthe document template or anythingit would throw off a lot of the um thedownstream the structured data that maybe fed into a knowledge graph orsomething Downstream nowum there's since they're a bit moretolerant and there's a lot more that youcan do with themum folks are saying Hey I want to useeverything that we're producing likeevery file type everywhere we'recreatingum we're recreating you know naturallanguage data that's relevant to ourorganizationI want to I want to use it inconjunction with a language model rightand so on our end um it's gone from lotsof super precise pre-processingpipelines to hey how do I take an S3bucket or an Azure block full of likeyou know just everything that you canimaginesend it through unstructured and thendump it and weviate and now I can chatmy data or interact with it that wayright and so that's um shifted some ofour like the Paradigm that we'reapproaching a lot of our engineeringwork and what we're prioritizinggotcha got you uh so I let me get yourthoughts on I was I remember like withLang chain and llama index when theycame out they had like a data connectorsHub and I remember things like S3 GoogleDrive notion uh you know like or justlike the PDFs so what was your reactionto that kind of thing is that that isthat kind of the like a lot of theunstructured part is like connecting tothese data sources as well as theTransformations oh yeah I mean Jerry andHarrison have done just awesome workthere and um and unlocking a lot of thisuh the potential over hereum we're looking to kind of build ontheir shoulders forEnterprise production grade deploymentsum and it's just a slightly differentsort of take on it that's feeding intoorange engineering requirements and sowhat that means in Practical terms isthat likewe're I'll talk about our connectors andwhat how worth thinking about connectorsversus data loaders right and thepopularity there and then also whatwe're doing on the transform side that'snot there and really this is a storyabout how do you enable all the datalike the thousands of data scientistsout there right now that are prototypingfor them to go from prototype toproduction and I know Harrison's beenspending a ton of time talking aboutproductionization of agents and theseother things we're we're in this narrowstrike we're staying disciplined in thisnarrow strike zone of ingestion andpre-processing so on the connector sidesome some things that differentiate ourabstractions fromum from those data loaders are that umthe data letters are kind of grabbingeverything and moving them over we'reable toum ascribe canonicalum numbers to them until we can measurenet new and so not grab everything andso we're not like you're not shifting alot of duplicate data inum and then also thinking aboutscheduling and these will be these willbe supported by us so if they break inthe middle of the night unstructuredwill fix it kind of like a 5tran typeapproach but they're built for theground up from the ground up with llmsin mind and so um some other things likethey real we're building them in such away that they lend themselves toparallelization across like CPUs and soum and so that's like a whole discussionin and of itself on the connector sidebut once you connect and you grab thatdata that net new data from G drive orSharePoint or Azure blob or whateverum we're doing all that transform workthat you know the partition cleanstage right and then handing offlike our goal is our is for our users tobe able to take raw data grab ittransform it and then hand off whereverit might go might be a vector databaselike leviate without having to worryabout any additional data engineeringit's ready to go and so we've gone fromraw to ml readyum with an instructionand so um so I think that there's a hugeinnovation in I wanted to talk aboutthis kind of like PDFs to weavia I thinkthis is one of the most exciting topicsis that you know we've seen this athackathons and stuff as people they wantto chat with their PDFs because it makesa lot of sense so could we talk aboutthe Innovations in unstructured like howwould you process a PDF withconstructionyeah we're we're trying we're doing aton of internal r d work right now um inthis area that um expands a fewdifferent approaches andwe're what we're imagining in our uh forour kind of Target Persona or our folksthat are pro like our organizations thatare producing huge volumes of data todayso we're thinking a lot about computeefficiency like some of the largeorganizations that we're working withtodayum create and process more than like aquarter million files a day and so youdon't want to burn up all your computeBudget on pre-processing right and llmsare even though you can use llms forsome of these things they're about ahundred times more expensive than theapproaches that we're taking and sowe're like okay how do you render thehighest quality data with the leastamount of compute so that you can get toa chat your data type approach and soone of them um we're taking like yellowX and we're you know fine-tuning that onour internal label data we got like tensof thousands of pieces of label datathat we've been curating over the lastseveral monthsum and so that's you know yolax'screating the bounding boxes you knowchoose your OCR vendor act whateverum and then couple it with our stagingbricks and now voila you're ready torock and roll that's one approachanother approach is like a donut basedapproach so uh ocrls Vision trends likeswim um Vision Transformer where we'vebeen doing a lot of work theresome performance you know benefits andtrade-offs between that and an OCRapproach and then also implications forcomputeand and so that's like another oneyou'll be seeing those apis coming outsoon our general pipeline which isum realized more on like you know moretraditional CV and NLP approaches andthat's available today and then we'realso um pre-training Our Own Foundationmodel from the ground upum that's our own Vision Transformerwhere we've curated all the pre-trainingdata and we've curated all thefine-tuning data and with the with theidea being hey let's train a model thatyou can throw almost anything at it andit's seen it and it can you know umtransform it and render it with a highdegree of performance that's in processtoo we actually are just starting we'rejust wrapping up pre-training andtransitioning to fine-tuning on that andso the goal is by mid-summer to have awide range of different kind of likearrows in our quiver that our users cancan use to um to get over that hump andto get to that clean Jsonamazing a couple things I want to takeapart for firstly I think just yourknowledge of the the state of the artand OCR and I think swin Transformerscan you maybe just kind of explain thatmore because I you know I I know I don'tknow that much about it I'm sure ourlisteners are curious about it sure Ithink I mean at a high level here a lotof the approaches to date have been okayum infer bounding boxes aroundum around document elementsand there's some models that that workpretty well on that we we've been doinga lot of really focused work on on Imeanfolks the stuff that'll just put a lotof people to bed uh to sleep but um howdo you accurately draw a bounty boxaround an image caption anddifferentiate that from a list from aheader or a footer or any of thesethings so that you haveaccurate metadata tags so you can stripall that stuff out and focus like alaser on exactly what you wantDownstream so you can curate your dataum and so that's you know Vision youknowum a computer vision model coupled withOCR to do the extraction and then youknow you can do some last mile tuningbefore you rollon the um Vision Transformer basedapproachum it's it's really interesting so it'syou know it's it's tiling up a um aparticular image and um and it's it'susing an OCR list approach with so it'sa vision encoder and then a text decoderin order to do the jump from an Imogento Json out and um and you know we werepretty optimistic about that approachboth from a compute standpoint and aperformance but depending on the needsof the user if the user is going to needlike coordinates for traceability likewhere a particular extraction came fromand which page and where on a documentum you know those all those specificsum May necessitate different like adifferent approach and so our goal is tohave like a nice wide menu of approachesthat kind of balancefeatures with with compute efficiencyit's so interesting to learn about allthis I mean Iwell I oh well I want to come back intothe plans of the foundation model I'mvery curious like how you're seeingtraining a custom Foundation modelingexactly what that'll look like forunstructured but I have to quickly I'mcurious about like with theum so so with visual document layout andthen I'm I feel like web scraping is ahuge application of this where I'm likeyou know we also just record a podcastwith the Kappa guys who took all thedocumentation blog posts of weviate andput it into one of these retrievalaugmented systems and I mean it's reallyremarkable and yeah so so like you couldyou just point me to a web like if Ijust had weevier.io just this websitehow would you then go about getting allthe data out of it like so like whatwe're doing isum instead of trying to parse the HTMLlike really effectively and because itcan be like there's nothing there's somesolutions for it that work pretty wellbut they're not likefantasticso what we're doing is instead we're umwe're reassembling the web pages andthen Imaging those pages and then um andthen feeding that those essentially likescreenshots of the web pages through themodel right in order to do thatum and we think that that approach andthere's been some recentlike really great Twitter threads andsome some papers on thisthe compute efficiency of these computervision models is coming down and so youcan do that at scale and the performanceof those extractionsum is going to be a lot better than whatyou can do in just the pure HTML parsingyeah that that is incredibly exciting Imean I thinkyeah wow it's really like you're reallypainting a compelling vision for thefuture of multimodal I think about justlike you know I like I like to read alot of scientific papers and I thinkabout how much information is capturedvisually in the diagrams but then thenext question and this is something thatcome that I've heard so many times withthe vector search is like so with tablesyou can also kind of like extract tablesand you know charts and stuff so is soyeah I mean I don't know if it's thequestion is maybe pretty self-ob obviousI think you do the same kind of thing asthe OCR models and all that so yeah so Ithink it would be a good transition tocome back to that Foundation model um sotraining Your Own Foundation model to dothis kind ofI want to say document layout parsing isthat kind of the idea yeah yeah raw toJsonum for files that contain naturallanguage and so feed in anything almostany fileum and what the approach that we'retaking isconvert it to with this particular modelof other options but with thisparticular model convert everything toan image Vision encoder text decoder andyou get Json out and it hasuminstead of just one big brick of textwhich is kind of like what you have withthe document loaders now right whatwe're doing is we're saying takingSnippets out and saying okay this is thetitle and having a title metadata tagand this is a body type body text Chunkso and so forth and so you can be muchmore thoughtful about one what you'rewanting to store and then two whatyou're feeding into prompts rightDownstream and so again everything atthe end of the day is going to come downto speed and cost of compute and you canchew up a lot if you're doing this inproduction for an Enterpriseum without without having done that workon the on the on the front end and sothe the goal then with this with thislarger model then istake huge volumes of heterogeneous dataand then you can curate it verythoughtfullyum before you decide to go spend a bunchof money on all of it within the LM andlike Jerry over it well I'm going to geta great piece in medium just um justlast week looking at at Claude at 100000 token window it's a dollar a promptalmost every every time you query themodel I mean if you're doing this acrossan organizationright like that's going to add up in ahurry and so what could you do to maybeavoid having it look across a hundredthousand tokens and do you want to rightyeah and that brings me to one of thetopics I'm most excited to talk to youabout which is this kind of like uhextracting structured data fromunstructured data and to as a quickbackground like with weaviate Vectorsearch in addition to doing Vectorsearch or hybrid search we also havewear filters so if you want to say likeyou know where web page equals webaio orlike where price equals less than orlike price is less than 100 like thesekind of symbolic filters they integratewith the hsw vector index to facilitateusing this kind of structured data inaddition to searching through the textchunks or bricks I think that I likethat abstraction a lot think about as asbricks but so I wanted to get yourthoughts on this one paper and I'll giveit tldr quickly but just in case youknow people for everyone out there sothis paper is called evaporate code plusis the name of the algorithm and so whatyou do is say you're looking atWikipedia pages of MBA players first youlook at a subset so like Kevin DurantJason Tatum LeBron James in it and sofrom you look at this whole thing withexpensive large language model take outthe symbolic attributes like collegeheight years in the NBA let's say andthen you say and then the large languagemodel writes python programs to extractthose feature those attributes and thenyou generalize the python programs todoing the rest of the Wikipedia Pages uhso hopefully that was a good explanationof the technique and if if it's notunclear I'd be happy to clarify but ifyou think about that kind of likeuh bootstrapped like here are theattributes and then here's a way toextract the attributes from unstructureddatayeah I think um once you get it intothat claimed Json that's prepped andready to go likewe think there's a lot of complexity inconnecting to everywhere that anEnterprise or an organization stores alot of this data and then hauls alongtele file formats and sowe've been less focused to date ontrying to adapt like gpt4 or a Bard orsomething like that in order to do allof thatum becauseover time we may pivot if the computeefficiency improves and like theperformance improves of that to say okaylet'slet's build an ingestion andpre-processing platform for the groundup to do that get it into that baseformat and then use llms on top of thatum on on top of that uh that data umit's just going to be a lot faster and alot cheaper and it's going to be moreperformant for the most part right nowand I think you're seeing some of thesechallenges with like agentsum right now and moving them intoproduction like incredibly promisingincredibly excitingnobody's using them in production reallyyet right and it's going to be a whileand solike we welcome open source Communitycontinuing to like to to work on thatand like as they progress like we makewe may pivot on our strategy but we'remost bullish on putting the llms againstthe clean Jsonum and then asking because you're justasking a lot less of the model whichmeans that that the structured data thatcomes out the other end is going to be alot a lot higher qualitygotcha I think I'm starting tounderstand about it so so say I havelike uh you know like parquet files orCSV files or um you know maybe I havelike a graph database like I have somedata in neo4j and I come to unstructuredand I say hey could you transform thisinto like I need a weevia database couldyou turn this into could you like readthe documentation and learn abouthowever you transform data from one fileto another is that kind of the generalthinking and so could you could you shedmore light on what exactly that lookslike because I'm sure you have a lotmore and knowledge about that thing yeahon the other side we've done a lot ofwork on staging bricks on likeum on like Json or markdownTransformations and so once we get itinto a particular base format um or dfvum adapting it to what's needed and thisis really like last year we were doing alot of work around like label studio andsnorkel and label box and like the longtail of labeling you know vendors andthen also some of the NLP vendors andeveryone had like a slightly differentJson schema requirement and so we weredoing a lot of work onthese staging bricks which would take abase one and then adapt it to the theschema that's required for thatparticular Downstream application and umwe're probably gonna need to do continuedoing more more of this butagain like our our kind of you know ourstrike zone is okay I have a file thatcontains natural language and I need toget that into a consistent format sowhen I can curate it for you know tocreate a knowledge graph or somethinglike that right and soum that's that's really kind of the sandtrap for data scientists today and whereI don't think we claim that we've solvedit by any means but we're we're you knowwe're running hard against that thatparticular problem setit really I mean I think like so so if Icould understand the problem a littlebetter so I know things like you knowsay I have some kind of specialcharacter in my unstructured text thatwhen I try to decode it it's like youknow ASCII can't decode this characteris it so mostly thinking about problemslike that like where you're ityeah the the and those are those arebound and so we have all those cleaningbricks around those those types ofproblems right that are just focused onthat that we're trying to actually embedthose cleaning bricks into these modelsand so these models are are doing thepartitioning the extraction the cleaningum all in one Gap and then you can stageit however you can make those choicesthat you want with our cleaning I meanour staging bricks Downstream but that'sumthat's where like you know datascientists that are dealing with thisnatural language data that are needinghigh qualityum you know high quality pre-processingthat's where they're getting bogged downtoday on just writing custom pythoncustom regexes including them and thenif you change the document template alittle bit everything breaks and theygot to start over again right and soum it's that sort of that that stepright there that um we think we'reprobably delivering the most valueso is it correct like I'm thinking aboutmaybe like with umlike uh how with like say like no SQLsystems you have like this evolvingschema you can transform your schema asyour data kind of evolves and you'readding new properties attributes andthen it's kind of like the syncing up ofthat at massive scaleit could be one other part of it whatwe've done is we've come up with likeour own ontology just mainly because theone hat that doesn't exist yet umelements so instead of likeum of columns and rows it's like okaylike we we featured a lot of thinkingthat's like over the winter in thespring we're like okay like we don't weneed a bunch of categories but we don'tneed too many categoriesum of like you know what what are commonacross receipts and Powerpoints andmemosright and like yeah in the naturallanguage domain where like you know someof them I mentioned already but likelists and headers and Footers andadvertisement and captions all of thesethings and so what you end up with aretokens fans with a metadata tag to whattype of document out like Universaldocument element that corresponds to andso you could say oh I want to likeexclude all of the headers and Footersand captions from what I send to weviabecause I don't want those gettingsucked up into what I'm promptingagainst for example right with an llmum or you know I want everything or Ijust want to isolate some things and soyou can that's that allows like thisrapid curation rather than having toguess and check you know writing regexesand Python scripts and then changes thenthe Smith categorizing itum you can do thateffortlessly with our toolkitoh so yeah that quick list of thePowerpoints receipts that helped me helpto click for me a little more just likewow like how much how much informationis visually organized I have to yeahlike I'm so biased in my own likeexperience of thinking just about likeyou know archive papers and like blogposts is the thing but I mean can youtell me more about just like what you'reseeing with all the information outthere that's like a visual documentlayout like you I don't know I meanPowerPoints receipts are youI mean the the world that we like thethe customers we're talking to areprimarily large organizations and justthink about like okayhow many data loaders have all the allthe developers that are supporting llamaindex and um and link chain and otherscome up with right and a lot of thoseare kind of like easier data set andthen you have like you have Salesforceand you have HubSpotum on the other hand you have slack andyou have notion and then you have stuffthat's out there on the internet andthen you havewho knows what in G drive or inSharePoint and then you have historicalthings that are that are stored in an S3bucket right that nobody's ever doneanything with you have recordings ofpodcasts and things like that right youcan run through your textsoh but the bottom line all of this likethere's this whole world of privateinformation that organisms aregenerating every day that's specific totheir particular mission right andthe pro like the exciting thing here islike how do we like for them it's how dowe take what's out there and publiclyavailablewhat we're generating internallysomewhere like weeviate and then utilizeit in conjunction with an llm hire toenhance productivity or to make betterdecisionsand um maybe we write memos a particularway maybe we make presentations acertain way maybe there's thishistorical knowledge about how we dothings and I could query you know I havethis vision of being able to query ithow do we take all that valuable naturallanguage data that's everywhere and thenget it into eviate for example right andand utilize and touch with llm likethat's what's so exciting for theseorganizations right now but that's therethere's some huge bottlenecks along thatthat Journey for them to to navigateamazing I mean I think about like howlike within our company how informationis kind of like visually organized andlike notion and Confluence and you knowmaybe like even like a jira board youcould visually look at it and then parseit out from that and even just like as Iwas doing did that you know let's listout some things I started thinking aboutlike resumes that are visually organizedand parsing that out yeah it's all justincredibly interesting so let me ask youabout this kind of chunking anddetermining Atomic units for text so wegenerally with text embedding models tryto get it to around like 512 tokens andum so like with you know Lang chain hasthis recursive character splitter thingas a part of their PDF ingestion or likeas part of their data ingestion Librarywhere basically what it does is you knowit like if if you have like it has a fewelements to it so firstly like if youhave 1500 tokens It'll like junk it upthe first 512 an x512 and the remainingthing send that all out to the databaseor whatever or it'll be like if you'retrying to parse like a python file It'lllike particularly have delimiters forlike new lines or like def for thefunction like these kind of littlethings um so do you think there's a lotof innovation in chunking yeah I thinklike what we're at least for on RN Ithink Harrison's on the right track withum with the recursive text splitter Ithink where where we're going with thatisum chunking into increasingly smallersmaller units that have the metadatatags associated with it and so you'renot like let's use a web page forexample let's uh let's say you scrape aNew York Timesum along New York Times article rightand what will happen right now is likethat'll get chunked into you know 512token spans or something like that rightbut within that you'll haveadvertisements you'll have links toother articles you'll have imagecaptionsyou'll have you know all sorts of stuffright maybe a text box about the authoror about a related article and all ofthat mm-hmmin an ideal world like if you're ifyou're just doing chat your data itprobably doesn't matter as much ifyou're wanting to continuously updateknowledge graphs or like Salesforce orsomething like that using llms matters alot more and so what you want right isto say okay here are these here's thischunk of like maybe body text has saidthat's it's split up by 512 tokens rightbut then I know that this is anadvertisement and I know this is a textbox and talking about something else andI know thatum you know these other elements aren'tnecessarily related to the topic of thisarticle right here that's corresponds tothis headlineand so you can just do a little bit ofwork on the front and like throw instructuredum so that you're not asking nearly asmuch of the llm and that when you'reready to turn the corner from chat yourdata to continuously update knowledgegraphs or automateum there's going to be a lot less noisegoing into that data itselfyeah it's super interesting and I reallyI do want to like so you mentionedknowledge graphs and this is probablylike thethe most use of structure is to you knowconnect each chunk to how it's likerelated to another chunk in this kind ofthingum well earlier I had mentioned thiskind of idea of just adding symbolicfilters to your chunks and then usingleviate to Surf through these symbolicfilters and then like it reminds me ofsay like the text to SQL kind ofresearch where you could have an llmgenerate a query that adds symbolicstructure to the semantic search querybutbroadly what I want to ask you about iswhat is your sentiment on knowledgegraphs do you think knowledge graphs areyou know like like if you're betting onknowledge graphs as being more or lessimportant as you know in the next threeto five yearsI think like what we see in the in thelast six months has been the fact thatlike chat your data has capturedeveryone's attention and I think thatthat's going to continue and it'sincredibly exciting what I also think isthat for a lot of business processes andorganization processesum you need a high degree of precisionin order to hand things off toautomationand that is in the realm of of knowledgegraphs and so if it's classification tolike you know in a or topic modeling onthe one hand to entity extraction torelation extraction among entitiesum and then and then more right thoseare still like there's still a ton ofmanual human work being done in thoseareas thatlike the previous generationtransform-based models were good at butreally expensive to set up and reallybrittleand that like you didn't see widespreadadoption of NLP across like Fortune 50Fortune 500 organizationsin large part because there's a hugeeconomic problem around it the economicsare changing in a really positivedirection butum but with it's still not a solvedproblem and so I think like we have arenewed opportunityum as as an NLP Community to go andattack that and to drive that value butthat value has not been delivered yet uhby and large right it's been it's beenprototyped and it's been described andthere's been lots of cool Twitter demosbut like there's still thousands andthousands of humans doing things thatthat model can do well and so now wehave a chance to go back and re-attackthat and so I think you're gonna havelike there's just you know thegenerative tasks and um and others wherechat your data or prompt your data isbetter but there's still a lot that likestructure like the creation andmaintenance of knowledge graphs arenecessary if you're going to hand it offfrom humans to uh to modelsI think um yeah so with with theknowledge graphs I I'm just reallycurious like umif we could unpack a little more of whenyou need to use a Knowledge Graph Idon't mean to be like because I'm justkind of skeptical I honestly I'm liketo me I think the best thing about aknowledge graph is that if we linktogether the how chunks are related toeach other maybe we can use some kind ofgraph neural network to kind of haveembeddings flow through the graph andmaybe get a more contextual sense of theembeddings that's more so how I like aKnowledge Graph can you tell me justlike about when you query like becauseyou have the the relational tuples likeConor and then you know all the thingsabout me says an example can you tell memore about like how you query knowledgegraphs and how that's useful in businessyeah I think um look there's events andthere's entitiesum right and the entities can beorganizations they can be skus they canbe um like uh supplier relationshipsthere's all sorts of things right andthen there's all sorts of um structureddata associated with that relationshipto um to meaning like what are keyevents that happenedum what are the transactionsum lots of things that are described innatural language that aren't that notnecessarily be in um in a structuredformat already that you need to goattach to that recordum right if you're actually going tohand things off and soum let me give you an example we weretalking with um an investment advisoryfirmum several months backand they advise like endowments andothers on investments and then and theythemselves are receiving every monthhundreds of limited part LP reports uhMcKinsey reportsum like some like Consulting reportslike all sorts of of data um some kindof in a gray area if it's private or notsome really private and they have awhole team of people that are justreading them and then just updating theminto a CRM so that they have like ahundred percent accurate information onany particular business entity at anygiven time to inform investmentdecisionsyes you can put an llm on top of thatand ask questions but are you we'regoing to trust it with a 100 milliondollar decision probablyprobably not right and so if you'regonna feed in all of those PowerPointdecks the McKinsey reports those LPmemos all that sort of stuff and youwant to create a hundred percentaccurate or 99.99 accurate records aboutcertain Securities organizationsum You're Gonna Want like very highquality Knowledge Graph associated withit even though you could chat your datahow the llms demonstrate that level ofperformance of the previous generationof Transformer based modelsum like it's going to be a challengeright and so that's there's that'sprobably the more common NLP use casetoday than Goldman Sachs wanting to chatall of their data across all of theirdivisionsyeah amazing I mean that example reallylike lit a fire under my ass of thethinking about I'm Goldman Sachs and Inow the like the hallucination thing ofyou give it the thing that hallucinatesso how are you thinking I mean that's agreat example of a hallucination casewhere you just absolutely cannothallucinateum maybe we could talk about this topicquickly about the Innovations andhallucinations like it seems like thereare some things you like as the languagemodels get better and better it'sbecoming less of a problem that maybe Icould just umwell I guess it's gonna do things likewith the knowledge graph thing it's likeis like do I need these Tuple like it'sentity relation entity this is how Ishould have my data structured in orderto hand it to the Goldman Sachs LM orcan I just have it kind of be Ascentlike a natural sentence that comes outof a retrieval engine to hand to thelarge language model yeah I think I meanyou deal with a lot of like type 1anti-2 problems here of like hey did Ium is is what's being generated anchoredon something real uh or on something youknow that that bears a significantresemblance or another thing is likeum is it missing something in thegenerative side right is there um anunknown unknown right and so um likeboth of those are are critical toactually saw like putting this intoproduction and so I think like by andlarge hallucination has is a lotless of a problem than it was than wethought it was maybe six months ago fromlike an open source developer Communityperspective from a productionperspectiveum this is an area ripe for Innovationum because we might be able to make surethat like what is generated is anchoredon something real but I haven't seen awhole lot yet around making sure thatyou have a totality of information of ofthe right information being rendered foryou and like look you can look at likethe you know like Google's llm versusopener eyes LM and some of the horseracing just in the past few days betweenthem on on what they're returning fromprompts from very similar prompts andit's still early days right and sothat's not to say that these aren'tincredible but like you know there's Ithink there's still a Chasm betweenpure like Jasper AI generativeapplications and replacing humans thatare doingum knowledge generation work right todayyeah amazing and yeah so I think it wasa really great coverage of all thesetopics I think what you've hit on withthe umparticularly kind of the visual documentlayout is what originally drew me toinstruction I think you have hit such animportant part of this kind of flow ofthese retrieval augmented LM Vectordatabase all the world that we're in andand yeah it was so interesting learningabout all how you think about all thesethings like the data connector uh likeyou know flowing data from say likeparquet into the Json as you describeall that is so interesting um before wewrap up maybe uh do you have any likeexciting upcoming announcements or anyuh tips and viewers for where to catchup with youyeah I'd say um you know we're in ourcommunity slack um welcome others to hopon in we have our our Engineers are inthere full time answering questions theones who are building that are answeringquestions correct and so we have anamazing teamum keep your um stay tuned for um somenew models to be dropping over the nexteight eight weeks we're going to have agood steady drip of those and justwelcome feedback we're trying to buildthrough real problems through real realusers and um and Leverage The Power ofthis open source community and soum grateful for any contributions orfeedback anyone anyone provides butConnor thanks for having me onthank you so much for your time Brianand yeah not to not to keep rolling outon the Azure but I as I did my list andthat that new when you mentioned thesuite of the new extraction models weare definitely going to be on top ofthat at weba trying to you know get helppeople get their data into using thesetools from unstructured and yeah thanksagain thanks", "type": "Video", "name": "Unstructured with Brian Raymond - Weaviate Podcast #48!", "path": "", "link": "https://www.youtube.com/watch?v=b84Q2cJ6po8", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}