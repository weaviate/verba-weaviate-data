{"text": "Brady Neal from Oogway talks with Connor Shorten from Henry AI Labs about causal inference and many more. See the ... \nhey everyone thank you so much forchecking out the seventh episode of thewe vva podcast today i'm joined by oneof the heroes in deep learning bradyneal who has made an amazing courseexplaining causal inference he's hadjoshua benjio come on his youtubechannel to explainthe latest ideas and things like systemone system two thinking theconsciousness prior and what causalinference might be adding to deeplearning brady is currently the founderof oogway ai a decision-making aistartup and he's also a phd student atmila so brady thank you so much forcoming on this podcast thanks for havingme connor so to kind of tip it off couldwe get into just right away what iscausal inference and what does it add todeep learning yeah so so cause ofimprint is essentially you're trying toinfer the effects of your actions whatare your actions causingand i guess the way it can be related todeep learning is so they're notnecessarilycompeting so you can use deep learningin causal inference there's a basicallyyou you might use machine learningmodels to help you do causal inferenceand deep learning models areour good kind of machine learning modelum and then there's also some insightsfor howcausal inference can help out withtopics that deep learning people careabout so for exampledeep learning people care a lot aboutout of distribution generalization or orcompositionality uhthat kind of stuff and and there's somecausal inference insights that that canhelp out there so could we start withthis idea of um of compositionalgeneralization and causal inference isthiswhere so say um we've seen things likedisentangled representations or sayyou're trying tohave that low dimensional vectorrepresentation be interpretable as highlevel variables and maybe you're tryingto drawcausal pathways between these high levelvariables is that kind ofthe thinking around what causalinference brings to that kind ofdimension of you can compose novelconfigurations of the high levelvariables and then having say theuh directed acyclic graph pathwayshelps you helps you like structure thecompositional testing is that the rightway of thinking about it yeah i thinkthat's i think that's very good way ofthinking about it so like one way a lotofpeople or at least maybe a few years agoyou would think about disentanglementwas this sort of statisticalindependence between these variablesright that means that these variablesare independent they don't they don'taffect each other at allumwhich iswhich is like kind of weird if you ifyou think about objects in a scene andyou seelike two balls and and one hits theotherthey it affects it it affects that oneum so yeah i think it it makes moresense to think about disentanglement asyou're kind of you're disentangling outthe factors in this causal graph so likeone ball and two balls in the scene thenmaybe there's like a background orsomething in the scene and then you canyou can draw arrows for which onesaffect each other and which ones don'tand such yeah but so yeah i think that'sa good view of disentanglement isfiguring out the causal graph and thevariables in the graphso do you think um so say with these uhgenerative adversarial networkarchitectures how we have a random noisesaying like the um the latest style ganframework they start off from a fixedrepresentation and then you have therandom noise that comes into the pathwayand it hits those intermediate kind oflayers is that a good example of saythese this unobserved phenomenon thatyou model in causal inference where saywith these randomized control trialdiagrams how you have someunobserved factor of variation that youkind of work into the model is thatsimilar to injecting noise andintermediate activations for deep neuralnetworksyeah okay so i'm not sure so so thisthis recent style again thing i'mprobably not familiar with it but i cani can imagine what you're talking aboutinjecting noise into layers of thenetworkuhthat almost to me is almost like youknow you have this function that you'relearningand you'reyou're like injecting noise in parts ofthe function that that's how like ithink of thatyeah so i don't know if i'm familiarenough with it but but if people haven'tlikeexplicitly stated that they're trying tocapture causal relationshipsand and stated it in more than a waylike an nlp you seethe phrase causallanguage modeling that doesn't thatdoesn't mean like what we mean in causalinference that just means they're theyhave like ordering tothe stuff but so yeah if they haven'tsaid it say it that way then i'mguessing they're not trying tocapture causal relationshipsyeah we are familiar with with the thisrecent style game thingcan we stay on this um this causallanguage modeling that you justmentioned seeing that so what it what isthe um so i guess the current thinkingis because you're using the leftmostcontext to predict the rightmost masstoken you say it'scausally the left is causing theprediction of the right but is thatmaybe not a honest interpretation ofcause of that causal phrase yeah well iget i get how people like people readfrom left to right so i guessi guess i can see how they're thinkingabout it as causal right like umas you're reading from left to rightyour brain might be predicting the nextwords or something andthose words that's predicting ourfunction of the previous words becauseyou're reading from left to right soit's like causing you to predict thenext words uh in your brainbutin terms of objects in a storyyou know it might be that the the causesmentioned later in the sentence wherethe effect is mentioned earlier so is sois that more akin to say like namedentity recognition not like named entityrecognition but that task ofuh assigning labels to each token so youwould say label this token as a causetoken and an effect token is that wouldthat be like a moreway to bridge the gap between nlp andcausal inference yeah yeah i guess uhyeah that could make sense you train anamed entity recognizer thingtoyeah to get the cause and the effectit's umyeah and then you have a data set rightwhere you try to where you try todo that that could make senseyou would have to have a model thatlooks both ways right you wouldn't youactually wouldn't want to have a causaluh language model i guess right youwould want it to be able to have thefuture and the the past context and thesentence so once you have the cause andeffect labelingshould youmap these tokens into one of thesesymbolic dag graphs thatlike would distilling your model'spredictions into one of these graphs besomething that can maybe facilitateexplainability and then want to go andget into counterfactuals next but do youthink that kind of distillation betweenuh maybe if you first fit your trainingdata set and then uh on your maybe evenjust within the same training data setyou go fromthe cause effect labeling and youdistill what it's predicting or what therepresentations it's learned into thesesymbolic dags and then the dag is thething that yourun inference with because you can lookat it and say okay i can see thepathways that are causing thisprediction yeah so so i actually haven'tthere are people who do researchon umcausality in textum buti'm not that familiar with it and ithink a lot of it ends up being likesort of propaganda umkind of research right like howeffective wasactually yeah i don't know i'm nottotally sure on that stuff but uhbut yes you could you could take textand then try touh figure out what are the variable likethe relevant variables that youthat make sense to attend to and thenthose are your variables in this causalgraph andand you could model causal relationshipsdescribed in the texti'm i've never done something like thatbefore but i could see someone do maybethere's a paper that does that kind ofstuffin terms of explainability like causalgraphs i definitely think aidexplainabilityyou know like any time someone's askinga why questionumyou you could potentially reframe it aslike what causedthisyou know there's a why this well whatcaused this and umso causal grafts you know you can havethe the thing that you're trying toexplainand then there's these proximate causesthe things that are closest to it thathave caused itand thenfurther down in the causal chain there'sthings that cause those proximate causesright and so the the freight i'm justusing uh terminology here in case peoplewant to google it but there'sapproximate and then there's likeultimate causeswhere umit's essentially that's the stuff at thevery beginning of the call of chain butyou know usually you can take causalchains very farfar backumand yeah souh i kind of lost where i was going withthat oh explainability right it wasunexplainability and so you can discussexplainability at various layers in thethe graph you know you can discuss itwell here was ten days agouh versus here was like one day ago sosay you're trying toexplain covid covid stuff you know thenthose time ranges uh there's differenttime ranges that make sense to look atand stuff so i've seen a lot ofinteresting things on say unifyingstructured and unstructured data wherewhat you do is say you go through thetable and you parse it into naturallanguage do you think going through soyou learn you create some kind of causalstructure with the approximate andultimate causes you think maybehaving some kind of parser that turnsthose into natural language phrases andthen just go ahead and language modelthose phrases do you think that kind ofdistillation of from the causal modelto thelanguage model could be a useful path sois it uh let me make sure i try tounderstand so you start with a causalmodel and then you want to produce somelanguage thing what is the point oflanguage thing is it to like explain thecausal model oryeah so so you maybe um traverse it andthen annotate and thensay like um x had this impact on a whichcaused this impact on b and you but thenyou phrase it as a natural languagesequence and just add it to the trainingdatai i see so you have someone cause amodel and you could potentially generatemany different sentencesthat follow that model like uh rightlike these it's like having a grammar iguess and they're generating a bunch ofstuff that follows the grammar yeah thatmakes sense and you could use that as uhas as data so so in this kind of thingwe're talking about now i think mostlyum trying to understand how we go frommaybe this kind of idea of observationalcausal inference where we try todiscover causal structure in say aninternet scrape of text data and thenthere's kind of like i guess theexperimentation side of causal inferenceand you touched on that quickly thisidea of text is treatment and i've seenthese papers where say umlike the propaganda studies wherethey're they're studying how differentways that you umthe different styles of text that youuse to present news the the impact ofthat is a treatment so it's like a btesting but directly using text andrepresentations of textto be the uh to be the treatment haveyou seen these kind of studies and doyou think that kind of stuff isinteresting yeah yeah there's a word forthis right like there's some phrasewhere it's like you frame it uh this wayand then peopleoverwhelmingly answer yes and then youframe it a different a slightlydifferent way but it's the same meaningand people overwhelmingly answer no ifigure what the phrase is for thatthere's some term uh in like uhpsychology for thisanyway but yeah that stuff's reallyinteresting right they're essentiallydoing an experimentwhere what they've held constantis the meaning of the thesay they're doing a survey they've heldconstant the the meaning of the surveyquestionum which is like controlling for factorsright causal influence roughly you couldthink of as justcontrolling for factors you've peoplehave probably heard that phrase beforemaybe doing it in a fancy way but it'scontrolling for factors so they'rethey've controlled for the meaningthey've held that constant and thenthey're just changing the phrasing sothe idea isjust the phrasing is enough to have verydifferent impacts on people's uhpsychology right their understanding ofthe question or of the statementso so with that idea of trying to youknow use the causal inference to takeaction and use a particular kind of textto influence people or whatever whateverthe um the setting is what's thedifference betweenstructuring that as a causal inferenceproblem or structuring that as areinforcement learning problem yeah wellsoso i haven'tthought a ton about the relationship buti've thought a little bit right soreinforcement learning you're you'reessentially trying to figure out whatactions you can take to maximize rewardum you know whatever or utility whateveryour uh the thing that you want rightand in causal inference you're kind ofdoing the same thing right you have someoutcome that you care about and you'retrying toget the best outcome you're trying toyou're trying towell if you're doing it prescriptivelyif you're doing causal inference decidewhat action to take like what decisionto makethen you're trying tofigure out whichuh action take to take the best out toget the best outcomeand i guess it's just that inreinforcement learning it's moreyou have all these trials where you canlike experiment a bunch maybe incontinual learning where it's like onlyoneone go right you when you die it's it'sthe end it's it's a little bit differentbut in reinforcement learning in normalyou have so many trials you can dieinfinitely many times so you can reallytest out like everything given enoughtimeand so i guess causal inference youmaybe you could view it as liketrying to be a bit more efficient withthe actions you try out and how what isthe information you're using that usingto do that it's likethinking about the causal structure ofthe problem what are the factorsthat could be confounding ummy actions and outcome and and that ineed to control fortoget a better guess at what action mightbe the best actionat a given point yeah it makes me thinkabout say model-based reinforcementlearning where you're trying to learnthe transition dynamics as well as howto act in itandi think maybe with the causal structureto learning yourworld model is more uh like you have theexplainability you have the high levelvariable compress compressionbut then you still kind of need to gofrom the high dimensional observationsinto the high level variable space rightso maybe things like self-supervisedlearning and how they say one paper iscurl where they do contrast toself-supervised learning with theobservations to try to facilitate thatrepresentation and then from there wecan maybe construct thesevariable models for our dynamics and ourtransition models soonce you have thesetransition modelscan you justunroll a counter factual by justyou know the proposing some kind ofaction or is it because it's out of thedistribution space that the model isjust gonna compounding error and maybecausal inference adds something to thatidea of counterfactually querying yourlearned transition model yeah so kind offactual share being likeum what wouldtheor or even maybe potential outcome is ais a more intuitive word in this contextlike what would thetheresult be if i were to take a givenactionyou know the potential or what would theoutcome be what was the potentialoutcomeso yeah if you have a perfect model ofthe world you can just run the dynamicsof that modeland change the different actions andyou'll see which actions will produce umbetter things and that's like that'sbasically what reinforcement learningdoesyeah when it's run in a simulator wherethe simulator is taken as the as theworld they basically have that umyeah and then counterfactual just toclarify some terminology a little bitlike when i use counterfact people useprimary engineering differently when iuse counter factual it's likeso you start off with two potentialinfinitely many potential outcomes asmany actions as you have that you have apotential outcome for each one of thoseactionsand then once you take an action like umuh say i take a pillthen i realized that potential outcomethe outcome when i take the pill andthen the other action like don't takethe pill becomes a counterfactual rightso uhit's counter to fact i can no longerobserve it unlessunless you have a model of the worldright so we actually uh i have a blogpost on this soa blog post and a paper so you um if youhave the the model of the world then youcan observe all counterfactualsbecause you just run through your modelyeah i blog post on this and thenthere's a paper called real causethat helps people essentially observecounter factuals and there's there'svarious work where people use deeplearning to try totry to get at counterfactualsso maybe from this um one of the earlyexamples in your course of uh sleepingwith your shoes on and that leading towhether you're woken up hungover or notor whether it's really because you'vebeen drinkingif if you unroll those into sequence soit's like several of these decisions canyou take me through what it takes togeneralize that kind ofthinking intoyou know you have five potential likeit's not just shoes or drinking it'si don't know whatever whatever leads togoing out like call from your friend orsomething like that yeah okay so you'retalking about a sequence of actions andthen the okay so say you have a sequenceof actions and then the outcomeis uh whether you wake up with aheadache or notuhyeah yeah soumso for peopleunfamiliar with this example it's justlike the idea is that you you noticethat umpeople who go to sleep with their shoeson often wake up with headaches soit must be that uh when you sleep withyour shoes on they cause you to have aheadachei like amusing examples is why i chosethat example but but you know it'sreally just there's some confounderuh that's causing both of them which islike you went to sleepuh drinking and so okay so you'retalking aboutthat's one action is a go to sleepdrinking but maybe there were actionsthat preceded that oneyeah soyes you could think about essentiallylike like the picture i have in my headis it is a tree rightum like weight but y has this nice treediagramwhere so like this is you at a givenpoint in timeand thenthis is one path of actions that's whereyou would end up this is another path ofactions another pathand you can view it as like a tree ofmany paths branching out every time youtake an actionand thenso in that example you kind offollowed one paththat ended up at going to sleep drinkingbut you could you could follow otherpaths thereum and eachyou know it's like the butterfly effectthing right where uhone thing a long ago could have aoutsized effect on let's saywhether you wake up with a headache ornot right like umif you decide totake a nap in the middle of the day oror maybe no maybe that's a bad onei don't knowanyway uhif you get put into a coma right thenyou're not gonna wake up with a headacheuh because you got put in a coma a yearago and it's like a long coma so uh sothat kind of reminds me of say the umlike the mu0 monte carlo tree search ofrolling out several pathways and thentrying to pick from the pathways andto me that's kind of an interestingexample of a neurosymbolic ai systemwhere the monte carlo research rolloutis like a symbolic program that youlayer on top of yourneural program do you think causalinference is like the way to bridgetheseneural predictions with some kind ofsymbolic reasoner and is that kind ofthe maybe the best way to beapplying it in with the current state ofdeep learning tools umi think that if you ask a lot of causalinference people uh they'll say yesbut you know pete whatever people aremost familiar with is the is the toolsthat they want to useso i mean causal inference is useful forumjust when you're trying toinfer effects of actionsandyou don't already know those effectsright like if you have a simulator or aaccurate model of the world which islike very hard to have right but if youwere to have that then you don't needcausal inference really or i meanyou could say that the causal inferenceis justrunning thething through your world modelthat's that's you doing causal inferenceand you've kind ofyou've kind ofpre-done all the work of adjusting forconfounders by building that world modeli guessumbut yeah so so always the question isshould we usecausal inference to build up that treeyes so the question is just kind ofabout this again this bridging betweenjust neural predictions and then somekind of symbolic program and i'mthinking about whether it's about uhjust this general concept of distillingyour representations into this kind oftree soi don't know whether you can and andthen the the connection of because youknow the um the connection between a tob or like did i drink beer last nightdid i uhuh take a nap in the middle of the daywith these vectors it seems also kind ofhard to connect themi'm kind of just trying to imagine thishow to how to construct these causalinference pathways from these vectorsand then how to kind of traversepotential causal pathways when you'retrying touh yeah like trying to make an inferenceon your learned world model or you'restill kind of building it i guess yeahwellokay so i'll try to separate a fewdifferent parts of causality outsosay youyou know you a causal graph rightvariables and arrows connecting thevariables saying which variablescause each other that's the kind ofpicture to have in mindum learning the edges or the arrows ofthe graphlike what causes what is called causaldiscovery and machine learning peoplehave been mainly doing a lot of the workin this right but it they've been doinga lot of workum whenever you have experiments itmakes it way easier whenever you can doexperiments likein gene regulatory networks right sogenes interact with each other toproduceto express themselves in youruh phenotype you'd write like uh justhow you look or whatever right somethingthat actually comes out into the realworldand uh that people do tons ofexperiments there that helps a lot withdiscovering which genes cause each otherokay butstarting with like implicit in that wasthat we already know what was a genelike we knew the variables in the graphand so the keyword thereis kind of new so i call it causalrepresentation learning there might beone or two other names for itumthere's definitely one or two othernames for it like uh eberhardt is thelast name of like one of his students isone of the first people who startedwriting stuff on thisum that's like learning the variablesrightwhich you could think of as thedisentanglement problemif you think about it from causal graphand um yeah so i that's a that's like anemerging area of research mila there's afew people at mila who are actuallydoing a lot of work on that or they usedto be amelia when they graduatedand umyeah so that's something i'd recommendpeople doing research interested in likea frontier of research they should checkthat out but essentially you could yeahyou could learn that whole graphbut then there's not just the variablesand the edges it's what are thefunctions that map fromvariables to edges right so the causalmechanismsis what they're calledand that's where you know like it's niceto use deep learning right so it's likeyou essentially fitfor everyfor every node all of its parents in thegraphthat's a functionfor mapping that's like the inputs tothe outputand you can fit deep learning modelsthere and stuff and and so essentiallyyou could learn all of thatlearn causal representation learning soto learn the variableslearn theedges and then learn the mechanismsand then that would give you a worldmodel essentiallyand that you could run anyquestion through like any uh kind offactual and stuff if you have all of thevariables right so then that's where noyou you end up having like noise so youyou model some stuff as like this isunobserved noise umideally there's not confoundinglike unobserved confoundingthat's that's where uh things get toughto do causal inference when there'sunobserved confounding so i want to geta little more into unobservedconfounding but but quickly just tobecause i need this understanding ofis what's the difference betweenmodeling the parents as a conditionalprobability distribution compared tothis uh do calculus idea yeah so forthese causal mechanisms you could thinkof them as just conditional probabilitydistributions or you could think of themasconditional probability but you put theword doin one of the find the conditioning barfor some of the variables right and dothere is meant to meanwhat would happen if i were to interveneon this variable like i were tophysically set it to that thingand okay so how is it different from aconditional probability distributionwellin regular distributions you're notintervening on things you're just givena data set you're just observing somedataand then you canyou know through like bayes rule orwhatever probability stuff you cancompute conditional probabilitydistributionsumright so it's really like the dothe do operator is the more intuitivething i think to humans uh we thinkabout intervening on things and it'sactually the conditional probabilitything that'sum less intuitive like that's wherepeopleuhthat's why confounding is so importantbecause it it willconfounding will affect conditionalprobabilities like they won't dowhat you intuitively think they do theconditional probabilities but then umif you have a a do behind the dooperator like do change this action tothis action it will do what youintuitively think it do like it'll bemeasuring what would happen if i were totake this actionand and so do calculus i mean is a verycomplicated thing right so do calculusis likehow do you get from conditionalprobabilitiesto putting do operators in there youknow because it's like well all weobserve is conditional problems we justobservedata we can't say we're not allowed totake actions how do we get tothewhat calculations do we need to do toget to the things where we're saying ifi were to take this action this wouldhappen and that's where do calculuscomes inthat's where causal graphs come in totry touh kind of inform how you would do thedo calculus it's just like a sequence ofoperationsumyeah do calculus is very fancy so it'slike that's the mostuhgeneral way to adjust for variablesso you it can produce many differentalgorithms do calculus canso at a at a basic level would i if sayi just introduced a second conditioningvariable like a binary do variable andi'm conditioning on that additional zeroone do variable and maybe thatconditioningcould be injected and say like uh thegain and the scale and shift parametersof batch normalization to liketo like shift the distribution such thatyou kind ofknow that you're um doing and and havemore layers of control integratedwhether it's like other kinds ofnormalization schemes to just let thisdo conditioning variable have morecontrol would like alike a binary do conditioning variablewould that be something that would maybebe useful for say like umsay it's like a reinforcement learningagent and you're trying to again learnthis transition dynamics model as sayyou're just um in like an atari game andyou're saying i'm gonna dohit left for like four turns in a row tosee what that how that changes my visualobservationswould that additional conditioningvariable be enough tosort of simulate the impact of ofdeliberate actionyeah well so if you canactuallyyeah and and so it does get a little bitconfusing and do calculate or in judahpearl's notation right the guy who madedo calculus with his with his studentsumit does get a little bit confusing thathe uses do umbehind the conditioning barbut you could think of that as like atotally separate probabilitydistribution like think of the the dothing as in like the subscript of theprobability distribution that's that'smaybe a more umnice way to think about it and so it'slike they're separate distributions butone iswhat you would get if you just observedthe distribution you have right away andthen i think the thing you're talkingabout is well what if i go and changethe program or the simulator or whateverand i add in thisthis parameter that allows me to tochange it to um like actually i canactually intervene on that parameterand those are like two differentdistributions right so so one is likewhere you haven't done anything withthat parameter another one is whenyou've changed it toto zero or you've changed it to oneum well maybe one of those is the sameas uh when you haven't done anything butthose are basically differentdistributions and they reflectwhat would happen in under two differentconditions when youdo that intervention or you don't dothat interventionand so when you can do that then causalinference is easy right all you dois just run the experimentyou know you don't have to do anyadjusting for anything becauseyou have access to the the world modelessentially and you can you can just runthe experimentso in another kind of sense of theseworld models and kind of transitioning abit to sayso say withcoming back to text data say we knowthings like negations if you insert anot it should flip the label orsay named entity swapping if you saysome specific fact about chicago and youswipe chicago with dallas you know thatthe faction like the question answeringsystem shouldn't produce the same answercan we use these kind of dataaugmentationinterfaces as our kind of interventiontool wheresay our simulator now has the knowledgethat if youinsert negations or if you swipeentities and the kind of things that areincluded in say like alike a checklist kind of system fordeploying an nlp model and justbenchmarking its robustness can we usethose interfaces toexplore causal interventions in ourin our text data okay so you you have alike is it like a grammar where you canput a not in front of things and thenthat produces a different sentence or itproduces a different meaningyeah and i think that's that is a goodway of thinking about it just like thesegrammars and just the formal languagesand the kind of idea that you have thiskind of precomputed dictionary of how itshould flip the labelbut i guess that would be abstracted tothe agent so the agent doesn't see thisgrammarit just starts uh it just startsperturbing the original text sequencesand the grammar flips the label whenit's when it has flipped the label so soyour agent isn't exposed to the grammarit's but the grammar is doing the labelswitching that the agent is canhopefully do the causal discovery of therelationship relationships between thewords becauseas it say inserts not and then thegrammar flips the labelmaybe that could inform it onthe relationships between the whole partof the text sequenceso so the agent is allowed to say likehey i want to put a not here and thenand then it can observe what the labeluhyeah like that kind of strategyyeah well so basically it's like umagents allowed to run that you're givingthem access to a certain kind ofexperiment right like where they canso so they're always observing this thislabel and then you're giving them accessto a kind of action like where they canadd this not not to things and then seewhat happens yeah and soif the agent doesn't know the rules ofthe grammar uh it needs to learn thosefrom taking actions rightuh if you could just tell it the rulesright you could encode it that that'snot the deep learning way but uh if youwere to tell it the rules then it wouldhave that knowledge too but um but yeahi can learn it from uh experimentsand that's uh just generally like togeneralize a bit like that is a thingwhere umyou're trying to learn causal graphsor you know or something else well yeahdoes it say causal graphs fromperforming experiments and people comeup with algorithms for thatyeah so like in reinforcement learningyou're trying to learnyou knowwhat what actions to take to maximizeyour your utilityuh so it's a little bit different likethat the objectives of the thing butthen just to come back to somethingearlier it could be that learning thecausal graph like this is kind of one ofthe hypotheses of model-basedreinforcement learning is that learningto cause a graph or some some worldmodel actually will help you uh withyour utility objective whichas humans feels like intuitive that thatthat should help right it's but it mightbe it's hard to engineer that kind ofstuff so one other topic in kind ofcoming coming away from uh the worldmodels and data augmentation and theidea of learning interventions andproducing counterfactual predictionsthrough these world models is i wascurious about this idea on whether youcan use causal inference to distill thecomplex pathways of these giant deepneural networks isis there a way that we can kind of wehave say 20 layers of computation andyou know they're wide and there's allsorts of things going on could we kindof do causal structurally causalstructure discovery within theseactivation graphs of these big deepneural networks to kind of compress itintolike symbols that kind of explain whyit's making its predictions yeah i meanuhi don't know is the answer right but uhlike that certainly it would be a coolthing right so people who say they needto be able to understand the neuralnetwork betterand they don't they're noteither they don't know about or they'renot satisfied with just like perturbingthe inputs and seeing how it you knowsensitivity analysis preterm inputs andseeing how it changes the outputsum like umwhat are the there's a few tools a fewtools that do this uhi forget what they're calledso it's like for any black box functionthey'll do a sensitivity analysis andtry to give you some interpretableresultsdo you know the things i'm talking aboutuh i've heard of a technique i think themost famous one is called limeyes lime and then shap and uhyeah okay yeah so those things so ifyou're not if you're not satisfied withthose things right but you want to getyou you essentially want to like heyhere's a neural network it has all thiswe've taken all this information fromour data set and injected it into theweights of the neural networkum great now the neural network's usefulbut like i want to understand umi want to you know i see the data and isee stuff now i want to go and look atthe neural network and see that samestuff like pull the objects out of theneural network and stuffum yeah i'm not sure if someone's triedto do something like that but yeah maybeit's like rather than learning doing repcalls or representation learning on adata set you're doing it on a on aneural network right likeyou could view the two as similar rightthere you're basically trying to pumpinformation from your data set into yourneural network over the course oftrainingyeah that'd be cool i i that would be avery cool thing to to doit sounds like a cool idea yeah thatkind of idea of like abstracting it intoit's you'relike say the teacher student and thestudent is like the inference networkand the teacher has like informationabout the student network maybe it'sused to train it has been the mostcommon thing like meta pseudo labelsteaching with commentariesmassive lists where the teacher islabeling the data oraugmenting it or doing something but theidea that the teacher has thecompressed causal structure of thestudent and is used for that kind ofinterpretability so i think we'vetouched on a lot of interesting topicsaround causal inference and i think wecould go on and on about all thesedifferent things and thinking about whatit might add to deep learning but i'mreally curious about oogway ai can yougive me the overview of what it's aboutyeah well and so to draw a line fromcalled inference to boogway so to uglyis about helping people make betterdecisionsand the the line there is is i thinksomething that i mentioned earlier it'sbasically justumthe decision that you want to make is isone that will give you the best outcomeum well at least for meumand you know anyone could say what theiroutcome is right people could havedifferentideas for what is the bestand um so causal inference helps youfigure out what will what action willlead to the best outcome rightanyway so yeah so oogway is about uhhelping people make better decisions andand essentially like so so where peoplehere there's like a an app for umi guess a consumer appthat we're building that will help withthat and and uh it's supposed to be forall decisions right so i know thatsoundslike academics will use the wordambitious right which is a which isnegativei think to sayum but yeah so so we have some ideasabout how to actually uhmake this feasibleuh which i can get intoand uh but one other cool thing aboutoogwa is just where umlike i call it company 2.0 i thinkthere's going to be a next generation oforganizationscomingwhere you know basically when the theconcept of a company was invented idon't know like 1700s or 1500s whateverthat wasyou know we could kind of take it forgranted now but it was a reallyuh valuable concept that produced lotsof value for the worldand i think that it might be the casethat we're getting to uh the nextlevel of that conceptlike say company 2.0 which i can talkmore aboutand so uber is trying to be one of thoseearlyorganizations that are like that andthe idea is likeit would be cool if everyone could feellike they're not wasting their time verymuchuh you know they're trying to they'reactually like maximizing the amount ofvalue they're they're producing um fromtheir time maximizinguh their interest and what they're doinglike people who want to learn new skillscan go and learn new skills and uhand just generally like people care muchmore about empowerment in our generationthan like one or two generations ago orsorry fulfillmentuhbeing fulfilled self-actualized soi think that there are some tools thatcan help a lot with that in this nextif this is the next generation ofcompaniesand so who is trying to to have all thatstuffuh you know you can't maximize all thosethings but you know there's somesome trade-offs of course but uh yeahwe're trying to that's the the goals socan you take me through kind of like theengineering architecture of combiningthese different tools like combininguh like a causal inference part of itwith say the we v8 search would say thehugging face models and is that the kindof the architecture is it's like arouting through the composition ofdifferent uh like ai as an api kind ofthings okay so the yeah so thearchitecture for this in the the firstimage here basically you can see thatthere's a a consumer on the left andthen there's developer on the rightand essentially you can think ofdevelopers asputting all these apps into a sort oflike app store like thing uh i call appstore 2.0 for for similar reasons ascompany 2.0and and basically the user is going tosee some unified interfacewhere they're routedat any given time they're routed to thesort of ai decision app that is mostrelevant for their given decision rightsoyou could start off with let's say icared about umdecidingum what restaurant to go toyou could have an app thatfirst is just a general search app rightso it's kind of horizontal so in the umin thesecond image hereuh it's horizontal in the sense that itworks forever everything like google isa general search appand then you could later on be routed toa different app that's say focused onon restaurant decisionsafter you it's gotten the informationfrom the searchand yeah so so different apps here andthey can have different scopes so theycan be across everything like horizontalor they can be narrowly focused to justoneone thing and um the reason that wedecided to do this so initially we werelooking at likeinitially the plan was like hey ai isgetting way better and betterum you know gpd three's got this coolstuffand so we you know we tested it out andit's like okay it's coolbut it's it's kind of like a noveltything wherepeople like it initially but then itdoesn't really solve their problems wellenough to be a good productand and so then we started developingmore specific stuff you know thatinvolves actually fine-tuning the modelsonce you fine-tune them you can getpretty good stuffbut then that takes developer timesothe the kind of realization was likeokay to get good enough quality for alot of decision apps you have to havesomenon-zero developer timeand so we um so that's where okay let'stry this app store modeland um over timewe can automate like more and more ofthat it can be so that it requires lessand less developer timebut then like does that look attractiveto developers you know if um they'rethey're gettingthey're they're spending less hours youknow they might be worried aboutcompensationso that's where it's like well you haveto compensate them in equity umlike ownership of the company right sothenas ai gets betterit helps oogway be better and helpsincrease the value of oogwaybut because the developers were at leastpartially compensated in equitythey're kind of hedged against theidea of like ai making ittake less and less time to develop theseai appsumand also you know it's like okay wellthat you need a hugeuhdeveloper supply to do all that stuffright to cover all decisionsand so the insight there for like whythis is kind of good timingisis i think thatthe ai developer supplyis kind of expanding to thesoftware developer supply which is likea larger supply rightas we get tools like hugging face andand haystack that helps helps withsearch and you know open ai as we getmore in tools like thatum it just makes the barrier to entryfor ad development much easier and kindof can expand it to general softwaredevelopers which were which we'restarting to see a bit of oogway withlike people who are not ai expertsdeveloping ai stuff too yeah i thinkthis is an extremely exciting idea so tofirst let's try to like just to kind ofunpack each thing thoughthe first thing i want to talkabout is some so these big models likegpg3 and maybe we can use prompting totry to get it to do more tasks or we cansay what they're really doing is themixture of expert style of say like thelatest glam model with the trillionparameters from google it's looking whatlike what they want to do instead ishave kind of sparse routing activationsinstead of one massive densearchitecture and so something that i'vebeen really curious about and it lookslike you're already set in motion ondoing this is applying that kind ofmixture of experts kind oflayer on top of say the hugging facemodels api where uh i think they havelike 25 000 models that are probablymoreactually i don't know i think probablyaround 25 000 but the idea of likeof having a layer on top of that thatlearns how to route classifications intothe hugging face model like the hostedmodels and then and then that'sprobably maybelike maybe is that more powerful thangpg3 or what's the comparison of thatkind of idea compared to prompting ingbt3 yeah yeah that's really cool yeahwe call it the routing team at uh at ubiright so likewho's in charge of like routing stuffuh yeah i mean it certainly seems thatright like take themacaw fromthe allen ai institutethey're supposed to be gpg3 at kind ofgeneral purpose question answering socertainly if you and like that's onhugging face rightum soyeah so that's just one example of whereif you routed everything to gp3 butquestion answering stuff you write it tolike macaw say then uhthis is one example right there'sprobably other ones then it should bebetter than gpd3 so so yeah so somethingthat i'm really excited about about uhbuilding out at wev8 is our uh vectorsearch demo api so right now we havewikipedia wikidata and um and newspublications and i'm working on addinguh the keras code examples and chaos apireference as search apis so similar touh querying a model and just getting aninference on and and we have say the t5that unifies every task supervisedlearning task into language modelingkind of so you can have this oneinterface for querying the 25 000 i haveno idea if that number is correct butthe x amount of models that are onhugging face similarly how does thatgeneralize to say not just querying amodel for uh for its inference but forquerying a search engine where eachsearch api you can have the differentretrievals so you can have tfidf bm25 uhexpert different ways of getting therepresentation from sbirt you can haveprompt burt so you can have like thisstack of retrievers the stack ofre-rankers and further processing andyou can have symbolic filters into theretrieval so how does the transition ohsorry this is likegoing off a little bit but thetransition from a mixture of expertsover the abstraction layer on top ofmodels to the abstraction layer on topof search apis okay so so it's you'retalking about umthe things that you're retrieving aremodels from hugging face right yeah ithink that's another way of looking atit is you could you could structure uhthis task as retrieved and read whereyou're retrieving hugging face models asand then read and then re-ranking themodels further like with that kind ofcoarse-grained and then fine-grainedstage one stage two processing pipelinesand that's definitely another reallyinteresting idea i guess what i'mgetting into more is so like the idea ofsay uh information augmentedmodels that uh query like documentindexes so like say the chord 19 dataset from the allen institute where theyhave 140 000papers related to cova 19 supplementingit with that knowledge store so so theidea of the abstraction layer on top ofwev8 vector search apis is you have thechord 19 corpus you've got say archivepubmedwikipedia reddit conversations twitterand they're all in separate search apisand so you'reyou're the orchestration layer is goinghow do i want to route my queries tothese different information sourcescompared to these the other idea whichis these different uh likesay like compressed brains for the modelinference so okay so you're talkingabout um like you're doingyou knowuh like a search engine right um wherethere's various components right there'sretrieval re-ranking and stuffum but but the routing is happening tothe to the data sets right so uh ratherthanjust within one data set you're rightyou're essentially finding thethe relevantdocumentsuh there's a there's a layer before thatright that's that's uh writing to thedata side okay so sorry i i think i ithink i understand that okay so as wethink about building these like metaabstraction layers that orchestrate alike it's it's so it's sort of likeinstead of trying to put an entire datadistributionin onemodel we simply we break up theinformation sources into different apisand then the oogway layer isyou know is the thing that routesthrough it and says here i've went andsearched the these specific knowledgesourcesand this is what i've produced kind ofso there yeah so there's models but thenthere's also information sources rightandso i guess the way i so i haven'tthought a ton about this right and it'slike not that i'm the main one who willbe uh implementing the search stuff butthe way i thought about this was thatthis is like the searchum apps job right like that they'redoingumthey're doing search right like like umyou know trying to do google like stuffand you know because google's trying todo this stuff really well right umand you know and probably succeedingreasonably well like they're they uhit's probably non-trivial to improve ongooglebut yeah soall this stuff about like routing todifferent information sources in my headi think of it as like um one app asearch like a search app but maybe itcould make sense to think about assub apps maybe right like i'm not sureum but but yeah that's how i think aboutuberway and it's like the search stuffis then just like one step in thedecision making pipeline like oneflow of that you might do in decisionmaking is i search for stuff i filter itdownand then i have some options and ievaluate the options right like that'sone exampleum and and that in that example searchis kind of like the first stepso like a common thing with google ispeople will go to google and thenthey'llthey'll click into the first linkthey'll skim through it click outclick into the next week skim through itclick out right and there's this likeprocess ofdownloading tons of information intoyour head umthat you think probably you coulddownload this information faster or youcould be targeted to this informationfaster and that's all about improving uhsearchit's just getting that informationbut then you have to likegrapple with that information so ifyou're looking at products to buy youhave to thinki need to remember oh what was thatproduct i liked that i read in the firstlink when you're reading the third linkand you're trying to evaluate them butyou have to click back and forth betweenthe two that's because google is focusedon search so you could have aan interface that helps more with thoseother stepssorry that i kind of kind ofyeah went on a tangent after answeringthe initial thingso i guess the kind of the analog i'mtrying to make is say um we we went onthis transition where you compared gbt3to say a mixture of experts over hostedmodels on the hugging face model hub andthis idea of a mixture of experts is soexciting because this attentionoperation it's differentiable itperforms this differentiable routingthat is causing this boom and mixture ofexperts i'm trying to make the analogthat say from gpt 3 to the model hub thehugging face model hub is equivalent togoogle to these we v8 demo or thesesearch apis that we're trying to buildwith wva where you have a collection ofdifferent information sources and soyou're routing through how you want tooccur these different informationsources compared to the tasks similar togpg3 of trying to compress the in allthe information into one api whichwhich is maybe too grandiose andrequires this i mean i know i guess likethe point of google is doing this kindof orchestration layer buti think asmaybe as you kind of fine-tune it fordecision making and for personalizationit has a different kind of flair to itbut so one other question i want totouch upon and as i'm looking at thisarchitecture is this human in the loophuman computer interaction and we'veread there's a lot of really cool papersthat are coming out about how people areusing writing assistants chat bots tohelp them write what kind of specificthings they're usingso what kind of like how does humanfeedback kind of play into this loop forrouting between the different ai apps aswell yeah well so we think of oogway isit helps people make decisions itdoesn't make the decisions for themmaybe in the future it'll be a subset ofpeople who or for a subset of decisionsthey wanted to be made for thembut that's not really somethingwe have in our sites right now uhor like maybe in the future you wantsomething to like automatically scheduleyour calendar for you or or somethingand you don't have to worry about itlike i think there's actuallyan app called motion i think that that'sor a company called motion that they'retrying to do thatbut yeah so it's just helping the personright so for example i'm trying tofigure out where to move to right nowand i care a lot about uh climate stuffright so i look at maps ofof um sun like sunlight hours uh so it'slike a us half an inch in it map andit's a heat map of the us for sunlighthours and then i do the same thing forlikeuh snowfall like i you know i want somesnowfalland for the maximum temperature in thewinter in the summeri don't want it to be too hotso for user like me the idea is like itgives me that informationum and then maybe i could also tell itlike okay i've identified that i havethese three constraints that i wouldlike or these like 10 constraintsand then you know like that i want thissnowfall above this amount i wantsunlight hours above this amount i wantmaximum summer temperature below thisamountand it couldit could also take that and say okayhere's a place that satisfies thosethis is like for me for choosingwhere to live but for other people itmight be different they might wantdifferentuminformationor and they might have differentcriteria but the idea is that uh theycould they could give the criteria um tooogway and then oogway helps them seehow the different options rankaccording to that criteria so like acommon thing ifthat people might do is they make thesetables where they have the options inthe rowsand then their criteria in the columnsor a transpose of thatumyeah and that's like a very simple thingthat you could think where it's likewe we could build those tables for forusers umif they want those tables right noteveryone wants those tables so it mightlook different for themsorry i think it's going to hit themso i'm thinking about this this kind ofidea of this application of trying tofind the perfect place in let's say uhlike the united states that you want tolive based on the climateso i'm thinking about this routingbetween these different apis and so withthe we va search engine what you coulddo is you could say one of these apis isum like all of reddit data and you cangive it the symbolic filters thesymbolic text filters uh that discussclimate and that are discussed in theunited states but maybe you want tomatch those regular expressions for thesymbolic filtering for the retrieval andthen you couldprepend like a chat bot that takes thatkind of informationand is maybe personalized to this useris that kind of the thinking of justlike so you take the the we v8 redditapi to retrieve information about thisparticular kind of idea of umyou know i want to have a some sunlightsome snowfall and then you have somekind of maybe likea reasoning model from the hugging facemodel hub that maybe has beenfine-tuned on some task of in-domaindata where about weather conversations idon't i don't know if such a data setexists but is that kind of the thinkingof routing between these different kindof likelike open source ai's api kind of toolsyeah um yeah so i think that soundsright like the um so the user might betaken to like a reddit conversationthat's relevant for them is that thekind of thing you're thinking ofwell i guess so i guess what i'mthinking is we're decomposing this taskinto two parts where we use the we vareddit retrieval to get like 50 000 ofthese conversations and then and then wefurther pass them into a re-ranker thatyou know compresses it down into thesalient points like maybe like anabstracted summarization model from thismassive context or something and thenmaybe some other kind of chat bot layerthat you're and you're just kind ofplugging in you're just like apirequests fromyeah yeah absolutely right soum[Music]so my uh co-founder federico he built aum a thing that goes through several ofthose stepsum where he's like doing searchand then he's extracting things and theneventually trying to summarize themumandand yeah so the idea is thatlikea a person a developer could go anddevelop whatever they they wantpotentially with help from this sort oflike core tooling right sothere's all this nice core tooling rightlike so there's there's hugging facethere's webv8 haystackum so we wanna like use all those youknow we don't wanna build uh our ownversions of those i thinkbut then there's places where so we wereusing haystack right and we were havinglike it's just not as easy to use ashugging face you have to handle hostingand stuff andso we were building our own like hostingstuff like well let's just make this anapi call um so that using haystack feelslike using hugging face for our for anydevelopers that decide they need searchstuffumbut yeah so people could string togetherwhatever they they want um and thenbasically they can developers can getdirection from from users umand so like you don't you don't takesolutions from users right you takeproblems from from users and then youyou try to figure out asolution and then we have some likeproduct people who like interface likeoh here's what the users seem to arehaving problems with how do we solvethis problem and like ai people are likeah there you go and it's uh yeah and butso so search wise right now we don'thave anyone so we're pretty young rightoogway launched its discordmaybe like two weeks ago it's just kindof this decentralized uhcommunity building this stuff togetherand umand yeah so it's basically open whereanyone can see like what's going on andstuff i even thought about putting asearch on top of discord to better helppeople understand like uhwhatyou know so they come in they're likewhat what's going on herewho who uh who do i need to talk toabout x and then that's where likesearch is is relevant for putting thatkind of stuff on top of discord butwe're young we haven't built that stuffyetum but yeah everything's like opensource it's supposed to be people canumhelpthey can use tools from open source andthen they can also push back out theother way where like we have auh say we have a thing that makeshugging fake or uh haystack easier touse then we would have thatopen source for anyone to use yeah solet me quickly uh point on two thingsand then i want to get into the um thisdecentralized startup idea so the firstof which is um so we we interviewed umuh malte peach fromuh haystack on the fourth we've apodcast i remember what number it wasbut um so they have the deep set cloudgui did you check out that with respectto setting up the apis with haystackyeah soi think they were in like beta and ilooked at their beta and it was like itdidn'tlike i wanted to not just douh i think they were doing like searchplus question and answering and i wantedto just do search or something it'sbasically something i could do withtheir python library i couldn't do intheirum thing at the timeand then when we asked them about scalelike how many documents they couldhandleit was basically like they they weren'tplanning to handle a large amount oflike over 10 million documentsanytime super soonso we had to host it ourselves and stuffto handle like more documents andand uh yeah but but you know maybe in ayear then they'll they'll have solvedthese issues right it's just kind oflike a timing thing that they might betoo youngyeah and i think um trying to helppeople understand the combinationbetween we've eat and haystack and ginaai and how these different things plugin together i think that's one of thecore goals of this podcast and thiscontent is to try to figure out howthese things fit together and so i yes ithink if you just want to use search andthen you want to do further pythonprocessing i think we v8 alone might bewhat you need because it has that kindofpython client where you can still justdo the get the near the nearest neighborkind of filter and then further processit with that so the second thing beforegoing to uh decentralized startups is umso we've partnered with um katie they'verecently uh integrated a questionanswering system into our slack sosimilar to what you're describing withdiscord where you have aquestion answering in slack and yeahthis idea to me is so exciting uh likeduplicate question detection i know inour weevie it's like i'm probably askingquestions that just make me look like arookie because i don't you know i i ican't find the exact question when i dolike the current slack search and sothis idea of trying to get rid of askingduplicate questions alone i i think likejust the burden on open source softwaredevelopment and getting new people ithink solving that alone is huge but ilove that idea of adding questionanswering to uh you know discord slackcommunity group chat so so now let'slet's talk a little more aboutdecentralized startups because i'mreally curious about your umthinking around this and like umyou know like this tokens and what'sdaos i don't understand any of this i'mjust kind of curious about how you'rethinking about this kind of ideayeah well okay so on the whole likecrypto stuff and dao stuff uh i know alot of people will be likeuh groves umso i'm i'm not like dogmatic religiolike religious about that stuff it'sjust that we had a problemthat we were running intouh namely thatuh the non-zero developer time problemper decisionwhere the solution was um some cryptostuff right where we can essentiallyput in a programcomp like equity compensationandand you can make it so that that programis not gonnayou can trust that it's not gonna changea bunch in the future like it's a commonpattern withuh some some past startups some techtech companies where in the beginningit's like hey everything's freeeverything's great and then later onthey're like well hold on we have allthese shareholders that we need tomaximize profits tothe shareholders are not necessarily theusersum we need to extract as much valueum out ofyou know for like companies where thebusiness models are adsthe the the users are the ones um liketheir attention they're essentiallyselling their their attention right tothe advertising uh the ad company theones that are running adsumand so you know they're trying toextract that valueand and so when youyou can make it so that um you don'thave to worry about the organizationlike changing like that in the futurewhere umwhere it goes against like users sayone by you can you can make these thingshard to change right so like when youthink about setting up governments likethe united states for examplethey tried tomake it so that it's hard to changeright like the presidentum especially at the beginning was notsupposed to uh have too much powernowadays people might think they havemore more power but they still don'tit's not like they're a dictator interms of the amount of power they theyhave right like the in the beginning theu.s i think there's some story aboutwhere um i think george washington wouldanswer the door to the white househimselfand there wasn't like someone else toanswer it was just himjust to give an idea for that stuffbut uh let's see you can put checks andbalances in companies as well or orfuture organizations andthenyou you can set up incentives such aslike who are the people who are makinguh decisions who are the like theshareholders in modern companiesit's like well that's just whoever likehas umuh decision making rights right so likeuh equity plusthis like governance stuff is i thinkhow you commonly see it allocated inmodern companiesbut you you can get those governance uhrightsso just for making decisionsorganizationand just give them to whoever right soif you want it to be that one-thirdone-third of it's allocated to usersthen there you go and then that's verydifferent incentivesfrom companies where their usersare only like 0.1of the shareholders or whatever rightespecially because the users areprobably notdon't have very much money so so they uhit's usually people who have more moneyare able tocontrol more of in the shareholdingvotes rightbut um yeah so in shortumwe can make it hard to changeuh like you can put out a smart contractright that encodes the equitycompensationand then you can make it so that whenthere are changesumit's it's thethe incentives are aligned it's not justum people who want to maximize profitsbut it's also people who are actuallyusing the product so if the people usingthe product don't wanna they're not infavor of the super big like extractingtons of value out of orout of them then then that can you canset up incentives that wayyeah i can definitely understand thatthe idea of the users adding value tothe to the product or even like evenaside from ai just like network effectsfrom things like you know twitterlinkedin facebook youtube like just thegeneral idea that you create the contentyou're what's making the platformvaluable and that idea shared ownershipand i think with ai systems it's goingto be even morelike even more on the nose about thatwhere you're creating the data that usedto train the model to replace you almostin in your role of the network but likeso just kind ofthis idea of organizing companies thisway but when you have like a decisionlike a vote is to be made like who posesa decision worth voting on is it like areddit forum where you say like ipropose this the acquisition of thiscompany and thenit's upvoted to the top so now we'regonna likevote on this acquisition dealyeah well so i thinksopeople have heard of dows a little bitumthe essential it's supposed to stand fordecentralized autonomous organizationit's that's it's not a very accuratename right for a lot of um a lot of thembut um a lot of the ways they have beendeciding this stuff is they have uhit's it looks just like regular publiccompanies to me like where it's theamount of shares you have that you thatyou bought right so it's proportional toyour the amount of money you haveis how much voting power you haveumyeah i mean ithis or or then the other one is oneperson one vote which looks like adirect democracy so one is like uhi figure what the word is where it'slike wealth umlike a plutocracy maybe where it's likeyour your voting power is weighted byyour wealthversus direct democracy or both themodels that people are using i mean umthis might sound bad to some people buti think both of those are areclearly uh wronglike especially like direct democracy ii know is like a popular thing but it'sum that's a good way to get mediocredecisions i would say sosoyou know there are pareto distributionsand people's competencesand umcompetition is a different thing sofor a given decision there might be likeone or two or three you know some smallnumber of people who are like waythey're gonna make a way better decisionthan anyone elseand and sothis is part of the company 2.0 stuff isuhwe'rewe're not we haven't done this wehaven't succeeded at this yet but we'retrying to set up a system whereyou're able to select out the mostcompetent people for a given decisionand you know it's probably not going tobe the founders for all the decisionsumand and then you can let them make thatthat decision like potentially in asmall group and uh the paretodistribution premise is kind of like abigbig thing thereand why do you want that it's like wellif everyoneum has some equity in oogwaythe the better decisions uboys makes thethe more value way will create and themore valuable you'll be so it actuallyhelps everyonefor umall like people as part of oogwayfor the the best people for a givendecision to make that that decisionand uhyeah so this is a this is a thing thatwe're working on now it's it's very hardproblem but but uh i think there's newtools coming out that make it easierthan it was in the past like moderncompanies look much more likeautocracies i guesswhere orlike the ceo but then i guess there's aboard right who can who who uh controlsstuff so that's more like allegoryoligarchy likeyeah i'm really excited about like umalso just like ai education tools andeven even it doesn't have to be likesuper ai even just like youtube videosthat are distributing more informationto kind ofsupplement the democracy one person onevote rather than the concentrationaround the experts who make all thedecisions i think having just bettereducation tools whether it'slikean interactive kind of quizlet stylething where you ask them questions andthey you know this kind of whole systemand then yeah like you earn tokens tokind of weight your vote further bydoing the education i think could be aninteresting idea and that kind of thingi'm just like i've never thought aboutthis before but i'm thinking but so oneother thing with this that i want totalk about is like what about like thelaws around accredited investors do youthink that's just likeis that a law that's trying to protectpeople from you knowforegoing their salary and then takingthese tokens that might not be worthanything or is it something that youthink is maybepreventing people from participating instartups sayyeah well souh a credit just for people who don'tknow a credit investor you knowbasically means you have one milliondollars in in a net worth not includingyour home oryeah or it's something like you makeyou've made two hundred thousand dollarsuh in salary for the past like threeyears or something like that rightit's all about umhow much money you have or or makethey're they're adding some stuff aboutlikeweird tests you can takeum that look likelike certifications that traders get andstuff to become accredited but that'sreally not how most people are going tobelike most normal people are going tobecome accredited so it's really aboutwealthandthe idea from the sec so that they'rethe ones who regulate whether you canbuyownership in a company umbefore it's on thepublic market so when it's private sothey they say it's about investorprotection they're protecting peoplefrommaking bad investmentsumit's a little bit weird thatyou can't become accredited from somemore like knowledge based thing then ifthat's the casebut so that's it and and there's yeahbut if you want to be compliant with thesec nowadaysthere areexemptions that you can use soif you like when people when companiesraise venture capital they're oftenusing the regulation d 506c or b exemptionand umthat allows you to raise money from froma venture capitalist basically and theangel investors and stuff who areaccredited investors but if you want toraise money from non-accreditedinvestors like just regular peopleyou can do that under regulationcf for crowdfunding is what it's calledyou can raise up to like five milliondollars and stuff but there's there'sother rulesthat make thingslike rules around advertising not beingable to publicly advertise stuffthat make things difficult so that's whya lot of crypto people end up justignoring all of the rulesand uhjust doing their own thing and they'llbecome like anonymous so that their thegovernment won'tknow who they are i guess ii don't think it's that hard to dealanonymize people but uh that but theythink they're anonymous at leastyeah i'm obviously not anonymous sowe've talked about a lot of topics fromcausal inference and deep learning tooogway ai the decision making layerorchestrating all these different ai asa service apis and talking aboutdecentralized startups and thinkingabout how we're going to how the futureof company 2.0 and organizing companiesis going to be with the advancement ofthese ai tools and i think of ai toolsparticularly i think nlp tools are goingto havea massive impact on the way that weorganize these companies things likeyou know question answering systems inour slack chats andjust the way that we can tradeinformation with each other is alsointeresting so just kind of one lastquestion i'm curious about like um yourinformation diet and kind of how you'reorganizing your time betweenare you you know like running a startupare you mostly putting out fires andbuilding new stuff or like how do youwait doing that and then say likeknowledge discovery reading papers anddoing the kind ofscientific experiments what's kind oflikeyour strategy forkind of continuing to develop yourknowledge and build this toolum yeah so i guess it's i try to figureout what are the most important thingsfor me to spend time on i make sure tospend time on those things like recentlyi had to learn a lot about legal stuffumbut alsojust my personality i'm uhi need to be learning about just likerandom stuff so uhi'll often just be learning aboutrandom stuff that it's like not obviousthat i should be spending time learningabout that at that timeumyeah so i think that'sthat's it i do you know i ai stuff it'slike i read it if i need to be readingitor umor if it just like has randomly caughtmy attention i guess is what i'm lookingat ai stuffso how often do um like when say the newpaper tweets that are coming out sofrequently how how does that kind oflike and i have that same kind oftendency of just like being curiositydriven and you feel like you need toread it just because it's gotten yourattention how do you kind of like putthe blinders on that kind of stuff andstay focused about getting too caught upin all the latest headlines that comeout seemingly every day yeah well okayso wheni guess when you said the phrase puttingon fires when there's fires it's not sohard to have blinders on i thinkumyeah some chemical thing in your brainmakes makes it that wayand then when there's non-firesi don't know i'm i'm umlike i am ai'm a fan of kind of like maximizingmotivation and uhit's a little bit radical but like likei think that discipline is maybe a bitoverrated and that motive motivationsunder natural curiosity is underrated soi just kind ofi'm more just go with the flow when uhwhen's when i want to be looking atstuffbut uh but yeah i mean you you need ithink different like a diversity ofpeople on that right you i think youneed people who are more likethey're really good at focusing and it'shard to distract like you can't distractthem and then people who are more umall over the place i i think that youneed a auh diversity in in organization of thosekinds of people awesome yeah so thankyou so much brady for coming on the wvvapodcast loved hearing about oogway ai itsounds like such an exciting project andhopefully you'll come back on thepodcast to discuss the update of it in afew months so thanks again for coming onthe podcastnice connor thanks for uh thanks fortaking the time", "type": "Video", "name": "Weaviate Podcast #8 \u2022 Brady Neal about Causal Inference in Vector Search", "path": "", "link": "https://www.youtube.com/watch?v=t7g9s1GWcB8", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}