{"text": "Thank you so much for watching the 42nd episode of the Weaviate Podcast! Ethan Steininger is the founder of Mixpeek, ... \nhey everyone thank you so much for watching another episode of the wevia podcast I'm super excited to welcome Ethan steininger uh Ethan is a prolific entrepreneur who's created a mixed Peak and Kali and he's really got our attention to someone who's building cool things with weeviate and generally in the uh search and AI space master of software engineering so Ethan thank you so much for joining the podcast I definitely don't know about master but I appreciate the enthusiastic intro awesome uh so maybe to then could you dive into your kind of background in software engineering and search maybe yeah maybe search is more the master yeah I'm actually only recently uh immersed in the search ecosystem but I guess that doesn't act as too much of a detractor considering the industry has changed so much in the last year alone uh so my background is actually as a software engineer went to school for programming actually started a lot of projects a lot of startups most if not all of them have failed uh so I've got my own if you've ever seen the killed by Google website I've got my own graveyard of projects and GitHub repos Etc that I've abandoned so no shortage of failed projects but uh my foray in the search was actually at my most recent job which was at mongodb uh they made the decision to actually embed leucine which is like the core for those unfamiliar with lucian's the core open source library that powers elasticsearch solar and a ton of other search engines out there and mongodb decided to kind of embed that open source Library within the database and coupled it together and because that's such a novel concept at the time they needed a special SWAT team to help take that product to Market and so that's what I was summoned to do which was really just a matter of defining the the customer profile building out technical collateral talking to customers and being that kind of liaison to the product who's actually I won't mention his name but he's one of the formal advisors of of weevier uh and actually one of my old-time mentors when I first got into search but that's kind of how I first got into uh the industry just being summoned to support the go to market of mongodb search product that's so interesting yeah learning about uh well I guess I'm kind of curious about this like leucine mongodb maybe I'm very like I don't know if it's the most entertaining podcast topic but I am very curious like like what are these Technologies kind of like um is it too because I know so little about this kind of world I've like kind of I joined wevie coming from like the Deep learning background of being very interested in just kind of representation learning optimized vectors and then sort of and then so from there I came into like elasticsearch Lucy and solar hearing about these things so I would even maybe talking a little more about like what is the underlying technology of [\u00a0__\u00a0] and solar and these kind of things yeah I love the fact that you take most things from a research angle like that the how does it work uh which is something that I need to get better at but I've historically tried to figure out like the the positioning of the product and like what kind of immediate business pain is does it solve I was in I was a sales engineer at mongodb uh but under the hood it's really actually quite simple I mean every database has a change stream API uh my sequel [\u00a0__\u00a0] Dynamo they all have some kind of listener that you can open up up like a cursor on the database so anytime there's a change you get alerted of that change and so the Architects behind that mongodb search architecture they basically coupled the chain stream cursor API which listens in on changes and replicated all of those changes into a leucine inverted index and so once you have those two data structures together then they basically created a a wrapper on top that exposes if it's called the aggregation pipeline in mongodb it's kind of like the querying language they expose this aggregation stage that allowed you to run a search query on the leucine inverted index and then once you have the results of that search query it would actually pipe it back into the mongodb database which would allow you to do uh kind of you have the term hybrid search that's hybrid if you had like database queries uh but I know you guys have hybrid as in like TF IDF plus Vector search so uh it's a little bit of a different positioning where customers that really value the acid compliant properties of a database and want to combine that with the uh with the full text search functionality yeah that's so interesting and I I think I was really curious uh you've written this great article on the ml stack where you have the acid compliant thing and then we get and I was like Olivier has the database stuff too I was like do you not agree like I'm curious like is there something to the acid support in um [\u00a0__\u00a0] that's left out of weave yet yeah I wouldn't say there's anything left out but I as someone that has sold to Enterprise customers for a long time customers are reluctant to change their system of record right and so like when people are already using a [\u00a0__\u00a0] or even an Oracle and they're using that as their source of Truth their their database it's a lot easier to tack on an existing search engine and have that just be coupled together via some kind of streaming technology rather than just replacing it all together so I I'm sure that the the weeviate database API set and methods are are super robust to just thinking about it from customer standpoint unless you're a startup and you're just doing something new for the first time they typically have big customers typically have a lot of uh Legacy code data stores in Old database Etc but it's a good question yeah that's interesting and I I'm kind of coming back to something else you said earlier about the mongodb aggregate uh pipeline uh Sebastian would Alexa also former uh [\u00a0__\u00a0] and and now Olivier he showed me like this kind of aggregate thing in the uh like thinking about the vva pipe design and all these different filters that can be attached to ev8 I think it's so interesting like the yeah just all these kind of things you can do to it so maybe pivoting topics um can you tell me about kind of the founding Vision behind mixed Peak and and the problem you're tackling yeah one of the most common problems that I had experienced so I I was traveling the world talking to our most challenging customers when I was at [\u00a0__\u00a0] and one of the themes that I had learned about pretty much every week if not every day was really that of how do we search the contents of files and most of the use case was really PDF files just because these were mostly Enterprise customers Enterprise uh companies have a ton of PDFs spreadsheets even like doc files document files uh and so I was just basically telling them all to hey like you got to create this framework and this architecture and you're using Tikka and you're extracting the Tikka contents into your search engine you're doing chunking and all these things and it got to the point where it's like all right this is a big enough pain I've seen this enough and the vector search uh projects were becoming more and more popular uh and this was right around when Lucine I forget which version it was but it's when they first introduced the vector k n uh capability into uh the the core leucine branch and uh that really unlocked a lot of super interesting opportunities and uh decided to to quit [\u00a0__\u00a0] and build that out full time and then I'm very grateful to have raised the seed round from some investors and that's given me a little bit of a cushion to explore some of these really radically changing areas uh I've I've been talking to a lot of startup I live in New York and I've been talking to a lot of startup Founders around the New York City area about like how has your road map really changed in the last three four months and every single one of them is like yeah drastically we've pivoted so significantly to keep up with all these changes and like open ai's plug-in Marketplace was just announced yesterday and you best believe at least thousands of startups are going to be considered moot because they've just built wrappers on top of the Chach EBT API so we're definitely about to experience an apple to App Store like uh paradigm shift and uh I'm super impressed that we V8 I saw Bob's post we V8 is on the the plug-in Marketplace you guys are well positioned to be that default Vector search engine that wraps on top of Chach EBT right out of the box so uh but yeah that was kind of like the intro to why I started to develop mix Peak and at its core it's really just a multimodal indexing embedding and search API so we take all the messiness around the database the uh keyword search the vector search the file parsers the file extraction and finally the search and we just exposed it as two API calls with re-ranking chunking analytics Etc all baked in yeah amazing that I think there's a few things on that I think I'd like to come back to the uh chat gbt plug-in because that's such an interesting topic but first I really want to just kind of pick your brain more about the mixed Peak uh goals and so so on this idea of and I also think maybe we could talk about Kali as well this like really nice user interface for getting like dropping PDFs in and then it's in the vector database in the chat gbt thing like all this kind of ranking all that is like accessible um so I I'm so curious about this kind of data ingestion like kind of I think the PDF ingestion with like I I recent called it OCR on the podcast with Dennis you from mem and he said no it's it's so powerful that you can't call it OCR like the way that gbt4 he said it's like having a human looking at your PDF but shortly these things will just make it super easy to get PDFs into you know Vector databases and stuff like that and yeah all that so I'm curious kind of like how do you see this these uh Innovations and maybe like this llm enabled ETL for unstructured data kind of as well as this kind of chunking idea like what are some of the opportunities maybe around chunking yeah uh I think one of the most impressive use cases that I saw for gpt4 uh by the way uh there was a paper that was just released by Microsoft research I believe two days ago the 22nd today's the 24th all about hey we think that chat gbt4 is demonstrating a spark of of artificial general intelligence and in that paper they used a couple of use case examples one of which was like you said somebody provided a picture and it not only described the picture but it extracted the contents from it obviously well beyond OCR it's like a human is interpreting it uh and that again that helps us kind of position what we think is the most valuable and at its core I mean this is my theory on like a lot of startups are having existential crises right now uh and like chat gbt is rendering a lot of them Moot and so what I'm what I'm telling anybody that's asking is like you need to get closer to the business and you need to understand what their pains are and build phenomenal user experiences on top of that right and so if you think about who your user is for me I'm building an API so my user experience is the developer experience and if I can really nail that developer experience and make it as sticky as possible that's my mode that's my competitive mode and I advise the same thing for anybody else because like in the end of the day and this is what I've always told people that challenged when we were positioning mongodb's Atlas search in the end of the day any engineer can build anything really at its core I mean maybe it won't be as good it won't be as robust it won't have high availabilities service level uh slas uh but they could build it it's just a matter of like hey do they want to and B is it as intuitive and clean as you could so definitely a couple of existential threats there but as long as you're getting close to the business and understanding the pain then you're in a good spot yeah it's so interesting I really like kind of the story that's emerging in our podcast so far of like this kind of like customer success oriented sales engineer thing and how that leads you to think about how the perspective that creates when you're building this kind of startup in the space and yeah so interesting maybe kind of staying on this topic of moot existential crises we could come back to the chat gbt uh retrieval plug-in um it's yeah it's so interesting this kind of like uh the marketplace for chat gbt being like hosted on the UI because we saw so much we saw so much like building around the API like uh like Lang chain llama index of the two things that I've been super invested in learning is like these you know building these kind of appendages around chat gbt rather than kind of the Chad gbt UI like hosts the apps and yeah I'm just curious what your perspective like I think it's such a fun like emerging topic is is like is this going to be the App Store kind of or like because yeah or would it be like the API like just how do you see the that kind of space yeah I'd I'd wonder and obviously everybody's just like conjecturing right now but there's really two Avenues I think it could take it's really either going to be a zap here like experience where you could just combine all these things together I can use chat EBT to be the link between I don't know my Roku with my Alexa and my thermostat right where catch EBT is the link between them all where the kind of adhesive the glue is my human language do this than that whatever that's one area and the other is really just like a standard marketplace where you can procure services from within the chat gbt like interface I saw a couple of examples where people were booking flights from within Chach EBT so that could be that single point interface for all of the apps within the marketplace it's probably more so going to be the second rather than the first because the first I mean there's really not much moat around just being a glue between everything but the first is really like if this is your default entry point for getting anything done accessing the internet your calendar your trips then that's it's remarkable yeah wow that is I mean um uh yeah it's like the the business model the mo is super fascinating like the um obviously they make like I think it's like 20 a month is the cost for this UI right like to even access the app store because it's only can is it is that a correct understanding firstly that it's only the paid version that has this Plug-In or I'm not sure about that but I am a very proud gpt4 subscriber so I'll find out soon yeah I'm subscribing as well it's funny I feel like uh no offense to anyone that works at Google but the last time I've Googled it I'm a I'm coding all day the last time I've used stack Overflow is like weeks ago it's exclusively been chat EBT which is really demonstrating a significant paradigm shift yeah I've also been just why I also am so excited to check out uh copilot X I haven't yet uh like spun it up yet but I mean the the ability of it to write code is amazing like my kind of story with this is that you know we've yet it's written in golang and I don't really know golang but I can like be with the help of Chad gbt it's just making the learning curve like so much quicker like I you know if I say hey I need you to sort this list like I have this dictionary I have these keys I don't even need to know about like the map interfa the map thing of golang it'll write it for me and then once I see it now I can quickly adapt so that's I remember there was this paper from Facebook that came out like three years ago is like translating between JavaScript and python I was like wow that's so ambitious but yeah now it's like I think yeah what I would love to talk more about get your perspective on how you see how uh gbt4 helps your coding productivity because for me that kind of helped me learn a new language has been just the biggest one yeah well before I answer that question I think that there is a big opportunity around using this large language model and really others I mean we shouldn't only focus on chat gbt like open AI is doing really well uh the open source models are super exciting I know that uh llama Stanford's llama is is generating a lot of hype just because it's smaller it's more lightweight it can run with on less CPUs and it's open source so uh just well there's there's other options out there uh but at its core like what I've been doing is I mean chat EBT for any use case in the uh we'll call it content creation so code copy whatever it it really just serves as like providing a framework for most things it's never really producing a final copy maybe for like a one-line email it will but for code I mean no one's just dropping it right into their code and deploy into production like if that happens then we've got things Glory now but I think we're a far ways from that uh let Famous Last Words obviously oh yeah so I think the like the Llama alpaca thing one thing about that that's so interesting to me is kind of like the licensing of it like with llama it it doesn't have a commercial license like you it's like I think it's like just for research purposes so like the L packets like you can use it for your phone but you can't like just use it and I think that is another interesting thing like open AI they have this thing where they're saying um you can't use the model to develop models that compete with openai so obviously like the knowledge distillation thing is like you could just copy the outputs basically and take any model so yeah that whole thing around like what you can use the models for is so interesting yeah uh I wouldn't be surprised if there is going to be need to be a like precedence uh shift around licensing for some of these code bases these models because like I'm sure that there is some uh finagling around the legal definition of Licensing when you've really trained a model on the entire closed Source database of GitHub uh so I I wonder if there's going to need if anybody is a policy maker out there that's a conversation that probably should be had sooner rather than later because like is it is it really fair to have that uh line item in your legal statement where you've kind of done something different I don't know uh but alpacas the way alpaca was trained is really really fascinating I mean they really just started with this open source library or this open source model called llama which I think was like seven billion parameters they started with that open source model and then they basically so whenever you uh whenever you fine-tune a uh we'll call it like these cheap gbt models you provide prompt completion Pairs and what Stanford did is they actually used chat EBT to generate prompt completion chairs and they uh Pairs and they actually created a feedback loop of uh positive versus negative rejecting versus accepting these prompt completion Pairs and fed that into llama and I guess that margin of new new pairs has got it to the level of really competing with the GPT uh maybe not four but maybe the three uh which is which is super valuable like imagine the the example that I always give is really like imagine these large language models at the edge right if we have an llm on our phone or even like some off-grid device it's able to of it's able to consume any kind of input and generate an output agnostic of its connectivity to the internet and using all the data that it's connecting it's kind of training on the fly at the edge in real time it's it's really a lot faster so that could be like another convergence of the two Fields is like this Ai and iot um it's the really first time I've thought about that I don't know what more use cases beyond what I just made up are if anybody has any of them feel free to share yeah the the iot thing is just it's super interesting I I don't know if this is productive to mention but I like I I took a little grad class in graduate school class in like um in the in the Internet of Things it was about like monitoring buildings and how you like send a pulse in one part of like construction like you know building and construction and networks and they're like okay the building is okay right and thinking about like I don't know could language models in that be be productive or something like the Internet of Things topic has always been interesting that I've known a little bit about not too much but anyway so pivoting to the first thing you said about how alpaca was trained in this kind of reinforcing learning from Human feedback thing um I think I'm thinking a lot about like as impressive as Chad GBC is what if I find like what if I fine-tuned it and labeled it on the prompt completions that I like like you said like I think a lot about this like like I've had a lot of conversations with like Jonathan Franco at mosaic ML and their perspective on uh businesses wanting their own custom language models I've also talked to Nathan Lambert about uh hugging faces reinforcing learning from Human feedback and like surge Ai and all these things like how this might emerge because I think about like let's say I trained a language model on like the weeviate slack like and then I start labeling its Generations I feel like that's just going to give me something better than Chachi BC yeah yeah and and that's a really interesting point and the exact direction that I'm taking with mix Peak which is around like domain specific data sets right so I mean what I'm finding and really everybody's finding is like exactly right I mean it's nice that chat gbt understands the internet but maybe the reason why the revenue model isn't quite sound yet is because they're just hoping that Conor and Ethan pay 20 a month which they happily do but the real the real money will always be in the B2B market and not only the B2B Market the Enterprise market and so there's a ton of companies out there including me including including mixed Peaks that are really trying to understand a company's Enterprise uh let's say knowledge base and training models maybe they'll call them agents trading these agents around understanding that data and then providing some kind of chat gbt like interface on top uh there's so many sources of information there's slack there's jira there's uh email inboxes Wiki like these companies have so many different segments of uh of knowledge and they're all really like very well formatted Like For Better or Worse their q a like slack someone asks a question I'm sure in the weeviate slash Channel there's an answer and that's a very obvious prompt completion pair that can be sent to a gbt model so to answer your question I think that there's really going to be a strong push towards uh towards customizing models but what I'm also really interested in is like where we V8 comes in is like you guys have a search engine and the search engine will always be the source of Truth and I was actually on one of Bob's uh Twitter spaces I forget it was somebody from cohere as well and I asked the question hey like what do you guys think about how generative models and AI how does that contend with the search landscape and a lot of I mean my initial thought would be like it's replacing search obviously because you can create information rather than retrieve but after hearing Bob's great explanation and doing a bit of research there will always be the need for kind of like referencing the source of how it came to this conclusion uh in perplexity AI has a great UI for that I'm quite fond of theirs and I'm modeling uh the interface that we're building on top of uh Kali around that that interface which is really like and I'm not I propose this like open source framework for kind of creating the verified truth of generative uh responses and it it really involves this search engine you embed an entire Corpus of content then you you chunk it so that each one has its own meaningful importance like modular meaningful independent importance and you embed each one and so then you run alleviate query across those entire embeddings you get the ones that are most important or maybe the top K most important and then you feed that into a chat EBT and so then Chachi BT has a little bit of context and it understands that little bit of information that can then be used to either create a summary or an answer or something so that's how I'm seeing a lot of people combine the search plus the generative and I see that as being a pretty standard frame framework for creating these things and I just wrote an article that was like here are all the steps that that need to happen to do that because I've built this internally for we could talk about Kali which is like a really simple user experience for embedding search on your application but like I've seen and built this framework enough where it probably makes sense where someone uh standardizes it so if anyone's interested in collaborating on that happy happy to talk yeah super cool and um oh yeah I love that topic I think kind of one thing about that that I still think is so exciting was kind of like the first thing is like right now the general how do you take the context and put it into the language model is like you just put it in the input like the input is like you know please answer the question based on documents documents you know go for it whereas there is the science is looking at these models like uh deepmind had a model called retro there's like these fusion and decoder layers there's this uh memorizing Transformers where you would uh to keep the embeddings keep the vector embeddings and then you say you like put that in layer 9 out of 12 of the language model and so this has really interesting scaling properties because uh you can you know the way that you can kind of like transpose the matrix multiplication you could put like pretty massive uh documents into these models and attend over more context so I think that's an incredibly exciting part of this is that I don't think we've seen yet the full power of these retrieval augmented language models and what they can do when they can take in re really massive context and then yeah as you mentioned I read your article as well and I've obviously I mentioned earlier I've been studying like Lang chain and llama index and all this kind of thinking of like okay how exactly are we going to organize the um you know the search results to present it to the language model is such an interesting topic um yeah maybe we could talk a bit about this idea that the Llama index idea like there's one thing that I really like about this integration where it's like you retrieve a hundred results from wevier and then you need to like extract structure from those results before handing it to the language model and the language model kind of use the language model is what's used to extract the structure as well so it's if it's like you want to turn the top 100 results into a Knowledge Graph you're using the language model to go like what kind of like relational triples are in this and it parses it out and then you have this new thing that then goes into the input to like another language model so kind of like yeah I'm just curious your thoughts generally on like how exactly do you hand the search results to the language model and how much opportunity is there maybe for more innovation in that yeah uh it's a good question I haven't actually explored the exact use case that you just had I love the idea of instructing the creation of a Knowledge Graph uh and actually that artificial general intelligence paper by market by Microsoft's research team actually showcase like they instructed Chach EBT to ask a sequence of questions to understand something I don't I forget what it was I think it was like the interior of a house and it demonstrated the creation of a mental map of that house that was accurate like a visual diagram of how that house is laid out from just asking questions so uh I I'm constantly finding myself not only a maze but like uh I'm finding myself like asking like why aren't I just using chat EBT to uh like even like an ETL an extract transfer load kind of uh activity they're they're very suitable for that I think the challenge that I have with like even feeding I think you said like 100 or so items like you're gonna fast come up to the uh the token limit for Chad EBT and I'd be curious from your standpoint is that a something that you see going away and B I mean this is the first I've heard about like the layering of llms which really would solve a lot of those token uh upper bound issues because if you can just like I I don't know enough about to even like explain that but uh seems like that would address a lot of the the the challenges with like feeding your entire Corpus in one point to the the Chachi BT API right yeah I mean I'm just fascinating on this topic this has been kind of the number one thing in my mind lately is like these like text to SQL is kind of the starting idea where the language model is like writing an SQL query to get information out of the database but I think we're thinking about like okay how do we do this with weaviate how do we get the language model to use the symbolic filters so it's like if I let's say I have the weviate code bases like the thing that's too many tokens to just put it all in there at once so we need to like cleverly search through it and we have these kind of symbolic filters of like the um like the folder structure I think is a really good one for these you know big code repositories you know you have like modules is here like repos adapters these kind of things and it's like how do you tell the language model how do you prior it like um a prompto sorry prior how do you prompt it to say um you know you have these categories that you can filter your search through so like do you only want to look through like you know this particular folder of the project structure I yeah that kind of thing of like um getting the language models to use the database interfaces so are you saying that the repository the the weeviate code base repo is an ancillary to the querying language and by understanding the relationships between the folders and directories they can they being the GPT whatever it can then understand how these manifest into the queries that yeah that idea is the one that I think is um yeah yeah I think that I feel sick too it's like the um because it's uh this General thing is on it on any code base right yeah I think that that linking that that does imply that your software architecture is like really well mapped around like the methods that you expose in the query language too like you said the modules the uh I don't know enough about you said it was written in Rust yeah yeah exactly this because all these code bases have this kind of and that's like the thing is like the structure is like um like we're talking with Dennis about mem and these like you know like workspaces where you create a workspace like I don't know like you have some chapter of your molecular biology class and you're like and then you keep like you create a page and then you add content and like there's like this structure as well and I think this structure is really well captured and like weeviate has the symbolic properties as well as the vector search and then the filtered Vector search is like made crazy fast like this bitmap index thing of like these intense like filtered uh disk and is a paper that just came out another like exciting idea of how you integrate those two things but yeah it is just such an interesting topic um so maybe there's something I really want to come back to with the mixed Peak and the uh specific problems thing is and then we touched on it already with this idea of how alpaca was fine tune with like a different data set of reinforced learning from Human feedback but how are you thinking about fine tuning I always I ask everyone about this topic because I love it okay so I'm gonna caveat everything which is by saying that like I I have a team of really brilliant Engineers that I'm working with and they're the ones that are really creating like I'm kind of creating the uh the little bit of an abstraction layer on top of like think of it as you have core services and then you have like routers on top of those core Services uh the really impressive team of Engineers is building like the core embedding inference and uh re-ranking and fine-tuning models but I think we actually talked about this at the weeviate Meetup Connor which is like how or maybe it was your colleague but I was I was asking like what are some of the like really suitable production use cases of learn to rank right and I know learn to rank isn't quite fine-tuning I think there's probably a little bit of a Nuance but uh I think that like re-ranking results around the previous sequence of some kind of uh signal that a user has and reordering it based off of some kind of conversion metric that's that's exciting but I think at its core again it goes back to understanding the end user so like for example I'm exploring with this Kali use case which is it's really like an embedded search bar on any website it's just like a JavaScript widget that you can embed on your site that understands all the files and the directory of the files and I think if if we're able to kind of track the activity of a user across a number of pages uh we can create kind of analogies between hey this sequence of activities which user a converted and converted is just a metric that is specific to this company so maybe if it's an e-commerce company they bought a chair or if it's a weeviate uh company they deployed some kind of instance so everybody has their own metric that they use to define in conversion and so if we can build a learn to rank model that is actually reordering the results based off the propensity to convert then I think that's really attaching ourselves to the goal of the business uh but I realize that's a little bit different than fine-tuning uh well yeah no I I think there is a relationship but uh yeah we'll come back to particularly like what kind of fun thing but yeah learning to rank I love this topic uh yesterday I record a podcast with um Erica from leviate and Roman and Siva from meta Rank and so oh yeah great people right yeah and so this kind of idea of uh you know using like an XG boost style model that takes in like user features as you mentioned like the um there's like the interaction event obviously like I think people are aware of this kind of like you know you people they collect click data about you and like when you like except yeah so so this kind of layer it I mean I guess um I think I at this kind of learning to rank thing I think I'm very curious about like the generalization of It kind of like the like because you're using all these features and I I wonder if that's going to be as kind of robust as like the content only based cross encoders and then I one idea I think is extremely interesting is maybe taking these tabular features and just kind of translating them to text and just making it a text based input and then you know then you can access the transfer learning kind of part of it where that's what I think is kind of the frontier of learning to rank per and I yeah I was I think um let me just make sure I understand that said differently we have user a that goes and clicks a button on the first page and then Scrolls down to the second page and then clicks another button on the third page that's three different steps that can be mapped to text unique text and then that sequence let's say it's an array of strings gets sent to a large language model or whatever and then the embedding is generated stored in a search engine and then you can run like sequential inference is that kind of maybe not yeah yeah that's well you you yeah like the because you kind of confused me a bit like in changing my understanding with how you're describing like the scroll down the interaction because I I suppose you could collect a crazy amount of tabular features that might be too much to translate into text but then I also think if you're collecting that many features you're going to overfit to some pattern in this feature Vector that you've done like you know scroll down for three seconds waited for two seconds hovered over this for five seconds right like I feel like if that kind of vector that kind of feature engineering is going to overfit but yeah no that's exactly it is like um you translate it all into text and then like I think about um if I had some kind of user description like usually cross encoder is like query document but like if it was like query user description document you know I don't see why that wouldn't work just as well and then also you used a large language model to do that kind of reasoning and then you distill it into like the 20 million parameter cross encoder that runs crazy fast and and then the the topic then I think is um you know they'll need to be like software layers to make that kind of thing accessible right like um training ranking model that's what I really thought that meta ranked podcast was great because I I really like what they're doing how they've built the like ml Ops just for ranking I think it's a pretty cool Niche like uh yeah because you have all the how do they do it if you were to summarize in like a paragraph well as I can say like coming from the beginning of it my interest with integrating this to weviate was like weevia has this module system where you access the retrieval from you know like vector search Hybrid search with the symbolic filters and then you can pipe that into like a question answering model and so we could also pipe that into these uh ranking models and so it's like if if we can just make that API to The Meta rank services so metal rank does the full set of like the you know inference hosting the model like the you know how you do the data ingestion how you do the model versioning the validation that is like quite a package of ml Ops things and yeah yeah I uh while I was actually at [\u00a0__\u00a0] I was trying to do my best on educating the entire Solutions architect org which was like 400 people at the time around Vector search and from interviewing all of the companies that were exploring it what you just described were the biggest pains around it which was like okay how do we do model versioning where do we host the models how do feedback loops work uh what's like the best reference architecture for the ml Ops to to handle all of this uh what I'm finding is like I'm forced to store model versions as like a string and some kind of other uh data store and then create an index for all of those different model versions because as you know if we just tune a model even slightly it kind of renders all the embeddings Obsolete and that poses an entirely new challenge which is okay how do you kind of refresh all of the existing embeddings and I've seen some people have some pretty impressed distributed compute uh workloads they're using spark to just run everything in like this horizontally compute fashion some people are using like serverless gpus to do that uh but yeah I mean it's it's a huge Challenge and this is why like if if anybody's exploring starting a company in the AI space it's tooling is hot uh just because of like no one knows how to productionize some of these uh applications and like I'm sure weeviate is doing phenomenally for that reason uh but like all these ml Ops and even what we're working on which is these API based companies to help abstract a lot of these complexities there's there's a lot of interest in that uh both from the market and from investors so uh you did ask a question earlier Connor which was like is there going to be a winner take all for these models I'm curious to hear what your opinion is because like everyone's it's so obvious right now that open AI is like absolutely dominating but like is is that really the future uh are we just gonna all be succumb to the whims of Microsoft um okay well I think um it's a it's a super interesting question I I think uh um yeah I mean well it it's I could see it being kind of Monopoly like in the sense that well this kind of like zero shot thing to me is very monopolistic like this you know like the whisper model that it goes audio to text similar to gbt4 this AG like I think AGI is pretty like I don't I don't think that you'll be like oh I didn't like the answer from gbt4 let me go ask coher or let me go ask Bard right this kind of thing like where you go ask another one of the agis like I think it's more likely that you'll just tune your prompt kind of if you're unhappy with the answer but I do think like I don't I don't know I'm not sure I'm not I haven't really thought about this too much but I um and it's such an interesting topic I wish I had a more like a more thought through yeah all right sorry to put you on the spot I know you guys we've yet has a lot of partnership with the different uh modeling companies and I've I've used most of most of them cohere uh hugging face I've I've used like so many of the llms out there uh but yeah I I think personally there's probably there's going to need to be some kind of like staggering of the offerings and we talked about how like the companies that are offering these like domain specific models and they're like supporting the training on companies knowledge bases they're going to do really well but then there's this entire uh arm of companies that really need security uh I went to a security meet up yesterday uh a company in my our lead investors portfolio and they like what we talked about earlier they've completely pivoted their roadmap to support how do we create what did they what did he call it like zero knowledge proofs around uh like what you're providing to the model and ensuring that you can validate that the model is actually I don't want to butcher it uh but it's an area of academic rigor and like the lowest hanging fruit is really the models that you can self-host and if that's never a direction that Chachi BT goes in which could unlock a can of worms in Pandora's Box because I know there's a lot of filters that they're putting on top but uh I think there's a lot of avenues different model companies can go after it's like is security the most important is domain specific the most important is General usage the most important cohere has a phenomenal API API Set uh so so maybe it's just like the developer experience so uh I think there's gonna be there's gonna need to be like different niches and maybe open AI does become the App Store but then like cohere becomes the sales force right like there's no sales force on the App Store maybe there is nobody uses it but like there's still a behemoth because they attack the domain specific knowledge base yeah and I like yeah we've had a lot of Partners and friends and like another company that you know that I'm super interested in is what Mosaic ml is doing and I think as you mentioned the sales force I think Mosaic ml is the like that is a company that's just super impressed me with their Pro I love like the um like I love this kind of business model obviously it's like the Wi-Fi business model so I like it a lot of like where you kind of Open Source the software but then the like Enterprise hosting is managed and so you know Mosaic ml they they have this composer Library they're sharing all this knowledge on regularization and you know they're they're hitting they're cutting the cost of training bro I think they just said that they trained Bert for like 20 and it's like they're you know they started out by saying I can get you GPT 3 at 450 000 and now today it's 300 000 and they're you know they're they're cutting this down and I yeah I think it's super interesting that kind of idea of the you know the language model for your custom thing but then there's is so so yeah I think actually we could segue this into two things there's the language model for your custom thing and we talked about the kind of custom re-ranker and how that learning to rank generally uses all these crazy specific like tabular features about you and then there's the embeddings models like like what do you think of and and you mentioned that problem of uh the re-vectorization problem that's a pretty big problem like if you update the embeddings and you have a billion embeddings you then need to recompute a billion embeddings with the new model right I've seen some interesting but like retrieve the original corpuses as well which is like its own mess of challenges and that's why these like hybrid search engines are going to become more and more important uh which is like you need the original content in order to do the re-embedding but sorry I cut you off oh no yeah that's yeah it's great I mean the yeah like vectorizing a billion yeah like I've seen a cool idea which is like like the Facebook DPR model like where you just update the query embedding model I think that's a potential idea where uh the zero shot embedding happens with the like you know the open AI or cohere embeddings model social coheres multilingual embeddings model is amazing and you you know that's your document embeddings and then you update maybe the query and then yeah another interesting thing from that uh cohero Twitter talk that Bob said was that he thinks uh like 80 of the cases the zero shot model paired with the like lexico bm25 and the hybrid setup that that's a pretty good you know that's a pretty strong bet okay yeah and I this is uh an interesting topic that I've been curious about which is like is tfidf and bm25 are they gonna become obsolete at some point like are we always going to need to do like string matches to some capacity or will Vector searches kind of just dominate uh obviously you guys have an opinion there because you've baked it into your roadmap uh but yeah so so you were saying sorry so the bm25 combination with what has excelled uh this is like a zero shot embedding model so you know like the open AI embeddings pairing that with bm25 for let's say looking through Air Airline manuals like some you know like some uh application like that uh yeah and I I do think BM 25 it's pretty interesting because there is definitely searches like I like this idea of having like intent and like prompting search as well there are a couple papers like task aware retrieval with instructions and instruct in the instructor models that are like you prompt kind of the embedding model as well because the embedding model also is like capturing a relationship so it's like a one of the great academic data sets is beer and beer has this one data set called arguana and in argue Anna you're retrieving counter arguments so you you put in some argument and it's not it doesn't say get me what agrees with this get me what doesn't agree with this and that little difference so like the retrieval models obviously aren't like adapted to that it's it's very interesting because like it's like that negation thing where you say like I am happy I am not happy and then you're like oh why are these vectors similar to each other and it's like well there are semantically similar and that's the relationship that's captured so see I might have gotten a rant there for a bit but I yeah yeah so because beer was trained on and it looks like it's b-e-i-r right yeah because beer was trained on like the inverse or the negation of some of these comments they have slightly better understanding of the use cases where like I am not happy versus I am happy which is interesting because like as someone that's experimented with a lot of the like hugging face off-the-shelf models that's something that has historically been quite challenging to grasp because you could have an entire sentence just one word makes that entire sentence completely different which is probably an area of academic rigor so task wear retrieval with instructions okay bookmarking that yeah yeah it's so fascinating and I think kind of one more thing I have strong opinions about b or b e i r because I've spent so much time working on adding this to ligate but like so the beer is like this zero shot Benchmark it's like um I think 17 in total but three are like closed source so 14 are like openly available and so you don't do any training is the idea of it is like how well can a model trained on something else generalized to this and I think that that and that's like that and this uh I think like Ms Marco is one of the data sets but it also has a training set I don't think we have a good academic data set for the like continual learning case like that like it would be amazing if there was like an academic data set that was like let's say the pi torch documentation and like how it evolves over time I I think that kind of data set is needed because it you know it's like um like I love this example with weaviate where it's like in 1.16 we introduce ref to VEC and it's like model would have no clue what ref to VEC is you know like it's this kind of sequence problem I think and by sequence you mean kind of capturing the evolution of a corpus and and using the Deltas between these stages to tune the model uh I mean that's a that's a good point of like how exactly do you want to do this I mean I because I just think like the um it's like the kind of like the train test set is like uh there's usually like this IID you know like independent identically distributed where you you know have all your data and you're like randomly sampling the training set randomly sampling the testing set compared to this like historical data splitting where it's like you trained on 2012 to 2018 tested on 2019 2020 and I think that that's more realistic in my view right right and I think the use case for that are are really aligned with uh just this concept of a knowledge base where the content itself is evolving and you need to kind of not only train but test on like the the Deltas of those stages over time uh if it's all right with you I'd like to Pivot slightly into something that might be a little bit irrelevant to NLP which is some a little bit of a hot take that I have uh as of late so I've been going to a lot of these like meetups uh in New York I'm pretty new to the city uh including the Wii v81 and they often have you put like name tags on and what I've been really into doing is just like putting some kind of like contentious opinion on these name tags and I I think that one of the most important things as somebody that's building and trying to explore and leviates doing a phenomenal job at this is building in public and really showcasing not only like the final released version but all the steps that you got to get there and not only does it help you build an audience on the way but it really kind of showcases that you're a human and you're a little bit vulnerable because you're kind of maybe nervous about the steps that you're exposing but it unlocks so many different opportunities around getting feedback at every stage garnering like a really strong evangelist uh user base and the the best example of this is really open source right if everybody can see every uh pull request every commit every issue then it's really quite obvious how everything is going and especially if you're just like a sole entrepreneur engineer it's like everybody can see hey this is the progress this guy has made on on this product I don't know uh I don't know if you have any projects that you've done independent of we V8 in the past but like is that something that you've explored and are are there any companies that are doing that really well yeah oh I love this up again well I yeah because I think about it a bit I think like um oh it's very interesting it's like this question about like the business model of Open Source kind of in general and then as like as I think open source is also kind of like a Content strategy in a way like you you you're like Yeah by like constantly doing these releases and then explaining all the details of it you curate an audience and a lot of these products like having a community that's like super valuable because especially if they're making pull requests and things like that like I know like with yeah yeah like I think Lang chain is a great example of something that's achieved this like you know people especially with like the Integrations Integrations being such a massive part of this like with other software companies and it like so yeah like if you're creating this content and then um that's kind of what I like about doing this podcast as well is it's like the potential to you know have somebody who doesn't necessarily work at weaving but you can kind of like highlight what they're doing and it kind of incentivizes everyone to work together yeah it's pretty fascinating but I I think some parts of the business I don't know I like I think closed Source because there has to be some kind of Advantage unless you're like a Marketplace business like something where the community is the mode well the community is is often like a really powerful Competitive Edge uh I mean as someone that worked in an open source a company for a long time I can I can say that but I think the real moat is around the abstractions and like managing an open source project is is really going to be a challenge if you especially if you want to have some kind of semblance of high availability and an SLA guaranteed to your customers and so therein lies the importance of like having these servers all managed for you and there are some interesting companies that are like really abstracting the server uh the the like server architecture of I'm sure Mosaic is doing that to some degree uh there's one company I won't give them a name but uh they are purely just a decorator in Python that says hey Run This and like uh run this as a serverless function uh there's a couple of companies that are doing that that's why I didn't want to name anybody but I think that for me at how I'm trying to make myself uh mix be competitive is really just around hey let's do our best to abstract a lot of the patterns that we're seeing across these companies and for us it's like everybody is doing everybody is probably storing their files in some kind of content repo like an S3 they're all trying to extract the contents they have very strong variants of files we want to extract the contents in a way that it's maintaining the uh the the structure of the file so for example paragraphs and pages and PDFs rows and a spreadsheet we want to maintain that and then offer some kind of search interface that spans all of them and all of these are like all these different steps and if we were just to open source the entire mix be code based and we could probably people could probably do a lot of that but they won't get the same experience as just like a two API call kind of thing and hopefully you guys are doing are exploring that in the same way with weva cloud yeah yeah that is really interesting I get maybe um I if you could teach me a little more about the serverless thinking we had Eric bernardson on the podcast to talk about well I'm thinking of modal yeah and uh yeah I saw your article with uh banana as well could you kind of explain to me what these companies are doing okay just yeah I mean I I had an idea of like in in any industry the companies that kind of consolidate all the different steps tend to do really well uh and it is kind of compounded in the ml space which is like the more data you have the more valuable you are the more like we said the domain specific models to every customer is is the future uh like what Mosaic is doing and so uh if you're baking in the ability to spin up and inference engine a model in an architecture that is both cheap and fast and you're able to do that in a really simple way and expose that to your users then it's it's a really powerful asset that you have in making your competitive mode as as a company it's like the analogy that everybody understands is like everyone is coming from Google everyone's coming from Facebook they have expectations when they interact with your software and that expectation is it's fast it's accurate and very often it's a lot more affordable uh so and that's really why serverless is such a an interesting space but what's really fascinating and this is something that after talking to a bunch of Engineers at AWS gcp and Azure is like is that it doesn't seem like any of them are really attaching attacking these serverless GPU space I don't know why but like I've seen articles I've written articles around hey if you want to do like serverless gpus then you gotta create an elastic Cloud an ECS uh architecture then attach uh whatever the GPU instance is and kind of have scaling baked in with maybe like kubernetes and anybody that's used kubernetes knows that it's like the biggest pain and so like modal banana they're all abstracting the kind of deployment of these models via the serverless gpus and I like modals approach which is just a decorator in Python I haven't actually used banana but uh yeah Eric's great guy yeah that's amazing I I've always been so interested in this I mean full like disclaimer I'm like I spend most of my time reading research papers and stuff like I'm just these are just like we're on the time of edge not an expert on this but like the the thinking of like the kubernetes and the scaling different resources for different kind of uh jobs like I was always kind of as a as studying deep learning I was always really curious like how's weights and biases like how are they valued at like a billion dollars right because it's like a it looks to me like hyper parameter logging tuning logging and you know I did like some like marketing work with determined AI where they were also doing this like uh hyper parameter cluster thing and so I was like starting to learn about this kind of cluster management thing and I kind of came to this thinking that like the kind of like the callbacks require different resources than the training and so you know having this I kind of like with weeviate like where you uh have some resources for the weeviate and then you have different resources for like say the query embedding container like this these require different kinds of uh computers and like all that kind of thing and it's pretty I mean I I like I really don't know too much about kubernetes but like or like what the particular pains are but I that's how I understand the ideas like yeah like serverless to me it sounds like if you want to just have a query embedding model on you know running you just you know write decorator on a python function right and that kind of thing is super cool um yes I don't know if I have too many ideas on this but I think it's super interesting I mean Leo like what is the big kubernetes is a pain problem I've heard this so many times but I don't really know I think it's it's around like there's two there's two aspects to it one and I'm also not a kubernetes expert by any means I've just use it it's it's probably the fact that I'm not an expert which is like I'm creating a little bit of a bias in me complaining about how challenging it is uh which is like a kind of uh a weird situation but for me it's really like the creation of the kubernetes cluster in addition to the maintenance of it and I what what could be and this is my own theory is like when you have distributed inference engines the state is in consistent across them and I'm sure Eric could make some explanation on why uh it's like serverless environments can overcome that challenge but like if you have an app server and you're Distributing the workload across three different servers and you're routing the query across one of those three different servers how do you guarantee that there is like a state between the servers like the perfect example that everybody could probably understand is this concept of context with the chat EBT so let's imagine we have three different serverless functions or one even one serverless function and every time a user is calling this decorator it is deploying this inference on let's say serverless environment a and then another user is doing serverless environment B how is context shared between them because they don't there's no State it's serverless uh I I I'm probably just speaking out of my uh my key stir here because I don't know much about the space but that's an area that has always been challenging from purely I'm an app a software developer not a researcher but that's always been an area of challenge with like serverless functions and databases is you need some kind of State Management yeah amazing I've been learning about like replication consistency from we V8 and and it sounds is he related to the guy that created the Lampert clock oh oh no he Nathan hasn't been on the podcast yet just a friend of mine I'm talking about uh and I don't know about the clock thing but um yeah the Lampert clock is like the foundation of like distributed systems and like consistency oh God you gotcha yeah I've just yeah so I definitely didn't like study too much replication in school but I'm now listening to everyone like Eddie and Parker and red one and I think just you know being able to fly on the wall in these conversations but like the um yeah that kind of thing of like if I have if I need four gpus to run my chat gbt inference I suspect that's like the Azure cloud has maybe been built around this in tandem with open AI if I was because yeah that sounds like a terrible problem I think and I know the companies like Ray that do this kind of like uh distributed GPU management it's super interesting I mean I'm more so interested in this company called neural magic that's trying to compress the models and you know either run them on CPUs or they recently got the hunt there's a research paper with one of the two researchers has the neuromagic affiliation that's run it on a single GPU the 175 billion parameters and I like quantization sparsity these sparsity is like one of these things that hasn't been realized as like the lottery ticket hypothesis is like you could train the sparse networks from scratch but now there's like a lot of like okay how do we really realize sparsity but yeah I'm talking about it like user too like I know I'm thinking about these things too often but I do think this is yeah it really sounds like what you're saying is the the future of a lot of these models is you realize just them reducing in size but not only size but the ability to run on more so commodity Hardware because not everybody has other than the Bitcoin miners out there and not everybody has some like really significant GPU uh setups even in the cloud yeah yeah that's what I I mean I think you know they got the uh the alpaca model people are like running that on their phones so I was like yeah I saw Raspberry Pi running it which obviously implies there's no GPU involved at all I'm sure it's slow as hell but the fact that they got it running is just a magnificent accomplishment yeah yeah and I yeah I mean this I guess it's like the the the Transformers I think became so popular because of how it uses this big matrix multiplication thing for gpus but maybe new architectures like there's obviously like this sparse uh mixture of experts kind of model and yeah like maybe new architectures that don't that aren't so there's like this great paper from uh Sarah hooker called the hardware Lottery that's like talks about how much the hardware has influenced the architecture decisions and deep learning and these kind of things and yeah it's definitely not an expert in this space do you think in is there like the idea that the decision to build the models around these really large matrix multiplication and arithmetic is that really uh was that decided on because of GPU popularity you think uh or I guess are there other options when you do machine learning I've only known about just like the linear algebra um well I think um yeah it's like I'm trying to think because I I'm trying to think about how the difference in the computation with like the convolutional kernel is compared to the like attention Matrix multiplications and I do think that the convolutional kernels are implemented by just kind of like replicating the kernel and then making that also a big Matrix so I'm not yeah I'm not exactly sure yeah about um exactly how the Transformer utilizes more of the GPU than the convolutional model does or or if that is really the argument but I think it's just generally this thing of like you know big Matrix multiplications yeah as far as I understand it yeah no I mean no I'm definitely not an expert at all uh but it is interesting to see like there's only a couple of Hardware companies out there and they are I mean Nvidia is basically open AI at this point in terms of their like market capitalization on the ml space so uh it'll be interesting to see who else is gonna take over in the in the GPU offering space and maybe there's other options that aren't gpus who knows I went to a I went to AWS re invent uh last year and there was a talk on quantum computer and she like the lady I think it was uh Oxford research company she brings out this like physical quantum computer and it's like the size of my desk and I maybe that will be capable of some of these uh calculations at some point who knows yeah that sounds like a good topic for like a PhD student like yeah yeah one other point that I want to leave with before we go is like there's a lot of fud and real fun fear uncertainty and doubt there's a lot of fud and like I said before like existential uh threats and kind of just feeling people feeling generally really struggling with the idea of like these models taking over and while a lot of it might be true uh should always encourage like remove yourself from the situation I mean uh I'll tell anybody that wants to listen about this but I lived in a camper van for a year before moving to the city like outfitted a a Ford Transit High roof with electricals running water satellite bed everything and really just like removed myself for a year and I encourage everyone to not do that that's obviously pretty extreme but like operate in Sprints and uh and coasts so like you could Sprint for a couple of months but don't forget to coast and whatever coasting is for you like do it for uh uh like a like a fixed amount of time continuously it's not enough to in my mind just take like a weekend trip here or there and I realize not everybody has the the benefit of this but it's really helpful especially when you're trying to figure out how to uh resurge whatever project that was just rendered obsolete by the plug-in Marketplace but yeah and if everyone anyone wants to see how to build a van it's vanlifecoder.com is a a Blog that I maintain documenting the build out process and the travels and I'll be taking it again this summer hopefully to Northern Canada that's awesome yeah that's super awesome that's great advice I think um yeah it's I I think everyone's felt a little bit of existential like what is this with the gbt4 I think yeah people who are so dismissive of it I'm like it's obviously scary like taking over every single job I don't think replacement is the right word but like assets is certainly the right word anybody that's not using it is certainly going to fall behind yeah it is pretty intense I mean I think a lot I think the like the societal issue is like will this just cause more concentration of power at the top because now you're like the top can like has all has even more leverage with just the information uh control yeah yeah I think productivity is probably the best metric to to look at I mean if someone's able to produce more in less time then they clearly have an advantage hopefully these models again it's probably still in that like open source alpaca llama space once these models do become more and more commoditized and democratized then it does even the playing field and actually create equal opportunity uh for all which is really one of the most important things with creating like a a really robust Society is like if everyone has access to the same Baseline uh chat EBT model then I think there there's that even playing field and sure everyone's going to have their advantages in some capacity but we should do our best as a society to ensure that everyone is like doing their best or everyone is exposed to the same like Baseline opportunity and there's a difference between opportunity and outcome uh there will always be people working harder and there will always be people that have a little bit more of an advantage but if everyone has the same Baseline opportunity I think that's what we should strive for yeah there's a lot of great topics around this I think um I've recently listened to Richard satcher was on the gradient podcast and he was talking about uh his work with the AI Economist paper and this kind of like running simulations to inform policy decisions where the where the agents in the simulation are reinforcement learning controlled like to you know make it a little more realistic of a simulation and all these this kind of ideas uh yeah it's super interesting I mean I do like Francois Chalet had this interesting tweet where he was like uh saying that uh oh I'm gonna but I'm gonna do this like crazy but he's saying something like uh thinking that gpt4 is like super intelligence is similar to thinking that like a 3D printer is just like arbitrary manipulation of matter that you can just like and I don't know like that kind of thing of like how are we like it's scary but is it that far is it like complete control of the universe so to say you know like I don't know if it really is that in that productive yet but it seems like it's on the way to it yeah it's definitely on the way and I think that this plug-in Marketplace some of the use cases that I saw on the website are really like oh a phenomenal the paper that I was mentioning by the Microsoft research team which again is really biased because open AI in Microsoft are obviously together uh but they analogized this ability for Chach EBT to figure out problems on its own I Via questions Etc with the time in which Homo sapiens discovered their ability to use tools so like for example openai or chat gbt was capable of kind of like calling a third party API when it wasn't able to figure something out on its own it's clearly demonstrating enough reasoning to use a tool to accomplish its goal which is again that same overlap which is Homo sapiens discovering how to use a knife what fire is so we could be seeing like the dawn of a a new species a digital species uh and maybe there will be that like Evolution uh staging of of this new species who knows yeah a lot of sci-fi uh articles around it uh yeah I mean before we go I do want to stay on this little like I've been thinking a ton about this idea of how with language models they can have this like role playing like one language model is the writer the other is the editor or say wevier right where we get is like a remote company where you know for the most part we interface via texting each other and writing code and pull requests and like having calls with people outside the company and things like this it's like you could have the you know the core team full of language models that basically the difference is like what information the language model is hooked into so like if you're on the core team I don't know like if I'm sampling someone I don't know but like you know people access different information and I think what we'll see is like entire like digital like companies that are just like language models with different roles it's kind of how yeah I mean you're saying that there might not even be a single employee at a company it'll just be all these large language models that have their own personas and maybe they're trained on their specific skill sets you've got a large language model that is specifically marketing you've got one that specifically sales engineering Etc I could see that I did see a tweet I don't know who it was but he basically posed a challenge to Chachi BT to turn a hundred dollars into two hundred dollars and he was like do whatever means possible and I think what they amounted to was some kind of like affiliate marketing uh project which is like cool and all but like what are you gonna do with that so it kind of uh it suggested a domain name it wrote the website it uh reached out to a bunch of Affiliates and it kind of built the Analytics uh structure so I guess each of these they need to be modular right you need to be asking these questions in a little bit of a modular fashion and I think this is also something that chat TBT struggles with is like every ask is sequential and that's really what chat EBT excels in which is like you ask a question it gives you an answer and you ask it to tune that answer or replace it or whatever that sequence is how we all think but what if it could uh kind of go way back into some other state and this is again like that statefulness if it could go back into another state and uh resume some kind of context there and this is probably where the layers of large language models comes into play and even these are like the concepts of Agents maybe you have a version that's doing X and A version that's doing Y where Version Y has access to the context of uh I don't know some other agent uh this is interesting I don't know this is the first time I'm thinking about this but there's there's so many different opportunities this is a gold rush certainly yeah it definitely is and I really liked your article about the content verification layers because I've been thinking a lot also about like you can sample many different decoding Pathways it's like you're saying with the stateful thing like you know it's okay you know it's thinking and then it gets to this node and then you have like this like I think temperature is now like just if I say temperature people will understand that you can adjust the temperature to get different more random more deterministic outputs from the language model but really the way it works is there's like this probability tree that it's decoding through and you could take many pathways through that tree to get several different generations and yeah I mean yeah that is like because the thinking with the content verification layers is like you sample all these outputs and then you just kind of like filter it that way like obviously the most of the filters I think are right now like more like guard rails of like whoa don't say that like that kind of thing whereas I feel like if it's like writing code maybe a classifier could classify if this would compile or not or not even a classifier maybe I don't like I don't I don't know if that's a great example because like obviously you could like compile it but like this kind of thing of like sampling anyway so I do think I've gotten totally off topic uh Ethan thank you so much for joining the weba podcast I mean we covered so many topics that like really challenged my thinking and a lot of these different things and I hope I didn't say anything too stupid but yeah no I mean you could always censor it after post-production but no this is great Connor uh always interested in talking to people in the space and uh if anybody wants to reach out I mean I'm happy to talk uh building out this this project this startup doing it in public there's mixpeak.com then there's kali.ai and I am uh Ethan steininger on anything I mean I'm pretty googleable or I guess chat gbt maybe I should check if perplexity.ai definitely does I'd encourage anyone to search their name in there it's really cool yeah yeah it's super quiet the perplexity thing I mean I think yeah just the whole like kind of the new era of like you.com Neva these like just brand new ways of thinking about search yeah yeah yeah that's really cool cool ", "type": "Video", "name": "ethan_steininger_on_mixpeek_and_the_ai_landscape__weaviate_podcast_42", "path": "", "link": "https://www.youtube.com/watch?v=EDPk1umuge0", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}