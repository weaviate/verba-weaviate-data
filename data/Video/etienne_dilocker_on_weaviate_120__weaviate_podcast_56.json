{"text": "Hey everyone! Thank you so much for watching the 56th Weaviate podcast with Etienne Dilocker on the 1.20 Release! Check out ... \nthank you so much for watching the wevia podcast I'm super excited to welcome we V8 CTO and co-founder Eddie and dilocker for the weeviate 1.20 release podcast this is another packed release Abate with all sorts of cool things multi-tenancy PQ rescoring Auto cut re-rankers a new hybrid uh rank Fusion algorithm and some Cloud monitoring metrics every time we do these release podcasts it's always so much fun I always learn so much so Eddie and firstly thank you so much for joining the podcast thanks so much for having me same for me I love talking about these things I love the the great questions that you always prepare and I'm very very excited as well to talk about this one yeah amazing yeah I love just like the breadth of it going through all the different topics in weaviate and so starting off with I think just a super exciting topic diving into the database thing multi-tenancy can you tell us uh maybe just to begin in the highest level abstraction like the overview of what multi-tenancy is yeah yeah what it is and sort of why why you need it even in the first place so multi-tenancy uh for us and I always feel like I need to sort of because when people hear multi-tenancy they think of cloud operations sort of share resources and Cloud operations and of course we have a cloud service so multi-tenancy for us is not about how we run the VB account service yes you can use multi-tenancy on the cloud service but it's actually about multi-tenancy for you the user so let's say you have an application your application has separate users and they have somehow data that needs to be separated from one way or another so for example let's say you build an app and your app allows you to index documents that you have on your hard drive so maybe just sort of install something on your let's say something like Dropbox or so you want to search to all your your documents you only want to be able to search through them yourself you definitely don't want other users who happen to be using Dropbox to be able to search through your documents so that's kind of the the idea of scoping that to to individual tenants so you as a Dropbox user in this case would be a tenant or it could be that multiple users maybe instead of Dropbox it would be a workspace on a notion or an atlassian Confluence or something so in this in this setup basically a group of users could be attended so it doesn't necessarily have to be an individual user but it needs to be some kind of isolation unit basically and this is this is so far this is not even like a technical requirement that's essentially just an application Level how do you curate what do you have access to requirement uh but then where this gets super interesting for Vector search is that it kind of mixes with the technical requirements and it's almost like like it's sort of it perfectly aligns because in in Vector search we have the problem that we need to somehow figure out how to sort of limit the vector space and we've talked about H and SW the sort of the indexing graph in the past and now think of this whole graph that contains maybe a billion vectors but these billion vectors are spread out over a million tenants now you would have to sort of assuming there was no multi-tenancy you would have to essentially cut that graph into a very small chunk that only contains about a thousand objects each though for a million tenants have a total of billion objects that's only a thousand each uh chances are this graph becomes either it becomes disconnected or you have to travel a lot through that graph without sort of hitting notes basically that you're not allowed to hit or you're not supposed to hit so this single graph and filter kind of approach is at best the workaround like you can we have this in mediate we have this flat surge cutoff basically where if the the filter becomes too specific you actually do a flat search so this would be one way to work around it but then um you sort of lose the whole benefits of of uh the fast paid or the high throughput and low latency kind of search that you expect from hnsw so there is a need to to basically also do this kind of separation from a technical level and this is where we said okay enough workarounds another work around that that users have semi-successfully used in the past was to separate this on a class level because in deviate a class is already a an isolation unit so you could sort of uh say per tenant you create one class and they would all have all these classes would have an identical schema because they're it's all the same use case basically you just copy it for individual users so your schema keeps on growing and growing and growing and like each schema update made the whole thing slower and slower and this worked okayish for maybe two to three maybe five thousand tenants and one workaround and then really we're talking about work round after work round uh another one was then you could turn off graphql because part of the the part of rebuilding the schema part of what took so much time was rebuilding graphql so you could turn off graphql only use grpc that would make it scale a bit farther but we were really in the territory of like this is this is not a long-term solution this is this is happening from one workaround to another so we said we really want a dedicated multi-tenancy solution where the apis support tenants where uh the the architecture under the hood supports a lot of tenants and most importantly where this also somehow scales linearly and we can talk a bit more about about scaling yeah it's so interesting the um I remember when I first heard the question about doing this kind of thing I heard uh you know I was at the Meetup in New York City and someone said you guys support our back role-based access control and you know the time I'm thinking also that you could just have that filter through your class where you if you have like you know a document class and you have content you have user and you know user edian user Connor to only look at like Conor's emails instead of Connor seeing edian's emails and so hearing about the limitation of that as you know if you connect to hsw graph the filter it might not be connected still so you need to modify hsw itself um so as you were giving that explanation it really helped something click for me is the the difference between just kind of naively using multiple classes in weave as we understand it you know like I have an Eddie in class I have a Conor class I have a John class but so can you tell me a little more about the design of multi-tenancy and how you have native multi-tenancy and I think really to uh separate these two things hopefully it's not a selfish question from my understanding but this the difference between just creating a bunch of classes and you know maybe at 1.19 compared to the native multi-tenancy in 1.20 absolutely this is definitely not a selfish question I think our viewers and listeners will will absolutely appreciate that as well so um the this class-based workaround kind of work because one thing that a class already does is it creates something like some separate space somewhere uh basically in the class and internally in V8 this is a chart and within one class you could have anywhere between so so this is in the traditional mode without multi-tenancy you could have anywhere between one and any number of shards and A Shard is essentially you can think of everything that's in the databases contained in that one chart and whenever there are two shards and you want to maybe query across two shards then under the hood this is split into two queries each Shard does their part on their own and then somehow aggregated again um so this is why this workaround kind of work because by creating 10 classes you also under the hood created 10 charts um it was kind of doing a lot of overhead for essentially you just wanted to end up with 10 charts but you could kind of do it with with creating 10 classes the multi-tenancy feature the native multi-tenancy feature in the simplest terms you can think of it it's a single class but within that class we create one chart per tenant so shards are now no longer this static thing but they're completely Dynamic like you can add them on the Fly you can delete them on the Fly and um the the kind of cluster association with a chart still holds true so I need to need to explain a bit more for this probably so in a vv8 cluster let's say you have a cluster of three notes let's make it simple like multi-10 so you can go hundreds of notes but let's keep it simple three notes and let's say that in our class example uh you would have each class would just have one shard um A Shard is something that that can't be split further basically like if you want to split it you need you need more charts so in that old setup this chart with the specific configuration would live on one of those exactly three notes so one way of Distributing this around the the setup of it would be if you have 30 tenons or sorry in the old set of 30 classes I mean you would have 30 tens but you would model them with 30 classes each node could hold 10 of those basically and then that's that that would sort of be evenly distributed um but you'd have very little little control now with multi-tenancy we keep that idea of having one chart um but the shards have become much more lightweight so you have essentially we've run a couple of load tests and um we could in one example this depends a bit on on sort of what you actually what kind of properties you have in your class and these kind of things um and essentially the bottleneck is just the file descriptor limit that that Linux systems have and one test we could reach 70 000 charts per note and per node is now where this this sort of very interesting part comes in because you can just scale this by adding more and more notes to your cluster so um you would have a single class that class potentially spans the entire cluster but a single tenant still is isolated to one node so this this and and to make this a bit more complex we could then also add replication because then you can make sure that this node doesn't become a single point of failure but for now let's just ignore ignore replication so you could have you could start with a three node cluster let's say your tenons are really huge huge and you would only fit 10 per per note then you could fill up to 30 tenants on that cluster now if you want to onboard more users you just add a new note and per note you would again have roughly 10 tenants capacity and in reality it's more like 10 000 but for for our example that that um yeah makes it easier to to reason about and then vv8 under the hood make sure that you hit the right note so of course as a user you don't know where that stuff is scheduled but will be able to say like with every and this is the the only real API change for multi-tenancy is that now you have to specify your tenant key so you don't have to use a filter basically um in your let's say you do a a get near text search then you just have an additional property that's the tenant and then you should specify that and vv8 uses that under the hood to figure out sort of where in that three or five or five hundred node cluster where does the tenant actually live so then um so so I'm curious now about kind of maybe the design decisions behind the tenant key and kind of the the reduction of the sharding the sorry like taking down the size of each Shard maybe we could step more into the technical details behind what it means to have a Shard be lighter weight and dynamic yeah yeah yeah so um the the idea of splitting by Shard or by class or by index type for multi-tenancy that is not new I know for example that um for for if you need to be gdpr compliant and traditional search engines such as elasticsearch you would also try to use that same pattern where you would create an index I think it's called an elasticsearch per 10 and then to have that strict isolation so that is kind of where also this is essentially our class-based workaround um but this always yeah sort of comes with with limits so what we said is we need to make the chart more lightweight and lightweight making it more lightweight is essentially sort of a an umbrella term for all the kind of things that kept us from um from having sort of a lot of charts on the on the uh on OneNote one thing for example is asynchronous processes so since A Shard is its own contained unit in the past we would have lots of async processes so so for example for uh the agents W index for maintenance that would be an async process for every property that's stored in an LSM store one async process would be to switch uh from ment table to segment so when a mem table is flushed that's essentially the memory storage is then to disk then you have compactions in the background so you have all these kind of sort of background processes that tend to be relatively lightweight but let's say you have 10 per of them per Shard and now you have 10 000 charts because you have 10 000 tenants now all of a sudden you would have a hundred thousand of those backup processes and in our very first test before adapting anything we could see that essentially all CPU time was now spent on just idle background processes doing nothing and then to make this even worse some of those doing nothing kind of processes uh for example the ones that would uh check for whether a LSM segments need to be compacted they did that using a discrete so a very innocent sort of simple discrete of hey what is the state on disk right now but now this happens a hundred thousand times in parallel and now you hit your disk with all these unnecessary reads just to find out that you don't have to do anything so very simple changes such as sort of yeah making sure can we cache some of that information can we check less frequency less frequently if there hasn't been a change in that much time uh can we yeah so if all these these kind of kind of can we combine uh internally I think this thing is called the cycle manager and there's one discussion that we have like instead of having 10 per chart could we maybe have just one or could we have less than one because we take that outside of the chart so all these kind of optimizations um that enable us to run more charts under hood I'm essentially making them them more lightweight um another thing is also the memory footprint so in our very first test um we had a surprisingly large memory footprint for an idle chart don't remember what it was but I think it was something around five megabytes or so where in a single class single chart setup this would just never like you wouldn't even notice that that like you have one class with one chart and now you have five megabyte of memory usage not a lot but now again times a hundred thousand or times ten thousand all of a sudden you have this like 50 gigabyte of idle memory for or not idle memory 50 gigabyte of use memory for what is essentially idle classes so that that was another optimization again sort of a very simple optimization which is to to make this more more lightweight just look at what kind of memory how are we allocating memory how are we doing this dynamically are we sort of are we a bit too optimistic about where the the chart is going to grow to and just sort of setting more reasonable defaults making sure it can still grow so there's essentially no no negative user impact everything can still grow but just the defaults are more reasonable and more aligned for for having many of them and that's kind of what we mean by by making them more lightweight amazing that's such a clear explanation of it and um yeah it's it's really interesting hearing about these background processes um you know hearing about the Compact and merge I don't know too much about the LSM myself but that explanation I can understand how there would be you know background processes that check on the database and that kind of thing so um yes you mentioned the um you know seeing the five megabytes of overhead per class and you know kind of the insights that you gain by testing it I think that transitions nicely into this next question that I'm uh how have you been testing multi-tenancy and I guess it's kind of I think like some like what are the lessons from it like it sounds like with the megabyte thing you you know even though you have such a deep understanding of weeviate internals you still learn from your tests yeah that makes I'm curious like how this feedback process of testing it how exactly it's tested and then what has been illuminated from the tests oh yeah absolutely this this feedback cycle goes sort of Beyond just the the test it's also it's user feedback like I think the the whole journey of getting into multi-tenancy started out with user feedback saying like hey I'm trying to apply these workarounds but now I'm seeing large memory footprint or now I'm seeing seeing stuff slowing down or emptying my disk being hit even though I'm not querying and these kind of things and and that was I think in multi titanically that was the first thing that we that even made us aware of hey there is a need for something new there's a need for for a revolutionary change basically not just not just extending the the workarounds and then um I think they're in testing you could say that there were two major faces so the first one was initially when we started out this was in proposal phase and we just wanted to say like is this a viable idea we have the proposal out on because we do this out in the open of course we have that out on on GitHub and just it was sort of a mix of this is why we think it's technically feasible this is what it would provide and uh just ask a couple of of users for feedback and if that would solve their cases and that was overwhelmingly positive so that was great so then we just did a very simple load test essentially the the old setup which because we knew that the shark is going to be a Shard yes we're going to make it more lightweight but essentially we can already create a chart so I think the first test we use the classwork around but then we also increase the number of shards per class because our end goal was having as many shards as possible and then we we um I don't remember the exact numbers but let's say it was something like 50 shards per class and then we just kept on adding class after class after class and at some point we would say maybe hit 10 000 and we would say like oh all of a sudden the next query is now failing I'm going to investigate like why why are the queries failing what is what is going on and that that would be part CPU profiles for example seeing what do CPUs spend time on that is when that whole background cycle sort of thing became apparent memory profiles just to see like where is the memory actually used right now um simple queries or sending queries where we said like okay this query in isolation should be fast but now in this large setup it's slow and then sort of investigating working backward from that where where it is that that was the initial phase where we saw okay what we have right now is not a good sort of not not the final solution but we're well aware of what these problems are and how we can fix them sort of with the final solution so that was essentially proof of concept that we're on the right path and that was that was sort of it it failed successfully like it failed in the places where we expected it to fail um but also it proved that if we fix these kind of hurdles if we get rid of those hurdles it would essentially work and that was before we rode any kind of line of production code so that was basically back in you could say in the 1.19 release cycle we kind of prepared for the 1.20 really cycle where we've built multi-tenancy uh then came sort of a classical implementation phase of course Implement implementing that has lots of tests and everything and we would sort of constantly try to evaluate but also sometimes you just need to wait for it to be sort of like for to reach a certain level of majority that you can do sort of these these um Black Box end-to-end tests where sort of in unit tests and integration tests and and even to some degrees end to end tests you always have some kind of knowledge of the internals but with these kind of end to end really end-to-end black box um uh sort of API level tests you just don't know anything about the system you only get to use the same functionality that your user would use and then you just try this just try to replicate how would a user use it and and see where it goes and there was a point I think about two weeks or so before before the release uh when red run from our team I asked him sort of like hey what what can we do right now to to sort of help you give you confidence in the release because he took a lead basically on the whole Cloud orchestration side of multi-tenancy and he said please break it for me try to break it and then we got together and we tried to break it and only we found a couple of there was no no uh no fundamental issue but we did find a couple of things and um it was great that we found them because I think some of them like some of them were a bit on the edge case side that would have probably taken a few weeks for users to run into them some of them were a bit more obvious we really could find them out right away and then um we built this more or less elaborate I would say load test setup where he just kept on importing kept on querying measuring all the the metrics like what so so the the linear scaling was extremely important for us we wanted to make sure that if we have say a I'm trying to use the actual numbers that we used but I think we started with a three node cluster and we aim for 10 000 uh tenants per node so we'd start with a three node cluster with a total of 30 000 notes and then we said hey if we turn this into a nine node cluster and instead of 30 000 we would do 90 000 attendance then we still have that same ratio of ten thousand per node so everything should scale the same is that the case and then we tried and we could see and this was this was like one of those moments where it's like yes we've reached linear scaling where we could see that the the nine node cluster essentially is three times the three node cluster and that is that is very very comforting because then you know okay now most likely the 12-minute cluster is also going to be four times as large as the three note cluster and and all these kind of things and that gave us the the confidence that hey the these claims that we make about if you want to extend it just add more notes so this is really true and there's no additional overhead for say um for for import time or something because the the orchestration needs to happen in the cluster is minimal uh the node that owns the chart basically does all the work so by adding more nodes to your cluster you're not just are you adding a more more space in a sense like more disk space and more memory but you're also adding more compute power which can take off the load so um a very simple way of scaling import throughput essentially just using a larger cluster now is the the second testing phase where um sorry that gave us the release confidence and this is something of course like you would do testing and automated and in manual and explorative and all these kind of stress testing chaos testing you do this before any release but I think for for this release this was the most amount of testing we've ever done so this is the I would say the most confident we've ever been about a feature and this is mainly because we knew we have stakeholders that were really waiting for this and they're really saying like hey this is sort of not having multi-tenancy or not having a multi-tenancy solution that scales to millions of tenants is what's keeping us from from reaching the next level with V8 or maybe for others keeping us from from using vv8 in the first place so we really wanted to make sure that while we knew that there was a limit with the previous solution like now we want to confidently say these limits are gone you can use it for the kind of scale you want and now essentially the the only limit that there are two limits one is the number of file descriptors so that varies a bit and that is is local to one note so um the safe estimate is around 50 000 tenants per node most likely you're going to run out of um out of uh other resources before that um so that's the the one limit um so so upwards of fifty thousand per node and the other is just resources so resources is something that um that that it's just Vector search in general has nothing to do with multi-tenancy and that is also something that's easy to increase you just add more notes so these are the the two theoretical limits for number of tenants and both are very easy to overcome because both linearly scale with the number of notes in a cluster yeah it's so interesting hearing about the uh like try to break it test I think that's been one of the like one of my favorite stories following along with weaviate has been like you know the sphere tests and you know blog posts about that and you know trying to get a billion nodes into eviate and that whole like load testing thing has always been so interesting like I remember it's kind of like a theme of all of AI like you know like with language models it's like how big of a language model can you train it's like oh we train a 50 billion it's like hot so impressive and this is kind of like our analog of that is like how many vectors can we put into a into one index and I think also kind of seeing it you know across indexes and you know one kind of system across the nodes is so interesting um I have a quick kind of clarifying question selfishly for me hopefully there is a listener with it too so when you're sharding a class so I understand that each class you know it's the um you know it's the document class and I have a million documents in it and I have one vector index so when I Shard this you mentioned like searching each chart separately and then aggregating the results somehow can you maybe take me a little through further how you Shard a class so here we really need to um separate multi-tenancy from single tenant cases because in in multi-tenancy we use the these shards as isolation units because you only want to search through one like in in multi-tenancy like your query is for a specific tenant so we'd say there are 100 charts each corresponding to one Tenon we would pick exactly one of those hundred we would have that chart serve the request return it to the user nothing nothing changed so this is kind of the the um like a small portion of the data on the note is queried in isolation in the entire query is completely sort of self-contained in that chart if you Shard a so so for that case it doesn't matter how many charts you have like even if you have a million charts because you're always hitting exactly one there's never any overhead for for number of shards like whether you have whether you query one out of ten or one out of 100 or one of out of 100 000 it's always the same you're always querying one uh but in a single tenant case the motivation for sharding is different so in a single tenant case you don't have the tenant key so the results that you expect are the entire Vector space so now your motivation for sharding is kind of the uh the the other way around like you don't want to have as many charts as possible you want to have as few shorts as possible so for this the question is basically why do you want to so if you if it's better to have fewer shards why do you want to have shards at all and this is uh where where sort of um the the scale of a single index comes comes into place with respect to the hardware that is scheduled on so that was a very very complex way of essentially saying like you can only fit so much on one machine and if you need more than that one machine you need to Shard it across two or three or four so that is the motivation in let's say we have an index of a billion objects and each machine could only fit 250 million then you would chart that across four machines and now if the query comes in all four machines would say hey okay I'll give you the top 10 results out of my 250 million so you would now and end up essentially with four lists of uh each 10 top objects now you have to aggregate that list again that's super easy to do because you have let's say the distance metric so essentially you just Resort that list of 40 and cut off the the bottom 30 again so you remember you have the the top 10 remaining that is easy to do but from a sort of scaling out perspective you need four nodes to serve your query whereas in the multi-tenancy case because only a single node hits sort of owns the the data for your one tenant also only that one tenant needs to answer it which means all the other nodes and all the other CPUs on that node are idle to serve other tenants basically so when you when you have the hsw graph is there anything to how you partition so imagine like on layer zero I have like these are clustered and so we're going to go Shard together is there anything uh the the chart is at a higher level so the the hnsw index would be fully contained within them and it wouldn't even know that there are other charts like the the shards you can think of this like class Shard and then in the chart you would have Vector index uh regular index Etc so the the agent's double Unix doesn't even know that that said this is something I think that that would be interesting for for future research to see like could we Shard instead of sharding by sort of an application Level attribute so right now we use a hash on the ID you could also potentially Shard by Vector proximity so that would be the case where sorry if a bit similar to how IVF based uh indexes worker you have these kind of buckets so you could say if I know that my data set is going to have 10 charts could I try to sort of pre-select one of those 10 shards based on proximity and make sure that the vector sort of ends up in in The Shard that's closest to it problem is what we know from from IVF based indexes is with a multi-dimensional distance it's not as easy like in a sort of it's not a binary decision it belongs into bucket one or maybe along so in bucket ten so in IVF what you need to do is you still need to query multiple buckets like the let's say out of 100 buckets you still need to query the the top 25 closes so that's a like interesting for for for research um but currently that's that's not something so currently the the chart is at a higher level in a in a um single tenant sharded setup and the vector index within that chart wouldn't even know that other shards exist yeah it's really fascinating thinking about the you know like the vector indexes and then how you distribute Vector indexes across Cloud computers across the world and this is all really exciting I think kind of we're transitioning into this like you know future improvements Future Works uh topic broadly I I hopefully we didn't go too into sharding particularly but like back into the multi-tenancy like do you see you know what are some things that are top of mind now that rolling it out yeah there's one aspect that I haven't even mentioned at all yet that I'm super excited about um what what we wanted to achieve with 1.20 was we wanted to have a stable API we wanted to have uh sort of the main functionality which is scale so anyone who was blocked by not being able to to onboard uh or or to to get started with VBA because of lack of multi-tenancy features we wanted to say like you can get started with 1.20 but this is not where multi-tenancy ends basically this is basically just the beginning and one one big sort of potential for improvement that we have is cost reduction because something that if you have a multi-tenancy case most likely not all of your tenants are active at the same time if they are all your tenants would need to sort of Be Active and this is I'm using this fuzzy term of active versus versus inactive because that's that sort of yeah just just as an abstraction we can go into what that that means as well um but if we say that we have active tenants who are potentially expensive like let's say they need a lot of memory they need a lot of compute and we would add inactive tenants which are not expensive because they don't need memory or or they would need the memory at the moment that they become active but in their inactive State they would use fewer resources now what that allows us to do is potentially size vv8 cluster for the number of active tenants as opposed to for the number of total tenants so if you have let's say a hundred thousand tenants and you would say that you're in for a bill for a hundred thousand tenants is one thousand dollars if you know that only ten percent of your tenants are active at the same time and you have a way to deactivate those other ninety percent you could reduce your infer cost from a thousand dollars to a hundred dollars and this is where where it gets really interesting because that's the the cases grow like in Vector search right now almost all cases are always sized for data set like it's always the first questions like how large is your data I said what's your dimensionality how many objects do you have um maybe can you use compression what are your query requirements Etc but kind of the cost is linear to the number of objects but with multi-tenancy in the ability and this is sort of what's what's coming next the the ability to deactivate and then there's there's more things sort of in the pipeline um Beyond just deactivating them but just with deactivating them we would not need any memory for for those tenants anymore so if you can say that even if it's just 50 or so they're inactive you would essentially have twice the number of tens or could host twice the number of tenants on the same hardware and this is where I see a ton of potential for for even more cost saving because realistically there's always this like long tail distribution where you have a few tenons that have a lot of queries and a lot of tenants that maybe query once per minute or once per hour even or maybe just once per day and you can potentially serve them for much much cheaper hardware and that makes it way easier to to reduce operating cost with multi-tenancy hmm yeah that is it's really interesting hearing the hearing the uh yeah like the prioritization of certain resources and so on and I I just kind of was having this Epiphany about how this could help like if you know if I'm building a machine learning app with weavi and I'm collecting user data I'm thinking about you know I just kind of unlocked my brain how much this multi-tenancy enables like you know just as a random example our last movie a podcast was with Alexa gordich from ordis and you know if I'm collecting the user data of everyone interacting with his uh YouTube chat bot I I have this new multi-tensity way of collecting that data and feeding it into my machine Learning System and yeah just incredibly exciting as I am understanding it better from hearing you speaking about it yeah also makes it easier to get the data out for a specific tenant because essentially like a like a list all query is now specific to attendance so you're now listing all the data for for that tenant so yeah I guess like when I was first um thinking about this topic of multi-tenancy I hadn't understood the just users with the same kind of class well I hope I'm making this like I I thought about it like say I want to have all these different classes with different properties like for whatever reason Conor has a age property but Eddie and I don't know you don't have you just have like a HomeTown like you know what I'm trying to say it's like different properties or like different uh I'm ageless just a good example yeah like I I guess I was thinking like you know especially with the vectorization configuration I think maybe you know like different classes would vectorize a different property and stuff like that but yeah I think just you know having you know millions of users for the same kind of class that application makes a ton of sense and yeah it's just really really exciting stuff uh awesome so I think that's kind of a Roundup of multi-tenancy edian's written this incredible blog post in addition with the Alice Corp and Bob and car yeah like the examples and uh so you know release post uh release notes all this cool stuff so awesome so pivoting to our next topic um product quantization I think this was first released in 118 or 119 uh you know a super clever Vector compression technique I think the history of we V8 you first started exploring a vector compression with binary passage retrieval where you're like hashing you know positive negative of each Vector Dimension but the problem with that uh product binary passage retrieval I think is you really need to train with it it's not really something that you can do with vectors that just come out of the box from whatever model but this product quantization thing where you uh cluster the values and then represent them with the centroid IDs you can apply that to any kind of vector and so it's really Universal way to compress vectors and just overall really exciting way to kind of reduce the memory required in these Vector indexes um it could maybe start broadly on like where is your head currently at with product quantization yeah yeah as you said we we initially released this uh I think two or so releases ago one 1.18 and we put this we we slapped this experimental label on it and that was not because we didn't trust our implementation that was because we said we're hey we're Gathering feedback we want to see how do users use that what can we potentially do to improve it and and we were well aware that this could come maybe with with some API changes that in the sort of regular semantic versioning API guarantees we couldn't uphold so they said like hey this is experimental this is probably going to change maybe not but this is kind of the the evaluation phase at the same time we don't want to keep it from users here you are here please please do use it be aware that yeah either it would change or maybe you don't use it in production yet unless XYZ and that also in turn meant that at some point that the experimental phase needs to end and it needs to become generally available and that is now with 1.20 and that's the the one big change just in general we incorporated all that feedback um of sort of maybe it wouldn't work as expected in some cases maybe there are ways to to do things better like that is that is one part of of uh where we're at with PQ right now but then the other part is also we've improved it we've added what we call re-scoring and this is a very simple change that has a massive impact so in general PQ is a lossful compression so you reduce the number of let's say standard example you have um a 768 dimensional vector with PQ you would represent four dimensions or this configurable then let's say for example you represent four dimensions with a single byte so now what you have is previously for those four dimensions you use float32s so that was 16 bytes and now you not just do you reduce this or from float 32 into just a single byte but also you pack multiple Dimensions into a single byte so in this simple example you can reduce the memory footprint for 786 Dimensions um by a factor of 16. so that's that's extremely sort of yeah that's an extreme Improvement but at the same time if you if you do that you're recall would drop considerably likely you would you would yeah your memory requirement is is a fraction of what it was before but also your result quality is bad now so what we said is well but we're still using PQ in combination with this H and SW index and in hnsw essentially you have a a top K Heap so you have like a temporary list of what you think is the current results and while traversing the graph you sort of keep um improving upon that list and the final result is essentially the result that we pass to a user so that's a very simplified explanation of of how search results are gathered within hmsw and that means that if we use that that compressed PQ distance to make a decision of whether we wanted something basically whether we want to add it to the list or whether we want to drop something because the list has a fixed size so whenever we add something something that's a worse result needs to drop off and if we do that based on the compressed distance oftentimes that decision is wrong and then this this sort of small compression error would accumulate more and more and more and in the end you would have a lot less recall but if instead if occasionally we actually load the real Vector from disk and use the real Vector to make that that decision so this is the rescoring part we actually take the the original Vector from disk so now you're trading a bit of performance you have a discrete now where previously you wouldn't have a discrete but in exchange you now get way more accurate results and I think we have a blog post coming up in a week or so or maybe two where we have some some graphs in there and it's super exciting because in some cases the uh recall QPS trade-off sort of the these these curves were top into the right is typically best it's actually not any worse than it is without compressed so it's like you you don't lose anything that's not true for every case like in some cases there is still a bit of a trade-off where you can essentially say like am I optimizing more for efficient storage or am I optimizing more for for efficient querying but in in many cases it's actually not a draft anymore you just have to turn it on and this is this is in turn a bit sort of depends a bit on how much memory you have available in general because just of the way that operating systems work like a disk hit only makes it to disk if that that portion of the disk hasn't been cached in memory before if there is memory available the operating systems and it's the the page cache will start caching portions of the disk so what is technically a disk read might actually not even be a discrete if you still have some memory available so you get these like buffers zones in in which the performance is still great even though the memory um usage has dropped a lot and this is the second feature for PQ that we said like okay once we go for General availability we want to want to have that in and now we're at a point where previously it was like use PQ sparingly in some cases because it reduces your Recall now it's more like why would you use PQ please everyone go out and use it it's awesome yeah I think when I first heard PQ rescoring I thought of it in the re-ranking way that we'll talk about next where it's like you know you get the top 100 results with the compressed distances and then you just kind of bring the full Precision to just re-rank I didn't realize how uh deep into the hsw traversal how say you know I I am still like Curious so you're like exploring the neighbors of the center node and you're going to kind of like Resort the candidate I think maybe um yeah I don't know I think it's maybe outside of the scope of the podcast but like just yeah understanding that hsw has this like candidate list and dynamic nearest neighbors and you like rescore it to have a better search like deep into the traversal rather than just at the very end I think very important yeah yeah that's like the Elegance of it so I have kind of two questions I want to ask you about PQ the first is maybe a clarification of the in-memory on disk part of PQ so I'm curious if this is the correct understanding so you know we we load say 200 vectors into Eva and now we trigger PQ we fit the k-mean centroids so from then on do we a new Vector comes into weviate do we just send the full uh full Precision Vector to disk and then the compressed Vector that goes to memory is exactly exactly yeah so we always retain the full Vector basically on on disk like you store what the user provides which is the the the real Vector so to speak but we also compress it and then only keep the compressed one in memory additionally we also need to store the compressed one in memory so if the the server restarts you don't want to recompress everything so we we store both actually but exactly as you say initially we keep the the original on disk and keep the compressed in memory fascinating I remember one of the my takeaways from when we met with Matthias douzay from meta who's one of the pioneers of product quantization was this um like online k-means kind of problem and thinking about like you know how many vectors do you need to load in to compute the centroids and then how does this scale with incremental updates um like I guess the question would be like what are you kind of learning about that problem uh yeah you you need a certain so so it's very hard to pick an exact number because you need your data or that subset of your data needs to somehow be representative of the entire data set so in in the ideal case let's say you would have five clusters in your data naturally then those five clusters and then you want to train on ten percent then those five clusters need to be represented in your original um a ten percent if they're not for whatever reason like if let's say the fifth cluster didn't even exist and you only have four clusters for training then you're training on something where the fifth cluster doesn't exist yet so that doesn't mean the fifth cluster can't be assigned anywhere but it means the the sort of trained model isn't as good as it would be um uh if if that had print part of it the good news is that there's a certain size where it gets more and more likely that the the distribution of the data set sort of doesn't change of course that's no guarantee like it could completely change um but often what you see so for example on on um these a n Benchmark data sets that are typically between 1 and 10 million I think we typically didn't see any Improvement uh when using more than a hundred thousand vectors for for um for training so that that's still a very manageable amount I think it takes 10 seconds or so to train with with a hundred thousand vectors um yeah sort of a good a good trade-off yeah fascinating Yeah a hundred thousand just like getting a sense of that is something that I think is a really interesting research question when you're you know going into the details and yeah I think this whole like cluster analysis is definitely a lot to it I remember you know earlier we did a podcast on Bert topic and we were discussing like what would cluster analysis and weba look like I think it's starting to see I don't want to give away I know that we have something planned but like um you know we're seeing Partners Integrations other technology companies building further into this like embedding space visualization topic modeling cluster analysis and you know definitely people listening to be on the lookout because there's going to be some really cool stuff around that coming soon on embedding drift detection and all this cool stuff um yeah awesome so I think that's a great coverage of the PQ rescoring I think probably for me the biggest takeaway is um you know understanding How Deeply that goes into the hsw traversal and what you said I think is a huge point about originally you were thinking I'm trading off memory for accuracy but now that you know you have this restore ring there's no trade-offs so just use it that's kind of a funny takeaway yeah yeah for for some cases I mean there there is it's engineering so there's always trade-offs and it's kind of like not seeing our users data it's hard to make predictions but we're trying to model real life data usage with with as many different data sets as possible and I'm really surprised to see sort of how good it is in in some cases like yes in some cases there is still a bit of a trade-off but in others it's just not oh yeah yeah I think um having all these diverse data sets is such an interesting thing of machine learning generally like the very is still kind of no free lunch in a way like uh yeah awesome so I think kind of we have a trio of new features I'd like to kind of cluster into one kind of category which is kind of like we V8 being feature Rich for search you know like vector search obviously like search relevance is a huge topic and I kind of just to give a quick tldr of each of these things uh the first of which is re-ranking where so as I mentioned with my original understanding of PQ re-scoring re-ranking is this idea where you take your top say k 100 results from Vector search and then you're going to take each of those candidates in the query and pass each of those as input to a high capacity scoring model so where do these scoring models come from sentence Transformers has six of them that are open source and so we've built the docker image where you know similar to text to back Transformers you you have this like local image if you wanted to spin it up in your laptop host it yourself and all that good stuff we have the you know the cross encoders that have been trained from sentence Transformers are up there and then also really interestingly we have a re-ranking endpoint from cohere and obviously our friendship with Mills rhymers you've seen Eddie and those rhymers together um you know this is a an API for a re-ranking model and I think that's quite fascinating I don't think there's another another company doing that and it's pretty interesting to see that API and you know it comes into like weeviate's module system how we have these um this orchestration of API requests I think I think it's quite fascinating maybe actually adding we could take a pause and I I really wanted to ask you this question about how um I think an interesting thing about we v8's module system is how golang handles these uh like concurrent HTTP requests and maybe talk a bit about like um what goes behind that how golang is optimized for scaling this kind of you know orchestration to model inference because I think it's such a fascinating part if we get in general go is a language as you say sort of easy simple concurrency is one of the building blocks of why you would use go so go has channels and it has a simple synchronization Primitives such as mutexes and and these kind of things that you need for easy concurrency it has a race detector which is also a super important feature to to figure out like are you doing your concurrency safely are there any kind of risks so in general sort of go is a good I would say it it makes it relatively easy and then what that allows us to do is if a user just gives us sort of a a bulk data thingy like like a batch of data objects essentially and our job is to say vectorize them or or also re-rank them I'm going to pass them to to the model if we would do that sequentially that would also mean that whatever the latency is for one of those requests will be bounded by that but the kind of cool thing about these serverless applications is that they tend to scale way better than that so if you hit them concurrently that allows you to to sort of not have to wait for one thing to be finished but just do multiple of them at a time that said again there is a bit of a trade-off because um often these models have rate limits so I think something that that often pops up in our support forum is for example with open AI there the the default rate limit is I don't know what it is but something like I think maybe 60 per second or so that that has a 600 per minute so if you want to vectorize more than that you could potentially hit the the rate limit so also concurrency is not a free lunch here but it it helps to at least sort of max out whatever the the model provider can can use and this is the same for for if you self-host your your model um if you don't concurrently do something and it doesn't necessarily mean that it has to happen with with concurrent HTTP requests that could also mean that you bolt this up into a single request and then maybe the concurrency happens on the model side but in in general you want to sort of not have idle capacity around like like if there are either gpus or CPUs that are used for model inference you want to make sure that they're they're maxed down and that's one of the motivations for using concurrency in in that case and then sort of taking that back to to golang it's a language that makes concurrent programming rather easily I know that that some of our team members sometimes hate go for it because it's it's not perfect in any way but no no language is um but it's it's yeah a very good sort of fundamental concurrency kind of model yeah fascinating I think I also like the um you know like the Gen like our generate module with openai palm the code here I the way that particularly the way that it paralyzes doing the single result call I find a ton of use in that where you want to put a prompt of each of your search results and have that all be parallelized it's so much faster than looping through there yeah you know I think it's pretty cool so okay cool so on this topic again of search results so we have re-ranking now where you can get better search results and you know kind of building it right into weviate is why we took this tangent into the more technical thing you don't have to have you know manage some other service of your own to do re-ranking it's built natively into alleviate and you know taking that search result slices and either way so the next cool thing is auto cut so autocut has this high level motivation of you know how many search results are relevant if you sir you know if you search for I I've stolen Bob's uh landmarks in France thing I use this all the time now so if you search landmarks in front and you have the Eiffel Tower in Paris and you have some other things and you know say you only have like eight thing eight landmarks in France and your search result cutting off that nine and only showing eight results this is incredibly interesting for the language model retrieve blog went to generation thing as well because the language model can be quite thrown off by these irrelevant results so like I did see something about Spain it might write about that because it's in it says based on the search results and it's like well it's there so you know so I think the interesting thing about autocut also is understanding uh how that's done so it's like the um you know it's like calculating the delta in the slope from the point so you take each of the distances and then you have to interpolate a line from there and then you measure the steepest change in the slope so what we've got users will see is a hyper parameter on which of the steepest slopes because you know you have like two to three as each reverse of the thing so it's like how extremely do you want to be cutting it and then the last thing is you know one of our favorite topics is this kind of uh rank Fusion algorithm weeviate has built-in BM 25 as well as Vector search so what that means is you end up with two lists and so you know the first time we did this we were just doing you know just combining it based on the ranks so you know if you're like first ranked and BM 25 and like fourth rank in Vector search that ends up you know just purely based on the ranks and now we have like a relative score Fusion so yeah I think all these things um just all this new uh benefits in search I think the question I'd like to ask again is just kind of where your head's at on yeah like the search stuff I think you know maybe Berlin buzzwords there was some discussions on the search side of things I've been to the haystack conference personally and seen that kind of like you know world around search relevance yeah to to me this is a someone who's who's very use case driven this is for me extremely important that we think of these these use cases and search is a big one it's not the only one there's recommendations there's classifications there's called the of course the whole a generative AI part but generative search then also as a part in there um and for me it's I I like being able to give our users something that they can use end to end where they can say like hey I want to spin up bb-8 I want to have the data management part so you want to have the whole database I want to have the vector search obviously because that's the whole reason of of using bv8 as opposed to say traditional search engine uh the Hybrid part that's such a big one just the the bm25 like not having to spin up something else that that does the pm25 for you but being able to to do that um and then all these new additions and and re-ranking and auto cut these are just sort of the most recent ones but I think there will be way more down the line and a essentially if it makes your search experience better then it's something that we could potentially extend deviate within because of the module system we're very flexible and even adding sort of things that that essentially the module system allows us to add things that not everyone needs without loading VBA because if you don't need the module just don't turn it on so um this this kind of yeah everything that makes search easier to use better and I guess in a way also more accessible because there's like information res information retrieval is like this whole research topic out there and there's there's specialized conferences you just mentioned Berlin Bus words uh which are super fun for us but for someone who just wants better search maybe they're not maybe they don't want to get into the nitty-gritty details maybe they just want to have better search and there we're we're always trying to find that balance of making it easy for someone who's new to the space without sort of taking away the flexibility that someone who's more experienced needs and I think these these new re-ranking modules and then AutoCAD these are great examples because if you have let's say you have your own complex ranking pipeline that's way more complex than something that you could fit into evade just don't use the re-ranking module but for someone else who's already using bb8 maybe doesn't have a complex pipeline says like hey there's something that I can just turn on with a single flag and it makes my search experience better by all means do it that's that's what we're here for yeah yeah yeah I I think there are so many interesting tricks from the information retrieval uh Community we can get out of to get better search I think um I think with re-ranking to maybe stay on it a little more I think maybe some of the search zealots would be a little bit saying you know I still can't do symbolic re-ranking you know like if you're searching for products you might want to have the price and the relevancy these as features that you would put into re-ranking uh so you've seen we've had a podcast with meta rank with Roman and Siva where you know we're thinking about this thing as well it'll interface very similar to this re-ranking with coherent sentence Transformers we'll just send the property to the API as well and and it'll look super similar there's also a pretty interesting discussion around like multi-valued vector search like I have a title Vector as well as a document Vector so you know we're looking into all these things as well as like the the large language model re-ranking thing is quite interesting where you prompt it with kind of the symbolic rule so like please boost it if it has the if it's recent and then you and then you give it the result like Json dictionaries and that does work pretty interestingly I think it's also really exciting for Integrations with like llama index and Lang chain about and Samantha Colonel and Pat make a list of these things but I'll just leave it there about how you can you know just get better search results by just taking these things on and yeah I think also maybe just before uh graduating to our last topic I also just want to thank Dirk for this Dirk did an incredible job with you know the rank Fusion and the auto cut I remember like the other guy was trying to figure it out myself and it was such a headache trying to figure out how to do the um uh get the go the go numb uh thing to do the slope interfellation and I had showed it to Dirk and he was just like the next day like oh yeah here's the notebook to test it I was like that's so fast I've turned it around so really impressive stuff and uh yeah it's awesome so kind of our last topic is the um the cloud monitoring metrics with the Prometheus and I'm curious you know I imagine as you do we mentioned like the trying to break your testing with the multi-tenancy I'm curious like where your head's at with these you know how you you know basically observe this massive thing running in the cloud right yeah it almost feels like one of those boring features compared to all the exciting features that we just talked about but I think this is this is what's so important to to pay attention to the boring stuff because even something like multi-tenancy I mean we're super excited about it but in the end you could argue like from a from a AI perspective this is a very boring feature like it's not a New Concept but these are the kind of features that you need to run reliably in production for me anytime we we add something like that and we we sort of expand on on the observability stack like this is a simple change um essentially it allows you to to track your success query rates like previously you could only monitor I think latency but now you also get just a ratio of what queries have succeeded versus what queries have failed and and the failure reason so was this a user initiated failure so in HTTP status codes this would be like a 400 uh plus a status code or is this did something go wrong in vb8 so a 500 or so so internal server error and these kind of things and the ones from from out there um so so adding the the monitoring for this to get better observability basically on these kind of things it's very boring but it's very essential and I I love seeing bva grow up and and have our users ask for these boring things because in the end boring is production and production makes our customers money and that makes us happy so always always a place in my heart for the boring features yeah I think well yeah I think um like just for me learning about like datadog and learning about these companies that have built around that kind of thing is it's really fast yeah I mean it's it's to me it sounds like kind of like Insurance on your uh thing that you know like your data is up in the cloud it's a super abstract concept of like a little bit of it is in Virginia a little bit of it is in Germany so yeah yeah oh that is yeah data Doc is a massive company so that just goes to show like how much how much need there is or how much how much room there is for for these kind of observability topics exactly as you say it's like insurance it's early warning it's just sort of looking into that the the more complex that your system grows you can only see so much from the outside and sort of getting these insights and just seeing what's going on can help sort of in in hindsight for for debugging cases like what was going on but ideally before something happens like early warning systems like simplest thing uh monitoring for for memory usage or something like you see something goes up nicely in a linear line and you know that there is a certain limit with your capacity if it says 30 maybe that's fine 50 maybe that's fine if it's approaching eighty percent maybe you want to change something about your infrastructure so yeah these kind of operational things that you need um it's it's for me it's it's just such a positive sign of like hey people people do need them which means they're doing serious stuff with bb8 so I'm very happy to always ask our team to prioritize some some observability features yeah well eddian awesome thank you so much for another release podcast multi-tenancy PQ with rescoring re-ranking autocut new hybrid rank fusion and then new updates to the Prometheus Cloud monitoring all so exciting um and also you know you can join the community slack if you need help with any of these things everyone's always around and checking that all the time and uh yeah thanks so much for listening thanks for having me had a blast thank you ", "type": "Video", "name": "etienne_dilocker_on_weaviate_120__weaviate_podcast_56", "path": "", "link": "https://www.youtube.com/watch?v=xk28RMhRy1U", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}