{"text": "Thank you for watching the 17th Weaviate Podcast with Kyle Lo! Vector Search enables us to find semantically similar items in ... \n[Music] hey everyone thank you so much for listening to the vva podcast i'm super excited to be welcoming kyle lowe a research scientist at the allen institute of artificial intelligence kyle's work on applying nlp tools the application of scientific literature mining has captured my interest honestly more than anything else in deep learning research to give a quick preview kyle has worked on papers such as cybert a pre-trained language model for scientific text tldr extreme summarization of scientific documents fact or fiction verifying scientific claims and a data set of information seeking questions and answers anchored in research papers just to give you a quick sense of some of the things that kyle has worked on in the space of scientific literature mining science is becoming more connected and open which is good but exhausting for researchers trying to keep up with it kyle and collaborators are developing models for tasks such as summarization question answering and even fact verification that facilitate this problem of keeping up with the literature and in my view will greatly impact the efficiency of science kyle shared his insights in the podcast on building these kinds of data sets and the lessons learned in applying cutting edge deep learning algorithms to these problems to return to wev8 and vector search a bit i first became aware of vector search when researching deep learning applications for covid19 i saw the co-search system from salesforce research which uses vector embeddings of queries to match them with vector embeddings of scientific documents many of the data sets kyle has worked on are available on hugging face data sets and can quickly be loaded into we va to explore vector search in scientific literature mining so with that said enough for me i really hope you enjoy the podcast and as a quick reminder if you enjoy this topic you may also like our second we vva podcast with charles pierce who similarly discusses his work on scientific literature mining hey kyle thank you so much for doing the we vva podcast hey uh good to be here thanks for inviting me so i think a really great topic to start this off would be if you could open us up with what is scientific literature mining and just this kind of problem yeah so scientific literature mining is um essentially like a class uh the goal is to take the breadth of scientific literature that scholars have difficulty keeping up to date with and reading everything that's being published and trying to uh apply text mining techniques nlp techniques um that people have been developing to try to make scientists and scholars lives easier so this can be through extracting useful bits of information from these papers i could be summarizing these papers it could be building tools that help scientists discover um what's the right papers or discover new papers that they wouldn't have read ordinarily it's kind of this broad field of trying to make sense of large amounts of useful text yeah i think it's one of the most interesting applications out there and i i personally have definitely seen this problem of trying to keep up with the information overload of trying to keep up with all the cutting edge sciences is such a daunting task can you tell me about your kind of progression in your career and what led you to scientific literature mining and kind of how you use your own intuition of kind of like this meta thing of being a scientist and then studying the information acquisition process of leveling up your own sort of skills as a scientist yeah that's interesting so i did kind of stumble into this field um like when i joined the the when i joined ai 2 um i joined right into this somatic scholar team and i had like no idea i had no nlp experience i had no text mining experience um i was just interested like um i liked i liked reading papers um like uh i had kept up kind of in my professional career uh still reading stats papers machine learning papers um and uh so i guess there was always still a little bit of like attachment to like oh it would be nice if this was easier because reading papers and keeping with papers is hard um and uh so the when i joined and they basically pitched me this idea of like we'd be building tools that makes this thing make this life easier i was like i could use these these things that we built this is amazing and that's kind of how i got started in it because it's like an easy sell um and uh i guess like yeah in terms of like the relationship the the kind of the interesting circumstance of like i'm also like a user of the tools that we're like trying to develop um it definitely makes uh there's some things that are definitely easier because you stay motivated a lot because it's like at the worst case um whatever you build there's at least out at least i'll want to use it even if nobody else wants to use it it'll at least be useful for me probably and so um uh you know it's like i don't have to speculate too much um and maybe there's like you know like 10 people out there who also have like the similar uh way of consuming research as i do so there's not as much worry that i'm like hallucinating a task or a tool that nobody would possibly use um there's always a little bit of confidence there and though there are times where it's like am i designing systems or am i working on problems that are really specific to me and like nobody and like i really tailored to like how i do research or how i consume literature and like maybe it won't generalize to other people so there's that kind of balance and before kind of stepping back and getting into concrete works like you know cyborg biobird and all sorts of the things you've done that we're going to get into i do want to kind of stay on this a little more and ask this kind of question about you know you're describing say it reminds me of like developer tools where you're building something that you yourself would use and it really helps you guide the intuition and something i've always been curious about as i've been reading and i've read quite a few of your papers now about uh doing the scientific literature mining and i'm curious about like the biomedical domain compared to the deep learning domain and i know you have a paper uh q asper information seeking that is annotated nlp papers compared to a lot of this biomedical stuff so i'm curious from your perspective on developing scientific literature mining as you say like i'm the user that helps me guide my intuition do you think that it's better to make progress on like you know kind of the recursion again of nlp for nlp papers or this kind of nlp for uh biomedical or say physics chemistry like that kind of thing yeah so this is this is a really um we could talk for a real long time about this because i think this is also like a subject of debates on that team um there's a whether we a lot of projects um that i work on like i guess like my team works on we try to work on techniques that are fairly general so it's like not necessarily specific to biomedical papers or like you know calling it biomedical papers is actually overly broad it's like half of our half of literature is like biomedical so at any one sub subfield is bigger than all of nlp combined so it's like um uh working on just like nlp paper so working on just like ai papers computer science papers this only we all have like two million of these um biomedical we have like 40 million of these papers um so um working on any one targeted field um the more you focus on one targeted field i think you start discovering really interesting phenomena that are specific to that field and you start um and things that are like really critical to resolve for there to be adoption by people in that field um so like for example i i just was dealing with this uh recently um even the basic task of just like extracting references from a paper like the bibliography section extracting references from paper and then linking them to a database of known papers and that's how systems like smash scholar google scholar web of science like all these pubmed even like all a lot of these aggregators that report citation statistics either they get it directly as metadata from publishers or they have to extract it and link it themselves but physics papers a lot of them don't include titles in their bibliography entries and so if you develop a tool that's just looking at biomedical papers or computer science papers you would develop a tool to think actually title linking is like just matching based on title gives you really really good performance um and then you would completely like not build anything that's useful for the physics domain and so um it's like these kind of like little details as soon as you start focusing on particular disciplines um you you realize that like this is like it's like make a break it's like table stakes if you don't resolve this thing nobody in that community will use your tool um but uh but the problem with this is like is this like a scalable way of doing research like you know like you could imagine like discovering a lot more more with these things um you can spend almost all of your time uh trying to just like deal with this deal with this and kind of doing uh new tasks each time you try to adopt like break into a new discipline uh so there's a little bit of a juggle of like should i focus on biomedical only um if i focus on biomarker only you know it or like computer science only it's like a it's a familiar domain i have a lot more of experience i can build tools that work for this but if i spend too much time just focusing on this when i is it like like is it like redoing most of it uh when i when i jump to another when i jump to another discipline or are my techniques actually fairly applicable with minor minor tweaks um as i moved into some of this discipline so um [Music] yeah i guess different people on the team have like different strategies about what they feel safe committing to um early on uh for like i'm just gonna specialize on this discipline and hopefully it'll generalize later and other people go like actually no i'm gonna start my projects trying to pick three diverse disciplines and i'm just making sure my tools work across all three even if that means like a lot more upfront investment interesting so it sounds like uh you know like different disciplines of science have different challenges of reproducibility and different challenges of communication as you mentioned the title linking for physics papers is uh different from say computer science papers and um and maybe we could step a little back from the idea of say designing very specific tasks for particular domains but if we could kind of say that this general framework of say question answering or summarization is a perfect task set up let's say and it's all about uh just the particulars of the domains and so i kind of want to come into this topic of data domain uh domain adaptations say maybe things like semantic drift as like the meanings of words change as you go from say wikipedia and then into uh deep learning papers where maybe some word has some kind of different meaning so i i kind of really want to ask you about this origin of i think you were one of the first authors that did uh like bio bert cyber this kind of um language where it's data domain bert to communicate this idea of it's the burt algorithm but it's been trained on this particular data domain yeah yeah so um i think bio bert was from jin hyuk lee and others at uh korea university and uh yeah no no problem there they did excellent work um we were working on cyber at roughly around the same time and then where we were kind of going across just like everything that we had in spanish color um and i guess like both of our groups were sort of stumbling to like this idea of like um what it's it's like impractical to to like train a bird from scratch really um which is like a train retrain a perfection each time you have a new discipline you gotta train bird from scratch again um but someone had to like kind of go through that kind of mostly engineering work to to to prove it out um i think at the time it was actually kind of uh those decent amount of pushback actually unlike going going down that route like now we know that i was like that was a good idea but at the time it was just like why do you need that like are um what's it's it's a lot of effort to try to kind of wrangle google's like kind of early tensor like like like kind of first version of vert code um this is like pre hugging face era and uh and it wasn't clear why people needed um these like kind of discipline specific or domain-specific birds um because bird was trained on like such a wide crawl of of of different link like language like documents um and uh maybe i mean you could definitely make the case at the time i think like of like different language birds that they're so different that obviously you needed but like scientific texts probably some scientific texts leaked into the bird training corpus like why why do we need this specialized thing so that was kind of the motivation for this it's like are we do we believe in this idea that like actually no science is hard enough where just training on like wikipedia and like reddit crawls uh is insufficient for picking up kind of the the type of language that's used inside of the text and at least people on our team and people at korea university believed in this uh that it was different enough to invest in that rebuilding of from from scratch or at least like spending a lot of effort adapting a bird um yeah i think that was mostly an engineering effort and the types of things we found was like surprisingly um there's more similarity between these disciplines than we thought um and i think we know more and more nowadays that actually um vocabulary is one of those things that like you can get away with like not worrying not trying to retrain your vocabulary um we like found biobert actually does really well in biomedical tasks um even with the original burt like kind of wiki trained vocabulary so it's like that's fine um obviously there's been newer bio births like from msr there's like i think there's like pubmed burt and there's like uh there's like two or three of these at this point i don't remember which there's so many like kind of clinical bert i think um and some of them have retained vocabulary some of them don't reaching vocabulary it's kind of unclear it's not as like obvious to us like what's the impact of having like a specialized vocabulary um and seibert's uh results were also just like the vocabulary is like the performance boost was so small that like we weren't even sure if it's significant or if it was like didn't matter because it weren't like like just like rebuilding upper from scratch because those are really expensive um but so there's something else that's happening that's not vocabulary like it's some something but like the performance there was substantial performance boost on these scientific tasks these biomedical tests from adapting a regular burp to these uh to our disciplines but um it wasn't a vocabulary it's in like it's in something else in in the weights or something i don't know um so yeah i think the we've had some follow-up work in in this space too with like domain adaptation but um i think there's still room for a lot more study on like why is scientific text different or actually in what ways is actually maybe just the same and we don't have to spend uh like a ton of time adapting these bird models to handle everything maybe just the bits that are that are particularly particularly different particularly different in scientific text um so yeah that's extremely interesting and i at the vocabulary tokenization level i guess maybe like if you have unknown tokens like maybe i don't know like let's pretend like convalescent plasma therapy out all right some phrase like some gene or something like that right never appears in wikipedia so it's not even token it's unknown and then you completely lose the info and it makes a lot of sense why that would be a horrible uh information loss from wikipedia to the biomedical papers and yeah i agree with you i think the like it's more of that like the latent representations that it can do that kind of reasoning right with that kind of thing and uh so one other thing i wanted to ask you about on this topic is what what your take is on you know prompting gbt3 to try to get it to be knowledgeable about scientific text do you think that yeah so we have been playing wikividi iii quite a bit uh it is it is it is like exceptional it is it's actually kind of um shockingly like it just behaves so differently from from from models that we've we've dealt with in the past um i think like the like one one of the most fascinating things about it was like uh i think you could prompt it for for a particular behav like um for a response where the response was like a list like a little establishment oh like it would say something like um uh here's three reasons why and then it actually like maintains coherency it like it actually looks like bullet one list something bullet two lists something and sometimes it might even like refer back to oh yeah as a chain on to like kind of bullet two something something like there's there's like i've never seen that kind of like self fresh referential um behavior in in in generative models prior to gpd3 um at the time i yeah and then and then um but it definitely doesn't still doesn't work on on scientific pe tests um we've been using it for various projects like uh trying to generate concepts like descriptions for concepts like technical jargon we've been using it for summarization we've been using it for actually for just for extractive qa or even attractive keyway with from short snippets um and there's definitely like it it definitely doesn't work it works reasonably well but it doesn't work comp as well as just like kind of more accessible models just like take bart and then fine tune it on a little bit of data and it'll outperform uh sort of just like this pure zero shot prompt based uh gpd3 model so um it's definitely not solved everything uh the really cool stuff is it seems really good at doing kind of mechanical operations um so if you ask it explicitly to synthesize like a tldr uh from some input it's pretty good at that i think it's because tldr generation fundamentally is very much like a pick and choose these things and and you can do a lot of copying from the context that you supply and and for the most part those tlr summaries are will look pretty good um so it's actually quite good at that um it's quite good at like kind of rewriting um so scientific text if you just try to consume like definitions of terms if it's just like extracted context um you'll have a lot of these for example something something something or like as we mentioned before it's like these kind of dangling phrases where if you just remove these kind of phrases these extra punctuation ones stuff like that it would actually look pretty good the end result will actually look pretty good and gpd3 is pretty good at these types of mechanical operations just like cleaning up text to be somewhat self-contained um but anything beyond that uh i think our team would be hesitant to actually like put that output in front of real people if i could dig into the details a little bit about how you explore that do you are using gbg3 as an inference api are you exploring say supervised learning and uh maybe i don't want to ask a question that has too many things packaged into it so maybe let me just do it whenever we're using the the public api uh so as i guess it's a fundamentally um with like kind of the with whatever prompts um that they that they supply with like kind of the configurations for like temperature and whatnot and then um it's it's the in context learning setting fine-tuned setting so could you tell me how you adapt that for extractive question answering is it you give it it's like kind of the t5 style where it you first tell it like um question answering the answer is going to be in this passage like some prompt that describes the task and then it has the context and then the question and then it generates it right and you have to map the generated thing uh like you do like exact match with the text similarity of the generated thing with the ground truth answer is that the exact setup yeah that's right or variations of that we may or may not include like the the the instructions for the um for the task itself or it could just be like three examples five examples of like here's the thing here's a question here's the context here's the here's the answer as a string and then again like you said the exact match for evaluation or some per perturbation of like the ordering of these things you you kind of have to mess with gpd3 um prompt form uh quite a bit to get it to work reasonably and i think this is a great transition into our next topic where i want to talk to you about the different uh tasks in deep learning and i remember i think you were one of the first authors that really or and your team of course that developed this uh tldr abstractive summarization and to me it's just such a remarkably high output space that you have to have like compared to classification where you say have two labels and it's just one prediction of two labels whereas text generation you have say 50 000 potential tokens and then you also like unroll that into like 50 or so you generate like a long summary uh what are your thoughts on that kind of difference in output space i mean it's a pretty big question but yeah yeah um so tldrs um was a project with isabel cachola who's at jsu right now as a phd student um and with uh armand gohan my co-worker um and uh that was an interesting one because we actually did go into the project thinking um wow this is like an impossible task um this is actually like super difficult because of this it's so the output space is like 30 tokens and we gotta compress an entire paper like a gist of entire paper into like 30-ish tokens um people can do this our people have been able to do this sensibly because you can see on like open review which is the data set we used um authors are actually writing they don't teal the ads for their papers for other reviewers to read so at least it's not like humans aren't stuck at going like what should i put down that at least there is some answer that the authors will write that and we but trying to replicate that with the model seems seemed like really daunting um because of this word that like what it could be anything like you could just put anything into the art and like how would you know um but surprisingly as we were studying more and more uh the teal there's the authors tended to write um i'm actually more and more optimistic about this task being actually simpler than it um than we originally thought um yes it is the output spaces it is like a third compress a giant document into 30 tokens yes it is generated text so you can just say anything but in terms of like generating if the goal is to generate some tldr that is enough information to for a user for like a reader who's like kind of scrolling through like a search page if you're like google scholar or something going through a search page or going through conference proceedings or just looking at some author's profile and looking at a bunch of lists of papers um just enough information that helps people make a decision as to whether like a more informed decision as to whether they should invest like look into this paper or not um the the yes there are multiple right answers but you can really easily find reasonable sensible right answers from the paper itself and just kind of clean them up i think this is a function of just how papers are written if you look in the introduction if you look in the conclusion um even if you kind of search around like topic sentences in the um in a within within the paper authors tend to write in a manner that is very much like oh they skipped over the rest of my paper but if i had to give them one sentence to like so that they had a takeaway um i'll at least put like like a main sentence in the conclusion and so the task really becomes kind of this even if this is kind of what the book part is fundamentally doing um it kind of becomes can you find like a reasonable input context that probably contains these promising sentences that are already very tldr like or summary like put those in front of a model and then just the model just really needs to kind of shorten it move things around to kind of get it to be short pithy easy to understand self-contained and that type of operation is really suitable for for for kind of these like large models that we have today yeah it's super interesting that it seemed like the large models seem to be able to uh like decompose the task like that and i'm i'm so curious your thoughts on this because i it seems like you've done such an exhaustive coverage of text classification like natural language inference fact verification question answering summarization so i'm i'm curious and then kind of with what you're saying is um you could probably just classify like three salient sentences in a paper to do an extractive summary right and and wouldn't that be much easier to like label and kind of understand it than yeah yeah so so this actually stumbles onto a project that i'm working on uh with um uh uh lucas oldangmi who joined our team recently um which is like trying to identify salient sentences and papers so there's no we don't have any work out there right now this is actually like pretty like i think we are like a couple months into it um uh it is really it is really hard uh to find salient sentences uh once um if you i guess like there's like these kind it's almost like there's like the step function for like if you're if you're if your criteria is i want to find like one or two or three sentences max that really get at the core of a paper um and helps a person make a decision as to what whether to click this paper um that's pretty easy um papers are written this is kind of like what i was saying the papers are written sort of with these paper with these with these kind of summary like sentences speckled throughout so you can look like a lot of papers just have contribution sections where they just literally pull it out like we did this we did this with this and these are the three things that you should care about if you don't care about anything else in this paper um and so if your goal is to up like a tldr and just want to capture that type of stuff then the task really is find those sentences synthesize them into something that's kind of legible once you start going a little bit beyond that into going okay well i want something that's a little bit more like an abstract or i want something that's a little bit more just like uh maybe i want like a block like generate a blog post about like to summarize this paper so something that's like cuts the paper down in linked by half but not you know all the way down to like a tlr that's when it gets extremely subjective because we recently did like a a study um where we had like 12 people on the same team uh annotate the same paper and this is one of our own papers so we were all extremely familiar with it um and just had everyone just kind of read through and select like if you had to pick like thirty percent of paper to highlight that was like salient what would you pick and it was like aside from those few sentences or those few passages which would uh be prime passages for input to a tldr model everything else was just like hyper low agreement like nobody could agree that whether some of the math stuff was important so nobody agreed like oh this is actually a super important detail because if you knew this field then like this is actually like make or break for this paper like and like um so yeah i think that's kind of how i think of this like teal the art is easy when you start getting to like generate an abstract maybe that's kind of easy-ish also i think it's less easy but it's easy-ish because there's just so much data free data out there and then when you start getting into like i want something that's a little bit longer than abstract um like kind of like a blog post or something like that or like a kind of like a i don't know just like editorially type type documents for these papers um uh that's when it gets just like everyone's gonna argue about what is actually truly salient so there are so many questions i want to ask to attack that i want to try to maybe maybe keep keep with one that i think could be quick and then i want to ask you about these kind of data annotation uh efforts which i think is extremely interesting but um so this dense annotation where you take a a team of 12 nlp scientists and they each uh densely annotate a paper like write it write the comments right highlight why i like this and then do you think that the kind of idea of say semi-supervised label propagation where you try to deploy that team for say a hundred papers and it would probably be an expensive endeavor but and then trying to bootstrap that dense annotation to novel papers do you think that would work maybe maybe it's hard for me to say just because like um i don't even understand the phenomena that we're annotating yet like when what is sort of like the the function that everyone is employing when they when they're like received with the task would just annotate what you think is important and somehow they map like salient or important to i don't know k different criteria that everyone sort of forms and then they apply that annotation i don't know what that is yet i don't know what what constitutes an importance um i don't know how to break down what important means to people yet so so i'm not sure actually yeah yeah maybe like um like a like user embeddings some kind of model of you know kyle and connor are going through the paper and you have some kind of uh representation of kyle and conor's background knowledge to help you interpret the annotation maybe some kind of flavor of it like that maybe so like if you're bringing up like kind of like user embeddings or user representations um you're getting into this realm of like personalization um and and i guess that's kind of what i was saying is like i don't know if this uh if personalization is the right place to go now um in general yes i like things to be personalized to to people but practically speaking if we had to like start somewhere we want to start with kind of what's the most effective uh kind of making progress on this task and i don't know what personalization is the is the thing like it could be kind of like tldr's where uh if there is like a shared convention in it within a community where everyone kind of looks for contribution statements everyone kind of looks for these and then then it's not about um personalizing to a person's preferences it's about understanding what the conventions are for how to read papers within this community and then just highlighting those so like the biggest example of this is like pico uh the pico framework within um uh medical papers like clinical clinical trial papers this is like a convention that people the uh you know um clinical researchers medical researchers who read these papers have developed and refined over years and everyone agrees that like plus minus some variation of different frameworks but like that like yeah for a paper if you really want to summarize really quickly what this paper is about you got to know what the p participants are you got to know like where the population is you got to know what innervation is intervention is you need a competitor you need the outcome and just like if you can summarize this extract this information you can go through papers really quickly or you can toss them in database do some nlp to summarize and aggregate and so understanding that convention uh is sort of key and i like building tools taylor.com is really key for this community but and i don't know what this what like a pico type thing would look like for other disciplines yet or if it's like you can't there is no ego for for for understanding what people think is import uh salient and therefore you should invest more in like personal section i i think that is just such an such a fascinating kind of thing the pico analogy and that that's kind of what brings me back to the one of the first questions i asked you about is this idea of should we study deep learning papers or say are any kind of scientific papers because i think like with um deep learning experiments we can kind of identify say the symbolic components of what's not so like we could say um you know it's the bert model architecture and uh the learning rate scheduler is constant like we can construct the dag sort of of the dependencies and the experiments and say like here is the the normalization layers this is what was changed and like we could maybe yeah like extract those kind of graphs from papers and maybe those kind of graphs could for one it could quickly illustrate the paper like here is the paper like it's bert it's this data set is constant and then here's the thing that was changed the normalization layers this is kind of example and maybe that would be one way to like communicate the papers yeah i so yeah what should we be studying i guess like these papers or like this discipline of papers for others i don't know um i i try not to have like like i guess like like i guess at a high level like in kind of instinctual level yes i think we should be working on making medical research more like easier to digest and follow um because that seems like it's like a general good thing uh for people um for like like aipay first like i don't know maybe maybe um it's hard to justify that that that that working on our papers is more important than working on like helping like a doctor read follow follow the latest clinical trials um but i think um it depends on kind of what your goal is the way i try to do research and what i sort of um recommend to my mentees is like try to be deliberate in your choice of which discipline to to to a papers to study just because any project within our space is takes a really long time annotation takes a long time you got to hire experts and so i would say do the thing that actually actually allows you to like completely completely finish the study on the phenomena you're interested in so for example um if you want to build something useful broadly useful and that's it and you're like interested in just trying to understand how can we use these nlp tools to build something that's useful um pick and you have a medical collaborator go with that if you have if you don't have a medical collaborator um then i would hesitate to say that you should build something for medical uh for the medical population because if the goal is to build something useful and you don't have someone from that population to work with like how do you ever really know um you i would say in that case build something for yourself because at least you are you know what would be useful for yourself and that and i think that can help people keep focused if you're interested in studying particular language phenomena then you should like that you think are interesting uh for example math and like symbols and stuff uh in math papers um definitely just pick the papers that have as many much of that phenomena as possible and study that um so yeah i guess it depends on kind of what you're trying to get out of what what's interesting and what you're trying to get out of out of out of this work um so um yeah i think my thesis on this is maybe like maybe two grandiose sort of it kind of like um i guess kind of my motivation for this is i think this is something that dennis has said when they ask him uh you know why ai and he kind of says well intelligence is the thing that solves all the problems so if you can solve ai you solve all the problems kind of in that sense that it's kind of like and i and i definitely think that yeah thinking that like you can just build a super ai that becomes a doctor just like you know without any kind of human input i obviously understand that you would need that kind of connection to make that uh leap but i guess the kind of thing about it that has really captured my interest is say openai's codex and the ability of language models to write the code so they so with the ability to also write the papers digest the papers and then also kind of like write the pie torch code and then you could kind of completely encapsulate them in that environment with the datasets that were because we're like just kind of trying to figure out how to increase like the squad benchmark imagenet benchmark right so so it's like completely encapsulated in that environment which you say would be a useful generalization sort of like the idea that it could write its own papers oh papers um there's actually some i think uh there's some interesting work from uh kevin knight's group uh usc i think kanji group at uiuc who do like automatic paper writing i think something called paper robot from usc and then there's like hangi's group does like automatic review writing um i i would say that those like i'm i'm actually a huge fan of those works um i don't view them i'm not a fan because i like i think that should exist that we like need systems that will write papers for us um but i do think like from a research perspective from like uh what can we get out of understanding whether these these like are like modern ai tools can and can't accomplish when given these sort of like kind of absurd tasks generate a paper uh i think just studying that helps us understand limitations of these tools um and if and my takeaway from looking at those works is like okay what are things that these that we can kind of reliably trust these these models to to get right you know autocomplete type stuff autocomplete for boilerplate right is the thing that people love the most um for for for lms that write code and so what is the kind of the equivalent of that for a paper um and then how can we build tools uh around just this functionality but allowing a human to seamlessly still kind of do everything else i guess it depends on how sci-fi you want to like your like timeline is if you're like if you wanna if you if you don't mind your research being used like like kind of super super far out then yes i think you should totally you could probably work on like let's just generate the paper and end and not worry about the human component for me i'm more of like a i'd like to see stuff being used fairly recently and so uh for me it's more about like okay what can we figure out can we understand these to what extent these tools are useful now or in the next few years and then partner with like kind of experts um to build kind of these like synergistic tools uh yeah yeah i agree i think i i did get kind of carried away with the animation yeah i obviously love the human computer interaction i'm not like trying to put myself in us out of a job but with the idea of like just completely automate the role of the scientist in the middle but maybe we could um come down come down from the clouds to can we automate scientific reviewing in that question of can tldr abstract or summarization like what kind of tools do you inv i know obviously symantec scholar is like a platform with all sorts of things can you imagine like you upload your paper and then it you know all these tasks like cytance classification where you know the model analyzes one of your citation sentences and then gives you some feedback for your for for the human in the loop for the sake of here's how you write a better paper um that would be cool um i think i think anything in like the assistive writing uh space would be really interesting um our team has and i guess like my own interests also are definitely right now on the assist of reading um just because reading papers is really hard um and it seems like there's some reasonable promise um for nlp to actually help make papers make paper reading a lot easier um so and i also don't think that um working on reading and working writing are completely like disjoint paths for research like i do want some sort of assistive writing attack when i'm like writing reviews when i'm um writing papers um when i'm writing like a tweet about a paper that um i think that like the tools needed to to enable that type of assistive writing technology under the hood probably has to uh is like sort of doing some sort of like complicated reading comprehension term extraction definition uh uh like definition generation linking to to of these terms to like other papers linking a freight claims to other papers linking terms to wikipedia like these types of uh operations that you would kind of you would develop if you're trying to build an assistive reading uh as well so you know you can imagine as i'm writing a review i am actually reading the paper and so whatever helped me write this thing the pool probably helps me read it as well and so hence right now the focus for us is like let's help people read and then maybe once that's pretty much let's help people let's add some more stuff and start studying how to help people write um but yeah i think reading is like right now just like the the number one thing it's um i think it should be super interesting for nlp people who are tired of working on like little short texts you know titles and abstracts aren't super long um uh full scientific documents are extremely rich with like really difficult phenomena and models fall over when you're trying when you're trying to do anything on these papers um like if you look at the performance of our best models on casper it's like awful um and casper is like uh can you answer this basic question from the entire full paper the papers have figures tables um things are kind of out of order because like there's like layouts that's part of part of part of um part of papers like there's like positioning involved of stuff there's sections and subsections so things are organized hierarchically there's like interruptions because of footnotes and so like just like papers are this really rich structured document and the way we've been building these nlp models is sort of just assumes that we can just like treat these documents that one kind of giant string uh without her structure and so once you move applied techniques built under this this this line of thinking on these large papers everything kind of just doesn't work um and so that's like i feel like that should be like extremely exciting to people to work on um in addition to um this is like prime time to to apply if you're interested in information attraction extract information from it from extract these terms extract relations from papers and highlight them so that people when they're reading they can actually see what's going on and that's like a great application if you're interested in information instruction if you're interested in linking to databases yeah like literally it's awful for me when i don't know a term at the copy paste that term open a new tab and google and search and find the page and then jump back to paper apply your anti-linking thing to just like making that just like a one-click thing without having me leave the paper so like there's like a lot of really useful opportunity to apply nlp uh nlp techniques that people have been developing forever um to pay person to and then we make our own lives easier so that's a nice bonus yeah and i well so firstly i i will i really do want to return to the topic of getting together experts to annotate say knowledge intensive things like getting together 12 nlp scientists and i think with q asparagus a team of at the university of washington right and all that and i really do want to get back to that topic but quickly you touched on something that is just so important to we v8 and our vector search community and as we've been partnering with gina ai we've seen their doc array and how they're organizing this idea of you have a very complex object that you want to search through so you have to decomp you have to segment it into different embeddings so as you mentioned like you would want say an embedding for the abstract and embedding for you not even like the whole introduction right because like two pages of introduction to uh first you gotta put it into like 512 tokens right and you can like average the embeddings of the 512 tokens but and then you have say images from the paper and then like the tables even kind of need to be formatted differently like the math for latex equations like this kind of segmenting and i'm so curious about like overall how you're segmenting are you right now is the approach uh use something like lin form or like you know the try to get the sparse attention so that you could maybe try to put the whole thing into one transformer or is it some kind of hierarchical uh you know segmentation and then propagation up however that might work honestly nothing quite works so um okay okay that's not true um uh i mean um from from the folks that uh from like is beltagey and armand gohan matt peters uh yeah too um with long former um we've been using long-form for a lot of our experiments and sort of the question is i think that you need like even though i think that you need some new model uh architecture that represents that can represent like this like this like heavy amount of structure that's in these papers um that's still a hypothesis like i could be wrong about that and it could be that models are just so powerful that today that you can just take the text linearize it in any arbitrary way just like take it and turn it to one giant thing and then just apply and like maybe with like a few tricks like add little set tokens between each section or something and maybe that's like the that just like is just better results and better performance than any sort of fancy hierarchical language models like architecture you could come up with i don't i don't know um i think uh i think it's too early to tell because there's not enough tasks that actually make use of um there's not enough tasks and data sets on scientific papers uh at scale that allow people to study this problem right like if you can't measure whether it's working either you you can't answer the question um there's just not enough people studying the problem so you know i like i like having i would trust kind of i would have more stronger opinion if i had like you know a dozen labs just like kind of saying like this is the right way i'm like okay yeah it seems like they've they've been studying this um quite a bit um and uh yeah i don't think there's just been enough experimentation um just just about different ideas um like if one hierarchical model architecture it doesn't doesn't work does that mean that like it just doesn't work or like or is there some some other thing um so there's just like too much work that still needs to be done for me to really have a strong opinion nested i i haven't i have a guess that it that something special needs to happen especially with latex equations and figures and tables um but there's also like a an ink language like maybe it's just about vision language models maybe just like taking like screenshots of of of tables and figures and then just like toss them into something and then do like a vlper type thing and then that's it um so so i don't know yet yeah seeing the demonstrations of flamingo on twitter deepmind's new tech image uh captioning thing is definitely made me think that that idea of the yeah like screenshots the tables i'm it's like you're pretty good you're good i was like so this is one of those things like i invest a bunch of time trying to develop like custom architecture that i think is tailored science documents work or is it really just about uh smashing these things together pre-training and then you know where to go yeah and um so i'm trying to think of if this would be too off topic but maybe very quickly like as we've been studying approximate nearest neighbor search we've kind of been thinking about whether this could be the answer to very long range attention like if you could uh you know have the the query key dot product instead turns into an approximate nearest neighbor search with an enormous set of key like the keys are vectors right and you're so you match the query like quickly i'm sorry this is kind of distracting from another topic do you think that kind of approximate nearest neighbor search into transformers could maybe be the solution to extremely large memory transformers or like extremely large attention inputs um i guess like a like a a cheap a cheating answer would be like maybe maybe what it means to be seen so so i guess um we uh we have some people uh kind of joining recently looking into like we were interested for a while until like rich people augmented language models which i guess kind of has that idea right it's just like oh a language model we don't need a language model like this like enormous language to memorize everything about about about text we should be able to offload some stuff like knowledge to some other components uh embed them somehow and then i guess like retrieve um when it's needed um this seems like a really appealing way of viewing the problem of like especially in science new things are are are being discovered new terms are being invented new ways of i guess like new facts about the world are constantly being being generated and uh it is impractical to just kind of like keep training well maybe it's impractical to keep training just like language models that kind of keep keep up to date with this thing so representing it separately and then having a language mod that kind of like and i'll either dot products into these embeddings or or or somehow just retrieves passages and then learns how to encode those like text patches into embeddings on the fly and then incorporate them into the language model seem promising to me um i know there's like the kill benchmark which evaluates uh like kind of like knowledge intensive models um and the retrieval augmented models are sort of doing the best on the benchmark but uh definitely i guess still need to see more of it for science um and i don't so i don't know it's like still right um this this line of work yeah that's really interested in uh when malt piece from haystack came on the ebay podcast you mentioned that that approach of retrieval augmentation it allows for better interpretability because you kind of see what it's using to make its prediction it's better for updating it and then i think uh i think it's better for maybe removing the biases that kind of thing because of kind of entangled interpretability i think maybe there was a third thing he had mentioned that i'm forgetting now and i actually read a recent paper uh the biomedical enhanced uh or biomedical evidence it's called like literature augmented clinical outcome prediction oh i think that's from akangshan uh and tom hopeless wong from from our team probably yeah yeah yeah and it's that kind of thing where you retrieve from chord 19 right and it helps you do the clinical narrative completion yeah that stuff is really cool yeah yeah that's so interesting and so sorry so let me come back to what we were talking about about this idea of uh taking in the whole paper as input really quickly so it sounded like maybe your issue with it would it would be hard to build the data sets maybe and and so i wanted to ask your opinion you mentioned open review earlier as a data source and i've seen you using techniques like this before uh so so do you think and that like i you know people review papers they compress the paper into you know strong rejects explanation or neutral like uh so do you think those data sets and from platforms like open review do you think that could be a way to get that going and yeah yeah so i um i'm like a pretty strong believer in deriving data sets from real platforms where humans are like actually using the platform to do something sensible um as opposed to kind of like contriving a task and then hiring people to do it um so open review i it i think is a great resource um for for real live sort of like um what are they uh blinking over like like natural or um it's fine but yeah you get the idea it's just like it just seems like it's a really great platform um i think my only concern with it is just it's just kind of not big enough uh so you know more success to open reviews so we can get more data but it's limited to particular fields of like particular computing related fields um so there's no like open review for biomedical literature clinical literature um so that's kind of that's kind of the worry so i think it's a combination of like trying to be opportunistic like finding these like really useful data sets and also investing in techniques that help you like not have to re build that open review for a new discipline each time just like you want some way to do an adaptation to these other friends when there is no data and this kind of takes me to another question i want to ask you related to the uh q asper work where i think it was you you hire uh graduate students at the university of washington to uh yeah yeah so so i was wondering about like if you had a platform that would you know pay people directly to do this kind of expert annotation what do you like i guess the question i'm i'm kind of asking you is like with your papers would you be willing to pay for such a review such an annotation of your own paper and then some maybe you know platform that ties into uh i like you also opt-in to let this be used as a data set such that the language model could be trained on it to maybe supplement you not to like put you out of a job as the annotator which i guess this kind of idea was doing from the start right it's like if you're yeah yeah um just uh just so i can understand question are you suggesting that we should be paying reviewers it's an interest i think it's an interesting thing because it the incentive system now does seem a little bizarre to me because you're kind of reviewing it for like you know the sake of you know like scientific rigor which is great you know which is great but if i feel like if you added the payment i mean you might get the bias towards like i want to write a good review because they've paid me for it or something along those kind of lines but i mean i'm curious what you generally think of it it's not like a startup that i'm launching or anything no that's really that's interesting [Music] i i generally so maybe spicy take but like i definitely think um reviewers should be compensated for their work because it's a lot of work it's a lot of expertise work in the community and like publishing communities rely heavily on this um we're sort of relying on volunteer effort right now obviously there's some like i guess like for academia we need to you know money like this destroys the sanctity of it but i don't know we should pay people for their labor so um i definitely review like review or work should be compensated i just don't know if i don't i i'm not sure about whether the author should be paying um the reviewer exactly or coming from yeah or like is it like through fees or something like that type of thing i think needs to be experimented a little bit because of i guess i don't i'm not like crystal ball enough to like figure out like what the ins how the incentives can align or don't align or something with in those types of scenarios yeah that's a pretty interesting detail but should the authors be because i guess it's like for me as a phd student uh publishing papers it's like if i can get a paper into iclr it would be it would benefit my career from the rest of my life sort of so i i would be willing to pay for this kind of review right and it's kind of like as you put together these teams these reviews are kind of you know you know i mean i don't know how like you know obviously you communicate a lot with your teams but the reviews are sort of the communication bottleneck at least with my ph in my experience with my phd lab the reviews is our kind of like interface with each other mostly with you mean with like other researchers in the field yeah like at least at florida atlantic university we have a little lab and we trade we trade reviews that's how like that's how we communicate we don't so like and i imagine a lot of other labs do interface that way so maybe you know can it connects you to the bigger you know market by having it be paid because then it's more like you know more people would probably sign up to that and offer their expertise because because again it's it's like expertise it's not like anyone could give you a thorough review of your uh yeah yeah augmented language model yeah yeah i um [Music] it's interesting it's interesting i don't know um i but i'd be interested in seeing work in this space um maybe some like kind of like longitudinal studies that in in yeah and maybe it comes back to like code reviews too like the same idea but for reviewing your code your commits is like this idea of kind of like science and also code and maybe even coming way back into when i mentioned kind of codex and why i see like the code the the language models that write code i think and the language models that interpret science i think should come together a little more i can see that i can see i can see that happening for um i think for select things i think for i think i can i can see it happening for um okay so like definitely for for a fairly low level writing clarity type things um you know i would love it if if something uh could tell me if like i have a variable that's unassigned which is very much like a kind of code thing but when i'm like typing up notation and i'm like come coming up with notation and overleaf um if i just have like like a subscript eye that is literally never used i want someone to tell me that that's the case and that's fairly low level it's just like checking like this thing isn't it used anywhere else um and then maybe i would i would like something which is like oh hey you're missing the citation um in as you're writing and i thought oh yeah that'd be good just like prompt it so then i can figure out what to do with it um so there's like a lot of like little things that i have to keep in my head and i just like don't want to keep them in my head and it would be nice if some agent could like help remind me of these things and then as it starts getting into the territory of like it helps me interpret my results like that's when i'm just like i don't know i don't know if i enjoy this um yeah i also don't think that code like code like the the code language model stuff is like even at that space either like nobody's like i don't think they're creatively writing like just like the idea of the software for people right yeah and so that's that's the other thing he said that's what i currently use uh literature mining tools for is to you know i have an idea is this actually a novel idea hit the literature or try to find the idea of the paper is that currently how you mostly use the tools can you tell me about like how you use semantic scholar and how that you know because again like you have this meta thing where you're developing the tools you have anything yeah it's a nice space to be in yeah i do have the same thing i i use um i actually use a combination of stuff because it's like missing something is catastrophic um so i don't trust any one tool i use mascara google scholar um and i use i use a lot of social signals i think i use a lot i just use a lot of just like everyday check archive just skim the the next batch of like 20 or so papers for that day it's like there's a lot of like you never know when one tool is going to miss something and i also just ask people constantly just like have you seen anything like this you've seen them like this um i do a lot of really manual traversal of citations um so uh it's all terrible it's all awful work um and we definitely need more things that make it make it um nicer because it really is like no one tool is is is kind of the thing at the moment yeah salute to you for uh checking archive directly i have to get it dumbed down through the twitter bottleneck but yeah that's that's really interesting and maybe like a concluding question on the podcast that i ask a lot of people and again i you're uniquely positioned to answer this is what is your information dot i mean you kind of did just answer the question but is there anything lacking from that of your kind of information diet for my information diet um wow that's the thing so the thing with lacking is like probably but i don't know what it is i think that's the problem that's the problem with this information diet thing right so um i actually don't know if there's something lacking but there probably is uh and um my information diet today is just like i check archive um almost every day i actually use this man scholar recommendation feed um which is quite good um it's quite i mostly read papers that like teammates um and like friends people i follow will share on twitter share in slack messages and stuff like that um and i actually i think like half the papers i read these days are are for like peer review i like peer view duties i actually really enjoy reviewing um because you get to see like what people are submitting um you get to see like like that's kind of like the glimpse into like what's happening right now that's not in the public and um it helps you read in a different way than if you're just kind of passively consuming what's on twitter they help like it keeps like kind of like the the critique engine sharp um uh yeah i guess that's probably most of my diet but if i had to pitch an idea for somebody to go work on something i would love somebody to build like a browser extension that tracks the stuff that i'm reading you know doesn't like do something weird with my data but like attractive stuff i'm reading summarizes it to me and maybe recommends me like papers that or like avenues for accessing papers that like i should be accessing but like i don't i don't even know exist i guess this is like i don't know what i'm missing in my diet so i want someone to tell me that i'm not eating enough protein or something yeah and maybe uh if it's not too much of a personal question could you maybe take me into like a day in your life at the allen institute i'm like very curious what that entails yeah so day in life um i think for me uh nowadays it's like i am in meetings with uh i guess like we're kind of entering this like summer uh spring summer and then kind of through fall so this half of the year it's mostly mentoring uh interns uh who are like kind of like current phd students um or undergrads who are kind of preparing for um their phd applications or to enter a phd program so it's a lot of mentoring uh people like kind of new people who are showing up for this half a year uh we're starting on new projects doing a lot of ideation a lot of just like sitting in a room and just like talking about like should we do this or do this uh run some quick experiments meet back again as i get stuck and try to interpret the results try and decide like should we invest more in this direction should we pivot it's a lot of these kind of uh really quick turnaround decisions about how um uh how to like go about answering questions that we're interested in um and then that progresses into more heavy duty development work um over time where it's like every day is just like kind of writing code running experiments checking those experiments and then revising or fixing some bug or something and then into like writing and submission or just like every day just like writing reviewing the papers chatting with people um and and continuing revising um so depending on the stage i'm in it's like all of it is chatting with people all of it is like developing or all of it it's just like writing super interesting well kyle thank you so much for coming on the podcast and i i really really enjoyed this conversation i love these topics and i think scientific literature mining and the work you're doing is just definitely my one of my favorite things to keep up with in deep learning and it seems like it's headed to such a such an exciting application being realized with deep learning cool thank you so much for inviting [Music] you ", "type": "Video", "name": "kyle_lo_on_scientific_literature_mining__weaviate_podcast_17", "path": "", "link": "https://www.youtube.com/watch?v=kUjhCsawgCo", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}