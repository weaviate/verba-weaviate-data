{"text": "Hey everyone, thank you so much for watching the 50th (!!!) Weaviate Podcast with Emil Sorensen and Finn Bauer from Kapa AI! \nhey everyone thank you so much forwatching the wevia podcast we have asuper exciting episode I'm so excited towelcome email and Finn they're buildingKappa AI Kappa AI is uh it's a superimpressive kind of like large languagemodel with Vector database for your ownuh code documentation blog post thiskind of thing and personally it saved mylife we have it in our wevia chat and Ineeded to say Hey how do I insertobjects one by one with JavaScript I askkapha it's like I got you correct answerso I am so impressed with this systemand I'm so excited to be talking toemail and Finn to hear about like howyou to think about the the evolution ofthis space so firstly guys thank you somuch for joining the podcastno thank you awesome thanks for invitingus awesome so could we kick this offmaybe just like with the origin story ofcapital like how you how you came to beworking on thisyeah for sure happy to give a quickrundownum maybe by brief intro I'm Emil Finnover here we go back about six sevenyears met while uh while doingUniversity initially studying Financekind of looked at each other and saidlike ah you know maybe this world is notfor us then actually jump ship and andwent to the post-grad and computerscience and kind of did a lot oftinkering back in the dayum you know tried to get a lot of littleside hustles off the ground and so onum eventually that wasn't the time sortof six years ago after the post grad soinstead went to our separate waysum I was uh I was a consultant for acouple years Finn worked as a softwareengineer and then I think last year wekind of started looking at each otherand like man there's there's a lot ofstuff happening in the space you know wewere trying to fine-tune our own GPTtrue models when that was the thing backin the day and both of us you know havea lot of friends that run like developerfacing companies so we just kind ofkind of like naturally began narrowingin on on this this this problem spacewhich for us now is really just helpingdevelopers useum use uh software productsum and I guess yeahyeah amazing and I I really want to getmore I think you'll have such aninteresting perspective on this kind offine-tuning models and how people thinkabout having their own custom largelanguage models is a topic I really wantto explore but firstly I have thisquestion about you know when you have anew company like weavier for examplethat you've gotten to Kappa how are youthinking about like how fast can I getthe documentation code like how do I getall this into these systems as fast aspossible sort ofright so maybe at the moment it takes usabout like a day or two depending on howmuch like the big the backlog isum so we've found so far like becausethis tag is super new and everybody isstill a little bit like uncertain whatit can do and how it works you kind ofneed like a very white glove approach tothis so likeum we're not really doing this like on aself-serve basis at the moment so welike we just ask the companies okay whatare your relevant sourcesum like what should your Bot know aboutand then we like we kind of do it forthem at the moment and like hold theirhand onto this in productionum just because it's so new and nobodyreally knows how to work like how wellit works what it can do what it can doum and we can we can talk about like howwe ingest it if that's interestingyeah I'd love to dive into that topic Imean I can maybe speak a little bit toI've been trying to take this podcastand put it into eviate and so I you knowI put it into whisper and I still Ithink white glove I interpret is meaninglike manual cleaning still needed solike I get the transcripts and I youknow specific terms like if we starttalking about like the Laura paper thelow rank adaptation like that particularword it'll probably translate Laura tolike lore l-a-u-r-a the name rather thanl-o-r-a like so these little things iswhat I'm finding so um yeah dataingestion this is the I'm so I reallywant to pick your brain about this likehow much manual effort is still neededare you excited about things like youknow unstructured the Lang chain kind ofPDF parsing thing like how you thinkabout data ingestion to these kind ofsystemsum I like I think sort of all Generalexperience so far has been like it wouldbe great if you wouldn't have to do sortof any cleaning on the data like if youcould just put a full Self Service thatworks automatically and you don't haveto touch it at all and it's very simplethat'd be great and we've seen likequite a few people or like projectsdoing thatum it tends to not work that wellbecause you get a ton of things in therelike that mess with the performance anddown the lineum so you do have to like like if you ifyou can get to a very clean state in thebeginning it'll work so much better inthe endum that'syeah especially also how we'veimplemented is like try and get to thecleanest version correct this version ofthe data before you ingest it into thesystem like thisyeah so maybe graduating from once weget the text from whether it's like aweb scrape and we got to kind of get theHTML tags out or you know PDF parsing wemight have some funky way it read like atwo column layout or something like thatso once you have the text how are youthinking about chunking it into Atomicunits and kind of having a symbolicstructure around it or maybe those aretwo separate questions maybe chunkingfirstum maybe like even like one step beforethat would be like how do you even getto like a clean representation becauseso for us like um yeah like everycompany has like a lot of differentsources that you would input it's notusually just a docs you have likedifferent pages for the tour the dogstutorials like discourse forums GitHubissuesum and they tend to all have likedifferent structures for every companyso you do need some type of like Generalapproach to doing this so which for usmeans that the like web scraping at themoment like having like a framework thatcan generally handle all kinds of pagesum so that's sort of the first part uhto be able to even get everything andthen the second step you can think aboutlike cleaning chunkingum yeahyeah I'm really I think a lot about likesort of mimicking the system you'vebuilt with our weeviate content and howthat would look I think about like justlike the symbolic filters where I'd havelike where title equals blog Play posttitle and then I like you know whateverit is and then I keep that with each ofthe chunksum yeah so that yeah it's reallyinteresting just like how you're parsingout the symbolic structure of someone'sdocumentation I imagine I really want todive into how you do you mentioned theDiscord can you tell me a little moreabout how you mine out like Discordslack conversations and add it to thiskind of retrieval system yep happy tomaybe maybe jump in here as well I thinkmaybe as a sort of a general theme whatwhat Finn and I are slowly discoveringas we're building out Kappa is that andthis maybe speaks to the broader natureof like you know where this you knowtypes of applications like composite inthe ecosystem right is that there's somany of these like you know bits ofNiche information that are specific to avertical we're very much in thedeveloper facing space right so thatmeans like going out and talking to 10of your customers to understand well howdo you view your discourse Forum rightokay you know you have a certain numberof people that reply that means okaywhen it's someone from the moderatorteam we should probably ingest that intoKappa doing just a full thread or do wejust ingest the question answer pair andthe only reason like the only way youwould ever discover this is by going outand having a lot of these conversationsum and that's what we've done for by nowI think a whole breadth of differenttechnical documentation sources be thatslack or discourse or GitHub issuesetc etc etcor maybe for slack specifically likeevery company tends to have a slightlydifferent opinion on how their slackshould be ingestedum so we make like with kind of a bit ofa configurable framework to tailor it tolike each company and then the case ofslack like what's good is thateverything is already in threats soeverybody every time somebody asks aquestion somebody replied creates athreadum and then for example what we do likeone company might do like okay take allthe threats with the last yearum only take threats where someone froma sort of whitelisted amount of peoplereplied or like from a team or specificpeople and drop all the other messagesand then take that sort of essayquestion answered herethat's sort of like you need to get tosome sort of non-noisy representationwhere you can be sort of short andnobody has said something wrong becauseyou might have a threat where somebodyasks a question then there's like twocommunity members that say somethingwrong and at the end you have someonefrom the team that actually gives thecorrect answer and if you take all ofthat you're just going to confuse yourBotso you need to again back to likecleaning and getting like a non-noisierrepresentationyeah that's such an interesting nuggetof how you do this I mean I I kind ofhad two ideas like thinking firstlymaybe like you know whether it's just aclassifier to run cheaply or it's alanguage model prompted like this to saylike you know is do you think theoriginal answer is asker is happy withthis answer and doing some filteringlike that the whitelist thing veryinteresting um the academic in me whoyou know I love the idea of kind ofbenchmarking these systems so that youcan sort of ablate the models and Ican't wait to talk to you about themodels but like do you do you maybe liketake 50 of these threads and then havesome system ofyou know replacing maybe the whitelistedperson with the language model and thenfrom there you get some sense of howwell Kappa works like is there some kindof like like I guess I'm trying to getout is have you have you thought aboutthis kind of like you know using likethese like these abstractive questionanswering extracted summarizationmetrics like Rouge blue scores on thesekind of bootstrapped examples ofconversation threads where say I'm onthe white list and you replace Connorwith you know with Kappa and then yousay here is the comparison of the finalanswerso so far we've just used the data as isum to keep its like so we can so you cango to the customer and tell them likehey this is exactly like what's going tobe in there like there's not going to beanything generated you don't control sothe company actually knows okay we onlyhave this set of information becausethey tend to get a little like skittishif you start to generate some things andso on like it kind of slips the controla little bitum and you might introduce some sort ofwrong things and some like somethinghallucinated that shouldn't be there sothe moment it's just like I'm sure someof these things would work quite wellum at the moment we're not doing it onething we're probably going to do is likegenerate a titleuh where you could go like hey here wasit because slack for it doesn't havelike a somewhere a summary title but youcould generate one of the conversationsaying like Okay this was what this wasaboutum that would probably helpumyeah I just saw uh Jerry and llama indextweeting about their uh like summaryindex where you summarize long thingsand yeah yeah it's such a cool idea andyeah the really cool first like I guessbefore transitioning to thehallucination problem which I think issuch an open such a big topic for thisapplication I I guess the question Ireally want to ask and get your you knowexperience of building this thing out islike you know how how do you tellleviate or whoever like how well Kappais how good Kappa is likeyeah yeah no it's a super good questionI mean the short answer is you know thethe potential accuracy threshold of anapplication like Kappa really alsovaries across projects right you knowyou might be lucky and and be dealingwith you know a company like VBA thathas fantastic dogs you know like a deepset of technical tutorials threads ohseriously rightum and and for which you can actuallyget a bot that can be quite performantum and then there are other times whereyou know the company think they mighthave sort of great dogs but thenactually when it comes down to itthere's a lot of sort of holes so so theshort answer is it's really just againhaving conversations with with thecustomers trying to understand like youknow average accuracy metrics based onsort of thumbs up thumbs down that helpskind of proxy thatum and then also just sort of settingexpectations for where the tech is todayyeah and I know from my experience usingKappa I've seen you know the thumbs upthumbs down and that that it will bewell I I want to park that because Iwant to come back into this idea ofowning your own language model and howyou could bootstrap data that way butlet's hit the hallucinations thingbecause you know naturally I think thethe biggest thing is I yeah yeah how areyou think about hallucinations right nowI mean I think that the short answer isit's probably the thing we'vehistorically thought the most aboutum you know I can I can name countlessof experimentsum conversations with academics likescanning through the latest archivepapers like trying to figure out likehow can we bring down hallucinations andthere's probably not an approach wehaven't tried but to be honest ask thesemodels are getting more powerful forexample you know upgrading from from ourfrom our case we run an open AI as ourlast sort of off-the-shelf generationmodel just making that bump up from uhdavinci03 to gpt4 already helps addressa lot of thatand then in addition you know we think alot about sort of nudging Kappa toalways produce an uncertain answer asopposed to a a sort of a potentiallycreative answerum which means today Hallucination isnot really a problem we deal with inproductionfascinating soso the current state of just kind ofprompting it like please ground youranswer in this context and then justplease don't make anything up in allcapital letters or something like thatjustthat alone seems to be strong with thestrong morals but so I guess my questionwith with that well it comes into thetransitioning topic of I'm very curiousabout how you think about the largelanguage models because um you know sothe running gbt for every query Iimagine is kind of expensive so are youare you interested in the open sourcemodels like you know llama gbt for alllike the Mosaic MPT these kind of Trendsand then but then did they not have thishallucin like then you have to rethinkabout hallucinationI mean the short answer here isincredibly excited about the potentialum I mean for one from a costperspective and potentially latency downthe line rightum but also very excited because I meanone of the common concerns right youhear when deploying these these modelsin production is you know ownership ofdata and transferring and so on so Iwould love to be able to offer Kappaon-premum as a service which would be possibledown the line with something like anopen source llama Etcum but right now honestly it's notsomething we spend a lot of timethinking about just because the I meanthe whole focus is reallyyou know a slower but better answer isis usually preferredand um you're just you're very rightlike these it's it's quite expensive atthis point which means from an economicperspective like some much largercontext Windows aren't necessarilyfeasible for this use casebecause you have to look at it okay likehow much is one question answered or onegeneration worthenter like in sort of the economic termsfor like your customer and can I evenuse something like a 32k like contextwindow like does it make senseum and for us like at the moment it'slike cost is fine but like for exampleusing like 30k context window willprobably wouldn't be we couldn't sellthat most likelyfascinating I mean I think umyeah so I think the gbt4 32k and then Isuspect that the anthropic uh 100K thatboth work with like this API thing Isuspect that yes both those are quiteexpensive but I'm so excited about thismosaics uh 65 billion parameter oh Ithink it's 7 billion sorry 65 000 inputwindow is um is something you can run inTransformers I think it's I thinkbasically the way they're doing this isthey call it Alibi attention and it'skind of like the newest sparse attentionvariant where sparse attention is younever have that like dense matrixmultiplication where you multiply 65 000by 65 you know and create these giganticmatrices that obviously can't fit on asingle GPU and the whole problem butwith with those kind of models it makesme think that maybe they canuh cheaply run it yeah I don't know toomuch about it but I'm the question Ireally want to ask you about these longinput length models is likewith retrieval and you know obviouslyyou're working on aggregating all theseinformation sources for companies likeWi-Fi documentation code based blogposts code examples this podcast mayberight so like do you think packing theprompt as densely as possible wouldwould be the future of thatI mean maybe at some point but right nowfor sure not I mean you really likeif like what you want to get is like thesmallest amount of relevant context toanswer the question like that's idealright so that's where you get the leasthallucination the most crisp answerumif you I mean you can fit a lot moreinto the context movement we're doingright now that wouldn't necessarily itwill make more expensive and it wouldn'tnecessarily improve the answer or makeit worseum maybe at some point it's so good butright like right now in a super future Ithink you should definitely want thisretrieval stepum to give you like the relevant contextI mean the more irrelevant things youadd the less you the worse your answergets hey everyone sorry we had a littlebit of a recording a little bit of aconnection issue but we're back uh sowhen we left off we were talking aboutthese kind of long input length modelsand the trends in that I think itactually would be great to kind of pivotinto this next conversation which is thetrends in fine-tuning your own largelanguage model which is we're seeingwith Laura l-o-r-a the low rankadaptation it's it's like a trend in thesparse fine tuning where now you canfine tune like a seven you know abillion parameter model for like 20bucks and or I don't know about 20 Ithink I've seen like a hundred dollarsis arbitrarily obviously depends on thesize of the model and how long you'regonna train it for but yeah so what howdo you think this will change everythingfine-tuning language models arecompanies going to want to own their ownlanguage modelI I think and I'm just speakinganecdotally here also with conversationswith other Founders in the space I thinkfine tuning has a lot of promise andwhere fine tuning can really delivervalue to folks in the real world that'svery much stylistically in terms of thethe types of answers it generates but ifyou're thinking about fine-tuning from alike accurate knowledge retrieval stepum I I think you'd still run into a tonof hallucination problemsat least anecdotally speaking you knowwhen we've done countless of fine-tuningexperiments and and it's it's reallyyou've just run into hallucination asopposed to havingum like I think I think Sam Altman had atweet a couple of months ago saying likelike talking about some new opening eyepricing and history was something likeyou know I think most people would behappy to pay like two dollars for athousand tokens but a thousand reallyhigh quality tokens and I still thinkthat still holds trueum also from I would say our perspectivemeaning I'd much rather have a a greatoff-the-shelf you know high accuracyHigh reasonability off-the-shelf modelmaybe just to add to that like umfrom what we've seen like with we hadtwo customers so far that uh like in thepast like sort of beforelike like sort of last year took a bunchof their support tickets so like some ofthem had like 3 000 emails for examplegoing back and forth and find you intheir own likeI guess like what was available like Iforget what exactly they find you butthey had sort of what Kappa is nowthemselves but purely find you withoutany retrieval and use that to likegenerate answers to support ticketsum and now I've moved off that and justuse usum and yeah like we we basically tookthe same data that you used to fine tuneput it into their botum and it like just anecdotally to themit works better this wayumyeah fascinating I think working on avector database I think you might expectthat my reaction to this news is justlike you know thumbs up all the way yeahbut I don't think these two classes ofalgorithms are like approaches have toconflict like this like I think you canstill have a retrieval augmentedgeneration with a fine-tuned model andyeah you still have the benefit of Itkind of can cited sources a little bityou can update the information based itso I don't I would start by saying Idon't see these as two like differentthings like if the Laura algorithm issuccessful that means and the vectordatabases and all that kind of thing butyeah I think something fascinating aboutit is likethere's this like robustness like thisGeneral language understanding thatcomes from these large models with howthey've been pre-trained and I do thinklike if you fine tune it you might losethat kind of robust like you as youmentioned like the hallucination problembecomes more so yeah and we've seen thatlike crazy with the there's a lot ofexperiments like clip there's this papercalled wiseft where they show that likeyou know if you just fine-tune clip youlose like the robustness of it and it'sreally interesting yeah I mean umthe idea of fine-tuning on supporttickets I mean have you ever like thewhen I've talked to like you know I'vetalked to Jonathan Franco in the podcastabout Mosaic ml's philosophy with owningyour own language model and it's likehave you seen someone who has just somuch Text data that you would pre-trainthe language model on it and so it's noteven like fine-tuning it's like the thelanguage modeling objective is done withthis data setmaybe the biggest we've seen so farlike you never really get a like passedagaintwo three thousand pages of textsort of like Loosely speakingum nobody has like an insane amount ofvaluable datafor like their specific I mean like thebest case scenario would be somebody haslike really really extensive talks andlike apri reference that has a ton oflike good GitHub issues in verylike big discourse forum and then maybelike their past like conversations buteven with like if that all is really bigyou don't really get past like 3 000pages of text or something like thatyeah that's really fascinating becauseit makes me think like good like gooddocumentation should be sort of compactI suppose like right like it's also Imean it's so interesting like shouldcompanies be trying to create as muchdigital content as possible in thisworld or should they be like have areally refined compactdocumentation or and there or are therelike two things you mix all right likeyou have the source of Truth highlycurated and then like you know thispodcast how we just fire it on and juststart talking and creating content liketwo kinds of contentI mean for some like like we sometimesdebate this with the blogs of companiesbecause some of these are like technicaltutorials almost how to do something andthat's relevant probably to developersbut also some companies write a lot ofblogs for sort of CEO optimizationuh sort of some general news which islike Loosely related to them and it'sthat's not necessarily super valuablefor you like the QA boardum it does have to has to be like reallyreally high quality text I mean oneinteresting thing regarding that as wellis that soyou will have a lot of relevant okaycompanies will have like a lot of techand documentation around their productbut to understand that you still need alot of other like more fundamentalunderstanding of things like programmingand like other Frameworks and operatingsystems and if you take all that awaylike then their technical augmentationreally doesn't make much sense so youneed some likemodel that understands sort of all thecontext beneath what their product isaboutfascinating I mean especially it's likeI think there are two things so this islike the gbt4 knows already knows Pythonand JavaScript so you don't need to goget the python documentation andretrieve from that but then it's likesay like with weaviate like if weintegrate like we integrate with Langchain and then like Lang chain iscontinually evolving such that the largelanguage models can't keep up so it soyou so have you ever thought aboutadvising someone to grab thedocumentation of like another librarythat they integrate with closelywe have like we have a few of thosecustomers likeum react this has been like kind of agood example of that it's a lot offramework front-end Frameworks a lot oflike things that interact with react forsome of them we've put it in like aswellum there's a little bit the like bit ofa business question of well do I want mybot to know like that well about thisother frameworkbecause data documentation will containalso some like oh how real how great isreact on its own and things like thisand it might lead a little bit away fromtheir product so it'sbit of anlike it's kind of like a case-by-casedecision if you put it in or not if it'slike valuableum I think so far it's been kind of yeahsuccessful for a few peopleyeah fascinating I think yeah I thinkthat's something that even humans likeas we create content we need to thinkabout it's not even just the languagemodels like am I just advising you touse Lang chain instead of the leviategenerate module this is a little thingto hit that but um okay so the nexttopic I really want to ask you about isalleviate we're curious about thisconcept called that we're callinggenerative feedback loops to describewhen the language model transforms thedata in some way like maybe you knowwrite us some like the summarization isone examplelike how do you think about transformingdata and then saving it to further helpwith the retrievalthat's I think that's a reallyinteresting area and to be honest wehaven't we haven't really exploreda lot of generative steps at theretrieval stageumI think it's one like I see Jerry fromllama index tweeting out a bunch ofstuff on this topic right so it is areally interesting thing that probablycould help with with retrievalum but maybe you're just one GeneralobservationisI think when working in in this spaceright we're always very quickum to quickly throw like a languagemodel on a problem like oh great youknow maybe we can summarize somethinghere maybe we could do something hereand I think that the general observationisit itkind of rarely works very very very wellbecause by every step you introduce likeanother GPT caller every step youintroduce one more thing you have tobuild a ton of guardrails to ensure youhave a stable system to handle edgecasesEtc so it's a lot of the time when sortof thinking about you have to reallyzoom out and think about systems designas you're introducing one more layer ofnoise that being said I think this isprobably a stage that is definitelyworthI explore any furtherumyeah it's fascinating I mean I thinklike it's like the large language modelOverkill solution is the pro is itpotential like I can say just from myexperience of like you know I have 477podcast Clips in this podcast searchdata set that you guys will soon be apart of and yeah and you know I as ISummarize each of these clips that costme about four dollars to do 477 so youknow and it linearly scales of courselike that kind of thing so so yeah likeI saw I saw Jerry what and one of thenewer ideas is use the large languagemodel to truncate the search results sosay you get you know 10 search resultslanguage models says only take the firstthree and then to the next step and Ithink that might be it Overkillit's like also using it for re-rankingis yeah I think we had something likethat in the beginning which didn't likekept throwing out like really randomthings and then we got rid of it yeahyeah awesomeno go ahead Connor okay I was gonna askkind of um you know wrapping of thepodcast these are this has been a greatcoverage of just how you think aboutcustomer uh companies getting all of thelike particularly software companiesgetting all their content into thesesystems uh thoughts on the trends andlonger inputs and then fine-tuning andthen just generally thinking about howyou think about your content all thesetopics I want to ask kind of anopen-ended question of like what whatTopics in deep learning and AI isexciting you the most right nowman's like where to start right I Ithink you know as sort of the founder ofthe space or Founders in the space it'sit's always a daily battle betweentrying to you know dig deep into theinto the next new thing to read thatnext like paper to begin implementinglike another idea to improve the systemand then also justum just kind of getting back to thebasics which you know in our case istalking to customers and and BuildingProduct right so but but I mean toanswer your question without answeringyour question here I think maybe areaswhere we're really paying attention tonow is like well what does it mean thatyou have 100K context window all rightwhat does that unlock what new ways ofactually helping out your set ofcustomers can you do with this featurerightum and you know it could be a ton rightyou know to your point maybe it isactually worth this like cost latencytrade-off to you know have a longer calla more expensive call but a moreaccurate one rightI think that is probably the thingthat's at least keeping mepersonally up at night right nowyeah I like that point a lot too like II was very interested in like crossencoder re-ranking so that's likere-ranking search results with it not alanguage like a 80 million parameterit's still slower and yes I think alsoin this new world you're willing to waitlonger and that's a very interestingTrend uh let me ask you two startupquestions so first of which is thisthing you talk about talking tocustomers I'm just curious on generallygetting people's perspectives on yourwork like working on a deep technicalproduct do you think Innovation comesfrom you know the customers and seeingtheir use cases or maybe like this kindof likereading the science like you're you'reeither like deeply in the science oryou're at like the application layer andtrying to see like what's the next thingto do from those two perspectivesI mean think for us because we're sortof the application at the applicationlayer so directly like try I mean we'reproductizing something and putting it touse so it's less of an infrastructureproduct so here I think relate like Ithink what's been super successfulversus relationship management withcustomers sort of just like classic likebe available hold their hand talk tothem often and kind of like ease theminto deploying it so that's been superimportant to usum maybe less so than like very verydeep technical Innovationum but I guess this depends on where youare in this like sort of the stack Iguess if you go down one level toalleviate that's a little differentumand it's sort of sorry sorry no I wasjust gonna say and it's it's sort ofright it's one of these it's sort of oneand the other it's like it's totallytrue with Finn is saying it's likespending time like deeply empathizingwith customers to understand whatproblems are they actually hoping tostuff can solve like when you go pastthe cool Twitter demo or um or you knowLoom recording or whatever but at thesame time you know it is also such a newand fast moving space so you do have toconstantly pay attention to like what'shappening in the science you do have tolike every single like you know newsblurb that comes out like 100k contentyou do need to contextualize thatin from the perspective of the customersfor whom you're trying to solve aproblem forfascinating I'd say maybe like to give aquick example from weviate like the newand this isn't like I'm not likerevealing private information this islike something that we set up a pocketlike the the group by filter that camefrom like a customer request as anexample but then like on the other endlike the research we're doing on disk an and like you know kind of like the PQstuff and filter disconnect if you gointo the weeds of it that's not reallysomething that a customer would ask forbecause they don't think about Vectorindexes like to know what the like totweet to make it different so I thinkthat's sort of the perspective of of howI see the abstraction layers and yeahit's so super interesting um so then thesecond question I wanted to ask youwhich with kind of like a startupquestion you already hinted at it islike this CRM the trends in growing acommunity and a set of customers is likedo you think about like using the largelanguage models to manage your sort ofrelationships with people and it is arewe seeing like a huge breakthrough inthatyeah I'm happy to take this I mean I Ithink I'd like to thank both Finn and Iare kind of by this point quite likenative with like all this stuff I meanmost of our coding is done either withlike with Kappa dog fooding it ourselvesor like c54 or you know clutter orwhatever the latest model isum but but this stuff I think it's justold school relationship managementinstalled it that kind of worksum I mean don't get me wrong we'realways trying like new new launchesscouring product hunt but um I thinkyeah nothing beats sitting down gettingon a zoom call and actually talking toyour customersyeah I agree fully with the value indoing that I guess I just like somethingthat I find really interesting and thisis kind of like the angle that we'retaking with our generative feedback loopthing is like when a new blog post comesout or like you know we V8 1.20 comesout when we have this new set offeatures is like going into the CRM andsaying in Target and saying like youknow hey you would like this because youworked on this in the language modeljust does all thatI'm sure someone will build like anawesome likeum llm Native uh CRM I'd love to usethat tool yeahme tooso email and Finn guys thank you so muchfor joining the podcast I think this issuch a great coverage I mean you guysare at The Cutting Edge of putting thedocumentation into these systems I youknow just from I've used it the kaphaand it's awesome so thumbs up on howwell it works and good luck witheverythingthanks appreciate you having us onConnor and a ton of fun", "type": "Video", "name": "Kapa AI with Emil Sorensen and Finn Bauer - Weaviate Podcast #50!", "path": "", "link": "https://www.youtube.com/watch?v=cjAhve_DopY", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}