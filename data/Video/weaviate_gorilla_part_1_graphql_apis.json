{"text": "Hey everyone! Thank you so much for checking out the 1st episode of our Weaviate Gorilla project! We trained LlaMA 7B to write ... \nhey everyone thank you so much for watching movie it on YouTube I'm super excited to present the weeviate gorilla project we fine-tuned the open source llama 7 billion parameter large language model to translate from natural language commands into the weeviate graphql search apis so we're better equipping the large language models to use weeviate by interfacing them with our gorillas and gorillas are these smaller language models that are specialized on a particular set of API through the use of retrieval aware fine tuning so this project also has a nice side effect of making it easier for humans to use weeviate because by translating from natural language commands into levia's graphql apis it lightens the learning curve a little bit from learning how to do you know compositional queries and just knowing all the queries that you're able to execute which is another kind of nice side effect of this project is it serves as a query router there's a lot of interesting discussion about combining SQL queries with Vector search and the gorilla has a nice kind of off-the-shelf effective it takes the natural command and it out to whether the alleviate aggregate query which is how we do symbolic aggregations like SQL style or the gate query as well as these nice trick shots where you can do a vector search and then do symbolic aggregations on the results of the vector search so all sorts of interesting things there I would say with this project if you're curious about the future of llms and Tool use I would kind of say software broadly if you have a set of apis and you want to see how we did it how we use self-instruct prompting to generate a training data set how we fine-tuned llama 7B using hugging faces amazing path library and then substratus orchestrating kubernetes and k-8's training as well as you know visualizing the results and how we're thinking about evaluating this kind of thing so really quick one more thing that we'll get into in the discussion but I want to quickly present in the introduction is the impact that this might have on software Integrations we can imagine a natural language command build a question answering system using llama index and weviate visualized and streamlit and similarly to how you know code interpreter on open AI is able to just you give it a file and say make me a graph of this we're pretty we're getting pretty close to LMS that write code and use specific libraries like we V8 and all these kind of things so I think this is such an exciting project I had so much fun working on this let's dive into it this video explained the weevier gorilla project continuing on the introduction with a bit more of a visual Flair the high level idea is LOM tool use connecting large language models with external tools that allow them to supercharge their capabilities and be more productive generally so for example if we want to answer the question what's the weather like in Boston right now there's no way that we can rely on language modeling the internet or even reinforcing learning from Human feedback to answer a question like this the emerging solution to questions like this is to connect large language models with third-party apis so we have our large language model and then we have some kind of weather API in order for the large language model to talk to the weather API it needs to format its request in the API compatible way so this API may have input arguments like City where it expects a string like Boston Massachusetts maybe it expects a zip code and so you would have the zip code and needs to learn to format the request in this particular syntax so if it expects the string Boston Massachusetts for example and instead the large language model says zero two two one zero the API may be unable to respond as well as processing say the date argument so apis have a particular kind of syntax that they expect and we need to get our large language models to follow this syntax the API example may be how the weather example might be a little simple but you can imagine with weba we have more complex apis than that so the general idea is connecting large language models to all these external tools to supercharge it keep its knowledge up to date as well as to let it execute computation so we connect LMS to calculators or code executors search engines or databases as well as things like weather apis or say your personal calendar as LMS are becoming our co-pilots and assistants in our lives so let's dive into weeviate as a tool for large language models in this project we already assume that we have alleviate instance running with the data schema and data loaded into it and we're learning how to format API requests search requests to this running database later in the discussion we'll discuss the more open-ended idea of letting large language models create new classes populate new data and all those exciting directions so let's say we have some kind of natural language command with a pretty complex weviate query what is the average complexity level of yoga poses that are similar to warrior pose with a maximum distance of 0.15 we translate this into the weva query where we use aggregate our yoga pose class we use Vector search and then we use a symbolic aggregation to get the average complexity of the results from our Vector search so this is a bit of a confusing query that I don't think a lot of new leviate users know you can do this as well as other things like this so the general idea is to let you translate from natural commands into these queries so we need to generate a training data set that takes in these natural commands the API reference for this composition of aggregate Vector Surge and then calculating the mean of the results of a particular property in the wevia class we returned as well as the custom schema for yoga pose which has properties such as this complexity level as well as some vectorized property that's letting us Vector search warrior pose so as this may be already evident making it easier for large language models to use tools also makes it easier for humans to use them so this is a proposal for the auto API where we take a natural language query like show me the full name and email of contacts that contain John and their full name and then under the hood the wev8 graphql gorilla will translate this into the proper graphql query using your schema as well as the API reference and then execute the query also giving you the option to visualize the generated query using the following syntax so we'll talk about the auto API proposal a bit later in the video in addition I think this video will be interesting to anyone out there who's wondering how do I fine-tune an llm for my API so whether you have a set of apis and you want to also facilitate this natural language command to using your software I think you'll be interested in this project so let's dive into the overview of everything we did to create this model here's everything we did to train the wevigate graphql gorilla in a two minute overview we begin with four data sets we have a knowledge base where we have synthetic database schemas we give gbt4 a prompt on what a levia database schema looks like as well as asking it to write at least two text properties at least one integer or number property as well as at least one Boolean property and at least one cross reference from this class to another class we create 50 such toy schemas from Cars to instruments to all sorts of things so then we have our knowledge base of API references this contains all of the apis and we V8 how hybrid search wear Auto cut re-ranking 46 such apis some of which are Atomic for example bm25 only describing bm25 as well as some compositional API references like how to use bm25 in tandem with the wear filter then we have task examples these are manually written examples of how to translate from an API reference and a a custom schema into a new query as well as how to translate from a custom schema API reference and query into a natural language command because we're going to be generating synthetic examples to then train with so the first thing we do is we take the knowledge base of synthetic database schemas and API references as well as the example of how to do this task and we create new queries by looping through all of the database schemas and all of the apis so now we've created 2300 new queries for these synthetic schemas we then use this to create natural language commands for when someone would want to execute these queries so now we have this is the entirety of the self-instruct algorithm now we have these two data sets of queries as well as the natural commands for when we would want to execute these queries we also can use these uh this natural language command set to evaluate retrieval we'll put the natural language command as our query and we'll see if it's able to return the API reference that was used to produce the synthetic natural language command and synthetic query we then take the new queries in the natural language commands and we template this into our fine-tuning data so in the fine-tuning data the input will be the synthetic database schema the retrieved API reference as well as the synthetic natural language command and will predict the synthetic query we use this in substratus orchestrating kubernetes and k8's training also using the hug and face pef library to train our Guerrilla large language model we then evaluate our gorilla large language model by firstly simply asking does it execute and we V8 by looping through the schemas instantiating them in Wii V8 and then even if you don't have any data in weviate if you execute a query if you give it a incorrectly formatted query it will give you an error message otherwise it will just you know give you an empty list of results so we can test quickly if the query executed we also use llm evaluation which is where you give an instruction response paired to gbt4 and you ask it did this response follow the instruction you could similarly try to maybe correct the response by using things like reflection prompting but this is another way to get evaluation metric off the shelf we can also use the perplexity metric where we see uh you know we force the language model to generate the ground truth queries there is a bit of variance in the queries for for some of them that might make this difficult as well as the engram mesh so we'll talk more about evaluation later in the presentation really quickly before we dive further into the details of self-instruct prompting and how we train these models and evaluated them here is the proposal for the auto API so if you have a second I'd really appreciate if you could check out this API let us know if firstly you like the way that this looks if this is something that would be useful to you as well as some of the details of particularly how we would package this if if that makes sense so overall this is the idea of having an auto API where you could just give a natural language query and then you could see the generated query as well as under the hood Auto would translate and then execute the query so there's all sorts of details in this about how exactly we would serve the Wi-Fi Guerrilla model how exactly we will do the retrieval aware inference so if you have a second please leave a thumbs up if you think this would be interesting it really helps us prioritize what to work on so thank you so much for checking this out so diving further into how we train the Eva graphql gorilla we're going to cover four main parts self-instruct data generation fine-tuning llama 7B evaluating the model and then discussion and next steps for the gorilla project so beginning with self-instruct data generation so we start off by creating synthetic schemas for all sorts of fictional webia use cases we do this by prompting gbt4 here is an example of a weeva database schema and then we give it an example of the Json for a manually written schema then we give it information about weva classes and properties just describing how classes are the atomic abstraction for some kind of object and then it has these Associated properties and then some information about properties like how they can be text you specify one text property to vectorize the object as well as you know in properties Boolean and then cross references from classes to other classes such his book has author author being another class so then we prompted could you please design five more fictional schemas for each could you please include at least two text properties at least one inner number property at least one billion property and at least one cross reference for the cross-reference class can you please create that class as well with at least one text property and at least one in property so this lets us then generate synthetic cross-reference classes where say we we search through books then we get the author and then we want to see uh let's say the average let's say you want to do a symbolic query on the author that you've linked this way and you want to say the average number of books they've written or something like that so we can do this kind of thing by adding these properties and making sure that each of these synthetic schemas are able to cover an exhaustive set of then synthetic queries so for this step I currently did it by just manually looking through the schemas to make sure each one was correct because an error here would Cascade severely into the rest of the data the result of prompting gbg4 like this is that we end up with 50 synthetic schemas so for example here we have a book class a description a book from the library configuring the hsw vector index the model that we want to use in this case the hugging phase Transformers to vectorize each book object in our class as well as the properties we have title the text property summary text property page count in is available Boolean and then author the cross reference to the author class so we generate 50 such synthetic use cases of weviate across all sorts of different applications of leviate from music to video games to say clothing travel destinations or even AI models themselves we generate all sorts of synthetic use cases that we can then use to create these synthetic queries and then have training data for our gorilla model so overall it takes about two hours to generate 2 300 queries and this costs about 12 using the um in this case we're sorry we're using gbt4 and and the the GUI to generate the synthetic schemas and then we plug the synthetic schemas into a prompt template with the API references and an example of how to write a query for a custom schema into the prompt so there I'd say the interesting thing here is that there are two knowledge sources the API reference and the custom schema as well as a task example task examples a few shot examples kind of got swept under the rug a little bit with the whole rohf thing but I would say it is Paramount to the success of this project is using examples of tasks can supercharge this kind of prompting so I highly recommend you know whatever you're doing with large language model prompting to add examples of the task you're trying to complete so so then this takes two hours to Loop through the 50 synthetic schemas the 46 apis and generate 2 300 queries costing about 12 using the openai gbt 3.5 16k API so here are some future directions for the data creation so firstly validating the queries so we can you know Loop through each of the schemas load in the Json create the weeviate schema and then execute the query but I think you want to have a way of doing this asynchronously doing this in the background where you generate the queries on one thread and then you are validating the queries somewhere else so it doesn't block the program and slow this down because uh two hours isn't super fast and you don't want to be adding that extra layer in there that could you know blow it up so another way to get more data would be to add the previous example again so you'd say you know here's the previous example of a query generated and also if that did validate that would be a really great query and what that would result in is diversity in the kind of query sent the kind of filters used if you're prompting it to write a custom wear filter as well as which properties it then accesses from the query so another interesting thing is thinking about Atomic and compositional apis so this is quite a deep topic where with weaviate for example you can combine bm25 with wear as well as other features like using underscore additional to get the vector of the object so thinking about whether so we have 46 apis but it's not exactly that's not the combinator works of all the compositional apis that you could do from the set of the atomic apis so we could probably create even more documentation synthetically through generative feedback loops by combining our Atomic apis where appropriate and then creating more documentation on how you might combine all sorts of apis particularly from this perspective of uh gorilla so now that we have our training data set created by self-instruct prompting using the API references synthetic database schemas and task examples we can now fine-tune the Llama 7 billion parameter large language model so to do this we teamed up with substratus AI and particularly Sam stalinga really LED all this I'm going to do my best to kind of explain his work on this so we begin by loading the parameters for hugging faces a language model fine tuner we'll see this later then we load in the model we we already have this model saved in our directory where we've saved you do the Dot from pretrain and you have the Llama 7 billion which is pretty straightforward to figure out how you do that on on hugging face so this is one interesting detail I'm not sure if I have this completely correct but I think what you do is you load the model in 8-bit precision and then I think once it gets into memory then you convert it back to float16 I'm not sure exactly how that if that's correct but so then we load our data set and again our I don't know if some this in the video yet but our this data set is open sourced on WE V8 hugging face so then here's the prompt that we use we put in the instruction and then the response is the completion and this is what you use to pass to hugging faces fine tuner so then we have our model we have the config for our model things like the position embeddings the max length from the model then we add the special padding token with the you know the open square brackets capital letters pad then we tokenize the data set with the max length and then the padding truncating sequences that are too long okay so then we have our data set now we're going to use this incredible PFT Library so one of the most interesting things that are happening in deep learning is sparse fine tuning when we fine-tune something like Lama 7B we don't need to update 7 billion parameters rather we can up update a subset of the parameters and there are quite a few algorithms for what how exactly you update the few parameters Laura low rank adaptation is one way where you do a matrix factorization and you only need to update the eigenvalues of this Matrix factorization so it's a bit tricky but it is one of the most interesting ways of having this you know Vector that's sparse relative to all the parameters as you see we have uh 6.7 billion parameters in Lama and we only need to train 8 million of the parameters by using this sparse fine-tuning thing with Laura which it is interface with hugging face and you know I say this a lot on the podcast and stuff I feel like hugging face has created such an incredible Library also Mosaic ml they've made it so much like they've abstracted so much of this and it works pretty well that I personally don't find myself feeling like I need to invest into unpacking the details of these kind of things but unless that's your thing obviously then then dive into it but anyway so so now that we have our model set up and the training data set now we use the data collator for language modeling so right now with leviate Gorilla we're just language modeling by using that um you when you give it this prompt template uh sorry this prompt template shown here it will only language model the completion part instead of the instruction as well which is kind of a nice effect of this and it'll also do things like um you know back in the day with hugging face language modeling you would just basically concatenate all the text you had into one gigantic text file and it would just randomly sample from that but this is doing it has more stuff now on uh separating each input output example from each other so so then once we train the model you can see our loss curves that we'll visualize in a second okay so as shown in the notebook now I grab that uh the training loss and steps drop that into a text file and give it to open ai's code interpreter which I feel like is a neighbor to our gorilla project here and asked it to visualize this data so we see this learning curve you always have some variants in learning curves but generally we see this decreasing law us and this is quite lazy you know we only have the training loss for now it would be good to show that overfitting curve where you also have the test loss and you see if the test loss is decreasing with the training loss but for now we just wanted to you know get running with having fit a model but we do when we're showing you some results at the end of the uh the presentation as well as the blog post those are from held out schemas and API references and we have done some train test splitting just but we've just manually inspected it instead of having it in our quantitative evaluation Loop so with concluding here are some ideas I have about fine-tuning these kind of models so firstly in a new podcast that we're going to be releasing on Wednesday farshad farabakshian describes this idea of skill versus knowledge and giving the example of a lawyer for how to think about for one class of thinking about fine tuning in this way of thinking you are a lawyer and you have the skill of how to parse these legal documents you know like me personally if you gave me if I had all these legal documents and I was your lawyer you would be in trouble but uh lawyer who's been to law school has fine-tuned in a way to have the skill of reading the the knowledge so you have the retrieval augmented generation to provide the knowledge as well as learning the skill for how to parse the knowledge and I think that's one really strong way of thinking about fine-tuning you've always had this kind of argument around domain-specific knowledge like the idea that you would need to take the gbt3 language while on the internet and then fine-tuning it on medical information that idea has been around forever and I think that's very similar to the lawyer example but now we're seeing this new one of tool use so the exciting thing about tool use and I think one of our interesting research questions here is how much can we compress this gorilla model because the Llama 7 billion parameter model is cheaper to serve than these you know massive 200 billion plus parameter large language models so how far can we compress it to using the Translating natural language commands into the graphql apis and this would make it make the whole thing cheaper more economical and let it do several Generations like unlocking all this kind of like tree of thoughts planning all that kind of stuff okay so now let's dive into some results of our wivier graphql gorilla so taken from the natural language command and again we so we have a few training runs there are a few models if you go to hugging face and you look at the models on substratus a AI these are models that have done the train test split and we're looking at novel API references as well as schemas so what this means is that in this case we have a contact you know like this is like the contact schema and we're generating a new query so in this case just a single wear filter this is the new query that's being shown but in this case we still have we it still has probably seen the wear filter in the training data set but in some kind of compositional API or I think actually there are two API references one for the like operator particularly so that could be the case as well but anyway so it takes the query show me the full name and email of contacts that contain John and their full name and it formats the wevia query using the proper get syntax proper and so here's so there are a couple things to this so firstly it knows the names of all the arguments you know where is how you do the filter path full name being the property the like operator and then it correctly does you know John asterisks for how you would do the full name John and their full name and then it's accessing these properties it's correctly closing the square brackets the curly brackets and all these kinds of details for how you send an API request that will execute on the database here's another example of a more complex query so get me instruments with a hybrid search of piano so cutting results off with the first steep drop in the hybrid search score and showed the name description you're introduced and whether it is a string instrument and the name and genre of the players so now we're doing the composition of hybrid with auto cut so what autocut does as mentioned in the national language description it stops showing search results once they're no longer good according to the slope in Vector distance so we see how it's able to combine hybrid with autocut as well as do this cross reference for the players of the instrument so you know played by name genre so this I think is a great example of how it's combining all sorts of things about weeviate's graphql interface to write these kind of queries from a natural description okay so now let's dive into one of my favorite side effects of the gorilla and this training data set is Gorilla as an off-the-shelf SQL versus Vector search query router so for example when we ask it the natural language command show me the number of courses the gorilla is able to translate it to aggregate course meta count instead of just doing some kind of your search query so it's able to differentiate between Aggregate and get and let you plug in these two different kinds of categories now here is something that I really like is combining Vector search with aggregate queries what is the average complexity level of yoga poses that are similar to warrior pose with a maximum distance of 0.15 what this lets you do is search for the nearest neighbors to warrior pose in the vector space and then do a symbolic aggregation on the data points in that Vector space so I presented this at odsc London in 2022 I love using this for say Twitter analytics it's just something that I use in my life where you have all these tweets and you want to know things like you know what is the average link clicks do people like these kind of tweets and rather than having to categorize your tweets you can just give it a natural language query like tweets about new papers or maybe tweets about new llm papers and it can filter it and then you can do these symbolic queries so I love seeing that gorilla is able to to do this kind of thing from a natural command I'm so so excited about this SQL Vector query router provided from natural language commands Okay so although you've seen some good examples what do bad examples look like now this is a query from gorilla that will not execute because it's missing a comma between bm25 and where so it's the little details like this maybe hallucinating an operator or giving an incorrect say giving a string for Value number or a property that doesn't exist these are the kind of hallucinations that would cause this to fail so let's step a little more into how we plan to graduate our evaluation of these models so firstly there's sort of the does it execute thing that that this query would fail but you still could have cases where the query executes but it doesn't follow the instructions so one interesting strategy for doing this obviously there would be a lot of human annotation that went into projects like this in the past but we could maybe use the higher capacity gpc4 model and prompt it with does the response follow the instructions another interesting idea is reflex collection prompting where you use that kind of reflection to maybe correct the response so it did it follow the instructions no how would you fix it and that might be another way to fix the queries then and get them back into the trending data or something like that or have some kind of sampling mechanism like that a couple other more quantitative metrics we can do ground truth perplexity that's one of the most common metrics you see and I think that works pretty well where you for sorry the language model to Output the ground truth synthetic query and there's also an engram match where this is an interesting idea where for example if we're doing this kind of bm25 wear thing we would see how many of the keywords it matches with get job listing bm25 query so we could do something like this also to see how well it followed the particular API reference so here are some of the research questions we have about our wevia graphql gorilla as well as the future of this project so firstly the most important practical question does weviate graphql gorilla generalize to new schemas as we plug this model into the auto API trained on our data set of fictional schemas is it going to generalize to your schema to measure this where train test splitting our synthetic schemas into train tests but they're into 40 training 10 testing but there are other kind of ideas we can do like controlling the variance of how many text properties whether there's an in-property things like this to add more coverage to our data set to hopefully you can put it in the training data and it corrects it that way but also at least we have some kind of metric some kind of ability to see where it's failing the next interesting question especially with the maintenance of this model is does wva graphql gorilla generalize to new apis let's say the auto introducing the new auto API might be a little too meta but let's say we introduce a re-ranker that takes in symbolic properties to do the re-rankings of something like XG boost like some kind of new search API and we VA will do we need to retrain this model or by using retrieval aware training is it going to be able to just be able to read the new API reference and write the new queries so I think it's quite promising this is one of the biggest appeals of retrieval aware training is by putting the API reference in the input the model is learning to read the API reference and that might help with the maintenance and continual learning of this model the next big question is a bit more it's a bit more academic but it's very practical in this setting is atomic and compositional apis so the reason I say it's academic is this idea of compositional generalization is one of the most interesting things in deep learning that avocado shaped armchair the reason that's so popular is because you're combining the concepts of avocado shape with armchair and then creating this new image in the dolly model's case so in our case a compositional API would be you know combining bm25 with wear with say autocut as well and imagining whether our data set should only be Atomic apis with some examples of the compositionality and what kind of compositionality does it generalize to the next big question is retrieval evaluation this is one of the biggest findings from the original gorilla paper from shashir Patel tianjinzang and collaborators is showing that when they don't have the Oracle context in their fine-tuned Guerrilla model the performance degrades quite severely so how do we get the best retrieval for our wev8 graphql gorilla this has actually been one of the most eye-opening lessons for me in this project is in this case we have 46 apis and I think it's better to classify which API you want to use rather than doing the embedding similarity now the interesting thing with this is scale of course so you know when I only have 46 apis I can easily manage this you know classification data set where I train the classifier to go from the natural language commands to the API but if I scaled this and we imagine say 5000 apis then I imagine retrieval is going to be quite important but it really shows the value of having these symbolic wear filters and how that can help improve relevance in retrieval so I think this is going to be one of the most interesting questions with building these models further the next interesting question related to this is the robustness of the wev8 graphql gorilla 2 noisy retrieval so you know say we try to correct our retrieval by retrieving three results rather than just one is Gorilla still going to be able to parse that out and use the correct reference from the three search results as we saw from the Lost in the middle paper it doesn't look like this is going to be super easy to just correct retrieval Errors By retrieving more results the next big question is the robust business of the Wega graphql gorilla to paraphrases of the net of the natural language commands to then trigger the generation of the graphql so for example our gbt4 our gbt 3.5 sorry that is being prompted to generate the natural language commands it might have a particular kind of style in tone like get me the show me the and then it follows this particular thing of maybe it has the search parameters first followed by the properties it wants to see how robust is this going to be to super casual natural language commands for retrieving from your database or you know increasingly formal requests so we can similarly use self-instruct in gbt4 and so on these large language like the you know the largest large language model 7 billion is also quite large but to have paraphrases of the natural language commands see what happens by either adding that to our training data set or adding that to our testing evaluation so following the presentation of these research questions here's a discussion of how I see this space of ev8 and gorilla generally the gorilla idea of fine-tuning llms to use particular tools so the first thing I'm keeping an eye on is the development of the gorilla open source repository gorilla and API store for llms so I've done a paper summary video diving into the original exploration of gorilla that explores this really interesting thing of formatting apis for deep learning models so in this case the natural language command would be something that indicates whether you want to use an image segmentation model say an image generation model or maybe a you know sentiment classification text model from the hugging face model Hub torch Hub tensorflow Hub and routing this into formatting the correct API request so we V8 firstly is quite related to this where you know if you want to do cement embedding surge which is a deep learning model thing you also need this kind of infrastructure of building up a vector index which makes it a unique kind of model inference problem in this category so my big question with adding alleviate to the API zoo in the gorilla project is how we want to interface the class and the property so a common thing you see in in like laying chain or llama index is you just call the class like document and then you have a text key content and so you just kind of interface those two and then you just interface the uh the vector search but I'll be really interested to see what happens as different kinds of apis are added to this gorilla thing and more on this shortly the next Super interesting thing will be extending we V8 graphql gorilla to leviate python gorilla via JavaScript gorilla we via Java gorilla weba go gorilla so we V8 in addition to this graphql API also has programming language clients so for example to do the same kind of wear filter in Python you can do client.query dot with where and pass it in this python dictionary you can also do client.query.raw and pass in the string of the graphql query is one way of doing this in Python but the thing about python that opens you know opens the whole gorilla project up to probably it's more exciting uh you know General scope is allowing it to create classes to import data to do things like adding cross references or maybe the cloud management stuff like replication configuring multi-tenancy all this kind of stuff because once you open it up to alleviate python gorilla now it has all the levers it needs to control the leviate vector database now here's what I think is the really exciting future of gorillas and I think seeing maybe the python gorilla and kind of the vision of what that could be how it handles creating the schema connecting to the client you know maybe if you're running weeviate embedded it can instantiate all of weaviate but importing data as well as the querying I think this really opens up what we can do with these apis and probably the most interesting thing to me at least is how this will change software integration so imagine a natural language command where you say build a llama index query Engine with weeviate from my notion workspace titled biochemistry chapter four now it will just take that and do everything you need to create this kind of technology for you because the gorillas know how to use the apis to achieve each of these parts now we can imagine level 2 adding to the level 1 prompt please chunk the text with unstructured and vectorize the chunks with open AI embeddings please add a cohere re-ranker and Mosaic ml's MPT 30 billion large language model please create a visualization of the system and streamlit we can maybe imagine also saying please visualize the embeddings using arise AI or nomic AI so there's there's so much we can imagine to adding all the software together through natural language commands so the question to me though of how how do we get there isn't quite clear yet I think there are three General kind of Pathways we have one weeviate maintains one gorilla where we generate all the training data manage the model for all the Integrations that we've particularly you know decided to include in our trading data set so that would mean you know like we have we have coding examples of how to use wevia with llama index and so we've created those synthetic examples and trained our gorilla on it the second idea could be we have interfaces such that deviates Guerrilla talks to llama indexes gorilla so we you know both models know their apis and maybe they would also have to have some kind of orchestrator that knows what goes to leviate and what goes to llama index and then maybe the third thing is Gorilla emerges as an independent third party that manages all the documentation of these uh software tools and the apis and how they connect with each other so I think there definitely is going to need to be some kind of hierarchy some kind of orchestration that knows how to connect these things together as well as the formatting of the particular apis but as we saw from obviate graphql's gorilla's ability to do compositional queries it might be able to just you generate a reference of combining like five different things and it might be able to just do that from one query so I'm super excited about working on the development of this I think this will have a massive impact on how quickly we can test new ideas and I really just think it's continuing this theme of llms that write code and code interpreter but using particular libraries I think that really opens up what this is able to accomplish so thinking about how we VA can control the entire database through maybe the python clients creating classes creating new schemas adding properties you know importing data that might be a little too future looking I think the most immediate thing in front of us is interfacing this weeviate graphql gorilla to Eva users as well as llms through the auto API so if you have a second please check out the GitHub issue it really means a lot and let us know what you think about just thumbs up if this would be useful to you or if you see any holes in the design of it any ideas on this presentation so to conclude with some high level directions for this whole Space of llm tool use I think we're really seeing the evolution of retrieval augmented generation from the original conception from Lewis and others where we have this kind of embedding based retrieval to take context and decompose the problem of language modeling and to retrieve then read we're now seeing this use of search databases as a whole entire tool and I think it's really interesting to read this paper on self-driving database Management Systems quoted a true self-driving database management system automatically one decides what actions to use to optimize itself two decides when to deploy those actions and three learns from those actions all without any human intervention so I think this is quite a bold Vision where we can imagine you know actions to optimize itself whether it's configuring at the lowest level the hnsw parameters like the EF construction the number of neighbors or with product quantization how big of segments to use maybe the Precision for the centroid IDs all these kind of hyper parameters for tuning the search database as well as actions such as creating new classes creating new properties in the case of the generative feedback loops maybe taking your data transforming it into new data by prompting llms and then saving that data back into your database by using these API is interfaced with things like the gorilla algorithm and I think generally just having these databases that are observing the queries that you're seeing from I think the original conception of this was say building index structures like if you're seeing a particular kind of join in an SQL system or a particular kind of filter you might build up and you know cache these kind of indexes that you're seeing all the time but now we're seeing I think a more open-ended interpretation of that where you also could do things like you know create a new class that has a particular kind of data in it and Route queries to that or say if you go deep into hsw there is some research on how you can improve the speed of filtered Vector searches by increasing the connectivity of the graph but generally I think by exposing LMS to these apis we're going to see this really interesting evolution of llms and these search databases thank you so much for watching this presentation of the wevier graphql gorilla to connect with us at webva we have a slack Community where everyone on the team including members of the community are are trying to answer your questions about building semantic search applications it's I really vouch highly for this community I think you'll enjoy joining it and asking any questions you have about Building Systems with wev8 next up is the Twitter LinkedIn weeviate.io if you want to keep up with the new developments of weviate new releases of the software as well as blog posts and research projects like this all these sources will be great for you know keeping up with the information channels of weviate and finally I highly highly recommend checking out the new verba retrieval augmented generation with weeviate this is a full stack demo using you know a react front end as well as the ev8 back end and it's a really exciting new demo project from wev8 so please check that out and thank you so much for watching the weeviate graphql gorilla project ", "type": "Video", "name": "weaviate_gorilla_part_1_graphql_apis", "path": "", "link": "https://www.youtube.com/watch?v=Zqxd1BnoQQQ", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}