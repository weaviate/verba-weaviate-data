{"text": "This podcast debuts a huge new release from Weaviate... the generate module! The generate module is a new API in Weaviate ... \nhey everyone this is a super special episode of the wevia podcast I think we've all seen the power of these large language model Technologies like chat gbt and are curious about how we can use this for our businesses our productivity fun projects and so on uh today we welcome we VA CEO and co-founder Bob Van light with a huge announcement about how we're adding large language model technology to alleviate welcome Bob thanks so much Connor great to be back on the well no we've hit podcast of course awesome so could we Dive Right into how do we use it what is the new generate module yeah sure so what so as you might know it so um for the listeners of course that with it as a modular ecosystem so you can use weak Standalone to add your data or your embeddings however you can also use modules and the first wave of modules that we had were factorizers right so yeah text to hack for example or image to vac from different providers but when also introducing these um generative modules so basically what the generative module does is that it does something with the data in your database and so for example if you have a product stored in your V8 and you're looking for Adidas shoes for the summer then now you can also add a task or prompt for the model where you say Okay present the results as if they were Facebook ads or whatever you want to do with these results or summarize them all together and I think that it's super exciting because if we look at the origin right of how Vector database effective search engines have evolved we started to see this change that the inputs that we're giving as a query we didn't necessarily had to make 100 match on what was stored so for example if we had stored the Eiffel Towers in Paris we could locate it by searching for landmarks and Friends but now we're going to see the same thing for the output so we can actually do something based on the output and what we've stored inside the database so I'm super excited about this and this will just be a module like any other so you can just hook it up to Evite and press button and you're good to go yeah it's amazing I think the whole retrieval augmented language model space is just so exciting I at first can wear this idea with like retrieval augmented generation and then when when I first met you and I saw like oh wow Vector databases they're building the whole database part of it and seeing it because you can just update it with the the new information and it's so interesting to see that so uh could we dive into a little more on how to use it the details of um so I see single result and grouped result uh can you talk more about the design behind that yeah sure and then I'll can talk about a bit about it and then maybe it's good to just Dive Right into a demo because I'll just show also to the you know people watching how it works so what we've done is this if you query vpa to get a bunch of results right so you get like one results or more results um what you can do is we have a parameter that's called single results that will run the prompt over every individual result so for example where we have these um if we have an e-commerce data set where we say show me Adidas shoes for the summer and the prompt would be represent them as Facebook ads for this first product this would be the Facebook ad for the second product that would be the Facebook ad Etc or we can give it a task for all for the grouped results so we can say for example if we let's take with the e-commerce data set if we have reviews we can say okay show me the reviews related to these Adidas shoes that people were there where during the summer but summarize them into one um uh you know make one summarization for all of these reviews so that's the difference between single results or the individual ones or grouped results where we all capture them together um shall we dive into the demo yeah let's do it so let's use our news article data set so first let's look at the titles and summaries that we have stored in this webiate and now let's do a hybrid search so I'm going to go for a hybrid and then we're gonna go for the query Italian cuisine and we'll just limit that to the first result because it makes it a little bit more easy to read so this is pure hybrid search but what we now can do is that we can send the results to the generative model so let's first start with single results that basically means it will send every result with a prompt to the model so we say single result we give it a prompt so for example let's go for um summarize the following in a tweet and now we need to set the property as well so we only want to send the summary of the article and here you see the single result now contains a tweet based on the summary of this article let's try something else so we could say create a Facebook ad about the following and let's do that for a meet up in a Meetup in Amsterdam so we still also send through the summary and here you see we have a single result for a Meetup in Amsterdam or we can even do something like translate the following into well Dutch because the event is in Amsterdam so now we get back the results from the model and as you can see there are in Dutch well if you read Dutch as I do or we could do something like explain the historic element of the following to a five-year-old now note how it still Returns the actual article it does the hybrid search but it then also sends it to in this case the GPT model to produce whatever prompt we've given it we can also do that for full results so let's look at the Publications we have in we V8 so we have a bunch of news Publications and we can do a pure Vector search for example searching for magazines or newspapers about Finance and let's um set a certainty to 75 percent and then we get three results like the financial times the Wall Street Journal and the New York Times company what we can do is that we can also generate a result based on all the results in one so what we're going to do we're gonna go for a grouped result and the grouped result is not getting a prompt but it's getting a task we wanted to do something with all these results combined so let's say explain why these magazines or newspapers are about fashion uh Finance so you see the financial times mostly Journal as I said are all about Finance because so it gives a explanation so that was the demo with the single results and the group results for the generative module this was the demo in graphql but of course you can also use it directly in Python in JavaScript Java go whatever your favorite language is so uh what do you think yeah I mean it's so incredible just like the ability to summarize articles has come such a long way I always thought that was such an ambitious task of deep learning this idea that you could summarize a whole article but and then also Now summarizing search results so you take the top five and you put them all as input into the gbt thing and it's just crazy I think maybe I have a little story to tell like um one of the earlier software projects I was working on it is like this idea of building a travel itinerary so you know you travel and you're saying oh I'm going to Miami what should I do in Miami and this idea that maybe you search for like landmarks in Miami you get the top five and then like in addition to the content where it's like oh I don't remember Miami to I think it's like bisque in Bay uh Lincoln hotels right stuff like this and like in addition to like the description you also have the data from weviate like the location where it is and it can use that to sync up an itinerary that kind of thing so maybe the next topic we can talk about is the templating like using more than one data property from weviate that describes each class to pass to chat gbt so if I'm using the weba podcast I could say speaker said content on date and I can template the results to hand it to chat gbt that way yeah that is a that is a great point and so what's what's exciting about this is that so basically as you've seen in the demo what we're doing is that we're telling in the prompt these are the properties coming from website that you need to base your result on but because of course if we wouldn't use you know the the that in our templates the properties in our templates if we just send an empty request to the model we don't want that but um what we of course aim to do here is to basically more focus on the language understanding of the model than per se on the the knowledge that it has so um not to go too deep into the whole hallucination thing about this more but it's interesting here is that we can say you must base your answer based on this information that we're giving you from the database and if you can't you know find an answer or produce an answer based on what we give you tell us that you can't and I think that is very um that is actually very exciting because here we you know try to take these first steps in trying to solve that problem of like how do we make sure that the model is giving us um an answer that is like not hallucinated and that's actually true based on the data um uh that we that we have and an example that I often use and there was one example that I tried out this very simple example is that if you ask the model like where's the big then right so it's like it's in London but if you store in in vv8 in data object that says um the the big band was in London but it's moved to Paris by truck or by boat yeah that's both I guess by both by both so then um if you don't store it and then you ask at the beginning of the input and we've hit okay the query where's the big pen then it will find the data object that states that the big band originally was in London but it's moved to Paris and that information is fed into the model and now you tell the model you must answer based on what's in that data object and then the model might produce something like hey it was you know previously in London but now it is in Paris and that is how we try to solve that problem um of hallucination I'm looking forward also to what people will be building and templates that people will be creating because um probably a lot to learn um but that's you know that that's where we are right now and the the first results are just well as you've seen in the demo are just super exciting yeah super interesting and I'm really excited to come back to that kind of uh the prompting it to ground its results where you say like these little subtle details of like when you template it like please based on the search results or tell us if you don't know site which search results are the most relevant but one other investigation this template thing that I thought was so interesting is that it can kind of read the Json Keys like when you have a Json dictionary that you hand off from weeviate to charge EBT usually the semantic keys are are pretty good compared but then you also with the template you have these like little language biases like again like speaker said content on date like that little said and on provide like a little more semantic Clues than the key value naming maybe we could talk a little more about that just ability to read Json data and I think you also touched on it but the ability to Output Json data like that yes so so I think first before diving is like also a little story from my side I the I I also tweeted about this that um the first time that I used such a model which was somewhere you know um I mean the first time I used such a model was like quite some time ago because she had like a good letter from Microsoft but it was like you know it was okay-ish but now it becomes like really good so uh when I when I dove back into it again the this thing that you're just communicating with it in natural language and that you automatically start to use words like please Etc is is something that I just that's such a paradigm shift and in my in my mind so that is something that I'm just um yeah that is something I'm super you know excited about to see how that will um uh develop and then on top of that what you just mentioned is um indeed the fact that uh we the first experiment that I did myself was that I just gave it a prompt that I said you're gonna receive adjacent objects from a factor database called leviate and we're looking for X Y or Z and then just just copy paste it in to Json like literally the Json object that comes back and it just parsed it and that worked and I was like whoa and then I was like so what if I go to the take a Next Step here what if I say like okay I'm searching for something and I want to get a result from you and you need to represent it in this Json object and the result should be in the key result but I also created a key action where I said like okay if you want to keep searching you should populate the field action with a search and where this is working as well so now I was like this is super exciting so the thing that we can start to to to do uh Conor and this is really early days but we're creating these feedback loops back into the into the database so to make that very concrete um if I store products in with yet right and I use a vectorizer so or I bring my own vectors doesn't matter right so I have these vectors stored in new event now if it's for example products and I said okay show me all products related to the summer and or to the beach and create represent much Facebook ads then I get that as an output but I could choose to just feed that back into the database and store them now other than the class I don't know ads or something create Factory presentations for those and now we see that the database starts to populate itself with relevant information but also think about you know asking it questions so if somebody writes a very long review for your product you just automatically feedback okay summarize this review or those kind of things there's just so much exciting stuff that we that we see happening um or we can say well I'm we're going to search for um uh uh Sports Products for the summer but group them by different sports and feed them back so there's so much stuff that we can start to do there and the first step in this is releasing that generative model but then of course the next step is like that we start to create these feedback loops and that's just yeah I've never I've never seen that before like coming from a database right so that it really becomes generative itself so that's just you know super exciting yeah I think that's like the the mind-blowing where we're headed just hard to even wrap your head around and I think there's kind of like two things like the ability of it to Output Json data to format its output in a particular kind of syntax whether it's you know write a list of topics and separate each one with an asterisk such that when I'm in Python I can do you know dot split asterisks and now I have the list and things like this but that I can write the Json that it can format its output in an API compatible way to send it to the next thing and then just keep the loop going is unbelievable and so I want to come back to this kind of our follow-up questions needed multi-hop decomposition a little later but for now I want to stay on this idea of writing data back to Eva you you call Chad gbt or whichever large language model and it generates something and it writes back to wevia and also could be like the image models and the video models as this space emerges but it writes it back to leviate and how how do you see these this kind of play evolving like Can Can it have these like billion scale conversations with itself where because also a huge thing of deviated is like the scaling of it and like storing like hundreds of millions of these or billions even of these conversations it has itself and just exploding this kind of latent space of the conversation it has with with itself uh so I mean this is just because this is as you said this is so new what we will see happening is just I I don't know yet but if I just you know um have my imagination going right so for example one of the things that we one of the things that we could think about is for example let's say let's change um use cases right let's say that I'm storing uh documents or web pages or those kind of things on on that large scale that you mentioned and let's say that we want to give it some something additional to rank it right to say okay how important is this today as opposed to something else we could tell the model we can say like okay today maybe it's you know a specific Sports match is important so if you see stuff that's somehow related to that add to the ranking if you just go through it so you keep pumping in the data the model just starts to randomly search for things that might be relevant it might create its own semantic search queries and those kind of things and just you know improve the results after their edit without human intervention those kind of things I could really see happening what you said about like everything multi-model the same thing there because we're now very much focusing on text right text is the big use case most people are using it for text but what do you think about like images audio those kind of things especially when we get these multi-model Solutions I just imagine that you have a um based on what you've stored you're not generating Texas output but like images output you feed that back into the database create a factory presentation for that etc etc so um we just we're scratching the service here and you know we we of course you know work closely together with those building these these models so that we can make sure the infrastructure to support it and to scale it is is there so it's a you know it's a beautiful sanity but it's just it we're just getting started with this and so coming back to the our follow-up questions needed prompt and when you when you you could give it a question and then it could say uh you know it broke it has sub question question and so it can store its path of sub question asking in weaviate and then you can kind of trace it back maybe maybe we could imagine a future where you leave this running overnight and it's like you wake up and it has this like massive yeah so so with this kind of thing of our follow-up questions needed it's this topic of like multi-hop question answering where I'd say uh like did uh did Thomas Edison use a laptop questions like this where you first say when was when did Thomas Edison live when were laptops invented this kind of multi-hop question answering uh can you talk a little more about the thinking on this kind of like recursive our follow-up questions needed yeah so I think the the um the most important thing when it comes to that also in relation to to what we're doing from a database perspective is that what we see Happening Now mostly happens inside the model so it bases that on what's in the models we take the model and we just you know we go through it and I I think what we might talk about is you know later as well as soon approaches what we're doing with the database is that we're basically going to say we still want to you have the power of the model of the language understanding what we're seeing So based on the question for example that you're asking um but we want to feed it with real-time data right and here the fact that it's effective search and you're affected EB helps because the input is uh um you know it's a semantic search you know question so it's not a traditional keyword based question so that's going to be very very powerful so what we want to do is so for example the question that you asked about like um was handed Edison use a laptop that is something that can probably be reasoned from the model itself but let's say that you have a database with you know I don't know contracts from your comp and you purely as the model um did we agree on the contract to do X the model doesn't know but what we can tell the model is okay just try to find that in the database so you start you input that query in the database it feeds that into the model this is the data we have this is the question that we try to answer and then it produces an outcome or it says like search again so that's that feedback loop where we are today with the generative model is just it feeds it in the model and it generates an output but that's just no reason why we can't you know um start to to Loop that back into the model as well and that can happen from the from the wave hit module but we can also do that with you know new projects that now you know come into existence that we can use and leverage and collaborate with as well so I'm super excited about that yeah I'm so happy you brought it back to that topic of the specific data and adapting Chachi bt2 or whichever large language model technology so your particular data because like it inspires things like you know say we want to use we want to try to better understand the weviate code base and this kind of thing that you know the language language model probably hasn't been trained directly on that data but you can supplement it to kind of do this custom data reasoning so maybe it's a big topic here but with that kind of thinking how are you thinking about uh fine-tuning language models or is it just like do you need to fine-tune the language models anymore in your domain or do you think this just kind of retrieval augmented with these prompts and this decomposition of how you maybe you know parse the first five search results and the next five are like all these different kinds of ideas do you think fine-tuning the language models is going to be a big thing in the future yeah so this is this is super interesting right so the um because the the thing is this like basically we see like uh two camps in this and then we see that from a different perspective from uh the academic perspective from the product perspective from the engineering perspective from the design perspective so you know multi-dimensional ways of looking at this uh uh at this but I in my role also in the company I try to sit a little bit in the middle and and what I'm what I'm hoping for and but I think what I'm seeing is that on one hand you have this group saying like Okay we need to just fine tune the model to get the better results on the other hand say like no no we're not going to do that we're just going to use the model it's like these models are not good enough to be very sophisticated in parsing and understanding language we're just going to feed the information and the upside of that is that well you do not have to find you now where that becomes very interesting is that for example if you take the pure academic perspective so and you have to Pure um uh at the benchmarks and and what you know and what we see being tagged as state of the art and those kind of things I very much understand and I would also predict that fine-tuning it and then feeding it information will yield the best results right that that just you know basically intuition makes more sense and and that will happen and we'll see a lot of innovation there but if you look at a product perspective or like a product slash engineering maybe even or maybe more design and product perspective interesting how about wait a second if the large language model is good enough in parsing my question and based on that present the right results why would we go to the effort of extra fine-tuning the model because especially if we have data streaming in with it right so one of the previous podcasts was about the spark connector we're talking about you know uh we're not talking about like hundreds but thousands of like uh documents per second shooting in in fact they're shooting in so and that is something of course that we um uh you know that that is something that we how should I say um we need to somehow optimize for all these different uh uh cases it's a little bit the same as sometimes discussions that you see around like how many dimensions should an embedding have to be you know good or like good enough and then sometimes the answer comes from Academia is different that comes from product or from engineering and somehow somewhere in the middle sits that sweet spot and and so to recap I think that based on these on these two things I think that the um it will be a little bit more top heavy on the right hand side where we say do we really need to find you or that we get these llms like this llm is good at Medical Data this llm is good at engineering data yeah and then even if you have custom language custom nomenclature in your in your data set it will be good enough to parse that and get that feedback cycle going yeah it's incredibly interesting I agree completely with that like the the ability to have like a local memory and like cleverly manipulate the memory with tools like weeviate it makes a lot of sense compared to the fine tune on your custom data but then I also see that space emerging with like uh you know like the legal large language model the PubMed one like the the different specializations but still like a foundation model idea where it's this big model that probably hasn't seen your particular data but it seemed like kind of like like if you're an engineer it seemed like engineering data generally so maybe this is a good transition kind of as we talk about the models kind of like what like right now the first iteration of the generating module is integrating with the open AI models how do you see that space emerging with say the cohere models the Google open sourcing the flan T5 model seems like that's something that's like on the cusp of it so what how do you see this space of the model providers playing out yeah this is so this is super interesting right because the um I think um our our friends at open AI did an amazing job also in in positioning the model and because it just the the the I believe it's a DaVinci 2 model in um but using just as a layer on top the uh the the the the the chat functionality right and and this here and this is what I find super interesting right so we have in this one corner we have the the academic side of things but on the other end of the product side of things and they just did a great job there because they I was literally the other day I was in a bar and and and I explained to the bartender what I you know what it is like oh is it some kind you know related and I mean that's a good sign right so um so they did something great there but the point that I want to want to make with this is like we do not only have that model from open Ai and so we see what Google is doing indeed with flan which is super interesting because they decided to open source that so that means that certain people who just for whatever reason want to use a model that they can control themselves um that becomes available to them but we also see indeed authors who create generated modules right so for example like over here and what we do with all these models um within VCA design we want to support everybody so we just like we just it's up to the users and the customers who decide what they what they want and what they need for their use case we're just going to make as many as we can available so that people can just decide what they want and what I would guess is that if we take this out of the realm of Academia more into product and engineering and design it is um different people have different use cases have different needs right the size of the marbles can play a role West hosted can play a role how you can control the model might play role and the fact that we have this wide variety of you know flavors and choices and what kind of generative model we want to use Same by the way goes from models we use to create embeddings I think that's a beautiful thing so then people can decide you know whatever they want and and again we try to support as many as we can you know with your expertise in business and product development this is a kind of open-ended question but do you think like the cost of large language model inference will Trend to be extremely cheap over time so it well it has to right so there's always this there's this there's this trade-off right so for example um uh let's take the example of um using your own embedding from hugging face versus and hosted embedding let's say that your use case for your use case you're fine using a hosted embedding somewhere then there's this trade-off point right so if I run it myself I have that control but that comes with certain costs I need to pay for the gpus for the infrastructure those kind of things or it might tilt off like it became so cheap to run that somewhere else so we saw like of course these prices they're just going down down down down and it becomes super cheaper and cheaper to actually use these models so that it might tip over for your use case if you say okay now it's just cheap enough right so it's just it's the it comes with that ux inflection point and um that will really really really depend on the use cases that you have because if you're in hospital and your store and patient files and you want to quickly search through these patient files then it it can get as cheap as you want right so but it will not tip over it will you know stick to that other side and then maybe ways that people scale these models and work with them and how um Cloud providers interact with them might be more interesting for those kind of use cases so there's always this inflection point that has a combination of price and you ux you know so if it's um if the ux becomes if it's so easy for us it's so difficult to run one of the other it might just dip over or it becomes so cheap then it might tip over but sometimes it never tips off it's like sorry we just need to do it the hard way because that is for our use case important and that is why we also see um uh companies like I don't know like like Ray or something right you know we help you run these models which is you know great because there will be enough use cases I mean there will be so many use cases in the world that there's like enough for both worlds and also I think these embedding providers um they struggle of course with the fact that they somehow need to scale the effort of running these these models and um and but that's how it's that's how it's solves right so that's like the market just determines if it goes left or or right and that is the big difference between that academic side and the product side so that's what I mean when I say like good enough so I was like okay you know maybe on benchmarks XYZ in this article it says that a works better than b but B is so much easier to run and so cheap and I get the results I want so why not go for B and that is just that's just age-old wisdom coming from the market you know how it could operate so um well I hope that answers your question oh yes this is models yeah incredible I think that argument plays a lot into the like the fine-tuning discussion of earlier like right now it's I think like open AI coher they offer a fine-tuning option and it's sort of like you give them your data I haven't used it myself so I can't speak from experience but I think the model is like you give them your data they fine tune the model then you just pay a little more for inference compared to where like for you to fine-tune flan T5 on your data where you might need to deal with like distributed GPU training for most people it might just that tip that scale might just be like that's too much effort I can't be bothered with this a minute and then on the other end is the the inference side which is another extremely interesting part of it and yeah like like you need to host say like a 4 GPU inference for your very large model and so I think this is a good transition also to talk a little more about the weavate module system and how it helps you host the model so there's two options where you can use the open AI or the cohere models and there's also the kind of do-it-yourself in wevia you have the module can you describe kind of the web module system a little more yeah so I can and and let me before I do it let me explain a bit where it's coming from right so why why we have this so um if you if you have to just weave yet as just a um a database that you can use to store data objects with in fact I embeddings attached to it um we've seen like people need to get these embeddings from somewhere right and I'm a big believer in the um the innovators early adopters uh early majority kind of you know that that's used in the in in crossing the chasm that model right and what we see is that these innovators these what these folks have in common you also find them on Twitter they're smart right they know how this stuff works they know how to operate it they know how to get these ampedics and those kind of things but there are also people that you might find when you talk at a conference when we do alleviate meetups or they have you they go like hey this is amazing but I don't know how to run such a model that's just maybe a little bit too complex or those kind of things so that's where the modular ecosystem um that's where that idea came from that we say like what if we just make it easier for people that if they want they can pick a module they don't have to they can that for example takes care of factorization or that takes care of anything else like for example degenerative module you don't have to use it but you can use it if you want and how it works is actually pretty simple so we've hit itself so the database that we've had database is 100 Standalone so if you just run the database you will see that you just run one container at the least right and of course scale it up that you know can become more containers but just for the sake of argument choose one container that we create that's the thing that you find out get up that's completely standalone and then you can say I enable a module and sometimes the modules are built in into Wi-Fi so for example when you interact with the openai endpoints with the cohere endpoints vpa takes care of throttling and those kind of things and handling the errors Etc so that it doesn't do one request after the other but multiple ones Etc but sometimes modules are a little bit more complex so for example if you want to use the modules from uh hugging face and you want to run them yourself then you see that the second container will pop up one containing the model and inside the Vivid module for gpuf to GPU if you like and then we fit next to it so it did it just that it runs um next to each other and the idea again there is very simple just to make it as easy as possible for people to work with these models because not everybody knows how to do that and the upside of running a model as opposed to a database is that the model is stateless so you just if you just one more and faster you just have like a lot of models running parallel the problem is that that's crazy expensive right it's really really expensive um so helping people solve those kind of things that is what we aim to achieve with these um with these modules so you see this database in the center and then this collection of modules around it that you can use if you want but you don't have to um uh it is brilliant I yeah it's so exciting the web module system I think it adds so much to the vector database you have of course like the hsw product quantization the approximate nearest neighbor part and you know the as we're adding the hybrid search with the bm25 indexing and then all the database functionality like replication you have all that but then you also want to have like the things you're going to need to use it and I think the example in addition to the generate module D like the vectorizers is a really great example like when you have a query coming in you need to vectorize it to access the vector index and use it and yeah it's just it's so interesting hearing about that module system in the design uh that one other thing you mentioned was about the container design and I think that is just brilliant engineering maybe a little biased in saying that because but I really find that to be so brilliant like your weeviate instance could have a billion data objects and it needs to be this big computer but then your text vectorizer is just like a smaller thing because it's just vectorizing the queries and it all depends on like the trade-offs of the use cases and this kind of scaling out of different containers I think it's such a massive part that it adds to the search experience particularly and I think as we've it evolves and the whole AI first databases this separation is so interesting so now another topic that I think is extremely exciting is so we've talked to the web module system is like what should live and weviate how can it live in weeviate and interact with that Vector index but now let's kind of talk about some of these large language model orchestration tools this kind of category of yes yeah exactly that large language funnel orchestration so uh so things like Lang chain gbt index they kind of have and I think it's similar to this earlier thing about neural search Frameworks like as we saw Gina Ai and Haystack I think it's a similar kind of conversation so how are you thinking about this kind of topic yeah so I'm I'm super excited about it because I remember I think I'm not sure but I think I saw actually I saw GPT index before Lang Chang I think and then and I believe the GP index uses bank change so I was like I saw that first and then uh the second one and that was just that made immediate sense to me so I also internally you know I said like as you know of course like you know we need to help these people because it's going to be awesome if you also integrate this with the database and um it this just makes sense right so this has to do this might be a nice piece of the puzzle which feeding that data back into for example the database where you ask basic basically such an orchestration to like can you help me to figure out if I should add something to the database do I need to get something from the database and those kind of things and mostly we see that currently these orchestration tools work only with the model so what we discussed earlier there said get something fundamental but it would be of course amazing and that's something we already see happening because we see the alleviate integration with these tools right then we can say okay we now can integrate um um the database as well that you can see like as the model do you know this or should I get it from the database or you might be at some point you can orchestrate it to say like you know it must come from database first and then reason over it right so we can all these all these directions and we need orchestration for that so that makes just that makes a lot of sense and again so we are in the business of uh you know building a database technology to to to to scale working with your which are you know your embeddings and your data objects together and then everything that we can work together with and adopt the uh the the Frameworks you mentioned the um orchestration tools you mentioned the embedding providers that you mentioned Etc that is amazing it's like a new ecosystem that's starting to emerge and I mean I wrote a year ago maybe less than a year ago I wrote an article about this ecosystem I call it the AI first ecosystem and uh and yes we can debate about mlai the terminology but I just wanted to address a broader group of people right and um and that's now missing something the orchestration is not not in there so that then you see how quickly that is like evolving and makes so much sense so a long story short I'm excited about it the reason I'm excited about is because I think it will help in creating these feedback loops and those kind of things yeah the feedback loop thing I think is really well designed with Lang chain and I think because link chain and GPT indexes I understand them are two slightly different ideas as well as having a bit of overlapping functionality but yeah another thing about Langston that excites me so much with the wevia database is like asking the language model okay you have these classes like let's say I I'm creating a knowledge base of all my weeviate information I have the weeviate documentation I have the weba podcast I have the blogs and like I just have like stack Overflow questions so on and so I say like how do you want to just what do you want to search through and then maybe you can pick the class it can add wear filters like uh like here are some of the guests on the Louis vuit podcast which one do you want to search through I think that kind of orchestration of the Wii V8 uh how to search through the weviate is an extremely interesting thing as well uh yeah so maybe uh I think that's kind of a good topic a good covering of these kind of orchestration tools and I will get more into that later on with our content and uh so maybe it's kind of wrapping up the podcast and I think even it's a very open-ended question and even just a summary of the topics I think would be a great answer but like this kind of open-ended question of like what is the future of the wev8 generate module and how are you thinking about this yeah so what I'm really excited about and this is something that I I just it's so funny right so if you if you see it that you have like this huge Epiphany that I've always been thinking about working with these models on input right so that they can be that resolve the problem of just not having a hundred percent keyword based search but then we can um that we can have semantic questions that we can kind of do image search and those kind of things that was like I saw this beautiful uniqueness coming from a vector search engine or effective database but what we added to that is that not only the input in the database but also the output so they were basically saying like we're going to give you relevant information coming from the database um but that's not per se stored inside the database just that's that's new I mean it's like I just just think about like one of the most used databases in the world right so like postgres or MySQL those kind of databases right it only outputs what's in there right which makes sense because let's use it but now we have this thing working you can do that if you want that's fine you can do that but also it can give you information give your data that's generated based on a task or prompt that you're giving it and having databases that take this information make sense of it on input time and generate relevant new content if that's something you want as user I think that's a that's that's amazing and that's just getting started I mean we should do this podcast like a half a year from now again and then just see how it evolved and what's happening there because this is just too exciting man yeah we should have like a predictions that we revisit on the podcast yeah well awesome thank you so much Bob I think yeah the search experience is having a makeover with this large language model extension onto the end of it uh the whole discussion of the generate module demo of how to use it the discussion of the argument single result grouped result templating and the prompting all these things to think about and then uh think about the prompting like grounded generation prompts uh our follow-up questions needed cite your sources tell me you don't know and then I'm so excited about writing data back to Eva the language model orchestration and the module system all this stuff so thanks so much Bob yeah thanks for having me here Connor it's always great you know to share with the you know the wider world what we're working on so thanks ", "type": "Video", "name": "Bob van Luijt on Generative Search with Weaviate - Weaviate Podcast #35", "path": "", "link": "https://www.youtube.com/watch?v=ro3ln4A9N8w", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}