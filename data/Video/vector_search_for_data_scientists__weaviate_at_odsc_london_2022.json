{"text": "How can the latest advances in Vector Search help Data Scientists? As a Machine Learning content creator for a few years, I have ... \nhey everyone i'm really excited to be presenting our talk at the latest open data science conference in london as a part of our vector search track i was presenting vector search for data scientists i really hope you find the talk interesting and a huge thank you to the open data science conference committee and team for all sorts of things from guiding me through how to use the webinar software and generally accepting me to be a speaker at the conference thank you so much this was such an exciting opportunity and to everyone out there listening i really hope you find this talk interesting this presentation will explore the use of vector search for data scientists including a case study with twitter analytics so we'll begin the presentation by looking at how i'm defining data science vector search and twitter analytics case study before diving into the details of linking these things together so what are some common questions in data science we typically have some kind of thing that we're observing like the number of impressions that we're getting for our tweets or say views for youtube videos and we want to look at questions like how is my data distributed is it normally distributed power law distributed are there particular outliers in my data and the third question that i really want everyone to focus on with this talk is the question of are my variables correlated with each other and trying to explore relationships between our variables and seeing how they impact the distribution of the thing that we're the most interested in so say we're most interested in the distribution of impressions on our tweets we also want to look at variables like what time the tweet was sent or whether the tweet contains a url to try to see if that can give us some insight about the distribution of impressions on tweets in vector search we're looking at a new way of search where we represent objects with vector representations extracted from deep learning models so the key questions are how well can we capture the semantics in vector representations and what can we learn about our data from the semantic clusters that are formed by these vector representations so this tying these together data science and vector search is going to be done with the example of twitter analytics i've been making content on youtube and twitter for a while now and i've been really interested in this kind of problem of trying to figure out the best way to do this messaging how to really kind of you know promote the this content and make it successful so i'm looking at my own twitter analytics data to try to get a sense of if i can segment it using vector search to get a better sense of the distribution of the metrics i care about like impressions or url clicks and see if there's any insights i can gain from applying vector search for data science and understanding twitter performance so if you're interested in performing this analysis yourself you can easily go to the right top right corner of the twitter analytics dashboard to download the csv files you can use the calendar icon to download up to i believe four months of data and then you can just uh you know as this presentation will explain you can easily upload it into wev8 to enable these vector search functionalities so the twitter analytics it'll give you the csv data file where you have different columns like the raw content of the tweet itself as well as some features like what time the tweet was sent and the metrics that were interested in like impressions engagements engagement rate retweets replies likes user profile clicks or url clicks and the interesting one the most interesting detail about applying vector search to this is understanding this idea of having semantic representations from the raw unstructured text itself rather than approaches like feature engineering where maybe we try to extract features like whether the tweet contains an emoji what the character count is word count or if it contains certain keyword phrases like weaviate or deep learning and see what kind of segmentation we can do with those kinds of features that we manually engineer so with that primer on how i'm looking at data science how i'm defining vector search and overall looking at this twitter analytics problem this presentation is segmented into five key sections the first section is segmentation and data science understanding how we split distributions based on other attributes the second key takeaway is to understand vector representations of data how do deep learning models produce vector representations of real world objects that capture the semantics in them the third key topic is vector segmentation using these vector representations to segment our data the fourth key takeaway is this particular case study of using wev8 for twitter analytics and applying the intersection of the ideas and vector segmentation for twitter analytics using the wev8 vector search database the fifth key takeaway are research questions and discussions about continuing this kind of research and expanding on the capability of vector search and its applications in things that data scientists care about the first key idea is to understand the task of segmentation of analytics and data science so in data science we typically have a distribution of values of something that we care about now for the example of twitter analytics i'm interested in the number of impressions that each of these tweets receive so hopefully i can gain insights into what topics to tweet about what particular language and features like that that might lead to tweets that receive more impressions so following the csv export of the tweets i plot my impressions in a histogram and i can see the distribution of the impressions so as i look at this i see most of the tweets have you know in this kind of mid-level of impressions and then we see some that go into having many impressions although there are fewer of them so you know we're just plotting the the histogram bins of the number of impressions and then the count of how many tweets fall into each of these buckets so when we have a distribution of this the task of segmentation is to try to take apart this visualization to see how it varies with respect to splitting it on certain features so as examples of these features that we're splitting we might ask questions like what time was the tweet sent say we sent a tweet in the middle of the night does that result in very low impressions or say we you know sent our tweet right at 10 in the morning on monday does that result in a successful tweet then we might ask is there a url link in the tweet does maybe trying to point people off of twitter is that you know causing the twitter algorithm to not promote our tweet or something like that and that'll be our transition from understanding these splitting on these symbolic attributes compared to these vector attributes that are extracted from deep learning models for segmentation so the first question i asked is what time was the tweet sent so from the tweets i can extract this feature of tweets sent before 12 plot the impressions of those tweets and then i can look at the impressions on tweets sent after 12. and kind of one of the problems with this is you see how the the count is they're much more tweets sent after 12 and before 12 it's difficult to kind of automatically extract some insight from that question similarly we have the question of is there a url link in the tweet and looking also at the distribution between these two values so this example of splitting our data based on the symbolic attributes that are contained in the data these kind of attributes like the time or extracting a boolean value from the url clicks in this presentation we're trying to see if we can split impressions or whatever metric we care about based on the semantics of the content itself and expand on segmentation from symbolic attributes to vector representations of the high dimensional objects that the metrics are describing so the raw text of the tweet itself or as we'll also look at when describing other examples say movie transcriptions podcast transcriptions uh text and scientific paper or images of e-commerce products this kind of data that we can form vector representations from and then split our analytics based on the clusters in this vector space so for example without manually annotating the tweets we can segment them based on their vector distance to the phrases we've a podcast we vva tutorial and ai weekly update to split the tweets based on what they're about with with respect to these kinds of categories without actually manually labeling them we can similarly do this for say natural language processing computer vision or say robotics we can use these kinds of distances to these query points to automatically form categories and clusters that we can use to split the impressions of the of the tweets we perform segmentation to split the metric that we care about like impressions on tweets or say clicks on an ad and now what we want to do is see if we can segment these analytics based on the semantics of these high dimensional data objects like raw text images code even audio videos say graph structure biological sequences and really anything can be encoded into a vector representation like this and then we form clusters of the vectors to segment our analytics and see how these clusters vary with respect to the metrics like impressions or views and so on to summarize the first pillar of the presentation is about understanding segmentation and data science we visualize the distribution of our data to get a sense of it so for example we see that the impressions on these tweets are roughly normally distributed and then we ask questions with respect to the features that we have like is this distribution also the case for tweets that are sent at say three in the morning but now what we're looking at trying to do is say what about tweets related to the the topic of deep learning for robotics and we're not going to determine that a tweet is about deep learning for robotics based on keyword matching with say it contains the exact phrase robotics or deep learning for robotics we're going to try to encode tweets into a vector space as well as the phrase deep learning for robotics and use the nearest neighbor search to determine tweets that have the semantic similarity with deep learning for robotics and then use this to get a sense of the twitter impressions about on tweets that are about that particular topic of deep learning for robotics or similarly say deep learning for biology or all these different kinds of topics that i've been exploring as i've been making content about topics in deep learning the second key idea is to understand how we can represent real world objects with vector representations so we'll start by kind of looking at different ways of encoding things so to say so looking at symbols compared to vectors so some common symbols are categories so say we have labels about vehicles and we have ships trucks cars motorcycles and we use a one index to denote which category this particular thing is so say this one indicates that this is a an example of a car vehicle so this is an example of how we represent categories typically in these one hot encoded vectors similarly we have variables like numeric values like how old someone is or you know these metrics are numeric values like impressions these numeric values we can use thresholds like greater than less than intervals to to segment our data and get senses of it based on these numeric attributes and similarly we have boolean values true or false vectors are say n dimensional objects where we have values in each index of the vector that that represents something so this 0.1 0.8 0.34 this is a vector that you would say it say has a direction a magnitude but usually we normalize these vectors so they all have a uniform magnitude but it's really kind of the direction that is captured that it's hard to kind of reason about direction of these high dimensional vectors but that's sort of what we're looking at when we're looking at comparing the distances between these vectors as we'll explain further in the presentation so the key idea behind vector search is we're using these vectors to represent high dimensional data objects that are otherwise really difficult to kind of compress into a single representation and having this kind of flexibility with this n-dimensional vector interface lets us store the semantics of the high-dimensional objects so say we take pictures like these high resolution images of golden retriever puppies and some computer vision model is say gonna there are two things that generally we're looking at maybe the computer vision model is going to classify whether this is a dog or a cat and before it makes that final binary prediction it's going to have this latent representation of the dog at say layer 8 of the deep neural network and then we're then we're going to extract this vector representation to represent the puppy image overall and store it in our vector search database like what we v8 is but another way of doing this it's becoming more and more popular is to directly optimize for the vector representation itself so as we'll explore further in in this particular segment of the talk we're looking at predicting these vector representations for different images and trying to make say positive pairs like these two golden retriever puppies similar to each other and say dissimilar to other images like a brand new car image or something like that that would be different from these golden retriever puppies so we'll return to the details of exactly how deep neural networks are producing vector representations but let's stay a little further on what we're doing with these vectors once we have the vectors so say we have some deep learning model that maps from images to vectors once we have these vectors we can use vector distance to determine semantic similarity so a common vector distance metric is l2 distance you just loop through each index in the vector and then you just square the difference of the two sum that up and that's the l2 distance so say we have these three dimensional vectors that describe this puppy this puppy and then let's pretend that there's an image of an airplane so we would take the l2 distances between the two puppies and we see you know 4 minus 2 squared is 4 plus 1 squared 1 plus 1 squared 1 is 6 and then we'd also do the vector distance between the first puppy and the airplane so the vector distance between the vector 4 8 10 and 1 20 20 is a much higher number than six so we determined that the first puppy is much more semantically similar to puppy two than the airplane based on this vector distance calculation and there are a few different distance metrics that we can look at as we kind of mentioned direction and vectors and say cosine distance angular distance there are a few ways of looking at distances with vectors but generally it has this kind of form where you're comparing indexes of the vector with each other so with these kind of kinds of vector distances we can determine semantic similarity but we can also cluster the vectors using say common clustering algorithms like uh we could do things like k-means if it was in a lower already in a low dimensional space but commonly we do things like tsne pca umap projections from the 300 dimensions say into this three-dimensional space to get a visualization of of our cluster so we see how we have the chicken the image of the chicken the wolf the dog the cat they're all kind of similar in the vector space whereas they're dissimilar from say this google and apple logo or actual apple and then banana so we see how we have these clusters in the semantic space and so this is just the key idea to this presentation is understanding that we take a bunch of tweets from my twitter analytics data and then we put them into this embedding space such that we have say the tweets about computer vision topics the tweets about natural language processing topics and then we can further look at and then we take the cluster out extract the impressions data url clicks then we have a new interface for looking at the data science of seeing how these metrics are distributed before graduating from this slide it may be worthwhile to stay a little more on this idea of why the talk is titled vector search for data science say rather than vector clustering for data science so when we have these clusters it might not really be useful to just look at the cluster and imagine it's a cluster of like 500 objects or something like that we want to kind of have semantic names for the clusters so say we had done the search query animals and then animals takes us to this cluster of nearest neighbors that interface for segmenting the analytics is i think more productive than just trying to just take the clusters and say do the k-means or you just have these clusters that are closer to these centroid points and then just doing your analytics from there i think it's more useful to be remembering that vector search part of it where you have animals brands or fruit and using those search query points to provide like a semantic title to the cluster so to dig into this idea of vector representations of objects a little further you might be interested in how these index positions influence the representation of these objects can we say that say index zero solely determines how much of a brand if we're saying these are this is the brand clustered is say index zero of this vector is the density of that particular index determine how much this is that and the the thing about this is it's a pretty interesting kind of field of research and deep learning this idea of say disentangled representation learning where we can say what each of these uh index positions and codes and there's been some interesting research multimodal neurons from open ai really stands out as an interesting exploration into these latent representations from deep learning models but generally we can't just convert it from the vector representation right back into symbolic representations of what each index represents rather we kind of have this uh entanglement of the whole representation and so we need to kind of rely on these vector distance calculations rather than kind of extracting is this index say how much of a fruit it is and so on so another interesting question maybe is how much can we compress these vectors and in my view one of the most interesting things that we've done at we've eat is look at this binary passage retrieval idea where you can compress the values in the indexes from say 32 floating point values to binary values which is shown with this green being one white being zero to just kind of illustrate a binary vector and surprisingly you can still get impressive retrieval when you do this level of compression and there are other ideas like product quantization and say locality sensitive hashing that try to say compress the 384 part of it to 32d and one of the most rewarding experiences i've had at wv8 was to interview eddie and dillocker on his work in a n benchmarks and benchmarking vector search at large scales and seeing how computing these vector distance calculations using efficient data structures how that scales so if you're interested in these kinds of questions like the performance of vector search and exact numbers for that with respect to how many vectors you have the dimensionality of the vectors and then kind of high level ideas like the distributions of the vectors themselves you might be interested in checking out our webva podcast so i hope that was a pretty clear overview of what we do with these vectors so we have these vectors that are particular kind of way of representing something we convert from high dimensional unstructured data into vector representations we have say vector distance calculations that form semantic clusters we explore what's in these vectors and ask questions about how much we can compress them and how much these how much the performance varies with respect to the attributes of our data set and the vectors themselves so now let's dive into the question of how did we get these vectors to begin with so this is a very influential paper sentence sentence embeddings using siamese birt networks that explains this idea of having siamese twin tower networks where we have a copy of the same neural network that produces representation u and represent rotation b uh both of these bird neural networks share the same weights commonly there are some frameworks where it's an exponential moving average and it's things like that but generally it's a siamese copy of the same network and they both produce vector representations of two different inputs so sentence a goes through this copy of the network and sentence b goes through this other copy of the network and now we compare the vector distance between the representation u and the representation v and we have this loss function back propagate into the neural network weights of burt on both sides to update the neural networks to produce better predictions for these two sentence a and sentence b so the key thing is then determining well should sentence a and sentence b are they positive pairs or negative pairs should they be close in the vector distance space so to give more of an example of this we sample these points from data sets like wikipedia so often what we're doing is we're looking at heuristics to self-supervise the a and b positive or a and b negative scoring such that we can do this representation alignment from internet scale data and a lot of these as we get into this idea of zero shot pre-trained models they're trained on internet scale data such that they have this amazing generalization property so we start off with things like this query point so here's an example of a paragraph from wikipedia that describes the miami heat mba basketball team now what we determine is that the positive pair is the next paragraph in the wikipedia article so now we led by dwayne wade and following a trade for former nba mvp shaquille o'neal this next paragraph continues to describe the miami heat basketball team so we'd want to take these two paragraphs from wikipedia and then encode them into a into vectors that minimize the semantic distance between these two a vector distance between these two and we also have negatives like this article about deep learning so we would want to have a close semantic representation of these two paragraphs about the miami heat and then a dissimilar representation of this paragraph about deep learning in addition to this idea of using heuristic labeling like saying that the following paragraph is going to be a positive pair of the wikipedia article we also look at this noising or masking interface to form positive pairs so data deveck is a very influential framework showing how we can do this positive pair construction for really any kind of data domain because this idea of applying a noise mass can generalize to any kind of data that you might have so for example we take an image we apply this mask to it we have a speech file audio file and we apply this to it language mask out with and and then try to align these two representations so a couple of ways of looking at how we form these sentence a sentence be image a image b video a video b any kind of thing for these two things that we're trying to encode into the vector spaces so that's how we construct the optimization task to optimize the weights and thus optimize the vector representations of data so as you're looking at this you might be intimidated by this idea of do we need to train our own models are we going to need to optimize a model with these contrastive learning loss functions in order to take advantage of vector search for data science and what's the one of the most exciting trends behind this whole thing is the answer to that is increasingly looking like no and there are many pre-trained models that work well for a very broad range of data so from being trained on wikipedia or say query passage pairs in the ms marco data set we're seeing that models trained on data sets like that are able to generalize and produce reasonable representations for say my twitter data and things like that so if you're interested in this sentence transformers from hugging face is an excellent place to get started on looking at these models that produce vector representations off the shelf for all kinds of data domains to summarize the second key takeaway from this presentation is to understand vector representations of data starting with what the vectors are themselves and understanding that we can take all sorts of things like images text or code snippets and map them into vectors with predictions from deep learning models those deep learning models are trained to maximize semantic similarity between uh between each of the positive pairs and they're usually trained on massive collections of data like all of wikipedia or going even further for internet scale data like like on that kind of order and because they're trained on such a massive amount of data we often don't need to train the models ourselves for particular data domains to have a reasonable zero shot performance or a adaptation to our particular problem the third key takeaway of the presentation is vector segmentation which brings together these ideas of segmentation and data science where we split the metric that we care about based on other attributes that we might have like what time we send the tweet or whether the tweet has a url link in it and also this idea of vector representations of data so to tying these ideas together we saw how we can produce vector representations of text images code audio video all sorts of data can be produced into one of these vector representations through the inference of a deep learning model and then we connected it to this twitter example where say we want to split out the tweets based on the distance to we va podcast vva tutorial or ai weekly update to get a sense of how these things cluster and how they're semantically related so in this section i'd like to maybe just quickly describe some additional examples in addition to the tweet categories that i think hopefully will just kind of cement this idea of how we can apply vector representations to segment analytics and apply vector search in data science so we'll start with everyone's favorite task of house hunting houses in databases like these real estate platforms they typically have symbols like the number of bedrooms a number of bathrooms square feet these would be numeric values and then say we have cities that we might represent as a categorical feature or something like that so we have you know these kinds of symbols that we could split the houses say we want to split the prices of houses and we want to use these attributes like the number of bedrooms the number of bathrooms to split the distribution of the prices of houses with these vector representations we can have vectors that encode the visual aesthetic of the house say the neighborhood structure and this is in that idea of graph structured representations which kind of at the bottom of the list this graph structured embeddings you see things like graph neural networks or algorithms like deep walk and node to vect this idea of converting graph structure into vector representations as well is also something we can we can do and is becoming increasingly easier to do as well so we can encode the graph structure of the neighborhoods and then we also just have a more flexible interface to define features with text so compared to symbols where say we have a feature that is a pool does it have a pool one or zero boolean value whereas with text we can just kind of describe the features of the house in this flexible interface and then encode it in the vector representation that can encode all these things and then segment things like the prices of houses based on visual style or neighborhood characteristics that help us segment the houses by going deeper into meaning and semantics of what makes makes up these real-world objects of houses so similarly with e-commerce products we have symbols like what kind of e-commerce product it is like if it's apparel particularly we might have shoes t-shirt pants or say colors whereas with vectors we could have a representation of the visual style of the products themselves with movies we also have genre symbols like children action or sci-fi where vectors can encode say take a description of the movie maybe reviews of the movie to encode things like the themes characters story lines these particular kinds of nuances that make movies different from one another within genres and without requiring heavy manual symbolic tagging in scientific papers i think is one of the most interesting things to explore with this as well where say symbols separate papers like that are related to biology or machine learning whereas vectors can really encode the nuance of the ideas contained in the papers and maybe the writing style as well and then finally with music i think is another very interesting thing where we have these high level tags like hip hop or dance but vectors can encode sort of the musical style and i'm not an expert on music but things like this could be encoded with vector representations using this kind of way of thinking so hopefully these examples helped further clarify this idea of how vector representations what vector representations can help us capture in our data to segment it and better understand it and hopefully going through this list maybe made it more clear how to apply this to your own data sets so we'll end this section on vector segmentation with the following quote from francois chile in his book the second edition of deep learning with python that's the magic of deep learning turning meaning into vectors then into geometric spaces and then incrementally learning complex geometric transformations that map one space to another all you need are spaces of sufficiently high dimensionality in order to capture the full scope of the relationships found in the original data so there's a little more to this quote than just turning meaning into vectors describing how we have several layers of this kind of tensor space processing and these non-linear transformations between each space to the next space as you pass through a sequential neural network but the key idea in what we're interested in is this idea of turning meaning into vectors and the magic of deep learning and being able to do so and do so for so many different data modalities and use cases to summarize section 3 brings together the ideas of the first takeaway in segmentation and data science and the second takeaway of understanding vector representations of data so vector representations which are also the also commonly called embeddings enable an interface to split analytics based on the semantics of the content itself and this content could be text images code audio videos it's really up to your imagination to thinking about what you're going to encode in these vector spaces so now that we've gone through the background of what segmentation and data science is and how we can use vector representations to facilitate with segmentation i'm super excited to show this example of twitter analytics mining with the we v8 vector search engine and how we can use vector segmentation and twitter analytics through we v8 so as shown previously with the export csv twitter analytics gives you a csv data set like this where you get the the text of the tweets themselves as well as the time the tweet was sent and then the metrics like impressions engagements and so on so in this example we're going to be using the text column to vectorize it with a sentence transformer and then we're going to use this for nearest neighbor search to segment the content of our tweets such as whether it's about the ebay podcast coding tutorials or topics like natural language processing computer vision maybe even research niches like self-supervised learning or data augmentation and so on to get a sense of how the semantics of this different content segments the impressions or url clicks and these kinds of ideas so following this schema that you see in this picture this is how we upload this into wev8 through python so in python we create this dictionary where we have classes and then we name our class tweet we describe the class tweet analytics and then we list the properties of the tweet and we'll get in further to this graph data model of we've eight and more of what you can do with this but here's the the very basic of defining the property that has the text of the tweet and then say you have the author which is another text and you would add the impressions engagements so on as you fill out this dictionary changing the data type to say number or boolean and so on and telling wva not to bother with vectorizing uh text columns like author be as we'll see later in the example we're going to add tweets not just for myself but other people not to jump ahead but we want to tell we've a to only vectorize this particular column and we do that with this skip parameter in the text event transformers module so here's a quick overview of the schema in the python and uh so what i did and i think this is really interesting is go from google collab notebooks to the weev8 cloud service and there are many other ways to upload data into wev8 but i think this for data scientists is so accessible the way that you have these this jupyter notebook interface hosted on google collab and then putting data into the vva cloud service all of this can just be done with very little real uh or challenging skills with respect to the engineering of setting up all these different cloud services we can just kind of with a few clicks go from google collab to the vva cloud service to host this example so with that said with the twitter data hosted on the vga cloud service let's jump into the wev8 console and do some graphql queries on my tweets the screen is showing the we v8 console that has my twitter data in it one useful way to quickly get a sense of this is to click on the schema button and see the schema of the data that this that this console is currently pointing to so we see how we have the tweet text property we have the author of the tweet we have what hour the tweet was sent we have all these properties that describe each tweet in the data set so the first query we're going to do is to do the the highlight of the thing the semantic nearest neighbor search to concepts like natural language processing or computer vision so to do this we use the get syntax we passed in his argument the tweet class and then to this we pass in the argument of the near text search so so in your text we pass as argument the particular query that we want to make so in this case and i'm sorry that you're watching me kind of remember how to do this but in this argument we pass in what we want to search for so say natural language processing and to kind of jump around in the tutorial a little bit the next thing is going to be getting tweets other than just my own so in order to just search for my tweets from now we're going to also add this where filter where we add this argument path author being the property that we want to have this symbolic filter on with our return results operator equal and then value text my twitter handle which is c short and 30. so from there we have our filter on the tweets and then we tell it which uh attributes of the tweets we want to see in the return list so let's see the tweet text itself to sanity check this thing then we'll get the impressions and then let's also say see url click so so this forms our uh query in graphql in the vva console so when we click search it's going to find the nearest neighbors to natural language processing that i've authored and then and then return also the tweet text itself the number of impressions and the url clicks on the tweet so we send that query and uh so we see the top result is stack overflow down really looking forward to language models that can answer questions about python so pretty related and i guess so kind of looking through this i think maybe the most interesting thing to quickly focus on would be to understand that it doesn't have to have the exact keyword match natural language processing it can still kind of capture the semantics of that even if it doesn't directly ask for that so to also kind of just search through this we can try a wee va podcast and see what's returned in this case a lot of the tweets do say we va podcasts directly maybe we could do just deep learning broadly but anyways i think from this you get a sense of you can decide what you want to return from the properties of the thing of the class you have this near text filter where you pass in the argument through this interface and you can add additional filters this way in addition to the we vvade console we can also wrap up the graphql queries in a python syntax to send these queries in python then access the nearest neighbors in python to produce the visualizations of the impression splits based on say we've a podcast we've a tutorials an ai weekly update or also say natural language processing computer vision again the same kind of idea so this is the syntax of looking at how we can wrap up this graphql query into a python uh api so we have the dictionary we have the concepts query we do the the get query we pass in the class name tweet the attributes we want to see tweet text impressions then we execute the search and then we index the return data object so from that return data object we can put it into say seaborn matplotlib to visualize a distribution of impressions based on these different filters achieving this idea of segmenting our analytics based on what the tweet was actually about so following this analysis of segmenting tweets based on semantic content and distance to queries like we a podcast we vva tutorials or topics again natural language processing computer vision here are some additional questions that i'm asking about exploring twitter analytics with we v8 and seeing what else we can do and i'll continue this discussion in the fifth section of this talk under research questions and discussions so one question i wanted to ask is this concept of have i tweeted something like this before and trying to have some kind of say pre-flight checklist before sending a tweet to get a sense of how tweets like this have performed in the past so uh this is just showing putting the exact same tweet into the semantic space and seeing the uh return nearest neighbors just to get a quick sanity check of what this kind of thing looks like but probably more interestingly than just have i tweeted something like this before might be to ask you know who in my say network or in my particular industry like machine learning has tweeted something like this uh recently so in order to explore this question i aggregated a list of people who've come on the wvva podcast going through the we vva podcast guests and going through their twitter usernames to hit the tweepy api and add their recent tweets into this database to have this semantic search to see what all these people are tweeting about and see if they have similar interests to the thing that i'm interested in right now so the first step in this was compiling the list of vva podcast guest twitter usernames then hitting the tweepy api like this using the keys you get from twitter api and then grabbing 100 of the most recent tweets from our from our uh i think roughly 14 or 15 we gave a podcast guest so another interesting thing with we've yet is i can use this same schema just populating the tweet text the author and the likes even though i don't have say the impressions or url clicks data i can still just populate these properties and add it into our pretty flexible data schema so now that we've added these using the additional twitter users let's see some graphql queries with semantic search so we're back in the we evade console and this time we're going to be querying all of the we vva podcast guests to see if anyone has been tweeting about the particular topic of generative art so in it's the same exact query from before we just have the get filter tweet and then from the tweet we want to get the tweet text and let's say the author and then we add our near text filter to it like this where we have concepts and then generative art cool so this is how we form our query and then we send it and now we're searching through all the authors on the eva podcast and seeing if they've been tweeting about this concept of generative art so we see han zhao has been tweeting a bit generative art is a creative process we see a lot of say exact keyword matching we see some ai generated artwork so it isn't exactly identical to our key phrase but it still pro props it up to one of the top results we see bob tweeting uh generative models like dolly for music are around the corner so generative music art so anyway so this is the idea where we can you know have these semantic searches we can add a bunch of authors from twit twitter users and use this in order to do the semantic search about what people are tweeting about which i think could be a really interesting tool and i'm really excited to continue exploring this application if you're interested in playing with weaviate yourself and sending some queries out we have a live demo of wikipedia that you can explore so before diving into the console itself with the wikipedia data let's look at the schema for this wikipedia example so there's one key thing that i've left out of the twitter tutorial that is one of the most exciting parts of wva generally which is the graph like data model so in the wikipedia example we have articles and then we have paragraphs of the articles in the articles as two separate classes so the article has the property of the title of the article say miami heat to reference the example from earlier and then each paragraph in the article is represented as a separate class that has the title of the article i believe and then the content in the paragraph itself and then the order that it appears in the wikipedia article so for example coming back to the positive sampling with the miami heat articles this would be the first order because this is the first paragraph in this article about the miami heat and then this is the second paragraph in that particular article so this allows us to create these relations have named relations between objects and also we see this recursive relation where say the the miami heat article links to the nba and then we have this recursive relation back to the mba article so i'm going to get into more on this in a little bit about how we could expand the twitter example to have this graph like data model and introduce some additional classes but let's get into the wikipedia example to have a quick sense of this in action so here we are in the uva console for the wikipedia example we can check the schema see the classes we have the article a wikipedia article with a title and cross references and then we also have the paragraph which is the wikipedia paragraph and within that we have the content which is the thing that is vectorized with the transformers so this is an automatically populated query from the web link that's in the description of this video to check out this wikipedia example and so we're introducing a couple of new things from the twitter example so firstly in addition to the near text filter there's also a question answering filter in we've eight and i think it's kind of out of the scope of this topic of this talk to completely dive into what question answering is but basically what it's going to do is it's going to retrieve the nearest neighbor content to the question and then it's going to classify using a supervised learning model classify the answer to this question within the content but the other thing that we see is the cross reference between the paragraph and then in article and then the title so this is how we do that cross reference to answer the question within wikipedia of who was stanley kubrick so to recap wev88 is a vector search database rather than a library such as facebook's face or annoy from spotify this means that wev8 has the approximate nearest neighbor search for doing vector search with an absolutely enormous collection of vectors but it also has database functionality like create read update delete support and lets you safely secure and persist your vectors which is pretty important when you have a massive amount of vectors and then we saw how we va has a graph like data model we saw the example with wikipedia of how you can use the graph data model to organize data in this way we can imagine building on the twitter example by say also linking these images by having a has image thing and it's generally such an interesting property of a way of combining say multimodal text image data but also kind of heterogeneous data say we have a tweet and it links to an article then we have an article which is also text but we have this other way of representing that article object so if you want to get started with wev8 i highly recommend checking out the quick start guide under we v8 if you want to get started running with the demo data set and understanding how to set this up and eventually graduating to uploading your own data sets into we vva so to summarize the fourth key section on using we've a for twitter analytics we see how returning to the whole theme of the talk we can segment the impressions on twitter based on the content of the tweet without manual labeling we can do it with these vector representations that we get from a pre-trained sentence transformer that hasn't been say optimized in my tweets or really twitter in general and then further we saw wev8 a vector search database and we saw how we can use it to store and search through semantic vector embeddings of data the fifth section presents some research questions we're exploring and a discussion around this project so here are three important research questions and general directions that we're exploring that will help with projects like this and this general idea of vector search and then also their applications and things like segmentation as outlined in the presentation so the first key question is should i fine-tune my embedding model or my model that produces the vector representations so early in the presentation i i claim that no you don't need to you can use these pre-trained sentence transformer models but this is still a pretty active area of research we do see a very impressive zero shot generalization from these models and it probably can give you a pretty good retrieval from your data but there are all sorts of things that are in the works uh say sparse fine-tuning that let you fine-tune more efficiently the continued development of these off-the-shelf models of course and then say hybrid approaches where we combine this vector model with say the keyword bm25 tf idf those kinds of features is pretty particular to text that particular approach but this kind of fusion of features is is seeing a lot of interest in how we produce the retrieval list and combining the features from the vectors and then also this bm25 algorithm so there is a very active line of research and just understanding contrastive learning generally i don't think contrastive learning is a it's kind of seems like a newer thing that's emerging say in computer vision we had sim clr moco and people started really taking that show on the road to text as well and we're seeing things like sentence transformers so i still think there's a lot of opportunities to explore how we train these models that are trained to produce vector representations the second key thing is this idea around large scale vector search and how we form the the how we do this vector search at the say billion scales imagine we have billions of tweets in our database and that's where these things called approximate nearest neighbor a n algorithms come into play and i think this is a really fascinating area of research to study particularly if you're interested in things like data structures and computer science and then finally the key topic that i really want to emphasize and is i think very relevant to this particular project of twitter analytics is how does vector search differ from classification or regression models and this idea of retrieval augmented say learning so let's start with this idea of vector search versus regression on impressions so imagine we have a model where you type in a tweet and then it will use your data to just predict how many impressions this tweet would receive i mean if if it has high accuracy you'd be like well sure this this is useful i suppose and then you tweak your craft your tweet modify some phrases until you can get this number to be as high as possible but instead of just having this blind regression model we might want to have a vector search model where again we put in our tweet and then we get the nearest neighbors to tweets we've sent before to give us this sense of what is this tweet i'm about to send similar to but what retrieval augmented classification is about is combining both of these things so the model is going to retrieve the nearest neighbor tweets in the input as it makes a prediction on how many impressions this individual tweet will receive then we have options where we could say perturb the input to have that interpretability and see how a perceived change in nearest neighbors would change the impression prediction and overall it just gives us more of an interface to see what is influencing this prediction why is it predicting this amount of repression impressions rather than just this kind of blind input output mapping so to continue on this discussion this talk was inspired by the twitter analytics data and trying to see what kinds of insights we can mine from it and how these new tools in vector search can help us gain additional insights from our twitter analytics data as well as maybe also connecting to the broader scope of what everyone's tweeting about and because you can still access say the like retweet count which can also help you get a sense of performance with tweeting about these kinds of topics so generally we want to ask questions like you know should i post this to begin with would this be you know would this just be a bad tweet to post and save yourself the embarrassment sort of when you do that kind of thing and just generally thinking well maybe it's a good tweet but there's just a particular time to post it coming back to the symbolic questions but then kind of this entanglement of the tweet itself what time is best to post this particular tweet and then you know the phrasing it's this entanglement between the symbolic attributes of the time to post it as well as the kind of the content itself if you want to dig deeper into that and then we want to maybe expand from individual analysis to team analysis and say you know how are aggregating the data from our entire team is anyone tweeted like something like this recently who on our team would be best fit to tell the story and what topics should we be tweeting about is say a lot of these um you know a lot of these content strategies have a few different topics that they could explore and should it may be time to double down on a particular thing like say fine-tuning retrieval models or a n benchmarks all these topics which ones should we be tweeting about so these are some of the questions that i was seeking to explore in this twitter project and i hope you found this interesting so to summarize the general idea is how can we improve these systems and what looks promising with respect to the particular ideas of improving these systems say ideas like fine tuning more approximate nearest neighbor research to understand how we can do this at larger scales and this general idea of retrieval augmented modeling to summarize his talk on vector search for data scientists we first looked at the idea of segmentation and data science how we can split metrics like impressions based on values of different attributes like what time the tweet was posted or whether it has a url in it in order to see the distributions and see the differences and distributions based on these values then we saw this idea of vector representations of data and how we can represent say text images and code with vectors and then in the third section on vector segmentation hopefully the additional examples made it a little more clear how we can combine these ideas of vector representations and then uh segmenting our analytics based on the the content of the data itself finally we looked at the we've ate example for twitter analytics i really hope you found this interesting and then finally we explored some research questions and discussion around this idea of vector search for data science as well as this application for social media analytics if you're interested in we've aid and these ideas around vector search please connect with us on the we vvate slack channel or our youtube channel we vv8 vector search engine on the youtube channel as well as spotify i'm the host of the we v8 podcast where we have a lot of interesting guests who are doing things in research around vector search or building applications with wev8 and i really enjoy these conversations so i hope you do as well and finally you can check us out on twitter at wev8.io finally i would like to give a special thank you to sebastian woodaleck in advising and counseling the development of this presentation as well as svetlana smelly nova for the help with the visual styling i'm so grateful for this help and putting this together and it really means a lot to me and in addition to everyone currently watching this video thank you so much for watching the presentation i really hope it made a compelling argument about vector surge for data science and this idea of vector segmentation and hopefully this twitter analytics project was somewhat of an interesting way to understand this concept so thanks again ", "type": "Video", "name": "vector_search_for_data_scientists__weaviate_at_odsc_london_2022", "path": "", "link": "https://www.youtube.com/watch?v=IRWHa57T-zk", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}