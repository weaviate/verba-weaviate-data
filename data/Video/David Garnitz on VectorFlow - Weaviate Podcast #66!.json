{"text": "Hey everyone! Thank you so much for watching the 66th Weaviate Podcast with David Garnitz, the creator of VectorFlow! \nhey everyone thank you so much for watching the 67th weave podcast with David garnets the creator of vectorflow really quick before diving into it I wanted to thank David for joining the podcast and please show some love to vectorflow on GitHub leave it a star if you don't mind open sourcing is always awesome and there are so many exciting ideas in the space of at the highest level how do we get data into Vector databases like we say you take you know text for example text has all these things like you know how are you going to chunk the text metadata extraction being an interesting topic then doing the embedding model inference and then you know with weave we've explored things like the spark connectors or grpc apis just you know like how do you get the vectors then into the vector database and so I had so much fun with this conversation I hope that you know you enjoy listening to the podcast and please check out the open source GitHub repository uh Linked In the description of this video D Garnet vectorflow hey everyone thank you so much for watching another episode of the weevier podcast I'm super excited to welcome David garnets the founder of Vector flow this is a super exciting new company that's looking at innovating on how data gets into Vector databases like we've yet the whole importing data data ingestion so we have so many exciting topics there's such an end-to-end stack with this firstly David thank you so much for joining the podcast yeah thank you so much for having me excited to be here so could we kick this off with uh you know what inspired you to create Vector flow and sort of how do you see the problems in the space yeah so that's a good question so like to just rewind a little bit on my background like I was a machine learning researcher in grad school and I worked on some developer tools and infrastructure at companies like SpaceX and affirm and ended up going to work for a pretty early stage AI company called file read and at file read we actually had to build like a massively parallelized Vector embedding pipeline because of the volume of data we were ingesting um and so I saw that pain Point uh with the team there and realized that there wasn't really any good solution in the market and decided to um go off and build it myself yeah amazing I remember like talking to you earlier about like load testing and and that kind of thing and yeah it's definitely something we see at weave it is like you know people who put just a crazy large amount of vectors into it and you know like whether it's you know billions of vectors and this kind of thing can you maybe talk a little more about um yeah just how you see the opportunity like with you know like using Vector databases with billions of vectors I think there's kind of like two sides of this like you could be you know someone who's hacking on your personal note-taking app and you probably don't deal with you probably have like you know 100 000 or so but or you're looking at these big Enterprise cases where you have just enormous amounts of data yeah absolutely so I mean we we personally think at Vector flow that um you know these these systems these retrieval systems are going to be ubiquitous right because every large company has an incredible volume of data that they can leverage both for external customer facing products but also internal products to save money and move faster um but obviously the space is is Young and a lot of the Technologies need to mature and so there's a big challenge there in terms of ingesting data at scale putting it in the store but also understanding which data is relevant so like internal there's a lot of old kind of stale data internally these large organizations too so you have not just a challenge around getting that in but figuring out which data to draw from yeah definitely I think um maybe if I could yeah this kind of old stale data there's I mean yeah there are so many components to the data importing into Vector databases kind of conversation I like this like you know maybe the llm knows which class so like maybe this is people who are watching this podcast probably already know that wevia uses the class abstraction to kind of like you know isolate some type of data you'd have like one vector index per class so I like this idea that llms maybe know like which class to put data in like if you're importing from multiple sources like a common thing that people are building is like chat with your documentation where you might be importing from GitHub YouTube say slack and maybe you route these to different classes as well so that kind of I see that as one thing as well as the you know the stale data kind of like um you know refreshes handing document update handling document updates as being another kind of part of this but um so okay so with our podcast I think it'd be really nice if we could just kind of come to an agreement on this end-to-end and what data importing looks like so I think there's kind of like connecting to sources is one thing like you know PDFs notion readers Google Drive readers then there's kind of like um I see this kind of cluster of things where there's like text chunking strategies like there's a lot of kind of naive text Splitters like the you know just rolling window chunking stuff like this there's also like maybe using llms that a language model would look at a text Chunk and be prompted like please split this up where you see semantic gaps and then it would do the chunking that way there's also kind of like this metadata extraction so it's kind of like that layer and then I see once you have the chunks you now pass this to the model inference to turn it into a vector and there's certainly a lot of innovation there and then I guess finally the last leg of the mod um the last leg of the mile is I don't know if that's the phrase but is then plugging in those vectors into Eva so is that kind of like a good uh storytelling of kind of like the end to end what it takes to import data into Vector databases yeah I mean I think that's a good high level story I think one thing that we've also encountered in our research and discussions with people and one of the things that we're looking at um actively building into Vector flow is also this idea of like pre-processing the data before you turn it into vectors so not just metadata like metadata I think is it's useful for different kinds of hybrid search right but there's trade-offs associated with using metadata like it can make the upload speed slower or maybe you have to use like a different retrieval algorithm so what we've heard some people do is to actually pre-process the data to put relevant information into the chunks um or even uploading it to the vector DB or before adding any metadata or anything like that so that's another stage that I think is kind of emerging it's pretty immature um curious to see though or hear like on the weviate side like what are what are y'all kind of encountering in terms of like usage of metadata and pre-processing and things like that yeah well I I think this is such an interesting topic that kind of take the metadata out to put it into the chunks that just kind of build on that a little bit more I see that as like um you know if you're parsing like a legal contract and it's like you know clause s3001 and then you would take that header and you'd kind of put that into the text Chunk like you know this is a part of this clause and then you hope that that Vector embedding is going to be better it's also probably going to be better for the bm25 scoring when you're then saying I don't remember the number like s3001 something like that like that kind of adding uh metadata into the chunk for kind of vector search but there's a few other ways of looking at this and I guess another idea like what I like a lot and what I think is kind of underrated is let's say I'm parsing Out Wikipedia pages of NBA basketball players right and I would and I come across like age like you know Jimmy Butler is 28 years old I don't know how old Jimmy so like I would take that out and put that as metadata so then if I want to ask a question like um you know it could be the semantic search kind of question where I have like a video of someone playing basketball and I'm like who does this guy play like who is older than 32 just a random question or it could be like an SQL style query where I say what is the average age of players on the heat so it's like I see this kind of metadata extraction thing as being quite important I had a bit of a disagreement with nose rhymers on a podcast where he was thinking that uh you know all this metadata can be kind of captured in the embedding in this kind of thinking and you know I so yeah so let me just pass it back to you to get kind of what you think of this kind of metadata extraction from text chunks yeah it's certainly an interesting approach I mean so I think the thing that we're trying to understand is like where do you how do you differentiate between putting things like say at the end of the chunks right so you talked about age like you could you could literally just put the age of Say Jimmy Butler at the end of every chunk about Jimmy Butler right but then are you outing the chunk um you know I think the benefit right is that like you can use a more standard kind of like a n algorithm to retrieve the vectors and then you can maybe have your re-ranking pipeline or whatever to kind of like get a high fidelity result whereas like with the hybrid search you have to potentially Implement something slower or you need to store the data in a different type of graph or something right um so there's different trade-offs um but what we're hearing is that in either scenario these things are really important to improve uh the Fidelity the results and that's kind of like the broader theme I feel like that that we're encountering I'm curious to hear if um you guys are hearing the same thing but for us it's it's a lot of like we we like the the systems they're good like they're easy to use but how do we get the results as good as possible and so that's where we've been trying to understand where in the industry are the best practices solidifying because it's it's still such a nascent field I agree 100 with that I think evaluation is nascent like absolutely like um you know evaluating how well your search is performing I feel like is more of an academic exercise and something you see I mean like I do think it's picking up though especially like um just yesterday we hosted a workshop with arise Ai and a rise AI they're building like you know like a visualization observability tool and so you kind of inspect like you know the vector spaces of the nearest neighbors to your prompts and sort of starting to get some evaluation metrics like I I that we could definitely dive into this thing about evaluation it's that's another one of these major rabbit holes but so okay so but staying coming back to the data ingestion kind of topic um so yeah metadata and I think we can definitely come back to that if we want to but I I really want to talk more about uh kind of like vectorization like once you have the chunks and now you're trying to like manage inference of like a hundred million I don't know a random amount of vectors but like of text chunks and you're trying to vectorize them all and get them into you know or just get the vectors to deviates then do the deviates half of the indexing part how do you see that space of like how can we vectorize this as quickly as possible yeah I mean I think that's where a lot of infrastructure comes in right where like you really need to um build out a massively parallelized Pipeline and that's kind of where we come into play where we enable pretty massive scalability as well as configurability leading up to what actually gets vectorized um or embedded um but the you you need to spin up like a message queue or you need to stream some in from an S3 bucket and you need not just to be able to do the calculations in a way that's that's quick and efficient but also cost effective but you also need to make sure that you're not dropping anything you know that you have a retry mechanism built in and I think that's where you get into some of these engineering challenges um and this is where like you know my work at SpaceX and affirm is kind of helpful in understanding kind of how to build out this infrastructure um and it's part of why we we took the approach that we took at Vector flow where we said well we're going to be more of an infrastructure first product so if you look at if anyone goes to our open source repo they'll see you know Docker images to spin up um you know a message queue in a database and making sure that you can get all the information you need to in the database if you're in the vector databases I think one of the one of the challenges that people don't realize is an issue until they start to operate at scale I'm actually curious um what you guys are hearing in terms of like something something that we we hear people say a lot in discussions is um yeah we thought this would be super easy when we were playing around with it and then we have a follow-up conversation with someone two or four weeks later and they're like oh we actually tried to turn it on to production we realized like oh we we didn't think through a lot of things yeah well I guess for me it's like um using the open AI or cohere embeddings apis I think that tends to work for most people like they they really enjoy just kind of sending their text to the apis getting the vectors back and however like batching is handled within openai or cohere is however that's handled but yeah I do think once you then are trying to like you know you've maybe fine-tuned a sentence Transformers model or you just want to deploy one of the models from hugging faces sentence Transformers just to try to save costs now you open up a huge can of worms of potential potential like problems to solve like you you now become interested in like you know the whole inference acceleration camp with things like the Onyx run time or maybe all the stuff neural magic is doing with like sparsity quantization of Weights just so much stuff there and as well as kind of like the the load balancing between uh you know different vectorization endpoints you might have as well as maybe um oh maybe you're dealing with like Ray actors and you're using gpus for these inferences and so no you've added like another layer of thinking about this so yeah maybe so so it sounds like to me like you're maybe more interested in in that kind of Latter half of you have some kind of vectorization model let's you know let's unpack the box and let's try to optimize all these parts of it yeah absolutely and I think um so what we've heard is that when you really want High Fidelity results you actually do need to use open source models because there's a lot more Innovation happening um in that space for specifically for embeddings like I think for the large language models themselves the the closed Source actors are still probably um in terms of usability probably the best products but I know um from customer conversations I know from my work experience at file read that those open source embeddings models actually do especially for certain use cases give you really high fidelity and when you are talking about moving a system to production really getting kind of the promised productivity gains and and things like that you want to do every single thing you can to make the results better which is you know why you're tweaking different steps along the the pipeline right so we talked about metadata extraction and chunking and this is another piece where like you can you can take your open source model you can fine tune it to continue to improve the results um it's definitely challenging though to use them in in certain ways like some of them are really big right like um I think it's like the extra instructor XL or something you need like 24 gigabytes of vram or like you know just just not that many machines out there that can do that um that are cost effective so you run into these different infrastructure bottlenecks um but I think we're going to continue to see that become more of a norm yeah I'm really curious about um I guess one question I have is like we just released our like text-to-back gbt for all where it's using the ggml acceleration like there's like llama C plus plus I'm sure everyone's aware of running llms on C plus uh you know on like CPUs with C plus and stuff and so I'm kind of curious like is like what is what is the state of that like can you really run it on a CPU instead of a GPU I yeah that's kind of one thing I mean I I don't know if it would be worthwhile diving too into it I think the topic that you brought up that would be more interesting to talk further about would be that um that fine-tuning your open source embedding model kind of thing okay I know uh Gina AI they're doing some awesome work with their fine tuner and I'm just curious like what you're seeing on um you know how common is it to fine-tune an open source text vectorization or you know any any image vectorization any kind of model I don't think it's that common yet like we've talked to a handful of people that have done it and the theme that we've heard is like the the complexity is actually more than you would think because like from what I can tell fine-tuning an open AI model is not super hard because they have kind of a nice interface for it they have like your notebook out but I think when you want to work with an open source model it's more challenging in certain ways um I think also it's it's not totally clear yet what what levers to pull in the space in terms of like what's actually going to make your results better so I think I read that with fine tuning you have to you need 10 instances of something in the data for it to actually kind of appear in the model weights and so if you if you don't really have well curated data to fine-tune on then it's not going to have that big of an impact whereas like you know altering the chunks or the metadata could potentially have like a way more outsized uh impact on your search results I think another theme in general though that is just evaluating right how do you actually even you know figure out what search results are good or bad for your data and your system um I think is a general challenge yeah well I think what's so exciting about the well I I've talked about this all the time because I love this topic which is like the um you know you can use the language model to generate synthetic queries for your documents and then use that as you know jointly your find e you'd use it as your fine tuning data for embeddings or you could train like a ranking model where the ranking models are like classifiers to take and query and document as candidate and it's slower but it's high higher Precision you know or you could use that kind of data set to evaluate different embedding models like you know now you have a synthetic test set so you can say okay open AI cohere sentence Transformers compute for my favor and you could do that kind of thing so yeah what do you think about that kind of you know llms to generate synthetic queries to measure search performance I mean I think it's definitely an interesting idea I've also like played around with this myself like I also like in more traditional machine learning tasks have played around with using llms to label data basically generate synthetic label data I think where you run into a little bit of a of a question mark in my view is that like no two kind of production grade data sets have the exact same statistical qualities and so how do you know that something is good for your data set if you're using an llm to generate something synthetic because also like llms I mean depending on the settings right they're they're inherently stochastic and so like you're not necessarily always going to get the same results in and out so you sort of feel like to really start to get something that's meaningful in terms of a test result you also have to run things through an llm a lot I'm curious to hear what you think of this idea like one of the things that we've talked about with people is if you know your data well you can actually rather than use the L1 to come up with the questions you can come up with your own questions and have stuff as a yes or no answer you could use the llm to grade it or you could grade it yourself and then you have a pipeline and you run stuff through the pipeline a bunch of times to kind of cover that stochastic variability and so like let's say you have 20 questions and it's like yes you know or they're right or wrong you can have a score out of you know 20 or convert it to 100 right and then you can do that like 10 or however many times and you can kind of give an average and you have a variance and all you do is you just you just toggle one variable for each test and then you can really isolate for my data how effective is tweaking the top you know K and top K results for example or swapping the embedding model or changing the the filtering mechanism on the map the metadata that's how we've kind of thought about it but I'm curious because you guys are you're so deep in the space too like maybe you have a totally different approach well I agree completely I mean it's like the the the large language models just kind of like erased the the favor like the popularity of like extractive question answering models where it classifies the answer in the retrieved context so there's like no chance of hallucination with that setup or you know just having the search route you to frequently asked questions and then it's like you know the embedding similarity of the new question with the uh the embedding of the frequently Asked question and then you know Neva had published this um awesome blog post about like uh question to question embedding similarity is a lot easier than a question to answer because I kind of like the style and yeah I think there's a lot of interesting things too you know that kind of component for sure and yeah it's it's like a I mean there there's so much Innovation happening on uh like what the search box looks like like um you know I think by the time we publish this podcast I I don't know if I should say this because then I like have to have the podcast publish after this but let me just not say it so like we're working on there is a project in development at alleviate that's like uh you know building those UI components for the search box with the site sources and like you know the UI for it there's like you know a lot of collaborations happening with like versel and replit uh you know I'm personally not involved with this but I'm watching this of like you know trying to make it easy for people to like plug in front-end components and there's so much like you know whether you're using extractive QA or whether you're putting it to a large language model like how you do the user interface is definitely a big opportunity you know like how we're talking about Vector flow and uh ingesting data being a new market category developing these kind of like front-end components like if you have like a library of react components that's also a pretty nice little you know as everything is developing further yeah one of the things I was going to say we're actually noticing the same thing too where it's like with Vector flow we're even trying to understand what is what is the entry point to the system for example like you know we started out with an API uh as the entry point to the system because that's um it's technology agnostic right vectorflow is designed to be Cloud agnostic too but we're also seeing like if you want to rapidly test and you want to iterate like you know you probably do just want a UI to configure stuff and so I think that's in some ways a sign of the maturing of the space where like it started out right with a lot of people that are research scientists data scientists traditional software engineers and now it's as it matures and it enters the mainstream we have to start moving to thinking more about usability and approachability for the average person to kind of get the most out of these systems understandability so I think there's a lot of opportunities in that um the vector flow is is really useful for running rap testing Cycles I think the whole industry collectively still has a lot of work to do around understandability why do you have results that you have um and kind of you know having you know Tracer I think they call it Tracer bullets right in software engineering ways to understand what Pathway to the answer did it take how can I repeat this and yeah things like that yeah no I agree 100 with that like the the ability of the zero shot models to evangelize building with deep learning tools for like software Engineers has just been the probably one of the biggest themes in the story of wevia in my view is like being able to just take the open Ai embeddings and then you can build a semantic Search application without needing to have the slightest idea in the world about how you would fine-tune an embeddings model which I think is quite a difficult task and so this is kind of like I'm doing like a long range reference back to uh the metadata extraction and kind of uh chunking with L I really want to kind of get for like this kind of um you know importing data into Vector databases I think the interesting thing is I think there's even more opportunity for zero shot models to be plugged into this workflow and particularly I'm curious what you think about the idea of like an llm that's been trained zero so obviously like gpt4 could probably do this task of where you give it that Wikipedia passage of Jimmy Butler and you say hey what kind of like symbolic metadata attributes are contained in this passage that you might want to extract and would go oh you know age 28 let me take that one out or this kind of idea of um you give it like some messy chunk of text and you say uh you know where where do you think you should split this text as another kind of thing that an llm could do one kind of idea is that you know they're gonna fine-tune or specialize llms that are maybe only like 15 billion parameters like 15 billion parameters I think the current state of the art in text to SQL like zero shot so fine-tuning LMS to plug them into kind of that part of the data importing stack yeah so I think um like to if you think about higher level like what kind of chunks you want to help with the search result right so if there's different types there are chunks that have like relevant information in them there are also chunks where like you know is it keywords in the chunks is it the whole kind of simple idea of like this is a sentence or this is a paragraph right and then there's um things that are more complicated so for example like I'm sure you've heard people talk about summary chunking and how like you want to do summarizing and it's like you get into some questions about like what are you summarizing you summarizing a paragraph a page a document do you need to be smart enough to understand um like oh this is you know section two you know um part four and so you wanna like however many pages or paragraphs that is you want to summarize that and I think that's where llm especially ones with decent sized context Windows could be really good um I think that the trick is like chat gbt you know especially gpt4 can be expensive um sometimes the API goes down things like that so if you have like a model that's specifically designed to do summarizing zero zero summarizing then that could be a really useful thing to insert into the flow um likewise with kind of extraction right like you could have a model that's just trained specifically for that um and use that to sort of like extract keywords or extract themes um I think for us one of the things that we're exploring that is kind of interesting is like um can you actually approach the performance of a deep learning model with something more traditional that's way faster and way cheaper so for example like could you use xgboost to um do sentiment classification and just put those sentiments in the metadata and that would be like substantially faster and cheaper maybe it's not the performance is not quite as high as an llm but you you get other benefits from that approach that allow you to make that trade-off yeah I I see kind of two things like this summary chunking and let's maybe come back to that I want to I want to stay on this XG boost for sentiment classification this is what Nils uh took apart with my argument on metadata extraction that I think is extremely you know I I think it's bulletproof is like you know if you're letting the llm just arbitrarily decide what metadata to extract from each chunk then how do you then query it whereas alternatively if you have some kind of classifier that takes a you know let's say you've trained just like I don't know if this is the best example but you've trained a classifier that takes in uh Wikipedia passages of NBA players and then classifies the age if it's mentioned or maybe the team they're on I don't know some like maybe like how many seasons they've played if that's maybe something that's kind of ambiguous that you would need to kind of derive from the text something like that right because if you train a classifier on one special kind of metadata attribute it's just so much easier to then interface it at query time because you know that like you know hey there these many this many ages age values or teams I don't know what the maybe we should have started with a better example of like a metadata to ground this like or sentiment why don't we just come back to sentiment like so then then you know at query time you're giving it a movie review you want to find the most similar positive movie review right because you have this kind of label on it yeah yeah hopefully that kind of said the transition nicely like what do you think about that kind of or just to keep kind of giving our thoughts on like this kind of idea of having a specialized classifier for the particular kind of metadata you want to label each of your chunks with I mean I think I think that's just like a kind of a broader trade-off that you have to think about making with any use of a large language model versus a bespoke machine Learning System like a file read we we looked at using different Technologies to parse um PFS um and ended up building our own classifier because like we had very specific needs um in order to be able to understand the documents and I think it's all about understanding the data right and so like you're at you're using metadata as kind of a way to understand the data and then add additional reference information to make the search better and so it some part of it comes down to the the engineering requirements and the the business requirements around like cost like do you have time to train a custom classifier is it easier in terms of the developer workflow to just use an llm or if there are these off-the-shelf ones like do they easily fit into the workflow I think I'm curious to hear your point on this but one of the things that we're observing in our discussions is people are using metadata in really different ways like it's not yet super clear like what fields are common and what things you want to put into metadata so like I think to answer your product question that approach sounds like it would work better right because I think for any specific machine learning task you probably could train a model that could beat an LOM at doing something very specific it just is a question of like is that specificity what is needed in the market hmm yeah I guess it's like um if I'm if I'm imagining I'm building a chat box or a search engine for like we V8 questions I might want to classify them into questions about importing data questions about the query apis questions about like replication scalability like Cloud management stuff so I might have these kind of categories that I would put on the chunk but like what Nils was saying that I kind of agree with is like um if you have some label that you can classify from the content alone you can probably capture it in the embedding as well like the embedding model could capture that kind of thing in its representation and that that's kind of what I see yeah I I guess for me the the reason I like this whole argument is because I like thinking about these systems as like like I like to try to point people to the weeviates aggregate API like we V8 it's a vector database but you also can do symbolic queries in weba like if you want to say what's the average age of NBA players you you can ask that kind of question to alleviate so that's kind of what I see is the clear value of the metadata you know for for kind of query parsing and adding filters to queries I'm not sure I understand it all the way I understand definitely like as one of our earlier conversation topics like a part I imagine with the end-to-end importing data to wevia is like you import it from multiple sources like you have tweets you have GitHub you have YouTube you have uh slack and so you might want to put those into like you might want to have a symbolic filter that uh you know separates that at query time yeah I mean I think the space is so young right that I think one of the things we're going to see emerge is sort of more standardized access patterns let's call them right and I think that's one of the kind of Visions for what we're trying to achieve with Vector flow is to be part of this process of like building almost building like an HTTP for for kind of querying your your uh semantic your contextual data right so like not HTTP itself but this idea of a protocol standardized fields and things that you're going to access so as the space matures we might understand like these are the types of metadata that are really important and that most people use and then it does make sense to have off-the-shelf classifiers uh to do those things to extract those things yeah because I well yeah yeah well let me get your temperature on that argument of if you can classify it in the content the embedding can also capture it yeah I mean that makes sense high level I guess like for me it's like the vectors are so entangled like of you know like you know Vector representation of an object like from open AI the 1536 Dimensions there's you can't like point at two indexes of that vector and say this is how much of a cat it is right like it's it's completely entangled across the entire Vector so that's that's why I still see like if you classify like give a picture of a dog and like yeah the embedding captures that it's a dog but if you have a classifier that uh does the like it might be a dog jumping to catch a frisbee or it's like in a pool or something and so it's kind of like entangled with like dog but also like dog swimming kind of like so so I see if you have some kind of attribute that you know is going to be super important like what kind of particular dog it is to classify that and have that in addition to the embedding I could see that as being some kind of lever that could improve your search relevance yeah absolutely I think we're agreed on that I think one of the things that I'm curious about maybe because we're so Vector flow is more on the ingestion side right and I'm I think at a certain point once you have the information in the vector database you need to be clever about how you retrieve it right so I think that's for like also certain things like re-ranking can come into play where like okay you have all these these different concepts of a dog in the vector embedding but it's like how do you actually turn that into like the right kind of overall view of the information to extract um yeah yeah no I think that's so it kind of comes like into you you mentioned writing custom code to parse PDFs and like you know unstructured is a really exciting company that's doing like um one I mean I think there's a lot of cool parts to instructure but one thing that Brian explained to me on the podcast was about uh doing like a visual document layout and so you know you visually put the document and you would say like and it say like this is an image caption and so because then yeah and then everything you do at importing time then is going to impact what you can do at query time because then if you want to have some you know it could be a filtered search where you only want to search through image captions or you want to have like a ranking kind of uh like a ranking equation where you say like boost the results if it's a image caption I don't know image caption is the best example but like that kind of thing yeah absolutely I mean unstructured is a really uh really cool tool we're playing around with adding it to uh to vectorflow to help with certain things for example like how would you ask questions about like a pitch deck or a cardboard slide and because maybe like you know that's portrayed as it's uploaded as a PF which our system accepts PDFs um but a lot of the information embedded in that might be visual or spatial I think an interesting idea is like take the idea of an infographic right how and let's say you have a textual system that will be very if you have an infographic in a PDF or a word doc or something how do you convert that into something that you can represent textually in the vector database for example and you know are you storing multiple modals in the same index or are you is there a way to do that transformation I think this idea of having tags about images as well um could be could be another interesting way around that but um congestion problem in general like there's so many different combinations of things that it's really exciting to think about how how you can best represent those in a vectorized form and make the the retrieval results ultimately better yeah I'm I'm so excited about this conversation topic infographic brilliant example I haven't heard that before and quick shout out to instructed with the uh shout out I hate that word I hate that I just said that but like a good guy like to unstructured who just did the like um processing tables visually And Turning Tables into uh searchable things that it's really inspiring work and so I'm I I want to kind of talk about I had this idea as you were talking like um you know with our whole conversation of metadata extraction maybe it makes sense to create a new class and weviate that only has like 50 objects in it but they're really like symbolically annotated and that's where you would store the kind of information needed for these symbolic SQL style queries like you know if you have country music singers and like the ingestion engine is parsing through an entire Wikipedia page maybe it just makes like one data object to put in this other class which is like symbolic country music singers I don't know and then all that but then all the chunks go to this other class which is like the sector class and so you so you separate it into two kind of classes in sort of like a weevia native way of thinking I'm sure all the vector databases do like some kind of abstraction between classes some something that sounds like that yeah absolutely I mean um I think how the different information will ultimately get tied together is still an open question that's why I was talking about like what is going to be the solidified data access pattern um you know perhaps like it makes sense to to have certain information in the vector database and because the vector database right it's not the result that you get from the vector database you're getting things that match your your input query right but perhaps like in your re-ranking pipeline or something you then go and you hit your SQL database to do certain kinds of searches that they're more optimized for um I think I I think these kinds of questions will the patterns and the best practices will become solidified over time but I think it's an interesting idea to see like what else could you put into a vector database you know and I'm curious for example like I brought up this idea of multimodal like how how well those things will play together you know like in infographic in in a PDF for example that has a lot of text um it's not something that uh Canada we've we've played around with yet but it's certainly I think an exciting New Frontier because if you think about like most data in like a corporate environment it's probably going to be mixed yeah no it like it it like I feel like the like with the world of vector databases I feel like there's kind of the power in the hnsw implementation and then all the database kind of stuff around making that like crud compatible and scalable and all that kind of stuff and then I see like these two layers with like the importing data into it and then kind of like the query engine the query interface and then of course there's like the language model which is another like major like PowerPoint I'd say because of how of like the mode of trying to create another one but like that kind of um the query engine part is quite interesting because you know there's like the llm Frameworks Market with like llama index Lang chain uh Haystack Gina and and others um maybe forget off the top of my head but like that they they would do like this kind of um you know like one thing like one of my favorite abstractions in llama index is the query router where the llm takes the query and it says is this a semantic search query or is this an SQL query or maybe it's like a Knowledge Graph query too that's where I think these when Frameworks will like always kind of sit in an interesting place because they can like combine SQL with Vector search with like these Knowledge Graph or like graph database Technologies and I think that's a really interesting kind of component to it but I do think it's likely that um you know and I'm like it's like maybe I should like get to working on this right after we hang up the call but like I think it's quite likely that we can build some kind of interesting uh query router into leviate for this is it's like a little interesting like how the design will work but we via has this module system where like you know we interface our near text to do Vector search or hybrid search our hybrid you know query API as well as re-ranking and so I think we can keep going up like do we want to go so far up to have like Chain of Thought and like Auto gbt style query engines directly into weeviate I'm really not sure but I do think that kind of query router thing it's gonna be a pretty important part of it I think I think I think you actually might talk about this the first time we had a discussion I may have also talked about it with the the head of research from weeviate that like um this idea of query optimization right it's like that's a that's a big thing in traditional databases um where you have you know entire in modules built into the system that can kind of translate things to optimize depending on what you want whether it's like a smart way to index or you're optimizing for Speed or whatever and in in terms of the performance of the query like as measured by the Fidelity the result right I think that's actually a really interesting Frontier for Vector databases and you know can you put um a query uh sort of Optimizer in front of it and like you know you were describing as a router but what I almost imagine is something that can kind of reword things and whether that's like you know a machine learning model like a large language model of some kind that specialized to that task or um whether it's it's the exact approach I think is unknown but this idea of like once you once you have enough data about how different queries perform you could actually start to alter the wording um to make to make them better I think is actually a really exciting Frontier it's it's outside of the box of what we're what we're doing at Vector flow but it's definitely something we've heard people ask for pretty consistently in the discussions it's like yeah we're not exactly sure the best way to hit the vector database to get the best results um and it would be cool if you guys built that because you probably have all the internal data to know like what actually does constitute like a good query versus a bad query for the same for the same index well this is so much cool stuff in what you just said I think start yeah like kind of yeah like the query formulation thing maybe just come on that and then and then hit it with the big payload thing guys change the conversation topic with this but like yeah that kind of idea you take like people are sending prompts and so you don't want to just send the prompt as the search query most of the time you'd want to like you know extract like what is a search query particularly send that to the vector database and then that's kind of how it all gets combined with the prompt and the new search results but here so here's kind of the thing that I think is like really exciting uh so I'm still in this term from Andy Pavlo who's a professor database is at Carnegie Mellon which is uh self-driving databases so it's about databases that kind of like optimize themselves the indexes they build based on the queries that they're seeing in coming in so like you know if you're like uh uh like a payroll database and you're seeing this particular kind of you know let's say you have like 30 tables you get like this crazy complicated payroll and so you're seeing this particular kind of joining pattern being queried a lot you would start to like build up an index for that join and so yeah I'm really curious like this also comes into this other topic at we get that we like a lot called um generative feedback loops or generative feedback loops are like you take some of your data you put it into an llm and then you save the resulting generation so so kind of marrying these ideas is like you're seeing particular kinds of queries coming in and then your llm could kind of like transform your data in your database to better accommodate these queries and I might I might be diving in too much let me like get your temperature on the idea at a high level and then we'll kind of go further into how this might work I mean I think it's an interesting idea as long as you basically accept the premise that a vector database doesn't provide the same set of guarantees that a normal database does right like you know with SQL database you have like the acid principles like um you know you want to make sure that the data is always like the transactions guarantee the Integrity of the data and certain things like that I think if you're treating the vector database as a store to to Really provide some kind of system capabilities like around end user performance rather than like we need this data for this system to kind of have its core operation like like to store it um to back up what's happened and to be a member but rather like this enables like the core flow I mean you accept that those are different premises I don't know if I'm describing it um the most adequately but I think you get what I'm saying then it's a really interesting idea because you can you can basically tweak things to to give better results and I think it's it's almost like the opposite approach of what I suggested where rather than optimize the query once you figure out what the access pattern is going to be you can then kind of tweak it um what you're almost describing to me sounds kind of like a like a recursive feedback loop along like okay we have our initial chunks we put a bunch of stuff in them now we have this feedback loop built out and we see how people use this let's um you know let's alter these chunks let's continue to evolve them where I see some complexity associated with this is it's my expectation that what's in the vector database is not going to be static um I think there's some scenarios where people will ingest stuff once but I think one of the problems that we're seeing is more like you have to sync data you have to ingest stuff like one of the things we're building in a vector flow is like a dedupe mechanism so the idea that you're not gonna um you know add duplicate vectors when you have to update and so how would this idea of having a feedback loop that alters the core uh you know embedding in the database play with the fact that you also need to like constantly refresh data in a production system yeah it sounds it is it sounds really exciting I mean I guess it's like maybe your queries tell you something about the data you didn't originally that like maybe the queries are telling you just like to add this kind of filter like look for this kind of thing particularly because I'm seeing this feedback where they like these kind of answers I kind of like that long tail versus the popularity kind of thing and yeah I don't know exactly but I think really the the the the key thing and how this ties into like how you think about how you import data is like um just probably uh I guess it's there there's definitely like levers of control here like with the um so kind of coming back to like the self-driving databases in more of an SQL sense is like you can build up certain index structures for different kind of properties but then speed up the latency of making those kind of queries and Vector search has a similar kind of analog with the hsw graph where uh if you know that you're always going to be doing this one kind of filtered search you can like increase the connectivity of that kind of surf so you know we the way that filtered hsw filtered search in hsw is implemented is you have like an allow list and it says traversing the graph and it's saying you know are you on the allow list well no okay well I'm still going to explore your neighbors but you're not going to be in the result set and so if you know that you're doing a particular kind of filtered search all the time you could kind of connect those filters and then that allow list will you know be pop you know the result list will be populated quicker and so you'll have a faster latency so I see that kind of angle to it where you're just trying to speed it up by by you know what queries you're having so you're going to index in advance but but then there's maybe something to like the queries tell you something about the information that causes you to think okay I need to change uh the information that's in my chunks or like something like that yeah I mean no it's definitely an interesting idea I mean I'm just kind of thinking out loud here that like you know if you have the capacity to do it like your organization you know you can you can afford to have multiple different uh versions of the data stored right like you could even think about having things in different actual graphs and some sort of proxy layer in front of the database that kind of decides at runtime like which which index you want to hit um I'm just kind of trying to connect the dots with like I know traditional databases have proxies that do a lot of interesting things it's more I think on the performance side like connection there sharing and things like that but like could you combine this idea of like the kind of self-driving database with the proxy layer that that provides the you know a smart interface um to to actually make the the results as good as possible yeah no I that sounds really exciting to me because you like coming back to the hnsw thing like having two separate classes one class is like this you know optimized for filtered searches and the other one is just the ordinary hsw and you have like these two classes and you know you have this kind of proxy orchestrator that's saying you know I'm not getting enough filter to search this anymore let me go ahead and destroy that because the memory overhead doesn't make it worthwhile anymore and yeah I did I think there's a lot to that I I think it's a pretty interesting kind of Direction and I the reason I think it might be relevant to you know Vector flow and managing Imports is I think a lot about this kind of like uh degenerative feedback loops and you know this is Bob's idea I'm just like sharing it but like this like um having like a rest API around the generative feedback loop that would um you know like so like one example is like when you're generating synthetic data from your for for your task or whatever you every like 10 you might want to like back up your database because you're paying for these LM inferences you want to you know save it if you potentially have it crash but uh if I save it every 10 and you know now I have like the the newest one after 100 it's like go ahead and delete those 90. that's kind of like the infrastructure management task I see is like okay I don't need these backups anymore go ahead and delete them it's kind of like like a garbage collector for the data backups so I could see that kind of thing being managed by an external service rather than leviate itself yeah that makes sense I mean I think in general it's kind of like the difference between the vector store and sort of this whole Vector Ops life cycle um and that's part of it along with just on you know it goes into um understanding when you want to refresh the data and that's a big part of ingestion as well and so I think I think the way that you see um tooling built up around traditional databases I expect you'll see tooling built up around Vector databases to help people get the most out of out of those systems I'm curious kind of like what are you guys sort of encountering on um the the sort of educational side of things because we're talking about all these capabilities that we see and how we could see stuff evolving but one of the things I sort of notice in a lot of the discussions that I have is that people still need to really be introduced to the idea of like what can you really do with the vector database and why do you need to put it into your into your system architecture um so I'm curious like how do you see this becoming more more approachable to like the general kind of community software engineering community as a whole um well it may hopefully not too redundant but I do think that kind of the zero shot model like what makes it more accessible is the zero shot models sorry let me can I ask you one more question before we come into this topic is sure quickly I just wanted to ask about uh the gbt cash idea where you you take llm responses and then you cache them and so now you take a new answer and a new question and you would map it to its nearest uh like you take a new prompt you map it to its nearest prompt that's already been answered and then you give them that answer and so I so sorry to let me like cut so back into this conversation sorry if you don't I just I just want to get this last song and then we'll yeah and then I'd love to come into the topic of how do you evangelize Vector databases but like this um yeah gbt Cache is like uh you know the llm does some kind of response with your data and then you save it it's so it is like a generative feedback loop where you save it back in the database but it's for the sake of you know question answering in the future sure yeah have you seen this kind of gbt cache thing yeah we actually looked into it um when we were first exploring like you know different ways to deal with some of these problems because like you know you do part of getting the best performance out of the vector database is like you know probably to do some caching so that you don't have to hit it right tons of traditional systems have caching of of one sort or another I think the trick is like to to do smart caching in a way that's not really expensive or really burdensome in terms of the memory um I think that's where actually this idea of like a zero shot llm that is designed specifically to to kind of do semantic caching could be really interesting where it's like you don't actually have to hit chat GPT maybe you could have a a model that's you know more parameter light that could even run you know like on a much smaller device like not on a GPU that just enables that to be really efficient because it's not the problem right is it's not like a traditional cash where you can have um like a key Value Store you need to really be smart about how you decide to cash and and what you decide to cash I it's interesting that that kind of key Value Store analog because I I know a lot of like you know researchers think of like you know deep learning is just like glorified hash tables and you could think of embeddings as kind of like a key hash and so it is it kind of relates to that idea but I guess the the thing about gbt cash and how I'm seeing most people using it right now that I find to be interesting with this more like autonomous self-driving database kind of concept is like usually gbt cache is triggered by a real person came and asked a real question and this is the real answer that they liked therefore in the future let's use that answer again whereas imagining like an llm that's just like you know prompting itself with your documents and creating more information and like would you ever want to return that information that's created by an out like an llm retrieves like two parts of your documentation and it says I don't know write a comparison of these two or like think about how you might combine like if it you take like Atomic apis and it's like how might you write a compositional API of these apis and and it creates like new data and would and would you want to then index that and maybe return that for a search so if I understand what you're saying are you basically saying like you you use the Ln to populate the cache kind of it's like so yeah yeah yeah definitely an interesting idea I mean I think you'd have to like somehow have some some human feedback in the system that's something we actually it's a topic we've kind of skipped in this in this podcast maybe the next one we'll talk about it uh come into this system but it's certainly an interesting idea and I think it goes well with the general theme of like at some point once these Technologies mature you'll be able to kind of use them for recursive sort of improvements where they can just make them look better with some some human guidance and I think that the idea of applying that to cash is definitely an interesting mechanism as opposed to like you know the more crude caching strategies that we have now you know like least recently used and you know putting time and cash and having maybe better ways to assess like what really needs to be stored hmm yeah yeah I love this topic it was such a fun conversation and yeah I I think there's so many interesting topics about all that and I think yeah I do like I think this kind of external uh document manager outside of wevia could eat up a lot of a lot of functionally what this kind of stuff ends up looking like but yeah let's take the step back I know this is like dreaming into the future I just like I always like doing that but this kind of topic of um you know how do you evangelize these new technologies I mean maybe let me first ask you where your head's obviously you know you're starting this new company Vector flow and you know trying to you know get off the ground get some content get some users and all that is maybe like yeah how do you see that in your focus and how are you currently thinking about it yeah I mean I think it's um I think for us we're really trying to more understand the axis patterns around Vector databases right now and that's why I was saying like you know we're still waiting to see how those mature like what are the common like we talked about metadata what are the common fields or you know like what are the common data sources what are the common formats what are what are the main kind of issues that you're encountering with ingestion because I think Vector flow is designed really to be sort of this um like let's get to production quickly type technology um but I think there's one of the things that we're seeing is a lot of people using llms that just don't really fully understand the power of semantic search and the wide variety of use cases right like people know about to some extent like this idea of ask questions of your data but for example a lot of people don't know that like you need to use uh semantic searches for example in facial recognition or like I was actually thinking this morning about how like they maybe could use it for inspecting rocket parts right because like these Rockets have I think I want to say this is a number off the top of my head but I want to say there's like two million Parts like Falcon and rocket and it's very precise and like what's the difference between a test part versus a flight part like it's minute and could a conventional model by itself just doing inference detect that or would you also want to do some kind of more complicated similarity search I'm kind of going down a rabbit hole here but it's like people still don't know about the broad applicability so I do feel like there is some degree of Education that we have to do as well because we are a pipeline that is dependent on Technologies like Levy so people need to first know like well what should I be using bb8 for it's like oh okay I need that well yeah and also like oh that makes sense and if I want to get stuff in easily and get to production quickly like oh Vector flow yeah yeah that that um that inspect the uh rocket parts that that's one of the use cases I've seen with levia that just always stuck in my brain is um it was uh Airline manuals and like searching through a particular kind of configuration of that it just it always motivates me to talk about this metadata thing and grounds my thinking and chunking just because of seeing what that kind of thing ends up looking like but yeah so exploring all the different use cases that can exist of uh you know Vector search technology very broadly I I completely agree I think like I don't think we have a good sense of what multimodal retrieval augmented generation is going to look like yet where you know I imagine it in the e-commerce case where I'm like you know show me what I would look like with a white pocket hat it's something like this like where it retrieves the images of me and it like fuses that with the image search of bucket hats and it like combines these like this kind of combination of like um modalities in retrieval augmented systems that yeah the applications certainly are still growing yeah well I I would like to pitch you one more thing kind of like future looking thing about what I think is going to heavily impact how you evangelize Technologies broadly is the gorilla large language models so with the gorilla large language models what I would imagine a user doing is you tell me a little bit about your data like your data Maybe gorilla designs a schema for you and then you say okay build me an ingestion flow from my notion to leviate using Vector flow and gorilla is an llm specialized on the apis that you know like levia in Vector flow collectively build and gorilla does all the intermediate stuff so like you know writing all the python code for doing this yeah what do you think about that kind of idea I mean it's definitely an interesting idea I mean I think like it'd be interesting to see how that would fit with Vector flow because our whole like one of our overarching design principles with sector flow is like to to just try to limit system complexity and it's definitely a challenge because um you there are a lot of things that you need to tweak and so our idea is like minimizing how much code you have to write having you know just a simple API endpoint or a config file and so I think um if there's a way to even remove that complexity it gets you to production faster I think the other thing like vector flow is really useful for testing right we were talking about like the need for more kind of formalized testing strategies and um the idea that you you probably need to to really figure out which hyper parameters work best for your retrieval system you need to test a lot of things in ingestion so this idea of the gorilla model just enabling you to go through that cycle really quickly or even doing it for you could be super super cool it could basically just remove a lot of like more grunty aspects of setting up these systems in figuring out exactly how well uh like the best way to make them play nicely together I think is a super interesting idea yeah because I think like with Vector flow you know you're going to create these this documentation that shows you how to do the particular syntax and then if you have a language model that I can just tell you like you know roughly what I want to do and it just translates it into the vector flow apis now the learning curve for me a new Vector flow user becomes a lot lower I think for weviate and like you know it's a bit more interest like we it's pretty interesting because like for us like uh users need to learn like the search API so like you need to learn the syntax to like put two filters on your vector search query and so that particularly that language of being like add these two filters and it generates the graphql that that excites me a ton you know it's kind of like the text SQL stuff but yeah because I feel like it just when you're evangelizing your new technology you have this like learning curve like you know learn my syntax learn my new you know my library that kind of thing yeah definitely I think that's that's why we we're we tried to stick with HTTP as the first way like an entry point into the system um but it's there's a lot of different things to tweak for example like you know a lot of traditional tools like allow you to pass in Python scripts to run specific um Transformations and I think one of the things that we are exploring actively right now is like what is what is the main flow you know and what is what what where are the real barriers to to entry in terms of getting things to production we know that one of those is infrastructure which is why um you know we're building what we're building but I think anything that reduces that friction whether it's a large language model or just you know a well done integration between two systems I think is ultimately going to allow people to extract a lot more in value from these systems yeah yeah I agree 100 yeah and I just think kind of the thinking about the future of what these Integrations are going to look like is pretty exciting like how yeah like you you mentioned like HTTP and like networking protocols and it's I guess it's almost like that like if language is the interface now between all software tools for integration sort of like how you know HTTP requests are the interface between like networking endpoints it's and that's kind of how I see the future of that kind of part of it but yeah anyway so yeah I love talking about the Guerrilla language models because I just think it's a pretty cool topic but anyways uh David thank you so much for joining the weba podcast yeah I I really liked our kind of initial tour of like jointly understanding the end to end of importing from you know connecting these different data sources the kind of metadata chunking visual document parsing all those that kind of topics and then how do you vectorize and then I guess the the last networking to get it into the database it's such an exciting space and you know thanks to the conversation around the self-driving databases generate a feedback loop I had so much fun discussing that and you know thank you so much good luck with everything yeah thank you so much for I'm super super cool to do a discussion like this where you just um you know you're in certain ways um getting a very specific exposure that I love to hear that the new ideas that come up and then we can chew on those and think about how we could put them into our system so it's a lot of fun super excited to see uh what happens in the space ", "type": "Video", "name": "David Garnitz on VectorFlow - Weaviate Podcast #66!", "path": "", "link": "https://www.youtube.com/watch?v=efAvOsO8Gik", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}