{"text": "Hey everyone! Thank you so much for watching the 66th Weaviate Podcast with David Garnitz, the creator of VectorFlow! \nhey everyone thank you so much forwatching the 67th weave podcast withDavid garnets the creator of vectorflowreally quick before diving into it Iwanted to thank David for joining thepodcast and please show some love tovectorflow on GitHub leave it a star ifyou don't mind open sourcing is alwaysawesome and there are so many excitingideas in the space of at the highestlevel how do we get data into Vectordatabases like we say you take you knowtext for example text has all thesethings like you know how are you goingto chunk the text metadata extractionbeing an interesting topic then doingthe embedding model inference and thenyou know with weave we've exploredthings like the spark connectors or grpcapis just you know like how do you getthe vectors then into the vectordatabase and so I had so much fun withthis conversation I hope that you knowyou enjoy listening to the podcast andplease check out the open source GitHubrepository uh Linked In the descriptionof this video D Garnet vectorflow heyeveryone thank you so much for watchinganother episode of the weevier podcastI'm super excited to welcome Davidgarnets the founder of Vector flow thisis a super exciting new company that'slooking at innovating on how data getsinto Vector databases like we've yet thewhole importing data data ingestion sowe have so many exciting topics there'ssuch an end-to-end stack with thisfirstly David thank you so much forjoining the podcastyeah thank you so much for having meexcited to be here so could we kick thisoff with uh you know what inspired youto create Vector flow and sort of how doyou see the problems in the spaceyeah so that's a good question so liketo just rewind a little bit on mybackground like I was a machine learningresearcher in grad school and I workedon some developer tools andinfrastructure at companies like SpaceXand affirm and ended up going to workfor a pretty early stage AI companycalled file read and at file read weactually had to build like a massivelyparallelized Vector embedding pipelinebecause of the volume of data we wereingestingum and so I saw that pain Point uh withthe team there and realized that therewasn't really any good solution in themarket and decided to um go off andbuild it myselfyeah amazing I remember like talking toyou earlier about like load testing andand that kind of thing and yeah it'sdefinitely something we see at weave itis like you know people who put just acrazy large amount of vectors into itand you know like whether it's you knowbillions of vectors and this kind ofthing can you maybe talk a little moreabout um yeah just how you see theopportunity like with you know likeusing Vector databases with billions ofvectors I think there's kind of like twosides of this like you could be you knowsomeone who's hacking on your personalnote-taking app and you probably don'tdeal with you probably have like youknow 100 000 or so but or you're lookingat these big Enterprise cases where youhave just enormous amounts of datayeah absolutely so I mean we wepersonally think at Vector flow that umyou know these these systems theseretrieval systems are going to beubiquitous right because every largecompany has an incredible volume of datathat they can leverage both for externalcustomer facing products but alsointernal products to save money and movefasterum but obviously the space is is Youngand a lot of the Technologies need tomature and so there's a big challengethere in terms of ingesting data atscale putting it in the store but alsounderstanding which data is relevant solike internal there's a lot of old kindof stale data internally these largeorganizations tooso you have not just a challenge aroundgetting that in but figuring out whichdata to draw fromyeah definitely I think um maybe if Icould yeah this kind of old stale datathere's I mean yeah there are so manycomponents to the data importing intoVector databases kind of conversation Ilike this like you know maybe the llmknows which class so like maybe this ispeople who are watching this podcastprobably already know that wevia usesthe class abstraction to kind of likeyou know isolate some type of data you'dhave like one vector index per class soI like this idea that llms maybe knowlike which class to put data in like ifyou're importing from multiple sourceslike a common thing that people arebuilding is like chat with yourdocumentation where you might beimporting from GitHub YouTube say slackand maybe you route these to differentclasses as well so that kind of I seethat as one thing as well as the youknow the stale data kind of like um youknow refreshes handing document updatehandling document updates as beinganother kind of part of this but um sookay so with our podcast I think it'd bereally nice if we could just kind ofcome to an agreement on this end-to-endand what data importing looks like so Ithink there's kind of like connecting tosources is one thing like you know PDFsnotion readers Google Drive readers thenthere's kind of like um I see this kindof cluster of things where there's liketext chunking strategies like there's alot of kind of naive text Splitters likethe you know just rolling windowchunking stuff like this there's alsolike maybe using llms that a languagemodel would look at a text Chunk and beprompted like please split this up whereyou see semantic gaps and then it woulddo the chunking that way there's alsokind of like this metadata extraction soit's kind of like that layer and then Isee once you have the chunks you nowpass this to the model inference to turnit into a vector and there's certainly alot of innovation there and then I guessfinally the last leg of the mod um thelast leg of the mile is I don't know ifthat's the phrase but is then pluggingin those vectors into Eva so is thatkind of like a good uh storytelling ofkind of like the end to end what ittakes to import data into Vectordatabases yeah I mean I think that's agood high level story I think one thingthat we've also encountered in ourresearch and discussions with people andone of the things that we're looking atum actively building into Vector flow isalso this idea of like pre-processingthe data before you turn it into vectorsso not just metadata like metadata Ithink is it's useful for different kindsof hybrid search right but there'strade-offs associated with usingmetadata like it can make the uploadspeed slower or maybe you have to uselike a different retrieval algorithm sowhat we've heard some people do is toactually pre-process the data to putrelevant information into the chunksum or even uploading it to the vector DBor before adding any metadata oranything like that so that's anotherstage that I think is kind of emergingit's pretty immatureum curious to see though or hear like onthe weviate side like what are what arey'all kind of encountering in terms oflike usage of metadata andpre-processing and things like that yeahwell I I think this is such aninteresting topic that kind of take themetadata out to put it into the chunksthat just kind of build on that a littlebit more I see that as like um you knowif you're parsing like a legal contractand it's like you know clauses3001 and then you would take thatheader and you'd kind of put that intothe text Chunk like you know this is apart of this clause and then you hopethat that Vector embedding is going tobe better it's also probably going to bebetter for the bm25 scoring when you'rethen saying I don't remember the numberlike s3001 something like that like thatkind of adding uh metadata into thechunk for kind of vector search butthere's a few other ways of looking atthis and I guess another idea like whatI like a lot and what I think is kind ofunderrated is let's say I'm parsing OutWikipedia pages of NBA basketballplayers right and I would and I comeacross like age like you know JimmyButler is 28 years old I don't know howold Jimmyso like I would take that out and putthat as metadata so then if I want toask a question like umyou know it could be the semantic searchkind of question where I have like avideo of someone playing basketball andI'm like who does this guy play like whois older than 32 just a random questionor it could be like an SQL style querywhere I say what is the average age ofplayers on the heat so it's like I seethis kind of metadata extraction thingas being quite important I had a bit ofa disagreement with nose rhymers on apodcast where he was thinking thatuh you know all this metadata can bekind of captured in the embedding inthis kind of thinking and you know I soyeah so let me just pass it back to youto get kind of what you think of thiskind of metadata extraction from textchunksyeah it's certainly an interestingapproach I mean so I think the thingthat we're trying to understand is likewhere do you how do you differentiatebetween putting things like say at theend of the chunks right so you talkedabout age like you could you couldliterally just put the age of Say JimmyButler at the end of every chunk aboutJimmy Butler right but then are yououting the chunkumyou know I think the benefit right isthat like you can use a more standardkind of like a n algorithm to retrievethe vectors and then you can maybe haveyour re-ranking pipeline or whatever tokind of like get a high fidelity resultwhereas like with the hybrid search youhave to potentially Implement somethingslower or you need to store the data ina different type of graph or somethingrightum so there's different trade-offs umbut what we're hearing is that in eitherscenario these things are reallyimportant to improve uh the Fidelity theresults and that's kind of like thebroader theme I feel like that thatwe're encountering I'm curious to hearif um you guys are hearing the samething but for us it's it's a lot of likewe we like the the systems they're goodlike they're easy to use but how do weget the results as good as possible andso that's where we've been trying tounderstand where in the industry are thebest practices solidifying because it'sit's still such a nascent fieldI agree 100 with that I think evaluationis nascent like absolutely like um youknow evaluating how well your search isperforming I feel like is more of anacademic exercise and something you seeI mean like I do think it's picking upthough especially like um just yesterdaywe hosted a workshop with arise Ai and arise AI they're building like you knowlike a visualization observability tooland so you kind of inspect like you knowthe vector spaces of the nearestneighbors to your prompts and sort ofstarting to get some evaluation metricslike I I that we could definitely diveinto this thing about evaluation it'sthat's another one of these major rabbitholes but so okay so but staying comingback to the data ingestion kind of topicum so yeah metadata and I think we candefinitely come back to that if we wantto but I I really want to talk moreabout uh kind of like vectorization likeonce you have the chunks and now you'retrying to like manage inference of likea hundred million I don't know a randomamount of vectors but like of textchunks and you're trying to vectorizethem all and get them into you know orjust get the vectors to deviates then dothe deviates half of the indexing parthow do you see that space of like howcan we vectorize this as quickly aspossibleyeah I mean I think that's where a lotof infrastructure comes in right wherelike you really need toum build out a massively parallelizedPipeline and that's kind of where wecome into play where we enable prettymassive scalability as well asconfigurability leading up to whatactually gets vectorizedum or embeddedum but theyou you need to spin up like a messagequeue or you need to stream some in froman S3 bucket and you need not just to beable to do the calculations in a waythat's that's quick and efficient butalso cost effective but you also need tomake sure that you're not droppinganything you know that you have a retrymechanism built in and I think that'swhere you get into some of theseengineering challengesum and this is where like you know mywork at SpaceX and affirm is kind ofhelpful in understanding kind of how tobuild out this infrastructureum and it's part of why we we took theapproach that we took at Vector flowwhere we said well we're going to bemore of an infrastructure first productso if you look at if anyone goes to ouropen source repo they'll see you knowDocker images to spin upum you know a message queue in adatabase and making sure that you canget all the information you need to inthe database if you're in the vectordatabases I think one of the one of thechallenges that people don't realize isan issue until they start to operate atscale I'm actually curiousum what you guys are hearing in terms oflike something something that we we hearpeople say a lot in discussions is umyeah we thought this would be super easywhen we were playing around with it andthen we have a follow-up conversationwith someone two or four weeks later andthey're like oh we actually tried toturn it on to production we realizedlike oh we we didn't think through a lotof thingsyeah well I guess for me it's like umusing the open AI or cohere embeddingsapis I think that tends to work for mostpeople like they they really enjoy justkind of sending their text to the apisgetting the vectors back and howeverlike batching is handled within openaior cohere is however that's handled butyeah I do think once you then are tryingto like you know you've maybe fine-tuneda sentence Transformers model or youjust want to deploy one of the modelsfrom hugging faces sentence Transformersjust to try to save costs now you openup a huge can of worms of potentialpotential like problems to solve likeyou you now become interested in likeyou know the whole inferenceacceleration camp with things like theOnyx run time or maybe all the stuffneural magic is doing with like sparsityquantization of Weights just so muchstuff there and as well as kind of likethe the load balancing between uh youknow different vectorization endpointsyou might have as well as maybe umoh maybe you're dealing with like Rayactors and you're using gpus for theseinferences and so no you've added likeanother layer of thinking about this soyeah maybe so so it sounds like to melike you're maybe more interested in inthat kind of Latter half of you havesome kind of vectorization model let'syou know let's unpack the box and let'stry to optimize all these parts of ityeah absolutely and I think um so whatwe've heard is that when you really wantHigh Fidelity results you actually doneed to use open source models becausethere's a lot more Innovation happeningum in that space for specifically forembeddings like I think for the largelanguage models themselves the theclosed Source actors are still probablyum in terms of usability probably thebest products but I know um fromcustomer conversations I know from mywork experience at file read that thoseopen source embeddings models actuallydo especially for certain use cases giveyou really high fidelity and when youare talking about moving a system toproduction really getting kind of thepromised productivity gains and andthings like that you want to do everysingle thing you can to make the resultsbetter which is you know why you'retweaking different steps along the thepipeline right so we talked aboutmetadata extraction and chunking andthis is another piece where like you canyou can take your open source model youcan fine tune it to continue to improvethe resultsum it's definitely challenging though touse them in in certain ways like some ofthem are really big right like um Ithink it's like the extra instructor XLor something you need like 24 gigabytesof vram or like you know just just notthat many machines out there that can dothatum that are cost effective so you runinto these different infrastructurebottlenecksum but I think we're going to continueto see that become more of a normyeah I'm really curious aboutum I guess one question I have is likewe just released our like text-to-backgbt for all where it's using the ggmlacceleration like there's like llama Cplus plus I'm sure everyone's aware ofrunning llms on C plus uh you know onlike CPUs with C plus and stuff and soI'm kind of curious like is like what iswhat is the state of that like can youreally run it on a CPU instead of a GPUI yeah that's kind of one thing I mean II don't know if it would be worthwhilediving too into it I think the topicthat you brought up that would be moreinteresting to talk further about wouldbe that um that fine-tuning your opensource embedding model kind of thingokay I know uh Gina AI they're doingsome awesome work with their fine tunerand I'm just curious like what you'reseeing on um you know how common is itto fine-tune an open source textvectorization or you know any any imagevectorization any kind of model I don'tthink it's that common yet like we'vetalked to a handful of people that havedone it and the theme that we've heardis like the the complexity is actuallymore than you would think because likefrom what I can tell fine-tuning an openAI model is not super hard because theyhave kind of a nice interface for itthey have like your notebook out but Ithink when you want to work with an opensource model it's more challenging incertain waysum I think also it's it's not totallyclear yet what what levers to pull inthe space in terms of like what'sactually going to make your resultsbetter so I think I read that with finetuning you have to you need 10 instancesof something in the data for it toactually kind of appear in the modelweights and so if you if you don'treally have well curated data tofine-tune on then it's not going to havethat big of an impact whereas like youknow altering the chunks or the metadatacould potentially have like a way moreoutsized uh impact on your searchresultsI think another theme in general thoughthat is just evaluating right how do youactually even you know figure out whatsearch results are good or bad for yourdata and your systemum I think is a general challengeyeah well I think what's so excitingabout the well I I've talked about thisall the time because I love this topicwhich is like the um you know you canuse the language model to generatesynthetic queries for your documents andthen use that as you know jointly yourfind e you'd use it as your fine tuningdata for embeddings or you could trainlike a ranking model where the rankingmodels are like classifiers to take andquery and document as candidate and it'sslower but it's high higher Precisionyou know or you could use that kind ofdata set to evaluate different embeddingmodels like you know now you have asynthetic test set so you can say okayopen AI cohere sentence Transformerscompute for my favor and you could dothat kind of thing so yeah what do youthink about that kind of you know llmsto generate synthetic queries to measuresearch performance I mean I think it'sdefinitely an interesting idea I've alsolike played around with this myself likeI also like in more traditional machinelearning tasks have played around withusing llms to label data basicallygenerate synthetic label data I thinkwhere you run into a little bit of a ofa question mark in my view is that likeno two kind of production grade datasets have the exact same statisticalqualities and so how do you know thatsomething is good for your data set ifyou're using an llm to generatesomething synthetic because also likellms I mean depending on the settingsright they're they're inherentlystochastic and so like you're notnecessarily always going to get the sameresults in and out so you sort of feellike to really start to get somethingthat's meaningful in terms of a testresult you also have to run thingsthrough an llm a lot I'm curious to hearwhat you think of this idea like one ofthe things that we've talked about withpeople is if you know your data well youcan actually rather than use the L1 tocome up with the questions you can comeup with your own questions and havestuff as a yes or no answer you coulduse the llm to grade it or you couldgrade it yourself and then you have apipeline and you run stuff through thepipeline a bunch of times to kind ofcover that stochastic variability and solike let's say you have 20 questions andit's like yes you know or they're rightor wrong you can have a score out of youknow 20 or convert it to 100 right andthen you can do that like10 or however many times and you cankind of give an average and you have avariance and all you do is you just youjust toggle one variable for each testand then you can really isolate for mydata how effective is tweaking the topyou know K and top K results for exampleor swapping the embedding model orchanging the the filtering mechanism onthe map the metadata that's how we'vekind of thought about it but I'm curiousbecause you guys are you're so deep inthe space too like maybe you have atotally different approachwell I agree completely I mean it's likethe the the large language models justkind of like erased the the favor likethe popularity of like extractivequestion answering models where itclassifies the answer in the retrievedcontext so there's like no chance ofhallucination with that setup or youknow just having the search route you tofrequently asked questions and then it'slike you know the embedding similarityof the new question with the uh theembedding of the frequently Askedquestion and then you know Neva hadpublished this um awesome blog postabout like uh question to questionembedding similarity is a lot easierthan a question to answer because I kindof like the style and yeah I thinkthere's a lot of interesting things tooyou know that kind of component for sureandyeah it's it's like a I mean therethere's so much Innovation happening onuh like what the search box looks likelike um you know I think by the time wepublish this podcast I I don't know if Ishould say this because then I like haveto have the podcast publish after thisbut let me just not say it so like we'reworking on there is a project indevelopment at alleviate that's like uhyou know building those UI componentsfor the search box with the site sourcesand like you know the UI for it there'slike you know a lot of collaborationshappening with like versel and replit uhyou know I'm personally not involvedwith this but I'm watching this of likeyou know trying to make it easy forpeople to like plug in front-endcomponents and there's so much like youknow whether you're using extractive QAor whether you're putting it to a largelanguage model like how you do the userinterface is definitely a bigopportunity you know like how we'retalking about Vector flow and uhingesting data being a new marketcategory developing these kind of likefront-end components like if you havelike a library of react componentsthat's also a pretty nice little youknow as everything is developing furtheryeah one of the things I was going tosay we're actually noticing the samething too where it's like with Vectorflow we're even trying to understandwhat is what is the entry point to thesystem for example like you know westarted out with an API uh as the entrypoint to the system because that's umit's technology agnostic rightvectorflow is designed to be Cloudagnostic too but we're also seeing likeif you want to rapidly test and you wantto iterate like you know you probably dojust want a UI to configure stuff and soI think that's in some ways a sign ofthe maturing of the space where like itstarted out right with a lot of peoplethat are research scientists datascientists traditional softwareengineers and now it's as it matures andit enters the mainstream we have tostart moving to thinking more aboutusability and approachability for theaverage person to kind of get the mostout of these systems understandabilityso I think there's a lot ofopportunities in thatum the vector flow is is really usefulfor running rap testing Cycles I thinkthe whole industry collectively stillhas a lot of work to do aroundunderstandability why do you haveresults that you haveum and kind of you know having you knowTracer I think they call it Tracerbullets right in software engineeringways to understand what Pathway to theanswer did it take how can I repeat thisand yeah things like that yeah no Iagree 100 with that like the the abilityof the zero shot models to evangelizebuilding with deep learning tools forlike software Engineers has just beenthe probably one of the biggest themesin the story of wevia in my view is likebeing able to just take the open Aiembeddings and then you can build asemantic Search application withoutneeding to have the slightest idea inthe world about how you would fine-tunean embeddings model which I think isquite a difficult task and so this iskind of like I'm doing like a long rangereference back to uh the metadataextraction and kind of uh chunking withL I really want to kind of get for likethis kind of um you know importing datainto Vector databases I think theinteresting thing is I think there'seven more opportunity for zero shotmodels to be plugged into this workflowand particularly I'm curious what youthink about the idea of like an llmthat's been trained zero so obviouslylike gpt4 could probably do this task ofwhere you give it that Wikipedia passageof Jimmy Butler and you say hey whatkind of like symbolic metadataattributes are contained in this passagethat you might want to extract and wouldgo oh you know age 28 let me take thatone out or this kind of idea of um yougive it like some messy chunk of textand you say uh you know where where doyou think you should split this text asanother kind of thing that an llm coulddo one kind of idea is thatyou know they're gonnafine-tune or specialize llms that aremaybe only like 15 billion parameterslike 15 billion parameters I think thecurrent state of the art in text to SQLlike zero shot so fine-tuning LMS toplug them into kind of that part of thedata importing stackyeah so I think umlike to if you think about higher levellike what kind of chunks you want tohelp with the search result right so ifthere's different types there are chunksthat have like relevant information inthem there are also chunks where likeyou know is it keywords in the chunks isit the whole kind of simple idea of likethis is a sentence or this is aparagraph right and then there'sum things that are more complicated sofor example like I'm sure you've heardpeople talk about summary chunking andhow like you want to do summarizing andit's like you get into some questionsabout like what are you summarizing yousummarizing a paragraph a page adocument do you need to be smart enoughto understandum like oh this is you know section twoyou knowum part four and so you wanna likehowever many pages or paragraphs that isyou want to summarize that and I thinkthat's where llm especially ones withdecent sized context Windows could bereally goodum I think that the trick is like chatgbt you know especially gpt4 can beexpensiveum sometimes the API goes down thingslike that so if you have like a modelthat's specifically designed to dosummarizing zero zero summarizing thenthat could be a really useful thing toinsert into the flowum likewise with kind of extractionright like you could have a model that'sjust trained specifically for thatumand use that to sort of like extractkeywords or extract themes um I thinkfor us one of the things that we'reexploring that is kind of interesting islikeum can you actually approach theperformance of a deep learning modelwith something more traditional that'sway faster and way cheaper so forexample like could you use xgboost toum do sentiment classification and justput those sentiments in the metadataand that would be like substantiallyfaster and cheaper maybe it's not theperformance is not quite as high as anllm but you you get other benefits fromthat approach that allow you to makethat trade-offyeah I I see kind of two things likethis summary chunking and let's maybecome back to that I want to I want tostay on this XG boost for sentimentclassification this is what Nils uh tookapart with my argument on metadataextraction that I think is extremely youknow I I think it's bulletproof is likeyou know if you're letting the llm justarbitrarily decide what metadata toextract from each chunk then how do youthen query it whereas alternatively ifyou have some kind of classifier thattakes a you know let's say you'vetrained just like I don't know if thisis the best example but you've trained aclassifier that takes in uh Wikipediapassages of NBA players and thenclassifies the age if it's mentioned ormaybe the team they're on I don't knowsome like maybe like how many seasonsthey've played if that's maybe somethingthat's kind of ambiguous that you wouldneed to kind of derive from the textsomething like that right because if youtrain a classifier on one special kindof metadata attribute it's just so mucheasier to then interface it at querytime because you know that like you knowhey there these many this many ages agevalues or teams I don't know what themaybe we should have started with abetter example of like a metadata toground this like or sentiment why don'twe just come back to sentiment like sothen then you know at query time you'regiving it a movie review you want tofind the most similar positive moviereview right because you have this kindof label on it yeahyeah hopefully that kind of said thetransition nicely like what do you thinkabout that kind of or just to keep kindof giving our thoughts on like this kindof idea of having a specializedclassifier for the particular kind ofmetadata you want to label each of yourchunks withI mean I think I think that's just likea kind of a broader trade-off that youhave to think about making with any useof a large language model versus abespoke machine Learning System like afile read we we looked at usingdifferent Technologies to parse um PFSum and ended up building our ownclassifier because like we had veryspecific needsum in order to be able to understand thedocuments and I think it's all aboutunderstanding the data right and so likeyou're at you're using metadata as kindof a way to understand the data and thenadd additional reference information tomake the search better and so it somepart of it comes down to the theengineering requirements and the thebusiness requirements around like costlike do you have time to train a customclassifier is it easier in terms of thedeveloper workflow to just use an llm orif there are these off-the-shelf oneslike do they easily fit into theworkflow I think I'm curious to hearyour point on this but one of the thingsthat we're observing in our discussionsis people are using metadata in reallydifferent ways like it's not yet superclear like what fields are common andwhat things you want to put intometadata so like I think to answer yourproduct question that approach soundslike it would work better right becauseI think for any specific machinelearning task you probably could train amodel that could beat an LOM at doingsomething very specific it just is aquestion of likeis that specificity what is needed inthe markethmmyeah I guess it's like um if I'm if I'mimagining I'm building a chat box or asearch engine for like we V8 questions Imight want to classify them intoquestions about importing data questionsabout the query apis questions aboutlike replication scalability like Cloudmanagement stuff so I might have thesekind of categories that I would put onthe chunk but like what Nils was sayingthat I kind of agree with is likeum if you have some label that you canclassify from the content alone you canprobably capture it in the embedding aswell like the embedding model couldcapture that kind of thing in itsrepresentationand that that's kind of what I see yeahI I guess for me the the reason I likethis whole argument is because I likethinking about these systems as likelike I like to try to point people tothe weeviates aggregate API like we V8it's a vector database but you also cando symbolic queries in weba like if youwant to say what's the average age ofNBA players you you can ask that kind ofquestion to alleviate so that's kind ofwhat I see is the clear value of themetadata you know for for kind of queryparsing and adding filters to queriesI'm not sure I understand it all the wayI understand definitely like as one ofour earlier conversation topics like apart I imagine with the end-to-endimporting data to wevia is like youimport it from multiple sources like youhave tweets you have GitHub you haveYouTube you have uh slack and so youmight want to put those into like youmight want to have a symbolic filterthat uh you know separates that at querytimeyeah I mean I thinkthe space is so young right that I thinkone of the things we're going to seeemerge is sort ofmore standardized access patterns let'scall them right and I think that's oneof the kind of Visions for what we'retrying to achieve with Vector flow is tobe part of this process of likebuilding almost building like an HTTPfor for kind of querying your your uhsemantic your contextual data right solike not HTTP itself but this idea of aprotocol standardized fields and thingsthat you're going to access so as thespace matures we might understand likethese are the types of metadata that arereally important and that most peopleuse and then it does make sense to haveoff-the-shelf classifiers uh to do thosethingsto extract those thingsyeah because I well yeahyeah well let me get your temperature onthat argument of if you can classify itin the content the embedding can alsocapture ityeah I mean that makes sense high levelI guess like for me it's like thevectors are so entangled like of youknow like you know Vector representationof an object like from open AI the 1536Dimensions there's you can't like pointat two indexes of that vector and saythis is how much of a cat it is rightlike it's it's completely entangledacross the entire Vector so that'sthat's why I still see like if youclassify like give a picture of a dogand like yeah the embedding capturesthat it's a dog but if you have aclassifier that uh does the like itmight be a dog jumping to catch afrisbee or it's like in a pool orsomething and so it's kind of likeentangled with like dog but also likedog swimming kind of like so so I see ifyou have some kind of attribute that youknow is going to be super important likewhat kind of particular dog it is toclassify that and have that in additionto the embedding I could see that asbeing some kind of lever that couldimprove your search relevance yeahabsolutely I think we're agreed on thatI think one of the things that I'mcurious about maybe because we're soVector flow is more on the ingestionside right and I'm I think at a certainpoint once you have the information inthe vector database you need to beclever about how you retrieve it rightso I think that's for like also certainthings like re-ranking can come intoplay where like okay you have all thesethese different concepts of a dog in thevector embedding but it's like how doyou actually turn that into like theright kind ofoverall view of the information toextractum yeah yeahno I think that's so it kind of comeslike into you you mentioned writingcustom code to parse PDFs and like youknow unstructured is a really excitingcompany that's doing like um one I meanI think there's a lot of cool parts toinstructure but one thing that Brianexplained to me on the podcast was aboutuh doing like a visual document layoutand so you know you visually put thedocument and you would say like and itsay like this is an image caption and sobecause then yeah and then everythingyou do at importing time then is goingto impact what you can do at query timebecause then if you want to have someyou know it could be a filtered searchwhere you only want to search throughimage captions or you want to have likea ranking kind of uh like a rankingequation where you say like boost theresults if it's a image caption I don'tknow image caption is the best examplebut like that kind of thingyeah absolutely I mean unstructured is areally uh really cool tool we're playingaround with adding it to uh tovectorflow to help with certain thingsfor example like how would you askquestions about like a pitch deck or acardboard slide and because maybe likeyou know that's portrayed as it'suploaded as a PF which our systemaccepts PDFsum but a lot of the information embeddedin that might be visual or spatial Ithink an interesting idea is like takethe idea of an infographic right how andlet's say you have a textual system thatwill be very if you have an infographicin a PDF or a word doc or something howdo you convert that into something thatyou can represent textually in thevector database for example and you knoware you storing multiple modals in thesame index or are you is there a way todo that transformation I think this ideaof having tags about images as wellum could be could be another interestingway around that but um congestionproblem in general like there's so manydifferent combinations of things thatit's really exciting to think about howhow you can best represent those in avectorized form and make the theretrieval results ultimately betteryeah I'm I'm so excited about thisconversation topic infographic brilliantexample I haven't heard that before andquick shout out to instructed with theuh shout out I hate that word I hatethat I just said that but like a goodguy like to unstructured who just didthe like um processing tables visuallyAnd Turning Tables into uh searchablethings that it's really inspiring workand so I'm I I want to kind of talkabout I had this idea as you weretalking like um you know with our wholeconversation of metadata extractionmaybe it makes sense to create a newclass and weviate that only has like 50objects in it but they're really likesymbolically annotated and that's whereyou would store the kind of informationneeded for these symbolic SQL stylequeries like you know if you havecountry music singers and like theingestion engine is parsing through anentire Wikipedia page maybe it justmakes like one data object to put inthis other class which is like symboliccountry music singers I don't know andthen all that but then all the chunks goto this other class which is like thesector class and so you so you separateit into two kind of classes in sort oflike a weevia native way of thinking I'msure all the vector databases do likesome kind of abstraction between classessome something that sounds like thatyeah absolutely I mean um I think howthe different informationwill ultimately get tied together isstill an open question that's why I wastalking about like what is going to bethe solidified data access patternumyou know perhaps like it makes sense toto have certain information in thevector database and because the vectordatabase right it's not the result thatyou get from the vector database you'regetting things that match your yourinput query right but perhaps like inyour re-ranking pipeline or somethingyou then go and you hit your SQLdatabase to do certain kinds of searchesthat they're more optimized forum I think I I think these kinds ofquestions will the patterns and the bestpractices will become solidified overtime but I think it's an interestingidea to see like what else could you putinto a vector database you know and I'mcurious for example like I brought upthis idea of multimodal like how howwell those things will play together youknow like in infographic in in a PDF forexample that has a lot of textum it's not something that uh Canadawe've we've played around with yet butit's certainly I think an exciting NewFrontier because if you think about likemost data in like a corporateenvironment it's probably going to bemixedyeah no it like it it like I feel likethe like with the world of vectordatabases I feel like there's kind ofthe power in the hnsw implementation andthen all the database kind of stuffaround making that like crud compatibleand scalable and all that kind of stuffand then I see like these two layerswith like the importing data into it andthen kind of like the query engine thequery interface and then of coursethere's like the language model which isanother like major like PowerPoint I'dsay because of how of like the mode oftrying to create another one but likethat kind of um the query engine part isquite interesting because you knowthere's like the llm Frameworks Marketwith like llama index Lang chain uhHaystack Gina and and others um maybeforget off the top of my head but likethat they they would do like this kindof um you know like one thing like oneof my favorite abstractions in llamaindex is the query router where the llmtakes the query and it says is this asemantic search query or is this an SQLquery or maybe it's like a KnowledgeGraph query too that's where I thinkthese when Frameworks will like alwayskind of sit in an interesting placebecause they can like combine SQL withVector search with like these KnowledgeGraph or like graph databaseTechnologies and I think that's a reallyinteresting kind of component to it butI do think it's likely that umyou know and I'm like it's like maybe Ishould like get to working on this rightafter we hang up the call but like Ithink it's quite likely that we canbuild some kind of interesting uh queryrouter into leviate for this is it'slike a little interesting like how thedesign will work but we via has thismodule system where like you know weinterface our near text to do Vectorsearch or hybrid search our hybrid youknow query API as well as re-ranking andso I think we can keep going up like dowe want to go so far up to have likeChain of Thought and like Auto gbt stylequery engines directly into weeviate I'mreally not sure but I do think that kindof query router thingit's gonna be a pretty important part ofit I think I think I think you actuallymight talk about this the first time wehad a discussion I may have also talkedabout it with the the head of researchfrom weeviate that likeum this idea of query optimization rightit's like that's a that's a big thing intraditional databasesum where you have you know entirein modules built into the system thatcan kind of translate things to optimizedepending on what you want whether it'slike a smart way to index or you'reoptimizing for Speed or whatever and inin terms of the performance of the querylike as measured by the Fidelity theresult right I think that's actually areally interesting Frontier for Vectordatabases and you know can you put um aquery uh sort of Optimizer in front ofit and like you know you were describingas a router but what I almost imagine issomething that can kind of reword thingsand whether that's like you know amachine learning model like a largelanguage model of some kind thatspecialized to that task orum whether it's it's the exact approachI think is unknown but this idea of likeonce you once you have enough data abouthow different queries perform you couldactually start to alter the wordingum to make to make them better I thinkis actually a really exciting Frontierit's it's outside of the box of whatwe're what we're doing at Vector flowbut it's definitely something we'veheard people ask for pretty consistentlyin the discussions it's like yeah we'renot exactly sure the best way to hit thevector database to get the best resultsum and it would be cool if you guysbuilt that because you probably have allthe internal data to know like whatactually does constitute like a goodquery versus a bad query for the samefor the same indexwell this is so much cool stuff in whatyou just said I think start yeah likekind of yeah like the query formulationthing maybe just come on that and thenand then hit it with the big payloadthing guys change the conversation topicwith this but like yeah that kind ofidea you take like people are sendingprompts and so you don't want to justsend the prompt as the search query mostof the time you'd want to like you knowextract like what is a search queryparticularly send that to the vectordatabase and then that's kind of how itall gets combined with the prompt andthe new search results but here sohere's kind of the thing that I think islike really exciting uh so I'm still inthis term from Andy Pavlo who's aprofessor database is at Carnegie Mellonwhich is uh self-driving databases soit's about databases that kind of likeoptimize themselves the indexes theybuild based on the queries that they'reseeing in coming in so like you know ifyou're like uh uh like a payrolldatabase and you're seeing thisparticular kind of you know let's sayyou have like 30 tables you get likethis crazy complicated payroll and soyou're seeing this particular kind ofjoining pattern being queried a lot youwould start to like build up an indexfor that join and so yeah I'm reallycurious like this also comes into thisother topic at we get that we like a lotcalled um generative feedback loops orgenerative feedback loops are like youtake some of your data you put it intoan llm and then you save the resultinggenerationso so kind of marrying these ideasis like you're seeing particular kindsof queries coming in and then your llmcould kind of like transform your datain your database to better accommodatethese queries and I might I might bediving in too much let me like get yourtemperature on the idea at a high leveland then we'll kind of go further intohow this might work I mean I think it'san interesting idea as long as youbasically accept the premise that avector database doesn't provide the sameset of guarantees that a normal databasedoes right like you know with SQLdatabase you have like the acidprinciples likeum you know you want to make sure thatthe data is always like the transactionsguarantee the Integrity of the data andcertain things like that I think ifyou're treating the vector database as astore to to Really providesome kind of system capabilities likearound end user performance rather thanlike we need this data for this systemto kind of have its core operation likelike to store itum to back up what's happened and to bea member but rather like this enableslike the core flow I mean you acceptthat those are different premises Idon't know if I'm describing itum the most adequately but I think youget what I'm saying then it's a reallyinteresting idea because you can you canbasically tweak things to to give betterresults and I think it's it's almostlike the opposite approach of what Isuggested where rather than optimize thequery once you figure out what theaccess pattern is going to be you canthen kind of tweak itum what you're almost describing to mesounds kind of like a like a recursivefeedback loop along like okay we haveour initial chunks we put a bunch ofstuff in them now we have this feedbackloop built out and we see how people usethis let'sum you know let's alter these chunkslet's continue to evolve them where Isee some complexity associated with thisisit's my expectation that what's in thevector database is not going to bestaticum I think there's some scenarios wherepeople will ingest stuff once but Ithink one of the problems that we'reseeing is more like you have to syncdata you have to ingest stuff like oneof the things we're building in a vectorflow is like a dedupe mechanism so theidea that you're not gonna um you knowadd duplicate vectors when you have toupdate and so how would this idea ofhaving a feedback loop that alters thecore uh you know embedding in thedatabase play with the fact that youalso need to like constantly refreshdata in a production systemyeah it sounds it is it sounds reallyexciting I mean I guess it's like maybeyour queries tell you something aboutthe data you didn't originally that likemaybe the queries are telling you justlike to add this kind of filter likelook for this kind of thing particularlybecause I'm seeing this feedback wherethey like these kind of answers I kindof like that long tail versus thepopularity kind of thing and yeah Idon't know exactly but I think reallythe the the the key thing and how thisties into like how you think about howyou import data is like um just probablyuh I guess it's there there's definitelylike levers of control here like withthe um so kind of coming back to likethe self-driving databases in more of anSQL sense is like you can build upcertain index structures for differentkind of properties but then speed up thelatency of making those kind of queriesand Vector search has a similar kind ofanalog with the hsw graph where uh ifyou know that you're always going to bedoing this one kind of filtered searchyou can like increase the connectivityof that kind of surf so you know we theway that filtered hsw filtered search inhsw is implemented is you have like anallow list and it says traversing thegraph and it's saying you know are youon the allow list well no okay well I'mstill going to explore your neighborsbut you're not going to be in the resultset and so if you know that you're doinga particular kind of filtered search allthe time you could kind of connect thosefilters and then that allow list willyou know be pop you know the result listwill be populated quicker and so you'llhave a faster latencyso I see that kind of angle to it whereyou're just trying to speed it up by byyou know what queries you're having soyou're going to index in advance but butthen there's maybe something to likethe queries tell you something about theinformation that causes you to thinkokay I need to change uh the informationthat's in my chunks or like somethinglike thatyeah I mean no it's definitely aninteresting idea I mean I'm just kind ofthinking out loud here that like youknowif you have the capacity to do it likeyour organization you know you can youcan afford to have multiple differentuh versions of the data stored rightlike you could even think about havingthings in different actual graphs andsome sort of proxy layer in front of thedatabase that kind of decides at runtimelike which which index you want to hitum I'm just kind of trying to connectthe dots with like I know traditionaldatabases have proxies that do a lot ofinteresting things it's more I think onthe performance side like connectionthere sharing and things like that butlike could you combine this idea of likethe kind of self-driving database withthe proxy layer that that provides theyou know a smart interfaceumto to actually make the the resultsas good as possibleyeah no I that sounds really exciting tome because you like coming back to thehnsw thing like having two separateclasses one class is like this you knowoptimized for filtered searches and theother one is just the ordinary hsw andyou have like these two classes and youknow you have this kind of proxyorchestrator that's saying you know I'mnot getting enough filter to search thisanymore let me go ahead and destroy thatbecause the memory overhead doesn't makeit worthwhile anymore andyeah I did I think there's a lot to thatI I think it's a pretty interesting kindof Direction and I the reason I think itmight be relevant to you know Vectorflow and managing Imports is I think alot about this kind of like uhdegenerative feedback loops and you knowthis is Bob's idea I'm just like sharingit but like this like um having like arest API around the generative feedbackloop that would um you know like so likeone example is like when you'regenerating synthetic data from your forfor your task or whatever you every like10 you might want to like back up yourdatabase because you're paying for theseLM inferences you want to you know saveit if you potentially have it crash butuh if I save it every 10 and you knownow I have like the the newest one after100 it's like go ahead and delete those90. that's kind of like theinfrastructure management task I see islike okay I don't need these backupsanymore go ahead and delete them it'skind of like like a garbage collectorfor the data backups so I could see thatkind of thing being managed by anexternal service rather than leviateitselfyeah that makes sense I mean I think ingeneral it's kind of like the differencebetween the vector store and sort ofthis whole Vector Ops life cycleum and that's part of it along with juston you know it goes intoum understanding when you want torefresh the data and that's a big partof ingestion as well and so I think Ithink the way that you seeum tooling built up around traditionaldatabases I expect you'll see toolingbuilt up around Vector databases to helppeople get the most out of out of thosesystems I'm curious kind of like whatare you guys sort of encountering on umthe the sort of educational side ofthings because we're talking about allthese capabilities that we see and howwe could see stuff evolving but one ofthe things I sort of notice in a lot ofthe discussions that I have is thatpeople still need to really beintroduced to the idea of like what canyou really do with the vector databaseand why do you need to put it into yourinto your system architectureum so I'm curious like how do you seethis becoming more more approachableto like the general kind of communitysoftware engineering community as awholeum well it may hopefully not tooredundant but I do think that kind ofthe zero shot model like what makes itmore accessible is the zero shot modelssorry let me can I ask you one morequestion before we come into this topicis sure quickly I just wanted to askabout uh the gbt cash idea where you youtake llm responses and then you cachethem and so now you take a new answerand a new question and you would map itto its nearest uh like you take a newprompt you map it to its nearest promptthat's already been answered and thenyou give them that answer and so I sosorry to let me like cut so back intothis conversation sorry if you don't Ijust I just want to get this last songand then we'll yeah and then I'd love tocome into the topic of how do youevangelize Vector databases but likethis um yeah gbt Cache is like uh youknow the llm does some kind of responsewith your data and then you save it it'sso it is like a generative feedback loopwhere you save it back in the databasebut it's for the sake of you knowquestion answering in the future sureyeah have you seen this kind of gbtcache thing yeah we actually looked intoit um when we were first exploring likeyou know different ways to deal withsome of these problems because like youknow you do part of getting the bestperformance out of the vector databaseis like you know probably to do somecaching so that you don't have to hit itright tons of traditional systems havecaching of of one sort or another Ithink the trick is like to to do smartcaching in a way that's not reallyexpensive or really burdensome in termsof the memoryum I think that's where actually thisidea of like a zero shot llm that isdesigned specifically to to kind of dosemantic caching could be reallyinteresting where it's like you don'tactually have to hit chat GPT maybe youcould have a a model that's you knowmore parameter light that could even runyou know like on a much smaller devicelike not on a GPU that just enables thatto be really efficient because it's notthe problem right is it's not like atraditional cash where you can haveum like a key Value Store you need toreally be smart about how you decide tocashand and what you decide to cashI it's interesting that that kind of keyValue Store analog because I I know alot of like you know researchers thinkof like you know deep learning is justlike glorified hash tables and you couldthink of embeddings as kind of like akey hash and so it is it kind of relatesto that idea but I guess the the thingabout gbt cash and how I'm seeing mostpeople using it right now that I find tobe interesting with this more likeautonomous self-driving database kind ofconcept is like usually gbt cache istriggered by a real person came andasked a real question and this is thereal answer that they liked therefore inthe future let's use that answer againwhereas imagining like an llm that'sjust like you know prompting itself withyour documents and creating moreinformation and like would you ever wantto return that information that'screated by an out like an llm retrieveslike two parts of your documentation andit says I don't know write a comparisonof these two or like think about how youmight combine like if it you take likeAtomic apis and it's like how might youwrite a compositional API of these apisand and it creates like new data andwould and would you want to then indexthat and maybe return that for a searchso if I understand what you're sayingare you basically saying like you youuse the Ln to populate the cache kind ofit's like so yeah yeahyeah definitely an interesting idea Imean I think you'd have to like somehowhave some some human feedback in thesystem that's something we actually it'sa topic we've kind of skipped in this inthis podcast maybe the next one we'lltalk about ituh come into this system but it'scertainly an interesting idea and Ithink it goes well with the generaltheme of like at some point once theseTechnologies mature you'll be able tokind of use them for recursive sort ofimprovements where they can just makethem look better with some some humanguidance and I think that the idea ofapplying that to cash is definitely aninteresting mechanism as opposed to likeyou know the more crude cachingstrategies that we have now you knowlike least recently used and you knowputting time and cash and having maybebetter ways to assess like what reallyneeds to be stored hmmyeah yeah I love this topic it was sucha fun conversation and yeah I I thinkthere's so many interesting topics aboutall that and I think yeah I do like Ithink this kind of external uh documentmanager outside of wevia could eat up alot of a lot of functionally what thiskind of stuff ends up looking like butyeah let's take the step back I knowthis is like dreaming into the future Ijust like I always like doing that butthis kind of topic of um you know how doyou evangelize these new technologies Imean maybe let me first ask you whereyour head's obviously you know you'restarting this new company Vector flowand you know trying to you know get offthe ground get some content get someusers and all that is maybe like yeahhow do you see that in your focus andhow are you currently thinking about ityeah I mean I think it's um I think forus we're really trying to moreunderstand the axis patterns aroundVector databases right now and that'swhy I was saying like you know we'restill waiting to see how those maturelike what are the common like we talkedabout metadata what are the commonfields or you know like what are thecommon data sources what are the commonformats what are what are the main kindof issues that you're encountering withingestion because I think Vector flow isdesigned really to be sort of thisum like let's get to production quicklytype technologyumbut I think there's one of the thingsthat we're seeing is a lot of peopleusing llms that just don't really fullyunderstand the power of semantic searchand the wide variety of use cases rightlike people know about to some extentlike this idea of ask questions of yourdata but for example a lot of peopledon't know that like you need to use uhsemantic searches for example in facialrecognition or like I was actuallythinking this morning about how likethey maybe could use it for inspectingrocket parts right because like theseRockets have I think I want to say thisis a number off the top of my head but Iwant to say there's like two millionParts like Falcon and rocket and it'svery precise and like what's thedifference between a test part versus aflight part like it's minute and could aconventional model by itself just doinginference detect that or would you alsowant to do some kind of more complicatedsimilarity search I'm kind of going downa rabbit hole here but it's likepeople still don't know about the broadapplicability so I do feel like there issome degree of Education that we have todo as well because we are a pipelinethat is dependent on Technologies likeLevy so people need to first know likewell what should I be using bb8 for it'slike oh okay I need that well yeah andalso like oh that makes sense and if Iwant to get stuff in easily and get toproduction quickly like oh Vector flowyeah yeah that that um that inspect theuh rocket parts that that's one of theuse cases I've seen with levia that justalways stuck in my brain is um it was uhAirline manuals and like searchingthrough a particular kind ofconfiguration of that it just it alwaysmotivates me to talk about this metadatathing and grounds my thinking andchunking just because of seeing whatthat kind of thing ends up looking likebut yeah so exploring all the differentuse cases that can exist of uh you knowVector search technology very broadly II completely agree I think like I don'tthink we have a good sense of whatmultimodal retrieval augmentedgeneration is going to look like yetwhere you know I imagine it in thee-commerce case where I'm like you knowshow me what I would look like with awhite pocket hat it's something likethis like where it retrieves the imagesof me and it like fuses that with theimage search of bucket hats and it likecombines these like this kind ofcombination of like um modalities inretrieval augmented systems that yeahthe applications certainly are stillgrowing yeah well I I would like topitch you one more thing kind of likefuture looking thing about what I thinkis going to heavily impact how youevangelize Technologies broadly is thegorilla large language models so withthe gorilla large language models what Iwould imagine a user doing is you tellme a little bit about your data likeyour data Maybe gorilla designs a schemafor you and then you say okay build mean ingestion flow from my notion toleviate using Vector flow and gorilla isan llm specialized on the apis that youknow like levia in Vector flowcollectively build and gorilla does allthe intermediate stuff so like you knowwriting all the python code for doingthisyeah what do you think about that kindof idea I mean it's definitely aninteresting idea I mean I think likeit'd be interesting to see how thatwould fit with Vector flow because ourwhole like one of our overarching designprinciples with sector flow is like toto just try to limit system complexityand it's definitely a challenge becauseum you there are a lot of things thatyou need to tweak and so our idea islike minimizing how much code you haveto write having you know just a simpleAPI endpoint or a config file and so Ithink um if there's a way to even removethat complexity it gets you toproduction faster I think the otherthing like vector flow is really usefulfor testing right we were talking aboutlike the need for more kind offormalized testing strategies and um theidea that you you probably need to toreally figure out which hyper parameterswork best for your retrieval system youneed to test a lot of things iningestion so this idea of the gorillamodel just enabling you to go throughthat cycle really quickly or even doingit for you could be super super cool itcould basically just remove a lot oflike more grunty aspects of setting upthese systems in figuring out exactlyhow well uh like the best way to makethem play nicely together I think is asuper interesting ideayeah because I think like with Vectorflow you know you're going to createthese this documentation that shows youhow to do the particular syntax and thenif you have a language model that I canjust tell you like you know roughly whatI want to do and it just translates itinto the vector flow apis now thelearning curve for me a new Vector flowuser becomes a lot lowerI think for weviate and like you knowit's a bit more interest like we it'spretty interesting because like for uslike uh users need to learn like thesearch API so like you need to learn thesyntax to like put two filters on yourvector search query and so thatparticularly that language of being likeadd these two filters and it generatesthe graphql that that excites me a tonyou know it's kind of like the text SQLstuff but yeahbecause I feel like it just when you'reevangelizing your new technology youhave this like learning curve like youknow learn my syntax learn my new youknow my library that kind of thingyeah definitely I think that's that'swhy we we're we tried to stick with HTTPas the first way like an entry pointinto the systemum but it's there's a lot of differentthings to tweak for example like youknow a lot of traditional tools likeallow you to pass in Python scripts torun specificum Transformations and I think one ofthe things that we are exploringactively right now is like what is whatis the main flow you know and what iswhat what where are the real barriers toto entry in terms of getting things toproduction we know that one of those isinfrastructure which is whyum you know we're building what we'rebuilding but I think anything thatreduces that friction whether it's alarge language model or just you know awell done integration between twosystems I think is ultimately going toallow people to extract a lot more invalue from these systemsyeah yeah I agree 100 yeah and I justthink kind of the thinking about thefuture of what these Integrations aregoing to look like is pretty excitinglike how yeah like you you mentionedlike HTTP and like networking protocolsand it's I guess it's almost like thatlike if language is the interface nowbetween all software tools forintegration sort of like how you knowHTTP requests are the interface betweenlike networking endpoints it's andthat's kind of how I see the future ofthat kind of part of it but yeah anywayso yeah I love talking about theGuerrilla language models because I justthink it's a pretty cool topic butanyways uh David thank you so much forjoining the weba podcastyeah I I really liked our kind ofinitial tour of like jointlyunderstanding the end to end ofimporting from you know connecting thesedifferent data sources the kind ofmetadata chunking visual documentparsing all those that kind of topicsand then how do you vectorize and then Iguess the the last networking to get itinto the database it's such an excitingspace and you know thanks to theconversation around the self-drivingdatabases generate a feedback loop I hadso much fun discussing that and you knowthank you so much good luck witheverything yeah thank you so much forI'm super super cool to do a discussionlike this where you just um you knowyou're in certain waysum getting a very specific exposure thatI love to hear that the new ideas thatcome up and then we can chew on thoseand think about how we could put theminto our system so it's a lot of funsuper excited to see uh what happens inthe space", "type": "Video", "name": "David Garnitz on VectorFlow - Weaviate Podcast #66!", "path": "", "link": "https://www.youtube.com/watch?v=efAvOsO8Gik", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}