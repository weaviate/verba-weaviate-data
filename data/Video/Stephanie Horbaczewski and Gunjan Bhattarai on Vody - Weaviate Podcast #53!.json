{"text": "Hey everyone, thank you so much for watching the 53rd episode of the Weaviate Podcast!! This episode features Stephanie ... \nhey everyone thank you so much forwatching another episode of the weviapodcast I'm super excited to welcomeStephanie horvachesky and gunjanbhataraii from voting we've beenfollowing on with voti for a long timealleviate they're building all sorts ofsuper exciting things with multimodal AIcombining different modalities from textand images this kind of idea so thispodcast is going to dive all into thattopic and learn about what voting isworking on so Stephanie and gunjanfirstly thank you so much for joiningthe podcastthanks so much for having us found outyou were one of our first startupfriends we're really excited to be hereyeah thanks so muchawesome so could we start off with thethe story of voti the problem that'ssolving in the founding visionyeah definitely um so I'm a second timefounder my first business uh was ahundred million dollar a year businesson top of Google and Facebook thatworked entirely with unstructured dataso when Bert came out I I grabbed acouple of my technical team and left uminvested some of the money I made andstarted a research company we did threeyears of projects working on embeddingsuh worked with Comcast HBO moved intoretail with Nike and then spent a yeardeveloping our product with Wayfairwow amazingyeah I'm so so if we could pack unpackthat a little bit so you're alreadyworking with kind of uncerted data yousaw the Burt Revolution and then youinstantly said multimodal is next Ithink or was it you grew into multimodalum you know what really happened is Isaid recommendations are bad and whyum I didn't understand I felt like therewas a ton of data and it didn't makesense to me that we were returning suchpoor recommendations uh which is when Idiscovered that what a customer is usingto make decisions for example um imagesq a reviews structured data is not umwhat's being used to make thoserecommendations and so I thought if wecould bridge that Gap we're going tohave some really transformativeexperiences in e-commerce and so we setout to build those specific models uhfine-tuned for the e-commerce mediaspace multimodal models to use all thesedifferent types of data that's sointeresting I've been yeah I've beenlike dabbling with recommendation andbuilding kind of like apparelrecommendations to illustrate our refdevec feature which is a way that we'vebuilt in to have recommendations withembeddings I recently had odsce listenedto mod hofthaker describe how they useuh embeddings at Shopify forrecommendation and you know hisexplanation of the business impact tothat was incredible and understandingthe scope of it so could maybe isleaving on the business CEO hat uh youdescribe new experiences withrecommendation can you kind of unpackthat a little moreyeah I mean what's amazing about theembedding is it's bringing all of theunderstanding about a product so youknowum a real complete human level ofunderstanding is going back to yourmodel to make recommendations so youknow when you'reum when you're looking for a pinkfuchsia chair with wooden legs umthere's not enough data labeling in theworld right now to do that but if youhave an actual understanding um of theproduct and so imagine maybe you cantalk a little bit about um ourmultimodal product embeddingssure so I'll go a little bit further onthe fucio chair and be like most modelssimply don't even have the capability tohandle that today and so some of thecore concerns of that is for example oneyou have to detect colorin a image and that involves Vision atthe same time though there may bemultiple objects in an image maybethere's something specific in that imageyou want the color of and so you sort ofneed to be able to guide the model as towhat you want and that would involvenatural language so that's one of thecore opportunities in multimodalpreviously we'd only have someunderstanding of how things might lookfrom an image perspective or a textperspective it's like someone who haspreviously illiterate but had perfectvision and then some person who had aPhD but was entirely blind and so we'reable to combine these two extreme formsof knowledge into one person andbasically have a model that knows allthe latest advancements in science andtechnology and like all sorts ofdisciplines but is able to actually seeand understand how the world looks likewith their own eyes so multimodalembeddings essentially deliver on thatpromise so the idea is that products aremultimodal in nature they have textualfeatures for example productdescriptions product titles reviews theyhave images namely the images thatyou're trying to have someone sell forand other details to like variousmetadata Fields things related to whatother products people buy maybe someproduct videos lots of other things andso the idea is if you can jointly alignthese types of representations togetherand be able to have a model thatunderstands all these we're able to haveembeddings that are able to understandthese products really well and you canunderstand what's similar what'sdifferent and then people can buildmodels on top of them and build searchthat's able to leverage that Newfoundunderstanding fascinating I want to parkthe topic of the color tagging thing Ithink that's really fascinating thisidea of how you kind of like useclassifiers to extract metadata from youknow unstructured data that's definitelya topic as we go into the weeds furtheruh but coming I really want tounderstand voting a little better sothis decision to do custom embeddingmodels that's that's a really uniqueangle I think I've seen you know I thinkopen Ai and cohere they offer a customtext embedding model uh right now I'mnot like I'm not so sure that thetooling on doing a custom embeddingmodel is is all that strong yet so itdoes seem like such an interestingMarket category to go after can youexplain a little further this kind ofyou know custom embedding models forpeople what what that kind of looks likeyeah so to get these amazing incredibleopen source multimodal modelsum ready to be used in in productionum in an environment in e-commerce umyou have to do a lot of things to themum so you have to you know buildextensive data sets very specific tothis you have to build differentarchitectures for the differentmodalitiesum optimize them for them and maybemaybe you can give them a few highlightsof what for example we did with ourfirst model so essentiallyum with our first model in order to makethese embeddings work we had to Firstunderstand okaythese open source models are really goodat say generic images and text stuffthat you can find easily the internetstuff on Google Images basic stuff theproblem though is that these companiesusually have very specific domains theywork in for example um e-commercecompanies like Amazon or Wayfair havevery different requirements of what theywant a model to look like then maybestandard Google or Microsoft becausee-commerce is inherently different fromstandard multimodal and so a model wouldneed to be domain adapted to be able tounderstand what these types of productslook like what text when you try to sella product looks like how products looklike the prices of these thingsbasically there's new details that thesetypes of models don't understand and soone of the most important steps is todomain adapt these models so they areable to understand e-commerce at ameaningful level and this can beextended to basically an industry realestate AR VRplenty of others so that's one of themost important steps as to what thesebusinesses need and these types ofmultimodal embeddingsthere are other things tooyeah let's giveyou understanding what life stage yourproduct is for what room you might useit in what style it might be you know ifwe look ahead to what you do withmultimodal llms imagine being able to goto an e-commerce site and generate astyle blog about how you might stylethis item particularly in your housethis weekend Jonathan and I spent abunch of time on mid-journing and chatgbtum you know in essence creating a way tostyle some different ways to do ourhouse and so you know that's reallygoing to be part of your new e-commerceexperience you're not just going to buythe products you're really going to beable to understand how to use them inyour life and it's gonna be reallyincredibleyeah that that paints such a compellingpicture that kind of I've seen likethese room GPT things on my Twitter andstuff and stuff like that kind of ideaand we're going to come back to that andmaybe as a thing if if you're listeningand you want to skip ahead to that youcan see the chapters in the video but Iwant to stay on this um custom embeddingmodels a little further because I thinkthis is such a deep topic like as anexample I like to kind of play withweeviate by taking these podcasttranscriptions and putting them inweaving so Stephanie and good Johnyou'll soon be in my data set and like Ifind with this that yeah like the customtext embeddings are getting me waybetter search results than the off theshelf any of the off-the-shelf modelsbecause of this you know this specificconversation we're having as we startsaying things like room GPT clipinstruct blip we started saying allthese buzzer these keywords and it'slike the models the domain adaptation tothem can we unpack that a little morefor like the multimodal e-commerce caselike say I'm like aloe like a you knowlike an apparel brand it's like theparticular style of my clothing is thatyou're gonna you know start fine-tuningthe embedding models on the descriptionsof the yellow shirt with the yellowshirt and then that kind of leads tobetter search is that kind of thegeneral thinkingyeah so um II are going to use this as a good is agood way to answer that um so bycreating first an embedding of theproduct using all of the data you haveso you have this human level ofunderstanding about it uh putting it inyour weeviate databaseum and then eventually creating anembedding of the search query from theuser and so now you're matching upexactly you know it's semantic searchexactly what they mean and let's take itfurther um when you want to startsearching and say oh I really like thisshirt but I want something brighter andI want it short sleeve and and itunderstands what you mean and is able toidentify that or our CEO is just lookingfor a mixed drink maker um and hediscovered as he looked through themthat he wanted it to work outside thathe needed it to be able to handlealcohol and being able to revise and andyou know have it understand that you'vemade these newum new decisions about how you want tohow do you want to search and how youunderstand the productimagine maybe you want to talk a littlebit aboutum you know that integration that theuse of those two sets of embeddingsin order to combine such theseembeddings essentially well first whereare you going to use these embeddings soin the case of something like productsearch you have two options essentiallythe core problem is that search is veryvery speed dependent and any and everyspeed optimization is going to beessential and if you're even a secondlate your business has lost millions ofdollars of Revenue and so things thatneed to happen is you need to take theseembeddings usually you're going to takeyour products get embeddings of each ofthese cache them and use them in somesort of vector database like for exampleweviate and you can use those embeddingsto be able to figure out what productsare similar to each other so you cantherefore say do on-the-flyrecommendations to your customers withyour website so that's one valuable wayin order to do Integrations of coursethere might be other ways like if you'redoing an offline application maybe forexample you want to use these embeddingsto train a model as step noted to detectthe style or what room this productmight be in and so that might just besomething where you do a data labelingtask offline use this model to addadditional information pass it intoelasticsearch or Cloud search or othertypes of services like that andessentially be able to use thatinformation on your existing searchwithout changing anything so theIntegrations are going to fundamentallydepend on the use case in the case ofe-commerce speed is usually Paramountand so that's going to affect most ofwhat you're doing but essentially itdoesn't look too different from howunimodal models like in text or Visionwork within the e-commerce space is justdumping in a new model and everythingyou would have used before would be whatyou would use today since I thinkthere's a great example from the projectwe did with HBO Thereally really really relate toum you know I loved Queens Gambit if Igo to HBO and I say hey can you show mewhat you have that's most like thisbecause they're using interaction dataand very limited structure they don'tknow what I mean rightum and so building an embedding of allfilms and TV shows what we did we wereable to work with HBO on how would theymake recommendations about data thatthey have no interaction that on willthey have a complete understanding withthousands of topical and emotionalattributes then you know theseembeddings understand the movies the wayyou and I do next by the way veryexciting would be to invent the movieitselfum but you could use those embeddingsinitialize your model originally as thecontent understanding and then no matterwhat we searched for on these platformswe'd be able to see what they have thatum is most like that showso enjoy so we're kind of staying on themovie example it reminds me of like umso so I think maybe there are two thingshere there's like where you are labelingthe categories as well as just theembedding by itself like the The NextStep being the embedding of the movieitself that just captures all thesemantics compared to where you likeclassify each uh label and then thatbecomes the vector it sounds like so youknow if I take a movie like Deadpoolright that's like action and also comedyso so I have those two labels and likeit's not horror like and making thatlabel so that's how you then bootstraplike a collaborative filtering kind ofthing where where you recommend like inthat kind of way so so it sounds yeahyeah Bingo um actually genres issomething none of us can agree about wecan only agreeon uswe'll wear um while that seems likehelpful structured data itdue to um you know consumer bias and nowthey have different opinions of what'sfunnyso so a lot of it is trendingclassifiers then as well right thenbecause you just it's not like thecontrast of uh image like you know Ilike I usually think when I think ofembedding models I think that you'recontrasting it compared to this soundslike you're pretty you're producingclassifiers is that correctboth so it seems that the community haslargely coalesced around using some sortof contrast of loss function in order tocreate so for example sentenceTransformers NLP based is done based oncontrastive learning and you might alsofind something like clip or blip or slipor all those other Alternatives those Imean they all use the same namingconvention so essentially all those usecontrastive learning in order to alignimage and text embeddings you can extendthat to other modalities as well likeincluding video and audio like what youmight want to do in the context of amovie so contrastive loss is still goingto be a key part at least in the stateof the art that I'm aware of classifiersmatter too and that the way that you canalso structure this contrastive loss asa classifier like if you want to dousing a momentum cue you can style itthat wayclassifiers also matter for example ifmaybe you have a lot of data that has aclassification label and you want to usethat in order to inform your learningfor example maybe you know that thesetypes of products belong in thiscategory or I guess in the context ofmovies the type of movies belong in thisgenre this other type of movies belongin this different type of genre and soyou can use that to sort of partitionthese embeddings away and that can alsobe used so I think mostly it's acontrast to problem classification is away in order to get more data to helpyou can structure a set contrast aproblem as a classification one althoughit's one option out of many contrastiveis a really big problem and of courseyou can find too many of theseembeddings in classifier type problemswhich we anticipate would be a massiveuse of these types of embeddings so kindof all the aboveyeah really so I think we're starting tounpack the meat of how this how thetraining happens in the in the finetuning so I guess my first question withthat is the thinking around like youknow fine-tuning from a foundation modeland that kind of thinking so does votingyou know do you take a pre-trained modeloff the shelf and then start fine-tuningit for HBO Nike these or do you trainyour own like kind of foundation modelwhere you pre-train it on a massiveamount of data set how are you thinkingabout that kind of thing yeah so webuild our own data sets we have hugedata sets for this purpose um and we arebuilding fine-tuned models that are zeroshot off the shelf so you can come andtake themum there are still a lot of hurdles asyou know to overcome in the data spaceum and so we are making it uh very easyfor our clients to come and take themoff the shelf we build our own modelsand we fine-tune others a particularshout out for me to junan Lee andStephen Hoy and the team at Salesforceauthors of lip we think their work issuper imaginative and love working withit but we work with all models umattention made me want to talk aboutsome of our most recent architecturessure so I built a bunch lately and soessentially some of this might involvefor example trying to fuse modelstogether like for example images andtext you want to fuse them you caneither fuse them right at the beginningor at the end when you build a model youcan say have some sort of clip-likearchitecture where you're not fusingthem at all you're basically trainingtwo separate models or multiple modelsif you want to extend this to additionalmodalities and basically have them allrepresent similar ideas that arerepresented in text or image or someother modality that's another option youcan also say combine these types ofideas first use this alignment beforethen fusing those together that might behelpful like for example if you want todo a primarily image based task and youwant to use a text to help guide theimage model to an answer that is anotheralternative it really does depend Ithink most of the decisions end up beingdependent on client specific needs andnamely for examplesay how fast the model needs to be howmuch memory you can use how accuratedoes it need to be those types oftrade-offs are mainly where most of thecustom model building happens becauseunlike an NLP or Vision where there'slike so many models and you can use themfor like every specific use case there'slike a super small quick version thatgets the job done there are giganticmodels that get you accuracy and prettymuch everything in between butmultimodal is so nascent and multimodalhas so many different modalities thatthere simply doesn't exist anything likeit in open source or even closed Sourceat this point and so we have to buildthose out a couple domain specificexamples um[Music]burp is a great idea and e-commerce it'snot usually the first shot of theproduct by the time you get to threeyou've got a detail shot maybe you evenhave a size chart right and so taking anaverage no longer works one of theimportant things in the e-commerce isyou can't have to retrain this modelevery time you update your catalog someof these guys are updated catalog youknow literally hourly and so they needto work and so there's you know a lot ofmaybe that's interesting talk aboutmaybe some of the instructor lip work todo thatsure so if we want to talk aboutinstruct blip essentially that would besort of like taking these ideas to thellm space so gbt4 proved that it'spossible to build a multimodal llm thattakes Vision as input or Texas input andis able to generate text based on thatthere were other types of Works beforethat like Flamingo and plenty of othersthatdon't come to my mind at the moment butsince GPT sort of generated this newgenerative wave no pun intendedum there's been a lot of other work totry and say replicate some of thoseInnovations and one of those wasinstruct blip which first tried to trainas efficiently as they could a way totake Vision as input and be able togenerate text based upon it in the llmworld and then instruct blip was likeokay let's go take every academic dataset we can find turn it intoinstructions and teach this llm thatknowledge so it can then generalize tonew tasks in a zero shot setting that wepreviously may have not thought aboutsince academic data sets only cover somuch we have a model that can generalizesort of like chat gbt can then it can doa crazy amount of things so that's theessential idea it is definitely embody'sinterest to be able to produce somethinglike that in the e-commerce space andfurther compounding that is the factthat every different company ine-commerce or say in the case of filmand TV have different things they wantto do a Model A model with and so theirtasks are going to be fairly company tocompanythat's a problem for us since obviouslythis is in the consulting firm and so asa consequence we want to build stuff butgeneralizes really well to other firmsand so this type of instruction tuningand having really good zero shotperformance that can generalize to newtasks that nobody at the firm eventhought about would be really importantthe idea of using instruction tuning togeneralize to new sorts of tasks issomething that allows for scalability itallows for this model to be used in manydifferent cases and makes us reallyexcited about the potential of whatthese models can be used foryeah without a question so there's a lotof information that quickly Stephaniementioned the re-indexing as you updatethe catalog and I just as to just pullthat nugget out of the podcast forpeople listening that's a huge problemre-indexing I think we see that all thetime and just something to tackle um butyeah I want to stay on this you know youmentioned gpt4 being able to handlemultimodal uh things we saw papers likeyou mentioned Flamingo we saw uh Frozenthis paper that was called likepre-trained Transformers as universalcomputation engines basically that youcan just put the image embeddings intothe latent space of gbt3 and then itseems to be able to just instantly beable to reason across the embeddingspace which is quite astonishing butthen yeah so I mean this idea of youknow Foundation model training is so Ithink at the time of recording thispodcast cohere has just announced raiseraising like 270 million dollars and sothis space is like is emerging so do youthink that the foundation modelproviders for multimodal you knowwhether it's and there's two things Iwant to parse out quickly so there's thegenerative models the multimodalgenerative models and then there's thedecision to train embedding modelsversus uh generative models embeddingmodels versus generative models so canwe start with umyou know like um like do you think themultimodal foundation generative modelswill be different from the text modelsyesnamely because they're going to have alot more capabilities right sofor example yes you can say use clevertricks to try and pass a purely textmodel information about an image lavafor example basically is able to getsurprisingly good performance from chatgbt which has never seen an image beforeby passing some context on the imagelike a caption as well as say boundingboxes determine what every object isthere and the model is surprisingly goodat creating data and generating it whichis helpful except there are somelimitations of that for example one whatwe found is that using an objectdetection based model ends up slowingthings down a great deal it's notefficient one paper called vilt was ableto remove that part and claim a 60 timesincrease in speed from doing so which issuper impressive and there's also theidea that these types of multimodalmodels because they're able to learnfrom each other are able to get a muchmore Vivid picture that is able tointegrate these two much better than saywhat a text model can do with say someclever ways of injecting imageinformation without training it so forexample bring back in something I talkedabout earlier in the podcast we havesomeone who basically only has seenimages like with their own two eyes andseen the world around them but isilliterate and then we have someone whois blind and basically has the knowledgeof like practically every PhD in theworld being Visionary those twocollaborating together is going to be avery difficult way to like work togetherthey're not going to be nearly as goodas if one person had those capabilitiestogether and the idea about having amulti-moon model is you want to havethat one Superstar person who can seethe world and know everything about itand have the knowledge of all Wikipediaas opposed to two people with theseextreme split apart skills trying theirbest to collaborate so that's theopportunity in multimodal I'm being beable to combine those two togetherinstead of just limping your way withtwo extremesexactly the way you are as a consumerKinder right when you're looking atbuying a product you're understandingthat structured data the price Etc whilelooking at the style and the image andwhat it's going to look like and you areprocessing them in the same space and soit's the same ideayeah brilliant I I agree fully with thatthat certainly there's something to bedoing the multimodal optimization I andyeah I think just that just that puttingimage embeddings into a text model thefact that that works at all is alreadypretty astonishing right and nowsurprisedyeah I think you know like how how couldyou have a good sense of audio or likehow you know songs sound from a textspace it doesn't make any sense socertainly like that example is obviouslyVisual and that makes sense too so couldwe take apart that other thing thedecision to train embedding modelsversus generative models in this spaceof being a foundation model provideryes building a model Library where weactually have a model roadmap to do allthe things we're talking aboutum I think probably the answer to thatcomes more from the startup EnterpriseMeetup which is um what is the morepressing need and and you know um rightnow we're building the models so thatthey can get more out of the modelsthey've already deployed by includingmore data in them and soum while working towards um sort of thefuture of introducing some of those likea multimodal llm and the productfeatures you would need to build tosupport that model on the site is evendifferent than the work rightyeah so essentiallyembeddings in general tend to serve twokey different purposes embeddings wellhas a unique advantage of you'll be ableto build services on top of them withoutchanging it like if you want to doproduct search and recommendation ormovie search and recommendation you canuse these embeddings do semantic searchbe able to Cache them in a vectordatabase so that you can do efficiencygains and that tends to be the best wayto do things to sort of speed limitedapplicationsum embedding is also the advantage thatyou can train task specific models havemuch more smaller ones that are fasterand that would be the core advantage ofembedding based models generative modelstend to be much larger they are muchmore generalizable are able to do acrazy amount of stuff that even peoplewho are well familiar with them stillget astonished with every single day andthat is a valuable advantage of them sothey can do crazy things can generalizeto a lot of things if you want to likecover some new tasks you didn't thinkabout these models can do that withastonishing ability so that's the ideabehind generative being able togeneralize some new tasks have crazygood performance and sort of like hitthat accuracy Frontier for customers whocare about it from bettings we're mostsort of concerned with more traditionalapplications speed related things and soit's sort of like a dichotomy wheredifferent customers are going to wantembeddings of different ones they'regoing to want generative just dependingon what their specific use case are theydon't really overlap simply because itseems that most businesses have veryspecific kpis they need to hit and theytend to already be pretty demanding onthem and one does not really work forthe other and vice versa yeah it's sofascinating I think umlike a couple notes I had for first dayI was thinking about how we're talkingabout kind of classifiers as well assort of the more open-ended likesimilarity search or generative and Ithink there's this paper called set fitwhich is about how you fine-tuneembedding models for classification andit's more efficient and reallyefficiency and the cost of these thingsthat's the big topic I want to stay on alittle further I think trainingembedding models embedding modelsgenerally aren't that big models like Ithink the open AI Ada they have someembeddings that come from that big modelbut I think mostly uh embedding modelsthey're not big models correct like youknow like around 100 million parametersand this is the typical scale we'retalking aboutis that correctwe could go a little bit smaller thanthat too the most popular I wantedhugging faces like the smallest minialbum It's like a 21 million parametersbut you're right essentially embeddingmodels tend to see saturationthink you don't get more than 300 400but honestly it's been a long time sinceI've checked the hugging phaseleaderboard and so uh viewers are morethan welcome to go check and figure outwhat the true numbers right now butthere seems to be a saturation pointwhere you don't really get betterembeddings after getting Beyond somespecific model capacity would suggeststhat embeddings are not necessarily thatdifficult or else only speaking for adeep learning model to learn and sobut they don't really get any benefitfrom additional parameters generative onthe other hand keeps on going better andbetter for islarge as I've seen I haven't seen likeany saturation point in generative yetmaybe some people at open AI have foundsomething maybe that's why they haven'tgone past 175 billion although maybegbt4 is larger I don't know maybesomeone does would love to knowum but yeah it just uh embeddings tendsto hit a saturation point where theydon't get better probably becauseembeddings and understanding somethingis a much easier task than being fluentin writing entire essays on some reallyrandom topic which Chachi BT does greatyeah that that to me is just one of themost interesting topics because like youknow if you're someone who's beenstudying deep learning for the last fewyears and you have all these skillsaround like training models and ml Opsand all that kind of stuff I thinktraining embedding models is theopportunity because they're smallerscale you can iterate you can apply allthe clever training tricks like as youmentioned like programmatic labeling andknowledge distillation this is more ofan opportunity to apply that and so thisbrings me into kind of like my hot takewe'll see how well this ages like I I'vebeen thinking a lot about like this newclass of like zero shot you know theretrieval augmented generation kind ofpipeline where the GB the reasoning partand the reasoning part in the searchmodels they're like separate and youdon't think about training the zero shotreader model and you only would thinkabout training the search models do youthink that that kind of Paradigm thatthat will be the way going forwardbecause like I guess the alternativeview is that as it's getting cheaper tofine-tune the language models with saylike the Laura low rank adaptation andthis kind of thing that the thinking isyou don't even need to do retrievalaugment to generation you could just youknow fine tune the language model or youknow the multimodal generative model onyour particular domain compared to thisother setting where you're just updatingthe search models and you keep the zeroshot generative model fixed do you thinkthat'll be the way going forward so Imean there are very easy ways to buildthat today and it's something that'sgoing to be very common in applicationsI myself use that to great effect whichhas been very funI am leery about making predictions ofwhat happens going forward sincehonestly my track record of predictingthe future is less than I would like Iwould be persuaded that aretrieval-based augmentation systemwould help for example Bard is usingthat chanchi BT in their plus form hasused that through the formula pluginsand it seems like people are going inthis type of Direction I think in thiscase uh you're more talking aboutsituations where you have a bunch ofdocuments or a bunch ofproducts or a bunch of movies and yousort of want to pick the one that's mostrelevant to feed into your llm orwhatever is doing this generation andthat I think is going to be valuable aslong as say attention remains expensiveand there have been some massiveInnovations in the space for the lastfour months this has been something I'vebeen following really closely I've seenmore valuable stuff in an exponentialPace in the last four months than I sawin the last three yearsand I'm just astonished and somaybe these types of things may becomeless relevant as attention becomes moreefficient and can generate up to like ahundred thousand tokens two hundredthousand into these types of crazynumbers but those are also complicatedto build out and I don't see thesesystems integrating that for at least afew years thanks to the fact thatdiffusion of innovation takes a whileand so maybe that because of that I cansee this how Paradigm being useful forthe next few years at least and evenwith like these efficient form ofattention they are using retrieval aswell so they're trying to retrieve froma database in order to try and makethings more efficient so maybe even thefuture of attention which is supposed tosolve this problem will literally beusing retrieval in order to solve it sowe might see it just go in a differentway we'll just change our idea ofattention and I think retrieval isvaluable and maybe that'll even becomethe future of attention I'll just stopat thatyeah there's quite a lot I mean likethere's like the if we want to likestaying on this topic of like the Retromodels where you're doing like Fusion indecoder and you put embeddings directlyinto the attention layer and you know Ithink we also took our top like there'slike the Laura there's like the sparsefine tuning that just makes it easier tofine-tune the existing language modelsand then we're coming into like thesenew models like mosaics MPT or theanthropic Cloud that they're like theyare designed to take massive inputwindows and I think that complementsretrieval quite nicely but sort ofstepping outside back into ourrecommendation hat I'm curious with thisperspective on recommendation with uhembeddings and then because the way thatI see right recommendation that it'stypically been done by these symbolicranking models where you take like youknow features about Conor like maillikes basketball maybe that's a featureI like basketball but like and then youwould have features for each of the Nikeproducts and so you'd feed these intolike XG boost and rank them do you thinkthose kind of models are still valuabledoes everybody think about trainingthose kind of modelsso I do think they are valuable forexample one focus of ours is to look atstructured data fields seeing they havea lot of context and those structureddata fields tend to align quite nicelywould sayperson is male and like these specifictypes of likeI'm in an opioid heart bag of wordstypes featuresum so essentially I do think that'sstill going to be valuable I think atleast in the context of e-commerce andmaybe like things like Netflixum it goes beyond say content-basedembeddings and you also have to startconsidering what other people want andthat would or what people want and whatsimilar things are what similar moviessimilar products that tends to be reallyvaluable to do recommendations in thisspace and so I think recommendations aregoing to be happy on that but in orderto even process and understand thatinformation effectively your model needsto have the full picture of like how touse that information it doesn't matterif you have like a billion like clickingof your movies if say you don't know howto use them like if all I know isum movie name title and I'm trying tofigure out clicks of like a billion fromthat I don't have much information evenlike fine patterns but multimodalbroadens that scope a lot and now I canuse say specific parts of that movie Ican use images the trailers the umentire transcript now I'm able tobroaden what I'm able to use to processthose people that like a billion peoplethat clicked on your movies to be ableto make recommendations sothis doesn't really change the fact thatseeing what other people want and seeinghow similar you are to other people thatis still going to be valuable but whatmultimodal allows you to do is be ableto take that information and use a lotmore of it than you were able topreviously so broadening The View yeah Ithink um well yeah for sure it seemslike the the embeddings from theproducts alone can get a lot of thatsymbolic data out of it and then it'sgetting so good that it captures it alllike if I click on a few pictures oflike I'm on Nike and I start clicking onlike LeBron James shoes or like a KyrieIrving t-shirt and now it can pick up onall these other kind of features for mepretty quickly so earlier we mentionedroom GPT and that we come back into thattopic later on so I'm tagging it now soum this kind of idea where you you knowretrieve and then generate likepersonalized retrieval so you know Itake Connor and maybe using the Wii V8reftube example just because this is howI think about it but you know like I'dhave Connor and I like this you knowLeBron James shoes Kyrie Irving t-shirtI average those vectors and then that'sthe counter vector and so now I likesearch for the new uh the new productsand then I would and then I would feedthose new products into like agenerative model where it's like apicture of me and it like you know putsthe shirt on me stuff like this how areyou think about that kind of like uh youknow personalized generation I hope Iset up the question correctly but thiskind of thing where you like to comeback to the room gbt it's like I take apicture of my uh you know my living roomand then the new products come in as abig catalog of like a thousand Wayfareryou know tables and stuff and then firstit ranks it by using the Connor vectorand then it puts it in my room so I cansee what it would look like in my livingroomthat kind of thing yeah so I find roomGPT interesting more so because it usescontrol net which I found interestingsince you're able to condition thatgeneration of say what you want a roomlook like and be able to better controlhow say your text to image model worksthat I find is really interesting ofcourse this is relatively nascent andcontrolling that is a relatively newmodel I mean I think 1.1 came outrecently but so I think it's going to bea very interesting space going forward Ithink there's going to be a lot ofinnovation I think it's cool right nowum definitely in the context of saye-commerce you will want to like whenyou know what room something wants tobelong in maybe there's multiplevaluable rooms that a object might berelevant in like lamps can be indifferent types of rooms and so if youcan use the sort of like text imagegeneration with control net or I guessmore generally room GPT you'll be ableto or retailer could easily just takethis andchange their lamp in a living room tomaybe a lamp in a dining room or in abedroom all with using a simple modelandI'm sure that would be a very valuableplace for being able to sell these typesof products in a way that more iscustomized with customer needs so Ithink it's really valuable I think it'sreally new research and I'm excited tosee say what's going to happen the nextfew months because I think A lot's goingto happen I think you'll be able you'llbe able to to index a little lower onpersonal information right so like wewon'tyou're putting in a room and youunderstand so much more about everysingle product that's in the room and ifyou're saying hey what could I put onthis wall right you'll be able to use somuch more context from the image itselfand rely Less on the user you knowNetflix does some incredible work withembeddings if everybody wants and wantsto read more on their blog um you knowand they're they're able to look atviewing patterns so you know againthousands of of topical and emotionalattributes and and say uh cut differenttrailers to show different audiencesum reorder they're they're evenpersonalizing the order you seecarousels in right by using all thisinformation and so it's it's not asreliant on umparticular user data but but on how muchmore data they can use to create theseexperiencesyeah that I think that paints such acompelling picture especially becauseyou know if you you no longer need tohave this Treasure Trove of data rightbecause the zero shot model from votingcan look at the picture and then extractall as you mentioned like if it takes apicture of my living room we'll see myother furniture and like get a sense ofmy style I suppose like yeah as becausebecause it sounds like quite thebusiness change like if I'm like I thinkI think a lot about this thing of likeum who should Implement search likeshould you know aloe that I mentionedearlier should they be thinking aboutlike we should have our own we shouldcontrol our own search on our website orshould we like use Amazon I think in thepast you'd want to use like Amazon ofthese big players because they had allthis interaction data and that was likethe only way to do it but now because ofthese embedding models that as youmentioned can just take a picture youcan get a sense of your style instantlydo you think more Brands will bethinking like I'm cutting out theaggregation platform I'm going to youknow build this right into my websiteand manage this kind of advancedtechnology myselfyes my answer to that would be um twoparts one is um like the distributionchannel the fact that they have theaudience on some of these biggerplatformsum is sort of one piece but when youflip the side of um being able tocontrol more of their own on-siteconversion um you know 75 of Revenue ande-commerce is lost from abandonedshopping carts um so if you don't knowyou have duplicate products you'reshowing a user they're gone if they youknow confused about what they're lookingat they're gone rightum so I think being able to use all oftheir own data that we're using to makepurchase decisions to personalize anexperience for us they don't need toknow as much about me anymore they nowhave use of the data that I was using tomake the first decision so I think itcan really you know transform how theypersonalize your home screen experienceto how they show your objects I mean Iimagine one day where they know I boughtthis shirt so the next time I open youknow skirts that I'm seeing it with thisshirt so now I'm imagining my own closetright like you can get to that pointwhere so YouTube you can kind of mix thepersonalizationumwith what we were saying earlier whichis not needing it because you have sucha great understanding of the product soyou don't need to know the product theuser as wellyeah I agree that's I hadn't thoughtabout that before but that is cool thatit like uh it has your closet in so itcan orchestrate it that way see what seewhat you're missing from the closet andum yeah I mean it's like um I'm curiousabout this kind of uh it's more of atechnical topic but this kind of likediversity and recommendations and howembeddings achieves that because thesemodels are trained for similarity and toshow you similar things to what youpurchased so how would how would asystem work that shows you some thingsyou you know like because like a lot ofexperiments with ref as I mentioned likewe're averaging the Lebron James KyrieIrving stuff we average that vector andso you know we're just going to get thathow do you think about diversity andrecommendation with embeddingsyeah so think about like cart fillingabout the productI want to show them more hammers ordifferent hammers you want to show themNails you want to show them whatevergloves you'd use whatever other toolsyou use with a hammer right so some ofthat understanding yeah um I think youcan get diversity from a lot ofdifferent ways so I guess we're talkingjust about embeddings and like thesetypes of encoder only models so to speakum you can achieve this by for examplechanging up Dropout a little bit you cansort of like maybe provide some sort oflike noise context to these types ofmodels which I don't think is somethingpeople do but it's entirely somethingthat's like theoretically possible ifyou want to take the idea from like Gansso that can helpum you can also change the way thatyou're doing similarity maybe if youwant to um you're taking the top 20 mostsimilar things rather than going one twothree four five six seven you can sortof like switch up that order a lot ofpeople use like a buy encoder crossencoder type framework and just changingup the way you use that might helpadding more context so you can creatediversity recommendations simply bychanging up your inputs maybe changingup the model a little bit or even likechanging the way that you likepost-process things so getting that typeof diversity is something that you canpretty much Tinker with at pretty muchevery stage of this overallrecommendation Pipeline and soultimately I think that's somethingthat's going to be up to say theindividual users like the companies thatare integrating these models and thesePipelinesinto their productsyes Stephanie you mentioned like basketsand good Judy also mentioned this ideaof like what the similarity function iscapturing and I think that's quite aninteresting idea because you knowinitially when I was thinking about thisI was thinking usually with clip modelsyou like have an image and then you haveits description and then you're you knowtrying to make that similar but anotherway to train these models would be toopt like the positives come from they'rein the same basket together and thenthat way the function doesn't just likecapture similarity captures like thiskind of like we have this relationshiptogether like we're in baskets togetherbut then I think with the basket thetraining of the models is like how wouldyou kind of bootstrap the data for thatwithout again coming back to thatproblem where you you need to be likeAmazon and you have this massivecollection of basket datasureum so the idea is you can use a largermodel to try and improve sampleefficiency that would be one step now ifyou're trying to compete with thebillions of data Amazon has you'reprobably gonna have to be creative andtry to like go back to like thecontent-based space and try to likereally richen up your content-basedembeddings incorporating multiple whatwill be a great step since these Sourcerecommendations simply don't use thatright nowum using multilingual which as I'venoticed is something that even Amazonstruggles with which was a really bigsurprise that I saw one month ago when Iwas exploring things and so trying tobuild better content-based embeddings isa way if say you don't have Amazon's ofvoluminous data in order to try andlevel up the playing field that would beone step and of course bytrying to figure out what type ofinformation is valuable that way if youhave really good content recommendationsthat itself was might drive some usersand be able to help you sort of like getthat interaction data that'll furtherimprove your models and so obviouslyit's going to be difficult to go upagainst Amazon but most of theseretailers that are like trying to buildthese sorts of models maybe more Nicheor more specific to types of areas andso they don't have to compete withAmazon everywhere it doesn't matter whatAmazon's pillow data is if you'reselling bricks for example like thattype of correlation simply isn't goingto matter too much for your products soif you focus on the things that you aretrying to sell and try to get reallygood embeddings and content on that andtry to get interaction data on thatspecific set of products and it'll befine like if you're a boutique retailerthat only like sells 35 different itemsit doesn't really matter of likemillions of people use Amazon you justneed a ton of data on those 35 productsand you're seeing a really understandyour 35 products really well with yourmodels and then you should be set so Iwouldn't worry too much about competingwith Amazon unless you're literallytrying to build the next Amazon in whichcase well I don't know much about that Ican't really provide any adviceyeah I think it well I think this isrelated to this concept of like you knowthe embedding models they're between 20million to 100 million parameters andlike being clever with the trainingtechniques like I'm sure I'd vote youyou know you have a ton of expertise onlike how you train these kind of modelsand that's a emote and I think it's avery interesting kind of startup topursue for people who've been studyingmachine learning and the reason I'msaying this is we're talking about thiskind of how you're doing the similaritywhere you're getting the data from thiskind of sampling of the positives andnegatives I think there's so muchopportunity to innovate in how yousample positives and negatives and learnthese kind of models and one idea I'vebeen thinking about and I'm curious ifyou have as well as like these kind oflike graph neural networks where youhave like user and then like you knowlike put it in a basket or and then youalso have a graph like these items theyshare the same brand and that could besomething as like a clever trainingtechnique right that maybe you'duniquely offeryeah it definitely is valuableum graph networks have been valuable intrying to like use interaction data inparticular for example since seeing thesequence of like the products you mightuse has been something that I think hasbeen historically of InterestTransformers themselves can beconsidered some sort of graph networksand that the attention itself is somesort of graph I do think it's valuableespecially for the interaction data it'snot something that I am particularlystrong in but it's definitely beensomething thatlooking at yeah super cool I I yeah I dothink that with the graph of nuts Iguess I don't know it's like because Ithink a lot about we we like we've wasoriginally a Knowledge Graph technologyand so this we still have these likecross surfences between the classes andsoI like to think a lot about how you canmaybe send embeddings through the graphto get better embeddings for yourparticular context that's kind of whatinspires me to think about this kind ofgraph neural network thing but so anywayso I I think it was a great coverage ofyou know getting into the meat of howthe models are trainedum maybe kind of a wrapping up topicwould be just like what is on thehorizon of multimodal that you're youknow that's immediately in front of youas well as something that maybe is likethree to five years outdifferent timelinesI think that having models thatunderstand multimodal like among textimages and then maybe uh domain specificmodalities like structured data clickstream videos audio what have you that'sgoing to be something that's going to beseen more and more across Enterprise AImaybe not necessarily academics sincethese types of things matter more from abusiness application and so you'll see alot more of that at least the company'shoping for thatum you'll probably also see a lot moreof these types of multimodal models beused for Generation capabilitiesdefinitely there's going to be a lot ofinterest in the next few months as longas generative way continues but evenwhen all the dusts and hype Settlespeople will realize that being able touse all these different modalities andbe able to generate texts like forexample if I don't know Amazon orWayfair want to build a chatbot to likeanswer customer queries something likemulti-modal is going to be very valuablefor that so those are the two areas thatat least I think that multimodal isgoing to be very valuable for um thesetypes of companies in the next two yearswe're expecting tons of adoption of thisand we expect it to completely TurboCharge and change the way that people dorecommendations and maybe even off of astretch like something like Bing chat ora chat GPT plugin people can simplyinteract with say a chat and be likeask for a product to be found and thenwe can even automate product search sothat rather than you having to siftthrough like 10 products and try to findthe right thing you can just talk tothis model and it'll just automaticallybe able to generate say the product thatyou're looking for have a simple clickbutton to like pay for it in one clickand then you're done and so that I thinkis one particularly exciting applicationof multimodal LMS being able to use likea Bing chat type application in order tolike automate large parts of productsearch so that well I mean we alreadyhave models that can search the webbetter than a human can at least that'sthe idea behind being chat why not havesomething like that that can search fore-commerce products better than a humancan and then probably save a lot of usso much time so that's something allthose things are things I'm reallyexcited about for the next few yearsI share his excitement on that side I'dsay what else I'm excited about is justthe ability to actually get any of thisdone right so thank you to our startupfriends like you guysum you know and actual ml offeringinference like you're there there'sreally going to be it's going to be mucheasier and the cloud platforms and theInvestments they're making to do allthis work to use these models to be ableto use these embeddings and get theminto into production and I think that'syou know in the next couple years that'sone of the things I'm most excited aboutawesome and so just one more thingpicking apart your brain particularlyStephanie being the CEO and buildingbusinesses I'm really curious how yousee uh kind of the space of Open Sourcewith this topic if you think about likeI'm curious how companies like yours uhuh you know offer this kind of likemodel training for particular companieshow you then also think about opensourcing some of the stuff you do andjust how you see that yeah so superimportant topic to us um all this workis because of Open Source right soum we embrace it we love it right nowwe're working on sort ofum more limited open source academicPartnerships places like that so that wecan make sure we're getting theappropriate amount of feedbackum and and keeping it out there I thinkour intentionum you know and as you said how dothings age so maybe I'll regret this butOur intentionum would be that as we continue toevolve and release more models we willmake some of them open source um andwe're working right now on um how we'regoing to approach that but for for theinterim um embracing it heavily becauseit's advancing our work as we said we'reusing a lot of Open Source models andusing a lot of Open Source techniques tomake these models work for e-commerce umand so we're going to find our way togive back to that we have great datasets they're enormous that for examplethose types of Partnerships so we canhelp more of that open source work thatwe use get published I would say is ourour first goal that it's so fascinatinglike the the explosion of Open Sourcemodels I meanyeah it's also interesting with kind ofthe idea of you could take one of thesecheckpoints from hugging face that comesfrom body and then you could fine tuneit or maybe you already do that kind ofthing and I know talking to Brian aboutunstructured and how they're doing howthey're going to train uh Foundationmodels for like uh informationextraction from say like PDFs and visualdocuments this whole emerging space ofthe open source models is alsointeresting um I have one more questionon that which is like um there's theopen sourcing of the model weightsthemselves but then there's also likelike something that I always like hottip to Mosaic ml4 and respect a lot whatthey did is they open sourced thiscomposer library and so it'simplementations of all sorts ofefficient uh deep learning techniquesthat it like like Alibi attention orlike stochastic path drop out like allthese kind of details of the models whatdo you think about open sourcing some ofthe like kind of like theyou know the the tricks of the trainingthat goes in as well or just kind ofmodel weightsI think there's room for both in thatsituation now given that Lobby'sInnovations Alibi attention rotary wellI don't even know if rotary embeddingscounts as an innovation this point somany models use it but essentiallyI think a lot of that's going to be opensource before we even like integrate itto a model since a lot of the mostvaluable work in this space has beenopen source open AI is basically justgpt4 was like let's go take everythingthat's open source available add to gbt3and then make gpt4 in essence and theyadded some of their own stuff butanything they added was very quicklyopen sourced by a lot of passionateresearchers and sowe are going to say open up that stuffbut by the time we do my guess would bethat they're already are going to bemultiple libraries out there that dothat type of stuff like it wouldn'tsurprise me with say a Phil Wong luciddreams it's a crazy smart person wholike opens horses every model like I I'mpretty sure like that's still going tobe the go-to placeum to find these types of Innovations nomatter what save body or any othercompany does likethere are open source is always going tolead the way andreally any business is just going to tryand combine those things to a modelsimply because there's tons ofInnovations but they all tend to be likeone specific area one specific paper onespecific model combining those is whereI think a lot of the emote and theopportunity liesand to your point guard our businessvery clearly sits in the um you know alot of great open source work not notimmediately applicable at a businesslevel right and so our business isn'ttaking a lot of this work and makingsure that it is Enterprise readyum you know for business applicationswhichum you know isis a sort of extension of that as I saidI think you know we'll probably end upcontributing before that on how can wehelp um Advance some of the work that wethen useyeah fantasticum awesome Stephanie gunjan that wassuch an incredible coverage of all thesetopics um I learned so much abouttraining embedding models all the thingshow you're seeing the space um as we'rewrapping up uh do you maybe have any uhlinks upcoming announcements things thatuh people listening can use to you knowkeep up with all the work you're doingum yes we are releasing a new web appwhere you can try our color modelum there'll even be some data so thatyou don't have to prepare your ownum and uh we'll be coming on our newwebsite on body.com so definitely checkit out and uh you can apply to get alink to the API if you want awesomethank you so much thanksso much thank you", "type": "Video", "name": "Stephanie Horbaczewski and Gunjan Bhattarai on Vody - Weaviate Podcast #53!", "path": "", "link": "https://www.youtube.com/watch?v=spaoRhZPGTM", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}