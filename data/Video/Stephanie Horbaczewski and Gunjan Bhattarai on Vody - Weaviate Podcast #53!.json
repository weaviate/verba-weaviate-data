{"text": "Hey everyone, thank you so much for watching the 53rd episode of the Weaviate Podcast!! This episode features Stephanie ... \nhey everyone thank you so much for watching another episode of the wevia podcast I'm super excited to welcome Stephanie horvachesky and gunjan bhataraii from voting we've been following on with voti for a long time alleviate they're building all sorts of super exciting things with multimodal AI combining different modalities from text and images this kind of idea so this podcast is going to dive all into that topic and learn about what voting is working on so Stephanie and gunjan firstly thank you so much for joining the podcast thanks so much for having us found out you were one of our first startup friends we're really excited to be here yeah thanks so much awesome so could we start off with the the story of voti the problem that's solving in the founding vision yeah definitely um so I'm a second time founder my first business uh was a hundred million dollar a year business on top of Google and Facebook that worked entirely with unstructured data so when Bert came out I I grabbed a couple of my technical team and left um invested some of the money I made and started a research company we did three years of projects working on embeddings uh worked with Comcast HBO moved into retail with Nike and then spent a year developing our product with Wayfair wow amazing yeah I'm so so if we could pack unpack that a little bit so you're already working with kind of uncerted data you saw the Burt Revolution and then you instantly said multimodal is next I think or was it you grew into multimodal um you know what really happened is I said recommendations are bad and why um I didn't understand I felt like there was a ton of data and it didn't make sense to me that we were returning such poor recommendations uh which is when I discovered that what a customer is using to make decisions for example um images q a reviews structured data is not um what's being used to make those recommendations and so I thought if we could bridge that Gap we're going to have some really transformative experiences in e-commerce and so we set out to build those specific models uh fine-tuned for the e-commerce media space multimodal models to use all these different types of data that's so interesting I've been yeah I've been like dabbling with recommendation and building kind of like apparel recommendations to illustrate our ref devec feature which is a way that we've built in to have recommendations with embeddings I recently had odsce listened to mod hofthaker describe how they use uh embeddings at Shopify for recommendation and you know his explanation of the business impact to that was incredible and understanding the scope of it so could maybe is leaving on the business CEO hat uh you describe new experiences with recommendation can you kind of unpack that a little more yeah I mean what's amazing about the embedding is it's bringing all of the understanding about a product so you know um a real complete human level of understanding is going back to your model to make recommendations so you know when you're um when you're looking for a pink fuchsia chair with wooden legs um there's not enough data labeling in the world right now to do that but if you have an actual understanding um of the product and so imagine maybe you can talk a little bit about um our multimodal product embeddings sure so I'll go a little bit further on the fucio chair and be like most models simply don't even have the capability to handle that today and so some of the core concerns of that is for example one you have to detect color in a image and that involves Vision at the same time though there may be multiple objects in an image maybe there's something specific in that image you want the color of and so you sort of need to be able to guide the model as to what you want and that would involve natural language so that's one of the core opportunities in multimodal previously we'd only have some understanding of how things might look from an image perspective or a text perspective it's like someone who has previously illiterate but had perfect vision and then some person who had a PhD but was entirely blind and so we're able to combine these two extreme forms of knowledge into one person and basically have a model that knows all the latest advancements in science and technology and like all sorts of disciplines but is able to actually see and understand how the world looks like with their own eyes so multimodal embeddings essentially deliver on that promise so the idea is that products are multimodal in nature they have textual features for example product descriptions product titles reviews they have images namely the images that you're trying to have someone sell for and other details to like various metadata Fields things related to what other products people buy maybe some product videos lots of other things and so the idea is if you can jointly align these types of representations together and be able to have a model that understands all these we're able to have embeddings that are able to understand these products really well and you can understand what's similar what's different and then people can build models on top of them and build search that's able to leverage that Newfound understanding fascinating I want to park the topic of the color tagging thing I think that's really fascinating this idea of how you kind of like use classifiers to extract metadata from you know unstructured data that's definitely a topic as we go into the weeds further uh but coming I really want to understand voting a little better so this decision to do custom embedding models that's that's a really unique angle I think I've seen you know I think open Ai and cohere they offer a custom text embedding model uh right now I'm not like I'm not so sure that the tooling on doing a custom embedding model is is all that strong yet so it does seem like such an interesting Market category to go after can you explain a little further this kind of you know custom embedding models for people what what that kind of looks like yeah so to get these amazing incredible open source multimodal models um ready to be used in in production um in an environment in e-commerce um you have to do a lot of things to them um so you have to you know build extensive data sets very specific to this you have to build different architectures for the different modalities um optimize them for them and maybe maybe you can give them a few highlights of what for example we did with our first model so essentially um with our first model in order to make these embeddings work we had to First understand okay these open source models are really good at say generic images and text stuff that you can find easily the internet stuff on Google Images basic stuff the problem though is that these companies usually have very specific domains they work in for example um e-commerce companies like Amazon or Wayfair have very different requirements of what they want a model to look like then maybe standard Google or Microsoft because e-commerce is inherently different from standard multimodal and so a model would need to be domain adapted to be able to understand what these types of products look like what text when you try to sell a product looks like how products look like the prices of these things basically there's new details that these types of models don't understand and so one of the most important steps is to domain adapt these models so they are able to understand e-commerce at a meaningful level and this can be extended to basically an industry real estate AR VR plenty of others so that's one of the most important steps as to what these businesses need and these types of multimodal embeddings there are other things too yeah let's give you understanding what life stage your product is for what room you might use it in what style it might be you know if we look ahead to what you do with multimodal llms imagine being able to go to an e-commerce site and generate a style blog about how you might style this item particularly in your house this weekend Jonathan and I spent a bunch of time on mid-journing and chat gbt um you know in essence creating a way to style some different ways to do our house and so you know that's really going to be part of your new e-commerce experience you're not just going to buy the products you're really going to be able to understand how to use them in your life and it's gonna be really incredible yeah that that paints such a compelling picture that kind of I've seen like these room GPT things on my Twitter and stuff and stuff like that kind of idea and we're going to come back to that and maybe as a thing if if you're listening and you want to skip ahead to that you can see the chapters in the video but I want to stay on this um custom embedding models a little further because I think this is such a deep topic like as an example I like to kind of play with weeviate by taking these podcast transcriptions and putting them in weaving so Stephanie and good John you'll soon be in my data set and like I find with this that yeah like the custom text embeddings are getting me way better search results than the off the shelf any of the off-the-shelf models because of this you know this specific conversation we're having as we start saying things like room GPT clip instruct blip we started saying all these buzzer these keywords and it's like the models the domain adaptation to them can we unpack that a little more for like the multimodal e-commerce case like say I'm like aloe like a you know like an apparel brand it's like the particular style of my clothing is that you're gonna you know start fine-tuning the embedding models on the descriptions of the yellow shirt with the yellow shirt and then that kind of leads to better search is that kind of the general thinking yeah so um I I are going to use this as a good is a good way to answer that um so by creating first an embedding of the product using all of the data you have so you have this human level of understanding about it uh putting it in your weeviate database um and then eventually creating an embedding of the search query from the user and so now you're matching up exactly you know it's semantic search exactly what they mean and let's take it further um when you want to start searching and say oh I really like this shirt but I want something brighter and I want it short sleeve and and it understands what you mean and is able to identify that or our CEO is just looking for a mixed drink maker um and he discovered as he looked through them that he wanted it to work outside that he needed it to be able to handle alcohol and being able to revise and and you know have it understand that you've made these new um new decisions about how you want to how do you want to search and how you understand the product imagine maybe you want to talk a little bit about um you know that integration that the use of those two sets of embeddings in order to combine such these embeddings essentially well first where are you going to use these embeddings so in the case of something like product search you have two options essentially the core problem is that search is very very speed dependent and any and every speed optimization is going to be essential and if you're even a second late your business has lost millions of dollars of Revenue and so things that need to happen is you need to take these embeddings usually you're going to take your products get embeddings of each of these cache them and use them in some sort of vector database like for example weviate and you can use those embeddings to be able to figure out what products are similar to each other so you can therefore say do on-the-fly recommendations to your customers with your website so that's one valuable way in order to do Integrations of course there might be other ways like if you're doing an offline application maybe for example you want to use these embeddings to train a model as step noted to detect the style or what room this product might be in and so that might just be something where you do a data labeling task offline use this model to add additional information pass it into elasticsearch or Cloud search or other types of services like that and essentially be able to use that information on your existing search without changing anything so the Integrations are going to fundamentally depend on the use case in the case of e-commerce speed is usually Paramount and so that's going to affect most of what you're doing but essentially it doesn't look too different from how unimodal models like in text or Vision work within the e-commerce space is just dumping in a new model and everything you would have used before would be what you would use today since I think there's a great example from the project we did with HBO The really really really relate to um you know I loved Queens Gambit if I go to HBO and I say hey can you show me what you have that's most like this because they're using interaction data and very limited structure they don't know what I mean right um and so building an embedding of all films and TV shows what we did we were able to work with HBO on how would they make recommendations about data that they have no interaction that on will they have a complete understanding with thousands of topical and emotional attributes then you know these embeddings understand the movies the way you and I do next by the way very exciting would be to invent the movie itself um but you could use those embeddings initialize your model originally as the content understanding and then no matter what we searched for on these platforms we'd be able to see what they have that um is most like that show so enjoy so we're kind of staying on the movie example it reminds me of like um so so I think maybe there are two things here there's like where you are labeling the categories as well as just the embedding by itself like the The Next Step being the embedding of the movie itself that just captures all the semantics compared to where you like classify each uh label and then that becomes the vector it sounds like so you know if I take a movie like Deadpool right that's like action and also comedy so so I have those two labels and like it's not horror like and making that label so that's how you then bootstrap like a collaborative filtering kind of thing where where you recommend like in that kind of way so so it sounds yeah yeah Bingo um actually genres is something none of us can agree about we can only agree on us we'll wear um while that seems like helpful structured data it due to um you know consumer bias and now they have different opinions of what's funny so so a lot of it is trending classifiers then as well right then because you just it's not like the contrast of uh image like you know I like I usually think when I think of embedding models I think that you're contrasting it compared to this sounds like you're pretty you're producing classifiers is that correct both so it seems that the community has largely coalesced around using some sort of contrast of loss function in order to create so for example sentence Transformers NLP based is done based on contrastive learning and you might also find something like clip or blip or slip or all those other Alternatives those I mean they all use the same naming convention so essentially all those use contrastive learning in order to align image and text embeddings you can extend that to other modalities as well like including video and audio like what you might want to do in the context of a movie so contrastive loss is still going to be a key part at least in the state of the art that I'm aware of classifiers matter too and that the way that you can also structure this contrastive loss as a classifier like if you want to do using a momentum cue you can style it that way classifiers also matter for example if maybe you have a lot of data that has a classification label and you want to use that in order to inform your learning for example maybe you know that these types of products belong in this category or I guess in the context of movies the type of movies belong in this genre this other type of movies belong in this different type of genre and so you can use that to sort of partition these embeddings away and that can also be used so I think mostly it's a contrast to problem classification is a way in order to get more data to help you can structure a set contrast a problem as a classification one although it's one option out of many contrastive is a really big problem and of course you can find too many of these embeddings in classifier type problems which we anticipate would be a massive use of these types of embeddings so kind of all the above yeah really so I think we're starting to unpack the meat of how this how the training happens in the in the fine tuning so I guess my first question with that is the thinking around like you know fine-tuning from a foundation model and that kind of thinking so does voting you know do you take a pre-trained model off the shelf and then start fine-tuning it for HBO Nike these or do you train your own like kind of foundation model where you pre-train it on a massive amount of data set how are you thinking about that kind of thing yeah so we build our own data sets we have huge data sets for this purpose um and we are building fine-tuned models that are zero shot off the shelf so you can come and take them um there are still a lot of hurdles as you know to overcome in the data space um and so we are making it uh very easy for our clients to come and take them off the shelf we build our own models and we fine-tune others a particular shout out for me to junan Lee and Stephen Hoy and the team at Salesforce authors of lip we think their work is super imaginative and love working with it but we work with all models um attention made me want to talk about some of our most recent architectures sure so I built a bunch lately and so essentially some of this might involve for example trying to fuse models together like for example images and text you want to fuse them you can either fuse them right at the beginning or at the end when you build a model you can say have some sort of clip-like architecture where you're not fusing them at all you're basically training two separate models or multiple models if you want to extend this to additional modalities and basically have them all represent similar ideas that are represented in text or image or some other modality that's another option you can also say combine these types of ideas first use this alignment before then fusing those together that might be helpful like for example if you want to do a primarily image based task and you want to use a text to help guide the image model to an answer that is another alternative it really does depend I think most of the decisions end up being dependent on client specific needs and namely for example say how fast the model needs to be how much memory you can use how accurate does it need to be those types of trade-offs are mainly where most of the custom model building happens because unlike an NLP or Vision where there's like so many models and you can use them for like every specific use case there's like a super small quick version that gets the job done there are gigantic models that get you accuracy and pretty much everything in between but multimodal is so nascent and multimodal has so many different modalities that there simply doesn't exist anything like it in open source or even closed Source at this point and so we have to build those out a couple domain specific examples um [Music] burp is a great idea and e-commerce it's not usually the first shot of the product by the time you get to three you've got a detail shot maybe you even have a size chart right and so taking an average no longer works one of the important things in the e-commerce is you can't have to retrain this model every time you update your catalog some of these guys are updated catalog you know literally hourly and so they need to work and so there's you know a lot of maybe that's interesting talk about maybe some of the instructor lip work to do that sure so if we want to talk about instruct blip essentially that would be sort of like taking these ideas to the llm space so gbt4 proved that it's possible to build a multimodal llm that takes Vision as input or Texas input and is able to generate text based on that there were other types of Works before that like Flamingo and plenty of others that don't come to my mind at the moment but since GPT sort of generated this new generative wave no pun intended um there's been a lot of other work to try and say replicate some of those Innovations and one of those was instruct blip which first tried to train as efficiently as they could a way to take Vision as input and be able to generate text based upon it in the llm world and then instruct blip was like okay let's go take every academic data set we can find turn it into instructions and teach this llm that knowledge so it can then generalize to new tasks in a zero shot setting that we previously may have not thought about since academic data sets only cover so much we have a model that can generalize sort of like chat gbt can then it can do a crazy amount of things so that's the essential idea it is definitely embody's interest to be able to produce something like that in the e-commerce space and further compounding that is the fact that every different company in e-commerce or say in the case of film and TV have different things they want to do a Model A model with and so their tasks are going to be fairly company to company that's a problem for us since obviously this is in the consulting firm and so as a consequence we want to build stuff but generalizes really well to other firms and so this type of instruction tuning and having really good zero shot performance that can generalize to new tasks that nobody at the firm even thought about would be really important the idea of using instruction tuning to generalize to new sorts of tasks is something that allows for scalability it allows for this model to be used in many different cases and makes us really excited about the potential of what these models can be used for yeah without a question so there's a lot of information that quickly Stephanie mentioned the re-indexing as you update the catalog and I just as to just pull that nugget out of the podcast for people listening that's a huge problem re-indexing I think we see that all the time and just something to tackle um but yeah I want to stay on this you know you mentioned gpt4 being able to handle multimodal uh things we saw papers like you mentioned Flamingo we saw uh Frozen this paper that was called like pre-trained Transformers as universal computation engines basically that you can just put the image embeddings into the latent space of gbt3 and then it seems to be able to just instantly be able to reason across the embedding space which is quite astonishing but then yeah so I mean this idea of you know Foundation model training is so I think at the time of recording this podcast cohere has just announced raise raising like 270 million dollars and so this space is like is emerging so do you think that the foundation model providers for multimodal you know whether it's and there's two things I want to parse out quickly so there's the generative models the multimodal generative models and then there's the decision to train embedding models versus uh generative models embedding models versus generative models so can we start with um you know like um like do you think the multimodal foundation generative models will be different from the text models yes namely because they're going to have a lot more capabilities right so for example yes you can say use clever tricks to try and pass a purely text model information about an image lava for example basically is able to get surprisingly good performance from chat gbt which has never seen an image before by passing some context on the image like a caption as well as say bounding boxes determine what every object is there and the model is surprisingly good at creating data and generating it which is helpful except there are some limitations of that for example one what we found is that using an object detection based model ends up slowing things down a great deal it's not efficient one paper called vilt was able to remove that part and claim a 60 times increase in speed from doing so which is super impressive and there's also the idea that these types of multimodal models because they're able to learn from each other are able to get a much more Vivid picture that is able to integrate these two much better than say what a text model can do with say some clever ways of injecting image information without training it so for example bring back in something I talked about earlier in the podcast we have someone who basically only has seen images like with their own two eyes and seen the world around them but is illiterate and then we have someone who is blind and basically has the knowledge of like practically every PhD in the world being Visionary those two collaborating together is going to be a very difficult way to like work together they're not going to be nearly as good as if one person had those capabilities together and the idea about having a multi-moon model is you want to have that one Superstar person who can see the world and know everything about it and have the knowledge of all Wikipedia as opposed to two people with these extreme split apart skills trying their best to collaborate so that's the opportunity in multimodal I'm being be able to combine those two together instead of just limping your way with two extremes exactly the way you are as a consumer Kinder right when you're looking at buying a product you're understanding that structured data the price Etc while looking at the style and the image and what it's going to look like and you are processing them in the same space and so it's the same idea yeah brilliant I I agree fully with that that certainly there's something to be doing the multimodal optimization I and yeah I think just that just that putting image embeddings into a text model the fact that that works at all is already pretty astonishing right and now surprised yeah I think you know like how how could you have a good sense of audio or like how you know songs sound from a text space it doesn't make any sense so certainly like that example is obviously Visual and that makes sense too so could we take apart that other thing the decision to train embedding models versus generative models in this space of being a foundation model provider yes building a model Library where we actually have a model roadmap to do all the things we're talking about um I think probably the answer to that comes more from the startup Enterprise Meetup which is um what is the more pressing need and and you know um right now we're building the models so that they can get more out of the models they've already deployed by including more data in them and so um while working towards um sort of the future of introducing some of those like a multimodal llm and the product features you would need to build to support that model on the site is even different than the work right yeah so essentially embeddings in general tend to serve two key different purposes embeddings well has a unique advantage of you'll be able to build services on top of them without changing it like if you want to do product search and recommendation or movie search and recommendation you can use these embeddings do semantic search be able to Cache them in a vector database so that you can do efficiency gains and that tends to be the best way to do things to sort of speed limited applications um embedding is also the advantage that you can train task specific models have much more smaller ones that are faster and that would be the core advantage of embedding based models generative models tend to be much larger they are much more generalizable are able to do a crazy amount of stuff that even people who are well familiar with them still get astonished with every single day and that is a valuable advantage of them so they can do crazy things can generalize to a lot of things if you want to like cover some new tasks you didn't think about these models can do that with astonishing ability so that's the idea behind generative being able to generalize some new tasks have crazy good performance and sort of like hit that accuracy Frontier for customers who care about it from bettings we're most sort of concerned with more traditional applications speed related things and so it's sort of like a dichotomy where different customers are going to want embeddings of different ones they're going to want generative just depending on what their specific use case are they don't really overlap simply because it seems that most businesses have very specific kpis they need to hit and they tend to already be pretty demanding on them and one does not really work for the other and vice versa yeah it's so fascinating I think um like a couple notes I had for first day I was thinking about how we're talking about kind of classifiers as well as sort of the more open-ended like similarity search or generative and I think there's this paper called set fit which is about how you fine-tune embedding models for classification and it's more efficient and really efficiency and the cost of these things that's the big topic I want to stay on a little further I think training embedding models embedding models generally aren't that big models like I think the open AI Ada they have some embeddings that come from that big model but I think mostly uh embedding models they're not big models correct like you know like around 100 million parameters and this is the typical scale we're talking about is that correct we could go a little bit smaller than that too the most popular I wanted hugging faces like the smallest mini album It's like a 21 million parameters but you're right essentially embedding models tend to see saturation think you don't get more than 300 400 but honestly it's been a long time since I've checked the hugging phase leaderboard and so uh viewers are more than welcome to go check and figure out what the true numbers right now but there seems to be a saturation point where you don't really get better embeddings after getting Beyond some specific model capacity would suggests that embeddings are not necessarily that difficult or else only speaking for a deep learning model to learn and so but they don't really get any benefit from additional parameters generative on the other hand keeps on going better and better for is large as I've seen I haven't seen like any saturation point in generative yet maybe some people at open AI have found something maybe that's why they haven't gone past 175 billion although maybe gbt4 is larger I don't know maybe someone does would love to know um but yeah it just uh embeddings tends to hit a saturation point where they don't get better probably because embeddings and understanding something is a much easier task than being fluent in writing entire essays on some really random topic which Chachi BT does great yeah that that to me is just one of the most interesting topics because like you know if you're someone who's been studying deep learning for the last few years and you have all these skills around like training models and ml Ops and all that kind of stuff I think training embedding models is the opportunity because they're smaller scale you can iterate you can apply all the clever training tricks like as you mentioned like programmatic labeling and knowledge distillation this is more of an opportunity to apply that and so this brings me into kind of like my hot take we'll see how well this ages like I I've been thinking a lot about like this new class of like zero shot you know the retrieval augmented generation kind of pipeline where the GB the reasoning part and the reasoning part in the search models they're like separate and you don't think about training the zero shot reader model and you only would think about training the search models do you think that that kind of Paradigm that that will be the way going forward because like I guess the alternative view is that as it's getting cheaper to fine-tune the language models with say like the Laura low rank adaptation and this kind of thing that the thinking is you don't even need to do retrieval augment to generation you could just you know fine tune the language model or you know the multimodal generative model on your particular domain compared to this other setting where you're just updating the search models and you keep the zero shot generative model fixed do you think that'll be the way going forward so I mean there are very easy ways to build that today and it's something that's going to be very common in applications I myself use that to great effect which has been very fun I am leery about making predictions of what happens going forward since honestly my track record of predicting the future is less than I would like I would be persuaded that a retrieval-based augmentation system would help for example Bard is using that chanchi BT in their plus form has used that through the formula plugins and it seems like people are going in this type of Direction I think in this case uh you're more talking about situations where you have a bunch of documents or a bunch of products or a bunch of movies and you sort of want to pick the one that's most relevant to feed into your llm or whatever is doing this generation and that I think is going to be valuable as long as say attention remains expensive and there have been some massive Innovations in the space for the last four months this has been something I've been following really closely I've seen more valuable stuff in an exponential Pace in the last four months than I saw in the last three years and I'm just astonished and so maybe these types of things may become less relevant as attention becomes more efficient and can generate up to like a hundred thousand tokens two hundred thousand into these types of crazy numbers but those are also complicated to build out and I don't see these systems integrating that for at least a few years thanks to the fact that diffusion of innovation takes a while and so maybe that because of that I can see this how Paradigm being useful for the next few years at least and even with like these efficient form of attention they are using retrieval as well so they're trying to retrieve from a database in order to try and make things more efficient so maybe even the future of attention which is supposed to solve this problem will literally be using retrieval in order to solve it so we might see it just go in a different way we'll just change our idea of attention and I think retrieval is valuable and maybe that'll even become the future of attention I'll just stop at that yeah there's quite a lot I mean like there's like the if we want to like staying on this topic of like the Retro models where you're doing like Fusion in decoder and you put embeddings directly into the attention layer and you know I think we also took our top like there's like the Laura there's like the sparse fine tuning that just makes it easier to fine-tune the existing language models and then we're coming into like these new models like mosaics MPT or the anthropic Cloud that they're like they are designed to take massive input windows and I think that complements retrieval quite nicely but sort of stepping outside back into our recommendation hat I'm curious with this perspective on recommendation with uh embeddings and then because the way that I see right recommendation that it's typically been done by these symbolic ranking models where you take like you know features about Conor like mail likes basketball maybe that's a feature I like basketball but like and then you would have features for each of the Nike products and so you'd feed these into like XG boost and rank them do you think those kind of models are still valuable does everybody think about training those kind of models so I do think they are valuable for example one focus of ours is to look at structured data fields seeing they have a lot of context and those structured data fields tend to align quite nicely would say person is male and like these specific types of like I'm in an opioid heart bag of words types features um so essentially I do think that's still going to be valuable I think at least in the context of e-commerce and maybe like things like Netflix um it goes beyond say content-based embeddings and you also have to start considering what other people want and that would or what people want and what similar things are what similar movies similar products that tends to be really valuable to do recommendations in this space and so I think recommendations are going to be happy on that but in order to even process and understand that information effectively your model needs to have the full picture of like how to use that information it doesn't matter if you have like a billion like clicking of your movies if say you don't know how to use them like if all I know is um movie name title and I'm trying to figure out clicks of like a billion from that I don't have much information even like fine patterns but multimodal broadens that scope a lot and now I can use say specific parts of that movie I can use images the trailers the um entire transcript now I'm able to broaden what I'm able to use to process those people that like a billion people that clicked on your movies to be able to make recommendations so this doesn't really change the fact that seeing what other people want and seeing how similar you are to other people that is still going to be valuable but what multimodal allows you to do is be able to take that information and use a lot more of it than you were able to previously so broadening The View yeah I think um well yeah for sure it seems like the the embeddings from the products alone can get a lot of that symbolic data out of it and then it's getting so good that it captures it all like if I click on a few pictures of like I'm on Nike and I start clicking on like LeBron James shoes or like a Kyrie Irving t-shirt and now it can pick up on all these other kind of features for me pretty quickly so earlier we mentioned room GPT and that we come back into that topic later on so I'm tagging it now so um this kind of idea where you you know retrieve and then generate like personalized retrieval so you know I take Connor and maybe using the Wii V8 reftube example just because this is how I think about it but you know like I'd have Connor and I like this you know LeBron James shoes Kyrie Irving t-shirt I average those vectors and then that's the counter vector and so now I like search for the new uh the new products and then I would and then I would feed those new products into like a generative model where it's like a picture of me and it like you know puts the shirt on me stuff like this how are you think about that kind of like uh you know personalized generation I hope I set up the question correctly but this kind of thing where you like to come back to the room gbt it's like I take a picture of my uh you know my living room and then the new products come in as a big catalog of like a thousand Wayfarer you know tables and stuff and then first it ranks it by using the Connor vector and then it puts it in my room so I can see what it would look like in my living room that kind of thing yeah so I find room GPT interesting more so because it uses control net which I found interesting since you're able to condition that generation of say what you want a room look like and be able to better control how say your text to image model works that I find is really interesting of course this is relatively nascent and controlling that is a relatively new model I mean I think 1.1 came out recently but so I think it's going to be a very interesting space going forward I think there's going to be a lot of innovation I think it's cool right now um definitely in the context of say e-commerce you will want to like when you know what room something wants to belong in maybe there's multiple valuable rooms that a object might be relevant in like lamps can be in different types of rooms and so if you can use the sort of like text image generation with control net or I guess more generally room GPT you'll be able to or retailer could easily just take this and change their lamp in a living room to maybe a lamp in a dining room or in a bedroom all with using a simple model and I'm sure that would be a very valuable place for being able to sell these types of products in a way that more is customized with customer needs so I think it's really valuable I think it's really new research and I'm excited to see say what's going to happen the next few months because I think A lot's going to happen I think you'll be able you'll be able to to index a little lower on personal information right so like we won't you're putting in a room and you understand so much more about every single product that's in the room and if you're saying hey what could I put on this wall right you'll be able to use so much more context from the image itself and rely Less on the user you know Netflix does some incredible work with embeddings if everybody wants and wants to read more on their blog um you know and they're they're able to look at viewing patterns so you know again thousands of of topical and emotional attributes and and say uh cut different trailers to show different audiences um reorder they're they're even personalizing the order you see carousels in right by using all this information and so it's it's not as reliant on um particular user data but but on how much more data they can use to create these experiences yeah that I think that paints such a compelling picture especially because you know if you you no longer need to have this Treasure Trove of data right because the zero shot model from voting can look at the picture and then extract all as you mentioned like if it takes a picture of my living room we'll see my other furniture and like get a sense of my style I suppose like yeah as because because it sounds like quite the business change like if I'm like I think I think a lot about this thing of like um who should Implement search like should you know aloe that I mentioned earlier should they be thinking about like we should have our own we should control our own search on our website or should we like use Amazon I think in the past you'd want to use like Amazon of these big players because they had all this interaction data and that was like the only way to do it but now because of these embedding models that as you mentioned can just take a picture you can get a sense of your style instantly do you think more Brands will be thinking like I'm cutting out the aggregation platform I'm going to you know build this right into my website and manage this kind of advanced technology myself yes my answer to that would be um two parts one is um like the distribution channel the fact that they have the audience on some of these bigger platforms um is sort of one piece but when you flip the side of um being able to control more of their own on-site conversion um you know 75 of Revenue and e-commerce is lost from abandoned shopping carts um so if you don't know you have duplicate products you're showing a user they're gone if they you know confused about what they're looking at they're gone right um so I think being able to use all of their own data that we're using to make purchase decisions to personalize an experience for us they don't need to know as much about me anymore they now have use of the data that I was using to make the first decision so I think it can really you know transform how they personalize your home screen experience to how they show your objects I mean I imagine one day where they know I bought this shirt so the next time I open you know skirts that I'm seeing it with this shirt so now I'm imagining my own closet right like you can get to that point where so YouTube you can kind of mix the personalization um with what we were saying earlier which is not needing it because you have such a great understanding of the product so you don't need to know the product the user as well yeah I agree that's I hadn't thought about that before but that is cool that it like uh it has your closet in so it can orchestrate it that way see what see what you're missing from the closet and um yeah I mean it's like um I'm curious about this kind of uh it's more of a technical topic but this kind of like diversity and recommendations and how embeddings achieves that because these models are trained for similarity and to show you similar things to what you purchased so how would how would a system work that shows you some things you you know like because like a lot of experiments with ref as I mentioned like we're averaging the Lebron James Kyrie Irving stuff we average that vector and so you know we're just going to get that how do you think about diversity and recommendation with embeddings yeah so think about like cart filling about the product I want to show them more hammers or different hammers you want to show them Nails you want to show them whatever gloves you'd use whatever other tools you use with a hammer right so some of that understanding yeah um I think you can get diversity from a lot of different ways so I guess we're talking just about embeddings and like these types of encoder only models so to speak um you can achieve this by for example changing up Dropout a little bit you can sort of like maybe provide some sort of like noise context to these types of models which I don't think is something people do but it's entirely something that's like theoretically possible if you want to take the idea from like Gans so that can help um you can also change the way that you're doing similarity maybe if you want to um you're taking the top 20 most similar things rather than going one two three four five six seven you can sort of like switch up that order a lot of people use like a buy encoder cross encoder type framework and just changing up the way you use that might help adding more context so you can create diversity recommendations simply by changing up your inputs maybe changing up the model a little bit or even like changing the way that you like post-process things so getting that type of diversity is something that you can pretty much Tinker with at pretty much every stage of this overall recommendation Pipeline and so ultimately I think that's something that's going to be up to say the individual users like the companies that are integrating these models and these Pipelines into their products yes Stephanie you mentioned like baskets and good Judy also mentioned this idea of like what the similarity function is capturing and I think that's quite an interesting idea because you know initially when I was thinking about this I was thinking usually with clip models you like have an image and then you have its description and then you're you know trying to make that similar but another way to train these models would be to opt like the positives come from they're in the same basket together and then that way the function doesn't just like capture similarity captures like this kind of like we have this relationship together like we're in baskets together but then I think with the basket the training of the models is like how would you kind of bootstrap the data for that without again coming back to that problem where you you need to be like Amazon and you have this massive collection of basket data sure um so the idea is you can use a larger model to try and improve sample efficiency that would be one step now if you're trying to compete with the billions of data Amazon has you're probably gonna have to be creative and try to like go back to like the content-based space and try to like really richen up your content-based embeddings incorporating multiple what will be a great step since these Source recommendations simply don't use that right now um using multilingual which as I've noticed is something that even Amazon struggles with which was a really big surprise that I saw one month ago when I was exploring things and so trying to build better content-based embeddings is a way if say you don't have Amazon's of voluminous data in order to try and level up the playing field that would be one step and of course by trying to figure out what type of information is valuable that way if you have really good content recommendations that itself was might drive some users and be able to help you sort of like get that interaction data that'll further improve your models and so obviously it's going to be difficult to go up against Amazon but most of these retailers that are like trying to build these sorts of models maybe more Niche or more specific to types of areas and so they don't have to compete with Amazon everywhere it doesn't matter what Amazon's pillow data is if you're selling bricks for example like that type of correlation simply isn't going to matter too much for your products so if you focus on the things that you are trying to sell and try to get really good embeddings and content on that and try to get interaction data on that specific set of products and it'll be fine like if you're a boutique retailer that only like sells 35 different items it doesn't really matter of like millions of people use Amazon you just need a ton of data on those 35 products and you're seeing a really understand your 35 products really well with your models and then you should be set so I wouldn't worry too much about competing with Amazon unless you're literally trying to build the next Amazon in which case well I don't know much about that I can't really provide any advice yeah I think it well I think this is related to this concept of like you know the embedding models they're between 20 million to 100 million parameters and like being clever with the training techniques like I'm sure I'd vote you you know you have a ton of expertise on like how you train these kind of models and that's a emote and I think it's a very interesting kind of startup to pursue for people who've been studying machine learning and the reason I'm saying this is we're talking about this kind of how you're doing the similarity where you're getting the data from this kind of sampling of the positives and negatives I think there's so much opportunity to innovate in how you sample positives and negatives and learn these kind of models and one idea I've been thinking about and I'm curious if you have as well as like these kind of like graph neural networks where you have like user and then like you know like put it in a basket or and then you also have a graph like these items they share the same brand and that could be something as like a clever training technique right that maybe you'd uniquely offer yeah it definitely is valuable um graph networks have been valuable in trying to like use interaction data in particular for example since seeing the sequence of like the products you might use has been something that I think has been historically of Interest Transformers themselves can be considered some sort of graph networks and that the attention itself is some sort of graph I do think it's valuable especially for the interaction data it's not something that I am particularly strong in but it's definitely been something that looking at yeah super cool I I yeah I do think that with the graph of nuts I guess I don't know it's like because I think a lot about we we like we've was originally a Knowledge Graph technology and so this we still have these like cross surfences between the classes and so I like to think a lot about how you can maybe send embeddings through the graph to get better embeddings for your particular context that's kind of what inspires me to think about this kind of graph neural network thing but so anyway so I I think it was a great coverage of you know getting into the meat of how the models are trained um maybe kind of a wrapping up topic would be just like what is on the horizon of multimodal that you're you know that's immediately in front of you as well as something that maybe is like three to five years out different timelines I think that having models that understand multimodal like among text images and then maybe uh domain specific modalities like structured data click stream videos audio what have you that's going to be something that's going to be seen more and more across Enterprise AI maybe not necessarily academics since these types of things matter more from a business application and so you'll see a lot more of that at least the company's hoping for that um you'll probably also see a lot more of these types of multimodal models be used for Generation capabilities definitely there's going to be a lot of interest in the next few months as long as generative way continues but even when all the dusts and hype Settles people will realize that being able to use all these different modalities and be able to generate texts like for example if I don't know Amazon or Wayfair want to build a chatbot to like answer customer queries something like multi-modal is going to be very valuable for that so those are the two areas that at least I think that multimodal is going to be very valuable for um these types of companies in the next two years we're expecting tons of adoption of this and we expect it to completely Turbo Charge and change the way that people do recommendations and maybe even off of a stretch like something like Bing chat or a chat GPT plugin people can simply interact with say a chat and be like ask for a product to be found and then we can even automate product search so that rather than you having to sift through like 10 products and try to find the right thing you can just talk to this model and it'll just automatically be able to generate say the product that you're looking for have a simple click button to like pay for it in one click and then you're done and so that I think is one particularly exciting application of multimodal LMS being able to use like a Bing chat type application in order to like automate large parts of product search so that well I mean we already have models that can search the web better than a human can at least that's the idea behind being chat why not have something like that that can search for e-commerce products better than a human can and then probably save a lot of us so much time so that's something all those things are things I'm really excited about for the next few years I share his excitement on that side I'd say what else I'm excited about is just the ability to actually get any of this done right so thank you to our startup friends like you guys um you know and actual ml offering inference like you're there there's really going to be it's going to be much easier and the cloud platforms and the Investments they're making to do all this work to use these models to be able to use these embeddings and get them into into production and I think that's you know in the next couple years that's one of the things I'm most excited about awesome and so just one more thing picking apart your brain particularly Stephanie being the CEO and building businesses I'm really curious how you see uh kind of the space of Open Source with this topic if you think about like I'm curious how companies like yours uh uh you know offer this kind of like model training for particular companies how you then also think about open sourcing some of the stuff you do and just how you see that yeah so super important topic to us um all this work is because of Open Source right so um we embrace it we love it right now we're working on sort of um more limited open source academic Partnerships places like that so that we can make sure we're getting the appropriate amount of feedback um and and keeping it out there I think our intention um you know and as you said how do things age so maybe I'll regret this but Our intention um would be that as we continue to evolve and release more models we will make some of them open source um and we're working right now on um how we're going to approach that but for for the interim um embracing it heavily because it's advancing our work as we said we're using a lot of Open Source models and using a lot of Open Source techniques to make these models work for e-commerce um and so we're going to find our way to give back to that we have great data sets they're enormous that for example those types of Partnerships so we can help more of that open source work that we use get published I would say is our our first goal that it's so fascinating like the the explosion of Open Source models I mean yeah it's also interesting with kind of the idea of you could take one of these checkpoints from hugging face that comes from body and then you could fine tune it or maybe you already do that kind of thing and I know talking to Brian about unstructured and how they're doing how they're going to train uh Foundation models for like uh information extraction from say like PDFs and visual documents this whole emerging space of the open source models is also interesting um I have one more question on that which is like um there's the open sourcing of the model weights themselves but then there's also like like something that I always like hot tip to Mosaic ml4 and respect a lot what they did is they open sourced this composer library and so it's implementations of all sorts of efficient uh deep learning techniques that it like like Alibi attention or like stochastic path drop out like all these kind of details of the models what do you think about open sourcing some of the like kind of like the you know the the tricks of the training that goes in as well or just kind of model weights I think there's room for both in that situation now given that Lobby's Innovations Alibi attention rotary well I don't even know if rotary embeddings counts as an innovation this point so many models use it but essentially I think a lot of that's going to be open source before we even like integrate it to a model since a lot of the most valuable work in this space has been open source open AI is basically just gpt4 was like let's go take everything that's open source available add to gbt3 and then make gpt4 in essence and they added some of their own stuff but anything they added was very quickly open sourced by a lot of passionate researchers and so we are going to say open up that stuff but by the time we do my guess would be that they're already are going to be multiple libraries out there that do that type of stuff like it wouldn't surprise me with say a Phil Wong lucid dreams it's a crazy smart person who like opens horses every model like I I'm pretty sure like that's still going to be the go-to place um to find these types of Innovations no matter what save body or any other company does like there are open source is always going to lead the way and really any business is just going to try and combine those things to a model simply because there's tons of Innovations but they all tend to be like one specific area one specific paper one specific model combining those is where I think a lot of the emote and the opportunity lies and to your point guard our business very clearly sits in the um you know a lot of great open source work not not immediately applicable at a business level right and so our business isn't taking a lot of this work and making sure that it is Enterprise ready um you know for business applications which um you know is is a sort of extension of that as I said I think you know we'll probably end up contributing before that on how can we help um Advance some of the work that we then use yeah fantastic um awesome Stephanie gunjan that was such an incredible coverage of all these topics um I learned so much about training embedding models all the things how you're seeing the space um as we're wrapping up uh do you maybe have any uh links upcoming announcements things that uh people listening can use to you know keep up with all the work you're doing um yes we are releasing a new web app where you can try our color model um there'll even be some data so that you don't have to prepare your own um and uh we'll be coming on our new website on body.com so definitely check it out and uh you can apply to get a link to the API if you want awesome thank you so much thanks so much thank you ", "type": "Video", "name": "Stephanie Horbaczewski and Gunjan Bhattarai on Vody - Weaviate Podcast #53!", "path": "", "link": "https://www.youtube.com/watch?v=spaoRhZPGTM", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}