{"text": "Hey everyone! Thank you so much for watching the 72nd episode of the Weaviate Podcast with Madelon Hulsebos!! Madelon is ... \nhey everyone thank you so much forwatching another episode of the weevapodcast I'm super excited to welcome Drmateline holos one of the world'sleading scientists on tablerepresentation learning andunderstanding structure and AI I thinkthis is just one of the most excitingtopics I'm so excited to talk to matonthank you so much for joining thepodcast wow fantastic Conor thank you somuch for the invitation and great greatto be here just one correction I'm not adoctor yet oh sorry no no no I'm aboutto actually hand in my te is NE today souh no it's almost accurate almost that'sawesome yeah I I actually had the samething where I did I submitted mydissertation and then did a podcast withBob like right after so awesome glad tobe a part of that but so it's successfultiming I guess right yeah yeah yeah soso this topic of table representationlearning I think this is just soexciting I've been trying to understandthe role of structure in Vector searchfor so long and I think kind of the thequestion I'd want to kind of set set thestage with is to understand you knowtabular data it's always has thisNarrative of you know all the world'smost of the world's data is in tablesand people have this mix of like numericcategorical features that they organizeand to just kind of understand like umthe intersection of that in embeddingsand how you kind of seethat um well I think in terms of theintersection of of tables and embeddingsIthink I think it's always been veryconfusing to people um as people well Ithink now most of the research has beenfocused on you know language embeddingsand just you know a very raw raw textembeddings and and then um everyone isof course very keen to adopt suchtechniques and models and embeddingsdirectly for tabulardata but I think what has been oftenmisunderstood is that tabular data isn'tas simple as as just text um so thereare many intersections though betweenthe two and of course many tables alsocontain text but as you said there isjust a lot of structure in tables youknow columns all relate to each otherrows relate to each other um but theyhave also very specific properties uhthat we I'm sure will get to in a laterstage um but yeah I think it's it's ahard Challenge and interesting challengeto also see how we can tailor all theseyou know representation learningtechniques towards embedding for examplenumeric data as well and see how theycan perhaps complement eachother yeah well I I and I I think Ididn't mean to limit it just toembeddings I mean kind of this wholeidea of machine learning on tables and Iknow things like you know table tosql isone of these topics that we'll cover inthe podcast and so I guess that kind ofidea I want to explore more like soembeddings of numeric properties somaybe like you have a patient electronichealth record and you'd have like anembedding for the age attribute so isthis is this kind of the thinking likeyou you take like a a table row thatwould describe you know electronichealth record maybe just to stay in thatone and instead of maybe translating itto text and sort of putting the texttranslation of the tabular row into likeopen AI embeddings you instead have someway of learning representations fortabularpatients ah yeah so I think there arequite some flavors to to this task oneof them is indeed for tabular machinelearning where indeed you have rowwiseembeddings and want to perform some kindof classification task on top of that sothat's one big field that is nowactually coming up with um pre-trainedmodels such as step pfn very interestinginteresting to to check that out it wasuh coming out last year um and you canjust use such embedding models on top ofindeed for example healthc care data toclassify for example diseases perhapsalthough it's of course um well relevantto First evaluate that on specific uhuse cases and domains um but thenthere's also another field that is thathas been training these models acrosslike Millions of tables to for exampleuh retrieve embeddings to service thesemantics of columns for example togenerate metadata so you know if youwant to build a data model and umperform for example data validation onall such columns that you know containage ages for example of patients um it'svery relevant to understand how you knowdifferent kind of tables relate to eachother so what actually I've seen a lotis that different Healthcare Providersthey want to integrate different sourcesof tables um of their data and then theywant to integrate this data together butbecause everyone has different conceptsto refer to ages for example anddifferent ways to specify suchattributes it's very hard to map acolumn of Ages in one table to anotherum so this is a very interesting usecases that can be uh performed if youhave you know accurate semanticembeddings of table columns forexample yeah I think as I was studyingyour work and learning about this kindof semantic type detection for columnsthat was quite new for me to to havethis idea of you're looking through atable and you're like what is thiscolumn uh talking about right is I'malmost like I'm I'm very like kind of atthe crossroads of where to go next inour conversation because we could talkmore about this kind of column uh typedetection or that idea of like tableembeddings like not just individualpatient level but searching through thewhole the whole table I find that to beso fascinating so yeah maybe if uh wecould stay on the type detection we'llcome back to the whole table embeddinglevel but this this semantic typedetection can you just kind of yeah likethe beginning because I imagine this iskind of new for a lot ofpeople yeah so I think one of the thethe key top asks in many data managementand many data analysis applications isdriven by the semantics of columns soindeed you know what do columns reallyrepresent what's in that column um inlike real world Concepts so for examplewith the with the example of Ages youwant to label that with likestandardized semantic types age addressand there are like millions of differenttypes that you can think of but then allin a standardized way and this isactually a key task for tablecomprehension so basically tableunderstanding as you might also see forexample analogous to for example imageswhere you have object detection you wantto understand you know if there is a youknow a cat in an image or something likethat you want to have something similarfor tables to understand what thesetables actually contain but this canalso be used for for example dataDiscovery um if you have accurateembeddings or representations of columnsum and you can generate metadata such assemantic column types from that then youcan use it also for data Discovery forexample or gener recommending datavisualizations for specific tables umgiven that the tables contain certainsemantictypes yeah I think the timing on that isamazing with snowflake acquiring Ponderand I listened to Doris Lee things likeStanford seminar this like kind of uhdata Discovery like kind of uhrecommendations on analyses andvisualizations of your data and I guessI'm I'm so just so curious like I I'veseen your work on git tables whereyou've extracted millions of csvs fromGitHub very cool I love the datacollection is always awesome but so soyou have all these tables and now you'renow you're trying to say I know likemaybe two three out of the fivecolumns try to use that in the values toinfer the type of the fourth one maybehelp me further understand just kind oflike the application of the typedetection of the column and sort of thethe context it sitsin um so ideally we want to trainmachine learning models over all thesetables to perform for example semantictypedetection um labeling table columns withsemantic types and we want to do thatalso for new and unseentables so that's basically the idea andwe can do that in of course verydifferent ways you can even just look atthe header so for example the columnname um but the the values in thesecolumns are very you know they providevery important signal to what thesecolumns are about of course so typicallywe try to embed full columns um topredict the semantic type ofit yeah I'm I'm so maybe looking at thedistribution of the values and so yeahwith that kind of thinking I'm verycurious like how language models andsort of you know chaining maybe you youhave like how you can call the pandasfunctions like DF do describe is kind oflike a common function that like givesyou all these views of your data andpandas how have kind of the languagemodels help with that interpreting whata column is in the characteristics ofcall well I think that's a that's a veryinteresting one and basically wherewhere I myself started off with um waswith word embedding so assuming thatmany tables actually contain textualdata and this is a severe limitationactually of this approach um what we didback then is just trying to map everycell value in a column to I think backthen we had like glove embed things umso we basically try to to aggregate allkinds of wordfactors extracted from from columnvalues now I think there is you knowwith representation learning and um thishas been extended to tables as well withtable representation learning we seethat typically what what what is wellmost common to do is serialize entirecolumn columns like cell by cell into asequence and then provide it to languagemodels basically and there are kinds oflike adaptations of the me uh attentionmechanism to also learn for examplerelations across different rows acrossdifferent columns so there are many liketailored approaches to adapt such uhlanguage models to tables as well andthis is super cool this has enabledactually very interesting applicationsfor you know Beyond table comprehensionso beyond semantic type detection thishas enabled for example questionanswering fact verification over tablesand it's really really super cool to seethat um but of course there are alsolimitations to suchapproaches yeah I think it would be areally nice transition to asking moreabout your work on that kind of uh tablebased question answering I think we'veseen a lot of uh kind of like in theretrieval augmented generation toolingWorld we've seen a lot of text tosqlkind of things and yeah if you couldjust continue on how you see that tablebased question answering uhapplication yeah I think it's so one ofthe driving forces of uh git tables wasactually that most of these models forexample um uh pre-trained table modelsfor question answering they have beentrained on tables from the web um andassuming certain structures of tablesthat were slightly shortsighted sothere's you know so many specifics totables that uh you cannot just you knowassume these models to to uh work without of the box um but for questionanswering what what I've seen isthat um when you try to serialize theseum these columns and rows one thing thatis typically missed is that there is youknow for example properties such as roworders insignificance and column orderinsignificance about in databases so inrelational tables and what what isinteresting about these these currentpre-trained Table models for questionanswering is that they can be verysensitive to such characteristics so oneof the the driving forces of for forexample get tables but also um moreanalysis uh work that we've been doingwith for exampleObservatory um is to try to figure outhow well these table embedding modelsgeneralize to different applications anddifferent data bases and I think one ofthe shortcomings now is that well wehave many steps to make still when itcomes to scalability so these modelshave been trained on well relativelysmaller scale um data sets of tables nowvery recently a huge table uh collectionhas been introduced as well byapproximate Labs very cool to to seewhere that is going um this data set iscalledtlib and then I hope that these modelswill also become more robust for exampleto uh for example small semanticperturbations in tables so what we'veseen with Observatory is that which isan analysis tool for um trying tounderstand what table embeddings howrobust they arebasically um one of the things we foundthere is that if you make very smallchanges to for example the question youask um or the table that you ask aquestion on even though the semantic sothe meaning of the question remains thesame um they don't really capture thatwell and I assume that when we skillsuch models upthat you know as with large languagemodels these models will also becomemore robust to such semantic variationsforexample I think with me I'm trying tounderstand um like the the the languagemodel kind of uses like uh pandas or SQLapis to kind of answer symbolicquestions like it could take in theschema of the table and then it canformat like you know select from wherelike the the CLA and so I'm trying tounderstand like how you would have uhlike instead passing the table directlyto a machine learning model that wouldoutput some kind of uhanswer you you mean if if that is whatwe should bedoing yeah I guess um sorry if this isisn't super bow form but I'm trying tounderstand that I guess for me it's likeum these tabular machine learning modelsI guess they take as they take as inputa tableright or maybe a a a row and and thenproduce the answer based on you know byprocessing the table with some kind oflike maybe columnwise attention and thiskind of thing and so I'm just reallycurious how that differs from say uhjust interfacing my large language modelwith like a pandas or SQL API where itcan um if it if it's doing symbolic a Iguess like if it's doing symbolicaggregation like what's the average ageof country music singers I I say that alot now I guess but like then you knowit just has to like select from thesinger's table where genre equalscountry music right so I guess that thatkind of yeah indeed so these Pro thisthis problem of question answering uhspecifically has been approached fromtwo directions indeed one mappingnatural language questions to SQL andthen executing that on a given tablewhere of course the table schema isintegrated into that uh because you needto understand or need to map the likeSQL attributes to a to a given column Ithink that's actually a very very goodapproach to take and it's I think a verylogical one also given the sequence tosequence uh modeling approaches that wethat we currently have um and I thinkwe'll we'llsee what I I think it's a it's a veryinteresting approach um to take but it'sit's of course also limited I think umwith what applications you can servewith thatum but I think the the other approach ofI think they they kind of relate to eachother uh as well a lot um but forexample for fact verification where youwant toum give a fact or check a a given factagainst a certain table then we mightneed different embedding and differentapproachesum to this problem um but I think textto SQLum I think it it works well forrelatively simple basic cases at themoment and I'm really keen on seeing howwell it works if our queries ourquestions become way more complex andvery recently there's a new Benchmarkintroduced that actually tests text toSQL modelsfor way more complicated schemas butalso more complicated questions becauseso far we've been only working with withvery small questions on a single tableand I think as soon as you want to youknow ask more complicated questions onyour data then it will become muchharder to generate relevanceequel yeah I think that's so inter II've studied a little bit of like thespider data set for text tosql and I I Ihave a bit of experience with thisbecause I've been working on the uh textto graphql gorilla with we v8's graphqlAPI and so yeah this this thinking oflike these complex schemas like with we8we kind of have like a collections likeabstraction where you have like a uh youknow like student teacher book these arelike separate classes and so it's likeyou'd only retrieve kind of one schemaat a time to format the query for oneschema hopefully that makes sense butlike if you're if if you're like joiningI can imagine like complex join is wherethis problem gets really difficult and Iactually think that would be a reallynice transition into the embeddings oftables as a whole because I imaginethere's maybe something to the Jointkind of with what you were sayingearlier with um I have this electronichealth record where I have patients andI have age is one of my column valuesmaybe want to try to join this to someother table but I'm not sure what whichtable right so I need to kind of likesearch through tables to maybe do thatkind ofjoins yeah I think especially for forsuch cases you really need to embed theunderlying data as well you just mappingcolumn names to um natural questions andlike generate SQL queries from that Ithink won't work um if you want to forexample integrate data from verydifferent sources um so I'm I thinkthere's actually a later a newerBenchmark called bird and birds reallyprovides more complicated queries thatall these models really really uh findchallenging so I think it's reallyinteresting to see that suchapplications will will you know Empowerus to ask natural language questionsover our data but there are definitelymany many challenges ahead also when itcomes to for example um the input limitum if you want to embed tables that's athat's a severElation um because of the amount ofcontext that you can provide um if youwould then need to scale up um acrossdifferent different tables this becomesalready quitechallenging yeah I think that therethere's so much that I want to keep I II guess I really like this kind of likewhen I thought back about likeelectronic health records and embeddingpatient rows I really like that kind ofrag where you would retrieve similarpatients and then maybe have that kindof you know the supplement in theinputting input that comes from thatyeah I guess um yeah I kind of want tostay more on these um complex questionsthis text to SQL thing and I'm verycurious about this kind of like um uhlike query execution planning like I Idon't know too much about how SQL doesthis but I I know that there's like alot of underlying mechanics to like howto optimally uh do a query do you thinkabout this kind of thing as well and howum machine learning could help yes yesthat's a that's a great question I havebeen thinking about this a lot um but Ihaven't been working on this and to behonest I think no one really has so Ibut there are so many opportunities Ithink when it comes to queryoptimization as well but this is justyou know gas work at the moment andneeds more experimentation but I thinkthere are very interesting opportunitieswhen it comes to for example cashing oruh you know as you said um yeah I thinkthat's an interesting um direction toexplorefurther super cool so uh kind ofpivoting topics a little bit I kind ofwant to talk about um the kind of I'mvery curious like what you think aboutgraph neural networks because I've heardyou talk about like columnwise attentionand these kind of particulararchitectures for tables and relationsand I think you know graph networks it'slikealways trying to figure out what whatit's going to be uh what that's going tobecome I have seen some interestingapproaches um that indeed use graphneural networksto try to model the relationships andtables and I think they are very successthey can be successful but I have I Imust admit that I'm personally not a bigfan of of GRA networks themselvesum I don't know why it's just somethingthat you know I don't know it hasn'treally taken off so far I think um butdefinitely there are different ways toalso you know perceive tables and youcan also see them as you know they canbe well represented by crafts as well soI think there's definitely value in thatum but I haven't looked into thismyself yeah I I guess it's it almost islike I guess with the graph Network youneed to have like the whole graph isinput and I always thought that was kindof like it makes the inference kind ofuh challenging and so I guess kind ofanother thing I'm curious about is yourcurrent sentiment on on like XG boostand like especially in kind of likesearch and recommendation that likehaving a symbolic features about yourcustomers to then like rerank yoursearch results with an XG boost model isquite a popular thing and so yeahgenerally I'd love to just get your takeonEX I think it it was super interestinglast year um we organized this Workshoptable representation learning for thefirst time atNubs and by then I've been mostlyfocusing on data management applicationswhere we were more interested in thesemantics of data in generating metadatawith for example semantic column typesthen when we organized this Workshop umpeople really related this to HG boostdirectly anything on tables people thinklike no but actuallyyou know outperforms anything and I waslike oh but this is a different task youknow that's rwise inference and notnecessarilyum you know across you know generatingGeneral metadata from from differenttables and I think now we see that thesefields actually approach each otherwhereas for example actually boost istypically trained on one giventable but now as we see also um morerepresentation learning being effectivefor learning relationships acrossdifferent tables for tabular likerowwise inference like actually boostand I think we see now more and moreapproaches that try to try to embed rowsand see if that works for such cases aswell and I think they have been shownreally competitive for um against XGboost and other tree based umclassifiers so so that is superinteresting to to see and come togetherbecause before all these um like morerepresentation learning approaches theyalso learned representations from onetable and now they start to exploretrying to learn representations and liketransfer semantics from other tables todo this kind of inference as well and Ithink that's a super interesting area toto see beingexplored yeah I think well I'm I guessit's like this idea of merging tablesand and I guess generally like thisgeneralization of uh symbolic that thatwould kind of be my uh why I don'tactually prescribe people to train an XGboost to rerank their search results isbecause the generalization probablywon't be as good as if you uh translateit to text and use like a cross encoderthat has a text is kind of myperspective is that because when youhave the symbolic things you're likereally like uh the machine learningmodel is like really fitting to theselike high frequency patterns I thinkcompared to kind of text translationswhere you have these like Richembeddings that I I think generalizedbetter U that yeah that's kind of how Isee that whole like generalization oftabular models but I I definitely am notan expert on it like just from yourperspective because you've been uh ofcourse coming from from a differentperspective um more on retrieval and umreranking so how do how do for examplecustomers as weate use EXO to to doreranking I guess it with recommendationit's like you have like age gender maybepurchase history these kind of maybetags and like I did some kind of likecustomer turn analysis and maybe I havea tag on you based on that other Mo whatthat other model said about you and so Ihave all these features that I then useto rerank like uh the shirts I'm goingto show you from this your home feed oryour search query so that that's kind ofhow I see it mostly but I can definitelyI'm I'm definitely starting tounderstand this better from listening toyou talk about it is this idea of likemaybe I have a data set of like I havelike my CRM data set and I you know Imight have come up with some columns todescribe my customers with that someoneelse from the company has differentcolumns that they've been using rightand now we're going to try to mergethese tables and it's going to be likeis this does this column agree with thisis this actually the same column kind ofI guess yeah I think what is interestingabout the approach of representationlearning for tab ml across uh acrossdifferent tables is that you canactually learn General patterns I I canimagine that you know differentcompanies although their data might beformatted very differently the samepatterns might hold and be relevant inin other companies for example in healthit it doesn't really matter how how thedata is specified if you can embed thisand and preserve the generic factorshidden in these for example rows thatmight represent patients or customers Ithink it can transfer such you know somepatterns from one say context toanother and I think that's that's aninteresting thing that actually boost II think will not be able to do it itjust doesn't have that you know highcapacity yeah that's really the well Ithink maybe it would be really nice tokind of like um come into so this ideaof transferring representations from onetable to the other I maybe like to havesome examples of visualizing it like Iimagine maybe I go to Archive and I getout like all of the experimental tablesor I know with your GitHub experimenthow you got millions of tables fromGitHub could we maybe have like someexamples of like I guess just like atransferring of table to table moreso yeah I'm I'm thinking of um beyondfor example the data integration umproblemum thinkingof well what we what we might see forexample in incertain certain health health data datbases Healthcare databases you mightfind specific patterns relating certainages with certaindiseases um and I think such suchGeneral patterns might transfer from youknow from one city to another city umfrom one country maybe even to anothercountry uh another I think good examplein in that sense is you have all theselike open government data portalsand there are like millions of T wellnot Millions but like thousands oftables on there um and I think there arejust general patterns to extract fromthese tables for example um relating GDPwith I don't know other economic factorsand and try to understand the worldbetter from that and I think if we wouldwe could use such if we could embed thatand like retrieve search data forexample using we8 then we could use youknow we could augment machine learninguse cases in companies for example thatwant to use that econom economic datafor I don't know in inference on theirvery specific company uhgoals yeah thank you so much for thatthat really helping me understand I I'veseen this data set called like Wikitables that was like also likeextracting any I'm really I think I'mdefinitely like uh my understanding ofthe whole thing is growing as we've beentalking about that all these kind oftables and trying to merge them alltogether into having this view of theworld is so interesting and yeah I guessfor me I've always kind of been thinkingabout like it's almost like with Vectordatabases and knowledge graphs graphdatabases I think about this a lot islike um what's the best way to kind ofrepresent a fact like can you just havelike a natural language sentence that'slike a fact or do you need like a youknow like a a knowledge graph tupal orlike a tabular data entry so how do youkind of think about just like that ideaof um you know the the I guess the otherapproach of instead of kind of mergingtables into a big big table I kind oflike translate them into facts and I tryto see like if the facts disagree witheach other kind of in in naturallanguage for like embedding basedretrieval yeah that's a that's aninteresting question I think so I'vealways been very driven by observingwhat data is around and how data isstored so one of my observations wasthat most data is collected and storedconsumed through tables in relationaldatabases for example or or perhaps lessstructured in CSV files so I always tookthat perspective and see that thattables typically drive very high valueuse cases from ter prediction to HealthCare Solutions and so on so I've alwaysbeen very you know fond of tablesbecause of because of that but yeah youcould you could also um present facts inin different ways but I think the powerof tables is that they are so structuredand they provide all these very you knowmachine readable interfaces to it um andI think that makes it a verysuitable a suitable format um to presentdata and facts in particular in umsometimes you know tabular formats havebeen taken a bit more flexibly so forexample Wikipedia now now we need toextract tables from Wikipedia and try torestructure it in a very structuredformat um I think if we would allpresent it in you know very basic tablesI think that would help a lot um andcouldalso yeah I think I think tables arejust super rich also because you cancomplement them withmetadata um you know they they come withSQL for example um as an interface tothis data and you can learn for examplepatterns across that how other peoplefor example query specific tables youcan learn all kinds of signals from thatum of course similar yeah similarprinciples or ideas hold to to lessstructured data for example just text umbut I've been just coming from okaytables are really everywhere indatabases so they're you know they theytake up the majority of the datalandscape the organizational datalandscape and they just serve so manyhigh value use cases across dataanalysis pipelines so that's actuallywhy I've been focusing on on tablesspecifically and structur data um butyeah there are different ways to topresenting facts and analyzing that yeahyeah yeah I love that I mean yeah like Iguess uh like looking at like uh ifyou're in the NBA and it's like who'sthe leading rebounder I like like statsyou'll see it in like the tables and Ilove that it's a good way of presentingdata I think that really Nails it itmakes me qu I'm very curious about likethis um you know now the language modelscan see right with like the gbt4V what do you think about that kind oflike that that they can they can see soyou might want like visually give them atable language models might want to passvisualizations of tables to each otheror is that like a native language butbut generally like I guess this conceptof like being able to see datavisualization yeah I think that's asuper interesting idea um and I've beendiscussing this with with someone thisweek actually that you know when peoplelook at at tables they kind of graspcertain patterns to to understand it andI definitely think that the visualrepresentation of a table just as animage of it yeah I'm I'm I'm supercurious to see how this you know howthis can help also for for the you knowfor embedding tables and and you knowgetting insights from them uh I haven'tseen much of the results I think justone paper on this where I was like ohthat's that's interesting but um it'sjust in very early stages to see to seehow it's used I think so far umresearchers have been evaluating the thecapabilities and thechallenges of using large languagemodels just without the visual um Umedium but and they've been trying tounderstand how you should best input atable into for example chat GPT um howsensitive it is for example when you umask a certain question and then swapdifferent rows in a table so Tables byby default by by their nature are wellfrom a relational perspective um theorder of the rows is insignificant sothat doesn't carry meaning but some ofthese large language models can be verysensitive to that just as they can besensitive to very small semantic changesin for example column name just the thesame semantics but just a different wordfor example can have actually quite someimpact so I think that that opens up newopportunities for for research andseeing how you know how we canintegrate properties of relationaltables um into such models and I thinkthe themultimodality will at some point alsoextend um to Stables but I'm I'm verykeen to see where this more like image Pperspective um and the therepresentation of a table from an imagewhere that will lead lead to I'm verycurious to see to seethat yeah I'm also like um I guessanother question I have is how the thestructured like in in kind of like thevector search world there's this oneidea that you have like an image of adog and then you have metadata about theimage like you know maybe the type ofdog the age of it or color and then You'maybe like uh embed the features as welland maybe average these embeddings soyou have some embedding of like GoldenRetriever and then you average that withthe image embedding for the dog um doyou think that kind of integration oflike maybe more unstructured embeddingswith the structured components do youthink that that you know that that canadd to the quality of theembedding oh for sure I think that'ssuch an interesting comment actually umwith this latest project calledObservatory we do some analysis thatrelate to that so the notion of you knowpreserving relations between objects inthe embedding space and I think ananalogy into the relational data modelis functional dependencies so given twocolumns in a in a table uh we mightperceive certain relationships betweenthese columns so for example uh thecapital if you have a column withcapitals and a column with country thenyou would well assuming uh you wouldalways see the same values for the samecountry so for example Amsterdam is thecapital of the Netherlands you wouldn'texpect other relations than that andwe've beenanalyzing um table embeddings to see anduh particularly comum embeddings to seeif these embeddings currently preservesuch relationship as well but so far wehaven't seen that so and I think thereis also from um the knowledge graphembedding space uh so you have forexample trans e uh which is a um anapproach for preserving suchrelationships across entities inknowledge graphs and I think this wouldperfectly extend also to the notionalfunctionaldependencies um to embed values in intables and then we can use suchembeddings if once they preserve thesefunctional dependencies so theserelationships between between columnsthen we can also better use that forexample for dataimputationum um which then serves perhapsDownstream a machine learning model so Ithink there is definitely interestinginteractions between these two yeahbetween thesefields and I think it's an interestingsuggestion actually perhaps moregenerally um to see how certain conceptsof of tables how that can be preservedor how we can find such relationsbetween certain semantic Concepts in theembedding space even Beyond functionaldependencies I think that's an yeahthat's an interesting um approach and weshould definitely see if we can preservesuch relationshipssemantically yeah I think that's sopowerful I I guess like um that thisthis kind of yeah like you mentionedtrans Z that kind of like uh embeddingof a particular kind of relationshiplike I guess right now it's likesemantic similarity is just like the onesize fits all for all but like insteadyou if you had some other relationshipand then uh if you're within this radiusthat means that you kind of uh you knowmight also have this relationship andthen we store the structure of that yeahit's alsointeresting yeah I think and um um onething that I was interested about fromfrom we's perspective is but this isperhaps a little bit off topic but I wasjust reminded of this but from aretrieval perspective but um what kindof embeddings do you do you currentlysee so have you seen at we8 you know doyou have customers or I don't know usersum that actually use or retrieveinformation and embeddings oftables um I haven't seen it personallyoh well actually I do well I remember aconversation like I remember earlierthere is a bit of like um how do youevangelize Vector search with the uh SQLpeople and and that was kind of an ideais likeum I guess back then it it was like youwould maybe have like an auto encoderand I mean this was like before the zeroshop mod I think now the the currentprescription would be could youtranslate your uh your table to textright to put it into these embeddingmodels but I haven't personally seen toomuch of that that's why I just find thisconversation to be so kind of like eyeopenening to me is I I had never thoughtbefore reading your paper on G tablesand I had never really thought aboutthis idea of searching through multipletables or having you know like a milliontables it's I think it's quite creativeto be thinking like that and yes I wouldsay it's quite novel to search uh to touse like vector search search in tabularstructured data I think yeah so one Ithink Avenue interesting Avenue is nowwith retrieval augmented uh generationthat ifyou I can assume if you have veryaccurate embeddings of I don't knowmaybe table rows or columns or maybeeven full tables just one centic vectorof a of a table I can imagine that ifyou would then you know this would helpout with with r as well uh from a WEAperspective I can imagine that so muchdata is eventually stored in tables thatis relevant to provide as context tollms I can imagine that that soon peoplewill start to embed their tables as welland I'm just super curious to see howyou know how they will retrieve likeconstruct their embeddings and um Keento see that unfold yeah I I think maybetwo things I see on that is the firstthing is um with weeva you define aschema where you define which propertyyou're going to vectorize so say I'mvectorizing content but then I also haveum I I don't know like metad data aboutthe page like a say I'm I'm vectorizinga the podcast clip and then I also havelike what number podcast it is who thespeaker is maybe how long the podcast isand then I can I can search with theembedding of just the content and thenpass in the symbolic data around theobject into then the language model andthen one other ID I see from llama indexis this kind of recursive retrieval andI think you might find this quiteinteresting this is where you would uhsearch through Wikipedia Pages likebased on their title and then whicheverone is the top match that one might havea a table inside of it and so so likeit's like a two-level query where firstyou're searching through the titles ofWikipedia pages that took me tobillionaires let's say and then withinbillionaires there is a table that waslike you know age and exactly how muchmoney they have and then you would dothe symbolic query within the linked uhtable super coolinteresting super cool uh so maybewrapping up the podcast I I want to askyou kind of like um you know this kindof like what's next question like whatkind of future directions are reallyexcitingyou yeah I think on thatum first I'm just so focused onfinishing this PhD and you know startingstarting uh starting up new things oneone particular direction that interestedin is going from you know Insightretrieval from questions to generatingquestions from tables so basically tableto to question I would say um becausenow um most like data teams let's saythey they rely on domain experts to formyou know formulate their qu theirquestions fortunately we can now do thatwith natural language and then translatethat to SQL so that already makes itmuch easier for domain experts to accessrelevant data but one thing that I'minterested in is see if we can somehowlearn what relevance questions are for agiven table to ask so that we can kindofenrich um theexpertise um without needing the thequery let's saybecause you need yeah I think there'sjust I think there there is this kind ofstat that says that we only use 1% ofthe data that we have in in datawarehouses or maybe it's 5% I don't knowspecifically U but we just use reallylittle data and I think if we would beable to learn access patterns you knowwhat kind of questions people ask oncertain tables I think we could thenkind of try to move towards morerecommendation systemfor insights saying for a given databaselike hey we see that I don't know thenumber of of diseases for example inthis area has gone up and then I thinkthat could be really powerful so that'ssomething that I'm I'm really eager onto movetowards I I love that I think that's soinspiring that kind of offline questionDiscovery and especially with thisperspective on tables and not just kindof like the long tail of documentsthat's all just so amazing I I alsoreally inspired by that direction goingforward of a more offline kind ofprocessing of your data and and thatkind of uh like self-directed researchit's really amazing matalon thank you somuch ohsorry yeah no yeah I'm I'm with you onthe excitement and I'm I'm just alsovery excited to see you know startupsnow ramp up in this space um moreresearch you know it's just increasingin in embedding uh tables properly andapplications of that and you know as wetouched on for example internal databaseapplications for example query planexecution optimization I think there'salso so much space to F to still exploreon the application side um so that'ssomething that I'm very keen on as wellbut yeah it's been really amazing totalk to you um about this topic I'mreallyexcited matalon thank you so much forjoining the podcast I've learned so muchfrom this conversation I'm sure ourlisteners as well as well andcongratulations so much on the on thePHD it's super well deserved so manyamazing Publications and this wholetable representation learning it's justsuper interesting thanks so much Conorthanks for having me was really awesome", "type": "Video", "name": "Madelon Hulsebos on Tabular Machine Learning - Weaviate Podcast #72!", "path": "", "link": "https://www.youtube.com/watch?v=BMqxJpC4-Co", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}