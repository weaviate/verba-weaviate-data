{"text": " \nforeign [Music] air we are super excited to uh have all of you here and it's something that I haven't done in the first and the second edition and I kind of corner pointed it out so a big big thank you for taking the time to listen to us either live or later or especially if you do it once live and then re-watch it because this is such an amazing show big big thank you for uh for doing that uh and thank you up front for asking any questions uh and and all of that because this is the the beauty of um this episode of will be there that this is a live show so I am going to encourage you to like please please ask questions uh and don't necessarily wait until the end of the session if you can ask questions at any time and I can do this thing like I'm just now like hiding and showing Banners at any time I could actually pop up your question and say like hey Connor that was like a really cool question for you or Zen I think I have something for you and that definitely I I really really encourage that uh for from all of you uh and then at the end we probably will do a bit of a q a as well so if you know you didn't feel very encouraged during the session we'll probably do that and then that's also something that Conor pointed out like somebody actually tweeted at us uh saying like hey why don't you do a q a like we want to do q a that's why we are all live here um a little warning I don't know how this is going to go but right now me JP and Zen we are in Prague we came here for a conference called devrocon so it's basically a conference for a developer Advocates like us uh which is the good side the bad side is that our internet can cut out at any point so if suddenly I go like this that means that I'm not being rude it's just like my internet went wrong or I'm just playing jokes on you so for now that's it this is not happening so that's enough about the intro and and I have like uh we have like quite a few things to show some of them are in preview not everything is super live just yet um but we we have a whole bunch of really cool things for you to observe and watch um so this is really uh cool and then that's a little message from Connor uh so we should be definitely uh doing that and that's how I can also show you a question so without any further Ado uh I think we have JP scheduled to uh talk to us about data importer JP take it away awesome thanks question yes um so hopefully my internet stays stable this whole time but so far so good I'm just going to keep my fingers crossed during the demo um this POC some of you might have seen before this really came about while we're on our location in Italy and in between all the team bonding and having lovely food and whatever um we got to work on some cool projects and one of the things that um the motivation for this for me was that because I'm new and I'm learning a lot in terms of what's we've yet and how do I you know uh just let all the wonderful things you can do with it um so for example trying different modules and what have you I wanted to come up with a way of quickly importing some data because I'm repeating that process over and over and over again and maybe trying different data sets right so the motivation behind this was how do I automate that process it's a little bit easier and repeatable and at the same time you know automated sort of to like press the same or come up with the same schema over and over again and whatnot um so I ended up building a little uh proof of concept app based on streamlit um Sebastian if you wouldn't mind sharing my screen I'll try and absolutely so much thanks you're very kind um so this is the um I just hacked this readme file together so it doesn't actually look like that in real life but that's the repo location um that's where it lives it lives under my username in GitHub but yeah it's a fairly simple um streamlit app and it's not really a lot of lines of code so if I have a look at uh it's um it looks like a lot of lines of code but it's just still under like couple hundred or okay under about 250 lines of code so it's not very much in terms of uh the volume and a lot of it is just kind of formatting and um kind of making it presentable more than the actual logic of what it does and to run this hooks to run this um right and you basically need to have a couple of packages open um sorry again installed in your virtual environment and really the only uh libraries you need a webiate client streamlit and pandas obviously you need python it's a python app so you're going to need that um so once you have these libraries installed in your poetry virtual environment or um kinda like I've got here whatever all you would need to do is obviously need to clone the repo and you need to get lfs enabled because there's a data file that I've saved in using gitmet.fs as well but you just simply run uh streamlit I'm going to screw this up probably but let's hope not um dot pi and what it does is to open the app and at this point I will have to stop sharing this screen and switch over to my other screen so I think I had to stop the screen and then present oops sorry the excitement of live um hey that's part of the game that's part of the game you know drop out fantastic so that's what the app looks like it's uh I tried to make it sort of as clean as possible and um uh it does really have all the Hallmarks of a POC app because I added this really silly joke that doesn't even make a lot of sense about Wizards um and in order to connect to a weeviate instance I've got one running in the background [Music] um localhost 80. you just type in the address here in the URL and it'll connect and you'll see the subsequent options underneath so you can have a look at things like a schema and there's nothing in there you'll get this lovely test message and I can see here and then there's a couple of options here right so you can go to demo data sets and I've got a placeholder data I've got a couple of place or data sets on the right hand side these are things that you know I think would be interesting to upload and I'll work on that probably later but I've got this Corpus of wine reviews from kaggle and what you can do is set the number of objects input so the default is set as 500 but I can just make it you know 50. um and just click import right and that'll connect to the instance it does have a predefined schema and upload the data so if we scroll back up and go update database stats and expand that blog window below you'll can see the schema and some example objects so that's uh designed to streamline that process and if I have different modules enabled then what have you um that you know I can try and see what might or might not work and then start querying it to see how those different vectors and different Vector space and vectorizers might change what happens the other thing I've tried to hack together here is the ability to import your own data um uh I haven't got any data here so that's on me but the idea here is that sorry I reset this repo and then when I re-download I realized I deleted my local data but you should be able to select your local Json and CSV files and it'll Auto generate a schema based on that it'll kind of give a best guess as to if it's uh um if it looks like a bunch of integers from pandas it'll infer that and give it an integer alleviate file data type it would be an object data type and whatnot and you should be able to click just a few buttons and import data that way as well um and also um JP yeah sorry to interrupt How brave do you feel because I could send you like a dummy data file uh to see if that works you're the trial yeah we could do let's let's how about we Circle back to it and then um I can try that while because I I guess the thing is on my phone I'll have to be downloading data and I don't know how Keen people are to be like watching me download that off slack and then put it somewhere and then copy it but yeah let's Circle back in it's a it's a two kilobyte file okay no excuse eight kilobytes apologies okay let's do it I'm dropping on slack um it's a date like a handmade data set I've created because I'm playing with it so it'd be cool to see what happens um do I get fired if it doesn't go live like if it doesn't go well is this like that um not on live like we'll do it after the call I like how non-committal you were about this by the way you had to like think about it for a long time well you know like so here's the funny thing he is just like to behind that wall so like it's not like I have to go fire him I can go and Shout at him straight up that would be a first time right nope no didn't work yeah so what I will say this is that your handwritten Json file is not the way I expected it so that's that's on you is um my take on this whole thing um so CSD csv's work better because I found that when I was pausing json's some of them were like nested by default um when I paused it and yeah it was a bit tricky but csv's work work much better on that oh okay okay sounds good but um the skeleton is kind of there and I'm planning on working on it when I get some time uh to to say add a dummy image data set and what have you and and the idea is that you know now you've got uh you know we're able to populate the the instance here just a you know a couple of seconds and if I'm making changes to the instance and want to reset it and then upload data I can just do that you know reproducible consistent Manner and if you have your own data set you can start from this and edit it and then you can have your own interface and this works with one of the WSS WCS instance as well so I won't go the same process again because it's just going to be me clicking the same buttons but if you replace this URL with I will do this though um I think it's called import tests and my network it'll connect to that right then if I go update database stats now you can see it's gone back to an empty instance and I can do the same thing in upload data so yeah um that's what it is uh please feel free to check it out and if you you know find it useful I'd love to hear about like how you would use it and if it's actually addressing any kind of like you know problems or annoyances you have in your Dev environment cool thanks JP and I think the intention of it as well is like it's not like this exact tool will become a production ready specifically application but it's also I think something that we are experimenting with and the kind of things we should have in the future once the real data importer arrives uh it's not like this is this isn't real this is still real but like you know something that we can we go like oh yeah uh we will definitely put our name behind it yeah I mean like it is uh yeah for sure it's a real app but it's not too far from just the idea of the ephemeral idea of an app um so it's it's it kind of does the job but for sure um yeah it's I'm not sure it's going to be production ready in its current state but um hopefully it sort of gets the conversation going and gives you some ideas about what might be useful down the line yeah absolutely and I think that that feedback for example on the ux of it or the kind of features or maybe the kind of files that you may want to import like JP was just saying oh I've tested on CSV but like Json you know different things may happen um I know like I think we could be looking at like the Json line files or or something so that kind of feedback is super useful as well so even if you just play with it and then try it out and then say Oh we work well for my scenario that was so and so then that helps us build that like uh the big thing later yeah yeah I think so and the only thing you would replace in this instance with yours would be the parser right for the file and then once that's done it'll read the file correctly so yeah yeah just like Vivid it can be modularized right so if you then suddenly have like a very specific file format right you might be able to just like add that uh importer right they not important but like the file parser right yeah if you look in the code um the file Plaza is modular so if it sees a CSV it calls a particular function if it sees a Json it calls a different function and you just replace just a few lines of code um I got a little bit lazy with the CSV and it loads it as a data frame whereas with Json I try and get it to user try to get it to user generator so it doesn't obviously you know Json files tend to be quite large if you're reading a corpus um but yeah so you can adapt that to your data set size if it's small enough just if you use a data frame it'll save you a lot of time because pandas are in for a particular data data type based on the kind of data you have um so did it up to like do as much work in terms of identifying data types yourself based on some samples of the data sounds good so and and I know you we had like a little request slash announcement slash uh thingy you know I can show the banner you know if you in case okay sorry yeah um I was just assume people would have had enough of me after 10 minutes so uh so if you did get the vb8 newsletter um that went out last week I believe you might have heard about this and the uh we've been making some updates to our documentation um the authentication part in particular and shout out to Dirk who's been helping me a lot a lot with this and he's uh in particular working on our clients and the authentication capabilities that they will support natively so it's been doing a lot of really cool work on that I've been kind of learning a lot about how oidc Works um one of the things we would like to better understand is your as webiate users your needs and your desires as to what are your authentication needs what is your preferred things like authentication provider what is your prefer what are your preferred clients in terms of languages is it python we think it's mostly python but would love to hear what the breakdown looks like um and and you know if you are using oidc as your authentication provider or mechanism what uh YDC what they call flows which are different kind of workflows or mechanisms by which they provide the tokens right so we've got a survey it went out along with the newsletter but um I'll just drop a line in the comments I believe with the link to survey so if this is relevant to your interests of course uh you know um if you're not don't worry about this too much but if you would love to hear from you um please feel free to a responsive survey or just even catch up with us on our slack Channel okay I think that's perfect thanks so yeah so that's I'll drop that up here about now YouTube in a couple of seconds when I get to it yeah I think sorry JP um all right this is excellent this is excellent so uh next up we have Zen uh Zen what are you going to talk to us about hello can you all hear me yeah I can okay awesome um so today I wanted to talk about a really cool data set that we've been uh working on and this is a data set from meta uh their AI team formerly Facebook it's called sphere so I'll post a link for this in the chat here for people that are interested and this is a um this is the announcement that they uh that they posted when they came out with this data set and and you should be seeing that in the comments uh soon here but the idea behind fear is that it acts like a knowledge base so that you can train large language models uh on using that knowledge base so where does it come from simply stated basically engineers at meta scrape the web they scraped about 130 million uh news articles and they chopped up those 130 million news articles into 100 word uh sentences or Snippets essentially and then they uh release that open source that as a as a data set so in total there's about approximately 900 million sentences or statements that consist of the sphere data set and the really cool thing about the sphere data set is that it's one of the very few large-scale data sets that have not only the sentence so something like McDonald's is an american-based uh fast food chain it will have that sentence but it also has the vectorized representation for that sentence so yeah 900 million very large scale so Sebastian if you share my screen here I can show you exactly what one data point looks like in this uh in this data set yeah so there you can see that every every data point has an ID just a unique identifier it has the URL the the article from which they got that sentence it has the title of that article and then it has the raw uh natural language what what are those 100 words that consist of that data point and then it has the vectorized representation for that uh for that uh for that text right so hey Zen um small Interruption can you like zoom in just like one or two steps just so that everyone can see and then please continue oh is that better yes yeah so okay I can zoom in even further if it's uh here let me let me I think should be fine yeah Okay cool so let me nice so this is just one you have 900 million of these guys this is just one uh one application right so this is the actual 100 word uh 100 word entry and then you have the vectorized representation for this which is uh which are all these uh all these uh floating Point numbers here right but the main idea behind this the reason why this data set is special is that you can conduct hybrid search on this data set so let's dive into what that means you can use conventional uh word matching search so you could uh you could say where in my database do I have the words High Commission of India in there and it would literally do like a regular expression type word matching and it would give you all the uh returned uh objects but you can combine that along with semantic search where now you can say show me all the articles that have these words in the in the article itself or they have these words in the name of the article but also they they are the closest in in terms of the vector space uh to to a particular article right so you can combine both Vector search as well as uh as well as your word matching and so this is a very large data set where you have both the vectorized representations and the text field representations of uh of your um of 900 million statements essentially and so the reason why this is why we're looking at this is because the amount of applications for this uh large code base are are Limitless essentially you can think about applications from fake news uh all the way to question answering um to hybrid search I mentioned one of the main applications of this is The Benchmark algorithms for hybrid search right and so what we did and we have a blog post out about this that came out yesterday so if you're more interested in this you can read more about it and I'll post that in the in the YouTube comments as well but essentially what we did is we wanted to make this data set very accessible meta AI open sourced it but one of the um the problems that it it's very hard to get it set up on your own computer I tried to get it set up on my system and it was quite difficult so what we did is we took the sphere data set and we chunked it up into 100 000 data points a million data points that essentially different size chunks and then we released it so that you can go to you can go to the blog post and you can get different chunks of this data set and then you can play around with it and so in our blog post we talk about how you can take different sizes of these uh of this data set you can bring it into a python environment or a spark instance and then you can populate uh weaviate you can import this data set into rewind and then you can do all sorts of interesting uh search or functionality that weva has on the sphere data set and so if you're interested in that let me let me post the let me post the link here actually I'm curious if that's going to work because uh it seems like YouTube doesn't allow sharing links so maybe what we have to do is uh update the description of the video after uh we're done and then possibly tweet those as well um yeah yeah we were just a bit too optimistic hey it's part of the learning process yeah but yeah that's that's uh essentially what we did here is we took uh we we took this data set that is really interesting to work with we chunked it up so that it's easily accessible and uh easy to import into python or spark for the uh wider developer community and then we we had some fun with it we basically imported this data set into uh into um into Eva and then we started doing hybrid search on this so all of that is detailed if you want to follow along all that is detailed in the blog so you can you can read through it as well perfect this is exciting um I know you have another topic that you want to touch on but uh I would like to maybe Circle back to you uh because uh you also just mentioned uh hybrid search which is sort of like a like a cue for Erica because I know Erica you are looking into that yes um do you wanna like let me maybe I can share your screen and then uh you can give us a bit of insight of what this is about yeah be great all right there we are all right so what hybrid search does is it combines Vector search with traditional keyword search in one query um so yeah I'm sharing my full screen um so here this is the GitHub issue for hybrid search and it is going to be released in the 117 which is planned for December 20th I believe uh this month um but I will say I can share the link in the YouTube comments I think you said it doesn't work but we can handle that at another time um but if you would like to vote on adding filters to bm25f and hybrid search you can click on this Emoji here and that's useful feedback for us to use and prioritize all right so back to the console um I have this query on how to fish an Alaskan pollock and what this is the alpha if it is set to zero at the moment it is done search and if it is set to one then it is sparse um so it's really um one I mean the whole point of showing this is that it's very intuitive and very easy to use hybrid search so if I just run this and if I have Alpha set to 0.5 this is hybrid search so I was using a mix of both so you can run this and the first result is The Ultimate Guide to Alaskan pollock fishing which is exactly what we're looking for right so it's just extremely easy to run this hybrid search and if we are curious on how it is scoring it um we can use this additional filter and then type in explain score and this will be cleaned up just a little bit but I just want to like show what it looks like right now um so we can see that bm25 has contributed this amount and whereas a vector it's missing in this little part but it's supposed to be Vector is scoring at this amount of The Ultimate Guide to Alaskan public fishing um so yeah I just wanted to show a quick preview of hybrid search and how easy it is to use in the VP console and I will share a link to the GitHub issues in a few thank you this is exciting that and I really love how like the kind of things that we're introducing on the Fly and then the kind of things that Vivid can do and is really exciting and I actually really love the fact that you were able to like do that preview of of what's going on because this is also a good moment for anyone to give us feedback right so if you look at this query and you're like well this looks actually rather complicated which I don't think it is actually personally it's pretty straightforward hybrid query Alpha but any ideas any suggestions like that there's still time for us to maybe act on it and then so feel free to give us feedback to to let us know or just say like hey double thumbs up this is amazing we love it right like so uh yeah absolutely so uh thank you for sharing this Erica because uh this is so cool this is this is mind-blowing and that comes from a person that didn't know the difference between Spurs and dense vectors uh six months ago so now I'm like so excited because you're always nice excellent excellent um yeah uh let me see who have we got next uh so um so Connor you you're doing like a an overview of like what's new in uh in Ai and research so how about we do that right uh tell us what's new in in AI in November right like what papers have you been reading uh what what really you got excited about because this is like yeah pretty exciting stuff as well awesome yeah well I think uh firstly the obviously the the chat gbt is the big uh is the big thing so but kind of I want to start with this new thing called Lang chain so uh Lang chain is about how we use prompting and kind of this glue in the middle of how we use large language models so as a quick example uh recently you know I was writing this blog post on ref to VEC and it had some errors in it JP Sebastian Erica Zane they helped me clean up this article and so I was thinking I wonder how well uh gbt can edit text and I've been fascinated with this idea of editing text for a while now because I think it makes a lot of sense is sort of how you would use a language model to generate text so the idea of Lang chain is to have these sequential prompts so first you say correct the grammar and spelling the sentence and then you say break up any long sentences into two or more shorter cleaner sentences and so I've purposely kind of uh deleted some of the uh like made the text incorrect a little bit like say ref the back is short deleted the T hey yeah should I be sharing your screen by the way uh oh sorry I didn't know apologies okay is it showing yes it is now yes a bit small but we can see your screen okay great uh so I'll zoom in quickly and explain this um so Lane chain is this new library as I just said it's like starting from the beginning but so it's like a way of uh chaining together prompts so you have sequential generation of language models as well as other things that we'll get into next so this is a quick example you give it the problem correct the grammar spelling the sentence Pat paste in the paragraph then it'll take that output as input to this input and then break up any long sentences into two or more shorter cleaner uh sentences and when it runs I guess it's running slowly but anyway so it'll iteratively edit the sentence so I think and then you can kind of monitor your billing with the open Ai and I think generally is just an incredibly interesting kind of thing uh so I wanted to start off by showing that quick example of langchang with the sequential prompting and that and that kind of idea of text editing to sort of set the stage for this idea of having iterative prompting and what that even means but sort of the thing that's more interesting especially in connection to alleviate is this problem of hallucination this is like the core problem of these large language models so I saw this on LinkedIn somebody said uh chat gbt is a search engine forget it the question is does Lori Mark Kanan play for the Chicago Bulls and chatur says yes he plays with Bulls when really he's been traded to the Utah Jazz so but what you can do is if you prompt it by saying here's a history of Lori markan's NBA basketball career then you copy and paste Wikipedia data then chat GPT will read the thing and tell you the right answer so the idea of levia would be to retrieve the context that's needed to ground Chachi petite and factually relevant information so what we're looking at with Lang chain is how to really integrate this tool use I mean there's of course the general idea of just you use the query to retrieve and then you just concatenate the retrieved context with the query and then just generate from there and that might work pretty well with just a general prompt like please ground your your answer in this factual information but there's also this exciting idea of a language model tool use so we view it has different kinds of queries so say you have all sorts of different objects and you're about to search through an object and maybe the language model wants to say how many documents do I have or how many uh I don't know paragraphs like imagine these data schemas with weave yet where you can have multiple classes you might want to run an aggregate query or you might want to run different kinds of hybrid search or vector search with wear filters all these different kind of filters that you could use and you can describe an interface that explains the tool to the language model and the language model will kind of know how to use it and this whole idea of self-ass Chain of Thought prompting I think oh this is just incredibly exciting so let's get a little more into chat gbt and why it's different from gbt3 so the the what they say in the paper instruct gbt the language modeling objective predicting the next token on a web page from the Internet is different from the objective follow the user's instructions helpfully and safely so thus this is this term called alignment that's what that's like kind of the buzzword that describes his research is aligning the language model with like what we wanted to do we don't just want it to predict what is most likely we wanted to do useful things so the way that they oh sorry I deleted this slide I guess oh all right anyway so what they do and I'll just describe this quickly is they hire a team of 40 expert uh 40 annotators and so what these annotators do is first they take the prompt that gpt3 received on the commercial API and they write their own answers for it so that's used as a supervised learning Baseline so you take the original gbt3 and you fine tune it to copy the answers that the humans had written then what you do is the the fine tune gbt3 is going to generate continuations and the humans are going to score are going to compare two of them at a time to train a reward model and then that reward model is used to continue training the open AI the uh chat gbt with proximal policy optimization so what proximal policy optimization is is uh the the the language model makes a sequence of decisions and eventually it receives a reward so the supervision is that reward for each of the decisions and it updates as parameters like that so I was a little curious about you know how much data did they collect I think that's kind of the key question is like what's the sample efficiency of this reinforcement learning from Human feedback like how expensive is it to train these annotators and get this data but they don't give you in the paper the exact number of annotations they just give you the number of number of that they have I think it's safe to say that times 40 is the upper bound maybe I you know assuming that each of the annotators annotated them once so that's kind of the key idea and you see this difference in gbt3 compared to instruct gbt on the question of what is the purpose of the list C in the code below so kind of pivoting topics I want to talk a bit about this idea of the chat gbt that's been trained on internet text compared to say some large language model that's been trained on like specific massive text so Galactica was this model for meta AI that came out trained on over 48 million papers and they actually took it down because this hallucination problem they you know it was people come to it for scientific facts and then it's when it does the incorrect generation it you know people were kind of irritated with that so this is kind of an example you know it's trained on this kind of things like Smiles strings code latex expressions and it's just a different kind of pre-training Corpus but I think the key question more so is Galactica versus chat gbt do we think these large language models need to be trained on specific data or do they just betray are they just train on any kind of data and then we can just retrieve the specific data and then just append it in the context and the language model is the general Reasoner rather than some kind of specific knowledge holder and I think we're leaning more towards the second Paradigm where you have the chat gbt can reason across any data and then something like weave retrieves the specific data to the specific problem you're working on and tools like Lang chain maybe glue in the middle part of it and we'll see how that all develops and of course weaviate is agnostic to exactly which part of the thing you plug in you could you know plug in your own language model or you could use open AI similarly with the retrieval models so now let's talk about model inference and I almost talked myself out of leaving this joke in here so hopefully it goes well so model inference is super expensive so you've seen tweets from Sam Altman saying that they're spending about 10 cents per every time someone interfaces with chat gbt and so I think this is a really interesting thing to consider as I've been playing with the Lang chain I've been spending about like one to three cents per generation but I think it's definitely something that will change the way you use these things if you have to be like every time I generate it it's going to cost me like 10 cents to probably change the way that you use chat GPT so there was a lot of stuff on model inference that came out I'm just going to focus on one quick nugget of it uh you know there's papers from Google on how they're running 500 billion parameter models and really detailing it but uh this this kind of so one of these researches is from neuromagic which is a company working on sparsity for inference acceleration that I'm really interested in and the headline of this they're you know they're using second order pruning this Hessian matrix thing is super interesting but the takeaway is allowing us for the first time to execute a 175 billion parameter model inside a single GPU so you know obviously if you could run this on a single GPU that is a breakthrough in how expensive these models are to run so I spent last week in New Orleans at nurips and this talk was the one that stood out to me more than anything else interaction-centric AI so we've seen this kind of this terming of model-centric data Centric uh where you know I didn't even realize data Centric had run its course I thought it was still the new thing but now there's interaction Centric so model Centric AI is about you know these metrics and you say they he shows this example where you know you're trying to generate images and you're solely measuring the quality based on you know how realistic these images are with metrics like Inception score on these ground truth image data sets then data Centric AI came along and I think startups like snorkel AI was were pushing this term really heavily where you know it's it's about let's clean up this data sort of similar to the idea of Galactica versus chat gbt where it's like instead of just language modeling something like the sphere Corpus let's language model like scientific papers and smile strings and stuff like that so data Centric I AI was all about you know curate that the data and organize the data really well and now interaction Centric AI is I think quite a different Paradigm it's about improving the user experience making it like usable and making sure humans can use the AI for what they are doing so uh Dr Kim explained this by showing this example compared to someone who's interacting with the generative model to produce an image you know you do this kind of prompt thing and what are the trade-offs of this user interface you know it's sort of intuitive as you you know prompt the dolly to then there are problems with it like trial and error you know lack of specific feedback and these kind of things is like The Guiding line for how we're building these systems so with our search systems are we just going to tell you the ndcg on you know the beer benchmarks or are we going to tell you you know maybe plugging it in with that gbt and measuring how long it takes humans to complete tasks and I think it's moving more so in like a human subject testing kind of Direction in addition to these kind of automated metrics and then it you know it's the idea of aligning models with user intent the alignment that we discussed from openai was obviously this reinforcing learning from Human feedback but now I want to transition into another paper that came out on aligning models with user intent in our search systems which is Task aware retrieval with instructions so this they're setting the problem particularly where you're using a search system and you're going to explicitly describe your intent as well as your query so they give the example of you search implementing batch normalization in Python now you could be looking for other questions you could be looking directly for the answer or you might be looking for the python implementation so the it's it's sort of like balancing all these intents in one in the query compared to where if you like explicitly tell it I want to find python code I want to see who else has asked questions like this or just give me like the you know like the Stagger overflow answer that kind of thing so they gather all these information retrieval data sets and they write instructions so natural questions has the instruction retrieve a Wikipedia being the data domain paragraph the units compared to say like sentence dialogue response that answers this question which is the intent so for all these data sets like you know natural questions arguana SCI fact they write this long list of explanations of the intent behind these tasks that they Benchmark and you see how they have so this is these are the instructions used in training and then say these are the instructions used in testing so you have train tests not only data splitting but also instruction flooding so I'll go quickly they you know they train the models particularly where you're encoding the instruction as well a couple different architectures for doing that whether it's just like a buy encoder style of doing it or whether you have the cross encoder style of doing it and then how you kind of sample the negatives and the negatives could be either you know a hard example where uh you know it's like um uh it's like a different kind of argument or it could be like an inverse of the instructions so the instruction is particularly being perturbed so the negative is that this thing didn't follow the instruction so here are some of the results and I I really think the most interesting thing is what we talked about in the first movie theater show which is prom together and comparing it with this kind of approach so prom together is the idea they use the large language model to generate training data for specific retrieval models for specific intents and at first I like that idea a lot but now it sounds sort of maybe overly complex compared to if this idea works I think this is a bit of a simpler way to do it and then this is the ablation of the impact of instruction so then kind of another paper that came out is about how we measure information retrieval benchmarks and I really like this is about the cost for one million queries you see say bm25 is really cheap uh and I actually the problem with this is I'm not super familiar with all these techniques uh so dessert ance I think these are like vector retrieval methods and I'm still kind of taking apart the differences in The Colbert implementations like how the plate engine speeds it up and displayed uh displayed vectors I think is like a language modeling uh head on top of the tokens but but I think what's more important is just this kind of focus on not only how well it can retrieve things but how much does it cost you to do that so then kind of another idea in efficiency is this paper called Citadel which is a way to improve the efficiency on Colbert so here are the results are showing that you know Citadel achieves really high you know performance as well as low GPU latency so the idea of Colbert is in addition to just kind of so the original Vector search idea is you pool along all the token representations to get the representation of the question the representation of the documents and then bang you search through the passages the passage vectors in Colbert what you do is it's a re-ranking thing where first you would search through this and then you have the maximum similarity of the inner product with the token vectors attending to all the other token vectors so uh you know it's probably gonna be like 512 token vectors so you have a vector Source through that so now what they're thinking about is like uh we speed probably doesn't need to attend to all these other tokens how can we have some kind of sparse Activation so they're using the same idea as splayed which is this kind of like add a mass language modeling head onto the token representation to get some kind of sparse Vector because Mouse language modeling is going to predict what token could replace it in the length of the vocabulary so it kind of produces these like keyword representation similar to bm25 so they're using that to have the sparse routing and make Cobra more efficient uh so then I'm super excited about this idea because I think it's very interesting to just be learning more about hsw and the you know the vector index structures so the key idea in Ood disk a n is that when query data is drawn from a different distribution so imagine that we build up our Vector index of image embeddings and the queries are text embeddings so it's actually disk a n face inverted file and hsw they're going to lose a lot of their performance Advantage when this happens so the authors proposed some improvements where in robust vamana you know when you have the graph optimization step where you're making sure that with the greedy search heuristic you can reach all the uh everything in the base set you're going to add a small set of the Ood queries to this kind of graph connectivity optimization that helps a lot I don't quite yet understand how they've modified product quantization but they're adding pivots to it optimizing with gradient descent and then they also have this parallel order thing which is right now when you're doing the graph traversal you pretty much you Traverse the hnsw graph and then you add the out neighbors into the memory and this is usually done via Random Access which has a lot of large cache Miss rates so they have this graph reordering to kind of put the out neighbors together in the cache so you retrieve it efficiently and this results in plus 40 latency plus 15 recall benchmarked on these data sets like searching through image indexes with text and I think this is extremely interesting for us to be mindful of as we're you know building up multimodal search indexes or so you could also have just like you you have a text index of nutrition facts and then the queries are like all about cancers something like that I know it's kind of a dark thing to go to but it's so it's it is a distribution shift in the queries compared to the documents and it would cause this kind of performance degradation in the index so to summarize I'm super interested in chat gbt and large language models Lang chain putting the glue between weviate retrieving the context and chat gbt you know reasoning across the context and I think it's a complete paradigm shift for how we're thinking about how you use custom data and deep Learning Systems I think this interaction-centric AI thing is a is a huge step in how we're thinking about evaluating these systems I'm super interested in these estimates of exactly how much these different retrieval strategies costs and then tricks like Citadel to you know make it more efficient and then I think this Ood disk a n was super Insight insightful about you know Vimana hnsw product quantization and the ideas I think that are core to alleviate as well as the user experience for search so thank you so much for watching uh please any comments I'd love to have answer and please check out we've yet to learn more about the web Vector search engine [Music] thank you Connor this was a very insightful and very interesting as always um and uh yeah if anyone has any questions about any of these parts that Connor described just now and and papers that he covered or any other things like feel free to drop us a question um and we talked about I I only mentioned something that Zen has another thing he wanted to share but I think it's also a pretty good excuse because then there's a question for you uh coming from Ari um basically I was like yeah for a sphere data set does the vector represent the embedding only for the raw text or Does it include the title Etc what do we know about it then yeah that's a good question um I actually don't know the answer to that because I would assume that it includes both the raw text and the title because the model that they're using to vectorize these uh vectorize this raw text as a sentence to Vector model it's actually the DPR model and it has the ability to you can concatenate multiple passages together and it would convert it into a like a 700 approximately 700 dimensional Vector but I don't know in the actual training of this in the creation of sphere whether they concatenated the title with the URL with the description that's a good point Connor do you do you have any idea around the generation did they do that or uh yeah so uh the the vectors came from a dense passage retrieval model from meta that is not open source but we were able to we uh grabbed a dense passage retrieval model from hugging face and confirmed like we ablated a few models and this one is like pretty close so it probably was like fine-tuned from that checkpoint and didn't change the representation space too much or something like that but do we know if they um so did they only train it on the text the raw text or did they also concatenate the title with the raw text in the generation of these uh 700 dimensional vectors oh that's a good question um sorry I'm not sure I know that in weave yet we we have the vectorized class name thing um yeah that's a good question I'm not sure if it's common to vectorize titles with the yeah that's a good point because I read the the paper on the sphere data set like the the web is your oyster that paper and I also read the DPR paper and it doesn't mention whether or not they concatenate the title with the raw text so that's still an open question that's a good point I don't know I you intuitively would think that they would do that but you know uh not exactly sure I also had a question about one of the things that you were talking about Connor um so I recently learned about the data Centric approach and there was actually a challenge around data Centric and now they're they're moving on to interaction Center um how do you validate or like how do you know if a model trained in this interaction-centric way is valid is it more around a b testing or like for for good old um model Centric I could just I have all these validation metrics and it's well studied and the theory is all known I have no idea what interaction Centric is yeah I I agree with everything you said I'm still trying to piece it together as well because you think that if it was the human demonstrations you could try to have that in a model-centric way where you still have the metrics kind of but I guess it's about thinking about how fast can JP generate the image of you know the if he's trying to generate some particular image how fast does it take him I think compared to just the quality of the image okay yeah it seems like every every day where I was only getting used to the data Centric approach and now we have another Centric approach and this field moves fast yeah I I was like I don't know why yeah same exact thought behind yeah we just need to wait for the corner Centric models and approaches excellent um so then I know you have one more bit to cover uh and uh go on yeah did we want to talk a little bit about the uh or not not yet yeah so the one billion oh yeah yeah yeah yeah I'm sorry yeah I I am recovering from bit of a cold or something that that's my excuse um yes so yeah very good point um so I was so excited about it that like my mind went completely blank um but intro the really interesting thing about this um the experiment that we went with sphere uh and then that's something that we've been also going uh as a true as an exercise uh we are we yet we are trying to uh push Vivi to go get to with one billion objects uh that will go in into vv8 uh so this is actually a really big deal and how big of a deal is it is um uh literally I was talking to attia and who's our CTO and he's like a year ago uh we're like pushing one two million objects into VV that was a big use case right and then so 12 months ago one two millions and we just literally go to like the next scale of like going to a billion and and literally the the the the process of how when we're importing the the big uh there's the 960m uh file that file from sphere that we're importing it and this was actually a conversation with I think that was Derek one of our engineers and uh and Tien I was like hey Derek what's going on how's the import going and then there was like yeah we are at 700 million uh everything looks fine like like it just keeps going like like super fast uh and then like it hit 900 and it was like yep didn't even we didn't even break the sweat you know that was like so cool uh where the only reason why we didn't hit 1 billion is because we ran out of data um but like resources wise and all of that it was uh it was super uh it was really great to see uh and and then like we had and this is the thing and it's not even a joke like we are already internally talking about like trillion objects right uh because 12 months ago if I told at the end I was like hey can we do a billion you probably have a heart attack and then today we were like trillion I was like yeah sure like uh what's the next magnitude once we have to achieved this uh this is absolutely uh crazy that we can achieve those kind of things uh without even being nervous about it uh and then this is just a matter of uh use cases and I think as then as you were preparing for the blog post uh Bob like our CEO he actually run some live queries for us that were looking to include in the blog post and I think you have so yeah we were able to even query that data set while the whole thing was importing and then at the end like the response rate was uh still pretty decent I'm not going to throw any numbers because I don't know them from top of my head but it was actually quite good right like it was like wow that's that's impressive that's impressive and I I think Zen you can tell us a bit in terms of like the structure of how the whole thing was structured in terms of that import um yeah yeah for sure so uh basically what we did is uh you have a way that if you want to get your feet wet I would recommend going just with the direct python import because that's about what 65 lines of code and you can get uh I wouldn't recommend putting all 900 million I don't think that's physically possible but we did it just uh just for fun so there's proof in the in the blog as well but I would recommend taking the 100 000 data points of sphere importing into vva and start querying that and just one of the examples that we showed uh in the blog post was you could import sphere and then you can ask it you can do hybrid search on it so if you're interested in the example we used was what is uh good food to get in Italy and then also do word matching at the same time to make sure that you're getting it from credible sources and yeah I'm not going to release any uh any stats around how quick or how slow that was but I think if there is interest from the community we're also planning uh to have a more specific uh post on how the how the road to 1 billion looked like right as we were importing all of those objects um so we're talking about releasing those numbers as well and if there is interest then we'll definitely do that as well yeah I mean that's kind of the beauty of being an open source company like a lot of the things we do is in the open right like even like our benchmarks that are on our website Etc uh like we we really proud about like what we are achieving and um you know always and this is always will be the case when we look back a month uh you know to the past we were like well we were you know not where we are today we're like so much better so much faster it's just like uh very exciting that like uh the sky is the limit right like uh I have a feeling that like five years from now we just ran out of data in the world and we and we will be like still didn't break a sweat that's that's kind of the Hope right in general yeah I was about to say is there a data set that we could try the one trillion on I don't think that exists um there is there is a bob already uh you know uh showed like a couple of data sets that had like a one and a half billion um and so we will be trying those out and we'll be sharing like uh our journey as we go and and everything I mean in a way in our quest to one trillion that this is going to be the new Quest probably now uh we have to find that data set is like uh to do that um so yeah it would be interesting to uh to see what we can achieve with that and I guess the other thing is not just like a sheer volume but also uh let's have a data set that makes sense because if you have just a trillion data points that you query but you don't get any meaningful information that that doesn't help much right so I think that's why that sphere data set was so uh is really great because we could ask meaningful questions and get meaningful answers and then whilst working at a huge volume uh so yeah that's kind of a trick and maybe that could be a question also to our audience right like where whether you watch it live and and respond now or in comments later like if you re-watch it later if you think of like some good use cases uh data sets that you think we should uh give it a go uh trying to see uh if you know if Sebastian starts get you know breaking sweat because this is a difficult task or not uh we should definitely do that let us know what you think uh that would be really cool to see and yes thank you Zen thank you for reminding me about the point because that's like this is the only announcement I have in this Vivid air and I forgot about it like how good is that or how about we were it was all planned we were saving the best for last all right well that is is that why you are going to talk about audio to back since you this is the last topic this is not this is not really a talk it's just a bit of a teaser I'm I'm working on a uh a uh with the engineering team a module that will allow you to uh pass in audio data and then we're using uh the uh whisper model from open AI that was recently open sourced and what we want to do is right now you have the ability to vectorize images and store the vectors and the objects into you have the ability to do that with text Data uh soon hopefully you'll have the ability to do that with audio data and the applications that you can think of uh are similar to like the Shazam app for example if you take a clip of uh of a song what are the 10 closest songs to that song Or if you hum a like just at the conference today I was talking to um talking to some of the speakers and they were mentioning that an application of this could be if you hum a song what are some of the closest songs to that to that hum right but uh I think that would be a really cool application uh when it does come out so this is not really an announcement or anything we're just we're working on it this is a bit of a teaser applications for it yeah yeah but I think it's not so much like an announcement by itself but but also this is something that is somewhat a work in progress like we we kind of like have it on the back burner and work it towards it so if any of you finds that topic interesting reach out to us like uh right right in here or on Twitter or on the on the community slack like uh you want to get involved like uh that would be great whether it's involved through direct contribution or you go like I just want to play with it and see if I can do like this exercise what are this uh recognize what I want because you notice this I immediately was thinking there's this special test where if you have a song in your head and all you have to do is tap it to the Rhythm and then get and make the other person uh guess what the song is and in your head you always feel like you're tapping exactly and anyone should be able to guess it no it's almost impossible so you'll be amazing if the audio to back could recognize that right you even I don't know you could try happy birthday and then none of us would know it even though everybody knows like the rhythm of happy birthday happy birthday so that could be a cool thing but yeah yeah you were gonna say something go on oh yeah I'm so excited for the music record the music search I think like we'll connect it with the generative models that will come up with like billions of potential Melody lyric combinations and then you can take a snippet and be like anything like this right and it'll just be crazy I'm just excited to see once we do have that out what what the community does with it right what types of application you can think of because now we're getting to a point where if you take any modality of unstructured data we have the ability to do semantically understand what you're talking about whether it's the podcast in in a uh audio file or whether it's uh images like that's the really cool thing to me yeah I mean you could like I was thinking like you could uh use it in a factory where like certain machines just uh before they break or just be when something's about to happen maybe they release a specific kind of noise or something so what if we could train like a model on like you know here's our healthy machine Sounds here is like when something's happening or blah blah blah and then you could uh very easily kind of like then go uh you know have something that constantly uh keeps checking and testing and then one point hey yeah there's something wrong with this machine um so that'll be interesting and as I was saying I remember I read about it uh at one point there was this experiment where like toddlers how do you call like a kids that are like up to a month old or something other toddlers right yes infants infants yes yes that's the word uh so there's a thing with infants that often you don't know what they mean when they cry but apparently somebody did the research where uh specific types of Christ uh kind of mean I'm hungry I'm in pain or I'm just annoyed or or something and then unless you like fine-tuned to to it uh you can't really hear it or understand so the whole thing was that that there's this people that they train the dog to recognize these things and there were this different kind of rewards depending on the kind of cry the dog was behaving in a different way giving signals on behalf of the baby uh and then they were like oh right okay time to change the nappy or like oh the baby's hungry and they're actually getting it right so maybe you could replace the dog with your audio to vac model then that would be cool yeah I was talking to Bob and uh Marco and Chris from Facts yesterday about like the interfaces for chat GBC and so kind of the transition from the story you're just telling Sebastian is I'm curious about like these brain computer interfaces like this whole idea of like you can uh you know have Bowens bark the natural language or babies cry to natural language is like I think we'll be able to read directly from our brain to send a gbt and it'll like suggest you things in text yeah yeah I we could do that go ahead I read a paper recently where a group out of um Stanford who's working on bci's brain computer interfaces they were able to reconstruct uh the imagery from a person's uh EEG so brain activity they could actually tell what you were thinking or what you were imagining in your mind just by looking at the brain activity and the results were fairly good if you looked at the image they were shown and if you looked at the Reconstruction from the brain activity it was pretty amazing that you could get that just from brain activity like it was amazing yeah the future is going to be weird yeah that's scary technology man it's gonna be great it's gonna be great um all right so let me just double check in case we have questions uh Shivani says hi so like let's all say hi Shivani thanks for watching joining us and uh saying hi uh who else wants to say hi he doesn't have to be a question or tell us where uh where you are coming from or should we bring an engineer or our CTO or somebody else like that it doesn't necessarily have to always be uh you know people from the developer advocacy team uh so let us know uh but we are all open to questions uh and if there's a request that you want to see Bowen again and I hope one of you will say because Erica is tired of doing it just because I ask and then please please ask to uh bring Owen back uh should we have a special camera for Bowen I don't know like I I argue we should but I think I needed some backing from the community uh and and I'm about to create a fake YouTube account just to do that she sleeps all day sorry to just watch her sleep I'll watch that yeah it's better than what yeah she's very cute yeah so while we waiting bow and watch yeah that's a very good idea then yeah we could definitely do that we could do some some sort of ml training that reacts is like yeah Bowen is hungry Bowen is you know sleepy you know all of those yeah we should definitely do that so whilst waiting for another question I'm going to do something go on I'm going you go I I can Circle back to the oh no no you need to be at your computer for this Sebastian um can you share my screen for me um well well listen I was listening to talks I did a couple of things so um Sebastian that data you said I can now import it was nice sexy and everything there we go so when you load the Json um it's loaded pause all the data it loads it onto the table and the cool thing about this is that you can choose the data type for each column basically um so you got the name link content and just the background this is uh list of meerkats or something is that what's going on Sebastian um I'm not necessarily to review but it's basically like a paragraph from Wikipedia in different languages at least the first three about meerkats but then you have like all four and then you have like other articles and some work you know what's going on uh testing things with different languages and and stuff so uh but uh I will talk we will publish a blog post about it on Monday uh that is a coordinated effort with some of our partners but it's something really exciting I can't tell you what it is just yet but I'm I swear it is going to be mind-blowing it's it's amazing what a teaser um that's an amazing kind of intro so basically what it does is um it's pars all that data and you can see the columns here with as in the schema Builder section and then what we can do is to choose the class name topless I'm just going to call that um meerkats let's say and we'll add a class and now if we check the data we've got now two classes in our data set we've got the wine review data from before and the meerkats data and all you would have to do then is click on add data and once that's all done that takes all of half a second we've got that imported um since Sebastian has hang on just two seconds I've just dropped something what hello where's your laptop hey we're real we're next to each other this is not worth a bit um I'm going back but now I want to see you come to my room uh you're in a different Hotel man some like TV production magic right here um at one point we're talking about whether we should do a bit where I'm basically getting broken into in the apartment in the in the hotel but then um didn't want to induce any any unnecessary Panic hey Sebastian can I ask you are you there now yes fantastic so can I share my uh vs card again you can't yeah well let's see whether I actually can um okay cool there we go um so if we can show that on screen so uh all I've changed in my code base is I've added this or these two lines of code right so that's all I've done to be able to like be able to uh not pause the Json before because this was my Json parser to be honest I'm not even sure if this was working before I think it would have been um but just because the structure of the Json was different um this is the only code that I've changed and now it is able to load the meerkat Json that Sebastian sent me so it's quite modular um I've got a separate function and there's a equivalent function somewhere for the CSV and just for the magic [Music] and what happens is um uh basically at one point it fires a callback when you press the button and this is the function that it calls and uh you can upload the data line by line and you can specify things like the maximum number of objects just so that you don't accidentally spend you know like an hour uploading objects and vectorizing them and when you just want to when all you want to do is test your test your setup so yeah um there you go are you problem solving done and hopefully I get to keep my job we'll see though okay you're fine you save I know where you leave you know so what next door your next door right um no this is perfect and I'm glad that you were able to update that because I think that's the the value of that in data importer uh and and I do hope that like next year we'll have something where you anyone will be able to like say like hey use all my data uh drop like this uh 900 million sphere data set you know objects and then go like import configure everything you know like it should be a lot more automated I think like the journey that we have today that you can write like let's say if you're using a python client or any of our client I think the import process is not that difficult but this was one of my first impressions when I joined the company I was like it's not that difficult but it could be even easier right so I was super happy uh when you started kind of working on like hey it's just like a proof of concept is the idea and I think that's where we all want to head towards to right like make working with weave or make work work with vectors and then your data as easy as possible right it should be a no-brainer it already is quite easy but I think there's still some improvements that we can make um and often uh we can only improve things that we know of so obviously the Importer was kind of obvious to me to JP and few of us but what if there's something else that we completely missing out on and it could be a low hanging fruit even right like something you could do uh quite quickly so please let us know um so I'm a bit of a spontaneous person and what I'm going to do like I'm actually pretty happy that like Ari and Shivani like say hi ask the question and everything uh so uh what I'm going to do is just say like I'm going to send you a t-shirt like to each of you like one of those green wavy t-shirts uh so probably what we need is uh if you're on like the community slack um like Ping me and then because obviously you don't want to share your address in here but yeah thing me with your details and I'll be very happy to send you a nice green wavy t-shirt um why not like you see this is what you do you say hi and uh you get a t-shirt bow and say hi said hi as well so I need to look for a bow and size t-shirt as well right but one you're excited you want some T-shirts yeah yeah I'll send some dog or snackles as well so that'll be fine um all right I think uh We've covered everything unless I really really forgot we had another thing but uh thank you all for uh for listening thank you for sending questions saying Hi and um thank you for being part of the community and then because that helps us do what we do and makes us enjoy it even more so that's for me thank you for uh for listening and uh thank you thank you thanks a lot Owen are you waving as well saying thank you no thank you bye bye everybody [Laughter] nice ", "type": "Video", "name": "weaviate_air__episode_3", "path": "", "link": "https://www.youtube.com/watch?v=EcXRX70t-Ts", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}