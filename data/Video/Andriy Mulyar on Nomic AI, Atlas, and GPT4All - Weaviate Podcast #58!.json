{"text": "Hey everyone! Thank you so much for watching the 58th episode of the Weaviate Podcast! I am SUPER excited to welcome ... \nthank you so much for watching the weviapodcast I'm super excited to welcomeAndre moliar from gnomec Ai and quicklybefore diving in this is Christmas comeearly for people working with weeviateand Vector embeddings no mix Atlasvisualization of embeddings is justamazing it's so cool to see theseclusters and we have an integration withweeviate to show but even before talkingabout that huge congratulations nomic AIon raising a 17 million dollar series aI can see in the uh the press releaseannouncement Atlas gbt for all all theseamazing Technologies so firstly beforeeven getting into any of the technicalstuff huge congratulations to everyoneat nomeg AI this is just amazing to seeand I'm so excited about how this spaceis developing so let's get into that alittle more about what this means ofspace developing so uh maybe you've seenthis kind of thing before this is topicmodeling we've had past podcasts withsay Martin Gruden doors from Bert topicyou know trying to understand how thiskind of stuff integrates with wiiate butso what you have is you know you takeall your documents and you embed themsay with open AI embedding things thatare 15 36 dimensions and then youproject that into a lower dimensionalspace with say tsne umap and so on andthen you visualize the lower dimensionalspace then you cluster and so then youcould like cluster the two-dimensionalpoints or you know you come up withthese topic labels for these differentkind of labels so you see like um if youzoom in these are like the point Cloudsegmentation this is these areembeddings for all the papers publishedin nuribs from 1987 to 2022 soyou know these kind of maps are sointeresting and so here's this tutorialon how to get your data from weviate tonomic Atlas for this visualization youknow flowing the vectors from Eva tonomec all the super exciting stuffpersonally I've added this to my weviapodcast search demo to play around withit this is like my dog food I use to seewhat all these things look like and soso what you see is you know you can takeyour data out of levia put it into thisspace and um uh hopefully that's a goodvisualization uh oh yeah sorry so let mejust increase the point size somebody'sa little clearer but you can see howeach of these podcast Clips you can seethe topics you click clustering andtopic modeling podcast Clips we weretalking about clustering and topicmodeling but anyway so this was so muchfun to play with with my own data and Ihope you know you enjoy seeing your datalike this I think it's we'll get into iton the podcast but I think this is oneof the most exciting ways to understandthe quality of your embedding spaces youknow it's one way to compare embeddingmodels is to see the latent spacesproduced by them and you know whetherthese topics make sense whether youagree with the Clusters and so on sowithout going into it too much furtherI'm so excited to present this podcastwith Andre this was such an exciting onehey everyone thank you so much forwatching the weevier podcast I'm superexcited to welcome Andre moliar Andre isthe founder of nomec AI nomik is doingso many exciting things for one examplethey have done this uh embeddingvisualization directly in the browser at10 million scale and Beyond it's soexciting to see Andre is also thecreator of GPT for all which is anotherlike topic that I'm sure everyone hasheard about so Andre I'm so excited totalk to you and learn how you thinkabout all these topics hey Connor thanksfor having me awesome so could we kickthis off by diving into the foundingstory of nomec uh like what led you downthis space sure yeah so like I thinkevery good ml story it starts at a nurbsmachine learning conferenceum this was like uh 2019um and uh this is when everyone in mlcared about images and nobody knew thatthat machine learning models were justthese amazing manipulators of text eventhough Transformers have been out forabout two years and I actually ended upgoing to this uh it was it was a birdMeetup so Bert was uh so before everyonewas naming um naming models after likellamas and alpacas it was Sesame Streetcharacters and there was this Meetup foruh for Bert which was like this modelthat released released in 20 2018 whichwas a Transformer model trained to sortof uh self-supervise over text and thenyou can do all these magical things withtext that you couldn't do before and atthat Meetup I actually met my co-founderBrandon um so Brandon CEO nomik he uh Iwas actually working at the time at thisstartup called red AI which was nowadaysyou call them generative AI startupsback then you call them machine learningstartups that uh things with textum and what they were doing is basicallysummarizing Radiology reportsautomatically with large neural networksand you can imagine because they'reusing at the time because fours came outthis is the one that always seems tobecame feasibleum and so we sort of met up there uhBrandon and I ended up working theretogether for about a year and we foundout all these sort of difficulties thatexisted in training large-scaleTransformer models on large quantitiesof text this is like way back beforegbt3 even came outum and so like we took those learningsand basically the tldr the learnings isthat you need to look at your data uhevery every everything in the processabout big neural network on a lot oftexts will eventually become replicableum everything from the data ingestionall the way to the model training themodel serving uh Mosaic ml for exampleis doing great things I'm like trainingreplicableum but the hardest part is always makingsure you have your data curated in theright way so that was sort of like thislike clear non-repocable problem that wesaw uh we sort of went to different waysafter the companyum and right now let's go work at Squareuh I went to start a PhD at NYU workingon Interpol machine learning and aboutsix months in uh basically what happenedis Brandon reached out to me he's likehey we both know this problem existslet's let's let's let's look at solvingit and sort of like the the interfacethat we imagined for how we would havewanted at the time to go about curatinglarge contents of unstructured text andlike a replica mirror to make it fasterwas sort of this interface that you seein our product Atlas right now right nowum this this this this idea that whenyou're going through and exploring largecollections of data you really reallywant the property where when you findone example of something bad orsomething good you find all of theexamples of eating this is actually aproperty that you only get when you'relooking for data through the context ofClick through like the lens of a neuralnetwork through the embedding space of aneural network that's what Atlas was atits core Atlas was this sort of solutionto our problem that we had where we haveyou know 60 million documents of textyou to clean them very very quickly andfigure out what's in there uh and thatAtlas was the sort of like answer tothat uh like overarching problem thatbecame developing and buildingbut that's sort of like the the theGenesis of noteum and we sort of sort of started outworking on that uh we had sort of theMVP uh in beginning of Spring 2022um we went out uh built out sort of likewhat you see right now in Atlas when youwhen you go to atlas.nomic.ai uh whereyou can go in just drop your data setsand you get these sort of observablesort of data maps that allow you to seetens of millions of pieces of textimages audio anything you can embed witha neural network all on one screen andthen be able to slice a dice andmanipulate it just like you wouldmanipulate for instance something in apandas data frame but you don't need tobe technicalum so that's what you see right nowyeah that that's a really compellingStory I mean I there's so many to somany stuff to that I think yeah I alsofirst became aware like coming back tothe neurops conference I think I I firstsaw Atlas um with the visualizations ofthe iclr papers of clustering them anddoing that kind of thing and I thoughtthat was so cool that's how I first sawthis whole topic modeling clusteranalysis idea but you've hit this anglethat I think is so fascinating of thedata visualization for curating trainingdata for large language modelsespecially when you're talking aboutpre-training them and doing the internetscale uh language modeling I think forme I first saw this data set called thepile with which is like 800 gigabytesand you had like this you know a nicelike diagram with the squares of likehow much is Wikipedia how much is GitHuband that was kind of like my first thingof seeing like the composition of thedata sources you mentioned Mosaic mlthey're doing a lot to educate here'swhere our data comes from the mix of itfor say MPT and you know I know uh withGoogle research quarkly is someone I'vebeen following for a long time and theyhave a paper called I think it's calledsomething like that where it's about thecomposition again like thisinvestigation um is if we could stay onthat a little more like so you know Ilove what you said also about findingproblems in your data set by looking atthe nearest neighbors so so you take youyou know you take 10 million or you knowmassive scale used to train a languagemodel and you embed it all into Atlasand then you start looking at like youknow here is where the high loss regionis like you know your model performedbadly on this point so the neighboringpoints tend to correlate is that howyou're thinking about kind of how youthen use this interface to improve thetraining data yeah so that's one way youcan use it um that's actually not theway that we had an envisioned in mindit's just like a useful way of using theinterface we're specifically debuggingfor instance large language modelfine-tunes that we discover the thingyou just mentioned looking at likethese sort of likeinitialum the the initial reason we sort ofbuilt it out built out Atlas was thisidea thatthe reason that large language modelshallucinate and we saw this we wereworking at this medical AI company rightwhen you deploy when you're deployingmachine learning model in like a medicalsetting you cannot have like a modelthat generates sex hallucinate right youdon't have a problem it's on a medicalAid product the model hallucinates andthe sort of core core reason thathallucinations occurum and we were able to stamp this outactually while working there is becauseof spurious correlations in the trainingdata that is there are things in thetraining data that shouldn't be seentogether at the model at the same timewhile it's trainingum and to be able to Stamp Those out uhyou just need to have some way of veryquickly sort of like scrubbing over allof your data to figuring out to figureout what's in it but then when you findone example of something that's in itthat you need to remove uh you should beable to very quickly find all of thoseexamples and this is sort of where likevector databases come into play right uhthey allow you know once you have oneexample if you're looking through itthrough the lens of like a neuralNetwork's embeddings you can find all ofthe examplesand what you see with Atlas is sort oflike you can think of it as a as avisualizer for the vector database likethe the sort of like my MySQL workbenchfor for the for the relational databasethat you see for a vector you can seethe high dimensional embedding structureprojected down into two Dimensions wellwe're both preserving the the sort ofproperties and patterns that you see inthe high dimensional embeddings but yousee those in two dimensions of that listyeah and I think if we could stay onthis a little further so I've I'vebecame really interested in topicmodeling I interviewed Martin grudendorsabout Bert topic uh so I like jumpingahead to the kind of this topic of likeyeah topic the topic of topic modelinghow are you currently thinking about itsapplications uh yeah yeahum so the way we think about it is asfollows right uh every single piece ofdata uh in the next decade will be runthrough some neural network whether youhave texts being run through aTransformer pre-trained represent textwhere you whether you have largelanguage models generating piles ofcontent to the internet that you need tosomehow make sure you know isn't goingto ruin the world and ruin the electionsor whether it's images audio runningthrough all the latest various neuralnetworks that are coming out and whatyou really want to be able to do is makesense of all this embedded data thatexists out there so Atlas under the hoodit's sort of built out as sort of thisfile system where you can just dump inbeddings and then get super powers overtop of them uh so one of thosesuperpowers is visualization that wewere just talking about that allows youto find patterns in your embeddingsum another one of these is topicmodelingso what this allows you to do is findlike a homogeneous regions of yourembedding space so like once you find acluster in your embedding space thatexistsum what we do is we ensure that thatcluster can be representable and sort ofaccessed over like HTTP API calls forexample or just like access to house soin beddings that are all similartogether sort of get represented as sortof one topic with yours and yeah I think without so like Iguess let me let me put it in thecontext just like a builder right whenwe were building out Atlasum the very first thing we did is weimplemented the sort of dimensionalityreduction algorithm that scaled to tensof millions of embeddings which doesn'texist you take umap or orum or tease me out of the box you're notgoing to be able to scale tens ofmillions of the betting so we had tosort of implement our ownum and once that was there it wasawesome look you could see all of thesedots on a screen and we would show it topeople and they'd be like wow this is socool what does this mean this makes nosenseum and topic models were sort of thething that allowed us to start educatingpeople who are like maybe not like mlexperts as to what they were looking atbecause you could very very easily getout get out sort of the trends that youwere seeing you would see like thosecluster protection it actually ishomologous it's all the same color forexample on the sort of 2D map view thatyou see in Atlasum and then sort of like the nextupgrade was like topic labels forexample uh you could go in and see uhnot only for instance if you hadembeddings of text you cannot see notonly like the actual topics uh via theclustering that you see on the map butyou could actually see like high levelhuman descriptions of those of labelsum and so this sort of like Segway isalso in the GPT for all I don't want toget get too much in there but you haveto generate those automatically somehowrightyeah yeah and of course we're comingwe're going to talk about gbt for allbut and yeah the ability of gbt for allto you know maybe do like asummarization chain as we've seen likeLang chain summarization chain using theyou know gbt4 models makes perfect senseof the gbt for all could be plugged intothat but yeah summarizing the topic thiskind of automatic category Discovery isso fascinating there's kind of one otheridea I wanted to uh ask you about I I'veheard this question from people like oneof my conferences and stuff is they wantto say you know take customer reviewsdump them into this kind of thing seethe you know clusters of their productreviews and then they want to do thiskind of thing of like the automaticDiscovery from that I also think aboutmaybe if there is a connection with thesymbolic metadata associated with eachcluster so you know you have threeclusters and then say you know say it'slike my tweets and I like you know Icluster my tweets and then I say likethis cluster has like you know this hasthe most Impressions like thecategory of tweet however they'reembedded in the space and so I kind ofvisualize my clusters with symbolic dataas well you think about that kind ofapplication a lot of like yeah I guesslike symbolic data as well forunderstanding what makes these clustersdiffer from each other yeah I mean solike at its core what Atlas is showingyou is not like black magic it's showingyou cystical properties that theunderlying model that embedded the datahas sort of learnedum and sometimes it is the case that forexample the metadata that you've seenhumans associate or like a platform likeTwitter associate with your data isactually already captured in theembeddings so that's why you see forinstance if you go on Atlas and youcolor by a piece of metadata you'll seelike it sort of replicates what thetopics discovered because the top thetopics are sort of like an unsupervisedway of seeing uh properties of thatvetting space but if you have thismetadata attached to your data it alsocan be you know explicitly representedum and yeah so likeat its core uh you should be able tosort of discover these patterns and wetry to we try to really be like umwhat's the word like we don't we don'ttell you how to use the platform it'ssort of your job to use it as sort of adiscovery toolum like when you're going through andactually doing the visual discovery ofof what what your embedding space ofyour data representsum and to like to your question it's alittle bit difficult toumif you have a model that can representyour data uh in such a way that it'scaptured all the various properties thatyou'd want to make you all the variousproperties of your data that you'd wantto use for some business purposeum then yeah awesome uh the unsupervisedsort of like system that we have builtout with uh dimensionality reduction andthe topic labeling yeah that canautomatically sort of give you access tosort of these sort of attributes butusually like you have some sort of likething over top of it that you reallycare about this is why for instance likevector databases like we V8 rightmetadata filtering is so important youcan't just do Vector search and I'm likeyou know you're done right metadatafiltering is everythingum so having access meta is like supersuper important and the embedding spacesometimes represents it sometimes itdoesn't uh but you need a way of seeingit rightI yeah I've also been thinking aboutthat kind of the category Discovery fromtopic modeling to be used in filteredVector search I guess my thing with thatparticular line of thinking is that ifit's a property that's captured in thecluster then the hnsw then the you knowthe graph in the vector index willalready kind of capture thatthis is where I'm a little torn on thatparticular part of it but but I reallythis idea of like debugging embeddingmodels is something I really want to getinto as well like so so like we talkabout um use training language modelswith this data and visualizing thetraining data for language models so isit thinking mostly around visualizinghow the Clusters evolved from the latentspace of the language model or say areyou also using this a lot for like thetraining of the embedding modelsthemselves like maybe just to give alittle more background like I my dogfooding data set with leviate is thispodcast I like to like take thesetranscriptions and put them into thevector space and so I've been thinkinglike I've been training embedding modelsin this particular data and I canimagine like visualizing the Clustersevolving over time like as I add thisnew podcast with Andre and then see howtraining on this data set changes mywhole Space could be really interestingso that kind of debugging models yeahbut here's how you think about that withno money yeah I mean I really think thateveryone should be everyone should belooking through their looking at theirdata through the lens of embeddingsum and embedding like data changes overtime right so embeddings will changeover timeum one of the things that you can thatwe it actually is not concretelyimplemented right now as sort of like anative way but one very natural waywhere embedding change over time is sortof the suitcase of like debugging likemachine learning modeling frames as amodel trains right every time let's sayyou do an evaluation on your like Devset what happens is you generate a newset of embeddings and it's for everysingle test point that you have in yourdata set you get an embedding of thattest point out and the organization ofthose embeddings is basically a proxyfor how the neural network is organizingyour dataum and the embedding space sort of givesyou a human way a sort of a humaninterpretable lens at looking at whatyour model is learning and what yourmodel is not learning we have this likereally nice demo example on mnist uhwhere you can go away to like like apytorch lightning hook with Atlas hookedin to train your model on every singleEpoch you'll sample you'll sample like aview of the embedding space and you canvery much see like the digits form overtimeum that reflects in like the like thelike quantitative metrics like the likesort of like accuracy that you see overthose 10 digits but you can really seelike as the model trains the embeddingspace sort of like molds like Play-Dohand like the eights and the zeros arecloser together than the eights and thesevens because well you know theembeddings for those are are are puttingthem apart there's there's more datapoints where it's where eights and zerosare being sort of like together causinglike errors in your actual quantitativemetrics but you can all you can see thiswhole visually so it's a good debug formodel trains as wellyeah that that's so compelling I theyeah I think I've seen an animation ofthat before of the light in spaceevolving on emness probably probablyfrom Atlas where I saw that that is socool and okay so I really want to comeback to this kind of topic modeling andproperty Discovery topic later on thepodcast but for now I think we have todive into gbt for all you know you'vehit the Grand Slam home run and AI ofcreating the model that everyone'stalking about how's your experience withthis man yeahum so it was really cool uh I'm notgonna lie it was it was it was it wasfun uh like usually when you build an mlmodel uh you read like at least likewhen I was doing like more research workas part of my day-to-day you build an mlmodel you spell multiple months buildingit and what happens is you go in and youwrite this big paper and you likerelease it out and then you hit you hearlike chirps about it and like this islike even before like everyone was likebasically publishing their Research onTwitterum so it's a discussionumyou would not get any feedback about itit was just it was really cool to sortof work with the team to get a model outthat everyone really cared about but IthinkGPT for all is not really about like heythere's this cool model and likeeveryone should use it and try itum because like the model itself it'sworse than for instance like what youwould get with a large language modelAPI if you use like anthropic or open AIbut that's not sort of like the the keyingredient the key ingredient is you canown your actual model the model runslocally and that only does it runlocally you don't need some acceleratedHardware like an Nvidia chip you can runit on your CPU so that's that was sortof the whole premise of GPT for all tosort of do a demonstration that with thecurrent set of ingredients that itemerged in sort of the technology stackso like the work of the gergenov and onllama that's CPP uh the work at metasort of like accidentally releasing outthese llama models that everyone wasn'tsupposed to have access to but somehoweveryone found the turrets forum and like all these ingredients havecome together uh to really demonstratethat large language models aren't theselike big black boxes that exist behindthe apis of cloud servers of a few bigplayers you could run them locally onyour machines that was sort of thethesis of GPT for all do a demonstrationof thisum and then it turns out everyone reallycared about it so we were like hey weshould maybe put spend some more time onthisyo I think you just hit the pinata withso many things I mean like first of allit was pretty fast after the release andthen the public reaction to Chachi BTthat you got this out I mean this musthave been like three months latermaximum so it's like the speed of it isastonishing the as you mentioned runningit on the CPU with the Llama C plusthat's like pretty crazy and then it'slike I do I'm a little curious about soso the the nature of this is you have apre the pre-trained checkpoint fromllama which comes from meta and then youfine-tune this to create gbt for all sothat would be that was in fact theoriginal modelum we sensed sort of like the models soif you go around like GPT for all.io andyou go in and you sort of download thisUI installer that lets you play aroundwith any of these local modelsum the reason we need this UI installeryou can't just like go on GitHub isbecause it's a lot more accessibleum it you basically we solve all thesoftware challenges that you used toexist like for instance when we firstreleased GPT for all what you needed todo is you needed to go in and likedownload this GitHub this git repositoryuh code it down and then you knownon-technical people don't know how todo that you know it's kind of accessibleoh you need to go take this random fourgigabyte file from the internet waitseven minutes for it to download andthen drag directory and then like youyou run some terminal commands and thatwas the setup process now it's a littlebit easier it's all the EQ and it's allthis desktop clientum and that's sort of like the chat inthe chat UI that you see there and itisn't just llama models uh you havemodels that are based on sort of likemore like less less licensed restrictedarchitectures like gptj or MPTum we actually just uh so this is notreleased yet but I guess it'll bereleased by the time this podcast comesout uh Falcon is about to launch verysoon so sort of like the the the moveover the last few months has been wewant to get the least restrictivelylicensed models in the hands of as manypeople as possible uh because that isthe way you ensure that these systemscan be deployed safe uh but the deployedsafelyumso that's what we sort of been buildingwith GPT for all uh and the core Focushas really been on like softwareecosystem stability uh the model themodel is amazing as you can if you cango you can go and hug and face look upthe word ggml and find million likeMillions hundreds of models that existthat are all different versions of likeLlama fine-tuned Or MPT fine-tuned orgbtj fine-tuned and they're all sort ofjust like they're there uh sometimesit's hard to use because the sort ofsoftware ecosystem doesn't uh supportall of them because there's a lot ofbreaking changes being introduced insort of like the core components thatmakes them make them make these modelsrun fast on CPU the ggml or llama CPPand sort of what we try to do at GPT forall is make it easy to use for Everyoneby like eliminating all those likesoftware errors that you might encounterand also create like sort of the moststable ecosystem all the way down fromlike a c plus code that's optimized tointeract with your L2 and L3 caches ofyour CPT CPU architectures all the wayup to like the docker images with HTTPAPI so you can deploy into productionand we try to give it out for free underApache 2 license uh that's sort of likethe the the the the the the the the thethesis everyone should have access tothis technologyyeah I mean um I learned a little bitabout this l2l3 cache like theoptimization for CPU inference with deeplearning from speaking with Michaelgoing at neural magic and learning aboutyou know what neuromagic is doing aroundCPUs is I think it's really hard toundersell like how impactful that can berunning it on the CPU because it makesit so much cheaper to sample all theseinferences and then you couple that withlike the auto gbt strategy of coming upwith these action plans and like youknow or like gbt team is like anotherexample of like you know you have roleplaying uh llms that communicate witheach other and with CPU inference beingso fast it's also fascinating but Iquickly before maybe if we talk moreabout the implications of models likethis I want to get a little more intohow you train this so you know thingslike say the low rank adaptation likebecause because I always you know thesemodels typically have come from likeGoogle open AI micro like billions ofdollars you know big so like how how areyou able to you know yeah let's let's beclear here like what what GPT for allhas built stands on the shoulder likethe very broad gigantic shoulders rightlike we put a bunch of ingredientstogether and spend a bunch of timecurating data like we use Atlas tocurate the data that was that was sortof like the edge that we had we can veryquickly take a large a large collectionof data make it clean so we can trainthe model into that model actually belike useful to interact with they canfeel like chat GPT even though it wasn'tthe type of quality of chatum and on most tasks uh like sort of thetasks that required like a reasoningcreative tasking is pretty goodum so like the ingredients like what arethe ingredients to make a GPT for allthat's I guess the question you'reasking uh the ingredients are as followsuh number oneum you need a veryyou need a very strong pre-trained modelbackbone that is a model of a largeTransformer model that has beenpre-trained on a whole internet worth ofdata trillions of trillions of wordstokensum you need a model like that uh that'svery expensive it costs millions ofdollars to train a lot like thatthankfully Meadow did this thing wherethey opened where they released one forresearchers to use in an open sourcefashion so the research Community canbuild around itum so that happened and now but everyoneelse is doing this right so the UAEreleased the Falcon model recentlyMosaic released MPT but the ingredientis a very very strong backbone becausewithout a model that knows aboutlanguage you can't fine tune a modelthat can be a chat assistance uh thesecond ingredient that you need is a lotof high quality data of humansinteracting with chat models now what wedid uh in March is we found this reallynice high quality model that you mighthave heard of it's called chat GPTum and we took a bunch of examplesby giving it questions that humans mightask uh toum sample what its responses would be sowe basically went on hugging face got abunch of data sets of people likeprompting uh models to do difficulttasks like write poems or like answer aquestion thoughtfully or use like Chainof Thought reasoning to produce outproduce out of half put and we samplethose responses I took that bigpre-trained backbone so long up and wetrained it over top of that data to beable to act like a China system so achat assistant has to know you know noone to stop answering uh have some ideaof like who it is what it can and can'tdoum and the way we were able to actuallymanipulate that data such that you canput it put it into training is usingthat tool Atlas we were just talkingabout uh there's many there's many waysyou can do it but the reason Atlas wasimportant is because if you sh if youtrain with data that for instance has alot of duplicates in it or the data isnot diverse what happens is the modelsort of the fine-tuned model that youhave sort of falls into modes uh whereit can't actually go in and you use willchat assistant sort of across the boardso Atlas was really sort of important inensuring the diversity of the data andensuring the data didn't have any likeduplicate examples because the secondyear of duplicate examples the modelSource memorizing with the duplicatesareum as opposed to learning how togeneralize over top of like all possiblechat interactionsso that's kind of ingredient number twoum ingredient number three is make themodels run fast on every single machinethat isn't equipped with the Nvidia GPUuh that's sort of like all the work thatcame out of the gergonov's ggml librarywhich allows you to run uh ml modelsquantize really quickly on CPU so youhave a custom matrix multiplicationkernels that allow you to executequantized model inference very quicklyum llama CDP which sort of gave us forthe interface layer over top of thatHTML Libraryum and sort of all those ingredientswere put together uh we curated datanicely spent eight eight hours on a uhdgx a100 to train the model and then outof out of the DTX GPU oven pops out theCPT for all of themthose are the ingredients and recipe ifyou're uh if you're taking notes yeahyeah I got my notes so let me rapid fireyou quick questions on the three thingsum so firstly the the large pre-trainedTransformer is the base you mentionedllama we see um you know Falcon MPT Tgbtj there's also I think you knowstability AI I think they have models sothere's like uh like Valkyrie withoutlike just saying how many of userswhich one which one do you pick is amatter of yeahyeah so are is islamas are most of theseopen source language models we'rehearing about are are they pre-trainedbase models or are they mostlyfine-tuned checkpoints from llamasshould people be the only pre-trainedbase models are llama Falcon MPT gptjand then probably all the ones that theall the closed Source AI companies haveunder the hood and are sharing forinstance like character AI open AIanthropic they all have their own onesbut those are those are the core becauseagain it's super expensive cost millionsof dollars to train one of these modelsand a lot of data curationum teams of people to get one of thesemobiles outum and uh those and every single othermodel you hear is a fine-tuned versionof one of these models so somebody tooka data set and they train it for alittle bit of time it's been a couplehundred bucks of gpus to get one ofthese models out so that's kind of whatyou're seeing fascinating so then mysecond question about the collecting thedata the first question I have is aboutlike can the language models talk toeach other and then you bootstrap thedata that way is that promising or doyou think you need the human in the loopfor it yeah so I mean look you canum the the issue is like the overalllike a lot of research papers sort ofinvestigators like what limited whatlimitations can what are the limitationsof taking an existing model and thetechnical term for this is distillationyou can also tell you also call ittraining on another models anothermodel's outputsum what are the technical limitations oflike how far can you get right can youget a GPT form quality model by takinggpt4 responses and trading against it umthat's like an open questionum and some people there's peoplethere's there's papers that have beenreleased in the last month that say noof course you can't and then there'speople don't say yes of course you canum and then there's models out therethat sort of conflate with like theresults of the first paper so thequestion is can you do thisum you can uh it gives you better modelsout that than what you'd get if you forinstance spent a bunch of time curatingthat data yourself or making generatingthat data yourselfum but I guess my hypothesis on on onyour exact question can you make modelstalk to each other and generate dataprobably not right because the modelsit's the models will not get the datayou collect from that will not be anybetter than the model uh than theinitial models that are talking to eachotheryeah that's that's a that's a good pointwithin the model yeah because you'rejust sampling it and I mean I guess yeahI'd love to know what this I haven'tcaught up I've seen there's a viralpaper title that's something that'ssaying that the knowledge installationmodels are just learning to imitate theydon't have the robustness kind ofproperties of the project so I thinkknowledge distillation is one of thosealgorithms that seems like like on theservice level it seems like a gold minefor deep learning because you'recompressing it it is right so like onthe technical level what's happening isthat these companies are spending Massamounts of resources to number onepre-train these models like sort of likecompanies like open the anthropic we'respending massive amounts of resources topre-train the models uh for millions ofdollars then they spend a couple moremillion dollars about tens of millionsof dollars to gather human feedback ofpeople interacting with their models andthen be able to use this process calledfor instance rlhf reinforcement learningwith humans back to tune the models bemore aligned with like what they whattheir definition of a good model is andthat costs a lot of money because youneed a bunch of humans to actually belooking at the model responses andsaying yes no maybe soum and then once you have that at theend of the day you still have the sameneural network it's a point in highdimensional parameter space like likeyou know our our our our 40 billion or175 billion point there and when yousample outputs from that point you mightnot have to go through the whole processof rlh being a model you might not haveto go through the whole entire processof for instance pre-training a modelthat's the exact same pre-trainedcheckpoint that open AI or in topicstarted with because what you're doingis you're distilling the sort of the waythat model generates out without havingto just for instance have an rlhf modeland a lot of Manila and that's sort oflike that's sort of like the promise ofdistillation is that you don't have youdon't have to go through all theseintermediate steps to get that thecompany created that original modelum you don't have to go through allthose all those same steps to relate tobe able to replicate that same sort ofpoint in parameter space which is thegpt4 model that everyone is using uhbecause you can just sample its outputand distillation has been a thing for awhile like I mean the I think the OGdistillation paper was like a hinted in2015.um like hugging face like hugging facesclaim to fame the way they got sopopular was like they did they have thismodel called distilbert which isbasically a compressed version of Bertwhich used uh like teacher studentdistillationum like 2018 2019 that's like what whatmade hugging things pop offum so it's a technique that works wellum the question is like will it give youwill it will it achieve you the goal ofgetting AGI probably not because likeyou're only going to be as good as theteacher model that you ever distill fromyou're not gonna you're not gonna learnanything new it's my opinionit but it's really interesting becauseit's like will it will that get you asmarter model probably not but it's likethis idea like I think the gorilla paperis the latest one hugging gbt where thesmart model orchestrates inferences tothe task specific models and then it'scheaper to run inference with the likeyou know I had the same the distilledBert had a huge impact to my like careerin this as well where it's like it wasjust small enough to fit on the gpus andlet you train it you know I think it waslike 330 million parameters for Bertcompared to like 50 million fordistilled birds so that difference inwhat it requires to fine-tune thesemodels run inferences like massive and Iyeah like I love this angle that you'resaying with Atlas with looking at thedata deduplication something thatinspired me a lot as you were talking aswe're thinking about distillation ismaybe to see the evolution of twocluster spaces with two models to likethe mnist example we saw where you'reseeing this evolution of the space maybeseeing how different models evolve haveyou played around with that kind of ideayeah so the the one place we've usedAtlas are internally at nomic in theloop as we uh so we have the softwareconsistent for GPT for all we also pushour own models out which we spend a lotof time curating sort of getting thebest models that are good with the breadand butter tasks llms need so we do alot of internal work of just liketraining the models making sure they'regood and we use Atlas uh heavily tomaking sure number one like the datathat's going into them is good but alsowhile the models are training figuringout what's going wrong with the finetuning process for exampleum the gpt-j model which is sort of likethe first Apache 2 license model that weput out thereum the issue we were hitting with thatmodel is so it's a much smaller modelthan what you get with the Kalama 13billion or a llama 7 billion it'ssmaller in the parameter count it's alsosmaller than the amount of like tokensthat it's seen during pre-training uhit's harder to trainum and what we were hitting was thisfact that while we were fine tuning onthe exact same data set we werepredicting llama on uh the model wasjust over fit like at a given point itwould just complete like the the Eva thetrading loss would just like likeplummet down uh Eva loss with Spike andwhat was happening happening there andwe're so confused like why is thishappening it works perfectly on thislike other model what we were able to dois actually take every singleum point that we were evaluating themodel rather every single point that wewere training the model on and take acheckpoint of the model right after itstarted overfitting uh during trainingand visualized sort of like the loss sowe had on Atlas mapped out the scalarloss so all the examples where it hadhigh loss all the examples that it hadlow loss and there was this region ofthe embedding space which was just likeall red which was like all correspondedto the region with super super low lossright before the model startedoverfitting and this region was on onCreative examples so we had all theseexamples of the model doing the tasticsummarization like finding namedentities in text but there was this oneregion where we were asking a model likewrite creative poems and the things withthe things we created the thing withcreative tasks is what happens is youhave the exact same question you mighthave many many different responses tothat question but they're all validresponses and what was happening is themodel started overfitting to that regionof the space causing an entire collapsein the model's ability to do anythingelse on other types of examples and theway we immediately realized that thiswas the issue is just so we looked atthe atlas map and it's like hey look atthis big bright red spot what's in thereholy crap like a creative examples thatgave us that hypothesis we down sampledthem and then the next model train Boomthe model was great gptj uh GPT for allJ was deployedwow that's such a compelling Story Imean I've I've seen things like uhworking with class imbalance where youknow you have one labels 90 the other is10 and so it's always going to predictthe 90 label so you do like random oversampling where you balance it out bysampling and I remember like back in thedays of training Gans you have the modecollapse where it's like just generatingthis deer that's like the middle of someregion of deers yeah all that's sointeresting so kind of coming out alittle bit of the technical details oftraining this I really want to ask aboutthe GUI that you've built around gbt forall how you're thinking about thingslike the chat gbt Marketplace and likehow those kind of apps into the UI fitsure yeah I mean I think long term theviability of selling outputs of largelanguage models goes to zero uh I meanyou've just seen it in the past two orthree months there are amazing modelscoming out that anyone can run their ownHardwareum so what we've really tried to do withthe GPT for all is sort of focus ongetting a very stable software ecosystemthat adheres to all the users of llmsand users of LMS are not just developersthey're everyday humans who want to forinstance like put in their local data orlike their local PDFs just ask themquestions about them rightum so the GUI that you see if you go toGT for all.io you can download a Macinstaller Windows installers Linuxinstaller uh what that is is it's a chatapplication it's written actually thisfor it's c plus private framework calledcute uh it also cute also runs like sortof the monitors of like mercedes-benzesor like Adobe Photoshop is built inqueue cool and what it's doing under thehood is it's talking with our c pluslibraries that all have that have forinstance llama C plus plus big din theall the versions of ggml baked inum all the various sort of ways you needto Define them the the weights of themachine learning models for thedifferent Transformer architectures gptjFalcon MPT all that sort of like underthe hood and all you see is one of thischat client that you get that you'd gofor instance when you see like you'regonna go to chat.openaium.comum and the reason you need to sort ofthis client is that like for local llmsthere are sort of a few caveats that youneed to do a lot of software engineeringwork around to make them as usable asyou would see for instance a model whereyou're just like interacting it overlike websockets or HTTP calls uh likeyou'd have like anthropic or open Ai andthat is like for instance when themodels all run on CPU right uh thesemodels are large they take up like fourto eight gigabytes of RAM on yourmachine and what happens is what youneed to do is you need to be able toCache the model's internal States if youwant a very clearly say resume a chatonce once you've stopped it there's likeall these sort of like nitpicky thingsthat are actually pretty hard toimplement if you don't have uh you knowa bunch of a bunch of elite elitehackers right writing the code to manageit so and that's sort of what we've doneum everything from again the C plus pluscode if you go to the GPT program it'sall apache2 you can go in rip it use ituse it for your commercial purposes uhdon't use lava use the Apache tubemodelsum you use it for your commercialpurposes like you're happy like we'rehappy to let we want to maintain themost stable software ecosystem aroundthese that's the attentionyeah I think you're quite anentrepreneur I think you have to doincredibly compelling businesses herewith with no make and then also thislocal chat gbt Marketplace like thisfirst thing of um porting tools to worklocally that's so fascinating I've seenlike Microsoft semantic kernel kind ofto me looks like maybe like aEBT Marketplace Lang chain that'sdesigned for a specific set of toolslike the Microsoft tools can you tell memore about so you know not making theHTTP requests keeping it on keeping theintegration of tools privately I meansure right likehave you have you called Nvidia in thelast couple months they're not going toanswer your phone they're doing great uhthe reason I can answer your phone isbecause if you go to an order let's saylike an h100 be like hey Nvidia get mean h100 uh they're gonna talk to you andthey're and if you're not on their likelist of like top 10 customers you'llprobably not get a response and then ifyou do what happens is tell you amazinghere's your h100 uh send us the moneywe'll send you the card in about a year48 weeks is the time to get a GPU rightnowum from Nvidia for like these like likethese server grade gpus that can runthese like large language modelsumeveryone has been ignoring this massivemassive ecosystem that we built of justlike machines that have CPUs on them andthere's a couple of difficultengineering challenges to make thesemodels work on these machines uh but ifyou can if you can leverage that everysingle like every single let's say CPUenabled machine that you have at AWSthat you have on Azure that you havelaying in your garage on the groundthose can run large language models onthem you don't have to sideload all youdon't have to run all your the wholereason you have to run the machinelearning models in like behind apis isbecause you have to run them on gpus noteveryone has access to themum so yeah that's like that's that's thethesis like models can run on CPU uh wecan demonstrate it like here I'll goplay with them in the chat client andlike hey go build go go build yoursoftware over top of it as well you canyou can you can own your L alumsumthere's another big there's another bigthing happening in this area right now Ijust wanna I just want to call out uhAppleum I know everyone's like freaking outabout their headsets right now they'refreaking out about the fact that uhthey've put in basically like gpus ontoevery single device in the world andjust like aren't using them right nowum Apple silicon that is plugged intoyour like M1 or M2 Macum I think the m2s have the sort of thebest gpus right nowum you'll be seeing in about a week'stime you can run a local llm at like 40tokens per second uh which is like thespeed that you get out of like a chatGPT model when you go to the web UI allon your machine just by having them justby having a Macum and like iPhones have these chips allthese other all these other devices havethese chips in themum you will be able to run giganticuseful llms all in your devices in likethe next few months it's it's realyeah the the Apple thing is certainlylike game changing something that'semerging uh Colin Harman on a lostpodcast is not Publishers were speakingbut I had mentioned this idea of thelanguage models come on the chips andApple devices and I thought that wasreally compelling I read that paper onuh bite uh it's like bytes Transformerwhere they're taking bytes directly asinput to unifying modalities a lot ofinteresting ideas coming out of Applefor sure maybe quickly I wanted tocomment on this idea of you know likerunning models on CPUs and I also kindof wanted to bring back to the Wii V8topic and talk about running big Vectorindexes on CPUs at Wii V8 and soSomething That We're researching that Ithink is incredibly compelling is uhdeveloping the disk a n Vector index sothe disk a n Vector index is aboutoptimizations to move some of thevectors to disk compared to having tokeep the whole graph in Ram and so thisway you could just you know on yourlaptop have like an 100 million Vectorindex and I'm sure you have some ideasabout like how you do like 10 millionscale visualizations in the browser andthis kind of thing of like not only arewe making it easier to uh you know runinference cheaply we're also making itso you can visualize embeddings of justand visualizing and also the databaseside do the nearest neighbor searches atabsolutely enormous scale what do youthink will be kind of the implicationsof that ideaI mean compute must be moved on the edgelike there's just not enough serverHardware like it's it's a fact the morethings you can do on on edge devices themore sort of offloading you can do theclever you can be about how like we'rein the very early days of ml a lot ofthe stuff people are using just writtenby researchers who cared more aboutmaking the systems work as opposed tooptimizing the systems for like actualutility uh and like let's say like inProductions like actual utility toEveryday humansum I think there's like amazing workhappening on that end like we're we'retrying to keep up as fast as possiblelike we you know we can we we can shiptens of millions of things to your webbrowser and run it on your gigabyte Macwhat you guys are doing in the disk andthen side like that's amazing likeplease please please show me and I willI will I will hack with it because likethat that that's just that's great thebetter you can utilize resources on thehardware that already existsum like the better the better it is foreveryone involvedyeah and one of the topics that likewhen people ask me like what is excitingyou the most about AI right now is thismarriage of say the gbt for all cheapinference and then the you know Vectorembeddings at scale with disc and Etcand and Atlas for visualizing it is likewe can have the language models liketalk to each other we call thisgenerative feedback loops and and likewe've written a Blog about this conceptof like the language models they produceall this data and now you need to liketame the data so it's like it's like youneed to produce the latent space butlike you know you it's like let's sayit's an image generation model yousample it's like a billion images fromit and now you need to like look throughit and say like okay what is it createdkind of right yeah yeah I mean I thinkit's going to be a gigantic issue rightnow like if you thought the 2016 USelection was was scary because socialmedia was in full swingum you're gonna be you're gonna befrightened by what happens in thegenerative AI election that comes up inthe next few yearsum every single like it is the themarginal cost of generating convincingcontent that's Ultra personalized toanyone in the world is zero right now itis or it is already zero you are whenyou go on Twitter you're probablyinteracting with LMS you don't know itum and they're personalized to you theyknow about you they've been promptedwith like your you know your bio andthis is a gigantic issue when the wholeinternet's gonna be flooded like it's agigantic issue for ML model trainingbecause like how do you train oninternet that's flooded with ML Tech mlgenerated text it's a giant issue forlike everyday people who are like youknow don't even know these systems existlike Chad GPT has only been used by likeyou know five percent of the USpopulation or something like this maybethat's a wrong metric but I saw thisfrom Recently like most people haven'tbeen exposed to this technology yet butthey're being exposed to the outcomesthe internet's being blotted with theoutcomes the outputs of thisactually one of the Theses about what wewere sitting building Anonymous is likewe if the whole internet's gonna beflooded with gender with generative AIoutputs be that images or text you needsome way of digging you through it uhyou need some way to have observabilityover top of it and atlas map is thatobservable interface I believeyeah I think so as well I I mean havingthe visual component to pair with whatwe're building or just hosting it it's apeanut butter and jelly type of pairingwith and with the gb2 for all thisTrends in cheap inference yeah well Imean kind of staying on thatpersonalized generation thing and sortof like the next generation of spamwhere it's like hey Connor I read yourblog post about this I have thesethoughts and it's like not even a personon the other end it's like I rememberlike one of the big AI things was likeIan Goodfellow is like he's gonna workat Apple and he did this interview withLex Friedman where he talked about howimportant it's going to be to have likecryptography uh cryptographic signaturesthat come out of the iPhone pictures tosay like this is a real picture do youthink a lot about that kind of thing andlike sort of how that will change theworld yeahum so what one thing we did with theoriginal GPT for all modelum I've never actually told anyone thisI guess you'll be the first to knowum is we watermarked it we we put inlike we put in like a um in the trainingdata like this phrase uh which the modeltrained onum and it's actually in the maps if youcan find it if you search for itbasically this ID and then we could knowanytime a system is using GPT for all byputting that like string and it'llgenerate out something that we know isthe It could only been generated out ifthey if they were using the modelum so like like we have thought aboutthis a little bitum and we're not doing anything with itturned out to be like a a sort of auseless thing to do uh because itactually didn't work quite well uh butit was the thought that we hadum but yeah this idea this idea of justlike how do you control for like how doyou determine if something is made by ahuman or not made by a humanum in a manner that doesn't require likegoing through like some centralizedentity to mediate to that uh I thinkit's a giant unsolved unsolved questionright now I mean I think likeI I believe this is like this is likethe thesis of like World coin right orlike what Sam ultimate is trying to dois like let's get everyone's eitheririses and then you know when they'rewhen you're when you tweet something youcan call their API and and verifywhether or not you're actually tweetingas a human or you know you're take aforward-facing feature of your camera Ithink that's where that whole thing'sgoingum but it's it's an insane problem I'msure every dictator in the world rightnow is ultra excited because it'sdecreased like their their like controlform costs by eight million percent tolike one developeryeah I mean I think it's great in thetopic I wanted to kind of anchor thispodcast with is your original PhDinterest and interpretability andmachine learning I think it's related tothis topic of like you know I think thistopic is like can we detect contentgenerated by these models which has alot of nuance to it and then generallymaybe interpretabilityum what initially Drew your interest tothat research category yeah so I guessyou you put a statement in there I wantto be very clear in my opinion of it wecannot like you cannot systematicallydetect generated content uh withouthaving some extra metadata attached toit that by the provider where thecontent was generated inum it it isn't it is it is anintractable problem to be able to sayhere's a piece of text did was thisgenerated by like a machine learningmodel not imaginary about a machinelearning modelum that's a factum the sort of like metal levelquestions like how why did I get intolike I I was just really interested inmy whole life I'm just like I wanted tounderstand how systems work and ml waslike the coolest thing to work on and Iguess very naturally like I I I I joinedlike the Deep learning world but veryvery lateum this was like 2018. this is likeafter like all the fun stuff had beendone and everyone was realizing thatlike data was the actual juice thatpowers these models and like after theTransformer model came out you see everysingle ml system that's being builtright now is being called theTransformer the model is staying thesame the training the trainingobjectives are staying the same theregularization methods are savings it'sall just different cocktails dataum and the realization is like you needbetter interpretability methods to beable to actually like these models willbe the different the differentiatingfactor between all ml models beingdeployed will not be usuallyarchitectures it'll be the actual datathat they're trained on you need betterinterpretability methods to understandlike the data going in the data comingoutum and that's sort of kind of what'swhat spiked my interest uh sort of sortof originally I I had realized like Ispent a bunch of time in my undergradworking on uh NLP for clinical domaindata it's like working with like medicalnotes and that was just all like a dataproblem it's just like the the methodswere fixed you just copied the methodsthat were the in general NLP ComputingCommunity was working on you had to getit working on the data that you had so Ijust had a lot of experience sort ofdigging with data that was non-standardI guess and that's allyeah that's really andI'd say with the first thing I sorry Isay with the first thing umI think with like uh the Style again Iremember studying like the watermarkartifacts that maybe with imagesgenerated there's something to thepatterns that can help you detect it andbut with text I agree it sounds likethere's no hope with that well even withimages right so if the malicious enoughactor is like I want to generate thingsthat are undetectable and then you goout and you make your detector amazingyou make your detector now they can useit or your detector to improve theirgenerator and then like it's it's a raceat the bottomlike it's like like there's like thestory of that college professor who likelike got like failed his studentsbecause he put in data into chat GPT andwas like hey is this real or not heactually didn't even use a detector heactually asked a model the itself whichwas just a different problem with thesemodels and people using them it's a UIproblemum but it's it's impossible to saywhether a string was generated or Not bya by a model uh like going forwardum we have to we have to think of othermethods of ensuring that these systemsare being sort of like safely utilizedin the world outside of saying like yesor no to whether or not this was machinegenerated it's not economy validsolutionyeah I think that's fair for sure Ithink maybeI I was thinking maybe there's somethingwith like adversarial no you know likehow you see like the it's likeharnessing adversary example is theclassic example it's like a panda andyou add the adversarial map to it andnow it's like given to show that likethese patterns are like so fine-grainedkind of like if you're talking aboutlike a 512 by 512 you know 255 in eachpixel RGB it's like maybe maybe there issome pattern that's detectable but yeahI agree I mean like yeah if you take amodel off the shelf that someresearchers have produced like yeahyou'll be able to you know detectwhether or not a sample is in thedistribution of that model or not uh ifyou have a company that really caresabout the quality of their models uhlike I I like if to be a little bitforward-looking here I think what willhappen is just likelikely the model the the outputs ofmachine learning models that most peopleinteract with will be coming from a fewlarge companies that have these modelsthat are managed and they're producingthese models and they're going todevelop their internal methods fortracking whether or not somebody youknow is using the other model's outputsin some way that you know they don'tlike or doesn't divide it but they'relike ethical or more to moral standardsthe bigger problem is not that thebigger problem is likewhen access to these models becomesdemocratizeddo you have to build tooling that canfight that being happened by monociousplayers and that's the kind of the a lotof people have asked me like don't youthink it's unsafe to be giving thesellms to everyone like shouldn't theystay behind like closed doors becausethat's how people can control them uh mysort of reputation of that is like ifyou keep it behind closed doors whathappens is great you've centralized theability of of of any actor in the futureto be able to say whether or notsomebody can use the models be somebodythey can they can Envy they can go inand say you know has this model has amodel output iterated by us or has notbeen generated by us from change thatpeople interact with with data based onum I think by giving the models out toeveryone it allows everyone to actuallysee like what is the actual currentstate of the art and what models can doand sort of build the appropriatecountermeasures for us um that's sort oflike my refutation there again that'senough for a lossyeah that's certainly one of the bigtopics emerging I mean I think that's agreat argument in favor of the openmodels and I think definitely you're theone of the big voices for open withmodels with gbt for all and being behindthat and all uh yeah I don't have toomuch of an opinion on it myselfum other than just seeing theconversation forming around it like SamAltman obviously going to U.S Congressmade a lot of noise about you knowregulate andand yeah I also I remember yeah there'salso like the topic of like the toxiclanguage models you train it on like youknow that kind of data and produce thatkind of stuff but yeah it's reallyinteresting uh really interesting topicand I also something that you said aboutthe the data Centric focus on AI comingout of that like the data Centric FocusI also agree heavily that it seems likedata Centric is the way going forwardlike you know it's not like neuralarchitecture tuning like I don't thinkthe variants in the Transformers likelike well actually well because I dothink like flash attention this thesekinds of things are how they're likescaling the input length so it'scertainly impactful but I think that thedata Centric is definitely the numberone thing to focus onyeah um again that'sthat's that's what I spent on money sortof thinking about and focusing on it'show do we make sure that when we havehundreds of millions of documents oftext or images or embeddings beingproduced out how do we make manipulationof that easyum there's a lot of ways to do it rightAtlas isn't the only way to manipulatelarge quantities of text I hope it'sgoing to be the easiest way that allowspeople who are increasinglynon-technical to be able to do it likethat's why we're building itum that's that's the premiseyeah awesome Andre well I think nomikand we V8 these two technologies pairtogether so nicely and you have so manyinteresting ideas you've built so manyinteresting things so thank you so muchfor your time on the podcast before wewrap up quickly do you have anyannouncements about upcoming projects orhow people can you know hopefullyMesmerize by all your knowledge can keepup with your future workyeah um I guess follow uh nomicunderscore AI on Twitterum we're constantly putting out amazingcontent about sort of democratizingaccess to the data and models thatsurround sort of the current AI systemsum I guess the biggest thing to look outfor uh very soonum is you can run big llms on yourcomputer uh and if you have a you havean M2 Mac you'll be able to run it atthe same speeds of like a chat GPT modelthat you that you that you see andthat's going to be released in the nextsort of week or two uh so go try outlocal LMS uh because you don't have tosend all your data to third-partyservices just to access this technologyamazing Andre thank you so muchthank you Connor", "type": "Video", "name": "Andriy Mulyar on Nomic AI, Atlas, and GPT4All - Weaviate Podcast #58!", "path": "", "link": "https://www.youtube.com/watch?v=qb2nLeRpMWQ", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}