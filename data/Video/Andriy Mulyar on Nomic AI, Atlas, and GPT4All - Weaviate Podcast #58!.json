{"text": "Hey everyone! Thank you so much for watching the 58th episode of the Weaviate Podcast! I am SUPER excited to welcome ... \nthank you so much for watching the wevia podcast I'm super excited to welcome Andre moliar from gnomec Ai and quickly before diving in this is Christmas come early for people working with weeviate and Vector embeddings no mix Atlas visualization of embeddings is just amazing it's so cool to see these clusters and we have an integration with weeviate to show but even before talking about that huge congratulations nomic AI on raising a 17 million dollar series a I can see in the uh the press release announcement Atlas gbt for all all these amazing Technologies so firstly before even getting into any of the technical stuff huge congratulations to everyone at nomeg AI this is just amazing to see and I'm so excited about how this space is developing so let's get into that a little more about what this means of space developing so uh maybe you've seen this kind of thing before this is topic modeling we've had past podcasts with say Martin Gruden doors from Bert topic you know trying to understand how this kind of stuff integrates with wiiate but so what you have is you know you take all your documents and you embed them say with open AI embedding things that are 15 36 dimensions and then you project that into a lower dimensional space with say tsne umap and so on and then you visualize the lower dimensional space then you cluster and so then you could like cluster the two-dimensional points or you know you come up with these topic labels for these different kind of labels so you see like um if you zoom in these are like the point Cloud segmentation this is these are embeddings for all the papers published in nuribs from 1987 to 2022 so you know these kind of maps are so interesting and so here's this tutorial on how to get your data from weviate to nomic Atlas for this visualization you know flowing the vectors from Eva to nomec all the super exciting stuff personally I've added this to my wevia podcast search demo to play around with it this is like my dog food I use to see what all these things look like and so so what you see is you know you can take your data out of levia put it into this space and um uh hopefully that's a good visualization uh oh yeah sorry so let me just increase the point size somebody's a little clearer but you can see how each of these podcast Clips you can see the topics you click clustering and topic modeling podcast Clips we were talking about clustering and topic modeling but anyway so this was so much fun to play with with my own data and I hope you know you enjoy seeing your data like this I think it's we'll get into it on the podcast but I think this is one of the most exciting ways to understand the quality of your embedding spaces you know it's one way to compare embedding models is to see the latent spaces produced by them and you know whether these topics make sense whether you agree with the Clusters and so on so without going into it too much further I'm so excited to present this podcast with Andre this was such an exciting one hey everyone thank you so much for watching the weevier podcast I'm super excited to welcome Andre moliar Andre is the founder of nomec AI nomik is doing so many exciting things for one example they have done this uh embedding visualization directly in the browser at 10 million scale and Beyond it's so exciting to see Andre is also the creator of GPT for all which is another like topic that I'm sure everyone has heard about so Andre I'm so excited to talk to you and learn how you think about all these topics hey Connor thanks for having me awesome so could we kick this off by diving into the founding story of nomec uh like what led you down this space sure yeah so like I think every good ml story it starts at a nurbs machine learning conference um this was like uh 2019 um and uh this is when everyone in ml cared about images and nobody knew that that machine learning models were just these amazing manipulators of text even though Transformers have been out for about two years and I actually ended up going to this uh it was it was a bird Meetup so Bert was uh so before everyone was naming um naming models after like llamas and alpacas it was Sesame Street characters and there was this Meetup for uh for Bert which was like this model that released released in 20 2018 which was a Transformer model trained to sort of uh self-supervise over text and then you can do all these magical things with text that you couldn't do before and at that Meetup I actually met my co-founder Brandon um so Brandon CEO nomik he uh I was actually working at the time at this startup called red AI which was nowadays you call them generative AI startups back then you call them machine learning startups that uh things with text um and what they were doing is basically summarizing Radiology reports automatically with large neural networks and you can imagine because they're using at the time because fours came out this is the one that always seems to became feasible um and so we sort of met up there uh Brandon and I ended up working there together for about a year and we found out all these sort of difficulties that existed in training large-scale Transformer models on large quantities of text this is like way back before gbt3 even came out um and so like we took those learnings and basically the tldr the learnings is that you need to look at your data uh every every everything in the process about big neural network on a lot of texts will eventually become replicable um everything from the data ingestion all the way to the model training the model serving uh Mosaic ml for example is doing great things I'm like training replicable um but the hardest part is always making sure you have your data curated in the right way so that was sort of like this like clear non-repocable problem that we saw uh we sort of went to different ways after the company um and right now let's go work at Square uh I went to start a PhD at NYU working on Interpol machine learning and about six months in uh basically what happened is Brandon reached out to me he's like hey we both know this problem exists let's let's let's let's look at solving it and sort of like the the interface that we imagined for how we would have wanted at the time to go about curating large contents of unstructured text and like a replica mirror to make it faster was sort of this interface that you see in our product Atlas right now right now um this this this this idea that when you're going through and exploring large collections of data you really really want the property where when you find one example of something bad or something good you find all of the examples of eating this is actually a property that you only get when you're looking for data through the context of Click through like the lens of a neural network through the embedding space of a neural network that's what Atlas was at its core Atlas was this sort of solution to our problem that we had where we have you know 60 million documents of text you to clean them very very quickly and figure out what's in there uh and that Atlas was the sort of like answer to that uh like overarching problem that became developing and building but that's sort of like the the the Genesis of note um and we sort of sort of started out working on that uh we had sort of the MVP uh in beginning of Spring 2022 um we went out uh built out sort of like what you see right now in Atlas when you when you go to atlas.nomic.ai uh where you can go in just drop your data sets and you get these sort of observable sort of data maps that allow you to see tens of millions of pieces of text images audio anything you can embed with a neural network all on one screen and then be able to slice a dice and manipulate it just like you would manipulate for instance something in a pandas data frame but you don't need to be technical um so that's what you see right now yeah that that's a really compelling Story I mean I there's so many to so many stuff to that I think yeah I also first became aware like coming back to the neurops conference I think I I first saw Atlas um with the visualizations of the iclr papers of clustering them and doing that kind of thing and I thought that was so cool that's how I first saw this whole topic modeling cluster analysis idea but you've hit this angle that I think is so fascinating of the data visualization for curating training data for large language models especially when you're talking about pre-training them and doing the internet scale uh language modeling I think for me I first saw this data set called the pile with which is like 800 gigabytes and you had like this you know a nice like diagram with the squares of like how much is Wikipedia how much is GitHub and that was kind of like my first thing of seeing like the composition of the data sources you mentioned Mosaic ml they're doing a lot to educate here's where our data comes from the mix of it for say MPT and you know I know uh with Google research quarkly is someone I've been following for a long time and they have a paper called I think it's called something like that where it's about the composition again like this investigation um is if we could stay on that a little more like so you know I love what you said also about finding problems in your data set by looking at the nearest neighbors so so you take you you know you take 10 million or you know massive scale used to train a language model and you embed it all into Atlas and then you start looking at like you know here is where the high loss region is like you know your model performed badly on this point so the neighboring points tend to correlate is that how you're thinking about kind of how you then use this interface to improve the training data yeah so that's one way you can use it um that's actually not the way that we had an envisioned in mind it's just like a useful way of using the interface we're specifically debugging for instance large language model fine-tunes that we discover the thing you just mentioned looking at like these sort of like initial um the the initial reason we sort of built it out built out Atlas was this idea that the reason that large language models hallucinate and we saw this we were working at this medical AI company right when you deploy when you're deploying machine learning model in like a medical setting you cannot have like a model that generates sex hallucinate right you don't have a problem it's on a medical Aid product the model hallucinates and the sort of core core reason that hallucinations occur um and we were able to stamp this out actually while working there is because of spurious correlations in the training data that is there are things in the training data that shouldn't be seen together at the model at the same time while it's training um and to be able to Stamp Those out uh you just need to have some way of very quickly sort of like scrubbing over all of your data to figuring out to figure out what's in it but then when you find one example of something that's in it that you need to remove uh you should be able to very quickly find all of those examples and this is sort of where like vector databases come into play right uh they allow you know once you have one example if you're looking through it through the lens of like a neural Network's embeddings you can find all of the examples and what you see with Atlas is sort of like you can think of it as a as a visualizer for the vector database like the the sort of like my MySQL workbench for for the for the relational database that you see for a vector you can see the high dimensional embedding structure projected down into two Dimensions well we're both preserving the the sort of properties and patterns that you see in the high dimensional embeddings but you see those in two dimensions of that list yeah and I think if we could stay on this a little further so I've I've became really interested in topic modeling I interviewed Martin grudendors about Bert topic uh so I like jumping ahead to the kind of this topic of like yeah topic the topic of topic modeling how are you currently thinking about its applications uh yeah yeah um so the way we think about it is as follows right uh every single piece of data uh in the next decade will be run through some neural network whether you have texts being run through a Transformer pre-trained represent text where you whether you have large language models generating piles of content to the internet that you need to somehow make sure you know isn't going to ruin the world and ruin the elections or whether it's images audio running through all the latest various neural networks that are coming out and what you really want to be able to do is make sense of all this embedded data that exists out there so Atlas under the hood it's sort of built out as sort of this file system where you can just dump in beddings and then get super powers over top of them uh so one of those superpowers is visualization that we were just talking about that allows you to find patterns in your embeddings um another one of these is topic modeling so what this allows you to do is find like a homogeneous regions of your embedding space so like once you find a cluster in your embedding space that exists um what we do is we ensure that that cluster can be representable and sort of accessed over like HTTP API calls for example or just like access to house so in beddings that are all similar together sort of get represented as sort of one topic with your s and yeah I think without so like I guess let me let me put it in the context just like a builder right when we were building out Atlas um the very first thing we did is we implemented the sort of dimensionality reduction algorithm that scaled to tens of millions of embeddings which doesn't exist you take umap or or um or tease me out of the box you're not going to be able to scale tens of millions of the betting so we had to sort of implement our own um and once that was there it was awesome look you could see all of these dots on a screen and we would show it to people and they'd be like wow this is so cool what does this mean this makes no sense um and topic models were sort of the thing that allowed us to start educating people who are like maybe not like ml experts as to what they were looking at because you could very very easily get out get out sort of the trends that you were seeing you would see like those cluster protection it actually is homologous it's all the same color for example on the sort of 2D map view that you see in Atlas um and then sort of like the next upgrade was like topic labels for example uh you could go in and see uh not only for instance if you had embeddings of text you cannot see not only like the actual topics uh via the clustering that you see on the map but you could actually see like high level human descriptions of those of labels um and so this sort of like Segway is also in the GPT for all I don't want to get get too much in there but you have to generate those automatically somehow right yeah yeah and of course we're coming we're going to talk about gbt for all but and yeah the ability of gbt for all to you know maybe do like a summarization chain as we've seen like Lang chain summarization chain using the you know gbt4 models makes perfect sense of the gbt for all could be plugged into that but yeah summarizing the topic this kind of automatic category Discovery is so fascinating there's kind of one other idea I wanted to uh ask you about I I've heard this question from people like one of my conferences and stuff is they want to say you know take customer reviews dump them into this kind of thing see the you know clusters of their product reviews and then they want to do this kind of thing of like the automatic Discovery from that I also think about maybe if there is a connection with the symbolic metadata associated with each cluster so you know you have three clusters and then say you know say it's like my tweets and I like you know I cluster my tweets and then I say like this cluster has like you know this has the most Impressions like the category of tweet however they're embedded in the space and so I kind of visualize my clusters with symbolic data as well you think about that kind of application a lot of like yeah I guess like symbolic data as well for understanding what makes these clusters differ from each other yeah I mean so like at its core what Atlas is showing you is not like black magic it's showing you cystical properties that the underlying model that embedded the data has sort of learned um and sometimes it is the case that for example the metadata that you've seen humans associate or like a platform like Twitter associate with your data is actually already captured in the embeddings so that's why you see for instance if you go on Atlas and you color by a piece of metadata you'll see like it sort of replicates what the topics discovered because the top the topics are sort of like an unsupervised way of seeing uh properties of that vetting space but if you have this metadata attached to your data it also can be you know explicitly represented um and yeah so like at its core uh you should be able to sort of discover these patterns and we try to we try to really be like um what's the word like we don't we don't tell you how to use the platform it's sort of your job to use it as sort of a discovery tool um like when you're going through and actually doing the visual discovery of of what what your embedding space of your data represents um and to like to your question it's a little bit difficult to um if you have a model that can represent your data uh in such a way that it's captured all the various properties that you'd want to make you all the various properties of your data that you'd want to use for some business purpose um then yeah awesome uh the unsupervised sort of like system that we have built out with uh dimensionality reduction and the topic labeling yeah that can automatically sort of give you access to sort of these sort of attributes but usually like you have some sort of like thing over top of it that you really care about this is why for instance like vector databases like we V8 right metadata filtering is so important you can't just do Vector search and I'm like you know you're done right metadata filtering is everything um so having access meta is like super super important and the embedding space sometimes represents it sometimes it doesn't uh but you need a way of seeing it right I yeah I've also been thinking about that kind of the category Discovery from topic modeling to be used in filtered Vector search I guess my thing with that particular line of thinking is that if it's a property that's captured in the cluster then the hnsw then the you know the graph in the vector index will already kind of capture that this is where I'm a little torn on that particular part of it but but I really this idea of like debugging embedding models is something I really want to get into as well like so so like we talk about um use training language models with this data and visualizing the training data for language models so is it thinking mostly around visualizing how the Clusters evolved from the latent space of the language model or say are you also using this a lot for like the training of the embedding models themselves like maybe just to give a little more background like I my dog fooding data set with leviate is this podcast I like to like take these transcriptions and put them into the vector space and so I've been thinking like I've been training embedding models in this particular data and I can imagine like visualizing the Clusters evolving over time like as I add this new podcast with Andre and then see how training on this data set changes my whole Space could be really interesting so that kind of debugging models yeah but here's how you think about that with no money yeah I mean I really think that everyone should be everyone should be looking through their looking at their data through the lens of embeddings um and embedding like data changes over time right so embeddings will change over time um one of the things that you can that we it actually is not concretely implemented right now as sort of like a native way but one very natural way where embedding change over time is sort of the suitcase of like debugging like machine learning modeling frames as a model trains right every time let's say you do an evaluation on your like Dev set what happens is you generate a new set of embeddings and it's for every single test point that you have in your data set you get an embedding of that test point out and the organization of those embeddings is basically a proxy for how the neural network is organizing your data um and the embedding space sort of gives you a human way a sort of a human interpretable lens at looking at what your model is learning and what your model is not learning we have this like really nice demo example on mnist uh where you can go away to like like a pytorch lightning hook with Atlas hooked in to train your model on every single Epoch you'll sample you'll sample like a view of the embedding space and you can very much see like the digits form over time um that reflects in like the like the like quantitative metrics like the like sort of like accuracy that you see over those 10 digits but you can really see like as the model trains the embedding space sort of like molds like Play-Doh and like the eights and the zeros are closer together than the eights and the sevens because well you know the embeddings for those are are are putting them apart there's there's more data points where it's where eights and zeros are being sort of like together causing like errors in your actual quantitative metrics but you can all you can see this whole visually so it's a good debug for model trains as well yeah that that's so compelling I the yeah I think I've seen an animation of that before of the light in space evolving on emness probably probably from Atlas where I saw that that is so cool and okay so I really want to come back to this kind of topic modeling and property Discovery topic later on the podcast but for now I think we have to dive into gbt for all you know you've hit the Grand Slam home run and AI of creating the model that everyone's talking about how's your experience with this man yeah um so it was really cool uh I'm not gonna lie it was it was it was it was fun uh like usually when you build an ml model uh you read like at least like when I was doing like more research work as part of my day-to-day you build an ml model you spell multiple months building it and what happens is you go in and you write this big paper and you like release it out and then you hit you hear like chirps about it and like this is like even before like everyone was like basically publishing their Research on Twitter um so it's a discussion um you would not get any feedback about it it was just it was really cool to sort of work with the team to get a model out that everyone really cared about but I think GPT for all is not really about like hey there's this cool model and like everyone should use it and try it um because like the model itself it's worse than for instance like what you would get with a large language model API if you use like anthropic or open AI but that's not sort of like the the key ingredient the key ingredient is you can own your actual model the model runs locally and that only does it run locally you don't need some accelerated Hardware like an Nvidia chip you can run it on your CPU so that's that was sort of the whole premise of GPT for all to sort of do a demonstration that with the current set of ingredients that it emerged in sort of the technology stack so like the work of the gergenov and on llama that's CPP uh the work at meta sort of like accidentally releasing out these llama models that everyone wasn't supposed to have access to but somehow everyone found the turrets for um and like all these ingredients have come together uh to really demonstrate that large language models aren't these like big black boxes that exist behind the apis of cloud servers of a few big players you could run them locally on your machines that was sort of the thesis of GPT for all do a demonstration of this um and then it turns out everyone really cared about it so we were like hey we should maybe put spend some more time on this yo I think you just hit the pinata with so many things I mean like first of all it was pretty fast after the release and then the public reaction to Chachi BT that you got this out I mean this must have been like three months later maximum so it's like the speed of it is astonishing the as you mentioned running it on the CPU with the Llama C plus that's like pretty crazy and then it's like I do I'm a little curious about so so the the nature of this is you have a pre the pre-trained checkpoint from llama which comes from meta and then you fine-tune this to create gbt for all so that would be that was in fact the original model um we sensed sort of like the models so if you go around like GPT for all.io and you go in and you sort of download this UI installer that lets you play around with any of these local models um the reason we need this UI installer you can't just like go on GitHub is because it's a lot more accessible um it you basically we solve all the software challenges that you used to exist like for instance when we first released GPT for all what you needed to do is you needed to go in and like download this GitHub this git repository uh code it down and then you know non-technical people don't know how to do that you know it's kind of accessible oh you need to go take this random four gigabyte file from the internet wait seven minutes for it to download and then drag directory and then like you you run some terminal commands and that was the setup process now it's a little bit easier it's all the EQ and it's all this desktop client um and that's sort of like the chat in the chat UI that you see there and it isn't just llama models uh you have models that are based on sort of like more like less less licensed restricted architectures like gptj or MPT um we actually just uh so this is not released yet but I guess it'll be released by the time this podcast comes out uh Falcon is about to launch very soon so sort of like the the the move over the last few months has been we want to get the least restrictively licensed models in the hands of as many people as possible uh because that is the way you ensure that these systems can be deployed safe uh but the deployed safely um so that's what we sort of been building with GPT for all uh and the core Focus has really been on like software ecosystem stability uh the model the model is amazing as you can if you can go you can go and hug and face look up the word ggml and find million like Millions hundreds of models that exist that are all different versions of like Llama fine-tuned Or MPT fine-tuned or gbtj fine-tuned and they're all sort of just like they're there uh sometimes it's hard to use because the sort of software ecosystem doesn't uh support all of them because there's a lot of breaking changes being introduced in sort of like the core components that makes them make them make these models run fast on CPU the ggml or llama CPP and sort of what we try to do at GPT for all is make it easy to use for Everyone by like eliminating all those like software errors that you might encounter and also create like sort of the most stable ecosystem all the way down from like a c plus code that's optimized to interact with your L2 and L3 caches of your CPT CPU architectures all the way up to like the docker images with HTTP API so you can deploy into production and we try to give it out for free under Apache 2 license uh that's sort of like the the the the the the the the the the thesis everyone should have access to this technology yeah I mean um I learned a little bit about this l2l3 cache like the optimization for CPU inference with deep learning from speaking with Michael going at neural magic and learning about you know what neuromagic is doing around CPUs is I think it's really hard to undersell like how impactful that can be running it on the CPU because it makes it so much cheaper to sample all these inferences and then you couple that with like the auto gbt strategy of coming up with these action plans and like you know or like gbt team is like another example of like you know you have role playing uh llms that communicate with each other and with CPU inference being so fast it's also fascinating but I quickly before maybe if we talk more about the implications of models like this I want to get a little more into how you train this so you know things like say the low rank adaptation like because because I always you know these models typically have come from like Google open AI micro like billions of dollars you know big so like how how are you able to you know yeah let's let's be clear here like what what GPT for all has built stands on the shoulder like the very broad gigantic shoulders right like we put a bunch of ingredients together and spend a bunch of time curating data like we use Atlas to curate the data that was that was sort of like the edge that we had we can very quickly take a large a large collection of data make it clean so we can train the model into that model actually be like useful to interact with they can feel like chat GPT even though it wasn't the type of quality of chat um and on most tasks uh like sort of the tasks that required like a reasoning creative tasking is pretty good um so like the ingredients like what are the ingredients to make a GPT for all that's I guess the question you're asking uh the ingredients are as follows uh number one um you need a very you need a very strong pre-trained model backbone that is a model of a large Transformer model that has been pre-trained on a whole internet worth of data trillions of trillions of words tokens um you need a model like that uh that's very expensive it costs millions of dollars to train a lot like that thankfully Meadow did this thing where they opened where they released one for researchers to use in an open source fashion so the research Community can build around it um so that happened and now but everyone else is doing this right so the UAE released the Falcon model recently Mosaic released MPT but the ingredient is a very very strong backbone because without a model that knows about language you can't fine tune a model that can be a chat assistance uh the second ingredient that you need is a lot of high quality data of humans interacting with chat models now what we did uh in March is we found this really nice high quality model that you might have heard of it's called chat GPT um and we took a bunch of examples by giving it questions that humans might ask uh to um sample what its responses would be so we basically went on hugging face got a bunch of data sets of people like prompting uh models to do difficult tasks like write poems or like answer a question thoughtfully or use like Chain of Thought reasoning to produce out produce out of half put and we sample those responses I took that big pre-trained backbone so long up and we trained it over top of that data to be able to act like a China system so a chat assistant has to know you know no one to stop answering uh have some idea of like who it is what it can and can't do um and the way we were able to actually manipulate that data such that you can put it put it into training is using that tool Atlas we were just talking about uh there's many there's many ways you can do it but the reason Atlas was important is because if you sh if you train with data that for instance has a lot of duplicates in it or the data is not diverse what happens is the model sort of the fine-tuned model that you have sort of falls into modes uh where it can't actually go in and you use will chat assistant sort of across the board so Atlas was really sort of important in ensuring the diversity of the data and ensuring the data didn't have any like duplicate examples because the second year of duplicate examples the model Source memorizing with the duplicates are um as opposed to learning how to generalize over top of like all possible chat interactions so that's kind of ingredient number two um ingredient number three is make the models run fast on every single machine that isn't equipped with the Nvidia GPU uh that's sort of like all the work that came out of the gergonov's ggml library which allows you to run uh ml models quantize really quickly on CPU so you have a custom matrix multiplication kernels that allow you to execute quantized model inference very quickly um llama CDP which sort of gave us for the interface layer over top of that HTML Library um and sort of all those ingredients were put together uh we curated data nicely spent eight eight hours on a uh dgx a100 to train the model and then out of out of the DTX GPU oven pops out the CPT for all of them those are the ingredients and recipe if you're uh if you're taking notes yeah yeah I got my notes so let me rapid fire you quick questions on the three things um so firstly the the large pre-trained Transformer is the base you mentioned llama we see um you know Falcon MPT T gbtj there's also I think you know stability AI I think they have models so there's like uh like Valkyrie without like just saying how many of users which one which one do you pick is a matter of yeah yeah so are is islamas are most of these open source language models we're hearing about are are they pre-trained base models or are they mostly fine-tuned checkpoints from llamas should people be the only pre-trained base models are llama Falcon MPT gptj and then probably all the ones that the all the closed Source AI companies have under the hood and are sharing for instance like character AI open AI anthropic they all have their own ones but those are those are the core because again it's super expensive cost millions of dollars to train one of these models and a lot of data curation um teams of people to get one of these mobiles out um and uh those and every single other model you hear is a fine-tuned version of one of these models so somebody took a data set and they train it for a little bit of time it's been a couple hundred bucks of gpus to get one of these models out so that's kind of what you're seeing fascinating so then my second question about the collecting the data the first question I have is about like can the language models talk to each other and then you bootstrap the data that way is that promising or do you think you need the human in the loop for it yeah so I mean look you can um the the issue is like the overall like a lot of research papers sort of investigators like what limited what limitations can what are the limitations of taking an existing model and the technical term for this is distillation you can also tell you also call it training on another models another model's outputs um what are the technical limitations of like how far can you get right can you get a GPT form quality model by taking gpt4 responses and trading against it um that's like an open question um and some people there's people there's there's papers that have been released in the last month that say no of course you can't and then there's people don't say yes of course you can um and then there's models out there that sort of conflate with like the results of the first paper so the question is can you do this um you can uh it gives you better models out that than what you'd get if you for instance spent a bunch of time curating that data yourself or making generating that data yourself um but I guess my hypothesis on on on your exact question can you make models talk to each other and generate data probably not right because the models it's the models will not get the data you collect from that will not be any better than the model uh than the initial models that are talking to each other yeah that's that's a that's a good point within the model yeah because you're just sampling it and I mean I guess yeah I'd love to know what this I haven't caught up I've seen there's a viral paper title that's something that's saying that the knowledge installation models are just learning to imitate they don't have the robustness kind of properties of the project so I think knowledge distillation is one of those algorithms that seems like like on the service level it seems like a gold mine for deep learning because you're compressing it it is right so like on the technical level what's happening is that these companies are spending Mass amounts of resources to number one pre-train these models like sort of like companies like open the anthropic we're spending massive amounts of resources to pre-train the models uh for millions of dollars then they spend a couple more million dollars about tens of millions of dollars to gather human feedback of people interacting with their models and then be able to use this process called for instance rlhf reinforcement learning with humans back to tune the models be more aligned with like what they what their definition of a good model is and that costs a lot of money because you need a bunch of humans to actually be looking at the model responses and saying yes no maybe so um and then once you have that at the end of the day you still have the same neural network it's a point in high dimensional parameter space like like you know our our our our 40 billion or 175 billion point there and when you sample outputs from that point you might not have to go through the whole process of rlh being a model you might not have to go through the whole entire process of for instance pre-training a model that's the exact same pre-trained checkpoint that open AI or in topic started with because what you're doing is you're distilling the sort of the way that model generates out without having to just for instance have an rlhf model and a lot of Manila and that's sort of like that's sort of like the promise of distillation is that you don't have you don't have to go through all these intermediate steps to get that the company created that original model um you don't have to go through all those all those same steps to relate to be able to replicate that same sort of point in parameter space which is the gpt4 model that everyone is using uh because you can just sample its output and distillation has been a thing for a while like I mean the I think the OG distillation paper was like a hinted in 2015. um like hugging face like hugging faces claim to fame the way they got so popular was like they did they have this model called distilbert which is basically a compressed version of Bert which used uh like teacher student distillation um like 2018 2019 that's like what what made hugging things pop off um so it's a technique that works well um the question is like will it give you will it will it achieve you the goal of getting AGI probably not because like you're only going to be as good as the teacher model that you ever distill from you're not gonna you're not gonna learn anything new it's my opinion it but it's really interesting because it's like will it will that get you a smarter model probably not but it's like this idea like I think the gorilla paper is the latest one hugging gbt where the smart model orchestrates inferences to the task specific models and then it's cheaper to run inference with the like you know I had the same the distilled Bert had a huge impact to my like career in this as well where it's like it was just small enough to fit on the gpus and let you train it you know I think it was like 330 million parameters for Bert compared to like 50 million for distilled birds so that difference in what it requires to fine-tune these models run inferences like massive and I yeah like I love this angle that you're saying with Atlas with looking at the data deduplication something that inspired me a lot as you were talking as we're thinking about distillation is maybe to see the evolution of two cluster spaces with two models to like the mnist example we saw where you're seeing this evolution of the space maybe seeing how different models evolve have you played around with that kind of idea yeah so the the one place we've used Atlas are internally at nomic in the loop as we uh so we have the software consistent for GPT for all we also push our own models out which we spend a lot of time curating sort of getting the best models that are good with the bread and butter tasks llms need so we do a lot of internal work of just like training the models making sure they're good and we use Atlas uh heavily to making sure number one like the data that's going into them is good but also while the models are training figuring out what's going wrong with the fine tuning process for example um the gpt-j model which is sort of like the first Apache 2 license model that we put out there um the issue we were hitting with that model is so it's a much smaller model than what you get with the Kalama 13 billion or a llama 7 billion it's smaller in the parameter count it's also smaller than the amount of like tokens that it's seen during pre-training uh it's harder to train um and what we were hitting was this fact that while we were fine tuning on the exact same data set we were predicting llama on uh the model was just over fit like at a given point it would just complete like the the Eva the trading loss would just like like plummet down uh Eva loss with Spike and what was happening happening there and we're so confused like why is this happening it works perfectly on this like other model what we were able to do is actually take every single um point that we were evaluating the model rather every single point that we were training the model on and take a checkpoint of the model right after it started overfitting uh during training and visualized sort of like the loss so we had on Atlas mapped out the scalar loss so all the examples where it had high loss all the examples that it had low loss and there was this region of the embedding space which was just like all red which was like all corresponded to the region with super super low loss right before the model started overfitting and this region was on on Creative examples so we had all these examples of the model doing the tastic summarization like finding named entities in text but there was this one region where we were asking a model like write creative poems and the things with the things we created the thing with creative tasks is what happens is you have the exact same question you might have many many different responses to that question but they're all valid responses and what was happening is the model started overfitting to that region of the space causing an entire collapse in the model's ability to do anything else on other types of examples and the way we immediately realized that this was the issue is just so we looked at the atlas map and it's like hey look at this big bright red spot what's in there holy crap like a creative examples that gave us that hypothesis we down sampled them and then the next model train Boom the model was great gptj uh GPT for all J was deployed wow that's such a compelling Story I mean I've I've seen things like uh working with class imbalance where you know you have one labels 90 the other is 10 and so it's always going to predict the 90 label so you do like random over sampling where you balance it out by sampling and I remember like back in the days of training Gans you have the mode collapse where it's like just generating this deer that's like the middle of some region of deers yeah all that's so interesting so kind of coming out a little bit of the technical details of training this I really want to ask about the GUI that you've built around gbt for all how you're thinking about things like the chat gbt Marketplace and like how those kind of apps into the UI fit sure yeah I mean I think long term the viability of selling outputs of large language models goes to zero uh I mean you've just seen it in the past two or three months there are amazing models coming out that anyone can run their own Hardware um so what we've really tried to do with the GPT for all is sort of focus on getting a very stable software ecosystem that adheres to all the users of llms and users of LMS are not just developers they're everyday humans who want to for instance like put in their local data or like their local PDFs just ask them questions about them right um so the GUI that you see if you go to GT for all.io you can download a Mac installer Windows installers Linux installer uh what that is is it's a chat application it's written actually this for it's c plus private framework called cute uh it also cute also runs like sort of the monitors of like mercedes-benzes or like Adobe Photoshop is built in queue cool and what it's doing under the hood is it's talking with our c plus libraries that all have that have for instance llama C plus plus big din the all the versions of ggml baked in um all the various sort of ways you need to Define them the the weights of the machine learning models for the different Transformer architectures gptj Falcon MPT all that sort of like under the hood and all you see is one of this chat client that you get that you'd go for instance when you see like you're gonna go to chat.openai um.com um and the reason you need to sort of this client is that like for local llms there are sort of a few caveats that you need to do a lot of software engineering work around to make them as usable as you would see for instance a model where you're just like interacting it over like websockets or HTTP calls uh like you'd have like anthropic or open Ai and that is like for instance when the models all run on CPU right uh these models are large they take up like four to eight gigabytes of RAM on your machine and what happens is what you need to do is you need to be able to Cache the model's internal States if you want a very clearly say resume a chat once once you've stopped it there's like all these sort of like nitpicky things that are actually pretty hard to implement if you don't have uh you know a bunch of a bunch of elite elite hackers right writing the code to manage it so and that's sort of what we've done um everything from again the C plus plus code if you go to the GPT program it's all apache2 you can go in rip it use it use it for your commercial purposes uh don't use lava use the Apache tube models um you use it for your commercial purposes like you're happy like we're happy to let we want to maintain the most stable software ecosystem around these that's the attention yeah I think you're quite an entrepreneur I think you have to do incredibly compelling businesses here with with no make and then also this local chat gbt Marketplace like this first thing of um porting tools to work locally that's so fascinating I've seen like Microsoft semantic kernel kind of to me looks like maybe like a EBT Marketplace Lang chain that's designed for a specific set of tools like the Microsoft tools can you tell me more about so you know not making the HTTP requests keeping it on keeping the integration of tools privately I mean sure right like have you have you called Nvidia in the last couple months they're not going to answer your phone they're doing great uh the reason I can answer your phone is because if you go to an order let's say like an h100 be like hey Nvidia get me an h100 uh they're gonna talk to you and they're and if you're not on their like list of like top 10 customers you'll probably not get a response and then if you do what happens is tell you amazing here's your h100 uh send us the money we'll send you the card in about a year 48 weeks is the time to get a GPU right now um from Nvidia for like these like like these server grade gpus that can run these like large language models um everyone has been ignoring this massive massive ecosystem that we built of just like machines that have CPUs on them and there's a couple of difficult engineering challenges to make these models work on these machines uh but if you can if you can leverage that every single like every single let's say CPU enabled machine that you have at AWS that you have on Azure that you have laying in your garage on the ground those can run large language models on them you don't have to sideload all you don't have to run all your the whole reason you have to run the machine learning models in like behind apis is because you have to run them on gpus not everyone has access to them um so yeah that's like that's that's the thesis like models can run on CPU uh we can demonstrate it like here I'll go play with them in the chat client and like hey go build go go build your software over top of it as well you can you can you can own your L alums um there's another big there's another big thing happening in this area right now I just wanna I just want to call out uh Apple um I know everyone's like freaking out about their headsets right now they're freaking out about the fact that uh they've put in basically like gpus onto every single device in the world and just like aren't using them right now um Apple silicon that is plugged into your like M1 or M2 Mac um I think the m2s have the sort of the best gpus right now um you'll be seeing in about a week's time you can run a local llm at like 40 tokens per second uh which is like the speed that you get out of like a chat GPT model when you go to the web UI all on your machine just by having them just by having a Mac um and like iPhones have these chips all these other all these other devices have these chips in them um you will be able to run gigantic useful llms all in your devices in like the next few months it's it's real yeah the the Apple thing is certainly like game changing something that's emerging uh Colin Harman on a lost podcast is not Publishers were speaking but I had mentioned this idea of the language models come on the chips and Apple devices and I thought that was really compelling I read that paper on uh bite uh it's like bytes Transformer where they're taking bytes directly as input to unifying modalities a lot of interesting ideas coming out of Apple for sure maybe quickly I wanted to comment on this idea of you know like running models on CPUs and I also kind of wanted to bring back to the Wii V8 topic and talk about running big Vector indexes on CPUs at Wii V8 and so Something That We're researching that I think is incredibly compelling is uh developing the disk a n Vector index so the disk a n Vector index is about optimizations to move some of the vectors to disk compared to having to keep the whole graph in Ram and so this way you could just you know on your laptop have like an 100 million Vector index and I'm sure you have some ideas about like how you do like 10 million scale visualizations in the browser and this kind of thing of like not only are we making it easier to uh you know run inference cheaply we're also making it so you can visualize embeddings of just and visualizing and also the database side do the nearest neighbor searches at absolutely enormous scale what do you think will be kind of the implications of that idea I mean compute must be moved on the edge like there's just not enough server Hardware like it's it's a fact the more things you can do on on edge devices the more sort of offloading you can do the clever you can be about how like we're in the very early days of ml a lot of the stuff people are using just written by researchers who cared more about making the systems work as opposed to optimizing the systems for like actual utility uh and like let's say like in Productions like actual utility to Everyday humans um I think there's like amazing work happening on that end like we're we're trying to keep up as fast as possible like we you know we can we we can ship tens of millions of things to your web browser and run it on your gigabyte Mac what you guys are doing in the disk and then side like that's amazing like please please please show me and I will I will I will hack with it because like that that that's just that's great the better you can utilize resources on the hardware that already exists um like the better the better it is for everyone involved yeah and one of the topics that like when people ask me like what is exciting you the most about AI right now is this marriage of say the gbt for all cheap inference and then the you know Vector embeddings at scale with disc and Etc and and Atlas for visualizing it is like we can have the language models like talk to each other we call this generative feedback loops and and like we've written a Blog about this concept of like the language models they produce all this data and now you need to like tame the data so it's like it's like you need to produce the latent space but like you know you it's like let's say it's an image generation model you sample it's like a billion images from it and now you need to like look through it and say like okay what is it created kind of right yeah yeah I mean I think it's going to be a gigantic issue right now like if you thought the 2016 US election was was scary because social media was in full swing um you're gonna be you're gonna be frightened by what happens in the generative AI election that comes up in the next few years um every single like it is the the marginal cost of generating convincing content that's Ultra personalized to anyone in the world is zero right now it is or it is already zero you are when you go on Twitter you're probably interacting with LMS you don't know it um and they're personalized to you they know about you they've been prompted with like your you know your bio and this is a gigantic issue when the whole internet's gonna be flooded like it's a gigantic issue for ML model training because like how do you train on internet that's flooded with ML Tech ml generated text it's a giant issue for like everyday people who are like you know don't even know these systems exist like Chad GPT has only been used by like you know five percent of the US population or something like this maybe that's a wrong metric but I saw this from Recently like most people haven't been exposed to this technology yet but they're being exposed to the outcomes the internet's being blotted with the outcomes the outputs of this actually one of the Theses about what we were sitting building Anonymous is like we if the whole internet's gonna be flooded with gender with generative AI outputs be that images or text you need some way of digging you through it uh you need some way to have observability over top of it and atlas map is that observable interface I believe yeah I think so as well I I mean having the visual component to pair with what we're building or just hosting it it's a peanut butter and jelly type of pairing with and with the gb2 for all this Trends in cheap inference yeah well I mean kind of staying on that personalized generation thing and sort of like the next generation of spam where it's like hey Connor I read your blog post about this I have these thoughts and it's like not even a person on the other end it's like I remember like one of the big AI things was like Ian Goodfellow is like he's gonna work at Apple and he did this interview with Lex Friedman where he talked about how important it's going to be to have like cryptography uh cryptographic signatures that come out of the iPhone pictures to say like this is a real picture do you think a lot about that kind of thing and like sort of how that will change the world yeah um so what one thing we did with the original GPT for all model um I've never actually told anyone this I guess you'll be the first to know um is we watermarked it we we put in like we put in like a um in the training data like this phrase uh which the model trained on um and it's actually in the maps if you can find it if you search for it basically this ID and then we could know anytime a system is using GPT for all by putting that like string and it'll generate out something that we know is the It could only been generated out if they if they were using the model um so like like we have thought about this a little bit um and we're not doing anything with it turned out to be like a a sort of a useless thing to do uh because it actually didn't work quite well uh but it was the thought that we had um but yeah this idea this idea of just like how do you control for like how do you determine if something is made by a human or not made by a human um in a manner that doesn't require like going through like some centralized entity to mediate to that uh I think it's a giant unsolved unsolved question right now I mean I think like I I believe this is like this is like the thesis of like World coin right or like what Sam ultimate is trying to do is like let's get everyone's either irises and then you know when they're when you're when you tweet something you can call their API and and verify whether or not you're actually tweeting as a human or you know you're take a forward-facing feature of your camera I think that's where that whole thing's going um but it's it's an insane problem I'm sure every dictator in the world right now is ultra excited because it's decreased like their their like control form costs by eight million percent to like one developer yeah I mean I think it's great in the topic I wanted to kind of anchor this podcast with is your original PhD interest and interpretability and machine learning I think it's related to this topic of like you know I think this topic is like can we detect content generated by these models which has a lot of nuance to it and then generally maybe interpretability um what initially Drew your interest to that research category yeah so I guess you you put a statement in there I want to be very clear in my opinion of it we cannot like you cannot systematically detect generated content uh without having some extra metadata attached to it that by the provider where the content was generated in um it it isn't it is it is an intractable problem to be able to say here's a piece of text did was this generated by like a machine learning model not imaginary about a machine learning model um that's a fact um the sort of like metal level questions like how why did I get into like I I was just really interested in my whole life I'm just like I wanted to understand how systems work and ml was like the coolest thing to work on and I guess very naturally like I I I I joined like the Deep learning world but very very late um this was like 2018. this is like after like all the fun stuff had been done and everyone was realizing that like data was the actual juice that powers these models and like after the Transformer model came out you see every single ml system that's being built right now is being called the Transformer the model is staying the same the training the training objectives are staying the same the regularization methods are savings it's all just different cocktails data um and the realization is like you need better interpretability methods to be able to actually like these models will be the different the differentiating factor between all ml models being deployed will not be usually architectures it'll be the actual data that they're trained on you need better interpretability methods to understand like the data going in the data coming out um and that's sort of kind of what's what spiked my interest uh sort of sort of originally I I had realized like I spent a bunch of time in my undergrad working on uh NLP for clinical domain data it's like working with like medical notes and that was just all like a data problem it's just like the the methods were fixed you just copied the methods that were the in general NLP Computing Community was working on you had to get it working on the data that you had so I just had a lot of experience sort of digging with data that was non-standard I guess and that's all yeah that's really and I'd say with the first thing I sorry I say with the first thing um I think with like uh the Style again I remember studying like the watermark artifacts that maybe with images generated there's something to the patterns that can help you detect it and but with text I agree it sounds like there's no hope with that well even with images right so if the malicious enough actor is like I want to generate things that are undetectable and then you go out and you make your detector amazing you make your detector now they can use it or your detector to improve their generator and then like it's it's a race at the bottom like it's like like there's like the story of that college professor who like like got like failed his students because he put in data into chat GPT and was like hey is this real or not he actually didn't even use a detector he actually asked a model the itself which was just a different problem with these models and people using them it's a UI problem um but it's it's impossible to say whether a string was generated or Not by a by a model uh like going forward um we have to we have to think of other methods of ensuring that these systems are being sort of like safely utilized in the world outside of saying like yes or no to whether or not this was machine generated it's not economy valid solution yeah I think that's fair for sure I think maybe I I was thinking maybe there's something with like adversarial no you know like how you see like the it's like harnessing adversary example is the classic example it's like a panda and you add the adversarial map to it and now it's like given to show that like these patterns are like so fine-grained kind of like if you're talking about like a 512 by 512 you know 255 in each pixel RGB it's like maybe maybe there is some pattern that's detectable but yeah I agree I mean like yeah if you take a model off the shelf that some researchers have produced like yeah you'll be able to you know detect whether or not a sample is in the distribution of that model or not uh if you have a company that really cares about the quality of their models uh like I I like if to be a little bit forward-looking here I think what will happen is just like likely the model the the outputs of machine learning models that most people interact with will be coming from a few large companies that have these models that are managed and they're producing these models and they're going to develop their internal methods for tracking whether or not somebody you know is using the other model's outputs in some way that you know they don't like or doesn't divide it but they're like ethical or more to moral standards the bigger problem is not that the bigger problem is like when access to these models becomes democratized do you have to build tooling that can fight that being happened by monocious players and that's the kind of the a lot of people have asked me like don't you think it's unsafe to be giving these llms to everyone like shouldn't they stay behind like closed doors because that's how people can control them uh my sort of reputation of that is like if you keep it behind closed doors what happens is great you've centralized the ability of of of any actor in the future to be able to say whether or not somebody can use the models be somebody they can they can Envy they can go in and say you know has this model has a model output iterated by us or has not been generated by us from change that people interact with with data based on um I think by giving the models out to everyone it allows everyone to actually see like what is the actual current state of the art and what models can do and sort of build the appropriate countermeasures for us um that's sort of like my refutation there again that's enough for a loss yeah that's certainly one of the big topics emerging I mean I think that's a great argument in favor of the open models and I think definitely you're the one of the big voices for open with models with gbt for all and being behind that and all uh yeah I don't have too much of an opinion on it myself um other than just seeing the conversation forming around it like Sam Altman obviously going to U.S Congress made a lot of noise about you know regulate and and yeah I also I remember yeah there's also like the topic of like the toxic language models you train it on like you know that kind of data and produce that kind of stuff but yeah it's really interesting uh really interesting topic and I also something that you said about the the data Centric focus on AI coming out of that like the data Centric Focus I also agree heavily that it seems like data Centric is the way going forward like you know it's not like neural architecture tuning like I don't think the variants in the Transformers like like well actually well because I do think like flash attention this these kinds of things are how they're like scaling the input length so it's certainly impactful but I think that the data Centric is definitely the number one thing to focus on yeah um again that's that's that's what I spent on money sort of thinking about and focusing on it's how do we make sure that when we have hundreds of millions of documents of text or images or embeddings being produced out how do we make manipulation of that easy um there's a lot of ways to do it right Atlas isn't the only way to manipulate large quantities of text I hope it's going to be the easiest way that allows people who are increasingly non-technical to be able to do it like that's why we're building it um that's that's the premise yeah awesome Andre well I think nomik and we V8 these two technologies pair together so nicely and you have so many interesting ideas you've built so many interesting things so thank you so much for your time on the podcast before we wrap up quickly do you have any announcements about upcoming projects or how people can you know hopefully Mesmerize by all your knowledge can keep up with your future work yeah um I guess follow uh nomic underscore AI on Twitter um we're constantly putting out amazing content about sort of democratizing access to the data and models that surround sort of the current AI systems um I guess the biggest thing to look out for uh very soon um is you can run big llms on your computer uh and if you have a you have an M2 Mac you'll be able to run it at the same speeds of like a chat GPT model that you that you that you see and that's going to be released in the next sort of week or two uh so go try out local LMS uh because you don't have to send all your data to third-party services just to access this technology amazing Andre thank you so much thank you Connor ", "type": "Video", "name": "Andriy Mulyar on Nomic AI, Atlas, and GPT4All - Weaviate Podcast #58!", "path": "", "link": "https://www.youtube.com/watch?v=qb2nLeRpMWQ", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}