{"text": "Hey everyone! Thank you so much for watching the second episode of AI-Native Databases with Paul Groth! This was another ... \n[Music] [Music] n [Music] hey everyone we're super excited to present the AI native database podcast series we've invited some incredible people discuss where these new advances in generative AI are taking us and the role that database systems will play in it this series contains four interviews with Andy Pavo Paul grth John Ma and Dan shipper all four podcasts feature weeva co-founder and CEO Bob vanl thank you so much for watching I really hope you enjoy the series hey everyone everyone thank you so much for watching the wva podcast I'm super excited to present our special miniseries featuring weeva CEO Bob vanl and Professor Paul grth from the University of Amsterdam this series is exploring all sorts of new topics about the future of databases and what it means to be an AI native database Paul and Bob thank you both so much for joining the podcast yeah great to be here again Connor and thanks for joining Paul yeah thanks to it's nice to be here fantastic so the idea is that we had this conversation so um Connor and I where we said like there's so much happening in the space and everybody is so excited about what's what's happening that we wanted to invite a couple of people to have these conversations with of like where they think we're like at the Forefront of what's Happening where they think that things might be going and where things might be leading and I had a conversation with um uh with with Paul during event uh in Amsterdam and he had some very interesting ideas so that's basically the concept we're gonna record a couple of these things and these conversations so um it's a bit experimental but we look very much forward to um you know um start the conversation of where everything is going with the models with the with the infrastructure side the whole AI native stack if you will so that is the um that is the concept of what we're what we're doing here so so Paul maybe you can share a little bit what you're working on today what you are what you're doing what you're working on and then we can lead into your ideas about how things might be evolving yeah so uh thanks for having me first of all um yeah we're working really on this kind of connection between I think of it as how do we build great data sets and that idea of building great data sets really has evolved around this connection between knowledge graphs and now with language models and how do those two connect right and obviously they do through things like vector database search but I think this is where we've been doing a lot of work using language models as encoders for parts of knowledge graphs or using um uh language models to help us complete knowledge graphs or answer queries over knowledge graphs those kinds of things but I think the thing that really got us going when we were talking right Bob we were we were having this chat and I was like you know one of the things I've been thinking about a lot and maybe this is the controversial idea is do we need any databases at all yes so I'm a data management guy right so I think databases are great right and I think you know having structured data you know I did a I've done a lot of work there how do we do data integration building these things and so these llms are really have made me start thinking in in questioning you know what is the role of a database in these systems do we need databases and and so that I think you really latched on to that and wanted to have a chat about that yes that was I was that that was a super interesting thought and like also thought-provoking of course and I think what's there are two things that I would love to double click on so the first thing is the um the when we talk about knowledge graph right so the um we had this this this knowledge graph that sits in the in the in the in the domain of like the um uh uh of the semantic web right where we have ontologies definitions and those kind of things so my first question is before we go to the uh uh to to the concept of like trying to answer the question do we need a database or what does that stack look like can you can you first share with us what a what is is the did the definition of a Knowledge Graph change because of llms or is it still the same thing and if it's still the same thing or if it has changed how do you define what a knowledge graph is today yeah so I think really when people talk about knowledge graphs what they're talking about our graph structured databases right and uh there's many different you know you have entities relationships between those entities the kind of canonical thing that Google introduced kind of introduced the name of knowledge graphs and I think there's a lot of different technology Stacks right that you can Implement a knowledge graph in whether it's a completely graphed database Implement you can Implement a knowledge graph from my perspective in a standard relational database you can use the semantic web stack you can com you could implement it in Python but really you're talking about how do we create structured databases where the entities and there entities and relationships between them and you know that the entities are somehow unique right so this is what I think of when I think of knowledge crafts and in terms of you know where language models have really come in I think there's two kind of perspectives that have been so uh one is what I would call language models for knowledge BRS right so this is how do we use an llm to help us construct a a a Knowledge Graph and we do a lot of research on how you know we can make information extraction a lot cheaper uh we don't have to do as much uh annotation maybe that kind of thing we've also done some work where you can think of yeah we have in a Knowledge Graph you have attributes right right and in a lot of databases you may have text in a in a column right or an attribute of an entity you may have an image you may have uh even a you know a protein sequence and then you can use a llm to start getting information out of those kinds of attributes so that's another kind of language model for Knowledge Graph and I think that's where we're kind of out now right hey we want to build a database hey we want to build a knowledge graph yet use an llm hey we have a Knowledge Graph can we use that llm to help us answer uh you know questions over it or help us do better link prediction over the top of that that kind of thing so that's where we're yeah so would it be fair to say if we say the um um so basically what what you're saying if I if I Echo this back correctly is that you're saying like it doesn't really matter how you implement the knowledge graph there's like many many ways to do that one way to do that is inside the llm so for example if we have um you know a Connor Connor works at weate right we can make that as an implicit Link in in in Vector space for example well I think I think that's a little that's the second thing so I think there's when we talk right now we have llms for creating knowledge graphs where the knowledge really uh canonically looking at a database you know there's an entity you can write a graph query over it that kind of thing and then there is a language model as a Knowledge Graph right and I think that is also something that's a direction we have and so those are kind of the two categories I would put things in and I think both are super interesting to explore and probably the louder go ahead B yeah but before we go to the letter I just want to make sure because I find it super interesting right so because one of the things that I've especially also when we started with we8 so um um we were also thinking about this in the concept of knowledge cfts right so before the term you know Vector database was used and conceptually the idea was like that there was an implicit relation between two data objects if they were just closer together in Vector space right that was the that was the implicit relation and I think that might be conceptually for the people listening interesting if we go to the next part of this um uh of this conversation is that we say that we that that inside the model if you will sits those implicit knowledge gra relations are in there just by being if we represent the the information in a in a fact betting that they just simply sit closer together yeah and I think what people have done a lot and you know we've had to work on that and it's a big you know research area and and people do it is this idea of what's called link prediction or even entity resolution and a classic way to do that is you get your entities you put them in a vector database and you compute the similarity and then you could say hey actually those two are the same or that there's likely a link there and you know actually using an llm to create those representations can help you we have some kind of really nice work on actually that right right so getting better representations your entities and then you can put it in a vector database and you can calculate the the similarity or calculate new links but then what do you do right and then do you leave it in the data leave it in the vector database or do you kind of reify it and make it a graph again right yeah and I think a lot of what we've done is say okay we'll use the tool of a vector database we'll use a tool of Link prediction to make a graph again like put in the graph do some stuff add some stuff back to our graph generate the graph and we have a nice graph database again and you could view that as okay we have some text we put it through an information extractor we get some information on the text we put it back into our database our graph our knowledge graph again and we keep building up our knowledge graph and that's kind of the view and I think it's a pretty strong view that a lot of people adopt now there's this secondary view is hey let's put everything in a vector database or even more boldly just leave everything in the parameters of our model right and that's a view that is is super interesting you get a lot of power from that but there's also some that's a maybe a direction you want to talk about a bit more exactly because that is something that I'm that I'm super interested in right so if we look for example um when I was introduced to um uh uh to rag right so retrieval augmented Generation Um somebody pointed me to a um there's like this folder in the in the uh the Transformers um on GI up there's like a folder in the Transformers library that is literally called Rag and what you see in there is that the um what they try to do they have these two models so these two DPR models and what they basically do is that the model yeah for La of better definition knows that it needs to do a retrieval from a from a vector store and in this uh in this example that you can find I'm going to assume it's still in there and if it's not you can just find it in in the of course in the G history um it it uses phas and it does on that it retrieves that from face and it um uh and it's used in generate the the the tokens for for an answer and what is of course very very interesting in that and kind of different than how we today the term rag is used is what I what I personally use is that I use two definitions I I use one I use um more primitive Rag and and novel Rag and primitive rag is just shooting something from the from the factor database in the prompt right so that is what we see the most right now we we have that too in which but that's what I want to bark I want to talk more about this this novel approach right so where basically theal against for lack of better terminology knows that it needs to do a retrieval from in this case the store and if I understand you correctly is that you're basically saying that even that step is an abstraction that we might be able to remove in the model right is yeah I think this is where yeah this is exactly so I think you have canonically right so people doing I guess technically would be fuse shot prompting right or if you had rag would get some examples from your vector database you put it in the you know you put it in your prompt and then you would prompt the language model to do some Downstream task right or you could have models in which the you know the the mod you train the model to actually run queries against whatever underlying database I'm kind of really going one step further further or not one step further I'm coming to coming back to this idea that uh one of the cool things about these very large language models is that you have data already in the parameters of the model so here I want to reference a really for me a gamechanging paper is this work called from Petron uh uh and and the senior author that was Sebastian Rell called language models as knowledge bases s cited thousands of times really uh a fantastic paper and it was been 2019 and this idea that you know actually all the data is already in your parameters right do you need to go out to uh other places right and now that's the that's the question I have right so and and as the models get more performant do we just leave all the data in our parameters of our llm and that's it right and or how do we think about that in terms of new architectures for systems but this is where I'm I'm going yeah so so let's double click on that right so the the um one of the one of the things that I'm intrigued by is the fact that the um that that the models are stateless right so basically we we take a snapshot in time and um um so we you know we take we take a data set data set X train the model and then there's like a snapshot in time right so the um um this is for for you know if you if you play around with these generative models like like jgpt right that it says like I'm trained in 2021 I don't know the answer right or I don't have access to this information because that was in the original uh uh uh in the original train set so what we try to achieve here is State fullness right so that we can say we can change the state so um in database terms basically we would say we want to add crot support to the model right so we want to create something in the model we want to be able to retrieve it or read it and update or delete it in the in the model and what I'm what what I find interesting is that if we if we talk about um the the the process of creation so the the C in CR then that might be a form of um or this this this this what what people are doing with with this novel rag approach or if we're saying we we don't do that but we do that more in the model then it would be like a a form of fine-tuning I guess if that's done we have the second one so the the of the the the reading so we can read it we can retrieve it because it's now captured in the in the weights but I'm not sure how we can um update and delete data yet right so let's say that we fine-tune the model on new knowledge how do we how do we give it that that those two other um uh functions how do we update the knowledge that the model has or how do we even delete it yeah so I think so this is this is the kind of fundamental question that that I've I've been thinking about a lot of in these architectur so I have a a large EU project called explainable uh machine learning over knowledge graphs and and that's with a a bunch of different partners and there we're trying to explore kind of these different I guess I would call it representations of knowledge right so you have the representation that's in your database or even in your vector database right of of your knowledge you have knowledge that you can in some sense have in the parameters of your uh language model and you could have knowledge in I guess I would call it on structured sources for lack of a better term here I'm thinking text I images or video right and now the the question I've been really thinking about is what is the architecture where do you leave your knowledge right so some of that knowledge is useful to have in the parameters of your llm but maybe as you said it's hard to update right and we can talk about okay we can do like adapter tuning so things like Laura to encode different knowledge but maybe that's too expensive but there's this cost actually of even getting something into your database right so there's this information extraction but maybe we can do that information extraction completely on the Fly and so that's the question that I've been really playing around with is like where do I keep my knowledge right what's the best do we have any ideas about where you know do I keep it all in natural language and let the llm process it for me on the Fly do I actually you know put it in a database where I have some structure I mean even in a in a in a vector database you have some structures right I think in we8 you guys do these nice little Json structures right that you can push in into the the vector database so that's pretty cool or do you like say look actually we put it everything in the parameters in the model and so that I think I think a lot of people or I think there's a that's the thing we're going to be playing around with in the future right so where do I keep my knowledge right yeah so if we if we assume that for the foreseeable future because that is also something we we could debate but then you know at the risk of getting too abstract let's assume that for the foreseeable future that the that the the the core architecture of the llm stays Transformer based right what is the and let's say we solve the read uh sorry the update and delete uh problem right so let's assume for the sake of argument it's a it's a Sol we can create read update delete um then still we we are dealing with the with the hallucination problem right so that um if we so and that kind of is related is if I understand correctly with the thing you previously said that if we add something to the model we need to make sure that whatever you know knowledge we gave it that it will reproduce the tokens exactly in that order as we stored them right yeah I mean I think there is um I think the question is what are we worried about with hallucination and what we're really worried about is that we're going to get you know unfactual things right we're worried about things that are incorrect coming from from the model and now the question is what's the best way to address it and right now I think one of the better ways to address this is these rag style models right so you're using the llms language capabilities but you're saying look you have to do it on these trustworthy sorts of documents I can control what you're doing there might be other ideas about how we can control the hallucination process right or to control Hallucination is what I mean um where you're where you're not where you're not looking at controlling the inputs to the model uh where I think is really interesting is what do you want to use a database for um and in the sense that I think with a Transformer based model it's going to be very hard to control exactly the facts you're using right and I think that's that's something like just think about it this way you're talking about a a customer relation management system yeah right and you have your customers and you have uh you know their contacts their you know where they are on your sales pipeline all those things and there I think you want control right and there you want to have precision and that's the fundamental thing where where we're going to just have to think about you know where do you want Precision in your whole system and that's where databases are always good right that's why we do recordkeeping fundamentally and the Precision we give up something when we use a llm and but we give it up for a reason right we give it up for the softness that it provides and the the capabilities that provide but I still think you know that I don't know yet how to Precision control the knowledge in you know a Transformer based llm right that's the that's where I think I haven't figured I mean if you have a good idea about how to do I think that's where because if we could Precision control that that then we would be in a very different space and maybe with in context learning we could push the model in the right direction right so making it you know prompt it to get to the right parameter space but that Precision control is like where we obviously have a set of technologies that can back a CRM system right yeah and so it can we so it would I'm I'm interested in reversing the argument right so let's say that the answer to Precision control is a database so that we say basically currently these weights is just it's just a big binary blob right it's just you know that's on this and then it goes through it what if there's a a way of storing those that information and these weights the other way around in a database that the the the the weights are stored basically and that we might have a um like um I don't know a g style tree representation or something that we say like okay we know how it's changed so um um uh um we added something to the D let's stay let's stay with the example of the um of the of the uh like the the the CRM system right the that the um we might tell the llm customer X moved from the beginning of the pipeline to the middle of the pipeline for example and that we can actually that how the weights change based on that rather than keeping that in the now finetune blob have a database that actually stores that information that we can also that for whatever reason if it's wrong that we can for example reverse it right so that is actually that the argument we that we do need a dedicated database but that we need to be become more sophisticated as Factor database provider in what the database can do and that it interact with and how it interacts with the model we because we kind of now assume and maybe this is just the most optimal way of doing it I'm just this is the the idea of these these conversation right that we that is like that we can start to think about these things maybe this is just having this binary blob of these weights is the optimal situation but I was intrigued when I saw for example um these get up repost pop up like GPT for all Etc where they started to Port the weights to C++ to run and and then you could run on GPU and on CPU those kind of things and I was like I don't see that why it needs to stop there right so um then actually you would need a specific database and rather than having a a blob of of uh weights you have a database with weights that gives you that yeah and I mean I think that's I think absolutely absolutely I think one of the interesting things is just even in a more a simple thing right let's say you have a database running your and let's say you have different llms so this is something that we've done quite a lot is we have for example we just we just put a a paper on archive um called bio blp and I can give you the links later but the cool thing about that is we use different llms as representation encoders right so here we use an llm for protein sequences we use another llm that's designed for molecules right so it's trained on sequen of molecules and then we use one that's designed that's a standard llm for text right for diseases and what we do is we use we get representations of all of those different entities out and we've got they're encoded in a binary blob right and you know that's a perfect situation where well what do we where do we put that stuff right and we put that on files right and we do some stuff with pytorch da da da and load those files but there's a place where actually having databases that are able to cope with those kinds of encoding approaches would be very nice we have that a bit with Vector databases absolutely but that kind of connection where you have a you have a data set that's your kind of semi-structured dat you know exactly how those representations are connected you R run them through multiple different kinds of llms and managing all that stuff I mean maybe maybe I just don't know but I feel like that's a place that you know having more systems to help us with that and to do that all super efficiently right I think for from my view that's a super interesting area or direction to go now it's not as far out right but I think practically that's something that people are going to I know we want it right and I think you know that means that I think other pokes are are going to want it as well this this is great so so I think if we so if we wanna want to get to like a conclusion of what we've what was discussed here is that he that that basically and please make sure to to correct me if I'm if I'm wrong but the that one way that we could say if we look at the where the llms right are right now or the multimodal models for that matter right we just can throw them on one pile just so the let's call them the just refer to them as the llms now right so you have the llms and the the the the problem that we have is that we're saying that um that the llm has in combination with the database is that you get a vector representation out that you store but that's like one directional right you can't go go back you can go from the vector embedding back into the model and reproduce you know however the you know um the weights were touched to get to that factor embedding but if the database would evolve where we would say no no we rather than having the binary blob of Weights we have a database representing all these weights so the the next gen uh uh so that that then then the fact database evolves in weights database I guess might be might be a way way forward so absolutely and I think what I one of the things you know I'm you know I'm an I'm an academic right and and one of the things I like to do is look a little bit at history right that and one of the things we had in the I think is in the two 2000s late 90s um there was a really in database archit ures one of the trends that people started to look at was they called them text databases but the idea was is we if we have specialized data types could we do inside our database managing those special data data types doing special things for example doing information extraction on on text right and that was a real architectural thinking back then and I think now we might be in a really good place to do that so what should our database look like when we're really able to effectively cope with data types that are not just relations right or not just exactly pure stru and so that you know sometimes you know these older ideas if they come back they come back at a at the right time right because in the 90s 2000s we weren't in a place that we had these this capability to very easily work with unstructured data but now we are at that place so now we need data management systems that help everybody do that now this is not as controversial saying we don't need databases but no no no no but so it's it it was not my it was not my intention per se to end this conversation as like that I wanted to somehow you know flip the argument but I what I find super interest so one of the things that I so you're an academic and I I run a business right so we the um that that's that's well different right so but one of the things that I like to say from that perspective is I call that the about what's happening in what we like to call the AI native stack so that includes the models and fact database today is one of the things that I like to say is that if you if you look at the the factor database you can make a um I like to call it the the pessimistic argument and the optimistic argument for what they where they sit in what they bring to the to the the data stack and what I mean with that is that with the the the the pessimistic argument is that it's just another no sequel database right so every data type you have these these handful of general purpose databases the postgress is my seals Etc of this world and then you have these dedicated um uh database around data types that often sit in the no SQL layer uh graph databases time Ser databases etc etc ET so what I like to call the pessimistic argument is that effectiv is just another nosql database but the optimistic argument that I like to make and that is related to what you just said Paul is that that I believe that the there's this Paradigm Shift happening on how we're interacting with our data and how we're storing it because of the the llms right so it's it's more yeah and this is the this is the thing that I think is so exciting is that we're now able to deal with unstructured like unstructured data can really become a peer to structured data and many times we may not even want to structure our database so think about your we we started CRM system maybe before right we had we had we have this emails back and forth with our context right and for one of the things that you might do is you might go do this manual work of you know getting out the phone number of your contact getting out the you know their willingness to pay I don't know I'm not a I'm not a great sales guy but like uh like you get all this all this information you put it in the fields of your CRM system right but now maybe we don't have to do that and maybe we don't have to run a complicated information extraction pipeline we can just say actually our llm is able to do that and reify what we need on the Fly and actually that's good because maybe the person updates their contact details right so we don't have to you know we don't have to synchronize and that's really the point synchronize our unstructured data which may be more up to date with our you know curated structured data now if I'm building a canonical data base where actually I've implemented a you know a UI around it and I'm collecting data from a form that or an app or whatever then actually my structure data is natively structured I would call it that way but a lot of data isn't natively structured so why structure it when you can quote unquote structure it on the Fly and I think if you think about Vector databases or I mean I mean in some sense in in databases what happens is all relational databases consume all like now if you look at graph databases SQL has a from last year has a you know a graph query language in it right but it took many many years to figure that that that's fundamentally what we need we need these graph people there's enough consumers that need that or time series or whatever but I think more the question is if you're building a data management system you really have to think about the idea that your unstructured data can be pretty peer and you're going to have a lot more of it because it's actually proba in many cases more up Tod dat than doing the structuring process and so that's the kind of data management architectures we're GNA have to think about and build right so exactly and I I I have a quick anecdote to share because I this was in the very very very early days of we8 so this was before you know sentence edings what have you so that was just everything was still based on um on on glove and fast text and I remember that I was at a um at a link data I was presenting at a link data conference and um the the what I was showing that was like in really the early days of we8 what I was showing was a lot of these debates that were happening at the link data conference was like how are we going to make well the relations right so how do we agree on on things Etc and um the topic of my talk was like What if we don't what if we let the the model uh uh decide right so and back then how that was done was that I what I did was that I took a a little paragraph of text and the the example that I always used was the um uh um that I had like the Eiffel Tower and I had like FIS as data objects and that the relations were made based on the fact where they set in Factor space and then if they were close a relation was made in the in the database so what I showed to the audience there was that was saying why just not why why shouldn't we just stop doing all this and just let the models and again those were the earli these early models let the models decide what the relations are well I can tell you not everybody appreciated that idea because if you built like 20 years of your career on figuring out how to structure ontologies and those kind of things and then somebody comes let them all do but I really do think that's paradigm shift that's happening yeah and I think one of the actually things I I gave a a talk recently um to folks that work on knowledge organization systems so these are really canonical people who build things like Library indexing structures and one of the things I messaged that I sent to them is a lot of actually what I think is the the cap the things that canonical knowledge Engineers do actually are even more important not the building of the ontology if you look again at very old Literature Like 1990s stuff right expert system stuff funnily enough a lot of those methodologies there's a big chunk about building ontologies and structuring things and formalizing things and building formal but a big chunk of that is actually talking to your customers understanding the organizational environment so essentially what we would call now data product owners product owners in general user interfaces designers those skills actually we with all of these llms what we're doing is we can Empower canonical knowledge Engineers to go faster because the knowledge representation language Now isn't an ontology it's you could almost view it as natural language right so that's really yes so when we talk about prompt programming right what we're doing or managing prompting is in some sense natural language has become our um you know our knowledge representation formalism for lack of a better term yes yes but the point is is that doesn't mean all of the things that you would teach somebody about talking to stakeholders understanding what they're doing figuring out the system they're in that all stuff applies and you just have a you know much more powerful as you call this AI native stack to build those kind of products right so it just makes everything yeah I think it's a little bit easier now to be because you're not mucking around with formalism so much and we have a new I'll I'll spend I'll send you a uh we have just a new kind of um paper on archive about this I'll send it into the slack uh called um knowledge engineering using large language models and this is one where we think about these two directions so one is using yeah go ahead Bob no I just want to say I I really want to want to emphasize how important this is because the um um to tr one more anecdote very early in my career um when I um uh I was like you know early 20s and I was um I was working on a I was hired by a a larger organization to help with like um that was an e-commerce related data structure right and my ambition was to capture everything for that large organization in one big um yeah adjacent object basically right so there all these definitions from what we had as customers and the person the senior architect hir me he like you can't do that and I was like of course I can that's and he was like and he he taught me a a little lesson so he he got me in touch with three people in the organization and he said just visit these people and Define just your Jason object for customer so I was like sure so I went to the first person had a meeting and a beautiful Jason object came out representing the customer so I went to the second person I like this is how I'm going to represent the customer and this person said oh yeah but that's not how we look at that's for us not a customer that is something else so long story short he taught me a lesson in a day that just by talking to three people in this large organization I was unable to to capture the definition of customer in one Json object and the what I mean with the paradigm shift now is that we are Outsourcing that as also what you said in like in the in the form of prompt engineering to the model they were basically saying like Hey listen this is our organization this is the structure of our organization this is how they look at customers this is how that part of the organization looks at customers now do something with our customers right so whatever whatever it is that you want to solve and I think that that's the Paradigm Shift right that we Outsource off to the models yeah I mean if you want a really far out thing uh we ran a hackathon in London this summer we call it the knowledge prompting hackathon yeah and we had 40 PhD students and we actually just said okay we want you to design we wanted you to prompt like crazy and we literally had so for example we were prompting these models to actually act as uh for example um user story creators so it would literally we could prompt it so that the model would ask you and it would build up user stories for you which was was super wild for me all the things that we're doing or for example another thing you know we having the model help you curate your undering knowledge so having the model drive you to help make decisions on for example definitions that you want um so I think that's really powerful but that doesn't I mean the other side of it is like actually I think this is a um I think this is the powerful mechanism of when do you need structure and when you don't and but we can now drive this from the from the needs of the business or the organization or the the product yes yes so this is this is this is this is super exciting and um thank you so much for for sharing these these viewpoints um uh uh Paul I think if we we want to recap all the the Lessons Learned after we've we've did done this this miniseries but I think that the the ideas of of this Paradigm Shift um um getting the question answered do we need a database in the future at all right is super thought-provoking and interesting and if the answer is if the answer is no then the question is what does that look like right but and then but if the question is like yes we do need it but then it's just it needs to evolve to capture really on a weight like basically have crot support on a weight level right so um this is this has been super um helpful and before we end it's just just something I told you in person but I want to also say this I have this on record that you've played a tremendous import you know important role for for me personally in the early days because it was early in my career when you were um introducing the models and like back then with glove and F like that was really a big eye opener for me so um uh indirectly you play a role in the fact that that there's now a effected databas company so thank you so much for sharing all that and for you know joining the podcast thanks for having me it's uh it's been really fun good luck with the series thank you ", "type": "Video", "name": "Structure in Data with Paul Groth: AI-Native Databases #2", "path": "", "link": "https://www.youtube.com/watch?v=3ET69F7smk8", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}