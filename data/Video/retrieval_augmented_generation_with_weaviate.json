{"text": "Meet Verba, the golden RAGtriever! Verba will not only help you build with Weaviate faster but also enable you to bring your own ... \nHi, I'm Philip from Weaviate. Welcome to our new series\nbuild with Weaviate. In this series,\nwe take you on a tour to do a deep dive to our newest Open Source Contribution\nproject and life demos. We will also show you our little tricks and techniques to build\nbetter machine learning pipelines. In this episode, we are going to reveal\nour tricks for better retrieval. augmented generation pipelines. RAG helps you utilizing large language\nmodels and reduce the model hallucination from more precise outputs\nand better results at Weaviate. We love open source and work hard\nevery day to make your developer life easier. Developing applications and machine\nlearning pipelines can be sometimes hard. You run into errors, problems and often you need to learn a new API or framework. Also, starting with Weaviate is learning\nsomething new. And here comes our newest addition to\nthe Weaviate, open source family into play. Verba. The golden RAGtriever, Verba\nis going to help you build with Weaviate faster, but not only with Weaviate\nbecause you can easily bring your own data\nand adapt Verba to your personal need. It is complete customizable because\nit's open source and we are releasing it with already included data to help\nyou build with Vercel for NextJS Cohere, HuggingFace and OpenAI. What is Verba doing under the hood? It used a so called RAG retrieval\naugmented generation architecture. This architecture\nhelps the LLM to generate better results. It also helps to reduce the time window\nwhen the model needs to be retrained or to give context to events with happen\nafter the training data ends. For example, looking at ChatGPT\nand the fixed knowledge horizon or the so-called knowledge cutoff. So the model hasn't seen\nany training after events happening after September 2021. RAG also helps\nto reduce the problem of hallucination. That is a model just dreaming\nfictional things that are simply not true or reflect a reality in easy\nthe model is making something up. But for you, it sounds plausible and without knowledge or fact checking,\nyou're going to run into problems. How does retrieval\naugmented generation work? In this example, a user is asking \u201cWhat is Weaviate?\u201d then we retrieve and search\nfor the fitting documents and chunks in an internal source\nlike our company knowledge base. We then pass this information\nas context to the LM\nto generate the answer for the question. And in this case,\nthe answer is 100% correct, because Weaviate is the coolest vector database\nknown to humankind. After all, also,\nRAG is not going to be the one silver bullet hitting all the pitfalls of machine\nlearning pipelines. So as often a hybrid approach or fine tuning RAG in combination could lead\nto the state of the art results. Let's now dive straight\ninto the action of the Verba Verba comes by default with all our Weaviate\ndocumentation. This includes our internal\nand external documentation, our engineering blog posts around Weaviate,\nthe transcript of our YouTube videos and everything that could help\nyou build faster with Weaviate little teaser. It also outputs useful code snippets. Let's see how easy you can use Verba,\nbut also how to transform it for your personal needs\nand bring your own data. Let\u2019s head over to verba.weaviate.io to interact with our Verba The golden RAGtriever. say hello to your new friend for building super fast with Weaviate. How we built Verba. It's using NextJS in the frontend, FastAPI, Weaviate and GPT in the backend. Let's start easy with some recursion\nand ask Verba what it thinks it is. Are we going\nto see the ghost in a machine? Do we explore AGI? Let's see. Nice or? Not only provided you with an explanation,\nit's also not claiming machine world domination, so humans\nwill at least survive for a bit longer. You can browse through Verba\u2019s outputs and go to the potential source\nof the information and learn more interesting insights. But now let's enter the serious action. We are new to Weaviate\nand we want to know how to use the nearText function in Python Voila, Verba comes up with the answer\nand the relevant code snippet. How nice you can now use this code snippet\nin your pipeline adapted to your needs. And the best part\nyou can use this live demo and ask Verba\nanything you want to know about Weaviate. In fact, we used Verba to build itself. How meta is that? Maybe we call the next generation\nof Verba,  Verba the Skynet edition Did you like the demo? Let's bring Verba onto your own machine\nand customize it to your needs. As Verba is open source,\nwe released all its code on GitHub. Little warning. The code shown in this example\ncould be to some degree outdated. So keep an eye on the GitHub repository. We want the installation of Verba\nto be super easy so it\u2019s just a pip install and whenever you want to run Verba\nlocally you can use Weaviate embedded. So no need to deal with infrastructure or use it in the cloud of via docker\ncontainer. To get started, make sure you're running\nPython 3.9 or higher. It is also a good practice to create a new\nvirtual environment for your project. So type in pip instll goldenverba\ninto your terminal and hit enter. Next, you will need to set up your OpenAI API key as an environment\nvariable named OpenAPI key. For now, Verba only supports Open\nAI models, but we are integrating more model providers in the future\nwith everything in place. Let's import the data we will use OpenAI\u2019s API documentation\nas example to import the data into Verba type verba import --path data/openai in the terminal. This imports the files into Weaviate and don't worry, Weaviate embedded\nruns smoothly in the background. once your data is imported fire up Verba with verba start you can access it locally. via localhost Congratulations. You successfully installed Verba on your\nlocal machine and imported custom data. Let's dive in and ask Verba\nabout OpenAI\u2019s new function_calling methods you maybe wondering how Verba integrates\nwith other frameworks or libraries. Well, we designed Verba to be flexible\nand modular. For now, it's optimized\nto work with Weaviate\u2019s native techniques, but also designed to integrate easily\ninto popular libraries like Haystack Llama Index, Langchain and many more. Let's take a quick\nlook at how Verba was built. We start by loading in the data\nand supporting a wide collection of file types like text and markdown. The data is then chunked and broken down\ninto smaller parts using spaCy. Our most loved and NLP library. After that,\nwe ingest the data and into Weaviate both the loading and chunking\nparts can be customized. Now, let's ask Verba \u201cWhat is Weaviate?\u201d It uses hybrid search to find relevant\nchunks from the important data. So traditional text\nsearch in combination with vector search. Let's look at one of our tricks\nwe use to improve the retrieval. We call this the chunk window approach. We add the surrounding information of each\nchunk to the context of the LLM. This helps to improve the results. As there is a high probability\nthat this information is connected and also relevant for providing\nbetter answers to the end user. The resulting context end user query is then sent to OpenAI\nto generate the answer. We are adding more of this retrieval\nand RAG optimizations in the future to Verba\nbut Verba is also open source. So we are happy to see you\nbecoming part of it of our open source Weaviate family. I hope you enjoyed our short\nintro into Verba. If you have ideas how we could improve\nor what new features we should add Just open an issue on our GitHub\nrepository or drop us a message. Verba should become your new entry point to build faster\nwith Weaviate and all other applications where you are in the need for user\nspecific context and relevant outputs. Thank you for watching\nand see you next time on this channel. ", "type": "Video", "name": "retrieval_augmented_generation_with_weaviate", "path": "", "link": "https://www.youtube.com/watch?v=OSt3sFT1i18", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}