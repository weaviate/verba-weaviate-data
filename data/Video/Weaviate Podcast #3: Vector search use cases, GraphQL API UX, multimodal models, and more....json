{"text": "Join \u202aConnor Shorten and Bob van Luijt for the third Weaviate vector database Podcast. During the show, they will be discussing\u202c ... \nhey everyone thank you so much forchecking out another episode of the wev8 semi technologies podcast i'm herewith bob van light ceo and co-founder ofsemi technologies me personally uh bobhas told this story that has reallycaptured my interest in the we v8 vectorsearch engineyou know listening to his story tellingit ondmitry khan's vector podcast and all thedifferent uh youtube videos on wev8 hasreally captured my attention and causedme to be really interested in vectorsearch engines so i'm sure that this uhyou know anotheredition of listening to bob tell thestory will help you also be furtherconvinced of how exciting thistechnology is so as we do this podcastum on the semi technologies youtubechannel we're sure most of our listenersarealready pretty familiar with the basicsof vector search engines if you're notuh there's an introduction to we v8 onsemi technologies i've made one as wellin henry ai labs there's all sorts ofresources to get caught up with thebasics we're going to kind of go intothe details and flesh out these visionsfor the future these exciting ideas likegraphql knowledge graphs neurosymbolicsearch these recent advances ininformation retrieval and supervisedlearning multimodal learning like imagetext search the demo endpoints the demosand how we see that playing out allsorts of really exciting things goinginto the research and the future and allthese kinds of exciting things so thatsaid um i think the first thing uh totalk about will be uh like what is yourvision how did you think of this graphqlknowledge graphs because i think this isa really unique flair of we've that idon't see a lot out there a lot wellyeah first of all uh uh thanks forhaving this conversation with me on theon our own podcast i guessbut umum yeah soso the originally the idea came from thefact that i was very intrigued byeverything that had to do with the withthe semantic with the semantic web andthose kind of things and if you dealwith a semantic web then you're veryquickly into the space of um ontologiesand there was this one thing inwhen i went to conferences when i spoketo people that just fascinated me andthat was that if we talk about thedefinitions that we have in ontologiesis that people don't agree on thingsit's just i that's something thatfascinates me and that this is causing alot of problems right so because the umuh if you wanna if we wanna searchthrough specific data sets or if youwanna wanna find something then umthat that's hard to do and i once gave atalk in greece and there was like thisuh this guy and he said like yeah and hewas like he was upset about this as wellbecause he says he said like i'm workingfor the european union and i'm test bycreating the ontologyfor rivers lakes and seas ineurope but he said you know what theproblem is i said tell meand he said like so what the rest ofeurope calls a lakethey call a sea in germany i said likeso we couldn't agree so i he said like idelivered the ontology and then thepeople in germany said like no no nothat's not a lake that's the seat and sothis wasn't a a constantly occurringproblem and if you see like publicdiscussions like relative wikidata etcyou see that this is a this is aconstant problem sothen i got introduced toeverything related to factorization innlp bear in mind this is i'm not thatold but it's like this is before likebirth and transformers and that kind ofstuffand i i remember that the first thingthat i saw that it was that famousexample i believe that it was like uhking minus men plus women and then queenthat the result that was like the momenti saw that for the first time i was likeboom it just i was like wowand a few years later i got this thisvery simple idea which is a veryvery hacky idea right um but i got thisidea that i said likewhat if i just have a bunch ofdocuments just three documents edges orparagraph of text or something i justtake the individual wordsand i just calculate a centroid of allthe individual vectors of thisof these paragraphs i just wrote that inpython so it was really slow but thegoal that i had back then was just tofigure out like can ican i find that document by searchingfor context rather than for keywordsand when i was able to do that i waslike i had this thing that was likeboom this is like an opportunity becausenowwe can we can target a note in the graphwithout knowing what the currentanthology is or the keywords are we justhave to describe it and then we can findit and from there on we can traverse thethe graph sothat is what the what the original ideawas and and there's way more to sayabout this but we'll probably touch upon this andrelated to graphql that's another thingbecause i am a very very strong believerina developer user experienceand now you have like a problem ifyou're dealing with a graph like datamodel and the problem is thisso how much flexibility do you want togive the end user to traverse down thisgraph right so do you want to besuper expressive then something likesparkle is interestingor do you want to be less expressive butgive a lot of make it easier to use andthen we of course have something likelike graphqland just asking developers about youknow their you know what was veryintuitive for them and and and what youknow how they um i wanted to use theinterface we actually learned that a lotof them liked graphqland that's also what i see when i givedemos i just i don't even i i sometimessay it's graphql and if people say likei never heard of graphql it's like don'tworry because i'm going to type it outand it's going to be intuitive andumplus the fact that we are not a graphdatabase but really a vector database wesaid like we we don't have tosupport all these specific graph thingslike neo4j does or or draft does that'sfor them to solvewe have this graph like data modelwhere we want to target these nodes andwe're going to use graphql to do itbecause we believe that it's the easiestway for people to use it so that is thethe history of that uh interface i haveto say umthat was just the the concept the actualdesign uh is done by my colleague lauraand she saw there's also some videosfrom her online and that kind of stuffwhere she explains what she did there socredits for the design go to her but thethe overall concept was like how can wegive the best ux to end users to getaccess to thesevectorized data objects yeah and i thinkum one of the like visualizations ofvector search that we like the most issay image search like where we say takefashion mnist and we go find the nearestlooking bags but that's missing themetadata of uh of the underlying imagedata and so having that metadata andthen using the graph syntax to be ableto traverse it is something that i thinkis a very exciting visualization and youknow the graphql demo the semi consoleit's a very cool way to start playingaround with vector search engines andsomething that i'm also working on islike a tutorial on using graphql butgenerally it's pretty easy to getstarted and wevg8 has at least 15examples on the documentation for anyoneout there looking to get started withgraphql and and again wikipedia uhwikidata and also news articles are upthere already if you wanted to startplaying around with uh graphql so tobacktrack a little bit you were talkingabout uh text embeddings and graphembeddings and this idea that you woulduse the say average of a paragraph aboutand lakes in greece to get a textembedding and then go hit the semanticwebgraph and i was recently talking tocharles pierce akinius about how they uhhow they have thisscientific literature suggestion whereyou highlight some text and then it goesinto the knowledge graph of a citationnetwork so is that the vision that youstart off with the text embedding andthe text embedding takes you into yourposition of the graph and from thereit's about traversing a knowledge graphyeah so so the original idea was alittle bit more abstract so the originalidea really was likehow can i target this data objectin in the graphwithout knowing what's in it how can isolve that and that is what theembedding sold for meum that can be text embeddings but thatcan also be as you mentioned imageembeddings and those kind of thingswhat's also important to bear in mind isthat i i came from from somewhere elseas where for example data scientistscome from so a data scientist might comefrom hey this is really cool we can canbuild a recommendation engine so i'mgoing to take mnist etci really came from the from theengineering slash database perspectiveso like how can we have build a databasethat is just smarter air quotes in umstoring data so it was a completelydifferent approach and that is forexample why i love the case of of uhcharles and kenya so much becausethat is that is the that idea in actionso they're using that exactly so youhave a database that makes it easier forthem to search for the um into thecontent so that's it's a little bit of adifferentuh point of view and the embeddings arejust and i have to be honest here we'rejust also a little bit lucky right sowith with everything that's happening inresearch because we're now often talkingabout uhimage or text embeddings but um we'relooking at use cases for cyber securityembeddings and healthcare embeddings youname it and you can store all theseembeddings including the meta that arein in wi-fiso i really think that the coming onetwo years we're gonna see so many cooluse cases from stuff that people arecreating embeddings off so that that'sgoing to be yeah that's just going to beamazing yeah and we'll definitely getmore into all these different use casesand the multimodal andpersonally i'm working with uh thewe're working with the memorialhealthcare system data where we haveconvalescent plasma therapy treatmentsfor cover 19 and we're trying to do thiskind of vector search where we'relooking for patients and it comes backto this topic that we're talking aboutwhere you have an underlying symbolicschema and this kind of idea of neuralsearch and symbolic search and the waythat you really want to do neural searchso a lot of the times and um charles andkenius outline this really well withthis concept serendipity and i hadn'theard this term before and i think thisreally describes it if you do a textembedding the result that you get mightkind of confuse you compared to aregular expression matching so if yousearch for keywordsor like whatever search you do you'regoing to expect the keywords to be inthe result kind of like it's just kindof i think how we're biased to usesearch engines given google and just ourexperience with this technology ofsearch engines so i think we're stillgenerally getting used to this neuralsearch and symbolic search so wev8 is aneurosymbolic system i think one of thebest one of the you know most applicableneurosymbolic systems that have come toexist so is this something that uh thisneurosymbolic kind of categorization doyou think this is something thatmotivates and trading off some symbolicqueries as well as this neural searchand how do you think about those twokinds of searches yeah so so um uh firstof all i i love that example because i'mdealing with that as well so so just forsome context so what am i doing likemostly on a daily basis i'mhelping users and talking to users likehow do you use it and one of the thingsthe patterns that i see is that ofcourse people have like a status quosearch engine that they use and thenthey move to something newand indeed keyword search is very binaryin its results right it's true or it'sfalse so if you search for apple andit's it shows you an article about uhtoothpaste then you go like ah that'sjust wrong it's a you can like in abinary face like this there's somethingwrong with this thing because i searchfor apple and i get toothpastethat's of course different when we'redealing with uma vector search engine because now wejust search for something in the in thevicinity of something andthe interesting thing was i was talkingto two researchers doing a lot of ofwork in this space and ilike a year ago or something i said likehow actually how do we knowand he says like well intuition right soif i if i type inlandmarks in paris and i happen to havea document about the eiffel tower and itreturns the eiffel tower or then it'sour human intuition that says like ohyeahthat that makes that makes sense soone of the things that we're playingwithis that we're saying like well we arereturning of course in the results alsothe what we like to call certainty whichis a little bit of an algorithm it'svery simple algorithm like how todetermine based on the distanceyou can also return the distance we'realso going to work with with differentdistance metrics and then you can saysomething about for example cut offpoints in the distances where it'sprobably a relevant umresult so that's that's the first partof my answer the second part of myanswer about keyword search that willdefinitelyplay a role i have a um an example onthe umin the in the in the demo data set inthe wikipedia demo data setum where we deal with named entities soi i often like to takemusic examples so i i believe that thequestion that i have there in the demothat it says like what was michaelbrecker's first saxophonewell if you run that query you will getthe answer but it's like on the third orthe fourth result and there's just oneright answer so what you can do is youcan mix in the where filter on the endthe named entity that's in theremichael brecker in this case so youstill have the semantic uhquery what was microbreakers forsaxophone but then you're only sayingthe where filter this must be related tothe keyword match michael brecker andthen you still get the right result sowhat we see happening a lot is thatpeople start to mix in these namedentity recognition things where theysaid okay if the query contains anamed entity we're going to do a ascalar search or just a bare filtersearch in our case on that named entityand the rest of the question we're goingto matchbased on the on the semantics of thequestion really depends on the use casethe third thing that i want to say aboutthat that is about that word serendipitybecause i love that that word as wellumwhat we see from uh from that point ofview is that people come up with a lotof very cool creative ideas to actuallyuse the serendipity to their benefitan example is thissowhat i did years back when i tried thisout with creating centroids on theindividual words we now see people dothis again for example in e-commerceso let's say you have 100k productsand they all have individual factoryrepresentations that they got forexample from a sentence uh birth umembeddingso these they have these embeddings ohsorry model so they have the sentencebird embeddings to them and now somebodyadds something to that card so threeproducts to the card what you canactually do is in with yet you can sayi'm gonna take the three embeddings ofthis product in that cartand i'm gonna represent the userbased on the centroid of these threeembeddingsso now if somebody searches forsomething else you can bias the resulttowards the centroid of what's in theirshopping cartso if somebody has so we weare about to release an e-commerce demobased on this so if somebody has like umall kinds of sporting wear for like awomen's sporting wear in there and thensomebody searches for a jacketthen you can actually buy his results tofor example women jackets orif somebody has a lot of things relatedto yoga in there and somebody and thenyou search for example for bowlthen you can actually show yoga bowlsfirstbefore showing soccer balls and thosekind of thingssothat is an example where thatserendipity is actually used to improvethe results and the umand create better search engines forwhatever use case people have yeahthat's such an interesting uh likeanother way of looking at theserendipity thing is thebiasing it with the nearest neighbor tofilter the search and i think generallyfor everyone listening i'm sure a lot ofour listeners have heard about this yetbut what makes a part of these vectorsearch engines so different from sayjust running your data through yourneural network and then taking theembeddings and putting them into memoryand doing some comparison is there arethese uh data structures like thecentroids bob is describing that speedup these searches so h and sw is a graphthat traverses these centroids to helpyou find nearest neighbor vectors so youdon't do order n compute that you knowgo through the entire vector set tocompute the distance with everyone whichwould be impossible for massivedocuments and all these kinds of thingsso that's a really interesting idea theserendipity of you bias it you say iwant something that's kind of similar tothis and then the h sw graph can be likeokay great i've already built up thisindex that already helps you do thatkind of filteringso then um coming back yeah the idea ofthe the human intuition that helps usinterpret these results and so anotherkind of topic and i love the idea ofthese user interfaces for humanintuition things like as you mentionedthe certainty that gets returned at theneural search is i've personallyuploaded my keras data set into eva andi've been playing around with searchingfor nearby code snippets in the kerascode examples and having that certaintythat matches it and tells you it's 72percent or 0.72as you go down and you see the certaintythat that really does improve the userinterface and it helps you feel morelike okay i i'm getting more of a senseof how this is working and yeah thenamed entity tagging i love that ideabecause you can um you can have like thekeyword filtering onto come back to the keras bird examplepeople out there are familiar with chaosit's a library for deep learningyou might want to have it tag entitieslike use the attention layer if you'researching for implementations of newtransformers so that attention layer islike a named entityuh layer of the kerasso then kind of a big a big topic tocome out of and then start talking aboutis going from human intuition tocomputer intuition and this idea ofinformation retrieval and supervisedlearning so i think you were reallyahead of this and thinking about thisearly on with the just the modularityand the building in of the questionanswering modules so what was yourthinking early on with the modules andhow are you thinking generally aboutthis kind of adding the supervisedlearning modules functionality on top ofvector search yeah so that's a uh umthat's a great question thank youbut umso one of the things that i was doing isthat i knew from the work that i wasdoing back then so basically helpingpeople to build systems for whateverthey were trying to solvewas that i knew that if i if we would beworking on a on a database that as ialways like to describe is the distancebetween the problem that the databasesolves and the problem that the end userneeds to have solved there's a gapbetween there becausea very simplistic example isumpeople don't directly connect their appor their front end to weave it rightthere's always a piece of middlewarethat does something for their for theiruse case like you do with any databasethat's not different for but weappreciate itso but i tried to what i thought aboutlike so what can we doto umbring make bridge that gap right so howcan we bridge that gap what kind ofthings can we release what kind ofthings can we make to that and themodules are an example of that so thenwe said like well you know if themajority of people are using for examplesentence transformerswhy not have a modular infrastructure toactually umjustoffer that to people andlikewise when it comes to the forexample the hsw plugin which had thishere the the whole the credits go to uhto hn for thatidea butprobably we will see others in thefuture as well besides hnsw so let's mayalready be preparedthen we can just you know change it andswap something else in so it's more likefrom the from the inside out and and i'mlooking from the outside in 2d to the toev8 and that's where the idea comes fromand now the cool thing is the connor andthat is um well i can use the wordserendipity againum that we have one like power userwho's like hey wait a second wait asecond we have like hundreds of modelsthat we usecan we actually automatically in thekubernetes cluster spin up and shut downthese modules around we evade wheneverwe need them and we're like yeah sureyou can so now we see these new usecases where people start to intertwinethe ml stuff with the devops stuff andcreate this super huge systems wherethese models pop up shut down and whathave you all thanks to the modularinfrastructure that we have so i thinkthat's aumyeah that's just amazing to see and andagain to recap this the goal here is tobridge the gap from the database towhatever the problem the end user istrying to solve yeah and that modularityconnects to sort of thechampion open source the hugging facemodels and it's such a nice modularintegrationand yeah you can just switch out the themodules if you have a question answeringsystem that's going to be uh performingthe task you can just swipe it out withhugging face and hugging face is reallygreat to upload your models to just apersonal testimony to it it's it'sreally nice to train like a languagemodel push it to the model face modelhub and then you can have the mask demonow they're building outhugging face spaces they've acquiredgradio they're really building this demointerface and you can take all thesemodels and plug them into wev8 and themodularity of that is so exciting i lovethe modularity of how you canswipe out all these different componentsi agree i agree with you and what yousee is like not only this like because iabsolutely am so the hugging faceplatform is fantastic and and and we webenefit from that as well so that'sgreat but let's also not forget likeplatforms like docker hub for exampleright so every time somebody downloadsum uh we're slowly approaching 600kdownloads by the way but uhthat's that's thanks to the dockerplatform so so so hugging face docker uhkubernetes of course we we benefit fromthese from these platforms so that isthat's just fantastic that they existand yes and to come back to ourinformation retrieval and supervisedlearning discussion too i personally oneof the systems that first caught myattention was co-search from salesforceresearch and this was around uh you knowthis was during peak kind of covet 19hysteria where we're trying to look foralso whatever application deep learningcan be used for and one of the mostinteresting ones was scientificliterature mining and so there was a labi think from johns hopkins and otherpeople they tried to build a labeledquestion answering data set but theywere only able to get like 124 pairs and23 hours of late of their work quicklyyou know this rapid scramble kind ofthing but so they weren't really able tobuild a big data set that we were usedto using with deep learning so we'relooking for these information retrievalcomponents and co-search was this systemthat said here's our vector index here'show it flows into a re-ranker questionanswering abstractive summarization allthese different tasks and you see howall these different components kind offlow through one another and build thiskind of complex system where themodularity of being able to replacethese different components is sointeresting and so we've seen also somereally exciting trends in informationretrieval like super charging supervisedlearning i think most recently wellwe'll talk about the most recent one butdeepmind's retro modelis an exciting example where theyrelease two language models this gopher280 billion parameter model and everyoneyou know says 280 billion parameters wowthen they're also saying here's retro ifyou add the information retrieval it'sbetter because it has thehas the context betterthat makes a lot of sense to have thecontext so then kind of coming into themost recent thing and i thinktransitions really well to weaviate andthis idea of computer intuition as wetalk about how you use these kinds ofsearch engines and then how machines usethese search engines it to me it startedoff the first paper i became familiarwith was facebook ai researchers had apaper titled internet augmentedgeneration where they're querying thebing search apitoto do the information retrieval so it'snot clearing a vector search index itqueries the bing search api and nowtoday or yesterday we have open ai's webgpt where the same idea you query thebing search apibut similar to what we're talking aboutwith computer intuition and how you usesearch engines they're usingreinforcement learning to train themodel on how to interpret the results ofthe bing search apiso what we've a lets us do is we haveour neurosymbolic search and againcoming back to this idea of serendipitysymbolic filters traversing graphs themodels can learn how they're going toreally query thissearch api so they can um you know dothese things like question answering andthen learn how to use their informationretrieval component which is somethingthat i see as being such a powerfulsupercharger of this yeah no i i agreeand i think so what's interesting aboutwhat you're saying um right there isthat from a from a research perspectivei completely get this right because youtry to solve a problem and then there'sthe bing apito give you these results but if we lookat one there was also one of the eyeopeners that i had that was likeumlet's if we would have if it would livein a world where all data would bepublicly availablethen there would be no room forexistence for something like we've hitbecause you just go to bing or to googleand you find anything rightso the idea is that i that i had waslike so how much data are we talkingabout actually so how much data isbehind closed doors and how much data isactually publicly availableand umyou know microsoft and google are quiteopen about how much data they have intheir search engines and then if youlook at estimations about how much datathere's there actually is in the worldyou get to a number of like 0.0001of data that google actually has indexedi mean i might be a adecimal point off there but the point isit's far less than one percent so thatwe thought like hey wait a second so youhave the the data you have the modelsbut you also need that search engine ifyou want to apply this to your own dataso if you're like if you're a bank ifyou're an insurance company if you havea startup with with which that'sgenerating their own dataandand that is what plays so well togetherwith these existing models that aretrained on this umthe information that's publiclyavailable andi think one of the one of the thingsthat i often am in discussion about withwith peopleis the whole concept of fine-tuningright soand that i sayi i understand the academ the academicargument for saying like in theory forevery case you need to have a fine-tunedmodeli get that argument but in practice howpeople use itthese general purpose models alreadybring them the results that they needandand i think that based on these examplesthat you just gaveconor i think we will see more and moreof these general purpose models as ilike to call them being used by more andmore people because they are alreadygiving them the results that they needeven if they are trained onstuff or getting to results that comefrom the bing api or whatever they arecoming from right orthe common crawl or those kind of thingssoum i think the point i want to make isthat i think like the thetrend that i think that i see in theusage of these modelsis that because these models become somuch better over timeusing them for general purpose use casesbecomes easier and easier as well so youneed to do less you don't you don't needa phd anymore basically to umuse such a model for your for yourbusiness or whatever youyeah i think okay so i'm gonna push alittle back on the fi on the abandonedfine-tuning idea and i i thought youwould i thought you wouldbring it on yeah so uh so i'll gothrough a list of citations and thefirst of which is uh don't stoppre-training it was one of the acl 2020best paper rewards or it says you knowbasically they set up their data set ofyou have news articles imdb moviereviews biomedical papers and computerscience papers and they showed how prehow continuing the pre-training for thedomain of interest is extremely usefulbuti think and and i do agree with thisidea that foundation model these biggermodels on more data are more generallyuseful but so i think the next paperthat we can kind of learn from is ithink it's the allen institutewhy is ft i've made a paper a videoexplaining this paper on my youtubechannel if interested and what it is isit shows this relationship between theselarge pre-trained models and then thesefine-tuned models with respect to indistribution accuracy and out ofdistribution generalization andflexibility so if you fine-tune themodel basically and the algorithm isthat you fine-tune the model and thenyou have a weight space ensemble alongthe path from the fine-tuned model fromthe original starting pointso i think that thatis going to be a very successful way totrade off the out of distribution thegeneralization the flexibility of theselarge pre-trained models but then youalso want to have you know yourin-distribution kind of fine-tunednature and i i guess generally as a phdstudent it does kind of break my brainthe idea that some kind of general modelcould be better than you know fine tuneon a particular data distribution thatidea doesn't really make sense to me buti guess ityeah let me let me let me try to to toto pitch it to you right so what theidea is what's happening there solet's go back to that example that ithat i gave off that we were searchingforwhat was michael brecker's firstsaxophone right so with this out of thebox model based on a wikipedia data setwe do not find the answer so from fromfrom an umfrom from an academic perspective if youhave definedyour research as can i find a answer toa natural language question based onthis data set it would say no becauseit's it's not there right the answer isnot there howeverif you then take off yourscience hat and you put on yourengineering hat you can say like wellwait a secondthe answer was actually in the in thetop five results it just wasn't thefirst oneso if i now run these five answers tothe q a moduleand i see what the highest stance whatthe highest certainty answer is therethen i do get the right answerso you can engineer your wayto getting the answerandwhat i'm trying to say isyes i agree that in the ideal world thatis solved on the academic levelhowever the models are already goodenoughto solve itto solve people's use cases so thequestions that people have so i agreewith your remarkyou're absolutely right but i'm i'mlooking at it from a differentperspective i'm looking at from theperspective of is the model already goodenoughto solvethe problem that a that this person hasand the reason why i'm talking aboutthis and i'm doing this and i'm sayingair quotes when i talk about generalpurpose modelsis that we see that if you engineer yourway out of itthese general purpose models actuallycan perform betternot on every use case of course but onnews articles products uh those kind ofthingsthen if we extra fine-tune it on theseum uhon specific data sets of coursefine-tuning is always better but that'snot always an option so i'm i'm tryingto say like what i love to see is likethat you have like the scientific sideand the engineering side and they gohand in hand and they help each other toto have these end users buildwell whatever they want to build withthese kinds of databases and modelscombined yeah so so my take on theengineering solution and thanks to drmohit bonsal from the university ofnorth carolina for sharing this paperwith me and helping me think about thesethings is so their recent paper is vnladapter so we're setting things likecompactor adapter high performer theseare strategies for how we can fine tuneclip while only really tuning fourpercent of the so particularly thispaper they only need to fine tune fourpoint four percent of the parameters ofclip to still retain the fullperformance as if you fine-tune thiswhole model so from the engineeringperspective of that headache of tryingto store the thing and do that finetuning thing and all that kind of stuffthese compact adapter layers aredefinitely pushing that in the rightdirection but then to kind of come backto one thing that i was curious about onthis idea when uh you mentioned thatlike the the answer isn't in the dataset the foundation model was say trainedon what do you think about saysay your goal is um is the we vvadocumentation right so you're training aquestion answering system on the we vadocumentationthen i think you need some fine tuningright to navigate the webviewdocumentation because it hasn't seenthat data beforeso yes yes and no so so yes it alwaysmakes the results better but againthat's so it it we need to be clearabout what we define what the rightanswer is soi define the right answer it's likecan we algorithmically get to the rightanswer in a decent amount of timewhich is different thandoes the model directly produce theright results and um so if you if you gofor the latter one then the answer isyes you must fine tune itif you go for the pre for the for thefirst one then you don't necessarilyhave to fine tune itit will be it will yield better resultsbut you don't necessarilyhave to do this andi know that i'm i know that i'm tryingthat i'm throwing a rock in the pond bysaying this but thethe the the point that i want to make isthatis actually a very optimistic point isthatthese models are already so goodthat they are helping a lot of peoplealready andyes making them betterwill bet everybody will benefit fromthis but i let me let me put it likethis sothe the other day so a metaphor for thisright so the other day i was um i wasmaking risotto and i have like certainingredients that i have in when i make arisotto so i was getting the stuff and iwas getting and then i wanted to have aspecific type of cheeseso i so i you know i mean in store and iwant to buy the cheese and guys like ahwe're out of this cheeseso now the question is like so from afrom a umuhair quotes academic perspective i can'tmake my risotto anymore because i needto have this cheese to go in the risottobut if not put on my engineering it'slike so what do you do you have likesimilar cheeses and i got like yeah youknow if you use this kind of wine youcan use this kind of cheese as wellso i couldn't make my risotto as ioriginally intendedbut from an engineering perspective ijust used the different cheese and thepeople who ate the risotto still werehappy with the risotto that i came upwith andthat is what i meant is that i am veryinterestedin how can we get these models and thethings they can do with them to thedatabaseinto the hands of as many people aspossibleandpart of that comes with the engineeringaround it and saying like hey thesemodels are so sentence birds for exampleso many people are benefiting from thatwithout touching the model once becausethey don't know how to do it right sothey go like i want to build this coolapp i want to build a cool uhsolution i'm going to get a website orwhateverand i just want to use that model andthey go like wow it works so all thekudos to the team that that worked on onsentence board but i again i want tomake this optimisticum[Music]your point then say like so it's alreadybringing so much value to peopleeven though from an academic perspectiveit can still be betteryeah i think yeah i think we'redefinitely on the same page with the twodifferent kinds of like mostapplications yeah like right out of thebox the sentence bird is so great andit's so interesting if you have thathuman intuition and then on the academicside as we you know look at things likewhat deep mind is doing what openai isdoing and our creativity goes crazy andwe think you know computer intuition thecomputers are going to be able to do allthese tasks and we get so excited aboutthat idea and then i think we thinkabout two different ways of using we vvawhere on the left hand for where youhave some use case and you have thehuman intuition already you just the endto end system is good whereas i thinkthe other side you're looking at thepython client the api and how you canput that sorry how you can put that intoyour into your training workflows and doresearch with it because it's a reallyexciting uh research platform we've itis like a product and it's like ascience platform the science platform isreally what excites me a lot as mycoming from my background and so i thinkthe transition let's talk about thedemos that we have and the apis that areavailable for the demos and how thisenables this kind of research so so withthe demos we have uh wikipedia andwikidata and you can you talk about yourconversations with wikidata and howexciting that's all been yeah that isthat is fantastic because the sothere was like so the wiki data it'sactually funny story because umi found this repofromfacebook research or meta researchone of the twoumwhere they had this this big graphdata set that they trained onwikidata andso one of the things that i constantlydo is that if people like something thatthey're seeing i want to know why so iasked them why do you like thisso what we noticed was that we did ademo data set with wikipedia data andwith wikidata sothe wikipedia one was just interestingfor people to see this at scale theythey love it they mostly engineers or asi like to call them tech savvy productmanagers that play around with the apiand you're like oh this is great i wantthis in my solution as wellbut but um the b graph one wasn't wasanother one so somebody was very umenthusiastic about this and so i saidlike whyand umthis person told me the following hesaidwell he said so we dowe do the researchwe generate the model and now we havemany gigabytes of vectors and umthe end the wiki entities related to itand we gotta kind of assume that itworksbecause how are we gonna test that inpractice are we gonna gonna load allthese things in memory in a pythonscript and search for it on our laptopsno we don't because it just it's notpractical he said and what you did withwe've yet it's just you were actuallyshowing in action what how it works wecould actually try out this model thatwe that we were so um enthusiastic aboutand that was such an eye opener for mebecauseumthat stepthis proof that step that i always saylike you need the in this newml first worlds right so you need thedata you need the models but you alsoneed the database to search through itandwe got so much positive uh feedback fromfrom the folks even on wikidata we werelike the tool of the weekand and then you go like oh this isamazing because people are actually theycan they can work with it they can playaround with that and we're definitelygonna do more work on making that dataset better we're just going to keep thatcompletely open source for people toplay around with it i even already knowabout people who reached out to me thatarechanging it they forked it and they'rechanging it to build it even in theirown solutions and our own products and iwas like yes that's exactly what we wantand i hope that that early next year ican share some more cool things aboutthis as well yeah it's so cool seeingthe graph embeddings and really lookingthrough graph embeddingswikidata is probably the best examplemaybe like if we constructed a twittergraph where it's people who follow youand then your nearest twitter neighboror something like that would be anotherway of seeing these graph embeddings inaction because when you pointed me inthe direction of pytorch big graph thatwas really the first time i'd reallylooked at uh pi source big graph do youhave any other kind of like examples ofgraph embeddings that inspire yourexcitement about these kinds of ideaslike wikidata graph embeddings yeah so iso sometimes the ideas come from datasets that i see sometimes the ideas comefrom the models that i seeso i was immediately intrigued by amodel which is called cyborgs and cyborgis it factorizes cyber securitylogsand bear in mind so what the problemthere is so the problem is um that ifsomething comes from like a weird ipaddress you can immediately block itbut if something else is happening thatyou can't solve algorithmically and ahuman needs to look at it that's anopportunity to do something with theseembeddings so people started to createembeddings based on cyber security logsand i was like oh boy that's amazingbecause now you can store all these logsin the weave gate and if new logs comein comes in you can do a similaritysearch onthreads from the past and say like isthis somehow related to each other sothat's just just one ideaanother idea that i had was thetremendous amount of work that i seehappening for example gene2vec and thosekind of thingsyou can easily store a human genome inweaviate with its vector representationsand search through iti would love to do some work there aswell so essentially we have genes wehavesame goes for proteinsbut also customers right so you have acustomer that behaves in a certain wayum you can create an embedding based onthese graph embeddings for this customerso now you get all your customers andalleviate and say like how are theyrelated to each otherthe the graph relations that people makeare amazing so for exampleum i saw an example about likehotels andpeopleso where you can say okay i have um ihave like connor and i have bob rightbob booked hotelsnow we get likejohn doe inand john doebooks one hotel or behaves in a certainway so the embedding that's created forjohn doe might gravitate towards youso then you know okay let's also showthe hotels or the types of hotels thatconner is interested in opposed to theones that bubba is interested in becausethis person behaves in a similar waythat conor is doing and this can go veryfar this can we're not only talkingabout the hotels that people booked buteven how they search for the websitewhere they where they click on and thosekind of thingsum and what we also see and now i go alittle bit away from the embeddings butsomething where the embeddings are usedis that there's this whole thinghappening with privacy and like a postcookie worldhere embeddings help as well because nowi can create an you can create anembedding about somebody without knowingwho this person is so it's very privacysafe if you will but you can still givea great user experience for whatevertool you are creating based on theembedding that you are creating for thisuser soit's just there's so much happeningthat's just fantastic yeah graphstructure data representations is suchan exciting idea and uh shout out tozach jost at uh welcome ai overlords ishis youtube channel and uh he's workingon this problem of like fraud detectioncyber security and how you can make it agraph structured problem reallyinteresting idea and all sorts ofcontent about graph neural networksgraph data is kind of themost natural way to represent some kindof data and yeah biology the proteinprotein interaction networks the drugtargetdrug repurposing being like the bigapplication sort of to look atall these kind of graph structure datais so interesting when i first was uhlooking at graph structured data i wasthinking in this context of like systemone system two kind of thinking backwhen that um when that book thinkingfast and slow came out and that was kindof an exciting idea in deep learningresearch and so i always thought ofgraph structure data as being like thesystem two component and kind of likecausal inference like the way that weyou know if we talk about informationretrieval and then supervised learningbeing like system one is informationretrieval system two is uh supervisedlearning reasoning say i always thoughtgraph structured representations wouldbe limited to that second part but nowi'm seeing more howgraph structure can help you build theembeddings of the data as well as sortof like raw text representations so onekind of question and you knowwe can just pass this if it if youhaven't thought about this but i wasthinking about like how do we combinethe wikidata graph structured embeddingswith the wikipedia text embeddings howdo those two things play together ohactually i love that question because ido have some thoughts about this soone of the things that we have withinwev8 and that's on purpose is that youdon't choose a modelto attach to a weave yet noyou choose a model that you attach to aclass that's in your weave yet so youcould create a weavieate that has likethe paragraphs from wikipedia with thesentence[Music]burton bettingsbut you can have another clause in therethat represents the wiki dataand that you say like okay we're goingto use the the big graphembeddings to store that in the sameweave yetnow now you might say like well but theyare completely different vector spacesthat is true that is but what you can dofor example is thatlet's say that you somehow start fromthe perspective of the wikidatathe big graph uh embeddings and you youtraverse the graph and you end up withstanley kubrick as i always gives anexample and now you say okay i want tonow know more about this director thenyou can just do one simple query and sayokay i found thiswikidata node in the graph now jumpto the most similara note in the in the part of thewikipedia embedding in the sentence andburnings and that is they are notclosely related from a from aembedding's perspective because thereare different embeddings but you can dothe query so you can say oh i know fromthe wiki data that it's a director withthe name danny kubrick and then you cando the semantic query that's handled bythe sentence birth model to find itthere so you can jump back and forth inthat graph and that's really what'ssolved in the database so umi wouldi would even like there to go a stepfurther i think that a year from nowwe're gonna see more and more use casewhere people have likefour or five modelsjust zooming in harmony in in one we'vehit instance yeah wow that that's reallyexciting and i think from here we'lltransition to multi-modal learning andgetting into this idea of four or fivevector spaces and combining the vectorspaces but just quickly before wetransition that i also wanted to justagain like there's wikipedia wikidataand i think another problem that mightmotivate listeners into doing thisresearch is scientific literature miningwhere you have citation networks andthen you have say the text embeddings ofthe papers i think that's anothervery inspiring kind of applicationdomain i see so many scientificliterature mining tools out there andpeople tackling this problem because ithink it's a really fun way to thinkabout it and build a research careeraroundthese kinds of problems so let's talkabout multimodal learning and you knowwe're talking about combining vectorspaces with queries with class propertyreference class references we're talkingabout the graphql kind of structurebetween different things but let's justkind of start with like the image textand the idea of the clip in the v 1.9from wev8 and how we do image textsearch and connected to all these kindof exciting ideas yeah so they're likethere there are two thingsum that i think are that are interestingto to say about this so so the first isumusing multiple models like for exampleclip and and and sentence embeddings inone we've gate i i will talk about it ina bit but the second thing isthat i wouldn't be surprised if at somepoint and this is out of my realm ofexpertise but i would not besurprised if at some point somebodysteps up and says okay wait a secondwe can make relationsusing v8 as the search engine betweentwo completely different models so forexample the the the big graph model andthe sentence bet embeddings sends birdembeddings for the wikidata i can saysomething about their distances byhaving a natural language query inbetweeni'm gonna train a new model that isgonna try to predict these relations iwould not be surprised if we see thathappening and if anybody's listening tothis podcast who wants to do a uhsome kind of a research project aroundthat then they should reach out to mebut to the first thing what we see a lotis this sosometimes you have use caseswhereum[Music]you somehow want to mix image search andtext search but alsodifferent types of model search into oneso so we see people create multiplegraphql queriesand then you have something which welike to call the the business logiclayer or the middleware layer where theydo something with these results thatthey're getting backbecause um umsometimes somebody uhmight uh uh let's say you have an ane-commerce uh solution somebody types inin natural language what they'researching forbut on the page that is being presentedto the end user you might want to havenatural language results so there theproducts come to the sentence embeddingsbut maybe you want to have an overviewof three products that aresimilar to the image of the first resultorimages that are being returned based onthe natural language input so then whathappens is you have like a e-commercefront-end somebody types in awhatever they're searching for in wev88we get do a sentence embedding searchand a clip searchthe sentence embedding search returnsnatural language results so that can beproduct descriptions reviewsbut the clip results return images andthen people do super cool creativethings in actually showing that to endusersand how they can umfind the products that they're lookingfor so umthat's a little bit different to yourquestion about like um uh uh thosemulti-modals but what i meant is morethatit sparks thea clip for example gives people thecapabilityto now go from a natural language to theimage something they couldn't do beforeand they built very cool things withthat and so the more of these types ofmodels we seei don't know what people will come upwith but the you know going from doingsomething with video or with audio orthose kind of thingsour users benefit from that againbecause they get they get morecapabilities toget to potential results in these searchresults so that serendipity that wetalked about earlier to present that fortheir end users and use these models inin in practice soum it's not a 100 answer to yourquestion but it's like it's more like anexample how thesenew models like clip are being used andwhat we see people build with them yeahand it's so exciting like um how itreally enables that kind of like turningsay instagram into a uh like a standardkind of e-commerce platform like if youimagine the way that say like i shop fornike basketball shoes i just look atlike the standard shot where they'relike centered in the white backgroundand they're like right in the middleimagine to like wanting to see you likewhat do they look like on the court likewhat does some players wearing them looklike and that kind of search and doingthat kind of nearest neighbor where youtake this little static picture of themin the shoe box and then you go see whatdoes it look like in action so to sayand that is a super sorry for quicklyinterrupting but i love this examplebecause that is exactly the examplewhere you have multiple models in thedatabasezooming you know and just constantlycreating these vectors but that allowsyou to create these kinds ofapplications and exactly the examplethat you just gavewas hard to achieveif not impossible without clip but nowwith clip you can do these kind ofthings um on the fly so that's nice yeahand clip this has to be in like the top10breakthroughs it's such an exciting kindof thing it really definitely isyeah so i agreeyeah and then so just like two otherthings i wanted to just touch on it'skind of like just image nearest neighborsearch but also with the metadataannotation and coming back toneurosymbolic search where you knowwe're talking about text neurosymbolicsearch which is kind of obvious becauseyou put the regular expression so to sayand it's in the original data point it'snot really like metadata compared to saywhen you have an image and then you havethe metadata so to give an example ofwhat i'm working with with the kova 19convalescent plasma therapy thing that imentioned earlier we have chest x-raysand we want to it's kind of like thisidea of precision medicine enabled withvector search where we want to try tofind like the nearest patient who had asimilar experience to you and kind ofwhat treatment worked for them andsearch through these large largedatabases and see you know how we cancustomize your treatment so to say sowhen we're searching for nearest chestx-rays maybe that's something we'reinterested in we could add the metadatalike whether you're a cancer patient aswell whether you had some kind ofpre-existing heart condition and thatkind of information would be reallyuseful for interpreting these kinds ofmedical images and that kind of ideaanother interesting idea is this newdata set that's being led it's ton oftons of researchers i think stanfordberkeley i don't know the whole listbecause it's a very long it's one ofthese favorites with like 20 30 authorson itbut the data set is called wilds and sowhat wilds is it's about annotating datafor the sake of measuring domaingeneralizationand so we have this kind of additionalmetadata annotations on our image datasets our molecule data sets our textdata sets and this metadata that you canintegrate through this graphql interfacethat's what i think really brings homeneurosymbolic search and that and that'skind of one of the things that reallyhas excited me so much about this andhelped me keep my interest on thisparticular idea so i think we've youknow covered so many topics and aboutlike the technical ideas and thesevisions for the future but i think a lotof people out there would be curiousabout like your journey as ayou know ceo co-founder of wev8 and youknow recently featured in techcrunch andkind of like so what's your experiencelike with building this out as a companyand your vision for the future on howthis kind ofgame is played so to say oh where tostart answering this question so the uhso so first of all it's likethe joy that you have if you see likedownloads going up positive feedbackthat you're getting that is fantasticand what you said about techcrunch thingetc that's justthat's just amazing and that just isproof of showing like hey there'sactually stuff happening in the in theworld people the mindset of people ismoving towards using these vectors in inin production andone of the things that is my personal umgoal is tohave these things likefor example the research that is that ishappening so that's that knowledge thatpeople like for example like you havethe engineering that is happening that'sthe knowledge that for example h-n hasthe the user experience part of it sothat's for example knowledge that larahasmy role is combining these thingstogether and figuring out how that helpspeople on a day-to-day basis right sothe example that you gave about the uhthe the the medical use case right withthe x-rays we we have we we knowunfortunately i can't say much about ityet but we have these kind of cases aswellthe most beautiful thing is if you sitwith a doctor and you show the firstresultsand this person was does have is likezero knowledge about how it works justlike this looks great i see where thisis going and i like it very much whati'm seeingthat is like in the in the journeythat's the mostum that's the most beautiful thing andevery week i'm uncovering more and moreof these cases so that is one of thethings that i'm very excited about likei keep just being surprised how big theopportunity is there's like i haven'treached the limit yet so that that'sand the third thing is likesome people who have looked at uh we'vehate might have seen some hints about italreadyand that is if you find a note in theweavier graph it has its ownscheme created to it so it's like yousee we've ate colon slashlocalhost slashand then you find the node in the graphwell the fact that it has localhostthereassumes that in the future it can maybebe something else than localhost so thenit can be somewhere else over theinternet andone of the things that is my personalbig dream with this if this keepsgrowingis that the big problem that we talkedabout earlier about the semantic web andhow do we connect these things togetheris that you can actually say well let'ssay that i havea dozenor ten dozen you know we've yetspread over the worldand i just spread a query over thenetworkand i let these models solve the answerand return the resultsso very simple example let's say youhave a vvh from the new york times andyou have a webview from the wall streetjournal and somebody says like okaywhat's happening now with covetyou shoot it over the network and bothof these vp8 instances return theresults and you can in graphql representthem to the end user so you get like aas i like to call it the knowledgenetwork and that is something that i'mreally excited about about these kind ofthings as well it doesn't exist yet butif you look at we've hit you see all theingredients already being there which isjustbe allowing people to connect data setstogether to connectknowledge together to connect insightstogether outside of one instance andthat is somethingthat i'm also super excited about sothat that would be my answer i guessyeah that is an exciting vision i reallylike thatlikelike with um hugging face spaces and youknow big thanks to merv and omar forhelping me with my personal experienceof setting up my kerasper with henry aion hugging face bases is yeah they'rebuilding up all these demos of deeplearning models and it's all in onecentral thing and imagine putting allthe demos of we've yate and then youbuild a model on top of that and that'sthat's excitingyeah and that is the beauty of course ofthe whole open source nature of it so ihad the hugging face has that we havethat it's likeit allows people to work together andbuild these amazing things and back toyour question about like what my role mythe only thing that i need to do isactually just make sure that thesethings resonate in harmony that'sbasically that's what i doso other people do the hard stuff i justi just try to make them resonate inharmony and um well some people mightargue that but that's anotherfor another podcast but uhumuh and that is just amazing to see howthat works and how enthusiastic peopleget about it and and that's just ii wouldn't be surprised if this goes x10 next year right so that's just uhthat's just fantastic yeah it's amazingi mean like you're definitely avisionary you see these things ahead andi think that's really exciting to belearning from you about that and yeahthat idea just then was so exciting tome and all these things so so thanks somuch bob i'm sure we'll do you know moreof these podcasts on our webview semitechnology podcast i think this is areally great one and thanks again fordoing this with menow thanks also for having thisconversation with me and as always it'salways great to talk to you connor sothank you so muchyou", "type": "Video", "name": "Weaviate Podcast #3: Vector search use cases, GraphQL API UX, multimodal models, and more...", "path": "", "link": "https://www.youtube.com/watch?v=8wdUREOzi2M", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}