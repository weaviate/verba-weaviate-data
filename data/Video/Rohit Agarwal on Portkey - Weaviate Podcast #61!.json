{"text": "Hey everyone! Thank you so much for watching the 61st episode of the Weaviate Podcast! I am beyond excited to publish this one ... \nhey everyone thank you so much forwatching the weba podcast I'm superexcited to welcome Rohit Agarwal fromPort key portkey is a super excitingcompany making it easier to use llmserve them in production and routing todifferent LMS saving costs all sorts ofinteresting details that we're about todive into firstly Rohit thank you somuch for joining the podcastexcited to be here Connor lookingforward to the discussionawesome so could we kick things off withkind of the founding vision of Port keysort of the problem that you you knowset after to tackleabsolutely so I think when building llmapplications will last three years or soat multiple companies and as we builtout those applications we realized thatbuilding out the first version is supereasy you can do it as part of ahackathon and you get really excitedbecause it's been 48 hours you've builtsomething that's worthwhile it'svaluable and llms and Vector DBS arelike really powerful tools that you aredisposing so you build it out and yeahyou're now excited to show it to theworld uh and the minute you start doingthat or getting to production yourealize that a lot of the engineeringchallenges that were solved in thetraditional engineering world areprobably not solved in the llnengineering world so think about uhyou're so used to using data DOC forlogging but how do you put these largechunks of strings in data dog it doesn'twork similarly Vector databases behave alittle different than regular databasesso how you're monitoring them how do youmake sure that your app stays reliablesecure compliant a lot of these things Ithink the gold layers of devops have notbeen built for the llm worldyou know an example I often like to takeis for every other API you get a successor an error so it's very deterministicbut for an llm or even a vector databaseit's like it's all probabilityeverything is between zero and one soevery system sort of has to adapt tothis new reality of probabilities andnot deterministic outputs so that wassort of the background where we feltokay there needs to be an llm Opscompany uh we had written down sort of aManifesto back in December January onwhat this could look like but then westarted building this out uh in earlyMarchis so many exciting points I like theyou know data dog the monitoring thingis core topic to weave yet I want tostay like hello llm Ops versus ml Opsmaybe uh separate the two a little moreabsolutely I think and it's veryinteresting I think they're they're veryjoint to some extent I've hadconversations with people where they saythat llmops is probably a subset ofenvelopesum I would disagree a little bit I thinkthe difference core difference is thatmlops focused a lot more on serverswhile llmops is more on Services uh forML Ops you had core metrics like driftaccuracyEtc whereas these terms are probablyalmost unheard of in the llm worldunless you're building your own corefoundational model where you'll worryabout drift Etc 99 of the use casesyou're using a deployed llm model that'spre-trained and steady to use so thenyou're more worried about using the apiswhat's the latency of these apis howaccurate are these which is where youknow stuff like evaluations outputvalidationsretry metrics or if these things comeinto play versus the amylopes metrics soI thinkit's a little bit different analog in insome places it can be extremelydifferent which is where I thinkeven think of the concept of test dataand train data right for an llma majority of the companies are notdoing any testing or training at all ifyou just pick an API and just start atit so I think that's that therein liesthe big difference between ml Ops andllmopsyeah I think before we step further intomaybe almost the more like academictopics around the whole you know LMS andmachine learning I'm I'm just kind ofreally curious about like the state ofthe market kind of like you knowyesterday I was you know Eric waslooking into llama 2 from weviate andwe've we've saw how you can serve itwith replicate and there's like brev andyou know I've known about like modalbanana like kind of like serverlesswhere you wrap kind of info and justlike hug just like so many options torun models like well it's kind of thestate of that like model inference APIMarketyeah I think it's amazing right tilltill you're back or till two years backyou had to deploy everything on your ownso people would rush towards you know astage maker kind of a setup where you'redeploying your own models you're testingit out and there's so much stuff to bedone and everybody was busy for six toeight months something would come outyou'd I trade and go forward from therebut then I think with these uh inferenceAPI endpoints that came out it the gamecompletely changed now most thecompanies are very happy with I'm justgoing to deploy or let somebody elsemanage my deploymenthow they speed up their gpus how theyeffectively utilize the entire core ofthe machine is not my expertise I'm notgoing to spend time doing it but whatyou can do is use these apis for youknow solving my business problems and Ithink that's where we also see I meanMosaic posted this very interesting statright or there was a database I thinkdatabricks posted this stat that saidllms are probably their fastest growingand fastest adoption segment acrossanything that they've seen before andthat's probably becauseearlier you were constrained by you knowa lot of data science machine learningtraining testing that needed to happenbefore business Logics could beintroduced and now you're seeingbusiness logic introduced on day one andthen maybe some testing traininghappening a little Point down the linewhen you're really looking at accuracyor latency or costso I thinkwe digest a little bit from yourquestion but uhit certainly is that inference APIendpoints have made it super easy for alot more companies to utilizeTransformers as the technologyand then because of that nobody wants todeploy these foundational models anymorebut you do want that you know next levelof fine-tuning or data privacy orcompliance metrics and in those cases uhthere's a there's a variety of companiesout there that just offer these servicesat a very very affordable price I meanwe work with players like banana modelbefore and it's just so easy to workwith them and they'll say hey we alreadyhave these models pre-deployed you canfine tune with us go live and it justbecomes so easy deploying a model soit's almost like the llm world to alarge extent is getting commoditizedreally really fast wherein I don't needto manage all of these gpus myself so infact you know I see almost three layersof companies evolving one is companiesthat are providing bare metal gpus soyou deploy a model you train it andthat's for that's not for thefaint-hearted that's where you've gotthe ml expertise you've got your datascientists in place and you're like okayfine I'm gonna do this on my own that'sgreat then you have these next level ofinfo companies which are like we'regoing to deploy all of the open sourcemodels or you tell us what you want usto do or fine tune and we'll do it atourselves so it's it's more a managedmodel service you know dlfx does thisobviously to some extent with MPT andothers but and then there's this thirdcategory which is the open AI anthropiccohere which is like Hey we're doing endto end you just need to worry about yourAPI key and everything else is managedso I think almost three layers dependingon how deep do you want to go or howmature you are as an AI company uhcompanies tend to choose differentlayersyeah I definitely want to like later inthe podcast take your temperature on theum like you know run a closed model theend to end versus fine-tuned languagemodels and just like kind of wear yoursentiments out on that one but I kind ofwant to stay on like kind of Port keythe product first of all that was areally great tour I love the threelevels of you know bare metal up to likethis kind of like um running open sourcemodels or running uh models where youfine-tuned it but hey you don't need toworry about now deploying it I'm gonnatake care of that I I really thinkthat's a super interesting emergence ofthis Market but okay so something Ireally took away from portkey that Ifind is fascinating is um you say I haveum you know I use gbt4 I use anthropicscommand nightly and I use cloud and I'mgetting rate limited by open AI so I sonow I can pivot how do you think aboutlike managing multiple llms in uh inyour appsabsolutely so almost think of this asthisso I end up drawing a lot of parallelsfrom the traditional devops world so thetraditional devops world has the conceptof API gateways and load balancers nowwe've sort of implemented the samethings here so you know think of an AIGateway or an llm Gateway and that isconnected and load balancing acrossmaybe multiple keys or multiple accountsof open AI or even between open Ai andanthropic so this could be loadbalancing this could be fallbacks thiscould be retries this could even beCanary testing and portkey makes all ofthis possible wherein we are in the endconnected by all of these differentproviders closed doors open sourcebanana everybody elseand the user just needs to call oneendpoint and Define a configurationsaying how do I want this call toTraverse and quad key can essentiallyorchestrate the entire call and makesure that you getyou know the fastest call with the bestaccuracywhich is also cost efficient so that'ssomething that we do on our side butyou get to choose whichever model youwant at any point in time and portkeycan manage the restso quickly let me ask aboutum so so the kind of model inferencesthat we're routing are are you mostlyseeing you know open AI cohere or maybelike you know Azure Bedrock kind of likesome of the more like major Google uhlike the major uh Cloud products likewhat what kind of um ensembles of apisdo you tend to see organized this wayyeah I think so it's been surprising tome the most common actually the mostcommon isn't surprising it's open Ai andAzure because people are trying to goover the rate limits and Azure givesthem the extra rate limits uh sometimesit's faster than open AI inferencepoints so multiple accounts of open AIor open Ai and Azure are the mostcommonly load balanced system that wesee on 4K todaya lot of companies are not trying outanthropic uh as a fallback becausethey're saying you don't want to getwhen they're locked into only open AI wedon't know how their model evolves sothere are companies that are fallingback to anthropic and also buildingthose capabilities and I think justbecause portkey makes it super easy todo this you don't have to set upanything at your end and we're justdoing uh you know we're falling back tomultiple providers as well the otherthing that's interesting and it's justbegun to happen and we discussed thisabout players like banana Etc is ascompanies become a little more matureand they're serving a lot more calls andthey have the data to fine-tune afoundational model for a very limiteduse case then they're saying hey let'stry a load balance between open Ai andmy fine tune model and evaluate theresultswhich is we'll do an 80 20. again if Idraw from the traditional engineeringboard you have these blue greendeployments I want to use the fine-tunemodel but I have no idea if it workswell enough so let's do an 80 20 testum I'm going to send 80 of my calls toopen AI 20 to my fine tune modelused evaluations and human feedback andthen compare the performance and if itstarts to do well then just startincreasing the load until I get to 100there yeah so that's that's somethingthat you're seeing and that's actually avery interesting space for companies asthey matureyeah that or yeah I think that's youpainted the picture perfectly how youthink about deploying a new model andhow you're going to integrate it withyour software definitely you know yeahit's all very clear I guess like um yeahmaybe we could just kind of step intothis broad like you know fine-tuningllms you know I I maybe just to tell aquick story I've been looking heavilyinto the gorilla models so this is whereyou like fine tune a language model touse a particular set of apis and so inour case we're fine-tuning languagemodels to use the weeviate API so whatthis could result in is like an auto APIwhere you would come to wevia and yousay hey I want to make a bm25 search butyou don't know the graphql syntax yet sothis language model would produce thegraphql for you and and so so this hasbeen my experience with you knowfine-tuning language models and so yeahso this kind of like you know it'sgetting so easy there's like a replicatutorial on llama too this is like howto find a tuna llm obviously Mosaic mlbeing Acquired and so it's like yeahthis kind of General thinking onfine-tuning llms can you just maybe ifwe could even step out of just like whatyou're seeing in the market just likeyour sentiment on oh well yeah it's likethe city has sentiment on the evolutionof how common that's becoming to doyeah I think again it's almost thematurity curve of organizations adoptingllms so as you you'll always tend tostart with something that's very safeyou know this is you know very welldefined and then you sort of say okaynow I want to extract the next level ofefficiency so I think there are probablytwo three factors that play one isobviously maturity so as you go moremature you want to fine tune you wantbetter results you want to outsmart thecompetition you want to build your modesso that's where fine tuning reallybecomes useful the second is I thinkI've seen companies which are reallyparticular about data and privacy theywant to only work with their own hostedfoundational models so it's interestingthat Azure today offersyou know co-located servers I don't knowwhat exactly they call it but they saywe can deploy or we'll VPC pure theopening apis to your API and that's howwe work soI think that's where companies arebecoming more and more interested infine tuning foundational models uhbecause I mean llamato is really good interms ofuh actual business use cases and how youcan get started with it especially forsmaller use cases like gorilla is abeautiful example I'd love a world rightwhere you don't need to probably invokeor remember the API endpoint makemistakes in the body of the post requestand then fail rather than just sending atext query saying that hey this is whatI want and if something fails it justreplies back in text and I can againreply so I think that can be a reallynatural conversation the same way maybeConor and rohith are talking on apodcast can servers talk to each otherusing natural language and that becomeslike the new API standard so that'll befantastic and these use cases which arevery limited can be done much muchfaster and using a fine tune llm soI think that's where people are nowtrying to figure out what are thesesmall use cases pick them out and createuh fine-tune llms on top of it I thinkit would beit would be unjust not to mention butalmost everybody is right now lookingforward to open air releasing finetuning capabilities for 3.5 in gpd4that could be a major step forward for alot of people because these models arealready really good and if I am able tofine tune them on my own data and getthem you know getall of its latent capabilities to mydata that would just be fantastic so Ithink fine tuning is going to becomeI mean I am definitely looking forwardto more and more use cases on finetuning both on foundational models aswell as on the the players that alreadyare ruling the marketyeah I have a couple interesting thingswith fine tuning that I've been thinkingabout the first well the the first one II don't even know if it's worthcontinuing on the conversation but it'sthis idea like earlier I think maybe uhseven months ago I had Jonathan Francoon the podcast from Mosaic ML and he wasexplaining this idea of like continuedlanguage modeling on your data so youknow say I want to do again this gorillamodel that writes we V8 apis If insteadof just going right to uh instructiontuning with the did you write the APIcorrectly I also continued languagemodeling like a gigantic data set oflike we VA documentations like so likethat kind of intermediate step and so Ido think a lot about is that needed andso that's like one topic but the secondtopic that I think is a little moreinteresting especially for weaviate andparticularly retrieval augmentedgenerationthis is another nugget I took out of thegorilla paper is that they're um they'regoing to be doing retrieval aware finetuning so what this would look like asyou know most of these like uh how tofine-tune an llm with replicate is likejust kind of give us the data right soyou would you would retrieve and thenyou would put that in the data and thenit's still uh instruction to so youwould put yeah put the retrieval withthe input andit's I think this kind of retrievalaware tuning it because you knowespecially for us maybe if I could askyou this broad question of how you seeretrieval augmented generation becausethat's especially with weaving andVector databases that's like the youknow the hot Gold Linetotally yeah I think no IDuh is definitely the flavor Everybody isusing it's alsoit's amazing when it works right so Iremember there was this blog post byGuan back in like late 2020 where he wastalking about context stuffing to makeyour LM outputs a lot better and contextstuffing earlier was examples or contextor everything that you can give the llmbecause then you're constrainingum the output that it produces and thatwas something a lot of companies and youknow early content generation companiesused quite a bit and then once the RDGpaper came out uh I mean not the paperbut the RG implementations came out itwas just amazing how people are gettingto use this for a variety of use casesright andum then so you have base RNG then youhave these interesting Chain of Thoughtexperiments on top of rig which is I'veasked you a question let's do a Chain ofThought analysis let's reason let'sclassify and get to the right answeraccurately so I think that entire chainthat started to evolveum is obviously very very interestingum Rog itself has multiple flavorsum somebody yesterday but we're talkingabout how internal company use casesversus external companies cases the rackcan be very very different for internaluse cases you need a lot morepermissioning access management ControlManagement Etc versus for external usecases so if you're doing uh you knowlike a customer service spot then maybeit's lesser restrictions orpermissioning but a larger set to searchon so how do you do that using sayhybrid search in review so those arevery interesting Concepts that peopleare nowum looking foruh interestingly data leakage wasanother thing we've been discussing withthe customer very actively so they'reworried about your leakage because nowyou have all of this so earlier the datais very very segmented in their databasebut as we know these embeddings to avector database how do the embeddingstay as segregated as their data hereand what are the you know compliancerequirements for that so those are allthe things that people are now gettinginto as they understand Rag and thenthey're taking these rag implementationsto Legal security to their instances Etcyeahyeah I can I oh I'm not sure I see itwith embeddings all the way yet but Ican I remember with the language modelshow you'd be like um you prompt it withlike uh the email these like from BobVan light to and then it's start yeahthat could be a problem with languagemodels yeah yeah and I mean yeah thatmulti-tenancy kind of thing you knowrv8120 introduce a major revamp for thatand people listening you know Eddie andDelacruz CTO he explains that and yeahthat's something that um you know that'ssomething I wouldn't have known aboutjust academically looking at the papersyou know that's something I think yougot to go out there and see a little bitto understand what a problem it is butyeah it's all really interesting I guessum so I get then kind of on this topicof fine-tuning and yeah there's defthere's you know compliance and it'shuge issues from you know justsensitivity within business I think alsolike you know it's kind of related tolike can gbt really be your doctor orlike that kind of thing and um but kindof pivoting a little bit I want to comeback toum this kind of like Port key and thiskind of um you know just like the costSavings of running inference and so Imaybe we've talked about a load balancerthat kind of like um you know is uhsaying mostly from the perspective oflike this is rate limited do you alsolike you know yeah I think people areaware that if you start paying it toofast it says hey stop it and so you youknow you go but you also think of it ofmaybe like first ask the question to thecheaper language model see the answerthat comes back and then say okay andsend that to the user versus like nolet's give that to the more expensiveoneyep absolutely I think we're starting tosee those implementations now whenpeople areuh and that is you know so llm call tothe cheaper model evaluation and thendecide if you want to send it forward ornot uh a simpler implementation of thisis also make the cheaper API call uhbecause I know that it works 80 of thetimes the user has the capability toregenerate especially for very lowhanging use cases so where it doesn'treally matter and the users totally fineregenerating the second call goes to theGPT 4 API so it's like a mixture ofthese two but definitely the LM callevaluation second LM call is usually avery interesting plate if you'restarting to see some companies do verymuch and this is almost a 10x pricedifference so it becomes so much easierand you know even in terms ofJanuary testing a lot of people willmake calls to both apis initially figureout the similarity between these apis sothat you have at least a minimal levelof confidence that this model performsat least 80 percent of the times andthen I can start to expose that a littlebit moreyeah because I think well I thinkthere's already going to be so muchvalue I imagine you know with justrouting it with rate limiting and thenrouting it for accuracy but then kind ofyou know dreaming into the academicthing and like one thing is there's thisidea it's kind of related to gorillawhere you have like gbt4 is kind of likethe Master model and it would route tolike in the gorilla sense it would routeto a tool and then you have a smallerllm that uses that formats the requestto the tool particularly like alleviatethe graphql API or there's this otheridea where it's like um you turn likethe hugging face model Hub into like umyou could think of each model as an appI don't know if I love this but like youknow you're the image segmentation modelso I go get my segmentation mask then Isend that to the classifier stuff likethis so do you think about that kind oflike orchestration of models as beingmaybe like an a direction Port key wouldgo inyeah I thinkum not right now uh we're mostly focusedon production use cases where we've seencompanies already go to production withit I think this is clearly in the realmof so you first have agents and toolswhich are yet to go to production in areally large wayum from our perspective uh and then youyou're thinking about models as Tools inagents I think that is clearly an areaof Interest right now a lot of academicPursuits happening in there but I havenot really seen anybody deploy this infact we've seen the reverse wherein youhave a very uh simplistic prompt routerwhich which is written before the llmwhich decides which llm should I sendthis call to or should does this needadditional context so I need to send ituh to my Vector database to get thecontext from there and then push in thellm so I think that smaller model isactually more efficient also becausemore business most business use cases Iwould not like the added latency of youknow the entire chain running and then Igetting an answer so you'd want to dropoff at the right point so you definitelydon't want to introduce additionallatencies in play and I know we'll talkabout caching a little bit later but Ithink semantic cash is one of thosepieces right which is how can I returnan answer faster so I think latency issomething that businesses really reallycare about so for anything that'sasynchronous and you don't care you'regonna get a reply later then yes we cando this you know interesting routingacross multiple llm calls and then getour output but for most other cases youwant it as fast as possible so yeahyeah uh yeah I love kind of like I'vebeen studying a lot of llama index andthese query routings kind of like whereyou have one language model that takesthe query and says you know is this myfavorite example is like is this avector search query or is this a SQLsymbolic queryso that kind of routing you don't needlike uh 300 billion parameter model toto do that yeah I think that's reallyit'll be really fascinating how this howthe um llm Frameworks evolve with thehosting of different models needed fordifferent kind of routing or toolselection it'd definitely be a whole ahuge thing but but yeah you mentioned itand let's get into it our debate topicwhen we met in Berkeley was um you knowsemantic caching I I admit when you knowwhen we first started talking about it Iwas kind of cold on the topic I didn'treally like because I kind of think thatlike the the Nuance of prompting is it'sso particular but you know you explainto me about question answering and andyeah and so I do agree that this kind ofsemantic caching of LM calls could bereally interesting could you maybe uhset the stage on the topicabsolutely so I think caching has been athing that's been long uh drawn we'veseen caching use cases across llms verywell but when we implemented caching sothis is almost something that we learnedacross the way right so when weimplemented caching for our users we sawa cache trade of approximately twopercent and I mean it's no surprise isthere because everybody's probablyasking the same questions but they'reall asking it in a very different wayand the wholepromise of llms is that there's noannoying what clicks anymore so there'sno deterministic inputs which meanscaching becomes that much harder becauseit's not the same query anymore rightbut if you still look at it for customersupport use cases employee support usecases ID support knowledge search60 to 70 percent of all of an employeequery or a customer queries are going tobe similar they're going to be somebodywould have asked them in the last sevendays even if I take a very short windowso how do you go about really makingsure that I can serve responses fasterif I've already seen that question sofor example somebody asks does vv8 havea free developer plan or somebody elseI'm a developernow these are both the same questionsand the answer is whatever it is butthen how do I go and make sure how Icould either do it through a complicatedllm call orif I canscrape the intent of the question fromThe Prompt coming instore it as an embedding in a vectordatabaseand then match every incoming query tomy list of already answered queries thenI can essentially build a very effectivesemantic cache so I think that's wherewe started saying okay this seems like astraightforward enough problem let'sbuild semantic caching and let's go tolet's try it out so I think the resultswereI think they were just amazing whereinwe saw without any optimizations 20 ofall of our queries would give us a 90 9596 accuracy so and 20 of these querieswould be 20x fasterum I think the challenges and this iswhat uh I think milvis launched with GPDcash uh and which also I think which isalso why you were skeptical and Iremember discussing it then then I thinkthe base semantic cash the base semanticcash implementation is probably just atheoretical uh foray but it doesn'tsolve use cases because in the fivepercent that it fails it's gonna giveyou a an extremely absurd answer andit's going to give it extremely fast soyou will feel like you know it's comingfrom cash it's absolutely wrong andyou've probably leaked some data becauseyour question and answer are completelydifferent that was a question asked bysomebody else the context was verydifferent and now I have leaked data soand this is where I think we started ourjourney of just understanding semanticcash how it works I think that's wherewe ended up implementing uh you knowdismatch patch librariesum we take out relevant parts of theprompt which need to be cached because amajority of the prompt is similar so Idon't need to Cache it so how do we takethat outum there's a lot of post-processinginvolved then to make sure there's aquick eval check to see you know is thequestion and the answer relevant to eachotherand then there's a constant evaluationmodel running to say that before Iintroduce semantic caching let's what isthe right Vector confidence to set sothat I get a 99 or 99.5so it's this entire production chainthat builds you a very robust semanticcacheum and that's where I think we've spenta lot of time just tuning these smalllittle things to make sure the entirepiece really works but now yes we areseeing customers when they start theystart on day one with 15 to 20 percentof their responses being served fromcashand in some cases we've seen it go ashigh as 60 percent of their responseswhich is like phenomenal for them mostfrom a cost savings perspective as wellas in terms of user experience I thinkyou still have to get we're constantlyworking to keep getting accuracy betterand better uh but I really think we'vegotten it to a stage where it's superuseful for Enterprise search use casescustomer support regyeah it just makes sense thereyeah as first I think there were a tonof nuggets on the great details and Ireally enjoyed it I like the as youended with the kind of well yeah thisthinking of like the how do you checkthe failures case can you maybecalibrate the vector distance of thesimilarity from the query to thequestions you have cached I find that tobe very interesting at wevia we'velooked into Auto cut where we we look atwe plot like you know Vector distanceson the on the y-axis and then X is likeresult and then you try to check a aslope to say only give you like threeout of the 100 search results to try tolike but but yeah just for the top one Idon't see a good solution to that I meancalibrating your vector distance scoresfor your particular you know that Ithink like I'm going off topic but likethere is like conformal prediction likethat that seems to be like when you knowlike causal inference like how they'relike these buzz like these emergingcategories of research I see that thatcoming up a lot so it does seem like youknow uh the research and uncertainty iscatching up but um yeah maybe even justtake a step back like I remember likewith weavier when we were brainstorminghow to add a site search to eviateI thought one thing that would be reallyinteresting is just to kind of you knowcreate a frequently asked questions andthen just the embedding of the query toone of the frequently asked questionswould be a way to do it but then youkind of mentioned like this cold startof like 15 to 20 evolving to 60 becauseit's like you might not have acomprehensive FAQ yet so you can startusing your you know language model QAsystem until you build up this base andnow this cache thing is really criticaland yet it's really fascinating I mean Ican imagine also like um you know youmentioned like data leakage like if I ifI'm coming to and I tell you about myparticular schema as I'm asking you thisquestion about my bug and then you endup like giving away some questions I canimagine maybe a language model thattakes that and then says and then sayslike please rewrite this to up to youknow make it more abstract and then thatresult is what's saved in the cache yeahI think it's a supercharger for questionanswering systems and I think that likeyou know chat with yeah like the wholequestion answering part of that is justone of the biggest applications but Iguess kind of my question then is umthis kind of like embeddings forquestion answering there's also likeembeddings for classificationis like how do you see like when a taskis ripe for just embeddings and justVector search alone is the way to do itcompared to when you're like no you needa generative model for this kind of taskyeah I think for anything to do withclassificationsum classification topic modelinganything that has a deterministic answerit's a deterministic question you need adeterministic answer or at least youneed options which are deterministic Ithink in those cases yes an embedding isprobably the best solution unless somaybe let's let's do this right so youhave deterministic input deterministicoutput embeddingsumsubjective input deterministic outputmay be functions so use Json form orfunctions or embedding so it can be bothso you can have an llm call whichreturns Json or you can have anembedding you'll have to see which oneworks bettersubjective input subjective output thenyou need an llm call with a cache on topif you see a lot of these similarquestions coming in which is where Ifeelrag use cases q a use cases search usecases these three are all subjectiveinput subjective output which is whereyou need an llm call with the vector DBin placethat's what I think but what what hasbeen your learning from seeing all ofthese systems at bv8 so farwell I think um one uh you know emailand Finn set up uh Kappa AI withleviated it's really cool I think theyjust got into Y combinator and it youknow they're you know going to the moonand it's really cool and it makes methink about how um a lot of peoplecriticized uh apps that that weredescribed as like a rapper around a gbtAPI but if you're a Kappa and you'recollecting uh you know customer questionanswer for you know like we via slag isin the Llama index Discord and like solike you build up this data and now yourremote is that uh data set that youcould then cache for cost savings andthat's kind of the light bulb that'sgoing off in my head is I'm thinkinglike you don't need to create a moatnecessarily by fine tuning withgradients with I think that's like a lotof what AI people would think is that helike you you know train train it reallynicely and yeah but like having a havinga really curated data set for retrievalaugmented generation it's yeah yeah Idefinitely let me pass it back to you onthat kind of like the debate between youget better performance by fine-tuningthe language model or just by reallyhaving great data for your retrievalaugmented generation stackyeah I think I would love to do like alike a proper Benchmark on this Ihonestly haven't seen oneI think my general guess would be ifit's a very repeatable use caseumuh then maybe fine tunes might work outbetterso you know it's the same characterlength output it's gonna contain almostthe same informationthe parameters might be different then Ifeel so fine tuning is great forconstraining outputs right so you cankeep improving accuracy up till thepoint and you can keep constraining youroutputs as much as possible whereas andI think that's where fine tuning isreally shines so if you're doing productdescriptions just keep constraining keepfine tuning keep getting a great dataset and then your product descriptionsare gonna look greatbut the minute you're going for varianceI think that's when you do needembeddings and Vector searchum at least that's been my practicallearning so far that fine tuningand these can still coexist in fact Iwould love to see an amazing lag usecase where you fine-tuned a model andthen you're passing in the right contextand this always knows that I've got tobe helpful this is my tone this is how Ireply I've seen these questions I'veseen these things before Etc so I thinkbut definitely giving it the context inthe prompt is more valuable than relyingon its memory or the llm's memorybecause there's just so much stuffinside it that's very hard for it toprioritize which one to pick at whattimeyeahIum yeah there's definitely I think umwith me with my sentiment on fine tuningI think like so that gorilla experimentI mentioned earlier is retrieval awarefine tune where you retrieve the APIdocumentation and then you know you getlike the schema the API for like how todo bm25 search and we get and then itwould write bm25 query and so I guessfor me so you mentioned fine tuning islike how you steer it into that narrowpocket of the space so it's like this isthe thing that you do and yeah like ifif I fine-tuned it on so on like youknow I have 60 apis and we via searchapis and it I've trained on 40 and thenit can't generalize to the 20 or or whathappens is I train it on the 60 and then20 new ones come out and it can'tgeneralize to the new ones as well asthe you know gbt4 can that would firstlybe kind of that would hurt my sentimenton fine tuning pretty pretty massivelybut I guess the other thing is like umI've been experimenting where Isummarized these podcasts with like oneof those summarization chains where it'slike you will receive the clips one byone as well as the summary so far andthen you know you use the localizer andso because it doesn't really deeplyunderstandyou know like um just what we're talkingabout right now right it'll it'll it'llusually just kind of collapse uh topicsinto like comma lists so like with thatuse case I think heavily that if Ifine-tune the language model doing thatlanguage modeling thing we talked aboutwhere you continue language modeling iton weeviate you know just like as muchinformation as I can find but I actuallyso sorry I kind of want to Pivot the thetopics a bit back into kind of you knowbeing at The Cutting Edge of you knowwith Port key and you know this kind oflike you know this layer between the LMSand overall understanding that this is anew layer in the software cycle gets uscloser to cheaper llm inference I wantto kind of talk about two things thatcheaper llm inference would unlock andsorry if this is a context switch butthe first is this tree of thoughts thingwhere you couldum go several Pathways you know it'saside from kind of the caching I supposemaybe you could cash paths but like thisas as inference gets cheaper we'll beable to do this kind of thing and whatdo you think could be the potential ofthatyeah absolutely I think so a lot of thethings thatum you needed deep decision trees todecide can now be left on reasoning bythe llmsum I think it's going to be a functionof cost as well as latency so you needto be able to process this fast enoughbecause if your router is going to takethree seconds then your infant's gonnatake another three you've necessarilydoubled your latency which which nobodywants so I think cost is definitelywithout it getting cheaper youdefinitely cannot route but then if Icould do you know two queries on add alevel cost and inference time that wouldjust be up that would be mind blowingbecause then I would imagine a lot ofsoftware developers will just stopwriting workflow code at all I mean whydo you need to create workflows andsoftware which are essentially justdecision trees and these decisions canbe taken extremely fast extremely cheapextremely accurately by an llm itselfyou know so a very interesting examplefor this isuse and throw software I think that Ifeel is going to get on the rise becauseof this wherein could I just spin up aquick replied instance and half chargeGPD write me some code that says hey Ijust want to create 50 PDFs with thistext in it these are appraisal lettersfor all of my employees create thesePDFs upload them to my Google Drive Linkthat's attached here ask me anyquestions and then if the llm couldnavigate the entire piece and give methe output which basicallyit's doing routing plus inference plus alittle bit more then I think it's it'sbecomes really really interestingbecause you don't need workflows anymoreyou can just rely on the lln buildingthe workflow and I think this isslightly different from Agents becauseagents are doing multiple inferencesGathering data points from thoseinferences and then using it out versusjust saying that create the entiredecision tree framing which is theworkflow and the llm is then in controlbut I'd love to see a time that it comespossible I don't think we're anywherethere yet because you need extremelyhighly extremely fast inference andextremely cheap inference to even makethat happenyeah I agree completely with thatperspective I and earlier we madepodcast Colin Harman had described thatyou know like Auto gbt it's yeah it'slike the search of a workflow and italso kind of reminds me of demonstratesearch predict from a marketab and thelab at Stanford where they give like afew input output examples and you'rekind of like compiling the the workflowthat would produce those input outputsand so yeah I I think all that kind ofthinking andyeah it's fast I mean as the the cost ofinference gets cheaper so this kind ofbrings so then the next thing I wantedto kind of talk about with as theinference gets cheaper is uh we've hadthis we've been trying to likeevangelize this generative feedbackloops or the ideas that you know you dosome kind of generation and then yousave it back into your vector databaseand so you know a simple example couldbe I get my new blog post and then Itake my crms and I write a personalizedhey why you should care about this blogpost to everyone in my CRM and you knowI and then you know I that kind ofthinking that I save that back in adatabase so maybe first you let me takeyour gauge your interest in that ideabroadly like that kind of thing of howpeople use well because we've alreadykind of talked about it with some it'sactually semantic caching is kind ofexactly this idea because you'reanswering the questions and saving itback but umso the the the big headline thing thatreally excites me is like you know likevector search is about scaling Vectordistance calculations like people kindof troll it on Twitter by saying likehey I just implemented a vector databasebecause you've like numpy Brute Forcelike 100 000 vectors but I think reallythe exciting thing is once you'retalking about searching through like 10million 100 million billion so on howbig of a number can I say right now butlike but um this kind of I feel likethis kind of generative feedback loopcan be the Evangelist for like whereeverybody is thinking about having dataof that scale to search through like youknow I I pointed to my blog post and itlike writes it Compares parts of theblog post I don't know it does usefulthings such that it's like it createsthis latent space what do you kind ofthink of that idea yeah no I think it'sinteresting I haven't given it a lot ofthought honestly until now but I'malmost now imagining people build dataLakes before this just to store a bunchof data so that I can query it and I canuse it for training or I could do somany things with it I think this isalmost like the vector data lake or avector link you have so much embeddingsthrown in it that now I can do variousdifferent computations calculations andtraining on it so that's actually a veryinteresting concept exactly semanticcaching is exactly that right so it'sstoring the outputs back in for aspecific use case but then I candefinitely think ofevaluations as another use caseum maybe some validations maybefine-tuning at some point in time sothese could be interesting things whatare other use cases you guys seen forgenerative feedback loopswell that um yeah well firstly I agreethat I think the semantic caching mighthave to be the new Evangel the new likeheadline thing for this but the you knowthat that thing I mentioned initially ofum you know you go through you you havelike your newsletter and you then have aCRM or something and you're writing likepersonalized that's a pretty interestingthing and then um there's another thingthat's particular to search which islike if you have like you know say Itake our podcast and I start indexingthis this is like my favorite data setto kind of like dog food the technologywith is like um if I just take the rawpodcast clip and then I pass that to alanguage model and I say pleasesummarize the content and then I indexthe summaries I actually I get waybetter search results with this with theindex because you know like this likebecause like um if it's like totallyruin the embedding sort of not totallyruin it but like it's not as good as thetransformed and so that's been probablymy favorite thing in the search case isyou transform it for a better indexthere's there's yeah there's also likeyou extract structured data from thechunks andthe chunking like chunking text of thevector databases that's quite a deeptopic but yeah totallyyeah I think kind of the headline herethough is like the you know Technologieslike portkey and this whole emerging youknow llm inference is getting cheaperthere are more options we've got thisllama 2 model like is the latest one aswe're recording now you know we had MPT30 billion come before that and like itcontinues to get cheaper and cheaper sothat's why I think this like tree ofthoughts and maybe like generativefeedback loops I think those things arecurrently blocked by the costand yeahyeah 100 I think it's just the cost ofinference as well as I think theresistance of pre-produced multiplelanguage models today I think I'm we'redefinitely going to see over the nextfew months people start to use maybe twothree ten twenty different models fordifferent use cases and then there'ssome orchestration between them just tomake sure you're getting the best youknow on the cap theorem so cost accuracyperformance you have the best on the captheorem and then you've got this tinyrouter sitting in the middle making surethat the calls are being distributed tothe rightum llms there and that I think willoverallenable companies to collect more datastore more dataobviously I'm guessing the way llminference is getting cheaper I'm sureVector searches are also going to getcheaper and as that happens people willdefinitely want to store a lot moreum I think just search a lot more ofthese data sets as wellyeah no I definitely I think kind oflike Bob gave me this term likespeculative speculative design Theorywhere we're like kind of going out wayinto the future but yeah I think justright now what you mentioned on managinglike yeah 15 to 20 model inferenceswhether that's like gbt cloud or or howyou're going to introduce models thatyou've fine-tuned I think that'sabsolutely like a super massive emergingspace of this whole like AI market andso on so yeah awesome thank you so muchfor joining the podcast I think this isan incredible coverage of these topics Ihad so much fun uh picking your brainabout these things and you've definitelychanged my mind on semantic caching Ithink that you know it's totally flippedI think it's very useful and awesome I'mso excited to be following along withportkey and see how all these thingsgrowabsolutely it was great chatting withyou I think as I was talking with you alot of new ideas also popped into myhead so I'm going to write about themnow but thanks so much for inviting meConor was amazing doing this video", "type": "Video", "name": "Rohit Agarwal on Portkey - Weaviate Podcast #61!", "path": "", "link": "https://www.youtube.com/watch?v=GnyajCD1Vrs", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}