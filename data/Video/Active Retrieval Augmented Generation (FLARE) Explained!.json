{"text": "This video explains the details behind Active Retrieval Augmented Generation from Jiang et al! This is a super exciting innovation ... \nhey everyone thank you so much forwatching weave yet on YouTube this videois going to dive into a super excitingnew paper on large language modelgeneration titled active retrievalaugmented generation the technique isalso known as flare flare is short forforward-looking active retrievalaugmentation so quickly before divinginto it this is one of the first papersummary videos on WE via YouTube so ifyou like this kind of content please letus know by giving it a thumbs up hittingthe like button and subscribing so withthat said let's dive into it so here's asuper quick overview of the algorithmflare in two minutes just in case you'rein a hurry and you don't care about theexperimental details and just want thegist of the new algorithm so typicallywhat we do with retrieval augmentedgeneration is we take a query likegenerate a summary about Joe Biden weturn that into a vector and then we usethat to Vector search for our nearestneighbors or maybe you do a bm25 keywordsearch with this phrase to get searchresults that help you write a summaryabout Joe Biden so now what you're goingto be doing in the flare forward-lookingactive retrieval technique isiteratively doing this retrieval wallgeneration not just retrieve once andthen generate so the way that that worksparticularly is you have generate asummary about Joe Biden uses that as thequery to get the search results then youdecode a potential first sentence fromthe generation Joe Biden born November20th 1942 as the 46th president of theUnited States and you use thatforward-looking generation as theretrieval query so then you delete thispotential generation you just neveractually generate it you just sample andsee what it would have been and then youuse that what it would have been as thesearch query to get information thatmight be a little more relevant so youknow say it's going to write this is the46th president of the United States thissearch query might result in searchresults that say you know thatexplicitly say you know what numberPresident Joe Biden is similarly withthe Next Generation Joe Biden attended ahallucination the University ofPennsylvania where he earned a lawdegree and other hallucination you usethis next potential sentence as thesearch query and then that will retrievesearch results that say you knowUniversity of Delaware Bachelor of Artsand history and political science sothere are a couple other interestinteresting details about just howparticularly you're going to sample thisquery they ablate an implicit explicitquery details that we'll get into in thepaper summary video in addition toexploring how to form these queries theyalso explore when to do these queriesparticularly when the the logprobability the language model put oneach of these tokens oh sorry in thetheoretical generation when that islower than a given threshold that's whenyou trigger the forward-lookingretrieval so they Benchmark thistechnique on four data sets that we'regoing to get into in the paper asqa hintis the same data set as sqa they comparetheir result of forward-looking with abaseline of using the previous window asthe retrieval this General frameworkthat we're used to of this single timeretrieval and then no retrieval at allalso in the details of the paper theyhave a really interesting comparison ofthis kind of technique with say searchagents where search agents is whereyou're prompting it to say uh you knowopen square bracket search open uhuh parentheses sorry and then the searchphrase you know that kind of searchaction thing so they have some reallyinteresting ablations really interestingdetails of the paper but if you wantjust a quick two minute overviewbasically the idea is to use thepotential next generated sentence as thequery to have an active retrieval whilstgenerating something long like an entiresummary of Joe Biden okay so followingthat two minute overview of the flaretechnique let's kind of walk through thewhole basics of the story of this paperand the context that it lies in so we'restarting off with this general idea ofretrieval augmented generation this issomething that at alleviate Vectordatabase is super excited about thisidea or basically what you do is youtake the prompt to chat gbt you turn theprompt into a query then you hit one ofthese search engines and then you getthe documents and then you take thosedocuments and you put that alongside theprompt to a large language model such aschat gbt and then this gives you acustomized response so you don't need tofine-tune the large language model toyou know generate things particular toyour data you can just take thesedocuments put it in the input and thenthe large language model is remarkablygood at kind of reasoning across thisinformation that's in the context windowso as a quick example that I like to useall the time you say you ask chatgbtwhat is reftube reftubec is a particularfeature to alleviate and you know chatgbt says you know I don't know what refto VEC is because it's not in thetraining data but when you do this whenyou take this same what is ref to vacand you use it as a query to then hitthe hit a data set of the wivier blogpost then you get this short descriptionof raftavec and then by prompting itsaying please answer this question basedon the following context now I can tellyou rough to Vector over reference thevector is a moduleeviate 1.16 so you seehow this kind of retrieval augmentationcreates a you know an answer where thelanguage model can reason about yourkind of particular information of coursethe reasoning I'm using that just likehand wave I don't mean like logicalreasoning but anyway so this is thegeneral idea is that you retrieve andthen it helps inform the generation inthis paper the authors are asking aboutwhat if we continually gatherinformation throughout the generationprocess rather than this retrieve onceand then generate kind of framework whatwhat does it look like to be iterativelyretrieving from our information as we'redoing some kind of generation so I thinkthis kind of class of thinking is mostsimilar to kind of Lang chain I don'twant to particularly say line chain thesoftware library but this kind of ageneral you know academic concept of yougenerate something and then you take theoutput of what you generated as theinput to the next large language modelcalled that kind of idea of continualgeneration or you do some kind ofretrieval I think we've kind of startedto see this thing and this paper has agreat comparison of flair with theself-ask prompting which I think is areally great uh analysis of these twoworlds of whether you're going to kindof chain these language models togetheror if this is something that's cookedinto the first sort of generate call andit's not really so much as like discreteprompts that that are applied in somekind of sequence but just something thatis a mechanism in a single generationbut we'll dive more into that theauthors ask can we create a simple andgeneric retrieval augmented languagemodel that actively decides when andwhat to retrieve throughout thegeneration process so this is why Ireally want to kind of hit thissimilarity between the self-askedprompting is we have seen this kind ofthing before where you're you knowprompted with some kind of tool like heyyou have a search agent if you want touse it then open a square bracket searchand then you know parentheses the searchterm so it's not like the first timewe've seen this actively decide when andwhat to retrieve but I think theparticular mechanism with this is prettyunique and as mentioned they have a youknow they compare these two techniquesside by sideso then finally they draw with this kindof human reasoning thing I always likethese kind of comparisons they saysimilar to how humans gradually gatherinformation as we create content such aspapers essays or books long formgeneration with language models requireGathering multiple pieces of knowledgethroughout the generation process so Ireally like this because you knowimagine that you have a prompt like youknow write me three paragraphs about howkubernetes works if you know nothing ifyou just start with a coup with what iskubernetes as the query as you startgenerating you know you'll discover somepocket of knowledge that you need to domore research in I think as all of ushave done this kind of like explorationof new ideas you find some some sub ideathat you didn't originally know aboutand now you need to go down that rabbithole and learn all about that thing so Ilove this kind of comparison to thinkingabout why we need active retrievalaugmentation and why retrievalaugmentation isn't just a retrieve onceand then generate away kind of thingokay so now let's dive into the detailsof the algorithm so as presented in theoriginal slide in the two-minute let'soverview the basic idea is that we startwith an input query such as generate asummary about Joe Biden we use aretrieval technique whether this be bm25search which is what is used in thepaper or vector search Hybrid search youknow search with cross encoder rankingor you know however you search so youretrieve some information with thisquery and then you're going to use theyou know the d sub queue the documentsformatted with this prompt and the queryto to do the Next Generation so just asa kind of a meta comment I think it's sointeresting how we see these papersaround large language models arestarting to annotate the papers withprompt templates similar to how we usedto see like pseudo code of algorithms soyou see like how the prompt they're likeat least five of these prompts in thepaper and just a detail I thought waspretty interesting okay so it takes theresults and then it generates ahypothetical next sentence and thenyou're going to use the the logprobabilities of these tokens to decideshould we retrieve again and then if soyou retrieve again and then you discardthe original context and you use the newretrieval to better inform thesequential generation in the activeretrieval augmentation so the firstinteresting thing is about when to queryso as I think people who've made it thisfar in the paper maybe you're familiaralready with large language models andhow particularly they generate thingsbut you know a large language model ateach step of generation if it'sgenerating this sentence if all tokensof S Prime sub T have probs greater thanequal to Theta at each step it's youknow it's like if mask and then if allmask if all tokens mask or you knowhowever it's tokenized it's just Wordlevel is easy to explain quickly andwhat it does is it puts a probability onthat word token so you know it's it'slike a say it has a vocabulary size of30 000 it's doing you know 30 000 labelclassification on classifying which ofthe tokens is most likely to come nextso you know it might say like if withyou know 90 all with like 100 or like99.9 then maybe like tokens has like 40so all of a sudden it's like you knowuncertain there's High entropy in thelabels that is predicting for this nextMass token so when you have like youknow thirty percent thirty percentthirty percentthen you multiply out the probabilitiesand then you say if that's greater thanTheta then go ahead and keep it but ifit's less than Theta that means that wehave this High entropy next sentence andthat means that the language model isuncertain of it it's like this idea thatyou can detect hallucinations by the logprobabilities of the tokens it's a superinteresting technique I don't know toomuch about all the literature and allthe tests on how that's worked but inthis paper that's what we're going to doin the paper the authors are going to beexperimenting with the open AI largelanguage models and I did not know thatyou can do this so just showing thisquickly maybe this will help somebodyelse know that you can do this sothey're using the log probabilities fromthese large language models so when youlook at a token so it also goes token bytoken obviously rather than word by wordI just like word by word it's easier toexplain to someone quickly but so youknow FL the token gets 99 here then ARthe token gets 100 or like you know C islike 99.999 so confident that you justsay 100 and then is only has 63.71 youknow so on so so this is like a sense ofthese log probabilities really reallyreally really really interesting thatthat openai is showing you this becauseI think originally I don't I think therewas something around they weren'tshowing you this because you could usethis for knowledge installation there'slike a lot of things you could do withthis but anyway so I try to get anexample of something that's uncommon uhUnum unconfident not confident butinterestingly I guess uh they havetrained on ref to back so restavac usedto be my example from opening the videoof like something that openai doesn'tknow about without the training withoutthe retrieval augmentation butand they've updated it still doesn'tit's interesting because you can see howthis still isn't as good of an answer aswhen we had um you know given it thecontext from the blog post but anyway sowow look at that I I'm just seeing thisnow is that you can highlight it all andit multiplies out the blog probabilityso this would be the log probability youknow for a sentence that we're comparingwith Theta so we you know highlight thisand then we'd compare the logprobability with that Theta kind ofinteresting but you can see like typehere had uh 5.19 uh that's sointeresting I get I guess you're alsoseeing kind of how their decodingalgorithm works and how the uh so youknow decoding also isn't generally likegreedy decoding where you just take themaximum probability of The Next Stepthey do things like you know beam searchand you know this kind of stuff for howthey uh how they decode so it's notalways just the most probable next tokenanother interesting detail if you'regetting into the weeds of exactly how todecode from large language models soanyways I think you get the high levelidea that you're looking at theconfidence by multiplying out theseprobabilities and that's how you'redetermining when to actively retrieve sothe next ablation of the paper is quiteinteresting as well which is how toquery so you know we've we've taken thisnext sentence Joe Biden attended theUniversity of Pennsylvania where heearned a law degree this had you knowHigh entropy and we're saying no we needto actively retrieve with this so thenthe question is do you do an implicitquery it's an implicit query the the youknow the naive way to do it would justbe take this sentence and use thatexactly as the query what they proposeis masking out the uh the tokensparticularly that have the high uhentropy so you know this phrase theUniversity of Pennsylvania if we go backto the open AI playground we'd imaginethat this phrase is highlighted in redand some of it with law degree so we'dyou know we'd hope these hallucinationsare highlighted in red I'm not exactlysure if that works completely fordetecting all hallucinations butyou know so the idea is you take thehigh uncertain phrases and you mask themout such that the query is Joe Bidenattended mask where he earned mask ormaybe just deleting it to the sentencebecomes Joe Biden attended where heearned but that might lose a little bitof the um I don't know maybe like thegrammar of it but anyway so so you usethis as the query to then retrieveinformation about uh Delaware and it's areally clever idea because if you usethe University of Pennsylvania is thequery maybe you'll end up getting moreinformation about the University ofPennsylvania say you're searchingWikipedia or something so this kind ofidea of masking out the uncertainphrases is quite interesting then theother idea is do you have an explicitquery where you kind of map it into thequery space so you know like like into anatural question so ask a question towhich the answer is the University ofPennsylvania and then the you knowanother large language model but thennow I you might be saying oh my you knowusing another large language model togenerate each of the queries is going toget expensive but you know you assumemaybe you could have some task specificmodel for this that generates queriesgiven phrases like this but you knowthat that kind of thing but so whatuniversity did Joe Biden attend whatdegree did Joe Biden earn you turn theseinto explicit questions to retrievethings and it's a pretty interestingidea you can also have this zero shotquestion generation you have the userinput so far the generated output so farand then given the above passage ask aquestion to which the answer is the termentity phrase and then say you know theUniversity of Pennsylvania so all ofthese kind of interesting ways of howyou form the query that you're going tobe sending to the retrieval engineand then so just this little detail thisis about when to mask in the implicitquery but what you see at the end is aswe'll get into the evaluation resultsthey don't actually seem to find a likea you know a difference between theimplicit explicit query so maybe this isjust something that's interesting totalk about you well I mean whatuniversity did Joe Biden attend comparedto Joe Biden attended mask or your maskit looks like they perform similarly onthe you know on how they measure that inthis paper but I think we'll get moreinto the benchmarking but it's hard toit's hard to say if these benchmarks aregoing to generalize to you know the appYou're Building you know you're buildingan app with leviate and uh you know anopen Ai and stuff is this is thisparticular thing going to hold to yourresults I don't I'd say try it out andsee how it works for your particularthing so I think that's a really goodtransition into how did they measurethis what are the experiments that wererun so let's start off with the Baselineapproaches so the the strong bass lineis to use the previous window sentenceas flare query well I'd call it a strongbass line because it's sort of like anobvious ablation the next thing isprobably the strong based on like thecurrent state of the art sort of way ofthinking about this but using theprevious window sentence is the flarequery I think the benefit of this is youdon't have to do that kind of extrageneration where you do theforward-looking step and then you querywith the forward-looking step if youjust use the previous previous sentencethen you know if you think about overallhow many large language model callsyou're making that might reduce thatkind of thing but so that's the Baselinethen we're going to get into self-askprompting how that's different from thistechnique and then they also have a avariant on self-ass prompting calledflare indirectso previous window sentence uh you knowpretty straightforward window meaningthat you use a token window where you'recounting the tokens for the you know theprevious sentences that are used as aquery whereas sentences you use theheuristic of splitting on you knowperiods exclamation marks question marksto you know say that this is a sentenceand then send that as the query so thisresult is quite interesting they'reshowing that using the next sentencecompared to the previous sentence givesa pretty high uh you know Improvement onexact match F1 Precision recall of amulti-hop question answering data setthis is where you multi-have questionanswering we'll get into it more quicklybut I think maybe you might want to havea quick explanation as you're looking atthis that's where you have to uh combinelike two sources of information so ifit's did Aristotle use a laptop you haveto first know when did Aristotle livewhen were laptops invented and then PSATtogether into a short answer and theshort answer is short answers are greatbecause then these exact match F1metrics are pretty good whereas you knowasqa I I think it's either long formquestion answering or open domainsummarization where you generate areally long answer so these metrics area little they're a little more difficultto really like capture the quality ofthe generated response compared to thegold annotation but anyways so what theyfind that's interesting is Boom hugeresult by using the Next Generation asthe query which is you know that's kindof the whole idea of this so definitelybefore reading this paper and I stillthink this is a very interestingtechnique self-ask prompting wasdefinitely probably the you know theactive retrieval augmentation that's howI I would think of it personally so theidea behind self-ass prompting is whenyou have a question like who livedlonger Theodore Hager or Harry VonWatkins you prompt it with are follow-upquestions needed here and then if thelanguage model outputs yes you then havefollow-up and then you have the languagemodel generate the follow-up questionhow old was Theodore haker when he diedintermediate answer and then so it willyou know retrieve with this and then itwill I always imagine that this kind ofcould work asynchronously as well whereyou could have the follow-up questionand then maybe do it but say that you dothis sequentially where thenintermediate answer Theodore haker was65 years old when he died and then youhave the next token so it could eitherbe you know so or follow so you look atthese two follow-up or so and so if it'sfollow up then you know another searchquery then another intermediate answerand then so the final answer is HarryVon Walken so this kind of idea of uhself-ass question decomposition has beenquite interesting where you ask thelanguage model hey you know is thisquestion too confusing to just likeretrieve once and then generate or doyou want to break this into subquestions and then have each of thosequestions answered so this is a prettystrong Baseline it's really interestingthat they're kind of putting these twoalgorithms side by side and then youknow seeing what happens so sort ofsimilarly to prompting the languagemodel to our follow-up questions neededhere we also have this idea of searchactions and when we prompt the agents touse these tools this is this is thewhole idea behind tool use you know likethe chat gbt Marketplace or say likeLang chain llama Index this idea or thereact paper is another really great onewhere you tell it hey you have like wolffrom alpha to trigger it say you knowopen square bracket you know calculatorand then this expression and then itwill execute it and you'll see theoutput so so the idea of how you woulddo this with search is you you know opensquare bracket search you know Joe BidenUniversity and then you get the resultsJoe Biden University and then you knowso like a keyword search that kind ofthing so so this is another reallyinteresting bass on there comparing itwith interestingly they're going tocouple these skills so so this is thisis the idea of skills tools I kind ofpersonally think of skills and tools assimilar things well skills is like youwould have like a collection of promptssort of that like are like describinghow to prompt a summarization or how toprompt like a grammatical editing ofsomething this kind of idea whereastools you think a little differently islike you connect to some kind ofexternal thing so skill one would belike an external tool and skill twowould be like an internal thing so skillone and instruction to guide languagemodels to generate search queries andthen you have search related exemplarslike examples so few shot learning hasbeen you know prior to the uh instructgbt the you know the rlhf and all thatthe whole idea of language models wasthat you would have to give it like fouror five examples of the task and then itcould fuse shot in context learn thetask from those examples it wasabsolutely remarkable that it was ableto do this I don't mean to downsell theability of this but so a lot of thistool use stuff people are still thinkingabout prompting it to uh you knowprompting it how to use the tool so forexample if I'm you know if I'm trying toprompt my large language model to writeweva queries or write weeviate schemasay I would give it like a coupleexamples of what we gave schemas looklike and then it could attend to thatand then learn the skill of how to writea schema or how to write a weba query sowhat they're doing is they give it someexamples of when it used a search termto get a better result similarly itgives it examples of umyou know multi-hop question answering somulti-hop question answering this kindof what the task related examples areare like this these kind of examples ofdecomposition which is needed formulti-hop multi-hop is the whole idea ofum you have to like compose facts youcan't just have one answer you you knowyou decompose so did erisa or who livelonger at the example on the screen islike you know okay how long did this guylive how long did this guy live andthat's how you would reason about wholive longer so so this is the prompta little bit of an interesting detail isthat in their algorithm they introduce abit of extra control by explicitlyboosting the probability of the open uhbracket when they want it to search andthen or you know down weighting it whenthey when it's just completed a searchand you don't want it to search againyou're like you know generate you justsearched so to dive a little furtherinto what this looks like uh promptinglarge language models to have the skillof using a search API as well as theskill of thinking step by step so theauthors they showed the like how theyprompted with this for uh the so theyhave four data sets that explore youknow two Wiki multi-hop QA strategy QAand asqa so uh so let's look at thisquickly so you know what what you do isuh you you have this kind of this againis how you search you have this it tellsyou to do this action and then as you'redecoding from the language model if yousee it do this you know open squarebracket and then you know you're lookingfor this kind of thing if you see likethese two in sequence that's usually howyou would decode a tool use sort ofbecause you could have other kinds oftools like calculator or python codeexecutor or you know any kind ofexternal API all these ideas so so youknow you give it some examples of butwhat are the risks during production ofNano materials and so answer with searchsearch nanomaterial production risk soso you give it a few examples of how touse search for some kind of you knowinformation heavy query like metforminis the first line drug for what so thenyou have these examples of thinking stepby step so this is this Chain of Thoughtprompting that has been super effectiveis uh showing the language models how tokind of break down a question so one ofthe director of the film of the filmhypocrite film die so the film hypocritewas directed by Miguel Mora Ada Miguelmoreta died on 19 June so you knowyou're showing it how to take apartthese questions and particularly whenyou're doing this task of multi-hopquestion answeringthis is a really useful skill to have soso then you kind of combine it by justanswer with step by step in search withthe new question and then you know yougive it a little more prom now combinethe aforementioned two skills firstwrite out the reasoning steps then drawthe conclusion where the reasoning stepsshould also utilize the search API youknow search whenever possible so so it'skind of interesting how you're thencombining these skills in the end younotice that the few shot props datathese don't have search in them sointeresting detail to this okay soso then you know you have all theseexamples soanother interesting thing is whether youwant to retrieve you shot examples is aninteresting idea I haven't really divedit into it too much but just somethingto know is you might want to retrievefew shot examples that are similar inthe semantic space to the current inputnot sure how well that works so here'sanother example of the strategy QA sogenerate yes or no to the font questiondo hamsters provide food for any animalsand then you show it how to do this kindof step-by-step reasoning and then thefinal answer is no so so strategy QA ismaybe the best Benchmark we have in thispaper because it is going to be yes noat the end of it which the reason I saythat's the best Benchmark is becausethen you can easily do the metrics Ithink like you know exact match ofquestion answering is a bit toughbecause you could you know phrase thecorrect answer in multiple ways for alot of questions that's kind of thewhole idea behind that asqa data setthey explore is that uh if you say likeuh where is the where did thePhiladelphia Eagles play you could youcould have like different granularitiesof how you answer that like you couldname the stadium that they play in youcould name like the city they play inthat kind of thing so so yeah so I likethe yes note anyways so the point ofthis is here are examples of what theseproblems look like for how you prompt itto use skills and how to kind of do thisfew shot reasoning so what I would saythat people taking away from this ifyou're you know building out so sayweeviate and large language models isthat for your task it probably makessense and you can probably boost theperformance a little bitby having these kind of few shotexamples of a skill but then there's thetrade-off of uh you know of the inputwindow length so there's we'll talkabout this at the end there's a lot ofhype now about the Mosaic 65 000 inputwindows or the anthropic 100 000 inputwindows and so what that might let youdo is really pack the inputs with allsorts of information a ton of theseexamples of how to use skills as well asretrieval augmentation it is so excitingthis idea of longer input windows but asof right now most of the most of thecases of large language models and allthose models they're probably going tobe expensive or not as strong as themodels that are trained densely with the4096 token like so right now you shouldbe mindful of how long these are goingto end up being in your prompts okay soone of the most interesting details ofthe paper are what data sets did theyuse to say that this retrieval augmentedgeneration technique is you know betterthan another so we have four long formknowledge intensive generation tasksslash you know data sets forbenchmarking these kind of systems firstup we have two Wiki multi-hop QA thisyou know multi-hop question answeringmulti-hub question answering CommonSense reasoning long form questionanswering your open domain summarizationthese are broadly described as tasks indeep learning literature meaning thatthey follow a similar kind of inputoutput framework such that you could youknow you have multiple data sets kind ofdomains that follow this sort ofannotation interface so multiphap QAmeans that you're combining you know atleast two facts to form the final answeryou know again who lived longer this guyor that guy you need to combine the twofacts of when they each lived strategyQA Common Sense reasoning quite aninteresting Benchmark for getting likesort of sort of say like the generalWorld understanding of these languagemodels that's where you you know you aska question like will an apple float inwater or like what happens if I toss anapple up in the air so that kind of likeCommon Sense physics like density andgravity so you know it's things likethis like you know that kind of Generalsanity the language model so to say asqais long form question answering and thenWiki ASP is open domain summarization sothese two data sets are quite difficultto measure the final result of so youknow if I tell you to write me an eli5summary of I don't know the densitything like how does density work whatwhatever the question is like what'swhat's the summary of this paper thatwe're talking about it's going to behard to exactly compare the generatedsummary with say like the humanannotated or self-supervisedbootstrapped gold summary so you knowsay you're doing the long form questionanswering of what is active retrievalaugmented generation about that's thequestion and then you need to write likea paragraph and say you compare thatwith the abstract of this paper as theground truth what you would do is you dothings like engram overlap you know howmany uh tokens between the generatedanswer to that question of what is thispaper about and then the abstract howmany of those tokens do they share andthen that's kind of like what exactmatch that's what F1 Precision recallthey all capture that kind of thing butwe're gonna get into a coupleinteresting metrics they use as welllike uh disam ambiguated uh aredisambiguated uh question answering aswell as this uni eval for you know maybeputting them into Vector spaces andusing semantic vectors to comparegenerated responses with ground truth alot of interesting ideas there sostarting off with two Wiki multi-hop QAthe way that these multi-hop questionanswering data sets are constructed istypically use the notion of relationaltuples so similar to you know like uhgraph database technology where you haveentity relation object you're going totake out those kind of triples and thenyou try to you know chain them togetherwith some maybe like a transitiveproperty or something like that so soyou're keeping these uh triples so thisis like the transitive thing where youhave you know a relation b b relation Cso then a relation C stuff like that soyou extract these relational tuples andthen you chain them together and kind oftranslate them into natural language andthen that's kind of like that's like howthey create these data sets to Benchmarkhow well systems can do this likelogical chaining of facts but but thennot in the you know that most of thesesystem like a baseline of how you woulddo this would be translate theparagraphs from the retrieved evidenceinto all these tuples and then you knowmaybe have like some intermediate stepof chaining the tuples but generallywe're trying to see if the languagemodel can just do it without Thatexplicit kind of relational extractionso this is kind of a look at what kindof things the multi-hop reasoning uhCompares so you know comparison questionyou have these relations like you knowyou would parse this out into a relationlike lived lived is the relation forthis song and then you have comparisonwho lived longer when they share thatkind of relation you might need somekind of diction like dictionary of whatrelations you can stack like this socompositional question is inferring thebridge entity to find the answer so whydid the founder of versus die so youneed to bridge this concept of versuswas the founder of uh or Gianni Versacewas the founder of versus and then hedied like this so you need to bridgethat relationanyway so this is this is the ideabehind these multi-help questionanswering data sets so it looks like thestrategy QA data set which is labeled ascommon sense reasoning has a similarkind of uh strategy for construction asmost of these multi-hop questionanswering data sets with a keydifference being that all the answersare collapsed to yes no uh it seems likethere's another interesting detail tothis which is about implicit facts andthen or this kind of like multi-step andthen implicit properties of the strategyquestionsso as an example you would say aresharks faster than crabs and for thisone you explicitly need the reasoningsteps of how fast their sharks how fastare crabs and then is one thousand twowhereas if you ask it something like aremore watermelons grown in Texas than inAntarctica you might not even need tobreak that up into the steps because youyou can just say like you know theAntarctica doesn't support growingwatermelons so that's kind of the idea Ididn't dive too much into the details ofthis but maybe if you're interested inpausing this or Consulting the paperitself to find out exactly how this kindof data set is constructed but Iinterpreted it to be similar tomulti-hop question answering with theyes no and then yeah maybe somethingelse for how particularly they say thatthese this is common sense like I'mcurious like you know would Hayes andOsiris hypothetically compete for realestate in the Underworld well okay so soyeah that's common sense because realestate in the Underworld I don't know Imean anyway so so the data set this iswhat it's trying to capture how well thelanguage models do and you know howeveryou set up the system to answer thesekind of questions so the next data setis measuring a long form questionanswering is asqa which stands foranswer summaries for questions that areambiguous so basically the idea is ifyou ask a question like who is the rulerof France until August 2nd 1830th andthere were two rulers of France duringthis time so so you could kind of haveeither like the summary could say eitherthing and kind of be correct so so it'sabout kind of disambiguation it's a youknow pretty interesting annotation dataset so uh yeah so that's as much as Ireally looked into this doing this kindof thing of answering these questionsthe the general thing I'd say to takeaway is that it's you have an annotationof long-form answers I guess it's allit's further compounded by this thisambiguation part of the data set so youknow definitely interesting if you'rekind of doing a tour of all these datasets so then rounding out our tour ofthe data sets used in this paper we haveWiki ASP so Wiki ASP is aspect basedsummarization so the key is you know ifyou say write me a summary about BarackObama that's pretty vague compared towhere if you break it into write me asummary about Barack Obama's early lifeand career Barack Obama's presidency orBarack Obama's Legacy so you have thiskind of notion of a particular aspectbased summary so you you know you kindof reduce the search space of thesummarization and what it could entailand I think that's a pretty interestingdetail for how you construct these youknow long summarization data sets so allin all here is a summary of theevaluation data set so even when theyhave more than 500 examples they willsubset the data set to 500 which in thepaper they describe is about the cost ofrunning these experiments you knowobviously these large language modelinferences aren't cheap and you want toablate the um like the impact ofprevious next the particular Alphaparameter for the umyou know Alpha parameter for when to usethe next sentence as retrieval as wellas a beta parameter for the masking onthe implicit query so anyways it's likeas you want to Blade all these detailsyou can't be doing it with like 200 000questions so you know we have themetrics that are used and then theCorpus using Wikipedia for three theopen web and the Bing search engine forthe wiki thing uh and then using youknow bm25 or like some kind of searchengine configuration so say it's like ahybrid search say it's like a you knowbm25 Vector search Hybrid search butthen also you have like symbolicfeatures that boost it like you knowwhat time the article was these are allthe kind of things that would be likeabstracted into a search engine likeBing or if we imagine kind of puttingweeviate into into the retriever here weV8 would be more like that where you youcould you can bm25 with it you can alsolike combine all sorts of things forranking search results uh so then thetop case how many of the results are putinto the prompt so two three three fivethe number of examples that are usedwhen you're giving it examples of how tothink step by step for a given task andthen whether you're doing that or not soyou only do the thinking step-by-stepexamples for the multi-hop questionanswering data setokay evaluation metrics so exact matchis you know what is the atomic number ofoxygen eight is the answer and then doesit exactly match that so this is thething where yeah so you know that kindof thing take it how you will it's it'sa pretty good metric for a questionanswering when it's a short answer andit's something like eight that's apretty good example but if it'ssomething that could be more abstract Ithink it's a it's not the it's a metricthat could use a little work it worksgreat for when you have yes no exactmatch so you know with strategy QA yousay so the final answer is yes no andthen you can exact match it you know andthen it's just like you know binaryclassification so token level F1 namedentity F1 so F1 is uh like a Precisionrecall waiting so uh you know Precisionrecall these are like um these are likefalse positive metrics generally whereuh Precision is saying like how many ofthe correct entities were were generatedand then recall is like how many of ourPrecision is how many of of thegenerated named entities are correct andthen recall is how many of the correctnamed entities were generated and thenan F1 score is a is like Precision timesrecall over Precision plus recall some Ithink it's like a geometric mean ofthose two metrics so token level F1 iswhere you do that for each of the tokensin the um in the gold answer and thename entity is where you only do thatfor the named entities in the gold scoreso the gold summary is the humanannotated they or just like the sourceof Truth where if it's like you knowBarack Obama was the 45th president ofthe United States and then so you youdis named entity F1 you disregard wasthe and you just use Barack Obama orlike 45th those kind of things so thenuh Rouge is this likeyou know Rouge is like the blue scoreBleu where it's like it's like a way ofdoing engram overlap between longpassages of text soso yeah so that's that's basically whatit is uh so disambiguate F1 uh so whatyou do with this is you generate asummary so you generate the the longsummary or the long answer to thequestion and then you apply anextractive question answering model likeRoberta on that and you see if it canget out the uh the the like the nameentities I think from the generatedsummary and so you have that kind oflike intermediate of you evaluate thegenerated summary by applying a uh likean extractive question answering modelon it and seeing what it is able to getout of that uh and then we have this unieval thing which I think is interestingenough to get its own slide so forexample when Google was evaluating theirMinerva chatbot originally they had allthese different dimensions of how theyevaluated the chat pod chatbot like howentertaining is it how factually correctis it so you have these like dimensionsof evaluation similarly we had thispaper called checklist which was likeyou know it was more of like arobustness test like does it do wellwhen you add negations when you swapnamed entities so it's like a checklistof General language understanding andthat's kind of I'd say uni eval to melooks like checklist kind of where soyou take the generated summary and youhave these different dimensions thatyou've trained task specific evaluationmodels they describe more about how thisis unsupervised so you know I didn't getinto details of that but I highlyrecommend checking that out so you saybasically you know how coherent is thissummary you know if you're evaluatinglong form or open domain summarizationor like is this summary relevant to thereference so you know looking at thesedifferent dimensions of a summaryoutputting the score and so yeah apretty interesting way of looking atlike multiple linguistic properties ofthis generated summary so prettyinteresting evaluation technique forthese long you know when you generatelike a paragraph of text how are yougoing to evaluate that so follow in thedescription of the algorithm whatthey're going to be ablating the datasets and then the metrics let's get intothe results that they find so startingoff with this is kind of the openingresult where you compare the flaretechnique with previous window retrievalshown in yellow and then singleretrieval so retrieve once and thengenerate so we get a huge benefit onmulti-hop question answering of the 51with the exact match so you know exactmatch of that final question of thatfinal answer 51 compared to 39 which isa pretty big Improvement uh the strategyQA doesn't seem to be as much of animprovement than the asqa you knowpretty minor there as well but when youhave the hint so the hint is where yougive it the disambiguation like thesummary should be about this particularwho uh like King of France in thatexample from earlier and then you knowperforming about the same on the aspectbased summarization but probably themost interesting test in my view is themulti-hop question answering becauseit's probably the most well understooduh you know data set and metrics forthis kind kind of thing so this abilityof the active retrieval to have thisImprovement when you're kind of tryingto like answer these complex questionsthat require retrieving from multiplesources or like asking new questionswhile you're generating that's quite aninteresting Improvement probably thereal question here though is how welldoes flare compare with self-askprompting where you explicitly prompt itour follow-up questions needed here youknow yes and then follow-up questions sowe do get a better result with flair itdoesn't seem to be a super you know waybetter kind of result but it'sdefinitely interesting to think aboutwhat's more efficient between you knowlooking at the log probabilities of thenext token doing that returning the logprobabilities it's very interesting aswe saw earlier that now open AI isletting you see that and you know andthen so it's going to be super quick tojust multiply those together and thensay if it's less than the threshold youknow compared to the our follow-upquestions needed which is more of ablack box algorithm so it's prettyinteresting when you use the logprobabilities to do this activeretrieval or whether you just kind ofblack box it and prompt it to ask it ifit wants follow-up retrieval so you knowthere are definitely a lot ofinteresting questions this is this isprobably the debate that I think is themost interesting with this of the futureof this kind of active retrievalaugmentation so some additional detailsthey explore they also look you knowdoing this across the data set so thisis again this is again showing you moreof the details behind how these plotsare produced so you see the differentmetrics and these different data sets noretrieval single retrieval or theprevious window so flare performing thebest but you know not too much on theseother data sets which is prettyinteresting but but again these datasets they're you know asqa and Wiki ASPas we saw these are like these longgeneration benchmarks and so I don'treally think we have the greateststandardization of data sets and metricsfor that kind of thing yetuh so then what they're ablating is whento retrieve so this this would be aninteresting hyper parameter if youimagine as we build this into alleviateyou'd have some kind of hyper parametercalled like Theta like similar to the umthe alpha in weaves hybrid search wherethe Theta is saying when like whatthreshold of uncertainty to do thisextra retrieval inso what they find is between 40 to 60percent uh uncertainty works the bestand more than that they find to beactually distracting because you're nowlike retrieving every single time yougenerate something and they find thatthat doesn't work as well as someoptimal intermediate value of uhcertainty another really interestingdetail they explore is that idea ofmasking the implicit query so whenthey're going to be doing that implicitquery idea which if I can find the slidethis is this idea of you you know youtake the query is going to be just thenext sentence and then you're going tobe masking out the uncertain words andthen you have a hyper parameter of withwhat log of probability do you uh maskout the word so they find a bit of avariance with this so you know you knownot mattering too much but showing thatthis 40 thing I guess is the best withmasking out those tokens that you sendthat over so after reading this paperhere are some of the reflections that Ihad on this General concept of activeretrieval augmented generation andparticularly this algorithm soparticular to this algorithm I'm verycurious about sort of the generalizationof alphago mu zero this kind of liketree search look ahead with textgeneration so one idea would be you knowas you're decoding the next sentence inlanguage models you could have severaldifferent potential next sentences thathave a pretty high total probability soI think it'd be very interesting to youknow Branch out this retrieval augmentedgeneration and have it go along theseseparate branches of the tree to dothese generations and then you know atthe end so you have the you know theforward look ahead kind of the MonteCarlo tree search exactly how it is inalphago and the game playing AI is whereyou you know you're looking ahead togenerate and then you kind of see howthat works at the end so I think it'svery interesting to see that kind oflook ahead search in text generation sothe second thing I think is reallyinteresting is just large language modelevaluation I'm very very interested inthis um in this software frameworkcalled chat Arena there's another onefrom UC Berkeley that I don't rememberoff the top of my head but basically theidea is that language models are givenlike an ELO rating similar to like likevideo games where they chat with eachother and then you have like a thirdlanguage model to decide like okay he orthat language model was smarter in thisuh debate or conversation so it's Ithink it's a very interesting way toevaluate language models it's you knowmore hand wavy than the benchmarks whereyou annotate you know ground truth andcompare it but I think it's a littlemore realistic for how people are youknow using these kind of systems andit'll be very interesting to see howthat kind of evaluation work evolves soas kind of my analysis of the paper wasI think when you have short answerquestions and you can do exact matchlike the multi-hop question answering Ithink you get a really great Benchmarkfor that kind of thing and then whatthese benchmarks can do is they capturethe long tail well you can see like thefailure modes when you create like ahundred thousand examples and then runit and then you can see like okay it'sbad at this particular distribution ofquestions what's the similarity and thenyou can maybe add data to fix it thatway but I think more so this broad likechat with each other who wins the chator like I don't know I think the spacesome evaluating language models alsoevolving is in a very interesting waythe third thing I think is extremelyinteresting is just this continueddevelopment of large language models sofirstly we're seeing the latest news isanthropic has a 100 000 inputs and thenyou know Mosaic ml they have their MPTwhich is 65 000. soyou know I think that we're going to beable to retrieve so much information toput in the context and you know I youcould say that I'm biased by working ata vector database and saying this but Ithink it's unlikely that the best way toretrieve is just to put like somemassive document of the same contextinto the window I think there's stillNuance to how do we use retrieval so youknow is the best way to say it's not thegreatest specific question but whenyou're asking that question of uh wheredid Joe Biden what what degree did JoeBiden do should you just look at theentire Wikipedia article or maybe yeahit's not a good example but like everylittle passage from everything on theinternet about Joe Biden's experience incollege if that makes sense like ifyou're looking at the thing of like whatis reftubec how do I use it for my apprather than just taking the one blogpost from we've ate on rustavec if youlook at like these Snippets like coderetrieving from every code base justlike you know retrieval from everythingthat uses ref I still think that's abetter way to like pack the problem withinformation and then I also think this100K and inputs it'll be veryinteresting how that kind of skilldescription so we saw these like fewshot examples of how to use a skill likeyou know compositional reasoning soyou'll be able to pack in like a ton ofexamples of how to use skills and it'llbe interesting to see if that kind oflike search action gets better withgiving it like I don't know 20 examplesof how to search but and then I alsothink this Laura thing is people arefine-tuning their models and theirdomains that's just another reallyinteresting thing to be looking at howwill that impact retrieve augment togenerationso then just the final thing of how dowe use retrieval I'm pretty curiousabout like yeah just generally like umyou know when it's retrieving so thatthis is the first this is one of thefirst things we're seeing on like thislike active retrieval you don't justretrieve one so yeah it's veryinteresting like as we have this kind oflike Auto gbt style you come up with anaction plan when in the action plan areyou going to do more retrievals and Ithink it's this is very interesting thatthis kind of like we like this idea ofgenerative feedback loops where you saveintermediate Generations as well so likewhen would you then want to retrievesome kind of intermediate generation asyou're like in the middle of executingsome wildly complex task so anyway so sohere are some takeaways I hope you findthis interesting thank you so much forwatching this explanation of activeretrieval augmented generation the flaretechnique uh please check out wevia onweave.io or GitHub weviate or Twitterwe've yet AO or please leave a like onthis video if you like content like thisit really helps us know that uh you knowcreating paper summary videos issomething that people like and want tosee more of and we're always on the huntfor these kind of new algorithms andexperiments that interface with you knowVector search and retrieval wintergeneration so please leave a like if youlike this video please subscribe to thechannel for more videos like this andthank you so much for watching", "type": "Video", "name": "Active Retrieval Augmented Generation (FLARE) Explained!", "path": "", "link": "https://www.youtube.com/watch?v=IVYwLLDABzI", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}