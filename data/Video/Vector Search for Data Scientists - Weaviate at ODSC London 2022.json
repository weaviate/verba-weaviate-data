{"text": "How can the latest advances in Vector Search help Data Scientists? As a Machine Learning content creator for a few years, I have ... \nhey everyone i'm really excited to bepresenting our talk at the latest opendata science conference in london as apart of our vector search track i waspresenting vector search for datascientists i really hope you find thetalk interesting and a huge thank you tothe open data science conferencecommittee and team for all sorts ofthings from guiding me through how touse the webinar software and generallyaccepting me to be a speaker at theconference thank you so much this wassuch an exciting opportunity and toeveryone out there listening i reallyhope you find this talk interesting thispresentation will explore the use ofvector search for data scientistsincluding a case study with twitteranalytics so we'll begin thepresentation bylooking at how i'm defining data sciencevector search and twitter analytics casestudy before diving into the details oflinking these things together so whatare some common questions in datascience we typically have some kind ofthing that we're observing like thenumber of impressions that we're gettingfor our tweets or say views for youtubevideos and we want to look at questionslike how is my data distributed is itnormally distributed power lawdistributed are there particularoutliers in my data and the thirdquestion that i really want everyone tofocus on with this talk is the questionof are my variables correlated with eachother and trying to explorerelationships between our variables andseeing how they impact the distributionof the thing that we're the mostinterested in so say we're mostinterested in the distribution ofimpressions on our tweets we also wantto look at variables like what time thetweet was sent or whether the tweetcontains a url to try to see if that cangive us some insight about thedistribution of impressions on tweets invector search we're looking at a new wayof search where we represent objectswith vector representations extractedfrom deep learning models so the keyquestions are how well can we capturethe semantics in vector representationsand what can we learn about our datafrom the semantic clusters that areformed by these vector representationsso this tying these together datascience and vector search is going to bedone with the example of twitteranalytics i've beenmaking content on youtube and twitterfor a while now and i've been reallyinterested in this kind of problem oftrying to figure out the best way to dothis messaginghow to really kind of you know promotethe this content and make it successfulso i'm looking at my own twitteranalytics data to try to get a sense ofif i can segment it using vector searchto get a better sense of thedistribution of the metrics i care aboutlike impressions or url clicks and seeif there's any insights i can gain fromapplying vector search for data scienceand understanding twitter performance soif you're interested in performing thisanalysis yourself you can easily go tothe right top right corner of thetwitter analytics dashboard to downloadthe csv files you can use the calendaricon todownload up to i believe four months ofdata and then you can just uh you knowas this presentation will explain youcan easily upload it into wev8 to enablethese vector search functionalitiesso the twitter analytics it'll give youthe csv data file where you havedifferent columns like the raw contentof the tweet itself as well as somefeatures like what time the tweet wassent and the metrics that wereinterested in like impressionsengagements engagement rate retweetsreplies likes user profile clicks or urlclicks and the interesting one the mostinteresting detail about applying vectorsearch to this is understanding thisidea of having semantic representationsfrom the raw unstructured text itselfrather than approaches like featureengineering where maybe we try toextract features like whether the tweetcontains an emoji what the charactercount is word count or if it containscertain keyword phrases like weaviate ordeep learning and see what kind ofsegmentation we can do with those kindsof features that we manually engineer sowith that primer on how i'm looking atdata science how i'm defining vectorsearch and overall looking at thistwitter analytics problem thispresentation is segmented into five keysections the first section issegmentation and data scienceunderstanding how we split distributionsbased on other attributes the second keytakeaway is to understand vectorrepresentations of data how do deeplearning models produce vectorrepresentations of real world objectsthat capture the semantics in them thethird key topic is vector segmentationusing these vector representations tosegment our data the fourth key takeawayis this particular case study of usingwev8 for twitter analytics and applyingthe intersection of the ideas and vectorsegmentation for twitter analytics usingthe wev8 vector search database thefifth key takeaway are researchquestions and discussions aboutcontinuing this kind of research andexpanding on the capability of vectorsearch and its applications in thingsthat data scientists care about thefirst key idea is to understand the taskof segmentation of analytics and datascience so in data science we typicallyhave a distribution of values ofsomething that we care about now for theexample of twitter analytics i'minterested in the number of impressionsthat each of these tweets receive sohopefully i can gain insights into whattopics to tweet about what particularlanguage andfeatures like that that might lead totweets that receive more impressions sofollowing the csv export of the tweets iplot my impressions in a histogram and ican see the distribution of theimpressions so as i look at this i seemost of the tweets have you know in thiskind of mid-level of impressions andthen we see some that go into havingmany impressions although there arefewer of them so you know we're justplotting the the histogram bins of thenumber of impressions and then the countof how many tweets fall into each ofthese buckets so when we have adistribution of this the task ofsegmentation is to try to take apartthisvisualization to see how it varies withrespect to splitting it on certainfeatures so as examples of thesefeatures that we're splitting we mightask questions like what time was thetweet sent say we sent a tweet in themiddle of the night does that result invery low impressions or say we you knowsent our tweet right at 10 in themorning on mondaydoes that result in a successful tweetthen we might ask is there a url link inthe tweet does maybe trying to pointpeople off of twitter is thatyou know causing the twitter algorithmto not promote our tweet or somethinglike that and that'll be our transitionfrom understanding thesesplitting on these symbolic attributescompared to these vector attributes thatare extracted from deep learning modelsfor segmentationso the first question i asked is whattime was the tweet sent so from thetweets i can extract this feature oftweets sent before 12 plot theimpressions of those tweets and then ican look at the impressions on tweetssent after 12.and kind of one of the problems withthis is you see how the the count isthey're much more tweets sent after 12and before 12 it's difficult to kind ofautomatically extract some insight fromthat question similarly we have thequestion of is there a url link in thetweet and looking also at thedistribution between these two values sothis example of splitting our data basedon the symbolic attributes that arecontained in the datathese kind of attributes like the timeor extracting a boolean value from theurl clicks in this presentation we'retrying to see if we can splitimpressions or whatever metric we careabout based on the semantics of thecontent itself and expand onsegmentation from symbolic attributes tovector representations of the highdimensional objects that the metrics aredescribing so the raw text of the tweetitself or as we'll also look at whendescribing other examples saymovie transcriptions podcasttranscriptions uh text and scientificpaper or images of e-commerce productsthis kind of data that we can formvector representations from and thensplit our analytics based on theclusters in this vector space so forexample without manually annotating thetweets we can segment them based ontheir vector distance to the phraseswe've a podcast we vva tutorial and aiweekly update to split the tweets basedon what they're about with with respectto these kinds ofcategories without actually manuallylabeling them we can similarly do thisfor say natural language processingcomputer vision or say robotics we canuse these kinds of distances to thesequery points to automatically formcategories and clusters that we can useto split the impressions of the of thetweets we perform segmentation to splitthe metric that we care about likeimpressions on tweets or say clicks onan ad and now what we want to do is seeif we can segment these analytics basedon the semantics of these highdimensional data objects like raw textimages code even audio videossay graph structure biological sequencesand really anything can be encoded intoa vector representation like this andthen we form clusters of the vectors tosegment our analytics and see how theseclusters vary with respect to themetrics like impressions or views and soon to summarize the first pillar of thepresentation is about understandingsegmentation and data science wevisualize the distribution of our datato get a sense of it so for example wesee that the impressions on these tweetsare roughly normally distributed andthen we ask questions with respect tothe features that we have like is thisdistribution also the case for tweetsthat are sent at say three in themorning but now what we're looking attrying to do is say what about tweetsrelated to the the topic of deeplearning for robotics and we're notgoing to determine that a tweet is aboutdeep learning for robotics based onkeyword matching with say it containsthe exact phrase robotics or deeplearning for robotics we're going to tryto encode tweets into a vector space aswell as the phrase deep learning forrobotics and use the nearest neighborsearch to determine tweets that have thesemantic similarity with deep learningfor robotics and then use this to get asense of the twitter impressions abouton tweets that are about that particulartopic of deep learning for robotics orsimilarly say deep learning for biologyor all these different kinds of topicsthat i've been exploring as i've beenmaking content about topics in deeplearning the second key idea is tounderstand how we can represent realworld objects with vectorrepresentations so we'll start by kindof looking at different ways of encodingthings so to say so looking at symbolscompared to vectors so some commonsymbols arecategories so say we havelabels about vehicles and we haveships trucks cars motorcycles and we usea one index to denote which categorythis particular thing is so say this oneindicates that this is a an example of acar vehicle so this is an example of howwe represent categories typically inthese one hot encoded vectors similarlywe have variables like numeric valueslike how old someone is oryou know these metrics are numericvalues like impressions these numericvalues we can use thresholds likegreater than less thanintervals to to segment our data and getsenses of it based on these numericattributes and similarly we have booleanvalues true or falsevectors are say n dimensional objectswhere we have values in each index ofthe vector that that representssomething so this 0.1 0.8 0.34 this is avector thatyou would say it say has a direction amagnitude but usually we normalize thesevectors so they all have a uniformmagnitude but it's really kind of thedirection that is captured that it'shard to kind of reason about directionof these high dimensional vectors butthat's sort of what we're looking atwhen we're looking at comparing thedistances between these vectors as we'llexplain further in the presentation sothe key idea behind vector search iswe're using these vectors to representhigh dimensionaldata objects that are otherwise reallydifficult to kind of compress into asingle representation and having thiskind of flexibility with thisn-dimensional vector interface lets usstore the semantics of thehigh-dimensional objects so say we takepictures like these high resolutionimages of golden retriever puppies andsome computer vision model is say gonnathere are two things that generallywe're looking at maybe the computervision model is going to classifywhether this is a dog or a cat andbefore it makes that final binaryprediction it's going to have thislatent representation of the dog at saylayer 8 of the deep neural network andthen we're then we're going to extractthis vector representation to representthe puppy image overall and store it inour vector search database like what wev8 is but another way of doing this it'sbecoming more and more popular is todirectly optimize for the vectorrepresentation itself so as we'llexplore further in in this particularsegment of the talk we're looking atpredicting these vector representationsfor different images and trying to makesay positive pairs like these two goldenretriever puppies similar to each otherand say dissimilar to other images likea brand new car image or something likethat that would be different from thesegolden retriever puppies so we'll returnto the details of exactly how deepneural networks are producing vectorrepresentations but let's stay a littlefurther on what we're doing with thesevectors once we have the vectors so saywe have some deep learning model thatmaps from images to vectors once we havethese vectors we can use vector distanceto determine semantic similarity so acommon vector distance metric is l2distance you just loop through eachindex in the vector and then you justsquare the difference of the two sumthat up and that's the l2 distance sosay we have these three dimensionalvectors that describethis puppy this puppy and then let'spretend that there's an image of anairplane so we would take the l2distances between the two puppies and wesee you know 4 minus 2 squared is 4 plus1 squared 1 plus 1 squared 1 is 6 andthen we'd also do the vector distancebetween the first puppy and the airplaneso the vector distance between thevector 4 8 10 and 1 20 20is a much higher number than six so wedetermined that the first puppy is muchmore semantically similar to puppy twothan the airplane based on this vectordistance calculation and there are a fewdifferent distance metrics that we canlook at as we kind of mentioneddirection and vectors and say cosinedistance angular distance there are afew ways of looking at distances withvectors but generally it has this kindof form where you're comparingindexes of the vector with each otherso with these kind of kinds of vectordistances we can determine semanticsimilarity but we can also cluster thevectors using say common clusteringalgorithms like uh we could do thingslike k-means if it was in a loweralready in a low dimensional space butcommonly we do things like tsne pca umapprojections from the 300 dimensions sayinto this three-dimensional space to geta visualization of of our cluster so wesee how we have the chicken the image ofthe chicken the wolf the dog the catthey're all kind of similar in thevector space whereas they're dissimilarfrom say this google and apple logo oractualapple and then banana so we see how wehave these clusters in the semanticspace and so this is just the key ideato this presentation is understandingthat we take a bunch of tweets from mytwitter analytics data and then we putthem into this embedding space such thatwe have say the tweets about computervision topics the tweets about naturallanguage processing topics and then wecan further look at and then we take thecluster out extract the impressions dataurl clicks then we have a new interfacefor looking at the data science ofseeing how these metrics are distributedbefore graduating from this slide it maybe worthwhile to stay a little more onthis idea of why the talk is titledvector search for data science sayrather than vector clustering for datascience so when we have these clustersit might not really be useful to justlook at the cluster and imagine it's acluster of like 500 objects or somethinglike that we want to kind of havesemantic names for the clusters so saywe had done the search query animals andthen animals takes us to this cluster ofnearest neighbors that interface forsegmenting the analytics is i think moreproductive than just trying to just takethe clusters and say do the k-means oryou just have these clusters that arecloser to these centroid points and thenjust doing your analytics from there ithink it's more useful to be rememberingthat vector search part of it where youhave animalsbrands or fruit and using those searchquery points to provide like a semantictitle to the cluster so to dig into thisidea of vector representations ofobjects a little further you might beinterested in how these index positionsinfluence the representation of theseobjects can we say that say index zerosolely determines how much of a brand ifwe're saying these are this is the brandclustered is say index zero of thisvector is the density of that particularindex determine how much this is thatand the the thing about this is it's apretty interesting kind of field ofresearch and deep learning this idea ofsay disentangled representation learningwhere we can say what each of these uhindex positions and codes and there'sbeen some interesting researchmultimodal neurons from open ai reallystands out as an interesting explorationinto these latent representations fromdeep learning models but generally wecan't justconvert it from the vectorrepresentation right back into symbolicrepresentations of what each indexrepresents rather we kind of have thisuh entanglement of the wholerepresentation and so we need to kind ofrely on these vector distancecalculations rather than kind ofextracting is this index say how much ofafruit it is and so on so anotherinteresting question maybe is how muchcan we compress these vectors and in myview one of the most interesting thingsthat we've done at we've eat is look atthis binary passage retrieval idea whereyou can compressthe values in the indexes from say 32floating point values to binary valueswhich is shown with this green being onewhite being zero to just kind ofillustrate a binary vector andsurprisingly you can still getimpressive retrieval when you do thislevel of compression and there are otherideas like product quantization and saylocality sensitive hashing that try tosay compress the 384 part of it to32d and one of the most rewardingexperiences i've had at wv8 was tointerview eddie and dillocker on hiswork ina n benchmarks and benchmarking vectorsearch at large scales and seeing howcomputing these vector distancecalculations using efficient datastructures how that scales so if you'reinterested in these kinds of questionslike the performance of vector searchand exact numbers for that with respectto how many vectors you have thedimensionality of the vectors and thenkind of high level ideas like thedistributions of the vectors themselvesyou might be interested in checking outour webva podcast so i hope that was apretty clear overview of what we do withthese vectors so we have these vectorsthat are particular kind of way ofrepresenting somethingwe convert from high dimensionalunstructured data into vectorrepresentations we have say vectordistance calculations that form semanticclusterswe explore what's in these vectors andask questions about how much we cancompress them and how much these howmuch the performance varies with respectto the attributes of our data set andthe vectors themselves so now let's diveinto the question of how did we getthese vectors to begin with so this is avery influential paper sentence sentenceembeddings using siamese birt networksthat explains this idea of havingsiamese twin tower networks where wehave a copy of the same neural networkthat produces representation u andrepresent rotation b uh both of thesebird neural networks share the sameweights commonly there are someframeworks where it's an exponentialmoving average andit's things like that but generally it'sa siamese copy of the same network andthey both produce vector representationsof two different inputs so sentence agoes through this copy of the networkand sentence b goes through this othercopy of the network and now we comparethe vector distance between therepresentation u and the representationv and we have this loss function backpropagate into the neural networkweights of burt on both sides to updatethe neural networks to produce betterpredictions for these two sentence a andsentence b so the key thing is thendetermining well should sentence a andsentence b are they positive pairs ornegative pairs should they beclose in the vector distance space so togive more of an example of this wesample these points from data sets likewikipediaso often what we're doing is we'relooking at heuristics to self-supervisethe a and b positive or a and b negativescoring such that we can do thisrepresentation alignment from internetscale data and a lot of these as we getinto this idea of zero shot pre-trainedmodels they're trained on internet scaledata such that theyhave this amazing generalizationproperty so we start off with thingslike this query point so here's anexample of a paragraph from wikipediathat describes the miami heat mbabasketball team now what we determine isthat the positive pair is the nextparagraph in the wikipedia article sonow we led by dwayne wade and followinga trade for former nba mvp shaquilleo'neal this next paragraph continues todescribe the miami heat basketball teamso we'd want to take these twoparagraphs from wikipedia and thenencode them into a into vectors thatminimize the semanticdistance between these two a vectordistance between these two and we alsohave negatives like this article aboutdeep learning so we would want to have aclose semantic representation of thesetwo paragraphs about the miami heat andthen a dissimilar representation of thisparagraph about deep learning inaddition to this idea of using heuristiclabeling like saying that the followingparagraph is going to be a positive pairof thewikipedia article we also look at thisnoising or masking interface to formpositive pairs so data deveck is a veryinfluential framework showing how we cando this positive pair construction forreally any kind of data domain becausethis idea of applying a noise mass cangeneralize to any kind of data that youmight have so for example we take animage we apply this mask to it we have aspeech file audio file and we apply thisto it language mask out with and andthen try to align these tworepresentations soa couple of ways of looking at how weform these sentence a sentence be imagea image b video a video bany kind of thing for these two thingsthat we're trying to encode into thevector spacesso that's how we construct theoptimizationtask to optimize the weights and thusoptimize the vector representations ofdataso as you're looking at this you mightbe intimidated by this idea of do weneed to train our own models are wegoing to need to optimize a model withthese contrastive learning lossfunctions in order to take advantage ofvector search for data science andwhat's the one of the most excitingtrends behind this whole thing is theanswer to that is increasingly lookinglike no and there are many pre-trainedmodels that work well for a very broadrange of data so from being trained onwikipedia or sayquery passage pairs in the ms marco dataset we're seeing that models trained ondata sets like that are able togeneralize and produce reasonablerepresentations for say my twitter dataand things like that so if you'reinterested in thissentence transformers from hugging faceis an excellent place to get started onlooking at these models that producevector representations off the shelf forall kinds of data domains to summarizethe second key takeaway from thispresentation is to understand vectorrepresentations of datastarting with what the vectors arethemselves and understanding that we cantake all sorts of things like imagestext or code snippets and map them intovectors with predictions from deeplearning models those deep learningmodels are trained to maximize semanticsimilarity between uh between each ofthe positive pairs and they're usuallytrained on massive collections of datalike all of wikipedia or going evenfurther for internet scale data likelike on that kind of order and becausethey're trained on such a massive amountof data we often don't need to train themodels ourselves for particular datadomains to have a reasonable zero shotperformance or a adaptation to ourparticular problem the third keytakeaway of the presentation is vectorsegmentation which brings together theseideas of segmentation and data sciencewhere we split the metric that we careabout based on other attributes that wemight have like what time we send thetweet or whether the tweet has a urllink in it and also this idea of vectorrepresentations of data so to tyingthese ideas together we saw how we canproduce vector representations of textimages code audio video all sorts ofdata can be produced into one of thesevector representations through theinference of a deep learning model andthen we connected it to this twitterexample wheresay we want to split out the tweetsbased on the distance to we va podcastvva tutorial or ai weekly update to geta sense of how these thingscluster and how they're semanticallyrelated so in this section i'd like tomaybe just quickly describe someadditional examples in addition to thetweet categories that i think hopefullywill just kind of cement this idea ofhow we can applyvector representations to segmentanalytics and apply vector search indata science so we'll start witheveryone's favorite task of househuntinghouses in databases like these realestate platforms they typically havesymbols like the number of bedrooms anumber of bathroomssquare feet these would benumeric values and then say we havecities that we might represent as acategorical feature or something likethat so we have you know these kinds ofsymbols that we could split the housessay we want to split the prices ofhouses and we want to use theseattributes like the number of bedroomsthe number of bathrooms to split thedistribution of the prices of houseswith these vector representations we canhave vectors that encode the visualaesthetic of the house say theneighborhood structure and this is inthat idea of graph structuredrepresentations which kind of at thebottom of the list this graph structuredembeddings you see things like graphneural networks or algorithms like deepwalk and node to vect this idea ofconverting graph structure into vectorrepresentations as well is alsosomething we can we can do and isbecoming increasingly easier to do aswell so we can encode the graphstructure of the neighborhoods and thenwe also just have a more flexibleinterface to define features with textso compared to symbols where say we havea feature that is apool does it have a pool one or zeroboolean value whereas with text we canjust kind of describe the features ofthe house in this flexible interface andthen encode it in the vectorrepresentationthat can encode all these things andthen segment things like the prices ofhouses based on visual style orneighborhood characteristics that helpus segment the houses by going deeperintomeaning and semantics of what makesmakes up these real-world objects ofhouses so similarly with e-commerceproducts we have symbols like what kindof e-commerce product it is like if it'sapparel particularly we might have shoest-shirt pants or say colors whereas withvectors we could have a representationof the visual style of the productsthemselves with movies we also havegenre symbols like children action orsci-fi where vectors can encode say takea description of the movie maybe reviewsof the movie to encode things like thethemes characters story lines theseparticular kinds of nuances that makemovies different from one another withingenres and without requiring heavymanual symbolic taggingin scientific papers i think is one ofthe most interestingthings to explore with this as wellwhere say symbols separate papers likethat are related to biology or machinelearning whereas vectors can reallyencode the nuance of the ideas containedin the papers and maybe the writingstyle as well and then finally withmusic i think is another veryinteresting thing wherewe have these high level tags like hiphop or dancebut vectors can encode sort of themusical style and i'm not an expert onmusic but things like this could beencoded with vector representationsusing this kind of way of thinking sohopefully these examples helped furtherclarify this idea of how vectorrepresentations what vectorrepresentations can help us capture inour data tosegment it and better understand it andhopefully going through this list maybemade it more clear how to apply this toyour own data sets so we'll end thissection on vector segmentation with thefollowing quote from francois chile inhis book the second edition of deeplearning with pythonthat's the magic of deep learningturning meaning into vectors then intogeometric spaces and then incrementallylearning complex geometrictransformations that map one space toanother all you need are spaces ofsufficiently high dimensionality inorder to capture the full scope of therelationships found in the original dataso there's a little more to this quotethan just turning meaning into vectorsdescribing how we haveseveral layers of this kind oftensor space processing and thesenon-linear transformations betweeneach space to the next space as youpass through a sequential neural networkbut the key ideain what we're interested in is this ideaof turning meaning into vectors and themagic of deep learning and being able todo so and do so for so many differentdata modalities and use casesto summarize section 3 brings togetherthe ideas of the first takeaway insegmentation and data science and thesecond takeaway of understanding vectorrepresentations of data so vectorrepresentations which are also the alsocommonly called embeddings enable aninterface to split analytics based onthe semantics of the content itself andthis content could be text images codeaudio videos it's really up to yourimagination to thinking about whatyou're going to encode in these vectorspaces so now that we've gone throughthe background of what segmentation anddata science is and how we can usevector representations to facilitatewith segmentation i'm super excited toshow this example of twitter analyticsmining with the we v8 vector searchengine and how we can use vectorsegmentation and twitter analyticsthrough we v8 so as shown previouslywith the export csv twitter analyticsgives you a csv data set like this whereyou get the the text of the tweetsthemselves as well as the time the tweetwas sent and then the metrics likeimpressions engagements and so on so inthis example we're going to be using thetext column to vectorize it with asentence transformer and then we'regoing to use this for nearest neighborsearch to segment the content of ourtweets such as whether it's about theebay podcast coding tutorials or topicslike natural language processingcomputer vision maybe even researchniches like self-supervised learning ordata augmentation and so on to get asense of how the semantics of thisdifferent contentsegments the impressions or url clicksand these kinds of ideas so followingthis schema that you seein this picturethis is how we upload this into wev8through python so in python we createthis dictionary where we have classesand then we name our class tweet wedescribe the class tweet analytics andthen we list the properties of the tweetand we'll get in further to this graphdata model of we've eight and more ofwhat you can do with this but here's thethe very basic ofdefining the property that has the textof the tweet and then say you have theauthor which is another textand you would add the impressionsengagements so on as you fill out thisdictionarychanging the data type to say number orboolean and so on and telling wva not tobother with vectorizing uh text columnslike author be as we'll see later in theexample we're going to add tweets notjust for myself but other people not tojump ahead but we want to tell we've ato only vectorize this particular columnand we do that with this skip parameterin the text event transformers module sohere's a quick overview of the schema inthe python and uh so what i did and ithink this is really interesting is gofrom google collab notebooks to theweev8 cloud service and there are manyother ways to upload data into wev8 buti think this for data scientists is soaccessible the way that you have thesethis jupyter notebook interface hostedon google collaband then putting data into the vva cloudservice all of this can just be donewith very little real uhor challenging skills with respect tothe engineering of setting up all thesedifferent cloud services we can justkind of with a few clicks go from googlecollab to the vva cloud service to hostthis example so with that said with thetwitter data hosted on the vga cloudservice let's jump into the wev8 consoleand do some graphql queries on my tweetsthe screen is showing the we v8 consolethat has my twitter data in it oneuseful way to quickly get a sense ofthis is to click on the schema buttonand see the schema of the data that thisthat this console is currently pointingto so we see how we havethe tweet text propertywe have the author of the tweetwe have what hour the tweet was sent wehave all these properties that describeeach tweet in the data set so the firstquery we're going to do is to do thethe highlight of the thing the semanticnearest neighbor search to concepts likenatural language processing or computervision soto do this we use theget syntax we passed in his argument thetweet class and then to this we pass inthe argument of the near text searchso so in your text we pass as argumentthe particular query that we want tomake so in this caseand i'm sorry that you're watching mekind of remember how to do this but inthis argument we pass inwhat we want to search for so saynatural language processingand to kind of jump around in thetutorial a little bit the next thing isgoing to begetting tweets other than just my own soin order to just search for my tweetsfrom now we're going to also add thiswhere filterwhere we add this argument pathauthor being the property that we wantto have this symbolic filter on with ourreturn resultsoperator equaland then value textmy twitter handle which is c short and30. so from there we have ourfilter on the tweets and then we tell itwhich uh attributes of the tweets wewant to see in the return list so let'ssee the tweet text itself to sanitycheck this thing then we'll get theimpressions and then let's also say seeurl click so so this forms our uh queryin graphql in the vva console so when weclick search it's going to find thenearest neighbors to natural languageprocessingthat i've authored and thenand then return also the tweet textitself the number of impressions and theurl clicks on the tweetso we send that queryand uh so we see the top result is stackoverflow down really looking forward tolanguage models that can answerquestions about python so pretty relatedand i guess so kind of looking throughthis i think maybe the most interestingthing to quickly focus onwould be to understand that it doesn'thave to have the exact keyword matchnatural language processing it can stillkind of capture the semantics of thateven if it doesn't directlyask for that so to also kind of justsearch through this we can try a wee vapodcast and see what's returned in thiscase a lot of the tweets dosay we va podcasts directlymaybe we could do just deep learningbroadlybut anyways i think from this you get asense ofyou can decide what you want toreturn from the properties of the thingof theclass you have this near text filterwhere you pass in the argument throughthis interface and you can addadditional filters this way in additionto the we vvade console we can also wrapup the graphql queries in a pythonsyntax to send these queries in pythonthen access the nearest neighbors inpython to produce the visualizations ofthe impression splits based on say we'vea podcast we've a tutorials an ai weeklyupdate or also say natural languageprocessing computer vision again thesame kind of idea so this is the syntaxof looking at how we can wrap up thisgraphql query into a python uh api so wehave the dictionary we have the conceptsquerywe do the the get query we pass in theclass name tweet the attributes we wantto see tweet text impressions then weexecute the search and then we index thereturn data object so from that returndata object we can put it into sayseaborn matplotlib to visualize adistribution of impressions based onthese different filters achieving thisidea of segmenting our analytics basedon what the tweet was actually about sofollowing this analysis of segmentingtweets based on semantic content anddistance to queries like we a podcast wevva tutorials or topics again naturallanguage processing computer vision hereare some additional questions that i'masking about exploring twitter analyticswith we v8 and seeing what else we cando and i'll continue this discussion inthe fifth section of this talk underresearch questions and discussions soone question i wanted to ask is thisconcept of have i tweeted something likethis before and trying to have some kindofsay pre-flight checklist before sendinga tweet to get a sense of how tweetslike this have performed in the past souh this is just showing putting theexact same tweet into the semantic spaceand seeing the uh return nearestneighbors just to get a quick sanitycheck of what this kind of thing lookslikebut probably more interestingly thanjust have i tweeted something like thisbefore might be to ask you know who inmy say network or in my particularindustry like machine learning hastweeted something like this uh recentlyso in order to explore this question iaggregated a list of people who've comeon the wvva podcastgoing through the we vva podcast guestsand going through their twitterusernames to hit the tweepy api and addtheir recent tweets into this databaseto have this semantic search to see whatall these people are tweeting about andsee ifthey have similar interests to the thingthat i'm interested in right now so thefirst step in this was compiling thelist of vva podcast guest twitterusernamesthen hitting the tweepy api like thisusing thekeys you get from twitter api and thengrabbing 100 of the most recent tweetsfrom our from our uh i think roughly 14or 15 we gave a podcast guest so anotherinteresting thing with we've yet is ican use this same schema just populatingthe tweet text the author and the likeseven though i don't have say theimpressions or url clicks data i canstill just populate theseproperties and add it into our prettyflexible data schema so now that we'veadded these using the additionaltwitter users let's see some graphqlqueries with semantic search so we'reback in the we evade console and thistime we're going to be querying all ofthe we vva podcast guests to see ifanyone has been tweeting about theparticular topic of generative artso in it's the same exact query frombefore we just have the get filtertweet and then from the tweet we want toget the tweet textand let's say the author and then we addour near text filter to itlike this where we have conceptsand thengenerative artcool so this is how we form our queryand then we send it and now we'researching through all the authors on theeva podcast and seeing if they've beentweeting about this concept ofgenerative art so we seehan zhao has been tweeting a bitgenerative art is a creative process wesee a lot of say exact keyword matchingwe see some ai generated artwork so itisn't exactly identical to our keyphrase but it still pro props it up toone of the top results we see bobtweeting uh generative models like dollyfor music are around the corner sogenerative music artso anyway so this is the idea where wecanyou know have these semantic searches wecan add a bunch of authors from twittwitter users anduse this in order to do the semanticsearch about what people are tweetingabout which i think could be a reallyinteresting tool and i'm really excitedto continue exploring this applicationif you're interested in playing withweaviate yourself and sending somequeries out we have a live demo ofwikipedia that you can explore so beforediving into the console itself with thewikipedia data let's look at the schemafor this wikipedia example so there'sone key thing that i've left out of thetwitter tutorial that is one of the mostexciting parts of wva generally which isthe graph like data model so in thewikipedia example we have articles andthen we have paragraphs of the articlesin the articles as two separate classesso the article has the property of thetitle of the article say miami heat toreference the example from earlier andthen each paragraph in the article isrepresented as a separate class that hasthe title of thearticle i believe and then the contentin the paragraph itself and then theorder that it appears in the wikipediaarticle so for example coming back tothe positive sampling with the miamiheat articles this would be the firstorder because this is the firstparagraph in this article about themiami heat and then this is thesecond paragraph in that particulararticleso this allows us to create theserelations have named relations betweenobjects and also we see this recursiverelation where say the the miami heatarticle links to the nba and then wehave this recursive relation back to themba article so i'm going to get intomore on this in a little bit about howwe could expand the twitter example tohave this graph like data model andintroduce some additional classes butlet's get into the wikipedia example tohave a quick sense of this in action sohere we are in the uva console for thewikipedia example we can check theschema see the classes we have thearticle a wikipedia article with a titleand cross referencesand then we also have the paragraphwhich is the wikipedia paragraph andwithin that we have thecontent which is the thing that isvectorized with the transformers so thisis an automatically populated query fromthe web link that's in the descriptionof this video to check out thiswikipedia exampleand so we're introducing a couple of newthings from the twitter example sofirstly in addition to the near textfilter there's also a question answeringfilter in we've eight and i think it'skind of out of the scope of this topicof this talk to completely dive intowhat question answering is but basicallywhat it's going to do is it's going toretrieve the nearest neighbor content tothe question and then it's going toclassifyusing a supervised learning modelclassify the answer to this questionwithin the contentbut the other thing that we see is thecross reference between the paragraphand then in article and then the titleso this is how we do that crossreference to answer the question withinwikipedia of who was stanley kubrick soto recap wev88 is a vector searchdatabase rather than a library such asfacebook's face or annoy from spotifythis means that wev8 has the approximatenearest neighbor search for doing vectorsearch with an absolutely enormouscollection of vectors but it also hasdatabase functionality like create readupdate delete support and lets yousafely secure and persist your vectorswhich is pretty important when you havea massive amount of vectors and then wesaw how we va has a graph like datamodel we saw the example with wikipediaof how you can use the graph data modelto organize data in this way we canimagine building on the twitter exampleby say also linking these images byhaving a has image thing and it'sgenerally such an interesting propertyof a way of combining say multimodaltext image data but also kind ofheterogeneous data say we have a tweetand it links to an article then we havean article which is also text but wehave this other way of representing thatarticle object so if you want to getstarted with wev8 i highly recommendchecking out the quick start guide underwe v8 if you want to get started runningwith the demo data set and understandinghow to set this up and eventuallygraduating to uploading your own datasets into we vva so to summarize thefourth key section on using we've a fortwitter analytics we see how returningto the whole theme of the talk we cansegment the impressions on twitter basedon the content of the tweet withoutmanual labeling we can do it with thesevector representations that we get froma pre-trained sentence transformer thathasn't been say optimized in my tweetsor really twitter in generaland then further we saw wev8 a vectorsearch database and we saw how we canuse it to store and search throughsemantic vector embeddings of data thefifth section presents some researchquestions we're exploring and adiscussion around this project so hereare three important research questionsand general directions that we'reexploring that will help with projectslike this and this general idea ofvector search and then also theirapplications and things likesegmentation as outlined in thepresentation so the first key questionis should i fine-tune my embedding modelor mymodel that produces the vectorrepresentations so early in thepresentation i i claim that no you don'tneed to you can use these pre-trainedsentence transformer models but this isstill a pretty active area of researchwe do see a very impressive zero shotgeneralization from these models and itprobably can give you a pretty goodretrieval from your data but there areall sorts of things that are in theworks uh say sparse fine-tuning that letyou fine-tune more efficientlythe continued development of theseoff-the-shelf models of course and thensay hybrid approaches where we combinethisvector model with saythe keyword bm25 tf idf those kinds offeatures is pretty particular to textthat particular approach but this kindof fusion of features is is seeing a lotof interest in how we produce theretrieval list and combining thefeatures fromthe vectors and then also this bm25algorithm so there is a very active lineof research and just understandingcontrastive learning generally i don'tthink contrastive learning is ait's kind of seems like a newer thingthat's emerging say in computer visionwe had sim clr moco and people startedreally taking that show on the road totext as well and we're seeing thingslike sentence transformers so i stillthink there's a lot of opportunities toexplore how we train these models thatare trained to produce vectorrepresentations the second key thing isthis idea around large scale vectorsearch and how we form the the how we dothis vector search at the say billionscales imagine we have billions oftweets in our database and that's wherethese things called approximate nearestneighbor a n algorithms come into playand i think this is a really fascinatingarea of research to study particularlyif you're interested in things like datastructures and computer science and thenfinally the key topic that i really wantto emphasize and is i think veryrelevant to this particular project oftwitter analytics is how does vectorsearch differ from classification orregression models and this idea ofretrieval augmentedsay learningso let's start with this idea of vectorsearch versus regression on impressionsso imagine we have a model where youtype in a tweet and then it will useyour data to just predict how manyimpressions this tweet would receive imean if if it has high accuracy you'd belike well sure this this is useful isuppose and then you tweak your craftyour tweet modify some phrases until youcan get this number to be as high aspossible but instead of just having thisblind regression model we might want tohave a vector search model where againwe put in our tweet and then we get thenearest neighbors to tweets we've sentbefore to give us this sense of what isthis tweet i'm about to send similar tobut what retrieval augmentedclassification is about is combiningboth of these things so the model isgoing to retrieve the nearest neighbortweets in the input as it makes aprediction on how many impressions thisindividual tweet will receive then wehave options where we could say perturbthe input to have that interpretabilityand see howa perceived change in nearest neighborswould change the impression predictionand overall it just gives us more of aninterface to see what is influencingthis prediction why is it predictingthis amount of repression impressionsrather than just this kind of blindinput output mapping so to continue onthis discussion this talk was inspiredby the twitter analytics data and tryingto see what kinds of insights we canmine from it and how these new tools invector search can help us gainadditional insights from our twitteranalytics data as well as maybe alsoconnecting to the broader scope of whateveryone's tweeting about and becauseyou can still access say the likeretweet count which can also help youget a sense of performance with tweetingabout these kinds of topics so generallywe want to ask questions like you knowshould i post this to begin with wouldthis be you know would this just be abad tweet to post andsave yourself the embarrassment sort ofwhen you do that kind of thing and justgenerally thinking well maybe it's agood tweet but there's just a particulartime to post it coming back to thesymbolic questions but then kind of thisentanglement of the tweet itself whattime is best to post this particulartweet and then you know the phrasingit's this entanglement between thesymbolic attributes of the time to postit as well as the kind of the contentitself if you want to dig deeper intothat and then we want to maybe expandfrom individual analysis to teamanalysis and say you know how areaggregating the data from our entireteam is anyone tweeted like somethinglike this recently who on our team wouldbe best fit to tell the story and whattopics should we be tweeting about issay a lot of these umyou know a lot of these contentstrategies have a few different topicsthat they could explore and should itmay be time to double down on aparticular thing like sayfine-tuning retrieval models or a nbenchmarks all these topics which onesshould we be tweeting about so these aresome of the questions that i was seekingto explore in this twitter project and ihope you found this interesting so tosummarize the general idea is how can weimprove these systems and what lookspromising with respect to the particularideas of improving these systems sayideas like fine tuning more approximatenearest neighbor research to understandhow we can do this at larger scales andthis general idea of retrieval augmentedmodeling to summarize his talk on vectorsearch for data scientists we firstlooked at the idea of segmentation anddata science how we can split metricslike impressions based on values ofdifferent attributes like what time thetweet was posted or whether it has a urlin it in order to see the distributionsand see the differences anddistributions based on these values thenwe saw this idea of vectorrepresentations of data and how we canrepresent say text images and code withvectors and then in the third section onvector segmentationhopefully the additional examples madeit a little more clear how we cancombine these ideas of vectorrepresentations and then uh segmentingour analytics based on thethe content of the data itself finallywe looked at the we've ate example fortwitter analytics i really hope youfound this interesting and then finallywe explored some research questions anddiscussion around this idea of vectorsearch for data science as well as thisapplication for social media analyticsif you're interested in we've aid andthese ideas around vector search pleaseconnect with us on the we vvate slackchannel or our youtube channel we vv8vector search engineon the youtube channel as well asspotify i'm the host of the we v8podcast where we have a lot ofinteresting guests who are doing thingsin research around vector search orbuilding applications with wev8 and ireally enjoy these conversations so ihope you do as well and finally you cancheck us out on twitter at wev8.iofinally i would like to give a specialthank you to sebastian woodaleck inadvising and counseling the developmentof this presentation as well as svetlanasmelly nova for the help with the visualstyling i'm so grateful for this helpand putting this together and it reallymeans a lot to me and in addition toeveryone currently watching this videothank you so much for watching thepresentation i really hope it made acompelling argument about vector surgefor data science and this idea of vectorsegmentation and hopefully this twitteranalytics project was somewhat of aninteresting way to understand thisconcept so thanks again", "type": "Video", "name": "Vector Search for Data Scientists - Weaviate at ODSC London 2022", "path": "", "link": "https://www.youtube.com/watch?v=IRWHa57T-zk", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}