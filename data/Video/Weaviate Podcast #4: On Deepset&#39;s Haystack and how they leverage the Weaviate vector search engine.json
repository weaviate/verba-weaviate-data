{"text": "NLP frameworks like Deepset's Haystack are powerful tools to help data scientists and software engineers work with the latest and ... \nhey everyone thank you so much forchecking out the fourth episode of thewe va podcast today i'm joined withmalte peach the cto of deep set ai whichhas created the haystack vector searchengine today we're going to talk aboutall sorts of topics related to vectorsearch engine the haystack library andthe different things that they've builtand all sorts of cool exciting thingsaround these ideas in neural search soto get started uh malte could you tellus about youruh mission of haystack and sort of yourvision and understanding of vectorneural search engines yeah sureum so um basically obviously my personalbackground is pretty much described withmachine learning engineering and umyeah there was probably onekey momentin my life that made me up ending endingup in nlpand i was when i did some research in usback on like healthcare data back thenum some numerical values some like umblood measurements time series data allof that we built like a fancy model uhpredicted some um some some nice thingsin the end doctors told us uh yeahthat's that's not new uh we knew that uhit's correct what you predicted but theyare not super exciting uh and then itturned out the only like or they wouldsay the most exciting part uh that theywere curious about wassome part where wemodeled or used data textual data comingfrom handwritten notes of doctorsand from modelling perspective that wasvery naive modelling i would say but inthe end it was the biggest value and umthat's i would say how i ended up in nlpand also why wefound a deep set in 2018 uh because wereally saw and believed hey like there'sso much text data out there in basicallyevery company but it's in most cases notreally used there's like it's alot of gold out there but not reallyleveragedand in uh 2018 we thought okay yeahthey'retooling was well okay but like themodels were quite primitivebut we really believed hey there must belike there must be something happeningin the next years there will beinnovation there will be somethingsimilar to like imagenet on on computervision there will be this wave of newnlp models that that make this text dataavailableand that's basically we started deep setandworked with many industry clientsthen bert came out there was quite likei would say thisimagenet momentand then things accelerated a lotum and yeah what we what we saw fromworking with all these industrycustomers um was basically thatit's alwaysevery company that we worked with hadthis issue of accessinginformation from this these text amountsthese documents and finding it andextracting it andthat's basically wherehow we started haystackwith the belief okay let's build aframework thatleverages latest researchand bridges this gap to production inthe industry to make it simple to adoptthese latest research methods and reallyuse them in that same production and alllet's say around this this case of uhnew research if you want to say so uh weusually say like about finding theinformation and also extracting it sonot only thisclassical search uh setting where youtype something in you get your resultsthat could be also that you extractcertainentities or certain um certaininformation and kick off other processesor so onumyeah so that's basically how we startedhaystack and uh got uhgot pretty well adopted faster than wethought uh yeah we have no contributorsfrom from all around the world fromnetflixmany other companiesuse it in production and um yeah what wewhat wehear basically as feedback is um andalso use it as astrong philosophy of usum that the framework is really a legolegobox building blocks that you can stickflexibly together to some pipelines andthen you move that oil production youcan exchange one of these pieces easilyif a new model comes out so i would saythat'sthat's pretty muchsomething i just want dna and in the dnaof this whole chainand yeah for me i'd say like my vectorsearch imagenet moment was when i firstcame across theco-search system from salesforceresearch and this idea of the pipelinesis the first time i really saw thisdiagram where you look at the documentindex the retrieval pipelines going intodifferent downstream applications likesummarization question answering or saya chatbot and this kind of idea ofhaving these flowing systems for meco-search was myimagenet realization when i first becamereally excited about these kinds ofsystems so i'm really impressed as i waslooking through the haystackhow to use it and the differentcomponents of it i really love thepipelines and so before we get into sortof the details of the kind of like thelibrary design i want to ask about moreof your opinions on retrievalaugmentation and kind of like retrievaltransformers and it's obviously verypopular now with things like deep mindsretro and open ai's web gpt this reallyexciting idea of retrieval augmentedtransformers and before i give you achance to talk i want to kind of quicklypitch this ideaof do you think that maybe the shape ofsayagi or like a powerful ai the nextgeneration of that will be that you havesuch a powerful reasoning model that allyou have to do is just plug in your dataand then it can just retrieve it andthen this reasoning model will be ableto adapt to any kind of data that youretrievesuch that the retrieval augmentedpipeline is so strong that you don'treally need as much fine tuning and thenfrom there we can talk about the farmlibrary and haystack and all sorts ofthings is debate around retrievalaugmentation and then the need forfine-tuning um yeah so i i definitelysee that retrieval augmented pipelinesor models are like asuper important piece towards maybe ajiat some pointand um and i mean i think you can takeit from two sides or two perspectives uhone let's say just coming from planetransformer models and if you talk aboutsomething like question answeringretrieval is just basically the way tospeed things up right and to make itscale to millions of documents havinglike some kind of filter before and uhand really just seeing it that way umand that's how we i think initially kindof started it umthen i think with the um say the rise ofall these gigantic language models andmore like generative approaches um it'sreally i think now about these twoschools to say mindsets to beliefs whatwhere what is today actually helpful andwhat where will end up in the futureumi'm pretty sure that for today and thenext years umi would place my bet on retrievalaugmented pipelines um mainly for tworeasons um one isbasically interpretationso um if youlet's say have something likea gpg3 or like a from omai one of themodels um and you ask the query aquestionand it gives you the answeruh it's super hard to understand ifthat's correct or not now you don't knowhow it was generated what kind ofinformation is it based on and that's avery powerful thing with the withretrieval and extractive methods rightthat youour users often it's they get theseresults but then they often also wherewas that written like give me the exactpassage andwhat what real data is that based on andthat's very easily possiblethe second thingyou already mentioned likefine tuning or the way to adapt it tonew data setsif i have something like gpd3 or anothergenerative modelumusually trained on a bigpublic corpus from the internet ifthat's your domain perfectbutour customers are mainly like enterpriseclientsmaybe from aerospace domain from somelegal domain from financesome very deep tech stuffand there are usually likewordsand meanings that arevery different to general internetwikipedia corpusandyeah i mean this this big models are nottrained on it so they don't workand they get outdated very easily rightifyou ask one of the not saying ourone-year-old models aboutomicron or kovit or whateverthey will won't know and it's very hardto to updatewhy with retrieval it's just hey indexon your document today and out of thebox my my modelknows or can find this informationum so i think these are two powerfuladvantagesum still like retrievers danceretrievers for example there there is ithink some um many cases need for finetuning and also like get a lot of boostif you fine-tune for your domainum but it's then usually some somethingyou do once and you can still likeingest new data and you're fine and youdon't need to retrain like every i don'tknow second day yeah i love those twoarguments for the benefits of retrievalthe interpretation of seeing what it'sreturned and then the ability to adaptto new data sets and uh changinginformation and i think those are twohuge selling points and so kind of todig a little deeper into the fine-tuningpart i want to ask a little more aboutwhat do you perceive as being morevaluable for most use cases fine-tuningthe vectorizer so say fine-tuning thedense passage retrieval step the thesiamese bird or whatever you're using toencode the data or fine-tuning thereader or the reason or say it and thatkind of thing andsay not only just fine-tuning it on yourdata set but fine-tuning it to uh to beprocessing the retrieved context so sowhen you're fine-tuning the questionanswering model have it be fine-tuned tothe retriever component of it so it'sused to seeing that additional contextand then that's very in that specifickind of retrieve context from theend-to-end system um very good questioni'd say it really depends on the domainand use case and the data set um soi look a bit back at our customersinmany cases i think wewe get a big boost when fine-tuning thethe reader part as wellespecially if it's like veryspecific domains very specific questionsumand the style is important about theanswer as well right sometimes it's notthese days not only aboutfactual questions anymore like asking iknow where was isn't that founded orwhich herebut it's really moreopen questions uh interpretory questionslike i don't know what's the what's thestrategy of this competitor in china andthere you can really get uh get goodperformance if a few labels doesn'trequire much fine tuning but the bithelps a lot umand um i think on the receiver side likevectorizer um it's it's still helpfulum if um your data set is very largeand umif um also like combinations ofretrievers don't work out for you sowhat wesometimes do is combining a sparseretrieval with dense retriever and ifyou kind of hit a barrier thereum then then of course fine-tuning uhpunching your achiever alsocan be an option nowandi think thatthis is aas i said like this as this depends it'svery important that you have the toolingto find out right that if you build apipeline um that you have a way toeasily analyze like what is mybottleneck is it the retriever thatdrops performance or is it the reader orany other component that i haveafterwardsand that was also i think one one of themotivations back then to to introducethese flexible pipelines in haystack andrecently we refactored the wholeevaluation part so you can get reportsokay that's performance of of retrievernode of the reader node and you reallyseewhat what is worth improving and finetuning super cool and um so i'm reallycurious about this uh farm library thatyou've built out for uh fine tuning andthe different components that go into itwe'd imagine that uh fine tuning anykind of deep learning model you'd wantto do things like maybe have theseadapter layers like the compactoradapter these kind of things so that youdon't have to fine-tune 100 of theparameters of the pre-trained model uhmaybe things like a hyper parameteroptimization with like a hyperband kindofthese kind of libraries for that kind ofsearchthings like you know sparsity and thenand then more so just like standardtools for training deep learning modelslike mixed precision and i saw gradientaccumulation in farm these kinds ofthings that generally uh facilitatetraining any deep learning any largedeep learning model so what kind ofthings arespecific to farm and specific tofine-tuning the reader models with theuh like with the retrieval appendage asa part of the pipelineum so farm was basically our i think thefirst open source project that wepublished back then and kind of haystackcame afterwards and actually nowmigrated quite some some parts from farmuhto haystack so we have it really likeall integrated so um basically all ofthe modeling related retrievers toreadersis now happening in in haystack um butstill farmersfrom adesign perspectivequite interesting the core idea backthen was that we really have onesay language model as a component andthen multiple prediction heads that youcan stick up the stick uponthis language model and um this issomething we for example now think a bitabout for for some question answeringtasks ummight be helpful to have one predictionhead that for example um gives you theum the span answerbut another one that for example can canclassify this answer or can give youum just binary like yes no like if thisis the span that i found and there'slike a yeah it's like a yes uh answer orno answer umand uh all what you mentioned to umfine tuningtooling that is out there um is i thinkthere's like a lot of potential and weuh we just implemented um a distillationapproachinstallation is tothere's a lot of methods out there but ithink there's still um a lot ofpotential to to improve and uhand um has a huge benefit for um forindustry and production um tiny bird isserving a nice nice idea and exampleum so it's i think it's one degree aboutthe modeling part distilling maybe froma larger model the knowledge tosomething smaller um but also combiningthis idea with something like data setaugmentation soeverything thathelps me as a developer in industry toto fine tune amodel with less data that's that's uhthat's super nice umand ideally it's then also faster forinference yeah and i think the um theadding of the retrieval component youdon't need to store the knowledge in theparameters of each of the separate headsso you can get even more efficiencygains of not needing to have some 12layer100 million parameter encoder base thatthen attaches to each of the headshopefully the retrieval model canreplace that and then these models canbe more parameter efficient so kind oftransitioning a bit away from the topicof fine tuning i was really impressedwhen i came across the deep set cloudgui it's beautiful i love looking at itsuch a cool way of organizing the tasksaround constructing your sayfirst example of vector search if anyonelistening to this has their data set andthey just want to see one of thesepipelines come together i highlyrecommend checking out this deep setcloud gui it's as easy as pick yourlanguage upload your documentsclick and pick the components of thepipeline from retriever questionanswering or retriever summarization andthen and then even from there the restapi deployment and i love that idea ofjust the easy search api deployment sohow do you see that kind of thing of umof this gui interface design what kindsof things have gone into the uhinto the development of the deeppsychology ui sofor us was really important to find asay a complimentary product or opensource uhproject haystack we didn't want to endup with a with a model where we say okaylike there's a i know super niceperforming model but we don't publish inopen source we that's part of the uh thecommercial offering um at the same timeof course we're a company we need tomaintain the open source project so weneed a sustainable business model so theideaof this combination now is really thatwe have haystack as a python frameworkuh who really helps developers um tobuild easyprototypes and solve the problem ofscatter technologies soyou have models you have databases youhave under rest apis it's quite hard tostick all of that together and[Music]have all these skills in say one personor a single team and hashtag open sourceis meant for that it's a pythondeveloper framework to solve itandif that's fine for you you can just goahead with that bring that to productionbuild everything you want around itfineum still what we saw is that it's notalways only about the technologies butit's also a lot about workflows aboutcollaboration and uh having really thewholeum say lifecycle theml of lifecycle of a product and um andthat's why we built deepsea cloud that'sbasically one sas platform where youreally can start as you said you uploadyour corpus you index it you canconfigure your pipelines in differentmodes either from code orjust from a yammerthat you put thereand you can easily share demos withcolleagues get your feedback hey dothese results make sense yes or no andinstead of then having this feedback inexcel sheets floating around you have itlike all integrated in your in yourdatabase and can easily say ah like okayi got this feedback this pipeline isbetter than this one or trigger someretraining fine tuning based on thisfeedback create labels so it's reallylike all integratedand once youonce you foundsay the perfect pipeline or that fitsyour needsjust basically one clickto scale it and bring it to productionand we will do all the the scaling inthe background in our cluster it's fullyhosted and you can integrate thembasically the api toyour own product uh wherever you wantthis functionality yeah i think thismodularity is so exciting this idea thatyou can justpoint to the api endpoint to add in somekind ofsay you want to have model inference asa part of the pipeline and you want tojust point to the api and plug it intothe existing pipeline and generally theidea of sharing the search api is suchthat anyone can integrate anyone's datasetconnected to these pipelines by justquerying one of these endpoint demos anduh say on wev8 we have the wikipediademo and we have just the publicendpoint so if you if anyone in theirworking directory wants to just paste inthis address you can just start clearingthe vector search engine and people canshare their data sets and this is reallyexciting like open source collaborationof uh sharing all these different thingsand so i want to ask you kind of uhwe've seen things like hugging facespaces and and the model hub asthey share model weights andincreasingly hugging faces hosting rawdata sets what do you think about thefuture of organizing these vector searchdemos and then having theseability to not only just access datasets but to be able to just quicklyapply all thesefunctionalities like search just throughthese public endpoints yeah so if it'suhfor support amazing work that tuckerfaceis doing and we areclose exchange with them and it's reallygreat what they are building and whatthey're doing for the whole whole spaceumand um like how we see how we see it andwhat we are doing um is basically iwould say one layer above probably sofor example we integrate with a hugephase model hub soif you use deepsea cloud you canbasically load any model that you havehosted there or that is public there andi still think that that right now whatis missing is this kind of let's sayorchestration layer where you bringthings together we can say okay thismodel this data set or my own data setum here's a quick demo and i can sharewith my colleagues so i canshare it in the public um and uh andthat's exactly i think whatwe are building what i think many othersare also building i think it's importanttoto have um to really empower developersto to cover this whole say lifecycle toshow something and i think that'ssomething missed also in in many uh mlprojects now that youfocus a lot on the modeling part butthen if you show your colleagues orothersand it's it's something quite hard torelate to it and for others to try itout and i think this is uh so importantlike umfirst of all to validate what you arebuilding as an engineerbut also tosay trigger creativity and andinspiration so um if for example peoplefrom from some business departments theythey saw some of our demos and um andthey thought ah like that's cool ihave some people be a different use casebut can i also use it forthis and thatfor for example chatbots we have somesome people whoum who use now qa for answeringquestions within chatbots or forparsing let's say if you have incomingdocuments every time an invoice comes inyou want to askfive questions and uhlike whowho did issue with this invoice uhwhat's the amount and kind of the answeryou use uh used tostore it in a structured database andkick off other automated processesand i think all this this kind ofinnovation is so much easier if you havesomething tangible something you can tryout and and sharewith others yeah and congratulationsagain on the deepsea cloud gui it issuch an amazing organization of thisorchestration layer and user interfaceto just point to different data setendpoints different retriever models andand now let's get into kind of thesedifferent flavors of retriever modelsyou mentioned the idea of combiningtfidf and bm-25 with say siamese birdsentence bert and having these twodifferent kinds of ways ofofdifferent ways of structuring yourretrieval model but i kind of thinkanother thing that's interesting as wetranslate into the connection betweenthe we vva document store and then thehaystack pipelines is the idea ofsymbolic annotation of the neuralvectors and of the neural search sofor example if you're searching throughscientific papers and you want to addthe symbolic filterpublished in cvpr or published in icmlor authored by a particularwell let's say an institution could sayan author might not have that manypublications such that you need toreally do a neural search through it butthese kind of symbolic filters on top ofthe neural search have you thought a lotabout what how that kind of changesneural search and how that kind ofmaybe offers another way of looking atthe retrievalpipelines between say tfid fbm25 beingone thing and thenvector distance being a different thingyeah i think that's aan absolutely brilliant feature that vb8has and uhand actually i think differentiates vvatomany of the other vector search enginesout there and um for us uh was also ithink the one of the main reasons why weintegrated into haystack because weheard that a lot from the community wesaw that a lot in our with our owncustomersthat really this combination ofsaystructured data and metadata that youcan filter plus avector search is super powerful and umi mean first of all it's a nice nice wayto kind of narrow down your search dayso if you have really like millions ofdocumentsin most use cases there is somesome text some i don't know time stampsomething that you can that helps you tonarrow down your search space immenselyandthis of coursethe die has direct impact onthe search qualityumand at the same time it was tricky toimplement so it's uhit really has to i think beclosely tied to the vector search engineumand has to happen if you have somethinglike approximate nearest neighbors umwhich i think is quite important at inproduction deployments and then uh itbecomes tricky and um and they arehaving this combination i think is supernice in general i would sayleveragingmetadata or structural information as uhis super helpful umone thing for example on our growthroadmap that you want to explore furtheras well toleverage document structure uh even moreso that you can say you havea pdf so you have like a lot oftitles in theremaybe captions of images andas of today most uh most search enginesand uh and models um treat that in theend assimilar like plain text uh but reallyusing the information hey this isanother headline orum this is i don't know the the title ofthe the chapter um this makes makes lifeso much easier at query time um and ithink also for uh like buildingembeddings then for these kind of uhinformation is a nice direction infuture that we're not it's into hardfiltering or thenbm25 on this kind of metadata but reallyembed those titles embed the wholedocument structureand and use that at queer time yeah andit's interesting because you can stilluse these filters in the h sw uh a nindex algorithm so we build these datastructures up so that we can facilitatethe speed of doing vector distancecomparisons but you can still add thesesymbolic filters within something likean h and sw graph as eddie and dillockerat wv8 has explained many times andfinally starting to understand it alittle bit but um so let's get into thisum query classifier the idea that thequery classifies whether this is atf idf query or if this is a neuralsearch query and i want to get yourthoughts on whether you think it'sa worthwhile direction to say have aquery refinement step where it adds thesymbolic annotation onto the neuralquery so rather than saying oh no thisis mostly keyword we want to make likesay um say you google something about acompany like uber and then you want todo a regular expression symbolicfiltering or say make sure uber is inthe title of the return articledo you think we could have aintermediate query refinement neurallayer trained with uh you know gradientdescent and some kind of annotated dataset for this that would add the symbolicannotation onto the neural search layerrather than sayseparately querying a tf idf index orseparately querying say like even an sqlindex because it's like a purelysymbolic searchor it looks purely symbolic differentfrom say you know some kind of sparsesome kind of like machine learning stylebased representation of the data do youthink that kind of thing could be uhfruitful yeah i mean in general i thinkit's it's uh it's an interestingdirection um the career classifier inhaystack pretty much comes from the uh iwould say from afrom the user behaviorwhat we saw is that ummany companies if they already have aproduct which allows searchpeople are used to keyword search rightthat's what they type in and it takessome time to um to make them aware heylike there is aif you there's a smarter way ofsearching right and uh if you really puta full sentence a full question whatevermore context in then you get bettersearch results and that's the same thingwith google right they are slowlytransitioning towards uhtowards say more contextual search andthey suggest queries and um they displayhey is that what you like the questionthat you they were asking and um and ithink that all goes towards this zeroclick search that you have searchedsomething and then you directly see whatyou want and you don't need to click onthe on the website and then find whatyou're looking forum but i think yeah there is this uheducational period and um and that'swhere we uh where we implemented where iadded this clear classifier that you canreally see hey this is a keyword queryuh and then maybe just use your existingold search engine probably just plainpm25 or whatsoeverandyou know on the other side if it'salready a semantic um query you canroute it to your to your second stackand uh and that really giving yousay both of the best of both worlds butif because if you have i think reallyit's like classical keyword queriesum what we saw is that at least the mostmost dense retrievers don't work thatwell if you just query for let's sayuberumyeahprobably really looking specifically forthis one um and and bm25 itprobably outperforms yeah we've talkedabout this topic many times we love thisterm of serendipity this idea of uh youknownot really knowing what you're searchingfor sort of is a good way of thinkingabout how totraverse through neural search resultsand also sort of blurring the linesbetween the task descriptions of uh likeretrieval re-ranking and thenrecommendation systemsall kind of blurring into the same kindof task with searching with uh neuralsearch and and yeah i really like theidea of using the user behavior as youkind of uh fine-tune this way ofthinking about it so maybe thinkingabout likeyou know dressing up the query withneurosymbolic and maybe even graphstructured cross-referenced linkingthen having maybe like a special userinput embeddingspecific to each user based on theiruser data or something to help them kindofex help them understand the neuralsearch because yeah it's it's definitelyinteresting when you search forsomething get a completely unrelatedthing but because it's like this fuzzyway of relating the things it'sdefinitely an interesting thing and umso transitioning into some kind of newinnovations around the space i reallywant to get your take quickly on umon what you think will be the impact oflike having longer input sequences totransformers so say going beyond 512tokens models like big bird sparsetransformerlong form and these kind of things ofespecially it seems with retrieval beingable to not just average out theembeddings but maybe have the attentionwithinin like the parametric kind ofprocessing within the same input um ithink that's aan obvious and a big step forward andand the super important one i thinkat the momentdevelopers pretty much got used to this512 tokens it already seems like afixedconcept in the mind of developers but ofcourse it's not andandi think it's just was for me butactually it's a bit surprising that it'suhit takes such long time tokind of make the steps i mean there's ofcourse some some models out there bigbird you mentionedlong former and so on umbut still i think for many tasks theydon't perform equally wellum i do also believe that for retrievalit can make a big difference umum and um you know that one big impact ithink will be on the speed side if youyou really have an efficient model therei don't need to like splitdocuments any further like in thesethese passages havesliding windows and all of thatumit will be immensely helpful and i thinkit's just a for me it's just a matter oftime touh that we get to a point where we don'tthink of 512anymore and i think maybe it'll come inparallel advances in thinking abouttokenization like uh things like i thinkit's called engram transformers theylook at uh different ways to not justhave subword or bipolar level encodingwhere they have other kinds ofstrategies of maybe tokenizing and thatmight come into play with thethings like bot like computationalbottlenecks compared to all isomorphickind of transformer designs and maybethese kinds of things and so one otherthing maybe is using the documentstructure could be a way to furtherincrease the retrieval step what areyour thoughts on kind ofincorporating the structure of adocument whether it's a scientific paperand you maybe have like this is theintroduction this is the related worksand i'm sure all sorts of industrythings have that kind of style wherethere's some mana it's not just like ablock of text from the top to the bottomyeah i think it becomes immenselyhelpful if you if you go towards a realworld documents that are like not alwaysstructured in the same wayum one example is for example uhso like you know companywikis or like short documents of let'ssay meeting notes uh no one writes likea fullfull length paragraph explainingeverything in full context but it'soften really i know five headlines threebullet points that's it anduh what we see is that um yeah if youhave a model that is trained on saywikipedia or natural questionsand then to try to apply it to such adocument format that's quite tough andum and that's why i think likemaking use of this kind of documentstructure even morelike formatting what is bold and justlike what we used from let's say thevisual editor i think is also helpful toto give us input to our models andand let them leverage this as well yeahand i really like this paper called ahypertext language model where the htlmwhere they keep the html html tagsfrom the web scraping so that you havethe uh you know ul tag li that you havethat kind of structure and then they dothings like when they're prompting it toget the inference they'll just add thetitle task to like title brackets maskand then it performs better because youthe scale of the data and the scale ofthat kind of free annotation so thenanother kind of thing that's likelurking in these text data sets mostlyare tables and especially with you knowi'm really interested in looking throughscientific paper data sets and havingthese tables and so what do you firstlywhat do you think is the best way torepresent tables for the downstreamprocessing is it say html tags where youhave say table header table row thatmaybe i don't knowif that's the best way of doing it i'veseen an interesting thing of likewrapping it in panda's data frames andtrying to have that kind of pipelinewhat's your thinking around tablequestion answering within this kind offramework um yeah so we see that's asuper interesting direction and um whenwestarted with haystack pretty muchfocusing on text data but it was uh ithink uh clear from from day one thatit will evolve to more data types andthe vision for hashtag is really to umto have a semantic layer that you havelike any say textual input any query youwant to find informationandquery that into your whole say database or data lake whatever is in thereandand of course many others are focused onimages also interesting but what we sawfrom community and then our clients isactually that tables is themore pressingdemand after the tax documentsespecially from financial financialinstitutions they have like a lot ofannual reports for example whereinformation is stored in tablesscientific papers you mentioned itin technical documentation we often havealso kind of table overviewsand making sense out of that issuper helpful umin terms of representationyeah i think umthere are there's still not really astandard out there when you look also atpapers and models how they do it umwhat we thought that we found is um dataframe is from a user perspective nowagain is kind of a standard interfaceit's notreally standard for the modeling partyet but it's a very nice let's sayexchange format and you can easilyget another table out of of sql put in adata frame you can pass a table from apdf document put it in a pandas dataframe you can do someinteractions easily with it you caninspect itand then you can kind of pass it down toa model and uh how it's representedthere it'sthen a bit of a question of what themodel wants and expects umyeah so overall i think it's a it's asuper interesting direction andlast year also like a lot of innovationhappened there withtapas making big steps forwardbut we also recently implementedthere's another model called raw columnintersectionso i think there's still alsoa lot of different approaches out thereand we'll see which will which will makeit at the end uh that's i think also theexciting part uh about this this wholefield still um but um i think they arenow at a tipping point where they becomeuseful for for industry and that's uhthat's quite quite interesting umyou they i'm not sure if everyone isunaware of what you can do there rightnow but it's uh it's you can reallyask questions where the model then findsthe the specific cell in the table soyou can ask i don't knowwho won the champions league last yearuh or soccer champions league last yearand it will find the the club who wonthat um but you can do also likeaggregation operations and that's ithink quite quite interesting that youcan askhow many times did uh bayern munich wonthe championship and it would basicallykind of create asomething like a sql query and ask likeokay where in this table is this cluband count how often it appears or youcan do like an average of thisand umand uh if you think this a bit furtherit opens up a whole new kind of data setwhich is then not onlythe small table in the paper but it'sreally like a whole sql table or wholesecret database and you have a modelthat kind of produces the sql query andum and i know you can have a uh maybe abusiness analyst and you want to asklike what's the average revenue foruh i don't know china last week in thisproduct segment and you can have abasically a model that builds this queryand gives you the answerand um i think this would be like it'smore powerful yeah i love this idea andi've um i like this paper called turningtables where the idea is instead to havelike anatural language generator from thetable so you parse the table intonatural language and thenmaybe you know you don't have to have beso multi-modal between table and textand you just kind of have all text evenif it's in these two different kind ofstyles i think it would be really coolkind of also maybe to try collecting uhdata sets ofhave a tablethe table and then the image like theplot that describes what's in the tableand then maybe also like what they callthe cytance or maybe like the figurescitants that where the figure is beingreferenced in the text to kind of comeup with those kinds of tuples and see ifthat cankind of power the application of thatkind of idea ofreasoning through tables and anotherreally like kind of interesting idea ithink is adding images videos i thinkvideos too there's like a lot of videoinformation things like as we recordthis podcast and all sorts of it kind ofis like easier to make videos than it isto make uhto write write articles and maybe alsothere's information in the audio likethe tonality the way that yousay things probably some moreinformation there for some applicationswhat do you think about like imagesearch and adding these kinds of datatypes as well to the pr probably liketext search is kind of like the dominantthing it seems um yeah i think it reallydepends on the industry and um and forsure i think there'severything that is in the web isis mostly i'd say on these data types uhimages videos audio like superinterestingum we had a few months ago like umsomething you call hacky friday so oncea month in the company every developeror every every employee actually canwork on anything right that he or she isinterestedand uh andtwo developers there built like apodcast search for example where you cansearch for podcasts andi think for these kind of use casesthat's super nice andand if you also think it a bit furtherumreally having them multi-modal modelsthatthat not only let's say can query it butcan reallylearn from different modes andcombine this knowledge i think that's aninteresting so that youmaybe your company you don't have somuch video data but you're using a modelthatused video data to to gather um say thethe initial knowledge and you can stillapply it to other data sources i thinkthat's super interestingandum think with uhsome of the current models we area bit going in this direction um[Music]so i think it's uhas we mentioned earlier um if you have amultimodal model i'm still curious if umif these models will become then veryefficient if you're say just using themfor text data or if you have like say alot of overhead a lot of extraparameters just tocover let's say audio but maybe in yourdeployment you actually don't care aboutthe audio sois is that really a value umor is it just overhead that you carryaround andyou knowlet models grow and influence times canexplode yeah i've been thinking aboutthat so much as well i really like thispaper called vulcanization where they'reable to use images to facilitate likethe glue benchmark for languageunderstanding anduh as i'm trying to process uh likescientific text and deep learningthinking maybe the figures theillustrations of of the algorithms couldmaybe be usefuland hopefully yeah like efficiency gainsachieved by this kind of multimodalfusion and and another thing uh justlike another kind of data modality wouldbe like graph structureso what do you think about graphstructure and i think when i think aboutgraph structure i think about how muchthe complexity of it can kind of blow upunless it's like a citation graph orlike a friendship or like somethingthat's natively graph structured butthis idea of like particularly addinggraph linking betweenuh any kind of thing that might be inyour database and then maybe having likeknowledge graphstyle like pie torch big graphdeep walk no defect these kind ofembeddings on that kind of graphstructureduh adding it to any kind of data saysort of artificially um yeah i mean umifif it works out if you have such a graphthat's awesome i think you can leverageit a lot um and and do a lot ofinteresting uh inference a lot ofinteresting queries um[Music]i mean i think there are these two kindof categories right like more sayclassical knowledge graphsandand a lot of stuff that happened also inthe early 2000s umand then more recently i would say theseuhgraph neural networksthat that reallyuse their structure also atmodeling time um for the let's say themore classical part more classicalknowledge graphs umwe actually explored this direction umearlier this year because we heard italso a lot from community peopleum having also some some of these likebigger corporates for uh especiallyhaving uh such uh structures in placeum and want to invent them to use or toleverage at uh at such time at querytimeum andif you have a high quality graph i thinkthat'sawesome and it's hard to beat and if youhave uh if you look at google search umstill a lot of the uh the answers uh arepowered by um some some knowledge graphumthe biggest problem that i see there inpractice isif you're not one of thetop 10 tech companies out there umit stillneeds a lot of effort to produce thisknowledge graphsideally automatically you just have acorpus and create this graphautomatically out of itand then to maintain it now to reallymake sure that the quality is isconsistent over time if things changebecausei think the worst thing is if you have aknowledge graph with every knowledgebase if you have a knowledge groupthat's outdated then it'sit's it's really bad um so i think ifthere is a way to efficiently constructthese kind of knowledge graphsthat would make a big leap um yeah rightnow i think it's for many companiesquite quite tough to create one yeah andi think there's kind of like two sidesof as we mentioned earlier with thisidea of query refinement and addingsymbolic tagging to your neural searchthat's one way to use the knowledgegraph whereas the other way would be youuse the knowledge graph in the trainingloop to influence the vectorization ofyour original data objects and thatsecond one being the one that's maybemore prohibitive and maybe morechallenging to really make it happen butthings like say the we v8 layer make itreally easy to just kind of use yourknowledge graph annotation to to filteryour search and guide it and help itthat way whether really getting amultimodal graph embedding added to animage embedding text embedding thatthing might be a little more challengingto reallyuh work out and practice and so thatkind of thing yeah thinking about thatkind of graph structured andwe it's definitely one of my kind offavorite things about the weevil vectorsearch so we've covered a ton ofdifferent topics i think already and i'mcurious like um kind of what's on yourroadmap in the terms of like which kindof idea is exciting you the most rightnow about vector search engines overallso i think it's generally like what weuh it's a lot still about uh makingthis uh easily scale and uh what we lookright now or like what we implementedrecently is uhis basically deploying pipelines uh witharray framework i'm not sure if you hada look at it but it's like really asuper exciting framework for distributedcomputation and uh really makes it easyum to for example take apipeline with different nodes and thenrunnodes independently on differentmachinesscale them independentlyand and have them all connected sothat's a second production deploymentside and quite interestingtheneverything let's say around just dataaugmentation and simplifyingthis fine-tuning domain adaptation stepsoum for example one paper that werecently looked at iscalled like gpl like generated pseudolabeling umfor unsupervised unsupervised denseretrieversumand the idea there is basically that youcreate you have a model that createssome queryanswer pairs or create document pairs inthat caseandyou use a cross encoder tokind of label this and score this andcross encoder usually gives you betterperformance for these type of querytasks and that's why they're used forfor re-ranking um but they are ratherslow so use this kind of let's saybigger model teacher model again tocreate a data setwithout any human labels just you put acorpus and you get these labelsand then you train a smaller buy encoderbased on this data set and i think thesekind ofapproaches like using models like biggermodels more expensive models to createsomething usefulthen to create like to train a smallerone this kind of uhlet's say tricksis what really interests me um and ithink are likequite helpful for for industry umthat's one directionthe other i think is everything aroundcontinual learning i think reallyincorporating human feedback um eitherif the model is already in productionum probably want to measure okay whatwhat are peoplesearchingwhat uh what do they think of theseresultsincorporating that in your in your modelretrainingbut also in earlier stepsif you do some labeling how can youfuse in model predictions there likeeverything that's i would say between uhhuman and machine collaborationis what we are doing these daysum it fascinates me yeah awesome that'sa lot of interesting stuff so to kind ofuh unpack it a little bit and go step bystep yeah as as we were talking earlierabout uh the farm fine tuning libraryand maybe things like that might bespecific to retrieval augmentedpipelines that idea of having theknowledge distillation from the crossencoder or like the tri encoder evenwhere you have say query uh relevant orlike umyeah like query relevant document andthen like negative relevant documentright and the kind of design of a tryencoder or as you generalize that tohowever but the difference being thatit's in the input directly and then youhave the attention over this whole thingthe idea of distilling thoserepresentations down into the siameseencoder which is faster doesn't requireas many comparisons for inference andyeah i think that's really interestingof like maybe like a unique part of afine-tuning library that would beappended to a vector search enginelibrary kind of so to sayand then yeah i really like the dataaugmentationuh the idea of like robustness and beingrobust to negations and that kind ofthing and really testing that in uhretrievals because uh yeah just ingeneral that idea that if you add likenot or switch the named entity itdoesn't react to that it sounds like ahuge problem for retrieval pipelines andthen yeah continual learning thatcatastrophic forgetting thing definitelystill seems to be like the achilles heelof deep learning even ifit does seem like big models somehowdon't aren't affected by it i guessbecause they have the parametriccapacity to store all these tasks in itand then kind of lastly um to come backto the distributed pipeline in ray anduh distributing the competition thenotes could you give me like an examplemore so so i can better understand howlike what parts of the retrievalcomponent are being uh parallelizedso it's uh i would say it's not so muchaboutsay the single retrieval um node but inin in haystack really more if you havebigger pipelines where typically let'ssay there's athis might be a dense retriever theremight be also a sparse retriever thenmaybe you can find it then there's areader model maybe you have a summarizeras wellandand what ray allows you is tobasicallyabstractfrom these nodes and put them on thelike encrypted with the resources theyneed so you can say okay maybe thisdpr retriever like super heavy modelokay let's put a gpu node on itbut uh maybe forsome i don't know sentence transformersor the sparse retriever um cpu node umand for thethe reader model again something elseand umandthat's basically what you what you needto think of from a developmentperspective the rest is more or lesshandled by ray um and if you will thinkabout production use cases um it'sreally that you can let's say the deployyour pipelineand then let it auto-scale and uh andwith some restrictions that you say okaythis node shiva is always on the gpu orlike there's many gpus and please equipa new resource and spin up a new machineif the traffic is kind ofgrowing and umandi would say what we are doing isbasically using this rare frameworkunder the hooduh again building the orchestration ontop so that it really works for forthese nlp pipelines retrieval basedpipelines umdoesn't really need to worry aboutscaling anymore so that's usually youneed uh like a single cloud instance andthen ray will wrap say like dockercontainers that partially like portionup the single cloud instance or is ityou have multiple machine multiple cloudmachines that ray isrouting it through entirely differentmachines so it's kind of like a hybridbetween the two where uh ray sayconstructs these docker containers thatsay how many what percentage of theresources say to use on each machine andthen it kind of routes through that isthat kind of the correct understandingof it yeah kind of so it's really iwould say like a similar bit similar tothe kubernetes clusterand you still havemultiplemachines on the cloud and then on topyou have this basically this ray clusterwhich makes use of these machines youhave like a master node you have some uhsome additional nodes and uh and and youcansplit them the resourcesindependently so you can say really okayi want to use for this node uh1.5 gpus and uh and ray will take careof placing thisuh within the cluster within themachines super cool and so it kind of islike a question of wrapping this up iwanna i'm curious about uh like as ipersonally have been kind of navigatingdeep learning technology and uh tryingto learn how to build things i thought ifound it very useful to have kind oflikeone application in mind that i wouldreally like to see come togetheris that or compared to say where you'rebuilding out atechnology company like haystack and yousee all these different use cases whatdo you think about kind of thephilosophy of either looking at likethinking about things that willgeneralize to all the applications ormaybedigging into one specific thing that youwould really like to see come to lifelike the podcast search for example yeahi mean so what we are i mean there's alot of use cases out there that we aresuper curious to to see and uhuh and really like when this when yousee that from community or from clientsbut um we as a company are not let's saybuilding or focusing on one of thesesolutions um but we reallyenable these use cases and build aplatform that helps developers out thereto build and toum and to help to build these productsand use casesas easily as possible and uh that's forme uh uh we have like a slack channelwhere we share this regularly like whatpeople from community share or likecustomers and it's just so fascinatingto see like what what people buildthemselves instead of let's say hey webuilt or we providea solution forfinancial research it's like of courseyou can like optimize this endlessly umbut it's quite narrow and uh and uh yeahi'm a big fan of uhgiving this flexibility umandat the same time probably yeah buildingthe tooling for let's say a few areas toreally make it easy to build these finaluse cases yeah i think like anyone kindof working in almost like developertools if you will like has to be able tohave that kind of generalist hatthinking where you kind of are switchingin a use case in your head and yeah ithink that's a really interesting way ofthinking about it might be do you thinkit's like harder to do it that way tokeep adapting to some new thing yeah imean it's like it's of course if youhave something very generic it's i wouldsay constant back and forth like youknow you have like your hypothesis heythat's what people needand then you need kind of to wait forokaythat's what they actually built with itand that's how they used it and this wasmaybe helpful it's not but you alwaysthink reliant on this feedback loop tounderstand and uh and that's why i thinkit's uh it's so important to do that onopen source because the feedback loop isquite faster you see what people aredoing what helps them what not andit really helps to build a nicedeveloper tooling andimagine a closed source you always haveto know sell it to someone ask them forfeedback umit's just ways more away slower awesomethank you so much malt for coming on thewvva podcast and uh to anyone listeningi highly recommend checking out the deepset cloudit's a graphic user interface forsetting up this haystack pipeline whereyou select your language you upload yourdata you can even drag and drop pdfsthey have a pdf extractor built into thehaystack library that could you coulddrag in some say archive pdfs and thenget your first vector search going thatway and it's a super cool way in andclicking around the pipeline i thinkit's one of the most exciting uh ways tovisualize what's going on with pluggingthese different parts of this end-to-endretrieval augmented pipeline together soagain thank you so much malt for comingon the wva podcast and thank youeveryone to listening and pleasesubscribe for more podcasts like this", "type": "Video", "name": "Weaviate Podcast #4: On Deepset&#39;s Haystack and how they leverage the Weaviate vector search engine", "path": "", "link": "https://www.youtube.com/watch?v=9DTDUHs3aB0", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}