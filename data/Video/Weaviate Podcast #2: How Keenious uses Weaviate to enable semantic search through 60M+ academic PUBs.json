{"text": "Join \u202aConnor Shorten (Henry AI Labs) and Charles Pierse (Keenious) for the second Weaviate vector search engine Podcast.\u202c \nhere with charles pierce a machinelearning engineer at kenius at keniusthey're building a really excitingscientific literature mining tool usingtools like deep learning and vectorsearch in order to let you searchthrough the scientific literature tofind relevant papers and relevantinformation for whatever you'reresearching so really exciting thingabout keeney is that stood out to me isit's uh plug into google docs andmicrosoft word as you're writing a paperor drafting up an idea you can highlighttext and then just query the keynesthrough this plugin and it will searchthrough the scientific literature tohelp you find relevant ideas so i thinkthis is an extremely exciting andpowerful idea so i'm really excited totalk to charles about kenius and allthese different ideas around scientificliterature mining deep learning andvector search and the grand vision ofwhere we think this kind of thing isheaded so thanks so much charles forcoming on this podcast connor thank youvery much uh i'm honored honored to behonored to talk about genius andour experience with kind of vectorsearch in general and what we've beendoing with we've a couple of the pastcouple of months so yeah thank you verymuch super cool so before we get intokind of the details of it could youexplain to me uh like where kenius is athow it's organized and sort of alsointroduced the problem of scientificliterature mining yeah so uh genius isit's an academic search toolum as you mentioned andi suppose the core of what kenius triesto do isjust to makeresearch um and academic publicationseasier to to search and explore and todiscoverso kenya's kind of takes kind of aholistic approach towards researchin that wewe try and cater to researchers from alllevels and that's kind of fromundergraduates all the way up to phdsso our kind of guiding philosophy shouldbe thatyou know we want to kind of create thatwikipedia effect of going down a blackhole of like you know discovering topicsand we want to do that with researchpublications and researchresearch journals or any kind ofentities within within research soour kind of guiding light is always tomake that process of exploration fun andadd a bit of serendipity that's kind ofhow we describe it it's serendipitoussearch so you know wewe're not like kind of an exactly atraditional academic search tool whereyou type in a single kind of forwardquery and get back exact results we ofcourse can do that but we also like toadd in this kind of serendipitous likeheyhere's an intro here are also some otherinteresting results that might not matchexactly on keywords whether within thekind of within the academic sphere ofwhat you're what your research you knowwhat you're looking for um and of coursethen clenius obviously haswe we don't limit ourselves to shortqueries uh you can search uh you cansearch on geniuswith uh documents of any arbitrary sizeso there really is no no limit you canif youif you're if you've got a paper with idon't know ten thousand to a hundredthousand words you can search using thatpaper our search engines optimize toconduct queries that bigso it's it's all kind of possible umand where we're at now with genius isthatprobably for around the lasttwo years um we've been kind of in heavydevelopment of the of the productum andover the last maybe eight nine monthswe've been expanding heavily tointegrate kind of some a lot ofknowledge graph features soum we've been working a lot on kind ofbecoming as well as being an add-onbecoming kind of a goat a kind of asinglecentralized like kind of just databasefor searching authors andtopics and institutions so you can youcan really search through any kind ofentity type now um and we'rewe're really trying to kind oflike kind of introduce a introduce a newway of discovering academia to kind ofcompete with google scholar in a waythat we think is more organic and in away that we think kind offacilitatesuh serendipitous discovery in kind of aneasier way super cool so i thinkstarting on the on the comparison ofgoogle scholar and so anyone umlistening charles has written a reallygreat article that details these topicsuh about using wev8 and searchingthrough 60 million academic papers andyeah this topic of serendipity reallystood out to me it's very interestingbecausewe talk about neurosymbolic search whereyou have the neural search where you'reusing these vector embeddings and you'relooking for the nearest neighbor inthese vector embedding spaces then youalso have symbolic filters which is likegoogle scholar where you have keywordsso say you type in data augmentationyou're doing data augmentation researchit'll match the data augmentationkeyword whereas this idea of serendipitythat charles is describing isin between matching the keywords as wellas this kind of neural search so it'skind of like this umthis uhrange i guess where you're eitherlooking for an exact match of thekeyword or you're looking for a setcenter of the fuzzy discovery of theneural search engine so this is thiskind of like how you're thinking aboutneurosymbolic searches and that itenables this kind of creativityyeah that's that's exactly the point isthattheirsearch is a funny thing and that thereare times when we have we do searcheswhere they arecertainly thethe intent is to do kind of a keywordsearch and that's fine and you know mostsearch engines today behave like thatand they're greatbut neurosymbolic search and kind ofneural searchis what's most interesting to us and tome i you know i'm fascinated by itbecauseit kind of it's it's only in its infancynow but already within in the ways that it's kind ofit's shown its usefulness it kind ofi think it works in a in a way that veryhuman and very unders like kind ofvery easy to understand from thestandpoint of how we associate ideas inour own heads you know they they're notnecessarily interlinked via keywordsbut they do exist in these spheres ofkind of context or sphere like you knowyeah exactly contextandsphereskind of these spheres of relatednessthat aren't explicitly connected throughkeywords sowhat we really try and do is kind ofwrite these heuristics where weunderstand we try and understand whenour user might belooking to do a very strictly a verystrict search and their intent is tofind exact keywords because of courseyou know that's a very it's reallyimportant to be able to satisfy thatneed as welland then we try and also solve for thecase where a user might be like hey idon't really understand this topic toowell or this is something that's new tome and i just want to ground myselfin all the relevant research around thistopic or this concept like kind ofconceptual fieldand to be able to conduct search thereis what really interests us and that'swhat we think we do welland that's what that's kind of i supposethat's our unique selling point at themoment is that we we really feelwe can we can do that kind ofcontext-based neurosymbolic searchreally welland help students and researchers findfind things they wouldn't be able tofind kind of via phrasing a query oranything like thatnormally yeah i'm super excited aboutthe idea of in context and i really wantto get into your use of knowledge graphsand the graph structured embeddings idon't think i think the use of citationnetworks is one of the most popularexamples for communicating the idea ofgraph embeddings in deep learning andyou know the you're probably 101 graphneural network tutorial will be with thecitation network so i'm really excitedto get into that but i want to stay alittle more on this neural symbolicthing and this idea of classifying theuser intent in order to maybe add onsymbolic filters because i think maybethe the interface thatscales you between neural search andsymbolic search like for example in wvayou have to use graphqlto add these kind of symbolic filters sois there a layer in between the searchthat classifies the user intent andpredicts whether they are going to needa symbolic filter on it or predictwhether they're in the say discoveryphase and they want that kind ofuhuncurated result that would come fromneural search absolutely yeah now we wehave we have kind of a bunch of uh kindof learned over time heuristics thathave kind of brought us that thatbrought us to that pointso you knowone very simple heuristic you can thinkof is literally just the length of aquery um if you if you have a if youhave a few short wordswellyou know i'm talking under five sixwords that querythere's probably a higher chance thatthe user there might know exactly whatthey're looking for and or like have afew keywordskeywords in mind sowhen we do our search it's not that weumturn off thenor symbolic search or the the semanticaspect of the search but we might dampenthe results you know so we have thiskind of trade-off betweenhow we value and weightumvalue and weight orweight or search results soone example of a heuristic might be withthis the length of a query that you knowif it's really short and kind of exactwell then we know that the keywordsearch might be really appropriate herebecause this person has an idea but thenwe also have a feature in genius whereif youif you like a particular paper you cankind of click and find similar articlesum or find similar papers buttonand we're always focusing on making thateven kind of that feature easier or moreapparent butwith that feature what it really does isin that when we can we consider thatintent as being the user really beingopen to explore you know because in inthat instance they don't have anexplicit query they're just saying hey ilike this paper um so then we reallywill really bump up the the semanticaspect of the search umand we'llwe'll really kind of focus in onlet's say using our knowledge graphsearch thereum and really valuing the semanticvectors over a keyword search because wefind them those to bemuch more useful especially with the notsince we've introduced the knowledgegraph than the text search because theknowledge graph tends to really find youthese gemskind of in the research proximity of apaper you know so you'll kind of findthat you start all these milestonepapers come up as well as just reallyinteresting kind of papers that youcouldn't really get um with it with anexact query because you don't know howto search you might know exactly whatwhat what the field is you're you'relooking at or the subfield you'relooking at is sodo you think about like um like userembeddings like you have returning usersand like you know maybe using theirspecific some kind of embedding theirspecific behavior to kind of guide thatyeahso that that's something where we we'velooked into that and we we have likesomepretty cool ideas to do that but we'renot doing it just yet um in terms oflike it there'sa lot of the time with search engineslike this there's kind of thisuh you need to hit a critical massum of like user embeddings to get therebecause we have you know over 60 millionpapers it takes it takes some time tocollect enough datatoto really build like rich and richembeddings off of that you know youactually need quite aquite a bit of quite a bit of user datato start getting useful uhitem embeddings for recommendation therebut that's that's certainly going to behappeningum and that's why actually knowledgegraph the knowledge graph embeddingswere so attractive to us because theykind of solved that cold start problemwhere you can get really goodrecommendationsum from papers with kind of azero like you know theoretically zeroinformation about users and and theirlikes and intereststhat's so interesting i'm definitelygonna be you know coming back to thisinterview and thinking further aboutthat one that kind of like maybe like asemi-supervised approach where you havesome label data and um and mostlyunlabeled data but in the case of this60 million graph it's likesemi-supervised in a very extreme casewhere you have a extremely small amountof labels so so let's transition to thiscold start problem and and um and likewhat that looks like with paperrecommendation i suppose the way that welook at the at the cold star problemwith the way we had looked at is that wewere lookingfor a solution wherewe let's say let's say there's aninstance where a user hasthey don't have a they're in there theydon't have a paper that they'recurrently writingand they don't have a query they don'teven have a query to writethey might havethey might have an idea of a field ofstudy or concept that they're interestedin they might have an author thatthey're interested in or maybe aprofessor mentioned a seminal a seminalpaper let's say we'll take for exampleattention is all you need you know thatthe seminal kind of transformers um burppapersothat might be all they have they mightthey don't know what query to write sowe kind of viewed the cold start problemas being a way togive recommend really richrecommendations for papers without likethe user having a document or having itbeing able to formulate a querysothe first stage of that has been withthis just the using paper identities butwe're kind of we're we're getting closeto being able to pretty muchsearch from any entity type which issuper exciting um but right nowthe way you'll be able to do it is ifyou have a paper like attention is allyou need that's the paper is all youneed to the name of the paper is all youneed tothat's an awful pun to umto get a recommendation really and youcan feed that paper in or you can feed acollection of papers in and we'll get anaverage we'll get an average kind ofembedding for you and we'll we'll searchoff that paper for you and findwe can find like hundreds of thousandsof papers in its proximity and you knowsome people might say oh you're justgoing to get the papers that it was thatit cited but no what's going to happenis you you're of course that actuallyhow we measure how good the model isthat we build is how manypapers obviouslyfrom that it's cited that it returns butafter that then you're going to startseeing you know these uh papers thatpapers inside decided and so on and soforth and you know you get this kind ofindirect relationship that's what graphsyou know that's the power of knowledgegraphsthey allow you to to query indirectrelationships um and the knowledge graphembeddings which we have you knowideally what they'll do is all thepapers around attention is all you needwill be embeddedin a latent space with other relevantpapersto that to that paper outside of itscitation graph so before we get into thegraph a little more do you think searchand recommendation systems are kind ofthe same tasks like do you find yourselfthe the lessons and i think like rexisis what they use in the papers it's allthe same kind of thing and also do youlike re-ranking is that also kind oflike all the same thing reallyit's we wewe we've had this conversation so muchat work whereit's it's kind of in my opinion it'sthey're kind of melding into into thesame thingit's getting to a point wherewhen does a when does a search become arecommendationwhen does a recommendation become asearch because you knowif if here's if we're getting users toinput like anything like entities suchas papers or authors or fields of studywell that's a query but the query is asolid entity and then we're recommendingbased off of that entity sothe line is the line is kind ofkind of pinning down i suppose becauseyou know the experience in genius isthat you can feed in both a query adocument or now just a paper name butwe're still going to give you the sametype of results list back of papers soif the outcome is the samethe the difference is starting to kindof becomehard to decideumso and in terms of the the re-rankingreranking's really interesting umbecausei i suppose with re-ranking the waylet's say for example how google use itthey have some really clever clever waysof using it where it is kind of maybemore in the traditional search space andthen they find all theseyou know they don't just use the queryas the re-ranking factor they use thingslike geographic location previousintents previous searches to to re-rankyour results for you soi think re-ranking is for me it's like away of ensuring relevancy from a resultset and that that's what i find reallycool you know we use re-ranking we havea genius has a really interestingbookmarking system where we if you asyou say if you find likehundreds of papers you're like wewill rewrite those papers when you gointo your bookmark system and let's sayif you're reading it if you're currentlywriting an essayabout um optimizers or and howoptimizers works work in neural networkswell then the papers that you see firstin your bookmark toolbar will be paperson that and we do that automatically soyou might have you and then the nextweek you might be writing a paper on umi don't know the attention mechanismsand then every paper in your bookmarksystem about it the attention mechanismwill be ranked first so we found that tobe a really interesting use case ofre-ranking where we we contextuallyre-rank depending on what you're doingso yeah i guess like with with the way ithink about it is like maybe when youhave the recommendation system kind ofproblem set up you maybe have like thereinforcement learning optimization asyou are and and i think the genius is avery cool interface for this as you likeas you if the uh user cites the paperit's like the reward for saying yourecommended a good paper they cited itso that kind of thing of um of havingsome kind of maybe reinforcementlearning feedbackis that do you think reinforce andlearning is useful or do you think it'smore of a researchy idea um i i thinkreinforcement learning is going to inthe next five years is probably going toexplode much more into the mainstreamkind ofml world than it has over the last 10 inthatexactly like you said i thinkreinforcement learning kind of the lastmaybe five or six years has been focuseda lot of it has been focused on likegames and atari and solving those typesof problems but now like the rewardfunctions are looking at things likeokay what we define a reward functionaccording toa user like or a user citing a paper andand nowall the same algorithms from that havebeen used previously are applicable andyou know the internet and recommendationsystems are completely based off of youknow netflix you watch a film that's athat's a a one plus in terms of uhthisagent system or this reinforcement andlearning architecture sofor from a lot of papers i've beenreading lately it seems that that kindof crossover into the mainstream ishappeningi'm not like hugely fluent on everythingreinforcement learning but from what ido know is uh over what i want what fromwhat i have read and heard is it it doesseem like it's it's kind of crossingthat threshold um kind of the way youknownatural language processing did six orseven years ago where it was kind ofbefore that image was theimages andwas it was everything and now the kindof text is text and natural languageprocessing is kind of thethe key modality right now when peoplethink of kind of state of the art and ithink reinforcement learning might begoing there next and primarily becausenatural language processing isincredible but most data sets that youknow users or companies have they're notactually text completely textuallydriven they'rethey're tabular they're they're split upinto scalar values and you can you canuse that much more in reinforcementlearning systems as inputs than you canpurely text data of course text isalways great because it's a you can usethose embeddings as additional featuresbutyou can use everything withreinforcement learning yeah and i thinklike with reinforced learning it's likeyou start off with these q tables thatevaluate state action pairs and that'sfine if you're uh say the cardinality ofactions isn't too bad but when you sayyou're using reinforced learning fortext generation and you're going toiteratively select tokens to generateout of a vocabulary of like 30 000tokens that's much harder than uh leftright up down in an atari game so ithink that's where a lot of the problemwith reinforced learning for like qlearning for say text summarization orchat bots or anything so i think but ithink with re-ranking you've alreadykind of filtered down the action spaceto theseuh like one 1000 or 100 that come fromthe course because it's like informationretrieval and then it's like therefinement stage so there's like it'slike this pipeline where we're seeinginformation retrieval and supervisedlearning or say reinforcement learningbut like this kind of uhstages ofcoarse-grained fine-grained optimizationkind ofand exactlyyou know i love that topic of like uminformation retrieval and supervisedlearning but before before i go gettingtoo off the rails with thatso i really want to talk about theknowledge graph at uh kenius and becauseit's so interesting that um that you'reusing the citation network instead ofsay the um the text embeddings thatwould come from like siamese bert and soagain tell me about like generally howyou think about graph embeddings andthen the things that go into thinkingabout building this yeah so for for thisknowledge graph part uh part of kenya'sresearch we're using uh knowledge graphembeddingsand you know i probably got into istarted using knowledge graph or likelearning about knowledge graphembeddings about like a year a year anda half agoand i just found them likeso fascinating the same i found themjust as fascinating as i foundtext and document embeddings maybe threethree years ago it got me probably moreexcitedand the reason i think it's so excitingis that knowledge graphsare a reallygreat abstraction around data becausethis is how i've come to understandknowledge graphs and that any data setcan really be abstracted to a knowledgegraph it's just it's really just kind ofan arbitrary way a very useful way ofmodeling modeling datain a graph like structure that weunderstand that can be multi-entitymulti-relationalwhatever you have soit's a very it's a very interesting wayof modeling data and becausewehumans are very good at understandingthese knowledge graphs and that modelwe've learned to build these knowledgegraph embedding algorithms off of thoseand what was so fascinating to me wasthatthat these embeddings could kind of feedoff ofall the different entities within uhkind of a reallyrelated graph and that they couldevery embedding it's not just it's notlike in text where that's the onlyinformation it has is the text it has itcan have so much contextimbued into the latent into latent spacefrom you know so many different elementsthat these embeddings are just thepotential for richness from them washuge huge for me andyou know in terms of if i thinkif i think of what's thekind of one of the greatest features insearch i think knowledge graphembeddings have to be up there becauseagain going back to that problem oftext text embeddings and documentembeddings are so powerful and soamazing and we really learned so muchabout embedding search from those butnot every organization or company orindividual has these super super richtext data sets but i'd be willing to betthat most companies and organizationshave fairly richyou know populated data tables and likethat's that's that's where the meat of alot of companies data is it's actuallyin these tabular data sets that are youknow they're if even if they're tabularthey're they're you those can be thatcan be abstracted to knowledge graphsbecause that's all knowledge graphs areit's an abstraction soif you can imagine your data set asaknowledge graph well then you canformulate a training process to turnthat knowledge graph into embeddings andthen you can turn every entityin your knowledge graph into asearchable embedding which means youcould do something like what like whatwe can do is you can get an author andyou can search papers from an author oryou can search authors from from authorsand you can do all this end to endrelational searches from any entity toany other entityso kind of for methe potentials there are fairlyfairly limitless because you can kind ofstart to do these really cool queriesthat you could just never do before withwith it with entities and you canlike kind of the same way wordembeddings work you know the uhman plusman plus uhqueen is equal to king you know justwith the same word embedding idea thereis thiskind of additive and like averageembedding factor where you can you canfind like the an average embedding foryou know two two types of entities andthen search off of that so that you canif they're a really really amazingand the entities embeddings are a reallyamazing tool to kind of conductkind ofso many different types of searches offof and i just find that really reallyinterestingum and i think what's happened maybe thelast four or five yearsis there's just been tons and tons ofresearch being published onum models uh kind ofmodels for graph embeddings and they'reyou know they're only getting betterum and itwhat from what i've seen in maybe thelast 18 months is there's just been anexplosion about you know graph neuralnetworks is slightly different the it'skind of there's a bit of confusion withgraph neural networks but the two fieldsare combiningand the graph embedding space has reallypicked up and there's just some reallygreat work being done there andespecially with the multi-entity stuff ithink that's really really interestingyeah so it's such a general frameworkand i'm even i'm having a hard timewrapping my head around kind of thescope of these i think ontologies asthey call them where you're just like asyou mentioned like you you we have thesesql tables where say you you have like apurchasing table and you link it with akey id to go get the item table and thensay like a customer database table as wehave these kinds of linkings in theserelational database systems and youcould turn any of this into a knowledgegraph where the um the edges that linkthe product keys between sql tables saythat's like adifferent kind of relational edge andthen they form these big uh graphs doyou have like a motivating example ofwhat an ontology looks like forscientific literature mining like icould imagine i have a collection ofpapers and maybe they linked to like anorganization idlike say they were published in euripsor icml and or maybe they were they'relinked to someuh like topiclike what are the what are theontologies look like for literaturemining yeah so here's here's ourontology like you know the basic umkind of academic graph umso you have entities so i would say thekind of the primary entities in anacademic graph are papers uh or journalsuhpapers or publications so that's onetype of entity umand then you haveuh journals so where the where thepapers are are published you haveauthors people that write the paper soall of these you can kind of see in away there all of these entities startingto revolve around the the paper entitybecause that's kind of the first classcitizen maybe of this one that doesn'talways need to be the case so you'vepapers you've authors your journals umand then you might have concepts so alot of a lot of papers are tagged withmetadata the way a film would be taggedwith a genre soum attention is all you need might betagged as computer scienceas kind of the high level concept andthen it might get natural languageprocessing and artificial intelligenceas the next layer maybe linguistics andthen you might start seeing stuff aboutthe encoder decoder architecture that'sgetting likethat's getting down on a lower level sonow all of a sudden the the paperattention all you need hasauthor x and yand has uh has the field of study orconcepts uh computer science uh naturallanguage processing and so on and thenthe the paper might bepublished at a conference so aconference becomes anotherum conference can become another entityumso and that's just that's a very basickind of description of maybe an ontologywith five five different entities andthen all those entities can relate toeach other in different waysso and then you know there'sthere's so many different like minientities you can add into this ontologyand you can even haveumtwo entities can have two differenttypes of two or more different types ofrelations between them so you know anddirect relations don't need to besymmetrical they can be asymmetrical soand the ac the asymmetry is alsoimportant when you're modeling theseembeddings because you know you don'twant to implysometimes that asymmetry is just asimportant to imply that something thatsomething in a network doesn't flow theother direction you know just becauseeyesight attention is all you need doesnot mean that i wrote ityeahyeah i also really like like with thatline of thinking like causal graphs andthinking about like putting the causaldags into graph neural networks and thatkind of thinking is really excitingso one kind of question i have aboutthis is like we know that deep learninggenerally with small data sets doesn'twork that well so do you need so then aswe're looking at this graph data are yougoing to need like a massive set of nodetypes and a massive set of entityrelations to kind of make deep learningwork well with this kind of thing thereis something interesting aboutuh this is something i personally i ihaven't read any research about this buti have been thinking about this in termsof knowledge graph embeddings which isthatmaybe overfitting isn't so much aproblemwith small graphs because and here'shere's the way i think about that inthatoverfitting might not be as much as aproblem with a a small data set becausewith a lot of knowledge graph umembeddings you can't you know if a newentity gets introduced to the graphyou have to you typically have toremodel everything you have torebuild the model because it's veryevery model every entity umevery additional entity has to be addedto theto the total embedding so thatit just doesn't work you have to prettymuch remodel redo the model when as newentities are added sogetting back to itoverfitting might not be such a badproblem with a small data set becausewhat you really want is just to find thebest the best embeddings to solve thistypically the prediction task is likeyou have a head relation and tail andit's like predict the tail given aheadin relation soyou know if you'reif you'reas you're building these models if let'ssay the validation and test results arecoming out good and you know havingscoring well on all your all yourranking metrics well then that meansthat your embeddings are you knowrepresentative and richso in a sense because it's kind of aclosed world of your just all theentities in your embeddings it's notlike new text is coming in as you mighthave in a sentiment classification taskslike these are your entities so whetherit's like 30 000 or 30 million like thisis the entity embedding of your of yourentire worldlike so to speak so in that case i thinkit's kind of interest i've been thinkingabout this recently that overfitting isokay or notthe model having a small set of datais kind of okay because if it's learningif it's learning those representationsand it's it's searchingwell around that space well then maybethat's good enough maybe that's all youneedso and you know i've had some kind offun side projects where i've you knowonly had 30 00030 000 or so funnily enough um entitiesandit's been pretty good from what i'veseenyeah i think there are definitelyapplications where overfitting is iseven the goal like like when you'retrying to compress images with deepneural networks overfitting is greatyeah like um auto encoders if you overfit it but then you have a that you canget a latent space from the overfittingthat's useful but i guess what i wastrying to get at is is and you talkedabout it quickly with this i think theword they use to describe this is openset classification so if you take likethe canonical deep learning example ofcfar 10 where you classify images intolike deership frog birdthat and then you you don't assume thatthat there are going to be new classesadded it's going to be those 10 classesall the timewhereas i imagine these graphs and wetalk about scientific literature miningas the application you would imagineadding new node types solike you want to have this flexibleinterface where you add new node typesnew edge types to keep this flexibilitywhere you're saying uh like you just canimagine any kind of relation between twoobjects kind ofso i see it as kind of an open setclassification problembut anyways maybe that is getting toooff the track with kind of just thinkingaboutgraph data and the differences i see inkind of making these ontologies workcompared to kind of like the problemslike glue benchmark c410 and most of thedeep learning academic data sets so whathave been some of like the keychallenges in organizing this knowledgegraph i i saw you recently spoke aboutpi torque big graph and i thinkthat that in itself is kind of alludingto what some of the biggest challengeshere and i think it a lot of it comesdown to a lot of these tools are a lotof the knowledge graph embedding toolsare kind of in their infancythey're not as they're not as kind offeature-rich oryou know the community communitiesaren't as big so probably the hardestthing i find withknowledge graph embeddings iskind of the some of the tooling aroundit like pretty much our entire systemfor building our knowledge graph iscustom because you know we had so manywe had like sothere's 60 60 plus million entities butyou know you know how big citationgraphs are that you're talking likebillions of edges between all of the theauthors and paperssothat gets really tricky to do um andeven just building those edge lists andbuild building those headsets um ittakes like we're lucky to have a bigwork kind of a or work workhorse machineum that we use and it can store all thatstuff in memory but it took a lot ofoptimization to get it there and thenyou know with the the graph trainingit's it takes up a lot of a you have toyou have to when you're training thesegraphs you have to hold the entire thingin memory soyou know you're if you're talking like500like uh embeddings of 500 dimensions foreach entity well then that's under 60million that's 500 multiplied by 60million multiplied by 32 or 64 bit andit starts it starts to hog memory prettyfastumand the problem is you have to typicallyhave all of these entities in memorysomewhere it's not likein in someuh language modeling tasks where you canmini batch it you kind of you alwayshave to haveins either in a in gpu or cpu rememberin those memories have all thoseentities ready to go and as well as therelationsum so honestly the hardest part aboutthis task right now isumjust the pipeline likekind of efficiently designing thesepipelines and i think you know that'swherewe're really happy where we got that tooand it was it wasthat was like80 of my work to get it to that pointyou know that we knew that we prettymuch knew that we could build thestructure you know the models wouldit was a you know a pretty we knew thatthe models would find the relationsbetween the academic graph you knowthat's kind of a proven problem so weknew we could get it therebut that the trick was just setting upthe infrastructure and i think you knowthat's the greater problem at hand andmost machine learning problems thesedays is good infrastructure andyou know i sometimes i don't thinkenough uhtime is given to in terms of tuning tojust like oh how do you how do you buildan edge list of three billion edges andlike load that into memory fast and howand how how do you how do you add customfiltering onto that to add some rulesand you know we had to learn a lot ofthat um and it's a it's a reallyimportant part probably the mostimportant step because if you makelittle mistakes there or the script thatdoes it is inefficient andthere's data loss well then you're introuble so yeah the the hardest part forus was infrastructure and you know weactually as i spoke about pie torch biggraph we initially started off usingthat tool but actually moved away fromit just becauseum it was really really really great toobut theyou know the the the documentationwasn't so up to date anymore and the thecommunityit's not been fully maintained and youknow we couldn't take onuhwe it was kind of a risk if we maybetook on something like pi torch biggraph and it it's not going to bemaintained going forward then we need tokeep this graph up to date soum you have to make choices like thatso with the infrastructure is it likemostly these distributed like datasharding model partitioning kind of orlike um i guess just embedding tableit's not really like a model right withthese kind of algorithmsyeah it's it's the it's the distributednature of doing it and at in the end wewere like just about by the skin of ourteeth able toumavoid doing a distributed setup fortrainingthe the knowledge graph but we knowthat's notthat's not going to be the case goingforward because our data sets increasingso fast so we needed to kind of findfind tools that are ready to go in termsofbeing distributed soyeah that that's what was reallyimportant um and there's there'sactually some really cool work out therein terms of that kind of big graph likeyou know billions of entity learning andlike by george big graph of course is upthere you know there it is it does dothat there's just a bit of trepidationas to whether how long it would becontinually going super cool i just wantto ask you a quick question about thenature of these graph embeddings andlikeyou have to have the entire data set asthe input and that's kind of the natureof these of working with graph datacould you maybe do likei don't know if this like i haven'tthought through this at all but likemaybe you have like a minimum spanningtree or maybe you can justkind of like isolate nodes and take alot of it off of the input is would anyof that kind of stuff work and then youhave the adjacency matrix right like umhave they really achieved like i theycall they talk about like permutationvariance to the adjacency matrices whereyou scramble the orders and then you geta totally different like kind of lookingone zero matrix with thething does that kind of stuff work orand are you convinced these graph neuralnetwork approaches or are you mostlylooking at like complex uh the pie torchbig graph the way that they do the justan embedding table it's not a sequentialneural networkyeah fori likefromfrom what we've seen and used right nowthe best results are coming from thesekind of just tables like kind of complexrotate transientall of that familyof embeddings where it's effectively avery very very shallow network it's likedo you even consider it a neural networkor is it just like a look up you knowthat's awhat but it works really welli think in the last two years or sorrysince 2020 which is two yearsunfortunately um is we've seen thatthere is research coming out that'scombining graph neural networkapproachesum the problem is it's it's adding waymore time to an already slow fairly slowprocess um in terms ofwhat changes can be made can you do wealways need to store the entities inin memoryi i'd like to think not i don't i'm notreally sure like what's being done inresearch right now in terms oflike counteracting that i'd love ifthere were solutions like that work thateventually do kind of solve that alittle bit better because it isit kind of makes it prohibitive to a lotof people that want to just get startedyou know if they have a large data setand they want to build a knowledge graphembedding you know they might need amachine with like you know 100 gigs ofram you know not everyone has thatumand then in terms even in terms of justhosting those things in in productionit's very expensive toprovision machines of that size soyeah i don't knowwhat the what what's ahead for like kindof these knowledge graph embeddingmethods whether they're going to focuson improving the actual just thethekind of the the scoring functions andkind of just improving the quality ofthe embeddings or if they're going tofigure out a way to optimize the howthey actually like do it whether it'slike a mini batch approach or whateverum obviously with the with the shardedapproach what is cool about you knowdoing the sharding isyou can of course theoreticallytrain on a graph of any size you justadd an add a new shardbut again you can only do that if thethe size of the data set is still ifit's a thousand gigabytes in memory wellthen it's just a thousand gigabytesstarted up 10 times into on likeso either way right nowas things standit's you're storing everything in memorysoyeah i don't i hope it would be amazingif we kind of move into an approachwhere you can kind of batch it to somedegree but ii'm not sure ifwhere kind of the research stands onthat right nowwell do you like the uh like deep walkor no to vect style of uh you just walkthrough the neighborhood to get sort oflike a self-supervised learningobjective where the goal is to predictyour neighbors such that you can kind ofbatch it up and isolate the nodeembeddingsyeah the the notes of echo idea is isreally cool you know it's very you knowcoming from natural language processingit it it kind of worksit's very intuitive to to understand itfrom what i know though is no no to vecand deep walk to date haven't had kindof the same results as kind of complexand rotate and these very specificumuh k knowledge graph embeddingmethodologies um but yeah you knowthat's not to say that you couldn't usesome sort of graph neural network ahybrid approach to to actually doexactly that to like to you just feed infeed in sub graphs as input and then usethose sub graphs tostart predicting it like adjacency andall of that kind of stuff yeah i thinkmaybe an interesting research idea wouldbe like because in uh transient complexyou have the contrastive alignment thathelps with embedding learning so maybeit with deep walk if you did uh like sowhat it is if people aren't familiar isit's you are in a graph and you likekind of flip a dice and uh or roll adice and then you traverse the edges tojust keep doing random walking along thegraph maybe you would have the positivepairs like if you do three traversalsand the negative pairs are like 8 10 youknow likek and n to do the two parameters andmaybe then having the contrastivelearning objective would be somethingthat deep walk is currently missing butyeah deep deep walk sounds nice becauseyou don't have to put the whole graphinto thememory and it sounds like that'ssomething that's given you a headacheyeah it doesn't sound funwell yeah that's exactly i'd i'd reallylove if that's the wayif with deep water that's the way thingscould go um it would be really amazingbecause it would just speed up trainingso much because the the problem is a lotof a lot of umgraph embedding models are actually doneon cpu becauseyou use the machines typically you usethe entire machinesram to store all the embeddings and theneveneven let's say any any of the theapproaches that are using gpus it'susually a hybrid mix of like well thethe embeddings are in uh kind of justthe machines normal ram and then we subbatch some of them intointo the gpus ram but you know there'snot a lot of gps out there with kind of100 gigs plus of ram so you're going toneed like an array of themso yeah if you can do this this minibatch you know like effectively justpumping in like sub graphsin as the batches into it that would belike a super super elegant approach soas i said you know i'm if i hope thatresearch comes out in the in the nextcouple years will be greatly helpful tome so if there's any uhresearchers thinking about working onthat that would be really amazingyeahyeah so so so many interesting topicsaround graph learning and um so kind oftransitioning into the webv8 vectorsearch engine and before we getparticularly into uh how you've used itin your experience with it i think kindof staying on this topic of the graphswe recently had two web demos ofwikidata and wikipedia so how do youthink about just from a high level likeof the embeddings and the differencesbetween what you would learn fromwikidata and doing say pytorch big graphcomplex these kind of algorithms on thewiki data embeddings compared towikipedia where you say siamese bird tocontrast sentences and overall thinkabout what what you would do with neuralsearch for those two kinds of data setswell the wiki data in data set is areally great example to work frombecause it's it's it's a naturalontology it the ontology is perfectlystructured to doto to build probably you know therichest knowledge graph umembeddings possible and that's you knowthat's what wiki data do provide youknow that's how wikipedia is part ofthey they power their own knowledgegraph using using that ontology soit's a what you described there's areally nice way of talking about thebenefits of both because you know it'sshowing thatyou you use it's using your data in theright way and squeezing the most out ofyour data so with wikidatayou you have this natural hierarchy andontology andrelatedness betweentopics concepts classes uh individualssothe whole modeling is done in this kindof graph like structure so you of coursewill when you're building your knowledgegraph embeddings you'll exploit thatgraph structure and you'll build youryour edge listbased off of that and then you'll trainyour model to predict on these tasks andthen you can do novel novel entityuh this uh kind of uh discovery viathose learned embeddings so you know youmight discover you know the thefootballer cristiano ronaldo even thoughimagine in a world where he's never beenrelated to manchester united might getlinked through you know they havesimilaryou know managers or teams that he'splayed for that those kind of thingswillappear um so yeah and then with thejust the wikipedia what's amazing is youseeyou know just the power of document andsemantic embeddings with textand again it just show it it really itkind of comes down to like what's yourinputwhat what inputs do you have at hand andthen what's your expected output becauseyou knowuh inand then you just this is how we look atsearch it's like it's kind of they'reall coalescing into one and that youknow you might have a search querytherefore you'd like text to work withtherefore you search wikipedia for textand you'll find things that aresemantically related there but if youcan identify an actual concrete entitywell then the richer search studio mightjust be to search on that entityso i think really what the two show isthatthey're both like the entity embeddingsand these like latent embedding methodsthat we've developed over the last 10 10or so years or even longer of coursei they're kind of all distilling down tothe same thing to the point that youknow you can use entity embeddings toimprove the quality of a language modeland you can use language modelembeddings to improve the quality ofknowledge knowledge knowledge graphic soreallyall of these embeddings are just kind ofdescribing like that we we've learnedways to model kind of semanticrelationshipsfor any type of input or entity and youknow this is extending just beyond uhknowledge graphs andtext but also to images and to sound youknow any modality can be embed into alatent space and you can search off ofthatyeah like uh the multi-modal learningthing is so interesting but i thinkparticularly the relationship betweengraphs and text is one of the mostinteresting ones to study right nowso i mean yeah this idea sounds so greatbeing like this is an entity-centricquery let's go get the graph embeddingfor this one it's particularly usefulfor this one but have you seen do youhave any like um references or anythingthat stood out on some example wherethey really do really bring this to lifei've seenmy favorite one is really where theytraverse the knowledge graphs to createnatural language sentences and just usethat as training data for the languagemodel it'sit'sit's a little bit of like a naive kindof data augmentation strategy but to meit's the paper the paper is titled uhturning tables and then some otherdescription but that's the high levelthing if you want to search it anyonelistening but so basically you traversethe knowledge graphs to form sentencesand the sentences of the training datafor the language modelone kind of interface for the two thingsbut like so so what you're describing islike a late fusion technique where youhave the two embeddings and then laterthey come together and or some kind ofquery model says let me see the graphembeddings for this particular one onething i think that i've noticed going onwith a couple of search orrecommendation systems i use both likeyou know google or something likespotify i a lot of those engines tend tohave if you put in let's sayif you type in a particular artist intospotify or let's say an actor benaffleck into google if they canif you search and they can relate thatentity with a very very high confidenceto an actual like like via text so likevia those two words just like benaffleck that the entity ben affleckexists exactly as it looks or likewithin a very very high confidence scorethis is that entity well then they justconduct they'll do their tech search andthen they'll conduct a knowledge graphsearchon that entity to find all of that nicerich metadata you see over on theright-hand side and that's really thepower of the two and it's the same inspotify where you know if you're typingin a musician or a song or somethinglike that they they do mix thesemodalities where they'll use text thatyou're they'll turn your text query intoan entity search in the backgroundbecause you know they can relate it tobeing a concrete entity um and that'si think that's you know that's kind ofshowingthat you know knowledge knowledge graphsearchcan be realized in many ways you knowall you need to do is get thoseembeddings and then you just need tofigure out ways to you knowof course you can't always guaranteethat your user is going to like selectthe entity for you but you know theymight they might do it via a query andif you can of course if you can findthat entity in some way or another youshould search on that because the entityembeddings tend to be very rich like wewere discussing earlier on because it'sthis kind of closed world andthey're allowed to overfitumso yeah that's kind of how i think abouti've noticed like a lot of a lot ofsearch engines typically do tend to dothat these days they tend to like to mixthatmakes the two or in the background kindof silently resolve to an entity if oneexists and the same with genres you knowif you type in a genre intointo into these libraries they'll findkind of the artists and the songs thatrevolve or exist within that within thatsphereyeah super cool that that definitelymakes a lot of sense and it seems likesuch an exciting area and i i like thespotify example a lot because of thesimilar to wiki day i guess similar tothe scientific papers too but it reallyhelps you visualize this concept of kindof like entitycentric searching so so now can youdescribe you have your graph embeddingsand can you describe your experiencewith using the wva vector search engineto search through the graph embeddingsand i really like one of my favoritethings about your article describingthis is how you describe how the apilets you rotate in plug and play thedifferent components like you can swapout your embedding model swap out your an index and that's also something that ithink is awesome about wv8yeah so you know we've beenonce we earlier like it's pretty muchsince we've had these embeddings uhtrained and you know ready to deploy wewere like okay what's the next step wellit's just not going to be possible to doabrute force search of like 60 millionplus embeddings inunder two seconds or under a secondwhatever we needso of course it was going to be um a nbut you know there there was a lot ofoptions there's a lot of options outthere to choose from but what reallyjust you know we really stood stood outfor uswith just because of like you know acouple of it's not because of one thingbut because a couple of really keyfeaturesand i think what all these key featureskind of allude to is that you know theweave the whole design of a vb8 searchengineisis designed for business problems likeours like they're not just trying totheoretically dovector searchlike straight vector search theyunderstand that vector search is neverjust you know a flat vector searchthere's often going to be cases where uhpost filtering is gonna or pre-filteringmight be neededandthat use case is just the the api isjust so well designed to those use casesthat it became really attractive soyou knowum and then we also needed we neededthis thing to be scalable like kind ofhorizontally we knewi we had experienced using h and sw butyou knowthere whether we could fit this all inone machine would be was kind of up inthe air and then you know by the rightnow as i speak from as of today you knowwb8 is horizontally distributed backthen it wasjust about maybe it was on the roadmapbut you know from what we had seen fromthe team and stuff like that they werejust moving so fast we hadso much confidence that they were goingto get there when by the time we wouldbe going live with them that's kind ofexactly what happened which was amazingbutyeah we v8 is really it's exactly whatvector search needs which isa really really practical useful userapi whereum this the core parts of the system areare really set in stone but they havethey take this modular approachspecificallywith the uh a n indexes and i thinkwhat's really useful about that is youknow if we think ofkeyword search and a lot of keywordsearch engines that are out there someof them are kind of stuck in the mud interms of using like olderolder algorithms just becauseuh 20 15 years ago the initial code basewas designed and it wasn't designedmaybe in a super modular way soswitching out these ranking algorithmsor whatever it is was difficult whereaswev8 is really interesting becausevector vector search is again andprobably still only in its infancy youknow what thethe a n indices that we usetoday probably in five years will bewellcompletely outdatedandthat built that that knowledge is builtinto the the architecture of we v8 sothat like gives you so much confidencethat like if when the new latest andgreatest uma n index comes out or a n strategycomes out we don't need towe're not going to have to like rebuildeverything or rethink our entirearchitecture we're just going to likeplug and play and that's likethat having that confidence in the thesearch engine you use is like incrediblelike it's it's so so useful to have thatand to have that you know as a as acompany that's trying to scale just toto to know thatwe're not gonna have to like spendlike hundreds of hours of engineeringtime again solving this problem that youknow when the feature is ready we'lljust it might be like a couple of linesor a config yaml change just to point tothis new and rebuild on this new indexso that's like you know that's that wassuch a win in of itselfand then it is we're pointing out thatyou know with the the filtering systemthe scalar filtering system that's doneon the index isvery very elegant because it's it's nota post search filter it's a pre-searchfilterand how they that's that was what wereally wanted because we had looked atyou know we could have developed our ownin-house system that did post searchfiltering but that's not really what wewanted we wanted pre-search where theywould have an allow list and a set listof entities that would be allowed to bein the search and then rank accordingand retrieve according to that waybecauseyour much higher chance of getting backif we ask for a thousand a thousandvectors or a thousand objects we have amuch higher chance of getting back athousand with the pre-filter rather thanthe post so that was very importantbecause you know if you're building asearch engine you kind of want toguarantee that you're going to give atleast a page of results to a userum so that was really that was reallyreally amazing to see thatand then you know there's you know whilewe're usingrev8 primarily right now for uh theknowledge graph embeddings that we'vebeen testing out some really fun um kindof future future projects with it in thetext in the tech space and you knowthey've it's really helped us re we wehad this kind of crazy idea almost justa couple of weeks ago and we're likeokay let's see if we can spec this outreally fast and we knowwith just a single weekend hack for me iwas able to kind of put it together andi kind of show an early proof of conceptjust using wev8and it's because of the the modulesystem in terms of uh you know text totransformers you know that thatentire pipeline just saves so muchboilerplate code and so much likethinking that you'd have to do as amachine or an engineer just to to get aproject up and running and you canreally kind of move fastand because because of that modularsystemit kind of has two advantages one isthat you can you can build a module onceand then it justyou don't have to worry about like wherethe inference is coming from and you caninteract with the api very humanly youcan just feed it in text you don't needto do this transformation step ofturning that text into a vector yourselfbecause vb8 handles that and it'sincrediblebut also you knowjust from an open source point of viewthe fact that these modules can be canactually work just becontributed as an open sourcecontributionyou know there's a lot of open sourceprojects out there that are open sourcein terms of the project is open sourcebut there's a team working on it the vbateam are open source and likei have to say they welcome the communitywith open arms in a very very true openopen source fashion and that's veryattractive for meum to kind of when i'm deciding on thesethings and and also to contribute to youknow it's it's been fun to contributewhere i can to ev8 because it's a verywelcoming communityyou know bob and etienne and everyonethere are veryvery welcomingyeah and to stay on the ver yeah itreally reminds me of hugging face howwell you can switch out the componentsand i think just yesterday deepmind hadreleased two papers where one of them isa 280 billion parameter language modeland that probably got most of theattention because of you know 280billion parameter model but the otherpaper they they published was aboutinformation retrievalplus supervised learning and so we'veeight is in my view the the perfectplatform for testing these ideas ofcombining information retrieval and thensupervised learning so you retrievedocuments and then you append on yourquestion answering system andthis modularity if you want to swipe outthe embedding model for the informationretrieval you just point to a differenthugging face model path if you want toswipe out the a n index same idea justdrag and drop h sw something else andthen if you want to replace thedownstream question answering thingsame idea just to point to another setuh it's you know it's it's simple whenyou think about it but the way they'veimplemented it you know it they've madeit seem simple butit they've really done like you knowsome really great work to get it to thatpoint and you you can tell how much kindof thought has been put into thearchitecture and then how passionatethey all are in in what they're workingwith andit is a very good community to be a partof and there's not all open sourcecommunities i like that and it it takesa lot of work to build that kind ofcommunity and i i i really reallyappreciate it and it's very funto engage with them then and to you knowlike there have literally beenreleases that solvemy problem like my specific problemthere was a release that fixed that andlike that's just incredible to toto feel thatyou know i presented a uh with some sortof problem and then my work alongsideethiopianand it gets solved via release you knowthat that's very special to to be ableto have that kind of helpum when when you're building with thesetools umand you know it's just as you weresaying that in terms of yeah you canswitch it out you know the vector searchis here to stay in it it's it's onlygoing to becomeuh biggersowe're going to need some really goodvector search solutions and from whati've seen so far like vv8 is by far kindof the most feature-richthat that there is and the the theeasiest to interact with and that's downfrom like you know the api design wouldbe a graphql down to its optimization inits back-end you know it'sit's written in go which is which ithink is a hugely under-appreciatedlanguage in terms of machine learningyeah the applications in machinelearningi've thought could be really reallyuseful not maybe not in terms ofexplicit like modeling or training butin terms of infrastructure and termslike you know vector search like they'redoing i thinkthey really understandwhat the strengths and weaknesses orsomething like go around that point inin that world and yeah they've they'vereally done a great job with ityeah if we could stay on the likelike go and rust and these kind ofthings is something that i can't help atall because i don't know too much aboutbut if we get just like i just want toget a little more into the vectorsolutions and kind of the conceptualideaof uh like the h and sw algorithm and ifyou wouldn't mind if i could kind oflike explain my understanding of hswfrom scratch and you could explain to mehow the pre-filtering thing works sofrom the high level overview with thesea n index algorithms we have continuousvector embeddings for everything in ourdata set and we want to try to do saythe brute force solution is to do thedot product comparison with every singlevector and obviously we want to avoiddoing that so we say a naive algorithmwould be to do a k-means clusteringwhere you have a centroid thatrepresents that routes you to likeyou're most similar to this centroid sogo look through thisuh you know the cluster vectors thatproduce that centroid in the k-meansalgorithm so the way that i understand hand sw is that you build up thesecentroids and then you connect thecentroids in a graphsuch that you can use the small worldnetwork effect to say thati'm likely six hops away from thecentroid i need to get toso is so that's kind of as far as iunderstand h sw how does the symbolicfiltering work with thatyeah and so i think with in terms of theh and sw then what you have is on top ofthe on top of the navigable navigablesmall world graph it's like it layers itlike an onion so it's likeyou you can kind of it getsit kind of saves you time andkind of com compute by each level as youtraverse down islikethe top levels are pretty sparseso because they're virus they can kindof find pretty quickly what the the theshortest distances to your to your queryare and then every every s that likedownward layer starts to is morepopulatedso you can kind ofpick a pick a few candidates from eachlayer of the onions so to speak and kindof find which one find which vectors areclosest to itand thenkind of decidepick on the two best routes down downthat avenue of uh layers and which rootsare going to be uh are scoring highestand then just pick the name theneighbors attached below that in eachpart of the graph that's on a high levelbut in terms of how i how i think thefrom what i've seen in the code and howthe pre-filtering is done isimagineuh in terms of post filtering you wereto do a query and you wanted it to beumif it's papers published let's sayacademic papers you wanted paperspublishedfrom the year 2018 onwards well in apost filtering world what would happenis you do your do your search get backherethe top 1000 results and then you postfilter you'd actually retrieve the dataobjectsand you'd retrieve the data objectsand then you'd filter that set of athousand to beumyou'd filter that set of a thousand andthen you might be left with like200 papers um left and thenunfortunately what's happened is you'veasked for a thousand papers and you'vegot 200 because you've applied yourfilter post factwith the pre-filteringwhat the way i think they've done it isyou have your query and it what it doesis ityou execute your filterfirst in terms of okaythis has to have an allow like this hasto be from the year uh 2018 onwards sothat creates what's called like an allowlist right it retrieves like a list ofids that meet that criteria so now asyou start to traverse the traverse thegraphwhat you do isyou pick your you pick your nodes alongthat in the in the h and sw graph fromall the layers down and as you hit anode your first testdoesn't meet the criteriaand if it meets the criteria great youcan look at the node and all itsneighboring nodesbut if not you have to you can youpretty much add that node to like a skiplist and you skip itand you keep trying to fill up theresult set until you have a thousand sothe idea is that you're you're skippingones that don't meet that like allowlist and you're you're finding the nextnearest ones after that because youhaven't asked for a filter so the filtermight get rid of some really relevantpapers or results but you you do have anexplicit scalar filter so that's okayyou so you just you fill up that resultsetwith the next 1000 results that wouldlikethat would have been um kind of rightafter that according to that filter sothat's why it's you know it'sit's very powerful to be able to do thatand as i said earlier especially whenyou're designing like kind of a searchengine solution like us where you'retrying to give as many results aspossible and trying to keep that resultspace um interestinghaving that pre-filtering super powerfulthank you so much for that yeah i i gotdefinitely a better sense of it now ithink that made it very clear i don'tknow if that's like a perfectdescription of it but from what i'vechatted with etienne about it that'slike the general gist but of course ifthere's if they if they listen to thisand they have any any extra thoughtsplease feel free to free to contributeyeah yeah eddie has been explaining itto me and i will probably even ask himagain i just trying to get it you know igotta keep hearing it to finally have ituhi don't know how he stores it all in hishead he is very uh he's very very uhwell read on hmsw and everythingactuallyyeah yeah they're such a great team andso so to step a little away from wev8and get back into scientific literaturemining and kind ofcould you kind of tell me like um youknow i also really like scientificliterature mining i think of what i dowith henry ai labs of doing paperreviews as being like a form of manualscientific literature mining and as ithink about it i try to think how couldi automate these tasks i'm doing howcould i turn this into something thatwould be like an automated system how doyou like what's motivated you to work onscientific literature mining in that bigpicture kind of thinkingyeah ii supposethe motivationthat i alwayskind of try andreally prioritize for myselfis thatit's as an undergrad it was really hardit was really hard to get startedand and ithat's something we always try toemphasize our keynes kenius works forusers like we we try and design ourproduct to be like here it's your firstsemester of university tobe your your final year of your phdumbut you knowfor people early on in their academiccareers it can be really really hard toconceptualize like a space and like a alike a topic or whatever it is you'reworking on it it can be really hard tounderstand what that even encapsulatesso for uswhat's really really exciting when wethink about academic search andresearch mining is figuring out a wayjust toto mine research andtoto make it really intuitive to discoverdiscover new things and you know earlieron i brought up the kind of thewikipedia example where you know we'veall been on wikipedia whereyou go into like a black hole ofdiscoveringinformation about topics andyou can learn really fast and you cankind of dig down into different levelsof granularity and that's what we trythat's what you know that's our northstar in terms of emulating that it'smaking like academia and topics andconcepts in academiakind of fun to exploreumand making that somemaking it so that we can enable someonein their first semester to get value outof uh out of academic publicationsbecause you know that it canit can be very intimidating academicpublications can be really intimidatingand it can be hard to look at a paperand to start working on it or startreading it or to to know even if it'srelevant so we feel that the sooner wecan get people comfortable withdiscovering papers and researching themand finding kind of the topics thatthey're about the better you know it'sit's it's good for researcherseverywhere if everyone learns to kind ofresearch a little bit better orunderstand what like what their owninterests what their field of researchrevolves around you knowso that's how that's kind of how wethink of think of itdo you think genius itself could becomea scientistlike it could have a text generationconnect to your system and it itselfwould be a scientistyeah we've we've we've done some we'vehad some fun with just uh textgeneration models just like internallyjust to play with and just to look ateven just to the point of like abstractgeneration umit is quite coolum i'm not sure if it's likeif genius is ready just yet to likedo it itself butit can write some kind of likesemantically convincing abstracts if andand paper titlesumbut and you know there there is aserious use case from that and that youcould actually have a block of text andyou could suggestlike a title or you could suggestum something like that off of off of adocument so in that case is thatconsidered writing science absolutely iwould say so you know titles are reallyimportant to finding papers butyeah i i think likeprobably it's most efficient use caseslike just being a really good aide rightnow to to to researchers of all kindslike all levelsand i saw one paper that was uh thatreally stood out to me and furtherdeveloped my interest in this is titledcan we automate scientific reviewing andsimilar to what you just said they dothe same thing of this abstract waswritten byyou know our text generation modelsso so yeah i think especially thingslike um survey papers literature reviewsit does seem like something like geniusor these models and implemented inkenya's areyou know maybe like five to ten yearsaway from being able to do a surveypaper literature review completelyautomatedso then so an idea that i've beenworking on is um and i published a papercalled keras burt which is where youtrain a language model on keras similarto the codex idea where you're trying toyou know write code with text generationmodels so trying to see ifyou can plug the language model intocara like interface it with keras towrite deep learning code and run deeplearning experimentswhere our data sets and all the ideasare completely digital anyways like ifyou have to say do uh by like a biologyexperiment where you have to have like areal physicalsay like maybe robot interacting withthe world i see that as being a slowerinterface with this but as you do deeplearning research like the kenius aiscientist can write experiment code anddeep learning get the results and thenlearn how to interpret the results andwrite papers and like be a scientist isthat where you think this is headedbecause that's that's where i think thisisi i think you're right i don't thinkit's outside of the bounds ofpossibility that that's like where we'regoing and i i you said it earlier withthe systematic review that's likethat that to me would bea really goodkind of firstattempt at like first attempt attrying to see how good it is at thattask becauseyou want to kind of ease it into itright you don't want to like take onsomething too ambitious and you don'twant it to publish something that's justabsolute junk or just like completelyoff the bat but this is something like areview a literature review that's likethat's manageable and like you know it'sit's it it feels like a task where itwould be harder to mess that up once itonce it gets to that stage andum yeah you know language generationstuff is getting really insane just withwhat it's getting i i saw thatpublication you were talking aboutyesterday i think the model is calledgopher from deep mind and like theoutputit was like the way it was likethe thein how informal it was in theconversation was justinsane umand soi think i'd be it'd be foolish to to saythat in the next 10 years we're notgoing to getsomewhere close to that point especiallyin terms of like a review literaturereview i think that's kind of inevitablealmost with with ai text generationso yeah so in addition to the literaturereviews where yeah it's like it's likethe product is the survey paper theintroduction to the topic forfor someone um so as someone withexperience building a software productand bringing this to like a real worldsas kind of thingwhat do you think would go into say youknow reviewing your paper you you querylike the gbt3 api say or whateverwhatever the model is called and thenyou're kind of building a sas productaround automated scientific reviewinglike from your perspective what like andi think you have a different way ofthinking about this because you have anactual software product and not justit's not just like research ideas sowhat do you think about that kind ofproduct idea and what what it would taketo bring that to lifethat's interestingbecause i thinkacademics are like people are where ofcourse people are uh creating thesemodels and thatthat they're you know the the advanceshave been really good in terms ofnatural language generation butacademics are also huge skeptics interms of like acceptingaccepting what the output sothis is a trick and we've discoveredthis you know we deal with academics alot so you know you really have to proveaproduct's worth in order to get it theresoi don't know like i think you'd have toyou'd have to show that the product islikecost saving or time saving in a reallyreally umin a way that's like kind of been unseenso whether it's likein terms of like reading givingsummaries of papers and reading them andgiving like umnotlike generated generated summariesbecause we've we've experimented withextractive summarization but abstract ofsummarization is quite difficult andwhen we've experimented with that it'sbeen pretty badum so i thinkif i if i saw in the next couple yearsthat abstractive summarization from likelarge language models was getting reallygood that would be a killer use casebecause in that case what you're talkingabout is you might actually be saving aresearcher like hours in terms of likehaving to dig through a papers um a 30page paper and being able to like findits key points and letting it letting aresearcher like giving it a kind of deepnot just like an abs like not like justthe abstract but giving it like keypoints from inside the paper about maybelike what algorithms are being used orthat stuff that they could like identeffectively helping them identify whatparts of a paper are relevant to theirown research area that to me would bereallyreally amazing and then you you reallyneed to figure out a metric for rankingthe quality of those abstractionsand that to me that that's a lot of thetime what weum always circle back to is how do yourank it's the relevancy like how do youhow do we judge a good search result andhow do you in this situation how wouldyou judge a good kind of abstractivesummary um of like 20 papers you know soyou'd and you just you do that based offofumwhen you're developing a product you cando that based off of like input signalslike uh whether it's a like button orthat they they cited it or whatever soyou just you you learn from thesesignals what's workingis that does that answer your question abit yeah yeah and um i think i'm one ofthe biggest fans of the research from uhthe allen institute with uh like whatlucy wang and um sorry and kyle lowepublished two of two researchers that ifollow closely like from on twitter andseeing all the things they do with theallen institute and semantics scholar isthey've done such a good job of breakingdown all these tasks like you mentionedlike um like classifying what they callsite tenses like citation sentences soyou classify it of the intent so so yeahit's like again there's there's likeabstractive summary pretty ambitiousfirst step it might be better to firstjust like have models that classify likewas this experiment set up correctlyuh do they properly[Music]you know communicate the background soyeah i definitely it definitely won't bejust right to the reviewsand there'll be all sorts of subteststhat's an interesting way of thinkingabout it because i think uh in terms ofjust actual like structure structurallooking for structural appropriatenessof a product of a project becausethere's there's been some really cooloutput just in terms of language modelsthat you know they look at like documentstructure um rather than like the textitself and i think that's a that's areally nice point in terms ofum you know you could even justidentifying that a cert like a certainpaper doesn't fit like the criteria of acertain structure like oh you can kindof make an inference that maybe it's notof high quality if you know they haven'tintroduced uh the the domain problemcorrectlyor you know that the the citationsaren't properly done or that it doesn'teven have a conclusion you know justbeing able to detect stuff there's a lotof you know the there's a lot of reallygood publications out there but there'sit's a lot easier to publish nowadayswithout any peer review and then thequality of those papers can beum fairly variable so being able toassess like high quality papers throughalgorithms like that and kind of largelanguage ones that would be very verypowerful like being able to kind of puta certificate just being able to tag apaper being likeof kind of a certainstructural qualityyes so interesting and um yeah like sowe're talking about like um scientificliterature mining review platforms likegoogle scholar is i think the famous onethat everyone uses to build up theirmaster's thesis and you know semanticscholar as i talk about the alleninstitute i think in my opinion they'veimpressed me the most with the researchoutput and almost really leading thetopic i'd say but um so what do youthink about papers with code are youfamiliar with that and how they organizeit how do you contrast that with geniusand think about like generally yourstance on what they're buildingyeah papers where code isreally you knowwe i've i look at them a lot in terms oflikepresentation of the information they areyou know a lot of what we do isum it just goes down to that ux end ofthings where you actually just try andfind a really nice clean way ofpresenting the information and you tryand you try not to overload the the usertoo muchand i think sometimes when we're dealingwith these like kind of high concept aiproducts that element gets overlookedand i think it's really crucial you knowthat's that's probably the part that wethink about and change up the most whichis like how good is this ux so thepapers were code isreally really good for that um and ii've always enjoyed going i alwayswhenever i go on to papers report i'vealways enjoyed just just the interfaceand like navigation across the site isreally really goodum i suppose with us because we weactually extend beyond just scientificliterature like we we actually can wecan do like science and sorry likeliterature the arts the humanitieshistory business so for usyou knowwe kind of we have to keep on with somemore of a generic like way of presentingour papers and and our publicationsbecause it can cover any field so papersof code kind of has the benefit of youknowit's papers with code so it it thesethese things have a kind of you have apaper and you have a data set and thenyour code presentation typically so wehave to abstract that down a little bitmore just to kind of a paper structurethat being said our data set doesactually include links to codeto code soit would absolutely be something wecould look into in the future when witha bit more um time and manpower you knowwhere a small team at right nowum but yeah whatjust to go back to it papers somethinglike papers or codethey are a shiny example of good designand like how good design when searchlike go hand in hand like search searchengine search recommendation systemslike it's like a 50 50 split between thequality of the recommendations andsearch results and the the interfacefor interacting with those results andthat that relationship is superimportantyeah yeah super and i i love the um thetext editor build in i think that ishuge i read a paper from some google airesearchers called wordcraft and itdocuments how they're using like a chatbot to help people write things and ithink that integration of the texteditor is is a really cool thing that'sin kenya that i haven't seen anythingelse that uh does it that well and i andi love the plug-in with google docs ithink that's a really nice uh transitionfor people so anyway so we've covered somany topics and thank you so muchcharles i you know i think this wasdefinitely one of the most informativepodcasts and you know it was it was alot of fun to do this so thank you somuchthank you very much this was a hell of alot of fun i really really enjoyed itand thank you very muchfor for having me on i really appreciateit", "type": "Video", "name": "Weaviate Podcast #2: How Keenious uses Weaviate to enable semantic search through 60M+ academic PUBs", "path": "", "link": "https://www.youtube.com/watch?v=hU7GJEidaUE", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}