{"text": "Please check out VL-Adapter (CVPR 2022): https://arxiv.org/abs/2112.06825 ! Thanks for watching the Weaviate podcast! \n[Music]hey everyone i'm super excited to bepresenting a new style for the we vvapodcast in this podcast episode we havethe authors of the paper yielding sung jmin cho and professor mohit bonsal aresearch team from the university ofnorth carolina at chapel hill to explainthis paper vl adapter parameterefficient transfer learning for visionand language tasks and the reason thispaper stood out to me as being sointeresting is you see how these thisbullet point right here with thehighlighted green text you only need toupdate about four percent of theparameters of clip when you'refine-tuning it for downstream tasks soyou don't have to have the cost storageoverhead of fine-tuning the full 100of a pre-trained clip model rather youonly need to have four percent of theupdated parameters which are added inthese sparse adapter layers that areinterleaved with the original modelarchitecture so this podcast willcompletely get into the details ofexactly how these authors set up theseexperiments for having these sparselayers that you use to fine-tunesomething like a pre-trained clipcheckpoint for different vision andlanguage downstream tasks and that'sanother really interesting detail aboutthis is exploring exactly what visionand language data sets the authors wereusing to explore these questions thispaper was accepted into cvpr2022it's a really great read i highlyrecommend it and i really hope you enjoythis podcast breaking down the detailsof the vl adapter paper so quicklybefore getting into the details of thepodcast i also wanted to point you tosome examples of using clip within thewev8 example so in case people out therearen't familiar with this if you go togithub semi technologies slash we've ateexamples and please leave a star whileyou're at it you can see all these greatexamples of uh things that people havebuilt with we've eat you see searchingthrough podcasts searching through plantinformation harry potter questionanswering and in this quick littleexample i want to show you this clipmultimodal text image search example sowithin this clip multimodal text imagesearch you see how you can build a userinterface for having your text to imagesearch through using this pre-trainedclip model so hopefully this kind ofseeing this in action and seeing how youcan use this will further motivate yourinterest in vl adapter and knowing thatyou could you know fine tune a clip addthat fine tuned model into we v8 andthen you can do this kind of text toimage search and it's very exciting ithink having this multi-modal image andtext space i think is really somethingnovel you see when wev8 as you'resetting up your docker compose file youcan go down tosay clip as the model that you want tovectorize your data hey everyone thankyou so much for checking out the wvapodcast i'm super excited to bewelcoming a research team from theuniversity of north carolina the recentpaper v l adapter is such an excitingfinding with the clip model the clipcontrastive language image pre-trainingmodel is such an exciting model forproducing embeddings of both images aswell as text and having this visionlanguage cross model multimodal kind ofcrossover and it's one of the mostexciting examples of this is the clipmodel it's built into the vva backendfor something that you can vectorizeyour input data it's available with ginaai's clip as a service and it's beensuch an exciting thing toplay around with generally the clipsoftware and i'm so excited to learnabout this vln vl adapter it's astrategy to only have to fine-tune aboutfour percent of the parameters of clipsso as we talk about taking this uhoff-the-shelf pre-trained model and thenusing it in the typical kind of transferlearning flow as we're similar withthings like hugging face where you takethese pre-trained weights and thenfine-tune them on your data set this newadvancement in vl adapter is going to behuge for the you know the costimplications of doing this and trying tofine-tune the massive uh model so sofirst of all team thank you so much forcoming on this podcastthanks very much thanks very much thanksfor having usand so uh can you tell us about like theif you had to explain vl adapter sort ofas fast as possible how would you uh umso view adapter is just a work that wewant to adapt to the existing languagemodel a combination of language modeland visual model 2 vision language taskwith the minimum parametersused possibleso yeah and we just want thoseadded parameters learn visual languagelike representations but the rest ofthem still keptthe sameas before soyeah it kind of brings a new paradigmthat how to design the vision languagearchitecture that you don't have to findtwo whole parameters you just only haveto add several pampers into thoseparameters and you can achieveeverything it's yeah it could beexcitingso can you tell me a little bit moreabout what kind of added layers you wereputting in the middle of these models isit just say like a mlp dense layer inbetween layers six and seven or is it asay normalization layer with where youhave the scale and shift parameters howexactly is the model surgery being doneas you add in these new weights for thefine tuningyeah so the layers we add is calledadapter layer and it we the architecturewe've tried several architectures solike the typical adapter is actuallyjust like a twouh fully connected layers and it formlet the two layers form a bottleneckstructure so the hidden dimensions soinputandfit to the first layer and then there'sa hidden dimension hidden representationand then the other layer will output thefinal representation and the hiddendimension the hidden representation issmaller than the input so it's abottleneck structure and this isactually a typicalstructure of adapterbut we also try several other variationslike compactor that canuse like low rank approximation todecompose thesum of the linear weights to likesmaller weights to save efficiencysomethingyeah to give a little more like detailof the adapter and who's not familiarwith like the vision languagepre-training so there's a recently uhfirst first of like a vision languagebird including like a billboardblacksmith blt5 uh dealer and so onwhere they combined the featuringlanguage model with some visionincorporation andand most of the previous models jointlybinding bothuh old entire language model and therecently uhthe adapter that are from nlp uh fieldand they only insert the specific beatboard network that is uh as i mentionedlike a bottleneck structure at eachtransformer blocks and we also followthatuh adapter layer and explore how itworks for digital equipment so i thinkthere's kind of two things in that iquickly want to unpack and uh the firstthing for yelin can you tell me so thiscompression bottleneck the idea of theadapter as well as the low rankfactorization is say the intermediatedimension of the representation is 1024you're gonna compress it into say 256and then blow it back into 1024 to putit back into the model uh did you maybelike ablate that detail is is thatdetail important for making this work ingeneralit it's not important but one veryimportant signal is that the bottleneckthe bottom time dimension isis better than smaller than the inputdimension it can be likeso like thisso for example if your input dimensionis 100 that usually the heat ofdimension you can either work with 30 5070 they will work they all work but ifyou put it as 100 or more than 100 itprobably won't work so yeah so anynumber smaller than the input dimensionwill work so yeah not super importantbut good to knowyeah i think that's an interestingdetail too is the compressionbottlenecks i found that that is like agood way to train neural networkswhether it's auto encoders or generalthat kind of thing and if we could talka little bit quickly about this strategyand transformers where you saymask out the cls token representation soit doesn't have the traditional kind ofcompression bottleneck thatconvolutional neural networks had butyou have that kind of indexing and thenmaybe you have a compression bottleneckin thatjust curious if you have any thoughts onthat kind of say isomorphic architectureas they call it and the kind of generaluse of compression bottlenecks in deepneural networks uhi actually do not have any too muchintuition on why this would like the thecompression bottleneck will work betterbecause if you see in the transformerblock there's a feedforward layers rightand they actually use the inverted likebottom block so the hidden dimension isbigger so i think that sometimes itneeds bigger but sometimes they needsmaller it's maybe like confused i'm notreally sure like why this would work notwhat has happened i think all i know islike empiricallyyeah adapter only works for like smallerdimensionyeah i think it kind of comes back tolike the categorical embeddings the wayit can like blow up the intermediaterepresentation space and say you havelike um the vector quantized variationalautoencoder the gans the way that theycan take these big latent spacescompress them all the way into discretebottlenecks and then blow back up thelatent space i generally just think thiskind of like the compression of it is sointeresting generally right and um soanother question i had for jamin withwhat you said is so there's two parts ofthe clip model there's the resnet imagehalf and then there's say the bardecoder half can you tell me a littlemore about uh that encoder decoder ofthe bar architecture as well as theresnet and like the particular detailsfor people looking to you know reallyget their hands dirty with clipokay sure yeah uh we use apart or any kind of like a sequelbecause language model in our experimentwe use a fourth host part and t5so we extend the encoder of the originallanguage model to multi-modal encoderactual disks are following thevlt5 of papersowe concatenate the text tokens andalso uh with the grid of uhclips visual features so welinearize all the visual features and wecan connect with the text embeddings andfeed all together to the part or t5select encoder so it um so the bartencodes on a concatenation ofwhat it's the original input of the textas well as the image representation andthen it as it decodes you slide thatauto aggressive mask right so it's likeiteratively decoding with the causalmask as they say that left to right kindof structureexactlyso in addition to uh adapter parameterswe also learn a very uh like light ithink of one fully connect layer betweenthe clip embedding to the wires the textembedding so itmaps the visual features to the likewe're defining space that bar tounderstand super interesting so anotherdetail of this study that i was reallyinterested in is the particular datasets that you're using to studyvision and language and maybe we couldyou could take me through the data setsquickly and then i could tell you theone that really stood out to me as beingparticularly interesting yeah soactually wefocus this on we focus on like visionlanguage text and also video languagetasks and mainly focus on like questionanswering and captioningso for like vision language i mean imagetext taskwe try vqa visual question answering andum gqa uh also question answering datadataset and nlbr which is a visualreasoning task and cocoa captions acaption task and for video languagetasks wewe try like two uh video questionanswering tests and two video captiontasksso the former two are for uh tbqa andhow to q8 and that's thelater are the tvc and yc2c like captiontasks super cool so quickly beforegetting into the data set i previouslysaid it was my favorite um could youtell me a little bit more about thevisual question answering is it um is itmostly common sense is it like countingtasks like how many apples are in thispicture or what color is the apple orwhat kind of questionsis it being tasked with there's allkinds of questions actually actually inthis so the accounting problems actuallyyeah they are some counting problem andthere are also some[Music]like yes no questions the most of thedata i guess no questions like is theperson's shirt the shirt on the the manisblue or something like this kind ofproblem and there are also some openquestions likeuh maybe askwhat this person doingwhat the man left doing something likethis kind of openuhanswerquestionyeah but most of that he has no questionand some other numbers like you said thecounting question andthe other outcome open ended questionsyeah to give a little more context aboutthe dataset creation we chose the fourtasks bqa gqa nlpr and the googlecaption to cover the diverse aspects ofdifferent visual language tests as youknow copy caption is naturallygenerative task and the visual reasoningvr is a like binary classification testlike a uh if the visual input and thelanguage corresponds or not and thebeginning is more like an entailmenttask corner like basically uh whereyou're given two or three images and thesentence will either entail uh beentailed by only one of the images orall the images so it's a multi-imageplus text entailment reasoning testthat's super interesting and it alsokind of facilitates the how you haveyour output space right because if youjust have uh zero one and intelligentnot entailment or neutral like threelabels with neutral also it you knowit's a lot easier than having the textgeneration kind of supervision rightwith how you kind ofget the signal back how well you did ondoing the task yeah i think i mean thatthat's the good part why we used uh andsorry to interrupt you jeremy so you cancontinue but i'll finish thisintermediate answer is that uhhonor so you brought up a good pointlike we basically the whole idea ofusing vlt5 which was jaimin's icml paperlast year uh was this whole motivationthat can webuild a sort of uhuniform model right that can handleyes no questions plus bounding box uh idselection versus captioninguh right versus entailment so can cancan there be a unified uh sort ofuh visual encoder plus textual encoderplus textual decoder kind of combinationthat can generateanswers ids answers captions for allthese tasks in the same model so that'swhy we've used clipbart and clipt5based on jimin's previous work as thefoundation of this adapter uhpaper right the cvpr one we're talkingabout but uh in the camera ready versionfor cvpr you'll see on archive two thatyelen and jimin added another modelwhich is clip willwhich is at iclr this yearandthat was to show that you can also getverygood parameter efficiency andclose to the full fine-tuningperformance even when usingstate-of-the-art discriminative visionand language models like clip willuhwhich maybe jam in uhafter finishing the data set thought youcan also mention the clip will partbriefly thanks we talked about nlprcaptioning and the other page which uhjust covered like the hqa consists oflike different types of questions yes nocounting special understanding and so onand uhyeah i think gqa came a little after thevqa which uh uh tries toaddress the competitionality of thevisual language uh tasks so i i guessthe questions areuh generated byrule-based compositionsyeah and that's my favorite data stepthingthe compositional generalization thingis has to be the most interesting wellit could it could be a pick for yourmost interesting thing going on rightnow uh so what do you think about thenlvr data set and the way that you canyou know really isolate this idea ofdiscrete atoms that define theenvironments and then test novelcompositions of the discrete atoms andthis kind of symbolic logic of how youput together the environments togenerate the vision language taski think conor is asking like if you havesome more thoughts on gqa and itscompositional challenges yessorry about thatno worriesas you mentioned like uh uh i believesuch a cooperational data dataset it'sreally hard to solve with thelike really recent trend of uh likeincreasing the model size and parametersbecause uh that compositional questionsofuh like recent like large modelsuh oh so sorry like a couple ofquestions are hard to tacklewhere there's a like data shortcut intheshortcut in the reasoning process so ifthere's a statistical like biases in thedataa model willuhaffect the questions based on theumlike for example uh if there's a fewquestions like how many drafts are inthe images and they're only the imageswell one job in the whole data set andthe model will not count the draft butit will directly memorize therelationship between the giraffe and theanswer one so it just uhyeah blindfoldedlyanswers zero so we might need afundamentally different uh like data setor bottle approach totackle those such like compositionalquestionsdo you think that's related to say it'sover fit to the background or thetexture of the you know the clothing orwhatever the thing is do you think youcan have say data augmentation that kindof disentangles the correlation from thecausation of what is supposed to becausing the prediction of you know birddeer truck and these kind of datasets doyou think that kind of like causalinference isthat is is a way to think aboutimproving that and do you think dataaugmentation is maybe you know apromising interface foradding that kind of causal bias toimages particularlyuh i yeah i think those are promisingdirection i'm not sure they're the onlydirections likeuhthat both both the call like inferenceuh causal modeling and the dataaccommodation have various likeuhthings are similar where they try tosmooth out the like thestatistical dishthe restriction holessobutif we try to tackle the real-world dataset there's much much more aspects andcombinations to cover like we need tocombine the different backgrounds anddifferent objects and different types ofquestions different kinds of answers andall these combinations can easily go toexponentialyeah alsoyeah so i thinkyeah we need tofigure out how tomake this data augmentationmore much more efficiently and itreminds me of another paper i read fromprofessor bonsol's lab is env edit it'sa way to randomize the domains forvision and language navigation and soit's kind of like this idea you've justdescribed the automatic domainrandomization it was used with openai'srubik's cube the way that they you knowthey had their physics simulator andthey also had visual conditions and sothey randomized all the environmentparameters and tried to cover the wholedistribution that kind of philosophythat you know we can use dataaugmentation or we can construct amassive data set such that we cover allthe distribution is do you think thatidea isyou know looking still promising or likesomething that is feasibleuhit might be an unpopular opinion but ibelieve the sleep like making the morelikeagreat uh simulator is a way to go forlearning moreuh a realistic data set of course itrequires a like really great deal like3d engineers but uh to deal with thereal world we cannot really collect allpossible images and and not only becauseof the data setcreation but it alsouh deals with copyrights and byand ethical issues so yeah i i stronglyuh want to likeuh there's a good game ai engine for aiyeah i think that's a fantasticdirection as well thelike the unity environments for forvision and language and and thecompositional questions of visionlanguage particularly because of how youcan uh control that kind of thing and soso generally on this topic of multimodallearning is um is this something video ican briefly mention also something aboutembedded since you brought it up so yeahso the idea there is more about uh sortof robustness and also making surethat the model is not overfittingor with some strange background noisenext to someone's microphone yeah sobasically uh the idea there is more thatyou have to generalize to unseen roomsuh right so can can the model sort of uhor the agent also do navigation inenvironments and objects and rooms andsurroundings that it's not seen beforeso the idea there from data augmentationor editing the environments is basicallyto make sure that the model is not overfitting to things that it's already seenright and it has sort of moreregularization from that perspective uhand hence creating different styles ofthe objects or changing the objectsthemselves uh and also the style of theenvironments right these things canmake sure like you said that the modelis notlatching on to certain sort of these uhshortcut sort of features right becausenow it cannot like it has to work in allthese different variants too so that'ssort of like a different anglethatthat kind ofwork would needbut in terms of learningcompositionality through dataaugmentation i sort of agree with jiminthatit's not that exciting in some sense tolearn all possible combinations rightand i mean in some sense it's a nicesort of balance between inductivelearning and deductive learning right uhlike is it like showing a lot ofexamples and learning the rule from thatversus being given a rule and generatingexamples from thatso so yeah so there's a lot of work incompositionality in pure nlptasks and papers toobut the nice example of this simulationengine that jaimin was referring to youcan see in his uh dally eval a paperthat i don't know if you've followed butwith all this recent sort ofuh discussion of openai's new dali modelwe had thisone of the first sort of text to imagegeneration evaluationuh works which basically useduh jimin's uhend of hissimulator based evaluation paint skilldata set that has very fine grainedcontrol on what kinds of spatialrelationships and counting andcompositionality challengesto create in a data set such that youcan actually test whether daly islearningthosespecific relationships or is it justuh sort of creatingan average ofeverything that it's seen in thetraining datalike can it understand uhthe chair left of the table versus thetableuh right versus the chair right of thetable and so on like all these differentcombinationsyeah like a blue sphere on top of a redcubeyeah he's crazyyeah i love that general thing of umlike generalization testing robustnesstesting is a particular kind of it adomain shift transfer and kind of likethis thing of studying these pre-trainedmodels like on the vva podcast here weare about building these vector searchengines where we put these pre-trainedmodels as embedding providers and thenbuild them into database indexes andthen provide database services so likewe love this topic of kind of studyingthese pre-trained models do you thinkgenerally that's kind of anhow do you think about that researchdirection where say for your new paperyou know you're not even going to traina model you're just going to takesomething off the shelf and then justdesign a suite of tests for ityeah i think that's where the thiscurrent sort of cvpr paper we arediscussing right here adapter makes uhsome good sense becauseas we start working or being presentedwith all these bigger and bigger modelsuh and like you said now you want to usethose models but for a differentdownstream task youdo not want to and also most of the timewill not be able to afford touh sort of retrain or fully fine tunethe wholehuge model uh on your new task so that'swhere i think this is the future rightwhere with just four percent one percentkind of parameter updates uh can youachievealmost the same performance as uh fullfine tuning uh and for more and morecomplex tasks that involve videos andlanguage and images and so onuh so to me that's sort of theexciting connection uh between thispaper anduh what you saidand then from the evaluation perspectiveyeah like as we were discussing uh dailyeval i mean that's sort of like aslightly different angle where insteadof makinguh these models sort of just bigger andbigger uh and also being excited abouthow they look visually uh this would bea more quantitativeway to actually measure whether thingsgot better at certain reasoning skillsof spatial relations or counting orcompositionality or biasesuh right so so it makes things morequantitative and alsobe able to compare to previousmodels and iterationsand i mean eventually it would be evenmore exciting and fruitful to put theseevaluationsinto the modeluh training optimization process rightso thatwhile improving the model whileoptimizing the model you aretrying to improve these metrics right aspart of the optimization directly butobviously that will become extremelycomputationally heavy itselfuh which is again where things likeparameter efficient methodsuh come into pictureokay that might be like one way to sortofuh combine all of these things and andclose the loop yeah that's a super uhinteresting picture you paint like verycomplete with the idea of having youhave the sparse fine-tuning it'scost-effective and then you have allthese generalization tests that you cando during training and maybe open sourcetools like waste and biases might besomething that pops into people's mindsis something that's creating these likemassive log reports you've seen a lot ofpapers and open source code like maybechecklist and nlp but yeah this idea ofhaving all sorts of logging andgeneralization testing and exploring allkinds of different kinds of behavior sokind of wrapping up the podcast ourgeneral goal with this one was to reallyhit the points and try to get the youknow all the technical information asfast as possible kind of maybe wrappingup uh maybe want to talk about likefuture plans or sort of that generaltopic of the code and what you plan ondue to doing to take this kind ofdiscovery further yeah my brief versionand then uhwillian and jamin add more uh like isaid i think sort of my previous answerconnects to this too uh that hopefullybeing able touh scale uh this parameter efficiencyuh to newer bigger models but alsoespecially in scenarios where you needuh maybe these kinds of evaluations assort of part of the model in the loopum so so so that would be an interestinguh future directionuh and also i think elin willprobably mention about some futuredirections of vl adapter because thecbpr version uh focused more onmulti-modal vision and language modelsbut the adapters were still moreeffective on the language sideso there's still a lot of work left tobe done in the communityon how to make the adapters also moreeffective on the visual encoder sideso that was that's definitely oneother uh future workwell super cool that's a superinteresting uh direction to take itfurther and i had so much fun learningmore about the vl adapter i think thisis such a huge discovery and i generallyi think just sort of the costimplications of this are absolutelymassive that you can you know only needto fine-tune four percent of the fullweights of clip to say down fine-tune itinto your downstream task for whetheryou're working in e-commerce or you havesome kind of creative idea for whateverit is with vision and language data setsso thank you all so much for coming onthe vva podcast i really enjoyed uhhearing the story of this paper and allsorts of details about thisthanks for having usyeah thank you[Music]", "type": "Video", "name": "UNC research team on VL Adapter for Efficient CLIP Transfer - Weaviate podcast #14", "path": "", "link": "https://www.youtube.com/watch?v=BNPxg5a3NaI", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}