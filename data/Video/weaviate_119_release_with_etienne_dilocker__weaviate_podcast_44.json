{"text": "Hey everyone! Thank you so much for watching the Weaviate 1.19 release podcast! We have all sorts of cool new features, ... \nhey everyone thank you so much for watching another wevia release podcast releasing we've made 1.19 we have all sorts of exciting new features but before diving into the into any of the features edian has on what I think is the best iteration of the Wii V8 shirt uh so Eddie thank you so much for joining the 1.19 podcast thanks for having me I'm calling this the 1.19 shirt I think it's not the the 19th iteration yet but it fits perfectly with all the cool stuff that we have in the new release yeah awesome I remember we joked about having like a t-shirt for all the blog posts the blog cards as philana makes and yeah yeah I really like the book of it uh so super cool so let's kick it off with the grpc API and the story around what led to that yeah uh grpc API was sort of an initially unplanned feature but we wanted to add a VBA to a n benchmarks and in a n benchmarks we've we've talked in in the past we've talked about the sort of Library versus database split and in a n benchmarks you have all these libraries and um but now also a couple of databases so we thought like okay yeah we have nothing to handle it let's let's get VIVA in there as well and uh what you can see in those those uh benchmarks is that especially for their so so for those of you who don't have the context in The Benchmark essentially you have a graph that on one axis shows you the the recall and then the other latency and with any a n algorithm especially with hsw but with any it's always a trade-off between recall and latency or throughput as sort of a as the inverse latency basically so throughput in the A and benchmarks is actually estimated because it's not really a throughput benchmark it really just a single threaded individual queries and then if a query takes let's say a hundred milliseconds then the throughput would be considered 10 per second so just one second divided by by the individual latency the the actual ladies these are way lower than 100 milliseconds but that's that's kind of the The Benchmark setup and what you can see in those sort of low recall settings of course the latencies go go down quite a bit but that also means that everything that you have as constant overhead so let's say a vector surge takes for example half a millisecond but you have a constant overhead of half a millisecond and all of a sudden in these low recall situations you now have 50 overhead or depending on how you see it 100 overhead and that's not great in benchmarks so um we thought well what can we do here and and can we do something that doesn't just optimize for benchmarks because that's kind of well I don't want to say it's pointless because in the end benchmarks are a great signal for users and they help them sort of decide but also to some degree if you over optimize for benchmarks you always have to ask yourself like what benefit does the actual end user have and we looked at the grpc API in um adding grpc API sort of in the background for some of the clients so what is super important for us was to make sure that there are no breaking changes or that users would have to sort of start changing their their usage patterns or would have to learn something new no we've sort of slowly started adding this this grpc API which some endpoints not all yet but some can run with this and the python client for example can automatically discover if grpc is enabled it's also an optional package on the on the client side so if you don't have trpc installed then you don't have to install it if you don't like it but if it's there it's it's used in the background and that allows you to to yeah basically have more efficient queries where just the the overhead and of course that overhead before I think it was was around 500 to 600 microseconds so it's a bit more than half a millisecond that was there in every request but the the shorter the request duration is the more noticeable is the overhead You could argue that a user would never notice in real life and that you only only argue it in that you would only uh notice it in benchmarks but nevertheless overhead is there and overhead is gone so making something more efficient is never a bad idea I think even if if you could argue that that most users might not see the the real life impact yeah um so definitely want to dive a little over a couple more questions about how these a n benchmarks and the recall latency but um maybe just super quickly so grpc graphql HTTP uh could you just quickly give me the difference yeah uh so so interestingly grpc actually isn't a separate protocol it runs over HTTP 2. so from a sort of these are all sort of one layer higher than the TCP stack they all run over TCP and then grpc basically runs uh via HTTP uh so the the the way and this is a very interesting because I think some people uh just assume that grpc is faster because it's a faster protocol but it's actually not a faster protocol because it's the same protocol as HTTP and by the way graphql this is also typically sent over over HTTP um it's simply that grpc uses the protobuf protocol which is very very close to how uh the the data types are represented in their respective languages so uh the the go memory model is very similar basically to to the protobuf structure and then also the memory model in Python so if you have this sort of network request goal which is vv8 wb8 server uh via a grpc to python they're simply way less sort of decoding encoding restructuring and not restructuring but so re-encoding that you need to do whereas uh both graphql and and uh rest that we typically use for for HTTP um use Json and Json basically is a is a sort of space inefficient protocol because it's just everything text and it needs to be parsed it needs to be sort of the same way that that if if you as a human right Json you have the curly braces and you have the quotes and then you have the colon then you know okay this is the key value mapping in the same way the server and the client for that matter have to parse that and that that simply costs time and it's it's not a massive overhead but in a benchmark scenario where you don't have a lot of time like even that can can um show up what was important for us was to to like not not add new overhead for the user like cognitive overhead we're reducing computational overhead but don't want to add a usage overhead um so you probably won't notice you will still be able to use graphql especially for for sort of just learning the vb8 API just discovering it playing around with it the graphql console this is an amazing tool but at the same time if you run your use case in production and you run it through the language interface that is or clients anyway you might not need graphql anymore so it's kind of a more more options yeah super cool so stepping into the A and then benchmarks a little bit I remember you'd published that amazing a m benchmarks collection and we also had a podcast on that I think it's like number seven earlier podcast and I think we're on like 47 47 or something but um wow I'm sorry I remember the um so like the hyper parameters of you know the effect of ef ef construction Max connections um so are all those fixed and then each of these you know a n providers all have to adhere to the same hyper parameters and the same machine as that sort of a like a fixed hyper parameter of the hsw kind of uh no actually so the machine is fixed yes and not even just the machine there there's actually a so I I said that the in benchmarks was single threaded before it's actually not entirely true they're limited to one CPU which basically is not the same as single threaded you can run multiple threats with that CPU but there's no benefit because you can't ever exceed more than than one CPU so that is one one restriction uh one restriction is the build time so I think if it times out after I think I think two hours is the limit or so so you can't run anything that takes more than than two hours but other than that it's completely free so you can set your own parameters I think all the hnsw based implementations do have the same parameter set so it's typically it's basically a grid search over over all parameters so you have if constructions from very low to very high you have EF from very low to very high and same for for Max connections and and since the hnsw algorithm sort of works the same I guess in a sense you're just comparing the the implementations and that also means that the the parameters typically perform the same so so maybe in in absolute terms like one solution would be slower than another but still for both Solutions like uh setting eight connections would probably have the same effect on both Solutions as opposed to setting like 64 connections um and then in a and benchmarks it's not limited to hsw based algorithms so for for those so so for example a PG Vector is in there the new uh postgres plugin which I think is IVF based or something or iufpq base so it has completely different parameters but that's the nice thing about a n algorithms you can compare them them still and and sort of yeah you're kind of kind of free to use all your parameters to get the best out of all parameters under the Restriction that you still need to be able to build it within two hours on a single CPU so so what scale is the test is it like the sift uh the sift one million vectors or like uh yeah we have sift one one m in there I think we have I forget the name but I think it's a deep one B which is not a billion that that would be that would be a bit large but I think it's 10 million or so so they're they're definitely smaller and larger but I think they're all within within yeah into one one to ten million do you think maybe that's something that the benchmarks uh like should aspire to is or you know just like the continued scale anyway yeah so the whole topic is so interesting um so so maybe pivoting topics a bit we also have in our uh release the uh degenerative cohere module and so I'm really excited to talk about this because this is actually the first module I've ever worked on at webiate and I mean it was pretty easy to the the the infrastructure that's been in place to uh extend these modules with new models or new external model providers so you know open AI Co here when the anthropic Cloud Model comes in the hug and face Transformers gbt for all the llamas like all these large language model providers can be integrated so easily into eviate and so yes I wanted to maybe ask you anything about um your latest thinking on the large language model stuff and that kind of um the the trends and all these new models that are coming out yeah cool so first of all I'm really happy to hear that this was easy to do because this is this is kind of the point of the module system like there are so many sort of complex tasks within the database both on the the victory mixing side but also on the just sort of old school database indexing site so so having that sort of almost sandbox scope of the module where it's very easy to integrate something new that's that's that's always awesome and and um we're always awesome to hear that it actually works like this as as it is intended um which serves a nice nice segue into everything that's happening right now because um we're really in this this sort of crazy mode of something new popping up and and breaking GitHub Star Records within a couple of days or something it has more than everything out there and I love that we're able to to integrate them I I was about to say react but I think react is not the right reward I think we're able to integrate them very quickly as they pop up because it's not react would imply that we somehow like have to chase them but it's not the case it's just enabling users they did There's Something New comes up a new be it a new model a new integration something that provides value in in this AI space and if it provides value then there's a good chance that it can also provide value to deviate users and I think being able to to quickly adapt there and quickly integrate those is is super valuable for our users and similarly um having vv8 basically as the the stable API for them I think is also super cool because if you if you manually let's say you're using generative open AI so the completion endpoints from openai and now you want to maybe test that against the cohere endpoint and then yeah I just want to see like okay what what performs better for my specific workload if you were to integrate with those two providers separately you would probably have to adjust some code because you wrote something that was meant to contact the the open AI API and now the the cohere API I haven't looked at them but I'd be surprised if they were identical it probably is some difference but if you use vv8 with all of that the VBA API is the same all you have to change is the model or change some string somewhere in the model that's essentially just configuration and that goes for for way more of these things like if you this is just one example you could say you could argue like yeah okay changing changing one of those requests is easy but now if you combine that maybe with with the generative search which typically is the the retrieval step and then the the generative part what if you want to change something on the on the retrieval side you could have text back open AI text to back cohere text effect Transformers contextenary uh uh text hugging phase for any hugging phase model probably new ones that that have been released during during the recording of This podcast so um you can combine all of these all of these five or however many it was you can combine bind them in a hybrid search with bm25 so so now we have this like large Matrix of different options and you can do this within rebate by simply changing configurations I think that's that's a nice addition additional value of having that collection of of different model providers within mediate yeah I think that um like they're kind of like model inference orchestration of leviate is so fascinating to me like whether you want to manage this on the database side or on the client side and I think by doing it in the database you just have kind of one thing with all this infrastructure and and yeah the whole thing is so exciting the head of the one paper I really liked that came out recently is hugging GPT where it's kind of about the you know the large language model is like the task decomposer that routes it to the models with descriptions of the models and I mean the the future of like we originally talked about this pipe API which is kind of like a dag of computation flow like retrieve retrieve rank read but now we're seeing such a dynamic uh kind of chaining of the things you can do with search interfaces and it's so interesting so the next topic is another innovation in search that I think is extremely exciting so much potential with this idea which is Group by arbitrary property and yeah can you explain how maybe with the document passage example I think would be the best like yeah yeah this this feature this was basically a a request from one of our commercial users but when they requested it we we kind of almost expect this already because it's just something that that is an Ever sort of growing problem that we now have a solution for so the idea is if you have your documents split into some kind of sorry not to document your your entire content split into some kind of hierarchy um typically you're you're either forced to give up the hierarchy or you have to do some kind of work around so so what I mean is for example let's say you have documents and you have passages and a document is sort of a grouping of a number of passages if this is a very small grouping you could for example do the ref tvec so you could have them in this as two bv-8 classes you can do the ref Tech and then reftube gets the mean embedding of those passages the problem with this is as the number of passages per document grows it gets harder and harder to represent so much information because essentially all gets combined into a single vector and there's only so much information that a single Vector can can hold so if you try to combine a thousand passengers and and these all have maybe diverse topics so some that says maybe not document passages maybe book and chapter or book and passage then it's very very hard to keep that meaning so what users typically do is they would want to search uh buy the the sort of smallest unit which in this case would be the the passage but then the problem is well how do I get back to my association off of the document like now I'm searching for passages uh maybe the top 10 passages are all of the same document maybe in the top 10 passages it's 10 different documents and what our users were missing is basically a simple way to to sort of keep that kind of Association and and make make that part of the ranking basically and with the group by feature um and it's and a group by by any arbitrary property I think it's it's most interesting for this particular example to group by the the reference prop that that would be the the association from passage to document it now allows you to have basically the first hit comes in and let's say it's off document one um then you can you can group all the passages that match that same document in one group and then you can either sort of group this by what is the the highest similarity per group or you could take the mean similarity per per group or something there's like more more options but basically if you want to display like if you want to search through passages but to your users want to display um uh documents you can do this now because by finding the the the one passage that fits the best you still get the the latest passages basically the the most related passages in that same document that you can represent to your users so it gives you like a nice way to to display that as like almost like a like a tree kind of way um or various other ways I think gives you more options to to show that to your users yeah that's amazing I mean um firstly quick shout out I was just at the um the haystack search and relevancy con a conference that I open source connections I know they would love this topic um yeah so like with the ref devec yeah you try to aggregate all the passages and averaging is maybe too much of a compression I used to think that maybe a graph neural network could aggregate those vectors and or maybe cluster some ideas like this it also kind of reminds me of like multi-vector representation where you you know you have a document and then passage that kind of thing maybe like title passage and so on author but yeah that's um it like I I like the analogy of the podcast like taking these podcasts and then um you know putting the transcriptions in and you know you have all the chunks and you have the similarity the chunks that you aggregate it's such an interesting innovation in Search and I like to call this like top level indexes and yeah I'm so excited to see how this uh evolves so so now transitioning into uh some new data oh yeah unless you want to stay on that no no just as I I love your example about the podcast and I just have to think because we opened with the topic of benchmarks and we also have a dedicated episode on on benchmarks I think that would be a great example because now it gives you the ability like do you want maybe that snippet from today's podcast which is very relevant to to benchmarks but the podcast is not about benchmarks or do you want more of the mean similarity where you have dedicated episode about benchmarks where maybe no snippet fits as good as today's snippet but overall you have the the podcast that fits the topic better and I think this is something you can very nicely represent with with uh the new grouping feature yeah amazing that's a perfect example yeah okay is the one passage Masters it enormously but then the whole collection yeah awesome stuff so um so okay so coming into the um the database stuff and I think this is so interesting how um so now you have a tunable option between whether you want to use the bitmap index which I assume to be like a super fast inverted index because of how the underlying data structure is implemented and then you also have the um the bm25 score index uh so so what goes into kind of this tunable indexing yeah so in the past we kind of made an assumption that if it's text you would always want to do a full text search and that that just I mean that may be true in some cases but it wasn't it wasn't granular enough because exactly as you say we've introduced in in the previous release we've introduced those bitmap filters and everything so so we had it for everything except text props for for this particular reason and um everything else is super fast now but text still kind of like was sort of still the old implementation because text was built in a way to be indexed for for bm25 which is or without going into into too much uh detail but it's very different because you need a lot more information so for bm25 every hit and every Association needs to be scored so you need to know uh the the frequencies and all these kind of additional additional things that you don't need to know for for Pure filtering but now if your use case is to Simply filter and nothing else on text on something that happened happens to be text um yeah right now you get options you can either index it only for filters then it's fast it's the bm25 index you can only index it for search then you can have then it's basically the same behavior that you have before so filters if it's not specifically filter indexed it can still fall back to um to the the old index to the searchable index but if it has both then it just takes the the best at each time then it's fully in your control um it's fully Backward Compatible so by by default we build both indexes but you can take the turn them off and then simply tune in so if you said Okay I I know I'm only going to need so so for example if the field is like a a user group that you need for permissions checking or something you're never going to run a bm25 search over that you know it'll only be there for for filtering so just turn the searchable index off keep the filterable index and then you get fast filters using bitmaps under the hood for string properties as well yeah it also makes me think about like you could probably represent categorical uh variables that way like if you have yeah yeah typical um so kind of yeah just just because it's a string doesn't necessarily mean it's it's full text that was kind of the assumption that we made before but it simply doesn't hold true yeah super cool and and you can use those with the wear filters yeah amazing stuff um so then there's um so now there's more tokenization options as well um yes you may be taking me through the thinking around the tokenization yeah yeah so um mention string and text before and I think this has been a big source of confusion for for some users because some users just assume that because it's string it would use a specific kind of tokenization some users didn't even know that there was a split between string and text so they used string for something that should have been text or use text for something that should have been string and as I'm saying this well what should have been a string what should have been a text like that's just not clear that's that's very hard to explain and that's why we've we've simplified this and again all in a non-breaking way so right now there's only going to be text string basically disappears nothing breaks for you you can still in your apis or in your your schemas you can still set string and it'll be automatically converted to to text and to keep that kind of difference that we have before there are now more tokenization options so you can tokenize if you want buy the whole field which is something by the way that a lot of people assume that that string would do but it would only do that if you set the tokenization to field so right now that's simply an option you can tokenize by word which basically means vv8 will split on the spaces but we'll keep the individual things you can can or or will sort of keep the alphanumeric contents of the the words that will remove special characters you can also split just by white space which is sort of again splitting at the word boundary but not removing those special characters so for example if it's like a a product description that has a special sign in it or something and you want to you want to keep that because it's relevant then you can so again more control and simpler API at the same time so keeping on the um the apis have been um so I've started diving into typescript and learning a little bit more about that I've seen more of the consistency level and um so the newest thing tunable consistency for Vector search and the get requests and all that um yeah the whole so replication is something that you can so I hope I'm getting this correctly and Eddie and will correct me in a second if I'm wrong but replication is like how you're replicating your data across nodes in a cluster so so how does this work with each tune each query you can tune this yeah so the the there's two sides basically for for tuning one is how do you write your data and how do you reach your data and they they somehow depend on one another because for example if you make sure that during writing every note already has the same copy it doesn't matter what you do add reading because if you just read it from one note or if you read it from from multiple nodes you already made sure that they're in sync during during writing however that has a large cost because now for every ride all the notes have to basically agree on it all the notes have to be alive so so um with tunable consistency the idea is to to make some trade-offs basically you could say I'm only uh writing with me with a majority of notes and I'm also only reading with the majority of notes and then you know okay we still got some consistency guarantees basically because um because if if both writes and reads happened with with a majority of notes then uh it should still be consistent or you could say it's actually not super critical that my data is always up to date maybe you want to accept that it's sometimes a bit bit um yeah eventually consistent so it could temporarily be out of date and hasn't been repaired yet and that may be fine because maybe something that you're updating is is yeah it's not a bank account details or something or or Bank transactions but rather product search where it's not a big issue if something is slightly outdated or maybe you can catch it in the application side if it's outdated and um yeah for for a graphql searches basically that was the the last point that we had where we didn't have tunable consistency yet and for those three queries you can now set it there as well amazing uh so everyone we are a little time constrained so we're gonna we're cutting it a little short but um as always the incredible devrel team and there's this release blog post with all these releases if you want to consult more details Eddie and I are available in slack and more than happy to answer any of your questions so grpc generative cohere Group by arbitrary property bitmap indexing more fine-grained control over that with the tech surge deprecating string with more tokenization than the tunable consistency and some patch releases So yeah thank you so much everyone ", "type": "Video", "name": "weaviate_119_release_with_etienne_dilocker__weaviate_podcast_44", "path": "", "link": "https://www.youtube.com/watch?v=Du6IphCcCec", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}