{"text": "Hey everyone, thank you so much for watching another episode of the Weaviate podcast! This episode features Matthijs Douze, ... \nhey everyone thank you so much forchecking out another episode of thewevia podcast this is a super excitingepisode of BBA podcast we have aresearch scientist for meta AI Matisseduzy uh his research publication list isjust amazing the amount of work he'sdone in the space of vector analysisproduct quantization all sorts of thingsof deep learning really is really trulyone of the most impressive scientistsI've had the opportunity to talk to soMatisse thank you so much for joiningthe podcastyeah thank you very much for hosting meawesome so can we kick this off bytelling us about the history of theresearch uh like how did you come to beworking on these things what firstinspired your interestsyeah soum yeah I can say a little bit about mybackground soum I have a PhD in computer visionbasically so that's my background uh Idid it in in France and uh then I movedtoumto a research uh institution the whichis inria and where I met harveyjigu whohe has an expertise more in anythingrelated to coding encoding decoding thekind of things that's used for GSMnetworks and uh and so he was the maininspiration to move towards thinkingabout how to better compress uhcompressed compressed vectors becausebasically there's one very clearapplication for this in computer visionat the time that was very importantwhich is that images were analyzed byextracting sift features from them andsift features I justum 128 dimensional vectors that are anew extractin the order of hundreds to a thousandsper image and those those were earlyembeddings so that means that it's arepresentation of a small part of theimage and then you could do a couple ofthings with those you could use them forimage classification but the the partthat we're most interested in was imageindexing which means uh which meant atthe time and still means now findingsimilar images for example images thatrepresent the same object or the samebuildingand to do that you had toum you had to find which where thenearest embeddings the nearest siftvectors from the ones that are in theother imageand so there was this um at the timethere was this very interesting workum that came from uh from the Andrewzisman's lab and uh uh and which whichwas basically led by Joseph Savage anduh which basically uh reduced this Bunchthis big number of CIS vectors so youhad like like a thousand sift vectorsfor one image which was pretty heavy andto reduce that into a bag of words andback of words just means that you reduceeach Vector into a single identifier asingle a single number that wasum assigned by nearest neighbor uhnearest neighbor search so searching thenearest centrates to each of the vectorsand just keeping that only informationabout that vectorand so that means that if you hadif you had a like say 1000 centroids youwould have the only information that youkeep from all those heavy sift vectorsin the on the image is a kind ofhistogram over the Thousand centuriesabout how many sips vectors were fallinginto those sift actionsso and it was calledum a bag of bag of visual words becausein a sense by analogy with text youwould reduce the the the you know thecontinuous space of the the set vectorsinto a single a single token a single asingle word uh and that represented theimage and then the images could becompared and the basically what's whathappened there is that this representsthis back of word representation thatwas used for many many applicationsincluding classification but what italso enabled was uh to be able toum to do large-scale indexing so thatmeans since it's so compact you couldsay okay I have uh I have uh tenthousand a hundred thousands a million Ithink before us the the largestapplication that was at scale 1 millionuh one million uh images and you couldindex those and that was that was quitenovel at the time that it was possibleto really find in in real time so therewas a workum that was coming from uh Nissa andstevenius where they basically had acollection of CD covers and they wereshowing it to a webcam and the webcamwould find in real time uh what werethose covers what those covers werecorresponding to and this was reallyenabled by this uh this bag of fartrepresentationso that was so the back and forth'srepresentation was used to uh quiteextensively at the time and uh it hasbeen expanded and so on but uh what Iwhat the the second or the TheInnovation that came from this work byuh by cevich and this woman was the factthat they used an inverted index so thatmeans uh it's inverted because you startfrom uh fromum from from those those histograms ofof visual words and instead of stackingthose histograms or visual words youinvert the index and you for each visualword index you record which imagesand contain that particle particularvisual wordand uh so that made it much fasterbecausewhat it enables is that if you increasethe number or the the size of the visualvocabulary so instead of having athousand you can say I have ten thousandhundred thousands Etcuh you can you you get a very sparse uhsparse histograms obviously and thatmeans that the inverted index when youactually want to find which visual wordsare in common between the query imageand a database image you need to visitfewer a very small fraction of that ofthat data setso that wasum so I think that that was a bit thebackground when we arrived so the thekey elements that we had there was uhthe the fact that we hadum we had this representation of uh Highdimensional vectors with a singleinteger which is exactly quantizationand so that's where you know theexpertise of harveya started to be veryuseful and we have this inverted filestructure that was the start of a lot offruitfulum indexing methods that were developedlater including the ones that wedevelopedso then we arrivedand uh so we we recognized this uh thequality of this uh of this inverted listand the potential the potential that ithad to do large scale indexing and umand so what we added to that the firstthe first thing we added is actually uhthinking that within those invertedlists so the inverted lists are in thein the in the initialum uh backup visual word representationscontained only the document IDs or thethe image IDs that contain thosespecific visual words and so the ideathat we came with was to add a kind ofpayload for each of those uh instanceswhere we kind of refine a little bit therepresentation of the of the the vectorsbecause uh becauserepresenting a 128 dimensional vector bya single integer is a very crudequantization and so what we did at thetime is that we we used a prettystandardsystem that existed at the time and itwas uh it was to do a kind ofbinarization and that we we calledHamming embedding and so havingembedding basically consists in takingthis 128 dimensional Vector applying arandom rotation to it and then keepingthe sign of each of the components afterrotationand so then we get a binary Vector whichat the time we didn't use the 428Dimensions but we used just 64. and thenwe had a binary representation of 64bits and we could do that both on thequery side and on the database side andthen comparing binary vectors it's veryvery efficient to to compute the tocompare those with Hamming distancesand so then then we hadumuh we had a image indexing system thatwas very efficient and that was based onuh on what currently we would call anIVF an inverted file and having abattery Banning soum a binary representation of the of thethe vectors that are stored with in thatinverted file so that was the firststageand we got we it was very successful interms of of large-scale image indexing Iremember redoing the demo with thewebcam with uh with a laptop and uh anda webcam and uh and I'd scale 10 millionimages and an external hard drivewhich uh so for the technical detailswhich which didn't have a partition atall on it and so there was no filesystem because we need to access veryquickly we need to access the images todisplay the results so we were accessingdirectly the offset on the disk uh ofthe the actual imagesand uh and this was this was workingpretty well and um and so that was thefirst iteration of uh of uh of uh ofthis combination ofum of the the IDF which where theobjective is really to to to veryquickly prune the data set though theparts of the data set where you need tosearch and an encoding method uh whoseobjective is to get an approximatelyreasonable or as good as possibleapproximation of the of theum of theof the vectorsso it turns out that so at the timethere was the the there was a lot ofwork around binary representations forfor vectors so uh there was a wholeliterature around localized localitysensitive hashing whichis not exactly uh the same as a binaryrepresentation but very often a binaryrepresentation is is based on thelocality sensitive hashing Theory and umand so that there were interestingtheoretical propertiesuh and uh so there was a spectralhashing uh which was a work by taralbaand uh so there was a literature aroundthat and uh but what we so there theexpertise of Harvey that he had uh forfor anything related to quantization uhcame in very useful because he knew andit became clear afterwards that actuallyuh binarization is a very crude way ofof doing quantization so basically it isquantization because you you transform acontinuous signal into into an integerbasically but there are much better waysof uh of or much less lossy ways ofencoding vectors than doing uh thandoing binarizationso that maybe we can go a little bitinto the theory of um of quantization souh the parts that I know if I'm I can'tsay I'm much of an expert but I got Iget got some experience and some verybasic principlesso the the firstum the first two principles are theLloyd principles so uh for quantizationand uh basically since you since you mapa continuous signal to our continuedcontinuous Vector continuous Highdimensional Vector toum to uh to one single uh representationso it's to one single integer then andyou always have a reconstruction of thatof of the approximation of the of the ofthe vectorand so uh so in order for thisapproximation to be uh to be optimalthere are uh there are two necessary andnot not sufficient but necessaryconditionsand the first one is that each they arevery natural the first one is that whenyou have a vector and when you look atthe whole set of possiblereconstructions that you can make withyour quantizer so what that we callcenturies in general uh then uh the uhthe vector should be assigned to thenearest centroid so it's if you pick onea centroid that is not the nearest thenby definition you are doing somethingsub-optimal so you always should alwaysassign to the nearest vectorand the second one uh so so this isquite natural the second one is specificto the to the to the L2 distance to theeuclidean distance if you minimize thethe if you minimize the the the squareduh error of the Reconstruction uh theneach centroid should be the center ofmass of all the vectors that areassigned to itin the distributionand uh so those those are two principlesand the actually the very nice thing ofof this of those two principles is thatit translates to the k-means which isalso called The Noise Lloyd's algorithmto uh to do uh to do clustering and aswell as to do quantization because thecanines is an uh basically what you dois you you take a training set that yousuppose is representative of the of thedata distribution and then you you youalternate between two steps the firstone is to estimate or let's start withthe assignment so you start with a setof of initial centroids that aredetermined with some heuristic orrandomly and then you you assign to eachuh you assign each training Vector tothe nearest centrate so that you you youbasically you do the assignment step andthe second step is you update thecentroids by by Computing the center ofmass of all the points that wereassigned to that that centroidso k-means is is one of the hugesuccesses of of uh of quantization andof many machine learning algorithms it'svery simpleand umand actually it gives you uh it givesyou a quantizer if you have arepresentative trading points it givesyou a quantizer that is that has thatfollows those two properties of the theoptimality of the of the two lightsconditionsand um so what's so this is very good sowhat's the problem with k-means theproblem is that say that you you havebudget toum to represent a vector with 64 bits souh what happens is that you cannotreally say okay I'm going to do ak-means where the the indices are goingto be encoded into 64 bits because 64 2to the 64. it's really a lot ofcentroids it's actually more than muchof uh of the high numbers that we findin modern computer science so so it'sjust not possible it's not possible todo this to use this amazing cayman'salgorithm at that scale it is possibleand that's what we do to use it for IVFfor cost quantization so for I forinverted files uh we have the the degreeof freedom to choose the number ofcenturies that we want to use and ingeneral it is beneficial to use a largenumber of centroids but we're not goingto to have 2 to the 24 the 2 to the 64thcenturies we're going to have like uhbetween uh uh a thousand ten thousandhundred thousand one million this thisorder of magnitude number of centroidsso for this we can use k-means directlyand we definitely do that uh to to to dothis first step of what we call coursequantization so the the inverted filethat's going to to allow us to to searchonly a really small hopefully a smallsubset of the data set and still notlose too much accuracy in this processbut when we when we when we're talkingabout the payload so the the the partsthat you going to use to approximate thevector and that you store in theinverted lists uh then you you cannotyou I mean if you have a payload ofeight bits it's fine but in general youhave a larger payload and the reason isbecause it doesn't make much sense tohave eight bits because the even theimage identifier is going to be longerthan thatand that's also start in the inverterlists and so if we want to go to 64 bitsthen uh so the the the fundamental ideaof the product quantization whichalready existed but was never used foruh similarity search per se the ideathere is to is to say okay we cannotreally afford to do a 64 bits by uh byhaving a vocabulary that's that spansthat has one explicit centroid storedfor every uh for every uh set rate ofthe two to to the 64. so what we'regoing to do we're going to do atrade-off we're going to just chunk orto we're going to take the vector theinput vector and split it into subvectors and then apply this quantizationthis K means quantization or thisexhaustive conversation apply it only onthe sub vectorsand this isum it's very it's it's it solves ourproblem uh of scale because let's say ifwe need 64 bits uh let's say that we cansplit those in eight times eight bitsand eight sub vectors that each areencoded in eight bits and then doing thek-means for to get 8-bit vectors it'sjust okay means with 256 centroids andthis is I mean this is you could almostdo it by hand and umand then uh to and then you can encodeeach of those Circ vectors into separateuh into a separate representation andthen just concatenate thoserepresentations and uh and you get anapproximation of the the vectorso uh so then we are down to uh tohavingumto having uh encoding costs that's veryreasonable because at encoding time whatyou need to do is is exhaustively findthe nearest centroid for each of the subvectors but since you need to searchonly 256 hundreds it's uh it's prettyefficient and uh and uh uh and and thestorage the other problem was thestorage of the centroids this and sinceyou need to store only small centroidsin in for each of the uh for each of thesub vectors it's it's pretty efficientactually uh the starting the centroidsis as large as storing 256 vectorsthemselves because if you add the sizesof the sub vectors you end up with theinitial size of the data vectorsso that was that was uh very interestingin terms of storage and in terms ofassignments and so decoding is just alookup and you separately look up eachof the vectors and one very interestingproperty that we already that we alsohad and that's uh that was kind of luckythat it turns out this way is that it'salso possible to do compressed domaindistance computationsso compressed demand distancecomputations means that if you have aquery Vectorum so in generalI wouldn't say in general but very oftenyou want to compress the databasebecause it's size constrained but thequery vectors come as a float so youdon't really need to compress them andsoum and so what you can do there is to sothe the basic algorithm that you woulddo with uh with an igf and the PQ orwith the PQ payload to compute thosedistances is to take the query vectorsthat are not compressed decompress thethe database vectors or you scan theinverted list you decompress each of thevectors and then you compute thedistance just by Computing L2 distanceand and so that's finebut it turns out that it does somethingeven more efficient that you can do isnot decompressing the the the databasevectors at all but when you know at thepoint when you know the query Vector youcan you can say okay uh for each of thesub vectors of those query vectors Ihave only 256 possible Dimensionsbecause there are 256 hundreds and so Icome I can just instead of computing itevery time you can just say okay I onlyhave 256 possibilities so I just need tocompute those this and all those 256distances and what I'm scanning I'm justyou know looking up those distances andand compute what's the what's the andand then I don't need to computeactually compute the distance and thisis possible so it's possible to do thatwith uh with any quantizer you cancompute the businesses with all thecentroids and since it's a finite numberof some trades you you always know thatit you you will have pre-computed thedistances but what makes this possiblewith the product quantizer is or whatwhat's convenient with the productquantizer is that uh the distance isdecompose across Dimensions with L2distance or with L1 distance or withmany distances actually you you can uhwhen you you can chunk the the vectorsinto parts and uh summing up thedistances in the sub in the subparts toget the total distance between betweenthe vectors and so that's or I mean it'snot true for L2 distance it's Squareit's too true for squared L2 distanceand so we always compute squared all twodistance and soand so in those sub vectors then uh wewereum we uh we're adding together the subvectors uh the distance of the subvectors and and and basically when wehave 64 uh bits uh 64 bitsrepresentation we have uh we just haveuh a lookup to do for every of the subvectors when we compute distances and umand then some summing them together sothat's for uh to do one distancecomputation instead of doing adecompression plus plus uh plus L2distance what we do is we do eightlookups and seven additions to get thethe actual resultso um okay so that's that was theprinciple of her quantizationuh so I I'm I see that I'm kind ofdiverging a little bit from the Historypart so let's go back back to the thehistory of product quantization sobasically this uh this all happened inthe so that that was back in 2009 Ithink so and this was all you know uhgenerated by the by the the the thebrain of uh are they and so I wasimplementing and together wewe made we made a paper about this uhthat we submitted to a journal to Pammyso that was the uh the IVF uh so it's itwas the product quantization paper whichcontains both the idea of doing uhproduct quantization so it's called yeahby the way it's called the productquantization because you have a productspace when you have sub vectors likethis it's like when you look at it froma space perspective you basically have aCartesian product of the subspaces andso that's why it's called the productquantizationuh but but the the term wasn't wasn'tinvented by by us it it existed beforeit's uh it's a relatively classic in thein the coding literatureso uh we published this paper and um andso there wereumuh so it we we compared it uh with uhwhat we had previously the our Hammingembedding method uh so which was basedon on a binary representations andum and it turns out so uh it turns outthat it was it was very fast and uh IthinkI think at the timeat the time maybe the it was so we werequite convinced that it was a very goodmethodbut in the end it took a very long timeuh before before uh people actuallyrealized that uh that uh binaryrepresentations areareum are just sub-optimal so it's it's soI think much of the story afterwards wasa kind of fight well Iit was a very you know very polite fightbut still it it took it took a lot ofEducation to uh to to to to convey thismessage that despite the hugeum amount of literature and theoreticalresults that there is around uh locallysensitive hashing uh that binaryrepresentations are just too crude to beuh to be efficient because uh becausetheir their representation uhcapabilities are insufficient I meanit's it's just you cannotumyou cannot you cannot even look at theat the Lloyd's optimality conditionsbasically a binary presentation in itsbest form would be so that's interestingactually it would be a productquantization where you have a single bitper you have sub vectors of size one sothat's a scalar and you have one bit perper sub vector and so it's a veryspecial case of product quantizationwith a very crude way of comparing themif you use Hamming distances but youthere are extensions of binaryrepresentations where you actually doyou do asymmetric binary search so thesame you you don't take code or youdon't take the binary representation ofthe query Vector but you you take theyou take the floating Point vector andyou can you canuh make it so that uhthat you can get to floating Pointdistance but then you you kind of losethe advantage of doing very quick uhhaving distance comparisons in thebinary domainokay so to come back to the history souh the the theproduct quantization uh basically it'suh we we've done we and then otherpeople have done many follow-ups in thatfield and so the follow-ups areare interesting I think that um so theresince there are two components of uh ofuh of the uh of the the what we call IVFPQ so inverted file plus PQ payloadrepresentation uh so there have been uhimprovements on both sides on both ofthe on the IVF side so the inverted filerepresentation and on the PQrepresentationuh so uh so before we started so I'mgoing to kind of to reattach this to thehistory I can say what what happenedbefore uh before we started on the theface Library which is uh which what thebig software and undertaking so at thetime so in terms of software let's talka bit about software so uh back in uh in2000 uh 2009 we we produced a softwarethat was called PQ codes and thatimplemented all of this uh in in uh in Cplus plus andum no in C actually right because we hadan inversion of C plus plus at the timewhich I to a certain extent that I stillhave and uh so it was in C and but itwas a closed Source library becauseum we decided that we wanted to sell itand uh uh and at the time it was not soclear that you could and be open sourceand sell something so it was close to soand we sold it to uh to a few companiesthat were using it for uh forlarge-scale indexinguh so uh what happened uh so that's whatso in a sense I think that the facts I Ithink back in the time people were notdoing that much uh open sourcing uh evenin the research domain it was not at allobvious that that if you if you foundsomething or if you publish the paperyou you'd open source it and we didn'tdo that we didn't do that for severalfor several of the papers and um andmaybe at the time it was not that clearas well that uh open sourcing is is justa a royal way to increase the impacts ofpapers and um and so we we kind of uh wekind of expected that people would bere-implementing it and uh that thatwould be enough butso I I think this is something thatreally flipped in the in the last uh inthe last maybe 10 years or so um and soso what what happened is that uh therewere PQ implementations that started topop up uh so uh at Microsoft uh atGoogle had a early PQ implementation aswell and so the the Improvement andbutum in this in this in this that it wasstill not clearat the time there was no real uh verybig uh or established Benchmark for uhfor uh for for a near sniper search andand basically the the open sourcing ofthose methods came in parallel withbenchmarks that were established to uhto actually really compare them and toto make the state of the art clearuh so but so that that's that's uhthat's something that happened on thesoftware uh and on the and on the youknow on theon the adoption side of of thingsso what happened so I can say a fewwords about what happened in researcharound the inverted files on the onehand and on the PQ on the other hand soum one of the one of the the main painpoints of the ivfpq method was thatthat the first level course quantizationwas uhwas was a limiting factor because if youwant to index more vectors you need tohave a largerum a larger vocabulary so I can explainthis a little bit sobasically uh when you do a search in anivfpqindex there are two components of thisof the search time and the first one isto do the course quantization so takingthe query vectors the query vector anddetermining which inverted lists must bevisited and so that boils down tofinding the topand so it's in face it's called n probethe top and probe number of invertedlists that need to be visited and sothat's the first stage and it's it isalso a nearest neighbor search problembecause you find the nearest neighborsof the of the query vector and thesecond the second stage is to doum is to actually scan those invertedlists and compute the distances usingthose lookup tablesand uh so it turns out so you need tofind a balance between those two costsand the the theuh it it turns out that when you scalethe data set to larger sizes uh ingeneral the number of centroids needs toscale as the square root of the numberof of the number of vectors that youwant to index because if you scale it asfast as the number of if you don't scaleit at all then the inverted list willjust grow proportionally to the numberof vectors so it so the cost is going tobe proportional to the um or the scalingcost is going to be proportional to theuh to the search time to the number offactors but and if you if you scale thenumber of centuries as quickly as thenumber of vectors on the other hand thenthe inverted lists stay about as longbut the course quantization cost isgoing to scale linearly with the numberof vectors and So to avoid this you kindof spread the effort onto both of themand to do this a rule of thumb is to isto scale it it's a as a square rootso if you scale it as a square root whenyou start getting to 1 billion vectorsuh it's starting to be a bit slowbecause you're in the order of hundredthousand vectors uh maybe a million ingeneral it's a bit larger so it might bea million and if you have a millioncentroids to compare with uh it becomesit becomes slowand so there have been several uhseveral propositions to improve this uhimprove this course quantization costand the first one which was quite cleveractually it was a method method bybabenko and lempitsky which consisted inuh in breaking down the the the the thethis are choosing the centroids as therepresentation space of a productquantization itselfso to make it very clear if you if youwant to have one million centroids yousay Okay I I haveum I have a thousand uh some trades forthe first half of the vector and I havea thousand some trades for the secondhalf of the vector and then I if I takeagain the the prob the Cartesian productof those two sets I get a millioncentroids and I can do efficient uhefficient uh uh lookup to lookups orefficient nearest neighbor searches tofind the nearest centuries of the thequery vector so this this was the firstum quite uh quite effective way of um offinding the news neighbors or or doingefficient course quantizationum or maybe there was an earlier onewhich was doing hierarchical k-meanswhich is also quite natural so it meansif you have a if you want to have amillion uh a million uh Sun trades youyou start by by doing a k means in 1000and then within each of those clustersyou do again one thousand and umso both so remember that that thek-means is in some respects it is theoptimalwell if K means found the global Optimumwhich is not true but which is a goodapproximation k-means gets you the bestset of centroids that you can find butuh given that you you need to find atrade-off between speed and accuracythose two methods doing hierarchicalquantization or hierarchical or k-meansand doing the doing the the the thewhat's what's what they call theinverted multi-index which means findingthe finding the two sub vectors andhandling those separately it wasum it was it was also a good option thatthat states the best so the best coursequantizer I think for large scaleapplications until uh until uh around2017 or 2018. and uh and uh when peoplerealized that you could as coursequantizer you could use to use graphbased methods to to do similarity searchso I know that you already had a wholepodcast about graph based methods and uhso maybe I can say a little bit how thisincludes in this uh in this story aboutuh about inverted files and productquantization so basically graph basedmethods are are very very fast andaccurate they are not very scalablebecause there's a very big overhead tostart the graphs themselves but yeah sothe graph based methods they um so theyare very they're very fast and accuratebut they have a scalability issuebecause starring the starring the thegraph structure itself becomes skills uhliterally with the size of the data setand uh and it becomes a dominant costat when the data sets when the data setbecomes larger it becomes a problematicbasically I mean it's it's always thesame problem in operations research onceuh when a problem doesn't is is not alimit you ignore it but once uh one isit becomes one is becomes a limitingfactor you you start worrying about itand uh so basically yeah so uh the uh soin particular hnsw which is really avery impressive algorithm uh it's itscales at the size ofuh of a million maybe 10 million butbeyond it's it's very low slow to buildand uh and and it takes just a hugeamount of memory and so um so so hnsw isactually but this makes it the perfectcandidate for for uh course quantizationand uh I think that um uh soum uh the same babenko and Yuri theymade they made a paper about using it asa cross quantizer and it's it is verygood and that's uh that's how the thegraph based methods can be included intoum into the ivfpq systemso that's what so that's about the crossquantization uh and basically whathappens is thatumevery Improvement that we have on thistask of news neighbor search it can beapplied to cross quantizers and sothat's and quite it's quite easy to uhto inject those improvements into intothe into the ivfpq framework so that'sabout the cost quantizer then we havethe the the product quantizerso there wereum there were several improvements overthe over theum uh of the the core product quantizerthe firstum maybe the well maybe the first one isis to just do re-ranking so basicallyusing a product quantizer as a firstapproximation that gives you the top sosay that you need the top 10 resultsthen you find the nearest neighbors withuh with with ivfpq and you take the top100 and then you you compute exactdistances or distances with a betterapproximation for the top 100 and keeponly the top 10. and this is this isreally a very effective method it's it'sit's really what enables you to get agood recall at one so really get the thegood results at the very first uh for asthe very first search resultum without impacting too much the thesearch time the problem is that you needto some auxiliary storage which might beRam but uh it could be disk also or SSDto start the the high qualityapproximate mention of the vectors orthe full vectorsso that's the that's the first thingthen there were someuh there were some attempts to improvethe to improve the the quality of theproduct quantizer and the first problemof the product quantizer is that it'sarbitrarily uh chunks the vector intointo subsections and um if you if if itso happens that in your data set all ofthe variants of the data set is only inthe last 10 components of your thousanddimensional vectors then you areallocating a lot of bits or a lot of uhencoding capacity to the first parts ofthe vector and those are completely lostand uh soum there's one very simple and theunfruitful method that was applied onprivate quantizer which is called optimoptimized product quantization which wasa an early weight work by Kevin hay whenthe and became the or the verysuccessfularchitecture or CNN architecturedeveloper that we that we know and itworks of meta now and that that methodconsisted in applying or finding andapplying a random not random but arotation to the input vectorsso that the the energy in each subvectorwas balanced the objective was to findthis this rotation so that the the thethe the energy was spread equally acrossthe sub vectors and um and since it's arotation a rotation doesn't change thethe euclidean distance so it's you don'tsee it on the euclidean distance andthis is really useful for many uh manydistributions that wouldn't naturally bewell balanceduh so then then there was uh maybe worthmentioning the lopq method which is alocally optimized PQ method uh and thisone consists uh but this is specific touh or it it applies on an igfpq indexand basically the problem with IVF PQindex is thatum each eachumeach uhin each event inverted lists you use thesame PQ the same trained productquantizerto to encode what's in those invertedlists and this it's a bit it's a bitunnatural because in fact since the roleof those uh inverted lists is is to makecells so it makes cells in the embeddingspace and so you could say that pointswhere you already know that they fallinto one of those cells they're probablythey don't probably don't have the samedata distribution of as if they fall inanother cell and so what lopq does isthat it trains as a product quantizerseparately for each of the cellsso the but then there's a trade-offbecause it's expensive to train becauseyou need to to store all thisinformation separately for each of thecells but it brings a a fair a fairImprovement of the of the recallsum uh on on most data setsumokay soum yeah so maybe so I'm going tocontinue a bit with the history becausehere we are about at uh 2015 so 2015um Harvey and I joined Facebook soFacebook is the old name of meta maybeyou'll rememberand um uh and we joined Facebook thatwas opening an office in in Paris and uhso uh we basically moved to Paris and uhand basically it was pretty clear sincethe beginning uh that we needed to dosomething about uh nearest neighborsearch in Facebook uh for productionsystems because uh the the systems werevery far from state of the Arts and umand so uh this needed to be improved anduh so and so when so have they arrived Ithink six or seven months before Iarrived and here I had already startedwith this face project and uh whichmeans uh Facebook AI similarity searchand um so uh so then we um so so wearrived there and um and so we we we westarted you uh creating this uh piece ofsoftware called face uh Facebook AIsimilarity search[Music]umbasicallyum uh we since since the start we saidwe wanted to be open source and uh uhand I did so uh the head of um ofFacebook uh AI was um Facebook AIresearch was Janika and he was verysupportive of that of that and so uh andso we started working on on faceand uh so we uh uh and basically our Imean I arrived after and how they hadstarted implementing it in C because youknowum because C plus plus is uh is toocomplicated so uh so but the people inproduction they were telling us uh okayuh I mean it's already complicated wedon't want to have this and see you soso I rewatched it in C plus plus and I Itook over uh the as as the leaddeveloper of face pretty quickly at thetime there was umuh there was aum everybody was using Lua as thescripting language and so there was ascripting language Bridge uh with uhfacethat remained internal until uh 2017 Ithinkum so so yeah I mean I my personal tasteis that Lua is really crappy languageit's and so I was pretty happy when uhso I I did python into face for facequite quickly and using Swig and um andI kind of and in the end we wheneverybody switched over from Lua topython when pytorch was created we kindof forgot about the Lua the Luainterface which is a good riddance andumuh and so and one important uh aspect ofthis is that there was an engineer at uhat Fair Jeff Johnson he's one of theoldest members of um of fair in terms ofuh you know number of years at Facebookand uh and he got this Library caughthis interest and he decided to do a GPUversion of it so GPU meaning Nvidia GPUbecause I mean that's kind of thestandards uh at Facebookand he started developing this and umI think it was a it was a prettyinteresting project for him because uhbecause there were several aspects orthe algorithms were not not out of OutOf Reach in terms of optimization ongpus which sometimes sometimes happensif there's really two irregular Behavioror graph algorithms are very hard tooptimize in gpus but this problem ofoptimizing ivfpq is was actually prettyum pretty uh pretty reachable for uh forfor gpus and it so he made a veryefficient GPU method and actually itturns out that we decided to publish apaper about face and the it was clearthat the flagthe flagship property that we want toshowcase for face was the the GPU theGPU implementationum and so so yeah that that was in 2016uh then 2017 started we started tonegotiate when we would actually beallowed so there were two aspects tothis the first one was internal adoptionso there was a lot of work that thatJaved was very much involved in inexplaining to uh to prod people atFacebook how interesting it was to haveto have this to use this library to usea conversation based methods to dosimilarity searchand uh on the uh on the other hand therewas uh there was the external impact sothat means how are we going to opensource it and actually it took us quitea lot of time to convinceum to convince our management uh wellfirst to uh to open source that but notso much of a problem but the realproblem was to get it to open source itwith uh with the MIT licenseand basically sothis is something well open sourcing wasreally something I discovered when Iarrived at uh at Facebook it's uh it isactually it is very hard for companiesto for other companies to adopt opensource software that that doesn't have avery permissive license because you knowfor example the GPL is not possiblebecause it's it's it's it contaminatesand so and so we when we initially opensourced the the library in with the theCreative Commons and non-commercialapplications the companies we talkedwith told us we cannot use it and sowhich makes sense and so we can we wentback to our management saying we want tohave we want this library to have realimpacts and so if we want that we needto open source it in uh with apermissive license and sothis took a long time to negotiate butin the end we were allowed to opensource it that wayand uh and so uh basically I think thatumour point was that the similarity searchspace was not mature yet so that at thetime in 20 around 2017 it was likepeople were using uh flan they wereusing uh annoy and they were using thiskind of libraries which in ourum in our opinion whereI mean not state-of-the-art compared towhat what you already had in researchfor several years and so uh having astrong solid open source Library whichwith more or less industrial support anduh that could could have a larger impactand so um so then we uh we we wereallowed to open source it with the so wegot the the the proper license that wewanted andum and basicallythen then that I mean that ball wasrolling and umso after that the the history of facewasum several stages of additions ofseveral methods so uh the first onebeing hnsw so it became uh when the whenthat work uh came out it became prettyclear that it covered a kind of a spaceof operating points where we were reallyfar from the state of the art so uh sowe implemented or implemented hnsw intofaceum then there was and umso uh there was aH so there was this method so the sothis is interestingone one uh one uh so when you look athow uh IVF PQ Works uh so you have theproblems of the Cross quantization andthe second part is how to optimize thescanning of the inverted lists which isthe second part of the cost and thesecond part of the costume basicallyit's the cost is dominated by uh by thememory lookups into the lookup tables soyou have lookup tables you do lookupsand the problem is that that modernprocesses are not at all efficient forlookups they are efficient forarithmetic throughput but not forlookups and so um so what we did so it'snot what we did but uh there has been aline of research around storing thoselookups in simd registers and um and sothere's early work by uh by people fromTechnicolor um I'm thinking of a guynamed Andre a and um they basicallyexplored this this direction and uh butthe the industrial application of thiswas the the scan uh algorithm or thescan Library uh which was recently opensourced by Google and basically theyhave a very very cleverly optimizedumivfpq implementation where the whetherthe lookups or the lookup tables arestarted matches this and well the wholeprocess of uh of computing distances isvery well optimized and uh so they theyhad very good operating points and so uhand so we also imparted this into uhinto uh into phaseso that's uh that's a bit uh of what wedid and I think in terms of so let tocome back to uh to uh to thequantization which is uh one of the themain topics uh uh of of face and also ofuh of what we were discussing uh so thequantization currently what we'relooking into is uh so we have theproduct quantization butum uh actually it's there there arequantization methods that get betteraccuracy uh because uh productquantization has this restriction thateach sub Vector is encoded separatelyand so uh so we lose the kind of uhstatistical dependence between the subvectors even if we do an a rotation sothat this dependence is minimizedum and so what we are looking very muchinto here currently is additivequantization methodsand the additive quantization means thatinstead of having sub vectors and youconcatenate sub vectors you haveyou have a lookup tables that span thewhole Vector to encode and but you haveseveral of the those lookup tables andyou pick one vector from each of thelookup tables and you just sum them upso and then you you have to encode onlythe the ID of the vector that you pickedin each of the lookup tablesso it can be seen as a generalization ofPQ because if the lookup tables are zerooutside of a sub Vector if each lookuptable is zero outside of sub Vector thenit boils down to doing PQum it's a generalization so it has thepotential to be more accurate and theproblem is uh what it it's much morecomplex to train the lookout tables andto do the encoding the encoding is uhit's not like just finding the nearestneighbors it's um it's a it's acombinatorial optimization problem uhthat that is NP hired if you if you wantto solve it exactly and so you solve itonly by approximationsand souh and so basically we are looking intohow to do those uh approximationsefficiently there are several uh severaldirections for this and two of them areimplemented in Phase the first one is uhlocal search quantization which is uhum which is a work via from the PHD ofJulieta Martinez and uh that uh thatoffers a good trade-off between uhbetween the accuracy and encoding speeduh knowing that uh that encoding speedcannot be as good as that of PQ but it'sstill it's based on the simulatedannealing uh from with randomindustrialization and it kind ofconverges intoum into a relatively good additiveadditive conversation methodand the second one is just residualquantizers so that residual quantizesmeans you use the you use the firstlevel quantizer and then you keep theresidual with respect to the vector thatyou want to encode and that gets you asecond level quantizer and then youencode it with a second level quantizeretc etcum and but if you do that in a greedyfashion it's not good and so in order toavoid to do this in a greedy fashionyou'd use beam search and so which isexpensive so again there's a trade-offin terms of of speed versus accuracyyeah so that's um that's a bit what wewhat we are currently working on uh inPhase uh So currentlyum just to say So within uh within uhmeta the the team the core team thatworks around face is uh is about fivepeople uh not everyone is workingfull-time on thisum but it's that's that's about the theskill of the efforts at Facebook it'ssuper cool yeah that was a brillianttour of product quantization so muchknowledge um I'm also really excited inthis VBA podcast to welcome AbdelRodriguez to the webia podcast uh Abdelis working on this kind of productquantization weavier uh Abdul could youtell us about kind of where we're atwith the product quantization and anyquestions you have for Matisseso thanks Connor and thanks uhMatthias for for the nice history andintroduction so I I actually we are averyin the very beginning of the productquantization part now because we we havebeentrying to improve the indexing algorithmand and trying to make it scalable onwhen when we have more datawell we have we will have morerequirements and and we we have to dealwith it and we are currentlyexperimenting with having someinformation on disk some information onmemory and and the part that we need inmemory of course issome representation of the vectorsand uh we are currently playing a bitwith the with thea compression of these vectors andand again we are we are scratching thesurface here now we we have a veryuhvery simple implementation of uh of PQcurrently with k-means and andsegmenting the the vectorsand we would like to explore a bit theoptimized product quantization theoptimized PQ nextand one thing we we have is one problemwe have is that normally we don't builduh so we we don't have all theinformation and we build an index in butbut we we we normally buildincrementally our our indexing whichmeans we need some some algorithms thatcould take over this uh capacity to toadd new vectors or delete vectors thatyou have instead of just buildingeverything together and of coursek-meanscould somehow be incrementally updatedif you keep at least the number ofclusters which is uh something that wehave but I'm also wondering about in thecase of up the optim opq this rotationMatrix how how how hard in terms ofperformance would it be to to make italso incrementally updatable things likethat I I don't know if you have someexperience in this direction that itwould be nice to to hear a bit about itsure uh soI think it's a it's a it's aninteresting and recurrent problemumuh what happens is that uh so I thinkthat there are two things to distinguishhere it's which is an increasingdatabase database side size uh because Imean indeed you add incrementally youadd more vectors uh the the other canthe other thing is um is the drift inthe data distribution addressed in thedata distribution is in addition toadding vectors they have they havebehaviors that you've never seen beforeand during the training phaseum I think the increasing database sizesize is not necessarily a problem if youif you've had enough data to train fromthen uhto be to be very concrete a way I wouldImplement a database where you don'tknow in the beginning how big it's goingto grow it's uh you accept the first 10000 vectors you don't encode them at allyou just keep them as is if you then youwhen you go to a million you you do somesome cheap or some simple indexing saywith hnswand then when it grows beyond that youand you start toto require some type of encoding thenyou can start thinking of uh training aproduct quantizer or some some othertype of quantizer but at that point youhave enough training vectors to actuallyactually train it so so it makes senseto to go to the to go that pathumuh which you I mean if you after if youhave 10 000 vectors you can'tuh 10 000 vectors use no I it's it's abit too small to to even train yourproduct quantizer so you would even wantto have moreum the other problem is uh is uhdrifting the data distribution which uhwhich we observe also with some kind ofapplications uh one one funny anecdotesmaybe is that we um uh we we have wehave an indexing we observe really a lotof data drift in in images that come inthat comes with that are uploaded toFacebook so I've worked together withpeople who index those images and youhave data drifts with memes but alsowhen there's a new Instagram filter thatcomes out that kind of you see a driftin the in the type of images that youget and uh and soso it's a and and the problem with thatis if you if you update the trainingyeah the main problem with any quantizeris that if you if you update thetraining then uh or if you do forexample online K means to adapt to sometrades of the k-means then the the thethe vectors that are newly encoded theyare not comparable anymore with the theones that were encoded before and so umand so it's it's not clear to me bydefault how uh how to how to use thoseupdated centroidsso um yeah it's asomething to think of yeah so when youhave the online clustering you can movethe mean but then you need to recomputethe centralizers at like very basicunderstanding the high level idea yeahit also depends I guess if you have moreintensivechanges in some parts you don't have torecode everything but that part'saffected I would guess butmm-hmmso maybe we'll also transition to thetopic of generally thewrapping the vector index around alibrary compared to a database so maybeedian could um explain kind of the someof the features of the database and thedistinction between how you're packagingup the vector indexso yeah sure this I think we've we'vediscussed this uh before already in inone of the podcasts but it is arecurring topic and we do see that thatcoming up from from usersum whereas I think one of the the firstand most obvious distinct differencesthat we see and end of charismatize I'malso super interested about yourperspective on this as someone who'sbeen working on the library becausemine's a bit biased of course becauseI've been working on the database sideuh what one that that comes up very highon the list typically is this thisincremental updateability which I thinkis not like this can be a library versusdatabase part but it doesn't necessarilyhave to be because hnsw you can use itpurely from a library perspective and itis incrementally changeable uh somethingthat needs to be trained beforehand sofor example a quantizer that is maybenot as updatable so in in this caseum the library versus database technicaldistinction doesn't so muchumsort of determine of whether it'supdatable or not more that within thedatabase you tend to go for these kindof updatable cases so for usum from a database perspective typicallywhat we say the kind of ux that we wantis the one that people know fromnon-machine learning databases so if Ijust spin up in my SQL database spin upbecause Android database typically Ishould start using it and I don'tnecessarily know what I'm going to dowith it tomorrow I might updatesomething I might delete something Imight read in between I might do thatall concurrentlyum and then of course you have to docapacity planning there as well in somedatabases scale more dynamically thanothers but this is a big part sort ofthis this usage journey of using itum yeah directly basically uh um orwe're using it like a like a databasebut there's there's uh way more so soone of the things for example that'salso super important to me is the kindof durability aspect failure recoverymode so so how does it react so so forexample something I think an inplacement has correct me if I'm I'mwrong I thinkum what I typically see in libraries forpersistence is snapshotting that youwould build something and once it'sbuilt you would snapshot it to disk andthen you could load the the snapshotum whereas in in a database such as VBAfor exampleum the the update process itself isalready persistent so if vv8 crashes Idon't know let's say you import 10million and bb8 crashes at number 7million then you can just restart it andimport 7 million and one so so this kindof incremental durability crash recoveryum everything that's written is writteninto a writer headlock is is a big thingand then um and maybe that's also aninteresting one uh with Facebook becauseI think there's a a separate librarythat I believe is not not exactly facebut but built on top of face thatdistributes a face across multiplemachines and that is also somethingthat's very big in the in theum in yeah for vv8 or for databases ingeneral soum yeah the whole scaling aspect issomething that you get out of the boxfor free as well that's that's my shortmy short overview of the the differencesbut I'm very curious to hear yours aswell in the type yeah sure uh soum I think that's face explicitly triesnot not to go into the field of being ageneral purpose uh Library so there aretwo reasons of that the first one isthat I think that face is already a verycomplex piece of software uh if you ifyou were to have to support that itwould mean that the number of code lineswould be multiplied by a factor three orsomething which is not something we weplan to plan to do the second thing alsois that in general I observed that SQLdatabases are I don't know about vv8 butvery often databases havean order of magnitude storage more thanwhat's strictly required by the amountof data that you have in this forexample I've read somewhere guidelinesthat if you want to set up a mySQLdatabase then you should plan for uhfive times as many disk space as what uhthe the raw data I would use and this issomething that's facewe want to give the opportunity forpeople to you know use 90 of their RAMand store the an index in that and thatthey don't wonder where uh about toomuch about overheads basically and so uhand this isuh yeah it would it's it happenedseveral times that we're operating veryclose to uh what's possible on a singlemachine and uh for this uh we we give upmany functionalities like what you sayuh being able to do uh ID lookups uhbeing able tobe being able to snapshots or this kindof things and it'syou know that's that's the point butdefinitely I wouldn't recommend face asa final production ready database systemit's it's really intended to be at thecore of maybe anothermore broad scale database systemfantastic well thank you so much fromtires Abdel Eddie and such aninformation dense podcast so interestinglearning about all these things thankyou so much for your timeyeah thank you thank you nice yeah yeahthank you all yeah I", "type": "Video", "name": "Matthijs Douze on Quantization and FAISS - Weaviate Podcast #29", "path": "", "link": "https://www.youtube.com/watch?v=5o1YTp1IL5o", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}