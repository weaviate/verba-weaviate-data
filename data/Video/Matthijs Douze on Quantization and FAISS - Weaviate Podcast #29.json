{"text": "Hey everyone, thank you so much for watching another episode of the Weaviate podcast! This episode features Matthijs Douze, ... \nhey everyone thank you so much for checking out another episode of the wevia podcast this is a super exciting episode of BBA podcast we have a research scientist for meta AI Matisse duzy uh his research publication list is just amazing the amount of work he's done in the space of vector analysis product quantization all sorts of things of deep learning really is really truly one of the most impressive scientists I've had the opportunity to talk to so Matisse thank you so much for joining the podcast yeah thank you very much for hosting me awesome so can we kick this off by telling us about the history of the research uh like how did you come to be working on these things what first inspired your interests yeah so um yeah I can say a little bit about my background so um I have a PhD in computer vision basically so that's my background uh I did it in in France and uh then I moved to um to a research uh institution the which is inria and where I met harveyjigu who he has an expertise more in anything related to coding encoding decoding the kind of things that's used for GSM networks and uh and so he was the main inspiration to move towards thinking about how to better compress uh compressed compressed vectors because basically there's one very clear application for this in computer vision at the time that was very important which is that images were analyzed by extracting sift features from them and sift features I just um 128 dimensional vectors that are a new extract in the order of hundreds to a thousands per image and those those were early embeddings so that means that it's a representation of a small part of the image and then you could do a couple of things with those you could use them for image classification but the the part that we're most interested in was image indexing which means uh which meant at the time and still means now finding similar images for example images that represent the same object or the same building and to do that you had to um you had to find which where the nearest embeddings the nearest sift vectors from the ones that are in the other image and so there was this um at the time there was this very interesting work um that came from uh from the Andrew zisman's lab and uh uh and which which was basically led by Joseph Savage and uh which basically uh reduced this Bunch this big number of CIS vectors so you had like like a thousand sift vectors for one image which was pretty heavy and to reduce that into a bag of words and back of words just means that you reduce each Vector into a single identifier a single a single number that was um assigned by nearest neighbor uh nearest neighbor search so searching the nearest centrates to each of the vectors and just keeping that only information about that vector and so that means that if you had if you had a like say 1000 centroids you would have the only information that you keep from all those heavy sift vectors in the on the image is a kind of histogram over the Thousand centuries about how many sips vectors were falling into those sift actions so and it was called um a bag of bag of visual words because in a sense by analogy with text you would reduce the the the you know the continuous space of the the set vectors into a single a single token a single a single word uh and that represented the image and then the images could be compared and the basically what's what happened there is that this represents this back of word representation that was used for many many applications including classification but what it also enabled was uh to be able to um to do large-scale indexing so that means since it's so compact you could say okay I have uh I have uh ten thousand a hundred thousands a million I think before us the the largest application that was at scale 1 million uh one million uh images and you could index those and that was that was quite novel at the time that it was possible to really find in in real time so there was a work um that was coming from uh Nissa and stevenius where they basically had a collection of CD covers and they were showing it to a webcam and the webcam would find in real time uh what were those covers what those covers were corresponding to and this was really enabled by this uh this bag of fart representation so that was so the back and forth's representation was used to uh quite extensively at the time and uh it has been expanded and so on but uh what I what the the second or the The Innovation that came from this work by uh by cevich and this woman was the fact that they used an inverted index so that means uh it's inverted because you start from uh from um from from those those histograms of of visual words and instead of stacking those histograms or visual words you invert the index and you for each visual word index you record which images and contain that particle particular visual word and uh so that made it much faster because what it enables is that if you increase the number or the the size of the visual vocabulary so instead of having a thousand you can say I have ten thousand hundred thousands Etc uh you can you you get a very sparse uh sparse histograms obviously and that means that the inverted index when you actually want to find which visual words are in common between the query image and a database image you need to visit fewer a very small fraction of that of that data set so that was um so I think that that was a bit the background when we arrived so the the key elements that we had there was uh the the fact that we had um we had this representation of uh High dimensional vectors with a single integer which is exactly quantization and so that's where you know the expertise of harveya started to be very useful and we have this inverted file structure that was the start of a lot of fruitful um indexing methods that were developed later including the ones that we developed so then we arrived and uh so we we recognized this uh the quality of this uh of this inverted list and the potential the potential that it had to do large scale indexing and um and so what we added to that the first the first thing we added is actually uh thinking that within those inverted lists so the inverted lists are in the in the in the initial um uh backup visual word representations contained only the document IDs or the the image IDs that contain those specific visual words and so the idea that we came with was to add a kind of payload for each of those uh instances where we kind of refine a little bit the representation of the of the the vectors because uh because representing a 128 dimensional vector by a single integer is a very crude quantization and so what we did at the time is that we we used a pretty standard system that existed at the time and it was uh it was to do a kind of binarization and that we we called Hamming embedding and so having embedding basically consists in taking this 128 dimensional Vector applying a random rotation to it and then keeping the sign of each of the components after rotation and so then we get a binary Vector which at the time we didn't use the 428 Dimensions but we used just 64. and then we had a binary representation of 64 bits and we could do that both on the query side and on the database side and then comparing binary vectors it's very very efficient to to compute the to compare those with Hamming distances and so then then we had um uh we had a image indexing system that was very efficient and that was based on uh on what currently we would call an IVF an inverted file and having a battery Banning so um a binary representation of the of the the vectors that are stored with in that inverted file so that was the first stage and we got we it was very successful in terms of of large-scale image indexing I remember redoing the demo with the webcam with uh with a laptop and uh and a webcam and uh and I'd scale 10 million images and an external hard drive which uh so for the technical details which which didn't have a partition at all on it and so there was no file system because we need to access very quickly we need to access the images to display the results so we were accessing directly the offset on the disk uh of the the actual images and uh and this was this was working pretty well and um and so that was the first iteration of uh of uh of uh of this combination of um of the the IDF which where the objective is really to to to very quickly prune the data set though the parts of the data set where you need to search and an encoding method uh whose objective is to get an approximately reasonable or as good as possible approximation of the of the um of the of the vectors so it turns out that so at the time there was the the there was a lot of work around binary representations for for vectors so uh there was a whole literature around localized locality sensitive hashing which is not exactly uh the same as a binary representation but very often a binary representation is is based on the locality sensitive hashing Theory and um and so that there were interesting theoretical properties uh and uh so there was a spectral hashing uh which was a work by taralba and uh so there was a literature around that and uh but what we so there the expertise of Harvey that he had uh for for anything related to quantization uh came in very useful because he knew and it became clear afterwards that actually uh binarization is a very crude way of of doing quantization so basically it is quantization because you you transform a continuous signal into into an integer basically but there are much better ways of uh of or much less lossy ways of encoding vectors than doing uh than doing binarization so that maybe we can go a little bit into the theory of um of quantization so uh the parts that I know if I'm I can't say I'm much of an expert but I got I get got some experience and some very basic principles so the the first um the first two principles are the Lloyd principles so uh for quantization and uh basically since you since you map a continuous signal to our continued continuous Vector continuous High dimensional Vector to um to uh to one single uh representation so it's to one single integer then and you always have a reconstruction of that of of the approximation of the of the of the vector and so uh so in order for this approximation to be uh to be optimal there are uh there are two necessary and not not sufficient but necessary conditions and the first one is that each they are very natural the first one is that when you have a vector and when you look at the whole set of possible reconstructions that you can make with your quantizer so what that we call centuries in general uh then uh the uh the vector should be assigned to the nearest centroid so it's if you pick one a centroid that is not the nearest then by definition you are doing something sub-optimal so you always should always assign to the nearest vector and the second one uh so so this is quite natural the second one is specific to the to the to the L2 distance to the euclidean distance if you minimize the the if you minimize the the the squared uh error of the Reconstruction uh then each centroid should be the center of mass of all the vectors that are assigned to it in the distribution and uh so those those are two principles and the actually the very nice thing of of this of those two principles is that it translates to the k-means which is also called The Noise Lloyd's algorithm to uh to do uh to do clustering and as well as to do quantization because the canines is an uh basically what you do is you you take a training set that you suppose is representative of the of the data distribution and then you you you alternate between two steps the first one is to estimate or let's start with the assignment so you start with a set of of initial centroids that are determined with some heuristic or randomly and then you you assign to each uh you assign each training Vector to the nearest centrate so that you you you basically you do the assignment step and the second step is you update the centroids by by Computing the center of mass of all the points that were assigned to that that centroid so k-means is is one of the huge successes of of uh of quantization and of many machine learning algorithms it's very simple and um and actually it gives you uh it gives you a quantizer if you have a representative trading points it gives you a quantizer that is that has that follows those two properties of the the optimality of the of the two lights conditions and um so what's so this is very good so what's the problem with k-means the problem is that say that you you have budget to um to represent a vector with 64 bits so uh what happens is that you cannot really say okay I'm going to do a k-means where the the indices are going to be encoded into 64 bits because 64 2 to the 64. it's really a lot of centroids it's actually more than much of uh of the high numbers that we find in modern computer science so so it's just not possible it's not possible to do this to use this amazing cayman's algorithm at that scale it is possible and that's what we do to use it for IVF for cost quantization so for I for inverted files uh we have the the degree of freedom to choose the number of centuries that we want to use and in general it is beneficial to use a large number of centroids but we're not going to to have 2 to the 24 the 2 to the 64th centuries we're going to have like uh between uh uh a thousand ten thousand hundred thousand one million this this order of magnitude number of centroids so for this we can use k-means directly and we definitely do that uh to to to do this first step of what we call course quantization so the the inverted file that's going to to allow us to to search only a really small hopefully a small subset of the data set and still not lose too much accuracy in this process but when we when we when we're talking about the payload so the the the parts that you going to use to approximate the vector and that you store in the inverted lists uh then you you cannot you I mean if you have a payload of eight bits it's fine but in general you have a larger payload and the reason is because it doesn't make much sense to have eight bits because the even the image identifier is going to be longer than that and that's also start in the inverter lists and so if we want to go to 64 bits then uh so the the the fundamental idea of the product quantization which already existed but was never used for uh similarity search per se the idea there is to is to say okay we cannot really afford to do a 64 bits by uh by having a vocabulary that's that spans that has one explicit centroid stored for every uh for every uh set rate of the two to to the 64. so what we're going to do we're going to do a trade-off we're going to just chunk or to we're going to take the vector the input vector and split it into sub vectors and then apply this quantization this K means quantization or this exhaustive conversation apply it only on the sub vectors and this is um it's very it's it's it solves our problem uh of scale because let's say if we need 64 bits uh let's say that we can split those in eight times eight bits and eight sub vectors that each are encoded in eight bits and then doing the k-means for to get 8-bit vectors it's just okay means with 256 centroids and this is I mean this is you could almost do it by hand and um and then uh to and then you can encode each of those Circ vectors into separate uh into a separate representation and then just concatenate those representations and uh and you get an approximation of the the vector so uh so then we are down to uh to having um to having uh encoding costs that's very reasonable because at encoding time what you need to do is is exhaustively find the nearest centroid for each of the sub vectors but since you need to search only 256 hundreds it's uh it's pretty efficient and uh and uh uh and and the storage the other problem was the storage of the centroids this and since you need to store only small centroids in in for each of the uh for each of the sub vectors it's it's pretty efficient actually uh the starting the centroids is as large as storing 256 vectors themselves because if you add the sizes of the sub vectors you end up with the initial size of the data vectors so that was that was uh very interesting in terms of storage and in terms of assignments and so decoding is just a lookup and you separately look up each of the vectors and one very interesting property that we already that we also had and that's uh that was kind of lucky that it turns out this way is that it's also possible to do compressed domain distance computations so compressed demand distance computations means that if you have a query Vector um so in general I wouldn't say in general but very often you want to compress the database because it's size constrained but the query vectors come as a float so you don't really need to compress them and so um and so what you can do there is to so the the basic algorithm that you would do with uh with an igf and the PQ or with the PQ payload to compute those distances is to take the query vectors that are not compressed decompress the the database vectors or you scan the inverted list you decompress each of the vectors and then you compute the distance just by Computing L2 distance and and so that's fine but it turns out that it does something even more efficient that you can do is not decompressing the the the database vectors at all but when you know at the point when you know the query Vector you can you can say okay uh for each of the sub vectors of those query vectors I have only 256 possible Dimensions because there are 256 hundreds and so I come I can just instead of computing it every time you can just say okay I only have 256 possibilities so I just need to compute those this and all those 256 distances and what I'm scanning I'm just you know looking up those distances and and compute what's the what's the and and then I don't need to compute actually compute the distance and this is possible so it's possible to do that with uh with any quantizer you can compute the businesses with all the centroids and since it's a finite number of some trades you you always know that it you you will have pre-computed the distances but what makes this possible with the product quantizer is or what what's convenient with the product quantizer is that uh the distance is decompose across Dimensions with L2 distance or with L1 distance or with many distances actually you you can uh when you you can chunk the the vectors into parts and uh summing up the distances in the sub in the subparts to get the total distance between between the vectors and so that's or I mean it's not true for L2 distance it's Square it's too true for squared L2 distance and so we always compute squared all two distance and so and so in those sub vectors then uh we were um we uh we're adding together the sub vectors uh the distance of the sub vectors and and and basically when we have 64 uh bits uh 64 bits representation we have uh we just have uh a lookup to do for every of the sub vectors when we compute distances and um and then some summing them together so that's for uh to do one distance computation instead of doing a decompression plus plus uh plus L2 distance what we do is we do eight lookups and seven additions to get the the actual result so um okay so that's that was the principle of her quantization uh so I I'm I see that I'm kind of diverging a little bit from the History part so let's go back back to the the history of product quantization so basically this uh this all happened in the so that that was back in 2009 I think so and this was all you know uh generated by the by the the the the brain of uh are they and so I was implementing and together we we made we made a paper about this uh that we submitted to a journal to Pammy so that was the uh the IVF uh so it's it was the product quantization paper which contains both the idea of doing uh product quantization so it's called yeah by the way it's called the product quantization because you have a product space when you have sub vectors like this it's like when you look at it from a space perspective you basically have a Cartesian product of the subspaces and so that's why it's called the product quantization uh but but the the term wasn't wasn't invented by by us it it existed before it's uh it's a relatively classic in the in the coding literature so uh we published this paper and um and so there were um uh so it we we compared it uh with uh what we had previously the our Hamming embedding method uh so which was based on on a binary representations and um and it turns out so uh it turns out that it was it was very fast and uh I think I think at the time at the time maybe the it was so we were quite convinced that it was a very good method but in the end it took a very long time uh before before uh people actually realized that uh that uh binary representations are are um are just sub-optimal so it's it's so I think much of the story afterwards was a kind of fight well I it was a very you know very polite fight but still it it took it took a lot of Education to uh to to to to convey this message that despite the huge um amount of literature and theoretical results that there is around uh locally sensitive hashing uh that binary representations are just too crude to be uh to be efficient because uh because their their representation uh capabilities are insufficient I mean it's it's just you cannot um you cannot you cannot even look at the at the Lloyd's optimality conditions basically a binary presentation in its best form would be so that's interesting actually it would be a product quantization where you have a single bit per you have sub vectors of size one so that's a scalar and you have one bit per per sub vector and so it's a very special case of product quantization with a very crude way of comparing them if you use Hamming distances but you there are extensions of binary representations where you actually do you do asymmetric binary search so the same you you don't take code or you don't take the binary representation of the query Vector but you you take the you take the floating Point vector and you can you can uh make it so that uh that you can get to floating Point distance but then you you kind of lose the advantage of doing very quick uh having distance comparisons in the binary domain okay so to come back to the history so uh the the the product quantization uh basically it's uh we we've done we and then other people have done many follow-ups in that field and so the follow-ups are are interesting I think that um so there since there are two components of uh of uh of the uh of the the what we call IVF PQ so inverted file plus PQ payload representation uh so there have been uh improvements on both sides on both of the on the IVF side so the inverted file representation and on the PQ representation uh so uh so before we started so I'm going to kind of to reattach this to the history I can say what what happened before uh before we started on the the face Library which is uh which what the big software and undertaking so at the time so in terms of software let's talk a bit about software so uh back in uh in 2000 uh 2009 we we produced a software that was called PQ codes and that implemented all of this uh in in uh in C plus plus and um no in C actually right because we had an inversion of C plus plus at the time which I to a certain extent that I still have and uh so it was in C and but it was a closed Source library because um we decided that we wanted to sell it and uh uh and at the time it was not so clear that you could and be open source and sell something so it was close to so and we sold it to uh to a few companies that were using it for uh for large-scale indexing uh so uh what happened uh so that's what so in a sense I think that the facts I I think back in the time people were not doing that much uh open sourcing uh even in the research domain it was not at all obvious that that if you if you found something or if you publish the paper you you'd open source it and we didn't do that we didn't do that for several for several of the papers and um and maybe at the time it was not that clear as well that uh open sourcing is is just a a royal way to increase the impacts of papers and um and so we we kind of uh we kind of expected that people would be re-implementing it and uh that that would be enough but so I I think this is something that really flipped in the in the last uh in the last maybe 10 years or so um and so so what what happened is that uh there were PQ implementations that started to pop up uh so uh at Microsoft uh at Google had a early PQ implementation as well and so the the Improvement and but um in this in this in this that it was still not clear at the time there was no real uh very big uh or established Benchmark for uh for uh for for a near sniper search and and basically the the open sourcing of those methods came in parallel with benchmarks that were established to uh to actually really compare them and to to make the state of the art clear uh so but so that that's that's uh that's something that happened on the software uh and on the and on the you know on the on the adoption side of of things so what happened so I can say a few words about what happened in research around the inverted files on the one hand and on the PQ on the other hand so um one of the one of the the main pain points of the ivfpq method was that that the first level course quantization was uh was was a limiting factor because if you want to index more vectors you need to have a larger um a larger vocabulary so I can explain this a little bit so basically uh when you do a search in an ivfpq index there are two components of this of the search time and the first one is to do the course quantization so taking the query vectors the query vector and determining which inverted lists must be visited and so that boils down to finding the top and so it's in face it's called n probe the top and probe number of inverted lists that need to be visited and so that's the first stage and it's it is also a nearest neighbor search problem because you find the nearest neighbors of the of the query vector and the second the second stage is to do um is to actually scan those inverted lists and compute the distances using those lookup tables and uh so it turns out so you need to find a balance between those two costs and the the the uh it it turns out that when you scale the data set to larger sizes uh in general the number of centroids needs to scale as the square root of the number of of the number of vectors that you want to index because if you scale it as fast as the number of if you don't scale it at all then the inverted list will just grow proportionally to the number of vectors so it so the cost is going to be proportional to the um or the scaling cost is going to be proportional to the uh to the search time to the number of factors but and if you if you scale the number of centuries as quickly as the number of vectors on the other hand then the inverted lists stay about as long but the course quantization cost is going to scale linearly with the number of vectors and So to avoid this you kind of spread the effort onto both of them and to do this a rule of thumb is to is to scale it it's a as a square root so if you scale it as a square root when you start getting to 1 billion vectors uh it's starting to be a bit slow because you're in the order of hundred thousand vectors uh maybe a million in general it's a bit larger so it might be a million and if you have a million centroids to compare with uh it becomes it becomes slow and so there have been several uh several propositions to improve this uh improve this course quantization cost and the first one which was quite clever actually it was a method method by babenko and lempitsky which consisted in uh in breaking down the the the the the this are choosing the centroids as the representation space of a product quantization itself so to make it very clear if you if you want to have one million centroids you say Okay I I have um I have a thousand uh some trades for the first half of the vector and I have a thousand some trades for the second half of the vector and then I if I take again the the prob the Cartesian product of those two sets I get a million centroids and I can do efficient uh efficient uh uh lookup to lookups or efficient nearest neighbor searches to find the nearest centuries of the the query vector so this this was the first um quite uh quite effective way of um of finding the news neighbors or or doing efficient course quantization um or maybe there was an earlier one which was doing hierarchical k-means which is also quite natural so it means if you have a if you want to have a million uh a million uh Sun trades you you start by by doing a k means in 1000 and then within each of those clusters you do again one thousand and um so both so remember that that the k-means is in some respects it is the optimal well if K means found the global Optimum which is not true but which is a good approximation k-means gets you the best set of centroids that you can find but uh given that you you need to find a trade-off between speed and accuracy those two methods doing hierarchical quantization or hierarchical or k-means and doing the doing the the the the what's what's what they call the inverted multi-index which means finding the finding the two sub vectors and handling those separately it was um it was it was also a good option that that states the best so the best course quantizer I think for large scale applications until uh until uh around 2017 or 2018. and uh and uh when people realized that you could as course quantizer you could use to use graph based methods to to do similarity search so I know that you already had a whole podcast about graph based methods and uh so maybe I can say a little bit how this includes in this uh in this story about uh about inverted files and product quantization so basically graph based methods are are very very fast and accurate they are not very scalable because there's a very big overhead to start the graphs themselves but yeah so the graph based methods they um so they are very they're very fast and accurate but they have a scalability issue because starring the starring the the graph structure itself becomes skills uh literally with the size of the data set and uh and it becomes a dominant cost at when the data sets when the data set becomes larger it becomes a problematic basically I mean it's it's always the same problem in operations research once uh when a problem doesn't is is not a limit you ignore it but once uh one is it becomes one is becomes a limiting factor you you start worrying about it and uh so basically yeah so uh the uh so in particular hnsw which is really a very impressive algorithm uh it's it scales at the size of uh of a million maybe 10 million but beyond it's it's very low slow to build and uh and and it takes just a huge amount of memory and so um so so hnsw is actually but this makes it the perfect candidate for for uh course quantization and uh I think that um uh so um uh the same babenko and Yuri they made they made a paper about using it as a cross quantizer and it's it is very good and that's uh that's how the the graph based methods can be included into um into the ivfpq system so that's what so that's about the cross quantization uh and basically what happens is that um every Improvement that we have on this task of news neighbor search it can be applied to cross quantizers and so that's and quite it's quite easy to uh to inject those improvements into into the into the ivfpq framework so that's about the cost quantizer then we have the the the product quantizer so there were um there were several improvements over the over the um uh of the the core product quantizer the first um maybe the well maybe the first one is is to just do re-ranking so basically using a product quantizer as a first approximation that gives you the top so say that you need the top 10 results then you find the nearest neighbors with uh with with ivfpq and you take the top 100 and then you you compute exact distances or distances with a better approximation for the top 100 and keep only the top 10. and this is this is really a very effective method it's it's it's really what enables you to get a good recall at one so really get the the good results at the very first uh for as the very first search result um without impacting too much the the search time the problem is that you need to some auxiliary storage which might be Ram but uh it could be disk also or SSD to start the the high quality approximate mention of the vectors or the full vectors so that's the that's the first thing then there were some uh there were some attempts to improve the to improve the the quality of the product quantizer and the first problem of the product quantizer is that it's arbitrarily uh chunks the vector into into subsections and um if you if if it so happens that in your data set all of the variants of the data set is only in the last 10 components of your thousand dimensional vectors then you are allocating a lot of bits or a lot of uh encoding capacity to the first parts of the vector and those are completely lost and uh so um there's one very simple and the unfruitful method that was applied on private quantizer which is called optim optimized product quantization which was a an early weight work by Kevin hay when the and became the or the very successful architecture or CNN architecture developer that we that we know and it works of meta now and that that method consisted in applying or finding and applying a random not random but a rotation to the input vectors so that the the energy in each subvector was balanced the objective was to find this this rotation so that the the the the the energy was spread equally across the sub vectors and um and since it's a rotation a rotation doesn't change the the euclidean distance so it's you don't see it on the euclidean distance and this is really useful for many uh many distributions that wouldn't naturally be well balanced uh so then then there was uh maybe worth mentioning the lopq method which is a locally optimized PQ method uh and this one consists uh but this is specific to uh or it it applies on an igfpq index and basically the problem with IVF PQ index is that um each each um each uh in each event inverted lists you use the same PQ the same trained product quantizer to to encode what's in those inverted lists and this it's a bit it's a bit unnatural because in fact since the role of those uh inverted lists is is to make cells so it makes cells in the embedding space and so you could say that points where you already know that they fall into one of those cells they're probably they don't probably don't have the same data distribution of as if they fall in another cell and so what lopq does is that it trains as a product quantizer separately for each of the cells so the but then there's a trade-off because it's expensive to train because you need to to store all this information separately for each of the cells but it brings a a fair a fair Improvement of the of the recalls um uh on on most data sets um okay so um yeah so maybe so I'm going to continue a bit with the history because here we are about at uh 2015 so 2015 um Harvey and I joined Facebook so Facebook is the old name of meta maybe you'll remember and um uh and we joined Facebook that was opening an office in in Paris and uh so uh we basically moved to Paris and uh and basically it was pretty clear since the beginning uh that we needed to do something about uh nearest neighbor search in Facebook uh for production systems because uh the the systems were very far from state of the Arts and um and so uh this needed to be improved and uh so and so when so have they arrived I think six or seven months before I arrived and here I had already started with this face project and uh which means uh Facebook AI similarity search and um so uh so then we um so so we arrived there and um and so we we we we started you uh creating this uh piece of software called face uh Facebook AI similarity search [Music] um basically um uh we since since the start we said we wanted to be open source and uh uh and I did so uh the head of um of Facebook uh AI was um Facebook AI research was Janika and he was very supportive of that of that and so uh and so we started working on on face and uh so we uh uh and basically our I mean I arrived after and how they had started implementing it in C because you know um because C plus plus is uh is too complicated so uh so but the people in production they were telling us uh okay uh I mean it's already complicated we don't want to have this and see you so so I rewatched it in C plus plus and I I took over uh the as as the lead developer of face pretty quickly at the time there was um uh there was a um everybody was using Lua as the scripting language and so there was a scripting language Bridge uh with uh face that remained internal until uh 2017 I think um so so yeah I mean I my personal taste is that Lua is really crappy language it's and so I was pretty happy when uh so I I did python into face for face quite quickly and using Swig and um and I kind of and in the end we when everybody switched over from Lua to python when pytorch was created we kind of forgot about the Lua the Lua interface which is a good riddance and um uh and so and one important uh aspect of this is that there was an engineer at uh at Fair Jeff Johnson he's one of the oldest members of um of fair in terms of uh you know number of years at Facebook and uh and he got this Library caught his interest and he decided to do a GPU version of it so GPU meaning Nvidia GPU because I mean that's kind of the standards uh at Facebook and he started developing this and um I think it was a it was a pretty interesting project for him because uh because there were several aspects or the algorithms were not not out of Out Of Reach in terms of optimization on gpus which sometimes sometimes happens if there's really two irregular Behavior or graph algorithms are very hard to optimize in gpus but this problem of optimizing ivfpq is was actually pretty um pretty uh pretty reachable for uh for for gpus and it so he made a very efficient GPU method and actually it turns out that we decided to publish a paper about face and the it was clear that the flag the flagship property that we want to showcase for face was the the GPU the GPU implementation um and so so yeah that that was in 2016 uh then 2017 started we started to negotiate when we would actually be allowed so there were two aspects to this the first one was internal adoption so there was a lot of work that that Javed was very much involved in in explaining to uh to prod people at Facebook how interesting it was to have to have this to use this library to use a conversation based methods to do similarity search and uh on the uh on the other hand there was uh there was the external impact so that means how are we going to open source it and actually it took us quite a lot of time to convince um to convince our management uh well first to uh to open source that but not so much of a problem but the real problem was to get it to open source it with uh with the MIT license and basically so this is something well open sourcing was really something I discovered when I arrived at uh at Facebook it's uh it is actually it is very hard for companies to for other companies to adopt open source software that that doesn't have a very permissive license because you know for example the GPL is not possible because it's it's it's it contaminates and so and so we when we initially open sourced the the library in with the the Creative Commons and non-commercial applications the companies we talked with told us we cannot use it and so which makes sense and so we can we went back to our management saying we want to have we want this library to have real impacts and so if we want that we need to open source it in uh with a permissive license and so this took a long time to negotiate but in the end we were allowed to open source it that way and uh and so uh basically I think that um our point was that the similarity search space was not mature yet so that at the time in 20 around 2017 it was like people were using uh flan they were using uh annoy and they were using this kind of libraries which in our um in our opinion where I mean not state-of-the-art compared to what what you already had in research for several years and so uh having a strong solid open source Library which with more or less industrial support and uh that could could have a larger impact and so um so then we uh we we were allowed to open source it with the so we got the the the proper license that we wanted and um and basically then then that I mean that ball was rolling and um so after that the the history of face was um several stages of additions of several methods so uh the first one being hnsw so it became uh when the when that work uh came out it became pretty clear that it covered a kind of a space of operating points where we were really far from the state of the art so uh so we implemented or implemented hnsw into face um then there was and um so uh there was a H so there was this method so the so this is interesting one one uh one uh so when you look at how uh IVF PQ Works uh so you have the problems of the Cross quantization and the second part is how to optimize the scanning of the inverted lists which is the second part of the cost and the second part of the costume basically it's the cost is dominated by uh by the memory lookups into the lookup tables so you have lookup tables you do lookups and the problem is that that modern processes are not at all efficient for lookups they are efficient for arithmetic throughput but not for lookups and so um so what we did so it's not what we did but uh there has been a line of research around storing those lookups in simd registers and um and so there's early work by uh by people from Technicolor um I'm thinking of a guy named Andre a and um they basically explored this this direction and uh but the the industrial application of this was the the scan uh algorithm or the scan Library uh which was recently open sourced by Google and basically they have a very very cleverly optimized um ivfpq implementation where the whether the lookups or the lookup tables are started matches this and well the whole process of uh of computing distances is very well optimized and uh so they they had very good operating points and so uh and so we also imparted this into uh into uh into phase so that's uh that's a bit uh of what we did and I think in terms of so let to come back to uh to uh to the quantization which is uh one of the the main topics uh uh of of face and also of uh of what we were discussing uh so the quantization currently what we're looking into is uh so we have the product quantization but um uh actually it's there there are quantization methods that get better accuracy uh because uh product quantization has this restriction that each sub Vector is encoded separately and so uh so we lose the kind of uh statistical dependence between the sub vectors even if we do an a rotation so that this dependence is minimized um and so what we are looking very much into here currently is additive quantization methods and the additive quantization means that instead of having sub vectors and you concatenate sub vectors you have you have a lookup tables that span the whole Vector to encode and but you have several of the those lookup tables and you pick one vector from each of the lookup tables and you just sum them up so and then you you have to encode only the the ID of the vector that you picked in each of the lookup tables so it can be seen as a generalization of PQ because if the lookup tables are zero outside of a sub Vector if each lookup table is zero outside of sub Vector then it boils down to doing PQ um it's a generalization so it has the potential to be more accurate and the problem is uh what it it's much more complex to train the lookout tables and to do the encoding the encoding is uh it's not like just finding the nearest neighbors it's um it's a it's a combinatorial optimization problem uh that that is NP hired if you if you want to solve it exactly and so you solve it only by approximations and so uh and so basically we are looking into how to do those uh approximations efficiently there are several uh several directions for this and two of them are implemented in Phase the first one is uh local search quantization which is uh um which is a work via from the PHD of Julieta Martinez and uh that uh that offers a good trade-off between uh between the accuracy and encoding speed uh knowing that uh that encoding speed cannot be as good as that of PQ but it's still it's based on the simulated annealing uh from with random industrialization and it kind of converges into um into a relatively good additive additive conversation method and the second one is just residual quantizers so that residual quantizes means you use the you use the first level quantizer and then you keep the residual with respect to the vector that you want to encode and that gets you a second level quantizer and then you encode it with a second level quantizer etc etc um and but if you do that in a greedy fashion it's not good and so in order to avoid to do this in a greedy fashion you'd use beam search and so which is expensive so again there's a trade-off in terms of of speed versus accuracy yeah so that's um that's a bit what we what we are currently working on uh in Phase uh So currently um just to say So within uh within uh meta the the team the core team that works around face is uh is about five people uh not everyone is working full-time on this um but it's that's that's about the the skill of the efforts at Facebook it's super cool yeah that was a brilliant tour of product quantization so much knowledge um I'm also really excited in this VBA podcast to welcome Abdel Rodriguez to the webia podcast uh Abdel is working on this kind of product quantization weavier uh Abdul could you tell us about kind of where we're at with the product quantization and any questions you have for Matisse so thanks Connor and thanks uh Matthias for for the nice history and introduction so I I actually we are a very in the very beginning of the product quantization part now because we we have been trying to improve the indexing algorithm and and trying to make it scalable on when when we have more data well we have we will have more requirements and and we we have to deal with it and we are currently experimenting with having some information on disk some information on memory and and the part that we need in memory of course is some representation of the vectors and uh we are currently playing a bit with the with the a compression of these vectors and and again we are we are scratching the surface here now we we have a very uh very simple implementation of uh of PQ currently with k-means and and segmenting the the vectors and we would like to explore a bit the optimized product quantization the optimized PQ next and one thing we we have is one problem we have is that normally we don't build uh so we we don't have all the information and we build an index in but but we we we normally build incrementally our our indexing which means we need some some algorithms that could take over this uh capacity to to add new vectors or delete vectors that you have instead of just building everything together and of course k-means could somehow be incrementally updated if you keep at least the number of clusters which is uh something that we have but I'm also wondering about in the case of up the optim opq this rotation Matrix how how how hard in terms of performance would it be to to make it also incrementally updatable things like that I I don't know if you have some experience in this direction that it would be nice to to hear a bit about it sure uh so I think it's a it's a it's an interesting and recurrent problem um uh what happens is that uh so I think that there are two things to distinguish here it's which is an increasing database database side size uh because I mean indeed you add incrementally you add more vectors uh the the other can the other thing is um is the drift in the data distribution addressed in the data distribution is in addition to adding vectors they have they have behaviors that you've never seen before and during the training phase um I think the increasing database size size is not necessarily a problem if you if you've had enough data to train from then uh to be to be very concrete a way I would Implement a database where you don't know in the beginning how big it's going to grow it's uh you accept the first 10 000 vectors you don't encode them at all you just keep them as is if you then you when you go to a million you you do some some cheap or some simple indexing say with hnsw and then when it grows beyond that you and you start to to require some type of encoding then you can start thinking of uh training a product quantizer or some some other type of quantizer but at that point you have enough training vectors to actually actually train it so so it makes sense to to go to the to go that path um uh which you I mean if you after if you have 10 000 vectors you can't uh 10 000 vectors use no I it's it's a bit too small to to even train your product quantizer so you would even want to have more um the other problem is uh is uh drifting the data distribution which uh which we observe also with some kind of applications uh one one funny anecdotes maybe is that we um uh we we have we have an indexing we observe really a lot of data drift in in images that come in that comes with that are uploaded to Facebook so I've worked together with people who index those images and you have data drifts with memes but also when there's a new Instagram filter that comes out that kind of you see a drift in the in the type of images that you get and uh and so so it's a and and the problem with that is if you if you update the training yeah the main problem with any quantizer is that if you if you update the training then uh or if you do for example online K means to adapt to some trades of the k-means then the the the the vectors that are newly encoded they are not comparable anymore with the the ones that were encoded before and so um and so it's it's not clear to me by default how uh how to how to use those updated centroids so um yeah it's a something to think of yeah so when you have the online clustering you can move the mean but then you need to recompute the centralizers at like very basic understanding the high level idea yeah it also depends I guess if you have more intensive changes in some parts you don't have to recode everything but that part's affected I would guess but mm-hmm so maybe we'll also transition to the topic of generally the wrapping the vector index around a library compared to a database so maybe edian could um explain kind of the some of the features of the database and the distinction between how you're packaging up the vector index so yeah sure this I think we've we've discussed this uh before already in in one of the podcasts but it is a recurring topic and we do see that that coming up from from users um whereas I think one of the the first and most obvious distinct differences that we see and end of charismatize I'm also super interested about your perspective on this as someone who's been working on the library because mine's a bit biased of course because I've been working on the database side uh what one that that comes up very high on the list typically is this this incremental updateability which I think is not like this can be a library versus database part but it doesn't necessarily have to be because hnsw you can use it purely from a library perspective and it is incrementally changeable uh something that needs to be trained beforehand so for example a quantizer that is maybe not as updatable so in in this case um the library versus database technical distinction doesn't so much um sort of determine of whether it's updatable or not more that within the database you tend to go for these kind of updatable cases so for us um from a database perspective typically what we say the kind of ux that we want is the one that people know from non-machine learning databases so if I just spin up in my SQL database spin up because Android database typically I should start using it and I don't necessarily know what I'm going to do with it tomorrow I might update something I might delete something I might read in between I might do that all concurrently um and then of course you have to do capacity planning there as well in some databases scale more dynamically than others but this is a big part sort of this this usage journey of using it um yeah directly basically uh um or we're using it like a like a database but there's there's uh way more so so one of the things for example that's also super important to me is the kind of durability aspect failure recovery mode so so how does it react so so for example something I think an in placement has correct me if I'm I'm wrong I think um what I typically see in libraries for persistence is snapshotting that you would build something and once it's built you would snapshot it to disk and then you could load the the snapshot um whereas in in a database such as VBA for example um the the update process itself is already persistent so if vv8 crashes I don't know let's say you import 10 million and bb8 crashes at number 7 million then you can just restart it and import 7 million and one so so this kind of incremental durability crash recovery um everything that's written is written into a writer headlock is is a big thing and then um and maybe that's also an interesting one uh with Facebook because I think there's a a separate library that I believe is not not exactly face but but built on top of face that distributes a face across multiple machines and that is also something that's very big in the in the um in yeah for vv8 or for databases in general so um yeah the whole scaling aspect is something that you get out of the box for free as well that's that's my short my short overview of the the differences but I'm very curious to hear yours as well in the type yeah sure uh so um I think that's face explicitly tries not not to go into the field of being a general purpose uh Library so there are two reasons of that the first one is that I think that face is already a very complex piece of software uh if you if you were to have to support that it would mean that the number of code lines would be multiplied by a factor three or something which is not something we we plan to plan to do the second thing also is that in general I observed that SQL databases are I don't know about vv8 but very often databases have an order of magnitude storage more than what's strictly required by the amount of data that you have in this for example I've read somewhere guidelines that if you want to set up a mySQL database then you should plan for uh five times as many disk space as what uh the the raw data I would use and this is something that's face we want to give the opportunity for people to you know use 90 of their RAM and store the an index in that and that they don't wonder where uh about too much about overheads basically and so uh and this is uh yeah it would it's it happened several times that we're operating very close to uh what's possible on a single machine and uh for this uh we we give up many functionalities like what you say uh being able to do uh ID lookups uh being able to be being able to snapshots or this kind of things and it's you know that's that's the point but definitely I wouldn't recommend face as a final production ready database system it's it's really intended to be at the core of maybe another more broad scale database system fantastic well thank you so much from tires Abdel Eddie and such an information dense podcast so interesting learning about all these things thank you so much for your time yeah thank you thank you nice yeah yeah thank you all yeah I ", "type": "Video", "name": "Matthijs Douze on Quantization and FAISS - Weaviate Podcast #29", "path": "", "link": "https://www.youtube.com/watch?v=5o1YTp1IL5o", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}