{"text": "We are super excited to publish this overview of how LangChain and Weaviate work together from Erika Cardenas! Erika explains ... \nhey everyone I'm super excited to sharethis presentation on using line chainand leviate to build awesomeapplications using large language modelslet's begin with what Lang chain is justin case you aren't familiarum so this quote is taken right fromtheir documentation and it states thatlarge language models are emerging as atransformative technology enablingdevelopers to build applications thatthey previously could not but usingthese large language models in isolationis often not enough to create a trulypowerful app the real power comes whenyou are able to combine them with othersources of computation or knowledge andin this presentation we'll really seehow using leviate helps the largelanguage model the presentation goes inthis order so first we'll start off withthe llm chains and what this does is itenables it enables us to combinemultiple inferences together so this isa step-by-step taskum so in this presentation I have anexample of asking the AI chat bot to usethe previous output as and can take thatinto consideration for its next outputnext I'll go over combined documents sowhat this is is it solves the problem ofThe Limited token length so I will sharefour techniques that are implemented inlink chain and I have a pres avisualization for each next is tool useand this connects llams to tools thatcan help execute it so for exampleconnecting it to webiate or a calculatorand again I have a a quick demonstrationon that and then for chat Vector DBum in the beginning we'll go through thedocumentation in line chain and thenwe'll end with a demo on using linechain and mediate so make sure you stickto the end as you can guess from thename sequential chains execute theirlinks in a sequential order so it takesone input slash output then it uses theoutput for the next step so let's lookat an example of sequential chains and aconversation between a bot and I beforewe get into it I want to credit theoriginator of this example their name isjaguely on GitHub and in the blog post Icredit the author so you can link rightto the GitHub repository all right let'sget on to the exampleall right so here I'm asking the botwhat type of mammal lays the biggesteggsand the response is the biggest eggslaid by any mammal belong to theelephant and as you can see in the Emojithat's a little confusing there's alittle bit of hallucination going on sowhat I'm going to do is ask it to it'slike a fact Checker so I'm gonna sayOkay given this statement how can youimprove your answer so let's seeum so here's a statement and it's theblue text that's why it is coloreddifferently so given the above outputmake a list of the assumptions you madewhen producing the above statement sohere it gave four assumptions that itused to generate the statement above sonow I'm going to ask okay so for thelist of assertions determine whether itis true or falseand you can see that in the curlybrackets and in the color text this iswhat it's using all right so let's seewhat it saysum the first one is true so the elephantis a mammal mammals lay eggs is falseand it also gave the um it also factchecked itself a little bit with mostmemos give birth to live young the thirdone eggs come in different sizes it'strue and then the fourth one is falseelephants do not lay eggs so all rightgiven thatum how would you answer the questionwhich is which mammal like the biggesteggsum in here all right and then we can seethat it has uh this question cannot beanswered because elephants do not layeggs and most mammals give birth to liveyoung and there you go it has it rightand that's kind of the sequentialchaining that I was referencing earlierof um it kind of fact checking itselfall right let's go over combineddocuments so what this does is itovercomes the limitation of the inputtoken lengthum so in line chain there are fourtechniques that are implemented and Iwill go into greater detail but at ahigh level I'll be going over stuffingmap reduce refine and map rebrink I justwant to make a note that there is notone that is better than anotherperformance wise it is just based onyour application so make the decision asyou wishokay the first technique is stuffing andthis is the simplest method where youstuff all of the related data into theprompt as context and then you pass itto the language model next we havemapreduce so what this does is itinvolves an initial prompt um so what isa golden doodle and it is used on eachchunk of data so we're going to thevector database we're grabbing therelevant documents and now we're goingtoum we're going to add in our initialpromptall rightso then a different prompt so here wehave a golden doodles and mix um we havethree different responses and now thenext prompt is to collect the answersand combine it into one sentence goesinto the large language model and boomwe have a golden doodle is a mix of agolden retriever and poodleokay next is refine this one isinteresting because it has a localmemory that is used to refine the outputum so an example of this is for thelarge language model to summarize thesearch results one by one and use thesummer summary generated so far torefine its output um so as you can seein the animation the green and bluedocument went into the large linkagemodel outputted the orange document andthen on the next round it used theorange document and the remaining searchresults to Output the answer of a goldendoodle is a mix of golden retriever andpoodle alright so mapri rank is a methodthat involves running an initial prompton each chunk of data and from here itwill assign a score based on thecertainty of its answer so here we havethree seven and five and now we're goingto rank itand then we're going to use similar tothe stuffing method we're going to putthat into the large language model toOutput theresponse next we'll be going over tooluse so this is equipping language modelswith tools like weeviate a code executeor even a calculator I am asking it towrite python code for the bubble sortalgorithmand I'm asking it to generate codeI'm passing it through the python repowhich is a code executor that isimplemented in langchain and itoutputted the response and then fromhere I'm asking the language model isbased on the output is this python codecorrectand then it says yes now we'll get intochat Vector DB so linktrain has manypre-built chains so we're going to gothrough an example of using chat VectorDB of it pointing to our Vector databaseand we'll take a deeper dive into howthis is implemented in light chainokay so now we're in the code base forthe chat Vector database we have thebase dot pi and prompts.pi all rightlet's start off with promptsall right so the template is that giventhe following conversation and afollow-up question rephrased a follow-upquestion to be a standalone question allright so we have the chat history andthe follow-up input as a question andthe Standalone question is using theprompt template and then the prompttemplate is using the following piecesof context to answer the question at theendum if you don't know the answer just saythat you don't know don't try to make upan answer and this is um helping withthe hallucination problem all right soit takes in the context and it has thequestion and then the prompt template sothis will only output four searchresults and you'll see this in the basedot Pi file and you'll see that righthere so in thisum in this file you have the two maincomponents of the uh using the chathistory and using the query to thevector database now on to the fun partof using Lane chain and leviate togetherum so on my screen you can see that thisis where we are connecting to our revateinstanceum just localhost 8080 and then inVector store this is where you'respecifying which class you want Langchain to see and then along with theclass the propertyum so I'm using the podcast search demothat Connor has created on GitHubum I can add a link to it and make surethat you star the repository but withinpod clip which is just thetranscriptions of I think almost 20podcasts it has content so that's theconversation that Conor is having withwhoever he has on the podcast this iswhere we have the QA so the chat VectorDB chain from llm We're specifying theopenai model and then we're storing thechat history so just using the stuffingmethod that we went over previouslyit's really cool that this is so simpleand it's what almost like 20 lines ofcodeum just to get this started so let's getinto it with going into the terminal andasking the large language model whatwere the features released in 117 firstlet's make sure that we are running thedemoall right so let's ask a question whatfeatureswere releasedin aviate117.all right so replication and hybridsearch as well as rough to back werereleased in 117. though reptavec isspecific to Evie and that is a coolthing with having it the large languagemodel connected to a vector database isthat it's able to grab theum context that is relevant to yourapplication so um I guess if I ask likea general large language model what isrough to back it'd be like I have noclue what that is but when we add in thecontext of the webiate podcast itunderstands it let's ask a follow-upquestion of explaining hybrid searchwhat is hybrid searchand there you go it's also able toexplain what hybrid search is and thatit was also implemented in mediate 117.um so it's just a quick demo and a fewqueries of using this but you are ableto run this on your own and I recommendcloning the repository of the podcastsearch to get started using thisI hope you enjoyed and happy chattingwith your data bye", "type": "Video", "name": "Weaviate + LangChain for LLM apps presented by Erika Cardenas", "path": "", "link": "https://www.youtube.com/watch?v=7AGj4Td5Lgw", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}