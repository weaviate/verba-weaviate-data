{"text": "Hey everyone! Thank you so much for watching the Weaviate 1.23 Release Podcast with Weaviate Co-Founder and CTO Etienne ... \nhey everyone thank you so much for watching another we8 release podcast releasing we8 1.23 with we8 co-founder and CTO Eddie and dilocker this is such an exciting release of we8 really in my view uh setting that next stage into the AI native database and this uh customizing the vector database per tenant and this is a massive production feature and Eddie just really going to explain these details of how all this came together so as with all of our weev release podcast we have chapters that go through each of the new features so if you want to skip ahead to the thing that's most relevant to you than by all means we have lazy Shard loading a flat index with binary quantization autoq default segment for PQ and then autor resource limiting generative any scale and then an update to the nodes uh endpoint API so Eddie and firstly thank you so much for joining the podcast thanks for having me it's been a while also super excited as he said one that 23 is is an amazing release with lots of production Readiness features and that's what it's all about yeah amazing so I think um in this theme of kind of maybe yeah I think our last podcast was the release podcast was the 119 podcast and we did our inperson podcast in San Francisco that unfortunately wasn't recorded but uh so if we could maybe even set the stage before we dive into lazy Shard loading with um this multi-tenancy native multi-tenancy in we8 uh tenants as a first class citizen the design around um shards and if you could just kind of tell this whole story yeah uh so tenants as a first class citizen is the perfect summary of of what the feature really is because when we deigned it one thing that we wanted to make make sure is that we we don't just make a tenant so a tenant in this case is a user grouping of data so if you have your own app and you have users or user groups you don't want to search or you don't want your users to search through vectors of another user so the example that I use that I used in a in a a previous talk that I really like is if you have this sort of chat with your hard drive kind of example like a mixture of chat GPT and and dropbo you would want to chat with your own data but you wouldn't want someone to chat with your data so you have this this kind of separation and you could do this without any Vector database specific features or I guess filters also specific feature but without any multi- tendency specific features by just setting a field and then filtering for this field so you could say user one two three and then you would set where user equals one 23 but the problem with this is is that everything is tied together if this grows you have this massive single index and and it's it's very hard to either move stuff around or or be sort of more more Dynamic and what we're seeing right now all these new features they build on this ability to to have this separated and have this Dynamic and and um more precisely what that means is every single tenant is a self-contained unit in vb8 and because it's a self-contained unit you can do whatever you want with it basically you could say you could deactivate it activate and deactivate it you could load it lazily we'll get to that in a in a second but yeah it's kind of this Foundation of really making this no matter how many tenants there are each tenant is self-contained and that enables a lot of other things yeah amazing I I that talk you gave at the AI conference which will be linked in the description of this video as well for people listening it it I thought it was so inspiring the way that you described how you reduce the memory overhead and then I love these directions for the future you outlined about the the the active and inactive tenants and how you think about managing memory and this kind of thing and so maybe if I could ask one more question before we dive into lazy Shard loading is um so uh I took away from your talk at the AI conference that the key difference between U say using just collections as tenants where each class is you know tenant one tenant two and and now the new native support is this um change from consistent ring hashing to uh having a lookup list and that and my understanding is that significantly reduces the memory is that a correct understanding yeah it makes it much simpler memory is a side effect like the memory reduction is a side effect but it's just this this onetoone mapping basically so and this is this is the same thing as um with using a large index and filtering it down you could still end up with data that spread across different nodes and and with the the multi-tenancy this is much simpler this is essentially a onetoone mapping and then one tenant would be uh sort of located specifically on one node or if you have replication on multiple nodes but it would be a sort of very easy cons I don't want to say consistent because the alternative is consistent hashing but it's still consistent in the sense of like it's it's a lookup table you just put in the tenant name and VBA tells you yes this tenant is scheduled on this particular node and that's very very simple yeah super cool so so I've seen uh without giving too many details on the podcast but I've seen you've been doing some like crazy load tests of like how many tenants can you put into we8 how many um you know and then vectors in each of them and it's also interesting so is is this how the kind of the lazy Shard loading like how exactly you you start up we8 and could you dive a little further into what's new with this yeah yeah yeah well we'll get to the low test when we talk about the the flat index because I think that's Al a crucial component of of the lotus like we couldn't have done it without that new featur slide slide teaser for the next section but on on lazy chart loading so the idea is very simple because we have these individual tenants that means we could have could end up with a lot of them so for example I think in a previous block post we said something like 50,000 or so is that the most that you could have on a single note but then you could add more notes and and scale it linearly uh but with with 50,000 what that means is if you start up a note and of course in a in a production environment or in a container environment startups are very frequently because they could happen because you you updated something or just because the the infrastructure changed the underlying infrastructure changed or because um yeah I don't know something crashed or something so so startups are a pretty frequent thing and with 50,000 tenants you would have to sequentially load 50,000 tenant and this is typically disc bound P they have to be be loaded from from dis not everything has to be loaded into memory but still you need need to go go through them so what we would see is with a 50,000 uh node or sorry not a 50,000 node a 50,000 tenant node startup would all of a sudden take three minutes or five minutes or seven minutes and that's a very long time because that's that essentially blocks you from from using Med so one way around that would be with replication and with replication another note that's currently not starting up could take the traffic but still if you replace three notes and they each take 7 minutes that's already a 20 minutes or event and now imagine 100 or a thousand no clusters of that that just doesn't scale whatsoever so lazy shart loading is the idea that rather than loading everything up front why not just load it on demand and um the this is built in a way that it has zero user impact or almost zero user impact the idea is at startup we just startup right away and vb8 is ready it's reporting as as ready and then if a request comes in for a tenant and here we're again sort of making use of the fact that not every tenant is active at the same time right like immediately after startup yes even if there's lots of traffic traffic you might get requests for for multiple tenants but it's very unlikely that you would get a request for every single tenant on that machine in one second after after starting so whatever the the the startup traffic pattern is every chard or every tenant because we have that 10 and to sh mapping of one to one would just be loaded on demand whenever the request comes in and the nice thing is that the the sort of penalty for that is that the time penalty latency penalty is very very small you almost don't notice it at all and then in the background we're still loading up those tenants sort of asynchronously so five minutes or so after after it started you'd still be where you were but if a request came in before it would just be served and you'd be n the wiser and all of a sudden your startup time is now 2 seconds rather than 5 minutes and it feels exactly the same so it's a from an engineer perspective it's a very very simple change just lazy loading but the user impact is massive for for these large scale production clusters so so our own sres and everyone who operates vva hopefully no gting yes this is this was my paino like startup times and now know that should be fixed for for multi- tendency cases it's so cool like the whole sophistication of it with say the asynchronous indexing in the last release and now this kind of lazy Shard loading it just sort of like this active inactive tenant thing maybe prioritizing which tenants are loaded first and this lazy Shard loading it it all inspires me thinking about this kind of like self-driving database and just like you know how much of your data management is orchestrated by these systems and I think this is just a perfect transition to our next key topic which is the uh the flat index and binary quantization and maybe maybe before we even describe it just explaining like how this fits into the picture of multi- tendency which I think is just in my view I I I never like to say like two adversarial things but I think this just really sells like the production Vector database compared to kind of like the numpy as all you need argument like having this sort of when do you need hnsw when do you need just brute force and so could we dive into the story of uh the flat index and binary quantization yes so multi-tenancy was the main driver in in building this this flat index so so for for context a flat index is basically just a brood Force index it just it doesn't userve any fancy graph like hnsw which we have a couple of couple of sessions on and I think sort of listeners might go like why why if you already have this s sort of super sophisticated hnw index why are you know go going back to something that you said is slower and is is um yeah sort of more more primitive in a sense because that's that's what the RO for index is but mult multi-tenancy plays a role here because with what we're we're seeing is in a multi-tenant case the total number of vector embeddings that users have are are massive so they would reach out to us and say like hey we have 20 billion or so vectors in our in our system and then first things like oh wow that's a pretty large scale but then you ask them so how many users is that and how many Vector edings per user is that and then typically something like oh it's it's a lot of users and it's only about 10,000 or 15,000 or maybe 100,000 embeddings per per user and all of a sudden these 20 billion aren't that many vectors anymore for for what is part of one search and this is served where the the tenant isolation again um plays a key role if you only have 100,000 embeddings to search through um it doesn't really matter that there are 19 billion 999 million I don't know a lot of others doesn't matter because you're you're not searching through them you're really only searching through this the small fraction and depending on what your latency goal is this is something that's easily brute forceable and then I'll get into the the binary quantization in a second but let's for for for now let's even just ignore the binary quantization just pretend that doesn't exist let just to say we're we're brute forcing the The Continuous Vector embeddings this is now it's completely feasible because it's it's yeah it's a much smaller scale per tenant and without the multi- tendency feature this wouldn't be feasible because without the multi- tendency feature we now say okay we have all these 20 billion Vector embeddings in in a single index um and now just the work that we would have to do to narrow this down to 100,000 would be so much like for example it would be um spread all over so even if you have we have the the the Roaring bit maps and everything to tell you exactly these are the IDS that need to be matched but just imagine this this I don't know multiple terabyte block on disk and now you have a vector embedding here you have one here you have one here so you're always reading this like tiny fraction doing a dis seek and yes this aren't spinning discs anymore disc seeks are much faster but still if you're if you're seeking for every individual Vector embedding that would just be super super slow but because we have the tenant isolation is really just one index it's one index and that's optimized for for having everything um in this continuous segment and um now all of a sudden this is this is this is very fast so I need to need to dive deeper into two more things that I just touched on one is um how this is stored on on disk and then the second thing is maybe before how it's stored on dis the fact that it's stored on dis because a flat index doesn't necessarily mean that it's stored on disk it could also just be a a in memory index and there's just no no Gra for example so compared to to hnsw um but was important for us for for the flat index was there should be an option for it to be disk based because we don't want the the memory usage and this is um the the low test that you mentioned about this is something where we could easily see there which you could have very large scale setups that that require basically no memory because everything is is on disk um and then the second thing is how is this stored on disk uh and this is where our engineering team uh managed to use our existing LSM store so we we've talked a bit about think in the past about the LSM store and the hsw index and it was always the LSM store was for the inverted index and for the object storage and now we're actually using that part and using it for a vector index as well and what the LSM store is extremely good at is handling deletes so this index does not degrade whatsoever with deletes because in an LSM store delete is just an append so it's basically a new segment and then over time the ments would be merged and specifically on the on the flat index they're actually Force merged into a single segment because a single segment then is is again um very very efficient to to just um read it from from sort of beginning to end and uh and Brute Force those Vector embeddings um okay that's that was I'll give you a chance to ask some question before before we touch upon um binaryzation that also coffe yeah I I love the the diving into I'm so interested like I think kind of the high level thing for me is like the most common thing I'm seeing is people building some variation of chat with your documents some of your users have 100 docu a thousand documents others have a million and that's why you need to separate uh you know different indexing but now this this thing of reusing the LSM for The Brute Force okay so sorry so I'm I'm a little slow forgive me the so so I under I think everyone listening probably understands this concept of you have all the vectors in memory and then you can brw force them in memory but to to have them on disk and then could you kind of maybe slow it down a little bit yeah yeah it's it's in s you can think of this as a very simple thing you could think of it just having all vectors in a in a continuous file where you just say like vector embedding one would be the first so so let's use 1536 dimensionals dimensions that's um four kilobytes per Vector embedding sorry four four bytes per Vector embedding so that's six kilobytes for for one vector so you could just imagine a file on dis where the first six kilobytes or or the first Vector the next six kilobytes or the next vector and so on and then you have the sort of continuous bite stream that you just go through in one go and and discs are very efficient in just sort of starting a read at one point and going through through all the way and then where the LSM store comes in is if there was no LSM store you could just create a file like this but you would have to know all the vectors in in their order basically up front but now if say a specific Vector was deleted and a new Vector was added now all of a sudden the simple file structure would have gaps in there and it would sort of grow and then you'd have over time you'd have larger gaps and this sort of nice continuous file idea which is super efficient for thiss this would go away over time but the LSM store actually the the goal of the LSM store and and and in particular in the way that it's tuned for for this use case the goal is to always get back to that single file without gaps so even if there are deletes yes temporarily something would be appended and maybe temporarily there would be a gap or something that that would be skipped but over time it would converge into the single uh file again then basically we have a a low test maybe we can can include the the graphics as an overlay we had a a low test where we tried constantly deleting and and like millions of deletes over time and both the insert latency and the quer latency was not affected so so this is this is great also for for use cases where we have the sort of constant yeah deleting and and and and updating um as well yeah so I uh I read uh data intensive designing data intensive applications from Martin kman from John tren's recommendation which is sort of helping me catch up and that I they one of the examples is with this hashmap where you have the the bite stream and so just maybe for people listening who are kind of at my level and trying to look for something to to keep up but um so my question then is is so I have this question for you generally kind of looking at the philosophy of this kind of like self-driving databases is Andy Pavo describes I've been really obsessed with this idea where it's like uh the database is optimizing how the data is stored on disk and you know you've clearly this idea of like brute forcing within the LSM store it's already like a pretty low-level optimization and so I'm curious with things like like another example that I've also got from that book is better understanding like rowwise versus column wise layouts of data on disk and how like you know rowwise is optimized for transactional processing columnwise for analytical processing and so like what do you think about this kind of philosophy of how much the database optimizes how it stores your data on disk for your particular workload yes I think this is this is a super important um aspect of sort of making everyone's lives Easier by having a database that does this this for you because coming back to that simple file Like You could argue I don't need a database I can just create that file myself but the moment that updates and deletes come in it gets super complex and and this is one of those example where the database is self-driving in the sense that yes it it over time it will be stored in that continuous file again and the database will just do it do it for you you just query you just add new objects you you update them as you wish and the database will make sure that it stays stays efficient and as for for row based or or column based axis you can kind of think of this this root Force index as a a column based data structure because the the the vector embeddings are part of the full object and the object while the object itself would be stored row based so one key one object and the object would contain the vector the vector index itself only contains the vectors and because we sort of the the example that I gave of f the first six kilobytes being the the first Vector embedding the next six kilobytes being the next vector embeding and so on that's essentially a column based index and column based index as you said are are optimized for for analytical loads and brute forcing through that data that's basically an analytical query because there are no transactions involved it's sort of touching every bite on dis and as we said sort of yeah discs are very good at at sort of reading continuous stuff and that's exactly the same same principle of of column based index so I guess you could say that the the flat index is a a columnar index W so trying to keep I'm learning so much from this and I think this is just I think this is just one of the most the pillars of the AI native databases this kind of topic and uh so also kind of relating this to one of your kind of anchoring future directions from your AI conference talk was this um uh like like cold cloud storage and how that might relate is that also kind of related to this topic like I from your talk I understood it's like you have one tenant who you know they came and used your app and then they basically left and never came back so you know you basically throw their data away does this also relate to this optimizing the storage of data concept yes yes absolutely it is and I love sort of how all of these things how how this is all connected because that that is you can really see s if we're we're really focusing on these multi- tendency uh production cases and you get sort of even if we ignore the the cold storage for a second you still get that cold versus hot kind of pattern with anything that's that's dis based simply because what the operating system does for you if if there is memory available it will start caching the dis basically so so called the the page cache and then essentially the disc or or operating system tries to predict what is it that you're loading from this right now maybe what is it that you're you're likely to load next and it will sort of try to to yeah Cache the dis in in memory and you can really see that with with the the multi-tenancy in combination with the the flat index so if you have way more tendons that could possibly fit in the page cache because of course the page cache is limited by your memory you can see that basically every first query for your tenant is a cold query and you you can kly see this in the in the in the latencies so if you send if you pick a random tenant that has not been queried before and you send 100 queries the first query will be slightly slower than the the rest because this is the one that actually has to hit the dis but the remaining queries um unless of course you you sort of did something else to destroy your your page cach again the remaining queries if they follow immediately after they will be much faster because even though it's dis based the operating system actually says you you don't actually have to hit the dis like I have this data here and I know that it hasn't been changed so we can actually serve that from from memory so even though it's a disk based index we kind of get that that inmemory caching for free just because it's built in a way that it's that it's cash friendly and and the continuous by segments again sort of plays a role here because it's much easier for the operating system to just cach I don't know two megabytes of data starting here ending here rather than it being being um spread out and then where where Cold Storage comes in and this is an option that that's currently not not part of 1.23 yet but it's the The Logical next step is we now have these these steps of the data is on disk the data could be in page cache but still if you have millions of tenants with with terabytes of data now you would still need a very large disc so so some of the low tests we did like 10 10 terabyte discs or so where where the the dis was more expensive than than the compute um so now the next logical step is rather than um having this on disk all the time if you know the query pattern and again this is where where the self-driving database part comes in so so ideally the database just figures this out for you and you don't have to manually do anything um but then we could say that that parts of those those tenants that maybe haven't been queried in an hour or in a day or so we can just move them to cloud storage and then cloud storage is way way cheaper than than dis based storage so now we can have our diss smaller on average but but nothing is free in in engineering it's always a trade-off now the trade-off is if you actually have a cold request coming in and cold in this case doesn't just mean loading it from dis into memory Cod actually means loading it from Cold Storage so from from cloud storage now you have more more um more latency overhead so it's sort of like the question is are we optimizing for for for a hot cach basically for hot queries that are super fast even if the the first request comes in but then we need more infrastructure or are we optim iing for lower cost where it's may be okay to wait 500 milliseconds or a second or so to pull something in from cloud storage but then not just is is it way cheaper but also we're achieving the separation of storage and compute where basically yeah as as the sort of the main idea behind separation of storage and compute is you don't have to size your infrastructure for the amount of data you can just import more because it's separated and cloud storage basically scales scates infinitely that's that's kind of the Assumption and then your compute could be be constant and that makes large scale cases where storing is more common than than querying a lot cheaper yeah that was awesome I so I think yeah personally I'm going to need to study it to understand it all better but I can just kind of a man this separation of storage and compute they're talking about you know cache memory disc cloud storage it all sounds just so exciting so just I know we're spending a bit of time on this topic but I think it's just so such a powerful one that I really want to dive into it um this I'm also curious if I had this question that re somebody came up to reinvent with this problem and I've seen this paper from uh when we talked about Patrick Lewis on the podcast about uh concurrent QA where the idea is um you know Eddie and and Connor we each have our tenants and then we maybe also share some index like say we have Wikipedia is in our same we8 you know it's one of the classes compared to the tenants do you are you interested in this idea as well yeah maybe we could start with there and then I guess kind kind of I'm broadly kind of curious about like the tenant abstraction and maybe how it can be expanded into that kind of like public private case yes yes so so we we see this kind of pattern in in some of our users where public private I think is the the exact thing where where this makes a ton of sense so for example if you have say public data about public companies like financial data is about financial data about public companies for example so every every publicly company needs to release um all these statements and reports and everything per per year and this is public data so you kind of need to index it only once but then you might have your own private data either about private companies or about something else that's that's related to to that and you can easily do you can easily combine this in in a single vb8 setup by having a single tenant class which would be the the public data and the multitenant classes which would be the the private data sort of scope by by tenant and then you can even use the the cross reference feature in in a way that um what you can do is search you can link from a from a multi-tenant class to a single tenant class that's fine because that's that's sort of the access pattern that's allowed what you can't do is a link from a from a from the public to the private basically because that's the the opposite direction so yeah this is this is a pattern that we do see and um I think there there are more options this is one option so sort of this is the option where you still all have it in one vv8 instance another way could also be to do this more sort of at the at the rack level where you're saying I'm just pulling in from a different data source where you're saying like if the public data is publicly available maybe through Google or or any way to to sort of Discover it maybe we don't need to index it at all so you could say like okay I'm only storing my private private embeddings in bb8 but then if I want to enrich it with public data I basically enrich it at at query time and and sort of have the llm combine the two and and then of course like various ious tradeoffs latency um how much do you trust s of these public data sources to be available to be in the right format and these kind of things but it's a different architecture like you you you can do it in vv8 or you could do it sort of at your application Level whichever you you prefer I love it I I guess um like for the open AI Dev day something that really caught my attention was this idea that you uh chat with the you chat with chbt such that it sends internet queries to then populate your data yeah and then this idea of but you know like I I think about like it's still private like if um you know you me and let's say Erica are on a team like the three of us and then we want our conversation to be private but so it's like I see these tears and so such a great coverage of the topic at and so many exciting ideas with this moving the storage around and then I think just to come back to the original thing this yeah yeah we haven't talked about binary quantization yet yes so let's let's dive into the the latest updates binary quantization okay cool yeah so so binary quantization is something that works so so currently for for for one23 binary quantization is only supported in combination with the the flat index but over time that will be also supported with with other indexes but binary quantization is a specific way of of quantizing that works really really well with the with the flat index because essentially what binary quantization does and this is this is sort of some of you some of you listeners might now think like way that's that's too simple can possibly work but it actually does all it does is it takes this this um this Vector embedding let's say we again have a 1536 dimensional vector embeddings and it completely throws away the actual numbers and only takes the sign so if your vector is plus 0.5 minus 0.3 plus 1 point something the only the only information that we would keep is plus minus plus so now for for every single dimension of the vector we have just two options and that's basically what a bit is and that's where where the name binary quantization comes from so all of a sudden we're taking 1536 uh of four byte so so that's 32 bits per per original continuous float vector and we're representing every single Dimension with a single bit so it's a 32 times reduction and it it it works really well with uh embeddings that have a large dimensionality such as a 1536 so the smaller and this makes sense right like if you you have a just a two-dimensional Vector embed embedding the sign like there's there's only this many combinations of of signs but if you have a 1536 dimensional Vector embedding all of a sudden just these combinations of signs they they retain enough information and then as with all quantization options you can always still rescore right so so the quantization the approximate trying to find the the say top 100 or so results but they're actually not in the exact order because we don't know the exact Order anymore and then you rescore them um for for those 100 and this comes in super handy for for the flat index because for the flat index as we said before what we're doing is sort of continuously reading data from disk and in our previous example we had these six kilobyte blocks so let's say we have a thousand of them then that's six megabytes to to read through now if we can reduce the amount of data using using binary quantization by factor of 32 all of a sudden this is just roughly 200 kilobytes or I'm not sure if the calculations is correct but it's like 32 times reduction and reading through 200 kilobytes is of course way faster than reading through 600 megabytes just because your your dis has um sort of a maximum throughput let's say it would be uh a th000 megabytes per per second or so then yeah so reading just 200 kilobytes is way faster than reading a six six megabytes and um then of course we can still do the the the rescoring which is again sort of hitting uh the dis in a in a slightly inefficient way because now we're just sort of reading 100 individual vectors but we're reading a 100 of them so if you have say a million Vector embeddings per per tenant um and now you're you're doing the brute force on the on the binary quantize vectors on on all million of them and then you're just rescoring a 100 or so then it's it's way faster so in sort of the the the super simple tldr is where given given a certain latency Target I'm not even saying a specific number but say you have a certain latency Target and with uncompressed vectors you could say brute for is 10,000 now with binary quantization because there's less data per Vector using the exact same latency Target now instead of 10,000 you can actually a br Force 100,000 or a million or so and um well I guess the I guess the exact Factor would be 32x um because that's that's the the reduction Factor so again with this this dis based um multi- tendency or or a dis based flat index in combination with multi- tendency all of a sudden now your tendons can be bigger for for the same sort of dis latency budget yeah I think just kind of yeah just kind of like recapping I think that perfect storm of those three things that I remember when you were first looking into binary passage retrieval and I remember my first reaction was wow Edan moves really fast with these kind of projects and young Connor had a lot to learn but this uh like this now we have these higher dimensional vectors that tend to like be binarized more easily I think at the time we're looking at like sentence Transformers like mini LM has 384 Dimensions so if you just binarize it I guess you lose too much information compared to the yeah and then that rescoring thing I think that was another Discovery with quantization that has had a massive impact right is how you you know bring the full Precision vectors back memory to get a better Direction and then understanding how it intersects with the um that continuous discre thing which I I'm learning about now I think from this podcast and yeah that that is really cool stuff and I think actually if we could I know we have our we told people in the beginning that we're going to go you know flat index PQ autop PQ default segment but I think it actually would transition really nicely to the default segment for PQ because um I think it relates nicely to understanding this concept of like uh you know say you have the coher vectors the V2 had 496 I think V3 has 2048 but just kind of understanding what we V8 is cooking with um uh understanding like we know the embedding model so we're looking at how well binary quantization is going to work what segments so uh could you describe the kind of default segment lengths for PQ and weeva yes so what our listeners need to understand is binary quantization is is very primitive in a sense it's not a a learn learned or algorithm is just s keeping the sign and throwing everything else away product quantization on the other hand is much more flexible product quantization actually takes your your data or a sample of your data clusters your data and then tries to basically approximate sort of shorter quantized vectors that roughly represent the the distribution in your in your data and what that means is so with with binary quantization we said it's a 32x reduction but with product quantization there is no such thing as product quantization does a reduction of a factor of of of xer rather it's configurable and again the the the embedding length plays a role here so in in product quantization the minimum reduction that you could do is a 4X reduction and this is simply because we're we're representing um a 4 byte float 32 as a single bite so even at that point it's it's always a 4X reduction but then where where the magic comes in with with larger embeddings is so so um maybe let's go for for slightly easier to to reason number um let's say we have 128 dimensions and then with the 4X reduction we would already reduce this to um to 32 sort of yeah or 4X would be sort of rather than 128 times four yeah I think that that's the that's the easier way let me okay let me go back and let me let me take the the 1536 again because then we have the the numbers that that were're already familiar before so with 1536 we said that that's six kilobytes simply because we have four bytes per Vector embeding but now with the 4X reduction it's actually just 1536 byes so 1.5 kilobytes so that's the the first reduction but now what we can do is take those we still have 1536 segments and the segments are just smaller but what we can do is we can combine multiple Dimensions into one segment so for example let's say we do this with a a factor of two then all of a sudden we would take these 1536 segments and now we'd only end up with 768 segments so now each segment represents two Dimensions so now our overall compression is 4X because of the the bite versus float and then 2x because of the the bunching up of of dimensions in the segment so now our overall compression is 8 times and um The Sweet Spot for a a 1536 dimensional Vector is somewhere between Six and8 Dimensions per segment so that gives you an overall compression rate of 24 to to 32 times and now where where the auto or or the better defaults come in is um what we would do in in previous vb8 versions is we would rely on the user to know their data distribution and know their embedding length and what the the best Dimensions per segment would be and if they didn't set anything we were very conservative and we would just go for one dimension per segment so with all defaults all you would ever get is the 4X reduction um but for for 1536 for example which is super common um you can go to six to eight times on top of that so so um now what we're doing is actually when you turn on um a product quantization and you don't specifically set a value of course if you specifically set a value we're not going to mess with that or trust you to set the right value but if you're not setting a a value we actually use more reasonable defaults giving given the the dimensionality so for I don't actually know what the exact numbers are but let's say for for 128 the most we could do is maybe two Dimensions per segment and then for uh 1536 we could do Six Dimensions per segment and then for these even larger ones like the the C ones you mentioned we could do eight Dimensions per segment and again sort of the the self-driving Vector database or the self-driving database VBA just makes it easier for you because you don't have to know sort of what the the right value is and and after lots of testing and lots of production experience we now are comfortable saying yes this is a reasonable default given that kind of kind of dimensionality yeah I guess it's like um yeah I think exactly what you said like we can I have seen like the test that John Tren grve has been running on like you know using the beer benchmarks data sets like dbpd and stuff like that to you know because we had know the embedding models maybe if you're if you're training your own embedding model or fine-tuning it maybe now it's like okay you got to figure out what segment's going to work for your custom embedding model and I I think like there could be this interesting orchestration where you run like uh like I've been pretty interested in the rag evaluation thing which is all these different knobs to tune and maybe you do want to have some kind of orchestrator of like okay I tested uh eight segments four segments two segments and here's what happen and and maybe that kind of thing evolves but yeah so I think this would transition perfectly into uh autop PQ and what we8 is now doing I think it flows with our whole story of understanding you know this evolution of we8 is heavily focused on production cases multi-tenancy some tenants need uh you know root Force some hssw and then PQ where does it sit in all this yeah so yeah so so um autoq is another one it's it's I think it's in the same category as as the the better default because what we made the user do in previous versions and this ties into into asyn indexing so previously indexing was synchronous and for product quantization as we just said you you it's a trained algorithm so you take a portion of your data you cluster it and then you you sort of train the the PQ codebook the PQ quantizer based on your data so you can or in in previous versions of ev8 you could not just turn on PQ when you had zero Vector embeddings imported because there was nothing to to train on so the previous workflow was import a certain amount of your data make a pause in which you turn on PQ but then because it's a learned representation off your data it takes some time so maybe 15 seconds or so so for 15 seconds your index would not be writable and then after 15 seconds now PQ would be on and now you have enough now you have your your code book basically so everything that you would import from that point on would then automatically be um be quantized right away but still the the the workflow from user perspective is import a bit turn it on wait a bit import the rest and it would be way easier way nicer if you could just say PQ yes and that's it you just import it and that's exactly what autop PQ is and it it it it makes use of the uh async indexing feature that we released in one22 and now because this is async under the hood it's still the same thing under the hood we still there there's no way around sort of starting the the product quantization only at the point once you have enough vectors that that represent your your data but from a user perspective why why would you have to to worry about it right now from a user's perspective you do exactly that you just import all your vector embeddings and then under the hood it would it would sort of not quantize them until a certain threshold is reached I Believe by by default it's 100,000 Vector embeddings once they're reach it will start uh training the the PQ compression in that time while it's training um all the the new vectors that you're adding they're just added to a temporary q and then once the compression is finished it's taken sort of from the queue and imported as as um as quanti Spector so over time you will just end up with a perfectly sort of compressed Vector embedding space and vva use the right moment in time to actually start training the quantizer and start doing the quantization so again fits perfectly well with the the self-driving data base yeah yeah I think that's been the the theme of A lot of these features is the self-driving the the more orchestration of the vectors and and yeah so yeah I think that whole topic is just so powerful understanding the async indexing and then how PQ is going to fit the code book in the background loading in the enough vectors to get the right distribution this may be also an interesting discussion around like distribution shift like your latest 100,000 vectors now are some different distribution now the compression doesn't work and I think it this this kind of building block just opens the door for us to explore that kind of thing and but so I think also transitioning another another like default in we now is this um autor resource limiting and I think this is one of those I've asked you about the garbage collector before on the the podcast and this is one where I'll probably just not along as you explain it but so what's the new thing with go me limit and this release is really packed full of features I've already forgot about this one so yeah um this is a very a very simple win but a very important win for for production so we we've talked that in past about about I think around don't remember when it was it was in in go 1.19 but don't know what what VBA version it was when we introduced the the go mem limit flag which basically tells the go runtime this is the the memory that you have available and S of you can you can make use of that memory but never go above that threshold because then you could risk your your BBN instance being killed and that was a that was a production Life Saver basically because if you set that value correctly now all of a sudden I mean you could still run out of memory if you if you did if you did just did something like I don't know provision a machine that was just too small but at least you couldn't run out of memory anymore just because of misconfiguration or or or rather just because that the runtime sort of assumed that the machine was larger than it actually is but the downside was that you would still have to set that variable and this is error prone you could for example you could change your infrastructure could resize your cluster and all of a sudden now it has if you if you size it up it could have more memory or if you size it down it could would have less memory um and you would always have to make sure that the variable is set correctly but in a at least in a in a sort of containerized production environment this information is actually present in the environment because you have these cgroup limits that that basically tell the operating system or or the container uh containerization orchestrator this is the the limit for this particular container so all that the auto mem limit setting does is it reads that value from the environment and if you're not sing that explicitly it it just takes that value so that makes it harder to misconfigure vv8 based on on this one setting and I think there's a there's a second one which is the the auto max proc setting which I I won't go into too much detail but basically this is this tells the tells the runtime how many CPUs are available this is on a shared kubernetes cluster for example you could have a a physical a note that has say 48 CPUs but because you're running other stuff on there as well you're only giving um vb8 say six of them then um it's just way more efficient if the runtime knows that it only has six available rather than sort of having 48 available but be limited to only use a total of of of six so that's another one of those settings where we just sort of optimize this a bit and and take the value from the environment automatically rather than having the the user or the operator set it manually H yeah it's also it's all uh I guess for me my perspective on it all it's like it's joining the vector database company has been learning way more about computers than I was maybe originally planning with going into machine [Music] learning I remember Abdel when he was building the autoq and and he was debugging um the async indexing with the codebook fitting and showing this like Heap allocation with the garbage collector and it was just yeah all of it just a ton of engineering goes into WEA and it's all super cool to learn about um so I think um before I think our concluding topic will be this kind of llms a little more higher level topic um if we could also cover the nodes endpoint update and um so I understand that's also related to multi-tenancy and just kind of uh reducing what you get with the nodes API because when you have these you know millions of tenants you know this massive uh res the response for the nodes is can maybe talk a little bit about what's new in the nodes API yeah yeah so that is that is it's a very simple change but it it makes it a bit more user friendly because as you said in in previous versions it would sort of list everything that's currently scheduled on one particular note so in our previous example we said um something like 50,000 tenants per per note so now if you have a 100 note cluster with 50,000 tenants each all of a sudden you have this massive list of of um information that you might not care about um so what it does is by default it re reduces the verbosity of this uh endpoint a bit and it just gives you the summary and if you do want the details you can still set a flag to to actually show all the details this makes it I think we had one one test case where all of a sudden the simple endpoint just returned 70 megabytes or Json or something it was like okay no that's that's too much let's reduce this by default and know it's just this very compact Json construct and if you if you do need the detailed info you can set a flag and still get yeah super cool awesome so so this anchoring topic I think is the the open source large language models I think the current state is a lot of people using we8 are building retrieval augmented generation or Ed the vector database to provide context for the large language models and we're recording this podcast at just such an exciting time for open source language models mistol has published their their mixture VOR model and and they did it by publishing like a tweet that has a torent link to download the weights and it was like a really clever marketing move and um so our generative any scale module is now plugging in uh the the Llama series of llama 70b 13B 7B as well as um um mistol 7B and the code llama 34b and uh so quickly before diving further into any scale Eddie and I just want to get your perspectives on like uh open source large language models and and how that could shake everything up yeah I think it's it's such an an importance of being being part of the the open source landscape and and having both open source and open source models of course there's there's sort of you're you're in in the open source camp in you want you want that that research that's happening behind closed doors also to make it to the the public and and just more more models is better right so more more different players trying to do research and coming with some something better that's that's better for everyone but also from an operations perspective um what enables our users with privacy concerns they can host VBA completely in their own environment so in the in the we call it bring your own cloud where you can run VBA end to end in the customers VPC or even on Prem if if if they would want to but then if they use an llm that's not part of that that's just sort of sending data over the the wire to to one of the the providers that currently doesn't support um hosting that in your own tenant then kind of you have that that private in data ownership in the storage part but still you're sending everything over the wire for for the rack part or for for the llm integration part um so having open- Source models that can also be self-hosted I think is a great opportunity for for those users and I know that not everyone has these these um sort of data storage and privacy concerns but some Industries do for for some Industries there's just no way around it and having the ability to run those open source models alongside the database completely yourself end to end I think is a massive opportunity for for those who currently just can't give their data away yeah I think that's that's a massive like one of the most common questions at um at the reinvent conference was people who want to run it in Bedrock to you know run we8 also in AWS and have that sync up and yeah I think so I think any scale probably also has some option like I think any scale is quite an impressive company my my little story with this I even have this little sticker here for as I tell the story I I saw this tweet from Robert the CEO of any scale that was like you know 25 cents per one million tokens inference and I was like that's super cheap and so I you know I went over to their Booth to talk to them and I mean it's the the I think the the big thing to say here is that the the cost of LM inference is going down and that will make all the rag cheaper these complex rag agent likee pipelines cheaper generative feedback loops using the LMS to transform your data create new data all of that will be more accessible and so I think the that's the I guess there kind of two things things I want to say the um so so for me I so with any scale they offer uh fine-tuning as well and so I've personally done the test of giving it my of giving them my credit card and testing it for everyone to make sure it's okay so I I find I did the experiment of fine tuning this llama 7B uh to do the text to weeva graphql experiment I've been playing with and so then this is my next question about open source LMS and how I currently see this like uh keep AI open I'm hearing nervs and that's like one of the phrases keep AI open it is like um okay so I I don't I don't mean to get in the wrong camp with anyone here but like do I need the weights for it to be open or is like any skill they don't give me the weights but they fine-tune it for me and they give me a cheap option to serve inference and so for me it's just I'm thinking about how open does it need like like I don't even agree with the like I think open AI is open because he get to use it and so you know I what are your thoughts on the this kind of thing yeah that's a that's a great almost a philosophical question of what what does it mean to be be open and yeah I didn't really think about this before but that's a good part like is it is it the ways is it the training data maybe even is it sort of the full reproducibility end to end or is it more the hey you get to use it for free and and I think in in sort of just in in software in general there's been this open core versus real open source where like the the permissiveness of of the license would play a role and I think this kind of gets into into that camp as well where where people are either sort of yeah saying like hey this is technically open but it doesn't meet my definition of what I wanted to to to be open I tend to be super pragmatic on these things I just love that that there's the sort of healthy level of competition between providers and some some yes some are are M closed but if they sort of push the stateof the-art and then others are yeah maybe doing the same thing in a more open way and then as you said cost reduction is is a big one so I think for if we had only a single or maybe two or so providers who would do that all behind closed doors they could just set the prices whatever they want but now if we have open source or or sort of more people doing that more in the open and and obviously we're a company that does everything in the open so that's something that resonates with me but if if if they sort of do this and show a way uh where it's just cheaper to run for example that again sort of makes it better for everyone because now the the the closed players um now they need to make sure that their pricing is still competitive and I think that's that's good that's just sort of uh it advances the whole industry and advances everything and um yeah it's nice to see sort of some players that where where people go like oh hey they they really they need to catch up like why are they so behind on on um on AI but them also recognizing it and now doing something I think everyone is better off as a result yeah yeah it's it's so amazing I I think yeah just the cost going down is probably the headline yeah and I imagine a lot of people seeing you know does the 7B llama can that you know function with my rag augmentation that you know the paper like uh Atlas as we had Patrick Lewis on the podcast that shows that with rag you can get really good question answering with these smaller models and then yeah I think in our whole theme of self-driving databases with this whole concept I think this model continual model fine-tuning thing does potentially have an enormous role to play in the story and now with any scale like uh we're you know making Pro and I'm I think from my machine learning background I'm I'm really excited about seeing fine tuning becoming more popular and you know all that kind of stuff so uh edian thank you so much for joining the podcast for the 123 uh release I think this one was just packed it's got the lazy Shard loading flat index of binary quanti ation autoq a default segment for PQ which plays really nicely with that autoq the auto resource limit uh the nodes endpoint update and Genera of any scale it's such a cool release thanks so much edian thanks for having me ", "type": "Video", "name": "Weaviate 1.23 Release Podcast with Etienne Dilocker!", "path": "", "link": "https://www.youtube.com/watch?v=e88O18_2wyo", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}