{"text": "Rick Lamers, CEO and Founder of Orchest.io. Orchest is a tool targeted at data scientists and this software simplifies building data ... \n[Music] hey everyone welcome to the 11th episode of the we vva podcast today i'm joined by rick lammers the ceo and founder of orcus.io orchest is such an amazing software it's one of those things when i first saw it in my day-to-day workflow i just said wow this is something that i need to be using and i'm so excited to be talking more about this and learning more about the future of this and how the idea came about and all sorts of things related to this super cool software so with that said uh to kick things off rick can you tell us about uh what is orcist hey connor thanks for having me of course happy to share more um orcas is really a tool targeted at data scientists to simplify building batch data pipelines and so what we believe to be true is that data scientists have this incredibly productive existing workflow and typically it involves rapid iteration use of jupiter notebooks and they're they're comfortable and productive in that environment and we wanted to take that workflow and bring it closer to the process of productionizing these types of scripts and notebooks to a batch runnable data pipeline with all kind of attributes one might expect a data pipeline deployment tool to have but we really had the vision to make it accessible and familiar to data scientists data analysts who are comfortable writing python and r uh and even julia or bash for that matter we're pretty language agnostic and that's what we've tried to build so if the audience is familiar with something like apache airflow they can see orcis as a similar version to that but a much uh simpler uh story around how to actually deploy it and and really a lot more accessible in terms of uh more similar to the vibe you get when you open something like jupiter lab yeah i think even if you have no experience with production and deployment if you go to orchest and you watch their introduction video you will instantly it takes maybe 30 seconds to understand how to plug in together the pipelines and i love the way that it decomposes your jupiter notebook so you know if you're a data scientist working with these jupiter notebooks and you collect these monolithic notebooks and maybe you're organizing them with like the hashtags to collapse the tabs and have that kind of organization compared to these pipelines or you can just separate out the things so what kind of things went into the design of this ui how do you see the future of ides with with data science notebooks and i love the you know the fusion between python scripts and jupyter nope and jupiter notebooks that are in the orchest layer so how did you come up with this kind of user interface design and what are your thoughts around it generally so so great question and we we really spent a lot of time thinking about the design tool and it was kind of building something we'd want to be using ourselves we'd been both working as data scientists in the field and larger teams and what we felt um was the what was needed was the flexibility to use the right tool for the job and so what we felt like was sometimes the right tool for the job was a notebook because we could really appreciate the kind of rebel oriented you know style of coding in a notebook and for particular types of tasks that kind of very rapid iteration can be very helpful especially if you're doing kind of exploratory data analysis but we also sometimes felt that once we had a very clear vision of some sort of isolated modular piece of code that is just really kind of um an abstraction or clear abstraction of certain functionality maybe in python classes that it feels very awkward to write that inside of a notebook so we wanted a tool that could blend the simplicity of scripting and notebooks and allow us to kind of flexibly piece them together in a way that requires very little changes to the way we worked because i remember looking at all kinds of either ml ops or workflow orchestration tools and they just asked so much in terms of like please change your entire world view and really buy into our framework transform your entire you know transform your entire code base into this you know application that's coded against our api and there's just so much buy-in at the code level uh into the scheduler framework that really didn't make sense to us we were like i just want a way to bundle and connect and schedule this code but i don't want to make the code like dependent on a third-party framework and really embed that into the code and i think you know apache airflow for all the the wonderful things it brings and it's really uh an innovative project that has brought many good things i think once you realize that like the the entire scheduling and data science processing code is kind of fully intermingled you can start to realize some of the downsides especially if you want to think about the deployments uh in a more a bit more flexible way where sometimes you're deploying on your local machine because you're testing but sometimes you want to deploy on a larger cloud cluster and like how do you not become intermingled with your scheduling framework too much yeah the modularity of it is incredible and i yeah i love that the minimal buy-in when you're adding this new ml ops tool i completely understand what you're saying where you have all this integration that you have to do to tie your code base into most of these existing ml ops tools so another thing i wanted to talk about and something that again just stood out to me is wow i need to have this this is so incredible is the the cloud abstraction that you've provided with oricas and things like say say you have a web scraping python script that scrapes your data from some website and you want to schedule that as a cron job where you have it run every day or every week or say you want to schedule the retraining of your models every week once a week and orcas has this beautiful integration where to access these cron job services is just like a few clicks to be scheduling it up can you tell me about overall this uh this idea of like these cloud abstractions where you simultaneously build a software platform for mobs but you also kind of abstract away the complexity of the cloud computing into the software as well yeah i i think you know you mentioned a great example scraping we you know a friend of mine was trying to buy a house and he really wanted to make sure that he got uh like quick information notices uh the moment something came available and so he he was writing these uh these clouds scraping um or these python based scraping algorithms and and and this bit of code and um the advantage that orcas then provides that if you're thinking about deploying this as kind of a regular cron scheduled job to execute you also often need to think about the dependencies you need to have installed and in particular i think he was because he was familiar with selenium he wanted to be he wanted to use selenium in the project and what orchest bundles as part of its platform because it's fully based on containerization behind the scene so not very explicitly exposed to the user you don't have to know anything about docker kubernetes but behind the scenes everything is a container it allowed him to just specify the system level dependencies even and the python dependencies of the selenium python package and he could just bundle this whole thing and deploy it very easily as a pipeline and all he needed to do is in the orca's web ui he just needed to run the bash commands he would normally use on his local computer right so it's like whatever you would normally type into your terminal to make the dependencies work basically one to one you can input that into the work is what we call environment setup script and it's it's just a bash file that gets executed and it's almost like fine-tuning an ml model but it's like fine-tuning a docker image so it's like the final steps to fine-tune your docker image uh to the dependencies that you need and therefore it becomes very easy to take your full pipeline and just run your quick setup script and then actually run your code and we can then deploy that anywhere so for him to be able to have system level dependence dependencies like selenium really made it easy to like deploy that as a bundle directly in the orchest environment without ever having to leave the browser right and i think that's really powerful that you can be at the abstraction layer of containers without actually having to worry or know about them so you get the power none of the complexity and that's really kind of in line with the main goal of the product it's extremely exciting for the adoption of the technology and yeah like you say like you don't even have to leave the browser to set up the different docker containers and integrate different scripts that use different kinds of uh dependencies and all flowing into this pipeline so kind of later on in the podcast we're going to talk about the integration with wev8 and how you uh connect these pipelines into we vvates say python script to process the web scraped documents and then put them into your webva database but first i want to talk a little bit more around these topics around ml ops and i think um so personally i'm just coming out of a project where i've developed this thing called karasbert it's a language model of keras information and i've been adding data sources with these pipelines that i just think would just fit so perfectly with orcas when i saw the initial diagram i just thought wow this is exactly what i'm doing in a much more organized way i love the look of this and so thinking about continual learning and that kind of research project as you're integrating and reusing your models and particularly these um as you add data sources and then you add new generalization tests is continual learning maybe the aspect of ml ops and then following from here i want to talk about hyper parameter tuning but is continual learning kind of maybe the the key focus of this i think continual learning is getting more attention and rightly so i i think the notion that you train your model once and that you're basically done is kind of an a notion that's no longer held by most of the ml community they they all kind of realize maybe you start out with something but sooner rather than later you're going to detect and identify the areas in which your model is not generalizing well and you're going to want to feed it examples uh that will basically fill out those holes if you will of the the generalization landscape and the way orcas thinks about this is that a lot of the retraining can easily be formulated as a as a scheduled job or job that gets triggered under a certain condition and so what we generally recommend people to do is they build a pipeline and orcas and to make it item potent so if you run it multiple times you will basically get quote-unquote the same result but if in instead of making it truly i item potent but really kind of make it refetch the latest kind of training source whatever wherever you have that stored you basically can kick off a pipeline many times whenever you feel like it's a good moment to retrain and of course you also want to get some insight into how that retraining went right and that's what where the beauty of having notebooks is part of your production pipeline uh comes in because a notebook has the element of capturing the rich data that is generated on disk in the file itself it's the jupyter notebook format it contains this output and it has a huge advantage which means that if you run this pipeline or retrain your model you can also capture with it not just the trained artifact but also all these kind of data points that you might care about like how does the what does the generalization look like or you maybe even have an updated test set because you've maybe have expanded your test samples in order to now account for these areas you discovered were underperforming so i think the the the model of batch processing goes hand in hand with continual learning of models even if your model is really just a fine tuning of a larger trained model in nlp this is often the case i think this this kind of batch model just really aligns with that reality of needing to continuously improve the quality of your predictions yeah and coming back to this idea of how it's bare bones there's so little buy-in for using the orcas framework if you want to integrate you know hug and face transformers the gina ai fine tuner or maybe you say mosaic mls composer rubrics data labeling or all these things like even weights and biases hyperparameter tuning or say determined ai hyperparameter tuning you can integrate all these things into the python scripts and they just easily plug into oricas which is another thing i just think is so exciting about this uh so can we talk a little more also about hyper parameter tuning i know some of that is uh built into orcas as well what are your thoughts around that kind of idea yeah i think you know anyone who's tried to train a model will discover that it's not so much of an exact science as much of a black art where you're really trying to identify how in what configuration is my model behaving optimally and it's not always clear a priori how that like what that looks like in terms of hyper parameter configuration one of the design goals for orcus has always been to take the best in class open source tools at the disposal to you know the data scientist or the machine learning researcher engineer and make them easily integratable in that environment so because we're at the abstraction layer of containers you can take advantage of the whole range of open source uh tools and the ecosystem available uh to do your uh your model training and you know a really cool example that i think the the especially the nlp crowd will recognize is the koki uh text-to-speech speech to text um tools that are available like its command line can just be integrated simply into orchest and that can that can even trigger like training jobs that are not even in a specific language but they're actually just like command line invocation of a tool that's available and i think this kind of agnosticity really allows you to take advantage of all the work that's going on the ecosystem and not really tie yourself into a specific language or framework and that's that's been the goal since day one is how do we empower that existing workflow but also how do we make sure we leverage uh all of the ecosystem and not just the the strict subset of python because frankly if you look at a lot of like sophisticated statistics a lot of really good work is going on in r and i think you know giving those people second rate experience would i think be really bad if you're building an orchestration tool right i think that should be at a much more generic level than a biasing a particular language yeah and i think the the integration of say in uh in weaviate as we build deep learning for search and we build search pipelines you could think of this orchestration layer is just so amazing the way that you can connect all these pipelines so usually in these search pipelines we have retrieval then we have some kind of retrieval fusion then we have say re-rank or question answering summarization supervised learning models at the end of it so in this orchestration pipeline you could add the inference features as well the routing in the pipeline connecting the scripts to each other so it's such a great abstraction for say you want to have your particular component where you're updating your embedding model or you have some other pipeline for how you update your re-ranker model and this whole thing of creating this whole end-to-end experience where it can be not just training but also inference have you thought a lot about also the pipelines for maybe say like ensemble learning and aggregating inferences i think ensembles are a great example of how real ml has converged to realizing that it's a powerful technique even though like the like the theoretical analysis of an ensemble model is super difficult because you're dealing with sometimes very foundationally different concepts of how prediction is generated so integrating that mathematically is super hard but in practice it tends to really work super well right so you might want to make use of it so if you're if you're doing kind of multi-model inference in order to get an ensemble result how do you make sure that the code ends up being same like how do you make sure that this thing still feels like something you can control i i really remember the stories of kaggle where competition winners would often often have solutions that are so complicated that the winners would not even be able to implement it for the company who was organizing the competition because it was just so complicated so what we're really trying to do in orcas is to get that modularity where a single step can be can represent the inference of a single model and you can even have you know these steps be parameterizable so you could even have a single piece of code or a single script or notebook that takes in a parameter which basically chooses the model and then you can use that same step multiple times in a single pipeline and then there's some some insanity and overview right you know exactly that this step is generating uh the result and you can have it very visually laid out and i feel like that approach of of providing structure so you at least can see at a glance of what is my pipeline doing in totality is very powerful if you're not working by yourself in particular i mean it's good to have overview for yourself but typically you have a pretty good uh representation of what you're building in your mind i'm sure all coders can relate that they can like blindly navigate the ideas they have but that's not so much true if you're working together right because you can't really look inside each other's mental models and so we have to indirectly communicate intent and i think the dag just really naturally lends itself for this right so that's why we really like um the experience that we've been getting described back to us from users of orcis they'll tell you look i remember uh inheriting a pipeline that was written in one of these abstract frameworks that allows like almost arbitrary degrees of freedom in terms of how the dag is actually generated so it can take on arbitrary complex shapes and then getting like an orcis pipeline where it's just like pretty declaratively defined it's just like this this one defined dag and i can much more easily find my way through like okay what's happening start to end and i think you know you asked about ensembles i think it's one of those examples where if you've ever touched a code base where ensembles are used you'll get that feeling of being a bit lost of like what's happening where yeah i think it's so interesting too for interacting components of systems like say you have again these search pipelines where you have several different components of it and and yeah i love the cargo competition pipelines that example like knowing about orcas now it makes it so easy for me to visualize these things that the dags as you say just thinking about plugging it into dags having the modularity and also the cloud integration coming back to that but it just makes it so easy to visualize how you would really engineer things like this and that's just something that is inspiring so much excitement with me about learning about orchest and also say things like reinforcement learning workflows where you might have some model based agent you might have your q learning you're separating all these components out maybe you have like a generative adversarial network that generates data that comes back into your supervised learning model all these components that flow through each other and this is just coming back to i think because there's such a little buy-in it's your python code it's so easy to understand how this thing connects together so kind of to come into a like these real examples you have with orchest um can you tell me more about your integration with um with the forum scraping and we v8 and creating the question answering system yeah of course so um the the really nice thing like you mentioned with orcas is that you can take an existing code base and you know you might have your work divvied up among like five notebooks you maybe have numbered them neatly and it's like one two three four five or you could have a couple of scripts that you usually execute using the command line and you parameterize them like the way to get started in org is just like you you drag that file onto the canvas and it's a step and it will get executed in its own container and you have to do almost no work to go from existing code base in a github repository you just import the repo repo as a project directly and you can get started and so one of the challenges that i wanted to use as a way to evaluate are we doing a good job with orcus is can i take like a very simple example of how to use we v8 from python for data ingest and then also for for searching with with graphql um can i can i make an orcish pipeline out of that that does this in the maze straight in the most straightforward way and um that experience honestly was was really uh like in a way it was a relief because it was like testing our internal hypotheses about like how the product should function and it turned out to work uh so that was that was a a nice experience and the end result is basically a code base that if you look at it outside of the context of orcis you could just run it and you you could basically just see all of what's happening for like the ingest and and uh querying the we v8 engine from a streamlit context so the way this pipeline works is we we allow you like i mentioned to integrate third-party services so we have a streamlit application aviate database running and then the orcis pipeline itself is in charge of fetching data from the internet uh scraping basically comments and then injecting them into the database and so the actual pieces of code are just like fully independent of work it's like streamlit is just a streamlit app it's a python file it loads itself it speaks for itself it's very simple and then the vv8 um service running is just like a very simple instantiation of the container service that we've yet provide so all of these pieces individually nothing of that really requires any kind of orcas knowledge but then the final layer that makes it all work is that you have this one pipeline file which is the orchest file and that pieces together all these things so it pieces together the container services so it has the definition for the streamlit container it has the definition of the vv8 container and it has the code inside of it to drive the streamlit app to drive the uh the the ingest into ev8 and so these pieces are basically uh glued together by orcis and the way you do that is by simply just connecting a couple of things in the graphical interface and i think like uh the the fact that that full definition is then fully versioned as part of the repository and allows someone to take this pretty complicated uh thing which is like web scraping that like actually scales ingests into a vector database then serves a an application on it on its own port that runs the streamlet uh you know code to to generate the dashboard that lets you interface with the vva database from a querying perspective like to have that has actually worked one simple project in a normal github repository that you can import as a template and just start editing and playing with i think that really shows the power of three really cool tools combined right like it's the simplicity of streamlight it's the it's the power of we v8 for both ingest and querying and then kind of like piecing it together as an orchestrator um that that didn't require to change any of those other two parts right which was the main main idea so yeah that to me was just usually exciting to be able to like see that thesis of integrating other best-in-class open source into a single tool come together that was just really fun uh honestly to build as a project yeah and we haven't even talked about like stream lit and gradio and how that brings the user interface also it as a part of these pipelines and and again just easy python code that can be wrapped into these pipelines it's just so exciting and then um so we've talked so much about the engineering details i want to kind of step up a little bit and talk about this application of uh searching through blogs and question answering through blogs do you see that kind of changing the fundamental way that we use say kind of like blogging and social media and that kind of general thing absolutely i think we have this unique opportunity today i mean everybody has been talking about big data and like so much data becoming available and like so much data being generated at every given minute but it's really true and if you think about the implication of having this much data it can actually improve fundamentally how you operate and i think now that tools are coming available to make this easier to do even with a simple example like like i made like what i built at a high level like without the technical details is it allows you to search through comments uh of someone on a forum and it's this this famous dutch forum it's called tweakers it's for people who love technology and i remember when i was uh thinking about going to grad school i was doing a lot of searching on the forum because i really wanted to understand people's experiences having gone through that and i realized that the data was publicly available because the forum was publicly accessible but the search was really honestly it was quite bad so i really didn't have a good way to get to that data and query it and so i think with the amount of data available and now powerful paradigms like vector search engines coming available to be able to actively activate that kind of knowledge which is like a huge sea of data but how do you actually tap into it i think that's the key and applications are now coming available that make this simple and i think codex you know is a great example of taking all this embedded knowledge of like how to write uh against a new api of something like writing your first flask app with and without codex is just night and day right and so that's tapping into source code i'm tapping with that example into all kinds of tacit knowledge that people have about like school and education specifically in the netherlands and so i feel like um actually being able to tap into that data um is such an exciting trend and i love seeing all these kind of like intuitive or like uh ingenuitive projects from people who find public data resources and really do something interesting and cool with it and so like concepts like open data but even just like accessibility of like reddit data form data like i think that's really really exciting your example with vector search has really opened up my thinking about what might be the future of say youtubing and making content for technical knowledge and i was thinking a lot about my youtube channel with deep learning videos and how we kind of with question answering we kind of have like two paradigms where there's extractive question answering and abstractive question answering and abstractive question answering kind of encapsulates this idea where like you turn me or say yana kilter someone who makes content into like a chat bot and now you can like talk to that chatbot and they can general generate novel responses compared to extractive question answering where you really just need to match your question with a clip in the youtube video and thinking about your example with vector search with this blog website i was thinking a lot about how you can transcribe all of the youtube videos to create this big text document and then with the question answering you can match it to a text snippet and then you know get the 30 second video and i think that might be the future of using youtube where oh my god i couldn't i couldn't agree more and this specific example is something i've been bugged out about for so long look i love podcasts but i have very limited time you know we're very busy and organized and we're like we're spending so much time working on the code and on the tool with the users so i would love to be able to tap into podcast content how do i do that without listening to all of it there's so much locked up and so what i really like is that they're in our tools that would let you do a hackathon project where you could literally take you know 50 of your favorite podcasts download all episodes do nlp to extract you know the transcripts and then to feed the transcripts into a vector search engine actually it might be my next project with we've eaten orcas but it's really really cool to be able to tap into that kind of knowledge with the effectiveness of search because without search you know um you you wouldn't be able to tap into that knowledge so you really need a way to be able to quickly look at a large volume of data and i think you know google really did a good job of doing search on the internet for web pages but the world has turned into so much more data than web pages in fact i think fewer and fewer individuals are producing web pages right like they're pre producing rich media content and it's it's so important that we actually tap into the knowledge of that type of content uh too and so yeah that just gets me super excited as you can notice yeah we had um we had alex kanan from zencaster which is the platform of recording this hollywood quality audio and uh he explained their efforts with uh with pods podcast transcription search and yeah it's just such an inspiring thing and and coming back to what you're saying with the rich media it reminds me of say jeremy howard with fast ai he made a lot of videos where he would be live streaming coding and so the live streaming coding he's going to encounter errors doing that that someone who's pre-meditating a youtube tutorial on say keras pie torch like here's how you do a data loader you're not going to see those errors you're not going to quickly solve them as much so the question answering flexibility that's enabled by these live stream coding sessions where you encounter the error and solve it in the real world and then users is put into stack overflow they're lost they need this new data source to solve this problem have you thought about like what if you try to digitize all your knowledge and create a system like this yeah i think the the potential like a lot of people gravitate towards low friction content creation which is why these live streams are such a nice medium because a lot of very smart uh people like jeremy howard they they need to be effective with their time so if they can just do a live stream where they go into uh projects and showing things or of how they're done you know already that's so much value to be given out but how do you then increase the value of such uh content that's being created i think it's by indexing it properly so so you can very quickly jump into the content i think a great example of this um where the necessity and the value of it is a simple phenomenon of youtube chapters right like you now go to videos that are long form or people commenting commenting like where are the chapters please give me the chapters and you know there's no reason why with the current state of technology we couldn't generate those chapters or markers or be intelligent about like allowing people to to jump to the relevant bit of content uh i think the jeff bezos has an interesting quote he always uh says like bet on the things that are probably not going to change right like what is something that probably nobody's gonna look back on like i'm not sure if that's gonna be something in the future and he mentioned the example of like people probably don't want to wait longer for the goods to arrive right like optimizing shipping time is probably a bet you can make for like the next 10 years i think attention and time kind of scarcity for for people especially like you know productive professionals is definitely going to be a thing in the future because people are just getting less and less time to get to all the kinds of things they could be doing there's just so much interesting stuff to work on and so being able to to to work on technology that taps into that foundational um trend where people are gonna have to do more and less time and are trying to do more and less time like i think that's really powerful and to me like these type of topics we just discussed they're like oh in that kind of bucket yeah i love that and i think yeah like what won't change like you can build this right on top of youtube there's no need to start an entirely new platform it integrates perfectly and the orchest layer of adding the web scraping from python it's it made like orcas is such a brilliant abstraction that it makes these whole this whole application domain and this whole thing that sounds like a grandiose vision it just it's so easy to visualize how it would plug together with this abstraction so coming back to orcas can you tell me about your experience with building a company your visions for the future and just yeah like what your experience has been building this company yeah happy to dive into that area i think it's it's it's one of those things where i've learned so much over the past two years i couldn't really imagine learning so much and i remember thinking at the beginning so we started a little under two years ago we started working on orchest and i remember thinking should i actually start a company now or should i maybe get a job first and maybe get a bit more experience and then maybe read a bit more about how to start a company and like lurk some more on hacker news and like get some more you know advice under my belt and i feel like at that point i realized that i should probably just go and start building the company because it feels like i'm i'm kind of hitting a plateau on the kind of knowledge that i could absorb from the sideline so to speak and so going into it i realized that was very much true it's very much true that there's only so much you can learn from observing other people doing a thing and i think it's true for many areas i think it's true for uh you know good examples for me that come to mind are mathematics and programming like you can read about it all day you can read all the programming books but unless you've been in charge of building an actual system and dealing with the problems of updating software state migration changes like all of those examples and theoretical explanations are not going to stick as much because you don't really have the kind of experiences to kind of relate that knowledge to and so i very much feel like that with the journey i've been on with starting these companies thinking about how to solve the problems that come up organically when you start a company and like how do you convince people to buy into the vision that you're uh trying to realize without having realized that vision like that requires in conversation to talk about the um you know the ideas you have in a way that makes it accessible for someone who doesn't know yet what you're trying to talk about and so vocalizing thoughts around a product idea like that's something that's so hard to learn from a book to be honest so i really um like that you know i decided to take that leap and i think i would advise if listeners are thinking about starting a company to just go and do it i mean i mean i know it sounds super cheesy and it's it's advice that's been repeated a thousand fold but i i really do think that at some point you just have to jump in and you'll start to discover all kinds of things about doing it uh and it turns out that there's also a lot of room for not doing everything perfectly so yeah that that to to me has been the reality is like you you kind of can only learn it and and grow so much from the sidelines and doing it um actually is is what actually makes you kind of learn what it's actually all about so i i really enjoy this um this new chapter of uh post graduation um company building it's been so much fun yeah i think our artificial intelligence systems can learn a lot from that lesson too where maybe uh like some kind of reinforcement learning some kind of action would help them even take that leap as well yeah yeah i think you know if you're never going to take an action you're never going to get that syncol so where's the reward right you don't know what has been the most challenging say new skill for you to learn as becoming a ceo i would hands down say thinking about how to make other people successful because computer science and software engineering and even you know theoretical research university is pretty individualistic you just have to really own the material you have to go to the lecture understand all the complex topics that are being communicated you try to absorb all of it and you try to synthesize that knowledge into uh being able to apply that knowledge to new problems and like in a way just generalize to new cases but that's like a very uh individual process it's like you and the theory right you and the knowledge and uh what i've realized uh when you start a company it's you're trying to set up a group of people uh for success and the way to do that is to figure out what's preventing them from you know operating at maximum velocity so what's holding them back and it's also about prioritizing but also figuring out prioritization in a way where you can direct people to work on the right things in a way that um uh actually works with with with the way they work so you can't just ask people to shift targets every other week right you need to also think take into account the cost of switching the the cost of realigning what you're working on so um uh and you also need to take into account what are their goals right like if they want to learn more about a particular area of programming or like how can you kind of make all these like combinations um that end up making people uh happy with the kind of work they're doing and like not uh waste their really good potential because you we we've been able to attract we've been super lucky to attract like really smart people who really know what they're doing and like i feel pretty big pressure um to not mess up you know their capacity like if i let them work on the wrong thing for months like that's on me i'm wasting their time it's not like their fault that they're working on something and it's like a two-way street right like you don't dictate like what everybody's doing it's like you talk about it with everyone involved like what do you think we should be doing what it what is your idea of like what's the highest impact thing we could be doing right now and so i think those are all new experiences that um i very much was and am challenged by and i try to do as good of a job that you can do without having a phd in that kind of stuff right yeah it sounds so interesting and definitely like something that you need to learn on the job too with the just the nuances of people's personalities and all sorts of things like that and so on this topic further of um the high impact thing and not to give away the secret but do you have like a like a thing in the say six months to x years or however the time span is that would be added to orchest that you think would be a game-changing thing we're working on something that i think is really big um and it's it's always been part of the vision but we're now actually close to realizing it and we've always felt that there needs to be a proper abstraction so that distributed systems level performance where things are fully distributed running independently and performing at the maximum capacity of of machines on which it's run not by like the the manual work of a coder that data scientists can actually take advantage of the hardware that we have at our disposal like these cloud platforms they have wonderful scaling capabilities but more often than not it's still frustrating and complicated to actually tap into it how often do you not find yourself kind of just serially saturating a single node that you're on and so we've designed orcas from the ground up because of the containerization primitives to be able to schedule over a large cluster maybe hundreds of nodes uh in a kubernetes cluster to be able to run all kinds of code in parallel and even within single pipelines being able to split up independent tasks and run them simultaneously even on heterogeneous nodes so maybe one task is very memory bound and one is very cpu bound and being able to allocate there and what we're soon to be what we're soon releasing is as part of the open source project it will be this fully distributed version where the control pane of orcas itself is fully independent from all of the scheduled tasks that are running as part of the pipelines and so this i think is really exciting we're actually going to do a live release at the upcoming data council event in austin so that's going to be hugely exciting for us and the whole team and you know that vision of making it that simple to deploy these production ready batch data pipelines on a large cluster of hundreds of nodes with the simplicity that we have in the tool today i think you know that is going to be a very exciting milestone for us yeah i completely agree i i'm thrilled to know about orcas now and i yeah that abstraction says amazing the clicking the swiping in and out of different cloud services within this pipeline abstraction already in the containers incredible so what is um so kind of like one more medical question about your general uh training so to say is how do you kind of do your information diet what kind of do you read blog posts do you read scientific papers experimentation it's a great question and i i uh i saw this around um i saw this hint being uh given by other people i think uh just recently i saw i heard it mentioned by the guy who founded appsumo who's an interesting character to follow but it's about um so there are a number of information sources but how i actually digest it is by writing down a lot i write down a lot and i think paul graham has this really cool essay as well about how if you haven't written about a subject you don't really understand it or you don't really know enough about it to be considered an expert on it because when you write you kind of have to be honest about what you actually mean right you can keep stuff pretty vague inside your head you can like keep it kind of like partially defined where you think you know but you don't really know and everybody everyone who's written a masters or pc thesis will know what it's like when you try to formalize your thoughts and then discovering gaping holes in your your understanding so in terms of information sources i try to really you know i'm a very active lurker on hacker news and sometimes responder where a lot of interesting software thoughts are being shared i think for academic knowledge on deep learning and i really like also kind of more um quote-unquote simple mathematical analyses of maybe like a support vector machine and its generalization capability so like for papers i tend to rely on like twitter feeding me uh you know the relevant and interesting articles that are up in archive um and then um i i tend to browse reddit because i really like how uh kind of brass tacks it is like people talk about very specific types of challenges that they're facing in their work and you know their work as a data scientist and like tool comparisons and that kind of information and then there are some quite good books that are being written so there's lately been a book i picked up that's about um we can maybe link it in the show notes because i don't know the full name and author but it's about basically the modern uh data pipeline uh set up and how you know to think about the differences in etl and elt and how you know all these concepts kind of map to different types of tools and technologies now so using like a very diverse collection from papers to social channels like hack news and reddit um and books i will try to distill uh the knowledge into written form that are like private notes i i don't keep them private because i don't want to share it's more that if i know that they're private i can write more freely and that it actually works more as a device for kind of processing the information and i also like to make lists so i have this website called www.alldatatools.com it's an open source list of all the data tools that i could find that are targeted at technical professionals so i i excluded like no code tools because i feel like it's a category that i'm not specifically trying to map and catalog but it's it's really kind of a way for me to also have a good kind of taxonomy and list of what's going on in the space and i like to use that as a tree to also kind of connect the knowledge that i'm gaining too so some areas of nlp some areas of more engineering topics and so you know that's just a little bit of how i try to structure my thought but i don't know what do you do oh uh well yeah i really like the idea of the list i've i've tried to take lists but i find it it can be kind of intractable like it blows up quickly when i try to just do a top-down list and i struggle to uh keep the high-level categories that map the list and keep that ontology sane sort of with deep learning research but yeah i completely agree i love writing i think writing it forces you to as you say you know be clear about what you really know and yeah i for me writing is definitely a huge source and then yeah twitter the social feeds i definitely disagree with all that and then i do think running experiments is also just a huge way to build your knowledge further as you mentioned with the experience of building a company you need to actually do it and just in general making sure you're doing it instead of just talking about it too but yeah really cool stuff nice so thank you yeah so thank you so much rick uh i thought this was such an informative podcast i'm so excited about orchest and i loved hearing about the integration with wev8 as well i think this is such an exciting thing for the future of data science and deep learning technology becoming realized into things like as we talked about the uh the future of blogging where you could have a direct question with a blog or a youtube channel and i think all these things are just really coming together in such an exciting way so thank you so much for coming on the we va podcast thanks for having me [Music] ", "type": "Video", "name": "weaviate_podcast_13__data_science_with_rick_lamers_from_orchest", "path": "", "link": "https://www.youtube.com/watch?v=QlhTJ2n_Kog", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}