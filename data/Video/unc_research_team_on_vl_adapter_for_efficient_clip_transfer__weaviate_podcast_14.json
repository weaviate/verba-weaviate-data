{"text": "Please check out VL-Adapter (CVPR 2022): https://arxiv.org/abs/2112.06825 ! Thanks for watching the Weaviate podcast! \n[Music] hey everyone i'm super excited to be presenting a new style for the we vva podcast in this podcast episode we have the authors of the paper yielding sung j min cho and professor mohit bonsal a research team from the university of north carolina at chapel hill to explain this paper vl adapter parameter efficient transfer learning for vision and language tasks and the reason this paper stood out to me as being so interesting is you see how these this bullet point right here with the highlighted green text you only need to update about four percent of the parameters of clip when you're fine-tuning it for downstream tasks so you don't have to have the cost storage overhead of fine-tuning the full 100 of a pre-trained clip model rather you only need to have four percent of the updated parameters which are added in these sparse adapter layers that are interleaved with the original model architecture so this podcast will completely get into the details of exactly how these authors set up these experiments for having these sparse layers that you use to fine-tune something like a pre-trained clip checkpoint for different vision and language downstream tasks and that's another really interesting detail about this is exploring exactly what vision and language data sets the authors were using to explore these questions this paper was accepted into cvpr2022 it's a really great read i highly recommend it and i really hope you enjoy this podcast breaking down the details of the vl adapter paper so quickly before getting into the details of the podcast i also wanted to point you to some examples of using clip within the wev8 example so in case people out there aren't familiar with this if you go to github semi technologies slash we've ate examples and please leave a star while you're at it you can see all these great examples of uh things that people have built with we've eat you see searching through podcasts searching through plant information harry potter question answering and in this quick little example i want to show you this clip multimodal text image search example so within this clip multimodal text image search you see how you can build a user interface for having your text to image search through using this pre-trained clip model so hopefully this kind of seeing this in action and seeing how you can use this will further motivate your interest in vl adapter and knowing that you could you know fine tune a clip add that fine tuned model into we v8 and then you can do this kind of text to image search and it's very exciting i think having this multi-modal image and text space i think is really something novel you see when wev8 as you're setting up your docker compose file you can go down to say clip as the model that you want to vectorize your data hey everyone thank you so much for checking out the wva podcast i'm super excited to be welcoming a research team from the university of north carolina the recent paper v l adapter is such an exciting finding with the clip model the clip contrastive language image pre-training model is such an exciting model for producing embeddings of both images as well as text and having this vision language cross model multimodal kind of crossover and it's one of the most exciting examples of this is the clip model it's built into the vva backend for something that you can vectorize your input data it's available with gina ai's clip as a service and it's been such an exciting thing to play around with generally the clip software and i'm so excited to learn about this vln vl adapter it's a strategy to only have to fine-tune about four percent of the parameters of clips so as we talk about taking this uh off-the-shelf pre-trained model and then using it in the typical kind of transfer learning flow as we're similar with things like hugging face where you take these pre-trained weights and then fine-tune them on your data set this new advancement in vl adapter is going to be huge for the you know the cost implications of doing this and trying to fine-tune the massive uh model so so first of all team thank you so much for coming on this podcast thanks very much thanks very much thanks for having us and so uh can you tell us about like the if you had to explain vl adapter sort of as fast as possible how would you uh um so view adapter is just a work that we want to adapt to the existing language model a combination of language model and visual model 2 vision language task with the minimum parameters used possible so yeah and we just want those added parameters learn visual language like representations but the rest of them still kept the same as before so yeah it kind of brings a new paradigm that how to design the vision language architecture that you don't have to find two whole parameters you just only have to add several pampers into those parameters and you can achieve everything it's yeah it could be exciting so can you tell me a little bit more about what kind of added layers you were putting in the middle of these models is it just say like a mlp dense layer in between layers six and seven or is it a say normalization layer with where you have the scale and shift parameters how exactly is the model surgery being done as you add in these new weights for the fine tuning yeah so the layers we add is called adapter layer and it we the architecture we've tried several architectures so like the typical adapter is actually just like a two uh fully connected layers and it form let the two layers form a bottleneck structure so the hidden dimensions so input and fit to the first layer and then there's a hidden dimension hidden representation and then the other layer will output the final representation and the hidden dimension the hidden representation is smaller than the input so it's a bottleneck structure and this is actually a typical structure of adapter but we also try several other variations like compactor that can use like low rank approximation to decompose the sum of the linear weights to like smaller weights to save efficiency something yeah to give a little more like detail of the adapter and who's not familiar with like the vision language pre-training so there's a recently uh first first of like a vision language bird including like a billboard blacksmith blt5 uh dealer and so on where they combined the featuring language model with some vision incorporation and and most of the previous models jointly binding both uh old entire language model and the recently uh the adapter that are from nlp uh field and they only insert the specific beat board network that is uh as i mentioned like a bottleneck structure at each transformer blocks and we also follow that uh adapter layer and explore how it works for digital equipment so i think there's kind of two things in that i quickly want to unpack and uh the first thing for yelin can you tell me so this compression bottleneck the idea of the adapter as well as the low rank factorization is say the intermediate dimension of the representation is 1024 you're gonna compress it into say 256 and then blow it back into 1024 to put it back into the model uh did you maybe like ablate that detail is is that detail important for making this work in general it it's not important but one very important signal is that the bottleneck the bottom time dimension is is better than smaller than the input dimension it can be like so like this so for example if your input dimension is 100 that usually the heat of dimension you can either work with 30 50 70 they will work they all work but if you put it as 100 or more than 100 it probably won't work so yeah so any number smaller than the input dimension will work so yeah not super important but good to know yeah i think that's an interesting detail too is the compression bottlenecks i found that that is like a good way to train neural networks whether it's auto encoders or general that kind of thing and if we could talk a little bit quickly about this strategy and transformers where you say mask out the cls token representation so it doesn't have the traditional kind of compression bottleneck that convolutional neural networks had but you have that kind of indexing and then maybe you have a compression bottleneck in that just curious if you have any thoughts on that kind of say isomorphic architecture as they call it and the kind of general use of compression bottlenecks in deep neural networks uh i actually do not have any too much intuition on why this would like the the compression bottleneck will work better because if you see in the transformer block there's a feedforward layers right and they actually use the inverted like bottom block so the hidden dimension is bigger so i think that sometimes it needs bigger but sometimes they need smaller it's maybe like confused i'm not really sure like why this would work not what has happened i think all i know is like empirically yeah adapter only works for like smaller dimension yeah i think it kind of comes back to like the categorical embeddings the way it can like blow up the intermediate representation space and say you have like um the vector quantized variational autoencoder the gans the way that they can take these big latent spaces compress them all the way into discrete bottlenecks and then blow back up the latent space i generally just think this kind of like the compression of it is so interesting generally right and um so another question i had for jamin with what you said is so there's two parts of the clip model there's the resnet image half and then there's say the bar decoder half can you tell me a little more about uh that encoder decoder of the bar architecture as well as the resnet and like the particular details for people looking to you know really get their hands dirty with clip okay sure yeah uh we use a part or any kind of like a sequel because language model in our experiment we use a fourth host part and t5 so we extend the encoder of the original language model to multi-modal encoder actual disks are following the vlt5 of paper so we concatenate the text tokens and also uh with the grid of uh clips visual features so we linearize all the visual features and we can connect with the text embeddings and feed all together to the part or t5 select encoder so it um so the bart encodes on a concatenation of what it's the original input of the text as well as the image representation and then it as it decodes you slide that auto aggressive mask right so it's like iteratively decoding with the causal mask as they say that left to right kind of structure exactly so in addition to uh adapter parameters we also learn a very uh like light i think of one fully connect layer between the clip embedding to the wires the text embedding so it maps the visual features to the like we're defining space that bar to understand super interesting so another detail of this study that i was really interested in is the particular data sets that you're using to study vision and language and maybe we could you could take me through the data sets quickly and then i could tell you the one that really stood out to me as being particularly interesting yeah so actually we focus this on we focus on like vision language text and also video language tasks and mainly focus on like question answering and captioning so for like vision language i mean image text task we try vqa visual question answering and um gqa uh also question answering data dataset and nlbr which is a visual reasoning task and cocoa captions a caption task and for video language tasks we we try like two uh video question answering tests and two video caption tasks so the former two are for uh tbqa and how to q8 and that's the later are the tvc and yc2c like caption tasks super cool so quickly before getting into the data set i previously said it was my favorite um could you tell me a little bit more about the visual question answering is it um is it mostly common sense is it like counting tasks like how many apples are in this picture or what color is the apple or what kind of questions is it being tasked with there's all kinds of questions actually actually in this so the accounting problems actually yeah they are some counting problem and there are also some [Music] like yes no questions the most of the data i guess no questions like is the person's shirt the shirt on the the man is blue or something like this kind of problem and there are also some open questions like uh maybe ask what this person doing what the man left doing something like this kind of open uh answer question yeah but most of that he has no question and some other numbers like you said the counting question and the other outcome open ended questions yeah to give a little more context about the dataset creation we chose the four tasks bqa gqa nlpr and the google caption to cover the diverse aspects of different visual language tests as you know copy caption is naturally generative task and the visual reasoning vr is a like binary classification test like a uh if the visual input and the language corresponds or not and the beginning is more like an entailment task corner like basically uh where you're given two or three images and the sentence will either entail uh be entailed by only one of the images or all the images so it's a multi-image plus text entailment reasoning test that's super interesting and it also kind of facilitates the how you have your output space right because if you just have uh zero one and intelligent not entailment or neutral like three labels with neutral also it you know it's a lot easier than having the text generation kind of supervision right with how you kind of get the signal back how well you did on doing the task yeah i think i mean that that's the good part why we used uh and sorry to interrupt you jeremy so you can continue but i'll finish this intermediate answer is that uh honor so you brought up a good point like we basically the whole idea of using vlt5 which was jaimin's icml paper last year uh was this whole motivation that can we build a sort of uh uniform model right that can handle yes no questions plus bounding box uh id selection versus captioning uh right versus entailment so can can can there be a unified uh sort of uh visual encoder plus textual encoder plus textual decoder kind of combination that can generate answers ids answers captions for all these tasks in the same model so that's why we've used clipbart and clipt5 based on jimin's previous work as the foundation of this adapter uh paper right the cvpr one we're talking about but uh in the camera ready version for cvpr you'll see on archive two that yelen and jimin added another model which is clip will which is at iclr this year and that was to show that you can also get very good parameter efficiency and close to the full fine-tuning performance even when using state-of-the-art discriminative vision and language models like clip will uh which maybe jam in uh after finishing the data set thought you can also mention the clip will part briefly thanks we talked about nlpr captioning and the other page which uh just covered like the hqa consists of like different types of questions yes no counting special understanding and so on and uh yeah i think gqa came a little after the vqa which uh uh tries to address the competitionality of the visual language uh tasks so i i guess the questions are uh generated by rule-based compositions yeah and that's my favorite data step thing the compositional generalization thing is has to be the most interesting well it could it could be a pick for your most interesting thing going on right now uh so what do you think about the nlvr data set and the way that you can you know really isolate this idea of discrete atoms that define the environments and then test novel compositions of the discrete atoms and this kind of symbolic logic of how you put together the environments to generate the vision language task i think conor is asking like if you have some more thoughts on gqa and its compositional challenges yes sorry about that no worries as you mentioned like uh uh i believe such a cooperational data dataset it's really hard to solve with the like really recent trend of uh like increasing the model size and parameters because uh that compositional questions of uh like recent like large models uh oh so sorry like a couple of questions are hard to tackle where there's a like data shortcut in the shortcut in the reasoning process so if there's a statistical like biases in the data a model will uh affect the questions based on the um like for example uh if there's a few questions like how many drafts are in the images and they're only the images well one job in the whole data set and the model will not count the draft but it will directly memorize the relationship between the giraffe and the answer one so it just uh yeah blindfoldedly answers zero so we might need a fundamentally different uh like data set or bottle approach to tackle those such like compositional questions do you think that's related to say it's over fit to the background or the texture of the you know the clothing or whatever the thing is do you think you can have say data augmentation that kind of disentangles the correlation from the causation of what is supposed to be causing the prediction of you know bird deer truck and these kind of datasets do you think that kind of like causal inference is that is is a way to think about improving that and do you think data augmentation is maybe you know a promising interface for adding that kind of causal bias to images particularly uh i yeah i think those are promising direction i'm not sure they're the only directions like uh that both both the call like inference uh causal modeling and the data accommodation have various like uh things are similar where they try to smooth out the like the statistical dish the restriction holes so but if we try to tackle the real-world data set there's much much more aspects and combinations to cover like we need to combine the different backgrounds and different objects and different types of questions different kinds of answers and all these combinations can easily go to exponential yeah also yeah so i think yeah we need to figure out how to make this data augmentation more much more efficiently and it reminds me of another paper i read from professor bonsol's lab is env edit it's a way to randomize the domains for vision and language navigation and so it's kind of like this idea you've just described the automatic domain randomization it was used with openai's rubik's cube the way that they you know they had their physics simulator and they also had visual conditions and so they randomized all the environment parameters and tried to cover the whole distribution that kind of philosophy that you know we can use data augmentation or we can construct a massive data set such that we cover all the distribution is do you think that idea is you know looking still promising or like something that is feasible uh it might be an unpopular opinion but i believe the sleep like making the more like a great uh simulator is a way to go for learning more uh a realistic data set of course it requires a like really great deal like 3d engineers but uh to deal with the real world we cannot really collect all possible images and and not only because of the data set creation but it also uh deals with copyrights and by and ethical issues so yeah i i strongly uh want to like uh there's a good game ai engine for ai yeah i think that's a fantastic direction as well the like the unity environments for for vision and language and and the compositional questions of vision language particularly because of how you can uh control that kind of thing and so so generally on this topic of multimodal learning is um is this something video i can briefly mention also something about embedded since you brought it up so yeah so the idea there is more about uh sort of robustness and also making sure that the model is not overfitting or with some strange background noise next to someone's microphone yeah so basically uh the idea there is more that you have to generalize to unseen rooms uh right so can can the model sort of uh or the agent also do navigation in environments and objects and rooms and surroundings that it's not seen before so the idea there from data augmentation or editing the environments is basically to make sure that the model is not over fitting to things that it's already seen right and it has sort of more regularization from that perspective uh and hence creating different styles of the objects or changing the objects themselves uh and also the style of the environments right these things can make sure like you said that the model is not latching on to certain sort of these uh shortcut sort of features right because now it cannot like it has to work in all these different variants too so that's sort of like a different angle that that kind of work would need but in terms of learning compositionality through data augmentation i sort of agree with jimin that it's not that exciting in some sense to learn all possible combinations right and i mean in some sense it's a nice sort of balance between inductive learning and deductive learning right uh like is it like showing a lot of examples and learning the rule from that versus being given a rule and generating examples from that so so yeah so there's a lot of work in compositionality in pure nlp tasks and papers too but the nice example of this simulation engine that jaimin was referring to you can see in his uh dally eval a paper that i don't know if you've followed but with all this recent sort of uh discussion of openai's new dali model we had this one of the first sort of text to image generation evaluation uh works which basically used uh jimin's uh end of his simulator based evaluation paint skill data set that has very fine grained control on what kinds of spatial relationships and counting and compositionality challenges to create in a data set such that you can actually test whether daly is learning those specific relationships or is it just uh sort of creating an average of everything that it's seen in the training data like can it understand uh the chair left of the table versus the table uh right versus the chair right of the table and so on like all these different combinations yeah like a blue sphere on top of a red cube yeah he's crazy yeah i love that general thing of um like generalization testing robustness testing is a particular kind of it a domain shift transfer and kind of like this thing of studying these pre-trained models like on the vva podcast here we are about building these vector search engines where we put these pre-trained models as embedding providers and then build them into database indexes and then provide database services so like we love this topic of kind of studying these pre-trained models do you think generally that's kind of an how do you think about that research direction where say for your new paper you know you're not even going to train a model you're just going to take something off the shelf and then just design a suite of tests for it yeah i think that's where the this current sort of cvpr paper we are discussing right here adapter makes uh some good sense because as we start working or being presented with all these bigger and bigger models uh and like you said now you want to use those models but for a different downstream task you do not want to and also most of the time will not be able to afford to uh sort of retrain or fully fine tune the whole huge model uh on your new task so that's where i think this is the future right where with just four percent one percent kind of parameter updates uh can you achieve almost the same performance as uh full fine tuning uh and for more and more complex tasks that involve videos and language and images and so on uh so to me that's sort of the exciting connection uh between this paper and uh what you said and then from the evaluation perspective yeah like as we were discussing uh daily eval i mean that's sort of like a slightly different angle where instead of making uh these models sort of just bigger and bigger uh and also being excited about how they look visually uh this would be a more quantitative way to actually measure whether things got better at certain reasoning skills of spatial relations or counting or compositionality or biases uh right so so it makes things more quantitative and also be able to compare to previous models and iterations and i mean eventually it would be even more exciting and fruitful to put these evaluations into the model uh training optimization process right so that while improving the model while optimizing the model you are trying to improve these metrics right as part of the optimization directly but obviously that will become extremely computationally heavy itself uh which is again where things like parameter efficient methods uh come into picture okay that might be like one way to sort of uh combine all of these things and and close the loop yeah that's a super uh interesting picture you paint like very complete with the idea of having you have the sparse fine-tuning it's cost-effective and then you have all these generalization tests that you can do during training and maybe open source tools like waste and biases might be something that pops into people's minds is something that's creating these like massive log reports you've seen a lot of papers and open source code like maybe checklist and nlp but yeah this idea of having all sorts of logging and generalization testing and exploring all kinds of different kinds of behavior so kind of wrapping up the podcast our general goal with this one was to really hit the points and try to get the you know all the technical information as fast as possible kind of maybe wrapping up uh maybe want to talk about like future plans or sort of that general topic of the code and what you plan on due to doing to take this kind of discovery further yeah my brief version and then uh willian and jamin add more uh like i said i think sort of my previous answer connects to this too uh that hopefully being able to uh scale uh this parameter efficiency uh to newer bigger models but also especially in scenarios where you need uh maybe these kinds of evaluations as sort of part of the model in the loop um so so so that would be an interesting uh future direction uh and also i think elin will probably mention about some future directions of vl adapter because the cbpr version uh focused more on multi-modal vision and language models but the adapters were still more effective on the language side so there's still a lot of work left to be done in the community on how to make the adapters also more effective on the visual encoder side so that was that's definitely one other uh future work well super cool that's a super interesting uh direction to take it further and i had so much fun learning more about the vl adapter i think this is such a huge discovery and i generally i think just sort of the cost implications of this are absolutely massive that you can you know only need to fine-tune four percent of the full weights of clip to say down fine-tune it into your downstream task for whether you're working in e-commerce or you have some kind of creative idea for whatever it is with vision and language data sets so thank you all so much for coming on the vva podcast i really enjoyed uh hearing the story of this paper and all sorts of details about this thanks for having us yeah thank you [Music] ", "type": "Video", "name": "unc_research_team_on_vl_adapter_for_efficient_clip_transfer__weaviate_podcast_14", "path": "", "link": "https://www.youtube.com/watch?v=BNPxg5a3NaI", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}