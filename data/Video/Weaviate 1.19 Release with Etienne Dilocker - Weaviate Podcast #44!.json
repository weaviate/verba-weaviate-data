{"text": "Hey everyone! Thank you so much for watching the Weaviate 1.19 release podcast! We have all sorts of cool new features, ... \nhey everyone thank you so much forwatching another wevia release podcastreleasing we've made 1.19 we have allsorts of exciting new features butbefore diving into the into any of thefeatures edian has on what I think isthe best iteration of the Wii V8 shirtuh so Eddie thank you so much forjoining the 1.19 podcastthanks for having me I'm calling thisthe 1.19 shirt I think it's not the the19th iteration yet but it fits perfectlywith all the cool stuff that we have inthe new releaseyeah awesome I remember we joked abouthaving like a t-shirt for all the blogposts the blog cards as philana makesand yeah yeah I really like the book ofit uh so super cool so let's kick it offwith the grpc API and the story aroundwhat led to thatyeah uh grpc API was sort of aninitially unplanned feature but wewanted to add a VBA to a n benchmarksand in a n benchmarks we've we've talkedin in the past we've talked about thesort of Library versus database splitand in a n benchmarks you have all theselibraries and um but now also a coupleof databases so we thought like okayyeah we have nothing to handle it let'slet's get VIVA in there as well and uhwhat you can see in those those uhbenchmarks is that especially for theirso so for those of you who don't havethe context in The Benchmark essentiallyyou have a graph that on one axis showsyou the the recall and then the otherlatency and with any a n algorithmespecially with hsw but with any it'salways a trade-off between recall andlatency or throughput as sort of a asthe inverse latency basically sothroughput in the A and benchmarks isactually estimated because it's notreally a throughput benchmark it reallyjust a single threaded individualqueries and then if a query takes let'ssay a hundred milliseconds then thethroughput would be considered 10 persecond so just one second divided by bythe individual latency the the actualladies these are way lower than 100milliseconds but that's that's kind ofthe The Benchmark setupand what you can see in those sort oflow recall settings of course thelatencies go go down quite a bit butthat also means that everything that youhave as constant overhead so let's say avector surge takes for example half amillisecond but you have a constantoverhead of half a millisecond and allof a sudden in these low recallsituations you now have 50 overhead ordepending on how you see it 100 overheadand that's not great in benchmarks so umwe thought well what can we do here andand can we do something that doesn'tjust optimize for benchmarks becausethat's kind of well I don't want to sayit's pointless because in the endbenchmarks are a great signal for usersand they help them sort of decide butalso to some degree if you over optimizefor benchmarks you always have to askyourself like what benefit does theactual end user have and we looked atthe grpc API in um adding grpc API sortof in the background for some of theclients so what is super important forus was to make sure that there are nobreaking changes or that users wouldhave to sort of start changing theirtheir usage patterns or would have tolearn something new no we've sort ofslowly started adding this this grpc APIwhich some endpoints not all yet butsome can run with this and the pythonclient for example can automaticallydiscover if grpc is enabled it's also anoptional package on the on the clientside so if you don't have trpc installedthen you don't have to install it if youdon't like it but if it's there it'sit's used in the background and thatallows you to to yeah basically havemore efficient queries where just thethe overhead and of course that overheadbefore I think it was was around 500 to600 microseconds so it's a bit more thanhalf a millisecond that was there inevery request but the the shorter therequest duration is the more noticeableis the overhead You could argue that auser would never notice in real life andthat you only only argue it in that youwould only uh notice it in benchmarksbut nevertheless overhead is there andoverhead is gone so making somethingmore efficient is never a bad idea Ithink even if if you could argue thatthat most users might not see the thereal life impactyeah um so definitely want to dive alittle over a couple more questionsabout how these a n benchmarks and therecall latency but um maybe just superquickly so grpc graphql HTTP uh couldyou just quickly give me the differenceyeah uh so so interestingly grpcactually isn't a separate protocol itruns over HTTP 2. so from a sort ofthese are all sort of one layer higherthan the TCP stack they all run over TCPand then grpc basically runs uh via HTTPuh so the the the way and this is a veryinteresting because I think some peopleuh just assume that grpc is fasterbecause it's a faster protocol but it'sactually not a faster protocol becauseit's the same protocol as HTTP and bythe way graphql this is also typicallysent over over HTTPum it's simply thatgrpc uses the protobuf protocol which isvery very close to how uh the the datatypes are represented in theirrespective languages so uh the the gomemory model is very similar basicallyto to the protobuf structure and thenalso the memory model in Python so ifyou have this sort of network requestgoal which is vv8 wb8 server uh via agrpc to python they're simply way lesssort of decoding encoding restructuringand not restructuring but so re-encodingthat you need to do whereas uh bothgraphql and and uh rest that wetypically use for for HTTPum use Json and Json basically is a is asort of space inefficient protocolbecause it's just everything text and itneeds to be parsed it needs to be sortof the same way that that if if you as ahuman right Json you have the curlybraces and you have the quotes and thenyou have the colon then you know okaythis is the key value mapping in thesame way the server and the client forthat matter have to parse that and thatthat simply costs time and it's it's nota massive overhead but in a benchmarkscenario where you don't have a lot oftime like even that can canum show up what was important for us wasto to like not not add new overhead forthe user like cognitive overhead we'rereducing computational overhead butdon't want to add a usage overheadum so you probably won't notice you willstill be able to use graphql especiallyfor for sort of just learning the vb8API just discovering it playing aroundwith it the graphql console this is anamazing tool but at the same time if yourun your use case in production and yourun it through the language interfacethat is or clients anyway you might notneed graphql anymore so it's kind of amore more optionsyeah super cool so stepping into the Aand then benchmarks a little bit Iremember you'd published that amazing am benchmarks collection and we also hada podcast on that I think it's likenumber seven earlier podcast and I thinkwe're on like 47 47 or something but umwow I'm sorry I remember the um so likethe hyper parameters of you know theeffect of ef ef construction Maxconnections um so are all those fixedand then each of these you know a nproviders all have to adhere to the samehyper parameters and the same machine asthat sort of a like a fixed hyperparameter of the hsw kind ofuh no actually so the machine is fixedyes and not even just the machine therethere's actually a so I I said that thein benchmarks was single threaded beforeit's actually not entirely true they'relimited to one CPU which basically isnot the same as single threaded you canrun multiple threats with that CPU butthere's no benefit because you can'tever exceed more than than one CPU sothat is one one restriction uh onerestriction is the build time so I thinkif it times out after I think I thinktwo hours is the limit or so so youcan't run anything that takes more thanthan two hours but other than that it'scompletely free so you can set your ownparameters I think all the hnsw basedimplementations do have the sameparameter set so it's typically it'sbasically a grid search over over allparameters so you have if constructionsfrom very low to very high you have EFfrom very low to very high and same forfor Max connections and and since thehnsw algorithm sort of works the same Iguess in a sense you're just comparingthe the implementations and that alsomeans that the the parameters typicallyperform the same so so maybe in inabsolute terms like one solution wouldbe slower than another but still forboth Solutions like uh setting eightconnections would probably have the sameeffect on both Solutions as opposed tosetting like 64 connectionsum and then in a and benchmarks it's notlimited to hsw based algorithms so forfor those so so for example a PG Vectoris in there the new uh postgres pluginwhich I think is IVF based or somethingor iufpq base so it has completelydifferent parameters but that's the nicething about a n algorithms you cancompare them them still and and sort ofyeah you're kind of kind of free to useall your parameters to get the best outof all parameters under the Restrictionthat you still need to be able to buildit within two hours on a single CPUso so what scale is the test is it likethe sift uh the sift one million vectorsor likeuh yeah we have sift one one m in thereI think we have I forget the name but Ithink it's a deep one B which is not abillion that that would be that would bea bit large but I think it's 10 millionor so so they're they're definitelysmaller and larger but I think they'reall within within yeah into one one toten million do you think maybe that'ssomething that the benchmarks uh likeshould aspire to is or you know justlike the continued scale anyway yeah sothe whole topic is so interesting um soso maybe pivoting topics a bit we alsohave in our uh release the uhdegenerative cohere module and so I'mreally excited to talk about thisbecause this is actually the firstmodule I've ever worked on at webiateand I mean it was pretty easy to the thethe infrastructure that's been in placeto uh extend these modules with newmodels or new external model providersso you know open AI Co here when theanthropic Cloud Model comes in the hugand face Transformers gbt for all thellamas like all these large languagemodel providers can be integrated soeasily into eviate and so yes I wantedto maybe ask you anything about um yourlatest thinking on the large languagemodel stuff and that kind of umthe the trends and all these new modelsthat are coming outyeah cool so first of all I'm reallyhappy to hear that this was easy to dobecause this is this is kind of thepoint of the module system like thereare so many sort of complex tasks withinthe database both on the the victorymixing side but also on the just sort ofold school database indexing site so sohaving that sort of almost sandbox scopeof the module where it's very easy tointegrate something new that's that'sthat's always awesome and and um we'realways awesome to hear that it actuallyworks like this as as it is intendedum which serves a nice nice segue intoeverything that's happening right nowbecauseum we're really in this this sort ofcrazy mode of something new popping upand and breaking GitHub Star Recordswithin a couple of days or something ithas more than everything out there and Ilove that we're able to to integratethem I I was about to say react but Ithink react is not the right reward Ithink we're able to integrate them veryquickly as they pop up because it's notreact would imply that we somehow likehave to chase them but it's not the caseit's just enabling users they didThere's Something New comes up a new beit a new model a new integrationsomething that provides value in in thisAI space and if it provides value thenthere's a good chance that it can alsoprovide value to deviate users and Ithink being able to to quickly adaptthere and quickly integrate those is issuper valuable for our users andsimilarlyum having vv8 basically as the thestable API for them I think is alsosuper cool because if you if youmanually let's say you're usinggenerative open AI so the completionendpoints from openai and now you wantto maybe test that against the cohereendpoint and then yeah I just want tosee like okay what what performs betterfor my specific workload if you were tointegrate with those two providersseparately you would probably have toadjust some code because you wrotesomething that was meant to contact thethe open AI API and now the the cohereAPI I haven't looked at them but I'd besurprised if they were identical itprobably is some difference but if youuse vv8 with all of that the VBA API isthe same all you have to change is themodel or change some string somewhere inthe model that's essentially justconfiguration and that goes for for waymore of these things like if you this isjust one example you could say you couldargue like yeah okay changing changingone of those requests is easy but now ifyou combine that maybe with with thegenerative search which typically is thethe retrieval step and then the thegenerative part what if you want tochange something on the on the retrievalside you could have text back open AItext to back cohere text effectTransformers contextenary uh uh texthugging phase for any hugging phasemodel probably new ones that that havebeen released during during therecording of This podcast so um you cancombine all of these all of these fiveor however many it was you can combinebind them in a hybrid search with bm25so so now we have this like large Matrixof different options and you can do thiswithin rebate by simply changingconfigurations I think that's that's anice addition additional value of havingthat collection of of different modelproviders within mediate yeah I thinkthat um like they're kind of like modelinference orchestration of leviate is sofascinating to me like whether you wantto manage this on the database side oron the client side and I think by doingit in the database you just have kind ofone thing with all this infrastructureand and yeah the whole thing is soexciting thehead of the one paper I really likedthat came out recently is hugging GPTwhere it's kind of about the you knowthe large language model is like thetask decomposer that routes it to themodels with descriptions of the modelsand I mean the the future of like weoriginally talked about this pipe APIwhich is kind of like a dag ofcomputation flow like retrieve retrieverank read but now we're seeing such adynamic uh kind of chaining of thethings you can do with search interfacesand it's so interesting so the nexttopic is another innovation in searchthat I think is extremely exciting somuch potential with this idea which isGroup by arbitrary property and yeah canyou explain how maybe with the documentpassage example I think would be thebest likeyeah yeah this this feature this wasbasically a a request from one of ourcommercial users but when they requestedit we we kind of almost expect thisalready because it's just something thatthat is an Ever sort of growing problemthat we now have a solution for so theidea is if you have your documents splitinto some kind of sorry not to documentyour your entire content split into somekind of hierarchyum typically you're you're either forcedto give up the hierarchy or you have todo some kind of work around so so what Imean is for example let's say you havedocuments and you have passages and adocument is sort of a grouping of anumber of passages if this is a verysmall grouping you could for example dothe ref tvec so you could have them inthis as two bv-8 classes you can do theref Tech and then reftube gets the meanembedding of those passages the problemwith this is as the number of passagesper document grows it gets harder andharder to represent so much informationbecause essentially all gets combinedinto a single vector and there's only somuch information that a single Vectorcan can hold so if you try to combine athousand passengers and and these allhave maybe diverse topics so some thatsays maybe not document passages maybebook and chapter or book and passagethen it's very very hard to keep thatmeaning so what users typically do isthey would want to search uh buy the thesort of smallest unit which in this casewould be the the passage but then theproblem is well how do I get back to myassociation off of the document like nowI'm searching for passages uh maybe thetop 10 passages are all of the samedocument maybe in the top 10 passagesit's 10 different documents and what ourusers were missing is basically a simpleway to to sort of keep that kind ofAssociation and and make make that partof the ranking basically and with thegroup by featureum and it's and a group by by anyarbitrary property I think it's it'smost interesting for this particularexample to group by the the referenceprop that that would be the theassociation from passage to document itnow allows you to havebasically the first hit comes in andlet's say it's off document oneum then you can you can group all thepassages that match that same documentin one group and then you can eithersort of group this by what is the thehighest similarity per group or youcould take the mean similarity per pergroup or something there's like moremore options but basically if you wantto display like if you want to searchthrough passages but to your users wantto displayum uh documents you can do this nowbecause by finding the thethe one passage that fits the best youstill get the the latest passagesbasically the the most related passagesin that same document that you canrepresent to your users so it gives youlike a nice way to to display that aslike almost like a like a tree kind ofwayum or various other ways I think givesyou more options to to show that to yourusers yeah that's amazing I mean umfirstly quick shout out I was just atthe um the haystack search and relevancycon a conference that I open sourceconnections I know they would love thistopic um yeah so like with the ref devecyeah you try to aggregate all thepassages and averaging is maybe too muchof a compression I used to think thatmaybe a graph neural network couldaggregate those vectors and or maybecluster some ideas like this it alsokind of reminds me of like multi-vectorrepresentation where you you know youhave a document and then passage thatkind of thing maybe like title passageand so on authorbut yeah that's um it like I I like theanalogy of the podcast like taking thesepodcasts and then um you know puttingthe transcriptions in and you know youhave all the chunks and you have thesimilarity the chunks that you aggregateit's such an interesting innovation inSearch and I like to call this like toplevel indexes and yeah I'm so excited tosee how this uh evolves so so nowtransitioning into uh some new data ohyeah unless you want to stay on that nono just as I I love your example aboutthe podcast and I just have to thinkbecause we opened with the topic ofbenchmarks and we also have a dedicatedepisode on on benchmarks I think thatwould be a great example because now itgives you the ability like do you wantmaybe that snippet from today's podcastwhich is very relevant to to benchmarksbut the podcast is not about benchmarksor do you want more of the meansimilarity where you have dedicatedepisode about benchmarks where maybe nosnippet fits as good as today's snippetbut overall you have the the podcastthat fits the topic better and I thinkthis is something you can very nicelyrepresent with with uh the new groupingfeature yeah amazing that's a perfectexample yeah okay is the one passageMasters it enormously but then the wholecollection yeah awesome stuff soum so okay so coming into the um thedatabase stuff and I think this is sointeresting how um so now you have atunable option between whether you wantto use the bitmap index which I assumeto be like a super fast inverted indexbecause of how the underlying datastructure is implemented and then youalso have the umthe bm25 score index uh so so what goesinto kind of this tunable indexingyeah so in the past we kind of made anassumption that if it's text you wouldalways want to do a full text search andthat that just I mean that may be truein some cases but it wasn't it wasn'tgranular enough because exactly as yousay we've introduced in in the previousrelease we've introduced those bitmapfilters and everything so so we had itfor everything except text props for forthis particular reason andum everything else is super fast now buttext still kind of like was sort ofstill the old implementation becausetext was built in a way to be indexedfor for bm25 which is or without goinginto into too much uh detail but it'svery different because you need a lotmore information so for bm25 every hitand every Association needs to be scoredso you need to know uh the thefrequencies and all these kind ofadditional additional things that youdon't need to know for for Purefiltering but now if your use case is toSimply filter and nothing else on texton something that happened happens to betextum yeah right now you get options youcan either index it only for filtersthen it's fast it's the bm25 index youcan only index it for search then youcan have then it's basically the samebehavior that you have before so filtersif it's not specifically filter indexedit can still fall back to um to the theold index to the searchable index but ifit has both then it just takes the thebest at each time then it's fully inyour controlum it's fully Backward Compatible so byby default we build both indexes but youcan take the turn them off and thensimply tune in so if you said Okay I Iknow I'm only going to need so so forexample if the field is like a a usergroup that you need for permissionschecking or something you're never goingto run a bm25 search over that you knowit'll only be there for for filtering sojust turn the searchable index off keepthe filterable index and then you getfast filters using bitmaps under thehood for string properties as well yeahit also makes me think about like youcould probably represent categorical uhvariables that way like if you have yeahyeah typical um so kind of yeah justjust because it's a string doesn'tnecessarily mean it's it's full textthat was kind of the assumption that wemade before but it simply doesn't holdtrue yeah super cool and and you can usethose with the wear filters yeah amazingstuff um so then there's um so nowthere's more tokenization options aswell umyes you may be taking me through thethinking around the tokenizationyeah yeah soum mention string and text before and Ithink this has been a big source ofconfusion for for some users becausesome users just assume that because it'sstring it would use a specific kind oftokenization some users didn't even knowthat there was a split between stringand text so they used string forsomething that should have been text oruse text for something that should havebeen string and as I'm saying this wellwhat should have been a string whatshould have been a text like that's justnot clear that's that's very hard toexplain and that's why we've we'vesimplified this and again all in anon-breaking way so right now there'sonly going to be text string basicallydisappears nothing breaks for you youcan still in your apis or in your yourschemas you can still set string andit'll be automatically converted to totext and to keep that kind of differencethat we have before there are now moretokenization options so you can tokenizeif you want buy the whole field which issomething by the way that a lot ofpeople assume that that string would dobut it would only do that if you set thetokenization to field so right nowthat's simply an option you can tokenizeby word which basically means vv8 willsplit on the spaces but we'll keep theindividual things you can can or or willsort of keep the alphanumeric contentsof the the words that will removespecial characters you can also splitjust by white space which is sort ofagain splitting at the word boundary butnot removing those special characters sofor example if it's like a a productdescription that has a special sign init or something and you want to you wantto keep that because it's relevant thenyou can so again more control andsimpler API at the same timeso keeping on the um the apis have beenum so I've started diving intotypescript and learning a little bitmore about that I've seen more of theconsistency level and um so the newestthing tunable consistency for Vectorsearch and the get requests and all thatum yeah the whole so replication issomething that you can so I hope I'mgetting this correctly and Eddie andwill correct me in a second if I'm wrongbut replication is like how you'rereplicating your data across nodes in acluster so so how does this work witheach tune each query you can tune thisyeah so the the there's two sidesbasically for for tuning one is how doyou write your data and how do you reachyour data and they they somehow dependon one another because for example ifyou make sure that during writing everynote already has the same copy itdoesn't matter what you do add readingbecause if you just read it from onenote or if you read it from frommultiple nodes you already made surethat they're in sync during duringwriting however that has a large costbecause now for every ride all the noteshave to basically agree on it all thenotes have to be alive so so um withtunable consistency the idea is to tomake some trade-offs basically you couldsay I'm only uh writing with me with amajority of notes and I'm also onlyreading with the majority of notes andthen you know okay we still got someconsistency guarantees basically becauseum because if if both writes and readshappened with with a majority of notesthen uh it should still be consistent oryou could say it's actually not supercritical that my data is always up todate maybe you want to accept that it'ssometimes a bit bitum yeah eventually consistent so itcould temporarily be out of date andhasn't been repaired yet and that may befine because maybe something that you'reupdating is is yeah it's not a bankaccount details or something or or Banktransactions but rather product searchwhere it's not a big issue if somethingis slightly outdated or maybe you cancatch it in the application side if it'soutdated andum yeah for for a graphql searchesbasically that was the the last pointthat we had where we didn't have tunableconsistency yet and for those threequeries you can now set it there as wellamazing uh so everyone we are a littletime constrained so we're gonna we'recutting it a little short but um asalways the incredible devrel team andthere's this release blog post with allthese releases if you want to consultmore details Eddie and I are availablein slack and more than happy to answerany of your questions so grpc generativecohere Group by arbitrary propertybitmap indexing more fine-grainedcontrol over that with the tech surgedeprecating string with moretokenization than the tunableconsistency and some patch releases Soyeah thank you so much everyone", "type": "Video", "name": "Weaviate 1.19 Release with Etienne Dilocker - Weaviate Podcast #44!", "path": "", "link": "https://www.youtube.com/watch?v=Du6IphCcCec", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}