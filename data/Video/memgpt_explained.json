{"text": "Thank you so much for watching our paper summary video on MemGPT! MemGPT is a super exciting new work bridging together ... \nhey everyone thank you so much for watching this pay-per-view video of mgpt mgpt is a super exciting new research paper that's bridging together Concepts in operating systems with large language model applications and this is probably the super exciting novel thing about this is this reframing of the perspective around retrieval augmented generation as well as kind of llm tool use into this perspective of thinking about the llm as the processor the kernel behind the operating system that swipes in its own memory similar to page replacement and all these kind of but quickly before we dive further into the details of mgpt the general setup is that large language models have a limit on how much text they can process as input so say you're just chatting back and forth with one of these large language models you can generally get a pretty large amount of messages so say gbt 4 with 8,000 tokens as the maximum input you can trade 140 messages of about 50 tokens back and forth before the chatbot is no longer able to uh reason about conversations you had in the past so you know say you fired on chat gbt 2 months ago and told it your birthday was October 11th then two months later it no longer remembers when your birthday was so this is kind of framing it in this chatbot perspective of how many chats you can have back and forth with one of these popular large language models if you also think about uh chat with your docs you then also have to have uh the specific information in the context window which also eats up some of these tokens and thus how many messages you can trade back and forth before it can reference old messages by just looking in the context window so retrieval augmented generation has emerged as a solution for this is a super popular technique where you use vector eddings and search queries to only populate the input with relevant information so here's an example without rag this use this example of time you say what is rtoc a feature in weate and chbt doesn't know what it is but if you instead you know say please ground your answer in the following information and then you ask it what is refc now it knows how to answer the question so when all the user wants to do is know what refc is we get away with this pretty simple rag setup where we can just retrieve some kind of information to use in our context window and then answer the question so say we only need 200 to 300 tokens in this particular example but now let's imagine a long conversation where the user is repeatedly asking questions about how to use refc for their particular application and we have to balance me um managing the memory of this conversation history as well as lookups from our retrieval database so the big idea in mgbt and this is super exciting is what if the large language model was aware of its own input window limitation and took actions accordingly so they're extending the large language model with the tool use of knowing when to add retrieval results or say things that it learns from the conversation into its working memory so let's step into the overall architecture of mgpt so mgpt is the operating system for rag applications bridging together memory with tool use and this particular focus on managing the main context memory so at the heart of the oper in system we have the large language model processor then we have the virtual context so the main context is what's currently in the input for the large language model to make its prediction what's in the external context is say our Vector database or our store of data where we can uh swap in and out with the main contexts like page replacement and operating systems to get the relevant context that we need to complete a certain task so this is the the memory part of it and then the memory is managed with functions and Tool use so tool use is a super exciting idea where we conect Lang language models to things like say calculators or maybe a weather API if you want to ask it the question what's the weather in Boston right now it needs to send that external request rather than relying on the information stored in its parameters or particularly in the weather API example it's unlikely that you're keeping your vector database fresh with with that particular information so you probably have these kind of external services that work into this picture as well but more interestingly are these functions around reading and writing memory so reading memory is where you you know read from the vector database we say retrieval a search query and then there's also ideas around pagination and query rewriting in this mgpc paper but we'll get to that later but this is the idea of where you read memory to potentially add to the main context by using these uh operating system functions like a pen to the working context we'll get into that later as well but also interestingly we have write memory so as you're having this conversation history as I'm talking to my chat gbt for a long time the mgbt has these functions around you know Connor just told me his birthday is October 11 so let me add that to to my uh my storage then we also have this idea of interrupts and events so events are what triggers mgpt to start doing some processing so whether this could be as simple as I come to Chad gbt and I say what is rtoc and then that triggers the event that starts all this processing or say I upload a document or say there's a system message like hey your context window is at uh 3,500 tokens let's let's you know start trimming it down or you have things like a timer like every five minutes maybe it trims down or something like this so you have this kind of idea around interrupts from operating systems where maybe you have some kind of asynchronous processing like to the large language model you say uh research how selfrag works selfrag is a paper similar to mgbc that came out around the same time so say you're doing that asynchronously while you continue the chat and then it interrupts and says hey I finished this uh research report should I work it into the conversation or what should I do with this so this is the general overview of how mgpt works so I took this quote from I don't know if it's an ex quote but just to reference that I got this from outside of the paper this was Charles Packer on the amazing run llm podcast with Professor Joseph Gonzalez and he frames this idea of mgbt as an agent that knows how to use memory management tools and I think that's just a perfect way to describe it a super exciting direction for this whole field of retrieval augmented generation is endowing the large language model with the option to write to the database as well or in this particular case read WR to its particular main context so you've always had this idea of read from the Save Vector database and then just kind of blindly put it into the main context but now you kind of have this layer in the Middle where you have search results and then you're saying what from the search results am I going to put into my main context and then kind of keep there as I continue the conversation so I think this quote is just so powerful an agent that knows how to use memory management tools so the general idea here is that we're building an operating system for large language models and a quick question I'd ask the audien is what you think about framing Frameworks like Lang chain or llama index in this kind of way is thinking about them as operating systems for large language models that orchestrate the connection between language models and tools and Vector databases and memory but so diving a little further Andre karpathy has also written a really interesting tweet on this kind of llms and operating systems and this is quite an interesting idea I think of uh taking that next step in the thinking of not just llms and databases but having a whole operating system that orchestrates this kind of thing so from Andre karpathy with many puzzle pieces I assume dropping recently a more complete picture is emerging of llms not as a chatbot but the kernel process of a new operating system so what we're seeing in mgbt is the llm is the kernel process that's saying I need to change my memory I need to use this tool or I need to respond to this event so it's the kernel and the operating system that just that is you know orchestrating how to manage its memory and use tools for example today it orchestrates input and output across modalities text audio Vision code interpreter ability to write and run programs browser internet access or embeddings databases for files and internal memory storage and retrieval so some examples of the different kind of uh functions it can it can do or memory it can access a lot of computing Concepts carry over currently we have single-threaded execution running at 10 Herz tokens per second and enjoy looking at the assembly level execution traces stream by Concepts from computer security carryover with attacks defenses and emerging vulnerability so that that I think there's just so much information so much interesting stuff to explore in that in that little text alone this idea of you know you know we have single threaded where we just have one llm process you know where most of I know at least I'm doing this is logging the llm in my terminal and just kind of watching it Go by because you know you're paying for the tokens but this kind of we SE things like uh maybe Lang Smith is a good example from Lang chain of visualizing these complex prompt chains for uh understanding these execution traces but I think this kind of like parallelism you know if you have like concurrency and llm tasks like again that example of like uh research self Rag and then it's doing This research in the background and then it says hey I finished like there's probably just so much opportunity from that and so finishing up I also like the nearest neighbor analogy of operating system because the industry is starting to shape up similarly Windows uh OS X and Linux GPT Palm cloud and llama an operating system comes with default apps but has an app store you say the chat gbt Marketplace and kind of how code interpreter is plugged in with coh here you have coral and so yeah we're definitely seeing that kind of app store around the language model and then most apps can be adapted to multiple platforms again the API say wv8 generate module showing how you can also take the the model out of the app store tldr looking at llms as chat Bots is the same as looking at early computers as calculators we're seeing an emergence of a whole new Computing Paradigm and it is very early so this is a super exciting tweet in this whole context of uh llms and operating systems so let's dive into a little more particularly what makes the the mgbt and operating system which is particularly this kind of memory management as a large language model tool so what they're adding in mgbt is the working context append replace these kind of functions so you have this kind of conversation where you say hello Chad welcome I'm excited to embark on this journey with you as a PhD in computer science blah blah blah and then the user says you know this information of my birthday is October 11th and my favorite cake is this chocolate lava my favorite cake so this is an example of how it takes the conversation history and it's deciding what is important to put in its context so similarly uh it might have a system message that says warning the conversation history will soon reach its maximum length you're making the language model aware of its own token limitation say Hey you know you've got 3500 tokens we're going to need to compress this so then it will compress it as following it takes the conversation history and it depends this key personality trait enjoys high-speed Adrenaline Rush activities like Formula One racing and intense gaming sessions in csgo so it's doing this to compress its context window so similarly with search we say search will quickly get into recall versus archival storage and how they differentiate that in the paper but you're having this conversation what was the artist you mentioned you could get into so the user is asking about you know conversation history so you're then looking into your you know your external storage your vector database where you might have two kinds of vector databases you can have all sorts of kinds of vector databases but for now you have the storage of the event history like all the conversations between you and your chatbot as well as say like general information like if you're chatting with your docs this is the conversation we've had these are the docs so two two databases so say you're searching the conversation history about music from there it recovers you know you're talking about you like Taylor Swift so it adds that to the working context similarly we also have replace so this is a super exciting idea where you have new information so you used to say you know I was super into horror movies recommend me horror movies and now your tastes have evolved and so now you're into romantic comedy so it replaces the in context with I watch horror movies to romantic comedies so I think another really interesting quot from the paper is to think about this as the the mgbt is documenting its progress on the task by writing to its own working memory so it's doing a long task like document analysis it's only writing to its working memory say important things that it's going to help it further so it's kind of like this idea of distilling The Core Concepts into you know what's helping you with your research or whatever you're doing so let's dive a little further into types of context so these types of context are the explicit labels of the parts of the input window to the large language model and I think this kind of Separation this explicit thing of these are the system instructions this is the conversation history this is the working context I think there's just so much more opportunity to explore that kind of separation of the parts of the input window so say you have kind of this uh say you're in these multi-agent Frameworks like autogen where you have this uh information about your persona like I am a software engineer I like goang and things like this and then you also have say a highle description of like what you're working on as well as this kind of retrieval context and maybe more immediate recent conversational context with the other software engineers and that multi-agent framework but this kind of separation of the categories of the input window and how you think about retrieving and augmenting each particular part of it so we start off with system instructions so system instructions you know these are like the pre- prompt is like you are a helpful assistant but now that you are a helpful assistant thing has been extended with the the descriptions of the tools that you have access to so in say open AI funks you get this like Json dictionary that tells you what a function does and how to format the input output Arguments for sending a request to that API so if it says hey this is a calculator you can use it to add multiply numbers together this would be how you would uh trigger a request to the calculator the conversational context is then a first in first out cue of recent event history and then an interesting thing is this recursive summarization so one of in my opinion one of the most interesting ideas of Lang chain in llama index has been this idea of uh where you where you have this kind of recursive summarization so if you have docu you have too many documents that you can fit into one input window you'll you'll have some kind of prompt like please summarize the following documents you'll receive them one at a time as well as a summary so far and then so you keep this local summary and you just keep looping through the documents updating the summary and so on so you do this to the end of the conversation history so say these are my last 10 you know back and forth of the chatbot and these are my first 200 messages these would be recursively summarized and just something that can fit in the input and then you have this working context so this working context is where is the memory scratch pad for where the LM processor is reading say what it's just retrieved and it's looking through the search results and it's saying okay I want to take that and put it in my working context or you know similarly looking through its conversational history so these this is what's currently in the in the memory to the language model what's currently in context so then we also have this external storage if we'll you know go all the way back to our picture of this whole thing we have the main context that's of system instructions conversational context and then working memory and then we have our external context which is where our Vector database comes into the picture so within external storage the authors are looking at two different kinds of storage so recall storage this is just like the Raw event log so say it's chat history or document processing just the raw you know what happened I uploaded this document I asked you this question about the document the system ended up sending this answer so that I then sent the next question which was this or just the um the archival storage which is the general read right store of say Wikipedia or you know your docs in the chat with docs classic example so these are kind of the two kinds of storage that we might retrieve from so the authors also present three different ways of querying these external database it can be time based where say you just get the most recent events which makes a ton of sense for conversation history but then say you have information storages then you have text search and embedding based search so the next key part to this and this is probably the key part is this self-directed editing and retrieval so again we saw within that function schema that goes into the system instructions are going to be how you do this working context out of pen working context out of replace the functions for how you will take say search results and then you work that into the working context or update the working context so then we have the control flow so events are going to trigger the start of this process so the you know user message system message user interactions and then we have this kind of function chaining for how we're managing retrieval results which is also a really interesting uh part of this paper that I think of as marrying mem gbt with web gbt but with this also kind of interesting rephrasing of the query so with web gbt you had search actions so you wouldn't just retrieve and then just take the top five results and just put those into the into the input you would instead say uh let me see the next page of search results because you know this is how humans use things like Google search as we you know scroll through the results to try to find the thing that we're looking for so mgbt is also adding this thing where uh in addition it might look through say you know three pages of results and then reflect on uh let me actually try a different query this query is not specific enough and I'm learning that by looking at the results that come from this query so that's a pretty interesting additional layer to this we say have things like uh query reformulation where you pass the query to the large language model and prompt it like here's a query uh could you please reformulate it to get better search results when passed to a search engine and things like this but making it more meta with this kind of self-correcting where it sends this query sees some results and then learns from that to reformulate the query so as a quick recap of the core ideas we have the main context that has these explicit separations of the part of the context from the system instructions to the conversational context or the event history as well as the working context and I think this that part of it is just super exciting the continued exploration of how we explicitly separate the context especially as we get longer and longer context models then we have the external context which has the recall storage and archival storage or say two different ways of thinking about it could be two different classes in a we8 vector database instance but two different sources to retrieve information from then we have this self-directed editing which is you know how we're augmenting our work in context with things from the event log or say our retrieval and then we also have these search actions through the retrieval like paging through search results or say formulating a new query then we have this control flow and function chaining understanding that we have system events that kick off the mgbt process as well as say the function chaining involved in paging through search results and reformating the query so kind of opening up the framework to be extended in the future now let's dive into sorry now let's dive into some of the experiments in mgbt so they start off with the the general questions of does mgbt improve consistency and engaging this so consistency does mgbt leverage its memory to improve conversation consistency can it remember relevant facts preferences and events from past interactions to main coherence this is the whole idea of you know two months ago you told chat gbt that uh your birthday is October 11th and now you're talking to it later and it's going to remind you of your birthday or things like this so then engaging this does mgbc produce more engaging dialogue by taking advantage of its memory does it spontaneously incorporate long-range user information to personalize messages so to evaluate this they use the multi-session chat data set so the multi-session chat data set is generated by human labelers who they're given a prompt to play a particular Persona and then they have five SE five chat sessions and each chat session has about 12 messages so this is the data set is you play a role like hey I am into to uh gaming and horror movies and things like this and then you chat with another human who similarly has like a Persona card so then the data set is augmented to add a single question answer pair at the six session that will do some kind of long range reference to something said earlier to see if it was able to use this uh particular way of organizing its memory to recall the particular thing about who it's speaking with so these are the results of uh jointly having the Rouge score which is like the some some type of engram overlap I'm not sure the exact details of but you know this kind of engram overlap between that ground truth answer and then what the language model produce as well as this accuracy which is this llm self eval thing where you give gbt for uh the question answer and then the gold answer and you say you know how was this answer is it accurate did it f is it close enough to the uh the gold answer so then another task they tested was conversation opener so this was about uh seeing how well it can open the conversation with something engaging Bas so this is measuring that engaging this whereas this measures a consistency so this is you know the the it has the gold Persona of the user and then it has this uh human Baseline of what was said so I'm setting the elsat I want to be an attorney blah blah blah and then has this particular preferences around I love coffee and I love tea so then these are three different kinds of responses from mgbt whether it's using the work in context and the recall storage to say work in the uh the tea and the coffee thing into the opening response versus just something generic like you know hey it's a pleasure to talk to you let's you know let's talk so so using this kind of stuff to have a more engaging opener and and they similarly measure this by having the similarity in conversation openers between humans and then the mgbt with these different contexts so here's another Super exciting detail to mgbt is overcoming this lost in the middle problem so lost in the middle is a famous paper that shows that uh in retrieval augmented generation the language model tends to only attend to the first search result or the last search result so if you have the information you need in the middle so return 10 search results to put in the input and the thing you need is at position five the language model is not really able to parse the search results and find it in position five so because mgbt does this kind of paging of the search results and only adding relevant information back to its working memory you have this kind of flat line of it doesn't matter how many documents you retrieve it's going to have the same performance because it's parsing through the search results to add it to its working memory and they also introduced this new task of nested key value retrieval so in Lost in the middle uh one of the tasks is you have key value dictionaries so it's like U ID key u ID value and it would say like what's the value for 94071 FF right and so then it has to look up the value so now you're kind of doing this chaining where it's storing the the nested values and so it's it's testing this multi-hop question answering where you're the you know the intuition what it would eventually be used for is you have questions like did Aristotle use a laptop you break that up into when did Aristotle live when were laptops invented answer each separately and then merge it together so this is kind of testing that ability to merge together facts awesome so that's a recap of the core ideas of mgbt and some of the experiments in the paper now let's dive into some of the future work directions outlined in the paper as well as some of my personal takeaways and how I think mgbt will impact the entire space of retrieval augmented generation so starting off the authors mention applying mgbt to other domains with massive or unbounded context they explore using mgpt for chat Bots as as well as document analysis in this case reproducing the natural questions experiment from Lost in the- middle and showing how mgpc can help with that they also discuss integrating different memory tier Technologies like databases or caches I think that is a super interesting one where you have different kinds of memories and you have different latencies for the different types of memory and so that is a super interesting topic then further improving control flow and memory management policies just I think just further understanding the action space and how to describe the tool of me of memory managing your memory to the llm and then fine-tuning an open source model for M GPT tool use so in the experiments they're prompting mostly gbt 4 gbt 3.5 to do this what would it take to get llama 2 to achieve the same kind of tool following for uh memory management so I want to kind of outline that a little more because I think that's a pretty exciting future Direction so we generally are looking at this idea where we're using gbt 4 to create training data then we use knowledge distillation to train a smaller model on the labeled example from gbt 4 and this way you compress it into the smaller models or say the open source models so this is kind of you know we're there are all sorts of tasks that you can kind of compress this way and I think that's one of the most exciting directions for the field right now is we're using language models for all sorts of things whenever you have a prompt for something you have a task that you could then generate labeled examples with GPT 4 and then compress it down into a model into say llama 2 with the 7 billion parameters it's going to be cheaper to serve and you know you run it well it's it's still in the air if that's going to be cheaper to serve because you know open AI they they have all sorts of you know infrastructure behind how they serve their API so it's interesting still exactly the cost difference between you serving your llama 27b compared to the gbt uh 4 3.5 turbo all these kind of stuff so there's also kind of the idea that say F 1.5 billion parameter that textbooks are all you need paper there's this idea of you can maybe keep going and keep compressing it and if we get to the point of say you only need a 300 million parameter transformer for your task and say neural magic is exploring uh like sparsifying these models to run them on CPUs so that could be really interesting in terms of just you know the cost of running these inference and if llm inference cost gets super fast and super cheap that unlocks all sorts of kind of new use cases so here are some of my personal takeaways from this so firstly for me it was really interesting to see this kind of explicit uh when to active retrieval thing and compare that with Flare which is the active retrieval augment to generation technique so with Flare what you do is you're sampling the next sentence and you multiply out the log probabilities of the tokens for that next sentence if that's below a certain threshold you'll do another retrieval because it's saying that basically that next sentence it wasn't grounded in facts you need to retrieve more information to help have a better next uh sentence generated compared to this kind of M GPT where you have this like explicit you know I I don't know what I I don't have what I need to write this next sentence so let me go retrieve and update my working context and so this kind of whether you just want to decode it from the probabilities of the language models or you want to have this kind of memory management active retrieval as an explicit tool that it you know calls with functions the next big thing and something that they talk about in the paper as well is kind of this latency of mgbt if every time you chat with your uh you know your chat body it has to do all these steps then that's going to be really slow and that'll be problem so there' probably be like a a layer on top of this where it's like kind of a quick answer compared to this whole like operating system thing and that's kind of related also to the you know the compressing the model and trying to make all this run faster the third point for me that I think is really interesting is the difference between this kind of uh web result paging or reranking so similar to web gbt mgbt is going to like scroll through search results and so I'm curious what people generally think about this difference between like next page previous page actions compared to just applying a reranking model which is a high-capacity model that generally takes in the query in each document and then gives it a higher capacity ranking score like matching score and then Resorts the list from say the course grain retrieval or we're now seeing these kind of ranking models that would take in like you know 10 documents as input and then rerank them by kind of looking across the documents in addition to just a sort of query and one candidate document at a time kind of setup so a lot of interesting things happen with ranking and it makes me I'm not sure I'm super bullish on this kind of paging concept because I think reranking is already kind of you know know the the better version of that so uh then in the paper they also have these kind of um perspectives on training longer context models sort of framing that the purpose of this uh paper is is hey we're you know we're never going to get models that can uh process like a 100,000 tokens so we're going to need these kind of memory management techniques and so what I've kind of learned about this especially in the weeva podcast with ofier press is that it's not just kind of the quadratic attention you can kind of have like gradient checkpointing and things like Alibi attention to get around sort of the computational complexity behind uh scaling the input length the real problem is sort of the uh training data and there's not a lot of good um training data that's naturally like 100,000 context length so that's more so the interesting thing is where do you get this data from so I've had these really interesting conversations with Owen kgve who's the founder of sci-fi and so this will be linked in the description it's a GitHub project where you're creating synthetic textbooks and I think this kind of synthetic data it could be the answer to how do we create the training data for these super long context models so some other takeaways is I think this kind of parallel asynchronous concurrent processing is going to be a super interesting direction for the evolution of this so you know say you're having this conversation and you we talking about mgbt and you also want to kick off this research on how does self rag manage rag selfrag is like another paper that came out it's like this is like the dog food as I'm doing this I'm thinking it would be interesting if I could also have a knowledge of that paper so imagining it's kicking off that async research task and then interrupt he finished the report or I've written it to the database all these kinds of things so then actually let me come back to the gorilla thing but so seven would be uh this idea of the use of databases in caches so a really interesting thing and I you know I'm not an expert on this but I learned so much from listening to uh Eddie and present how multi-tenancy was architected in we8 at the AI conference and some future directions like you you have all these kinds of U memory with computers right you have like the memory cache you have like L1 L2 and then you have like RAM and then you have ssds you know hard disk and then you maybe have like cold cloud storage so there's like all these different kinds of ways of having memory and maybe the llm operating system can more intelligently kind of cache memory and use like the physical storage so it's definitely not something I'm an expert on but it it definitely I can see kind of you know how that could be an opportunity so then let's talk about gorilla and so shashir Patel is one of the authors of this paper this comes from the same lab as the gorilla llm so that's kind of what drew me to this curious to see if there was a connection with the gorilla llm so I think the angle here is allocating as few tokens as possible with the gorillas so thinking about mgbt and Guerilla the idea is to describe the tool in as few tokens as possible because the whole idea of this is you know we need to be efficient with our token allocation similar to lead to like memory allocation is token allocation so if we can just describe our tools like you know in the case of the we8 gorilla you have access to a vector database API you can perform different kinds of searches such as bm25 vector or hybrid you can add reranking or you can add filters so you just have this succinct natural language description then it can do the natural language instruction of the search it wants to execute with you know more sophisticated searches rather than just uh search music right you can use all the apis of we v8's graphql API and then it can translate this natural language instruction into the graphql with the gorilla under the hood so multiple language models also in the intermediate say the parser step let say in this parser step you have another uh language model a gorilla that is H formulating the tool requests into the particular API so thank you so much for watching this explanation of mgpt I really hope you enjoyed it I'd be more than happy to answer any questions or discuss any ideas you had about the content you know explored in this video if you want to connect with me personally I prefer to manage Communications on X at C30 if you want to learn more about wv8 you can check out we8 iio or if you want to join the WEA community on slack so thank you so much for watching and I hope you found this useful ", "type": "Video", "name": "memgpt_explained", "path": "", "link": "https://www.youtube.com/watch?v=nQmZmFERmrg", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}