{"text": "Hey everyone, thank you so much for watching the 48th episode of the Weaviate Podcast!! This is a SUPER exciting one, ... \nhey everyone thank you so much for watching the wevia podcast I'm super excited to welcome Brian Raymond the founder and CEO of unstructured unstructured is such an exciting company for uh I'll explain kind of the way that I see the space of kind of how do you get day unstructured data like say PDF so these corporate data Lakes web pages all this stuff kind of getting it into systems like we V8 and so I'm so excited to welcome Brian thank you so much for joining the podcast thanks for having me excited to be here awesome so could you maybe kick it off I know I'm going to give it a course description but can you explain what unstructured is in the founding vision of it yeah absolutely so um so when we we embarked in uh on this kind of Journey that we're on today with unstructured the vision was um pre-processing is terrible nobody likes it it takes forever but it's a critical step if you actually want to um deploy language models of any type against data that's important to you and so um I had uh the last four years before I started on structured also the a great company called primer Ai and at primer we were fine-tuning models orchestrating pipelines building applications on top of them doing all sorts of really cool stuff with Transformer based models and um and we we'd have these you know fantastic applications and we'd show them to customers and say I want that on my data and then um we look at their data and we'd hold their head in our hands this is going to be a huge amount of work and rejections and Python scripts and OCR and everything just to get everything into typically like a nice clean Json format right so that we can feed it into an inference Pipeline and so the vision when we started out was hey hugging face is you know exploding over here with um you know tens of thousands of models incredible Community what if we did something similar to the left of hugging face and we made it cheap fast and easy for data scientists to get through that data engineering step so they can consume more of that and so um you know the you know initially it was hey let's help folks build out these bespoke pre-processing pipelines to take say a non-disclosure agreement that's in a PDF you want to train a classifier model or something on it how do you get that into Json or I got a bunch of PowerPoints or CRM data or scraped HTML how do you get that to a point where you can start interacting with it and um and so for the past 10 months or so we've just been been in build mode um you know focused like a laser on ingestion and pre-processing a little bit of work on connectors but mainly on that transformation that file transformation work to get it into a base format that you can go use with machine learning absolutely fascinating it it really you know captivating overview the thing I to kind of maybe to break that in so pre-processing is this like are you thinking about like in the I know obviously there's like the PDFs to Eva kind of angle that we're so excited about learning more about but I think also you're hitting on like kind of like the ETL like when you are a data scientist and you take like a CSV and you look for like missing values the range of the distribution of your features is this the kind of thing that you think about like that kind of pre-processing can you tell me more about the pre-processing yeah and it's its own weird space here on pre-processing with files that contain natural language data and so we think about it in in internally in terms of like three steps okay the first thing you want to do is you want to partition and extract that the natural language data from a particular file right and so if it's you know a JPEG that's of a billboard or if it's a Word document or a text file or whatever you still want to detect you know you want to detect like okay this is the headline this is body text this is an image caption um because you want to be thoughtful about down down the stream about what you put into your inference pipeline um you want to extract that data and that's a whole process in and of itself of doing that um and so Matt so once you've done that once you've partitioned and extracted that data um the next step in our our world is is cleaning and how do you get reduce it down to like a clean markdown file or Json so there's all sorts of artifacts that that that that you know wind their way in here so like sentence fragments and what weird white spaces and beautiful characters and all sorts of just like little gremlins that just burn up time on on cleaning that up and so we have spent a lot of time thinking about that cleaning step and so once you have that clean step as a data scientist your job's still not you're not ready yet you gotta stage it and there's a lot of work that is in the staging still which is Last Mile that burns that time so concatenation chunking tokenization creating embeddings mapping Json schemas for you know maybe you want to set up a labeling task maybe you want to dump it a vector DB maybe you want to send it to um a hugging face pipeline whatever that might be there's a lot of that last mile kind of staging work that has to be done so partition clean stage and then you're ready for ML and all of that today is still completely manual and that's why 80 percent of data scientists work in this domain it's actually on this pre-processing stuff that they don't want to be doing they want to be doing the modeling and so we're trying to give that time back to them yeah amazing data science is always saying like all the data cleaning the most painful part but the most productive part and so yeah the pro the pain point is so clear and the problem being solved you know so useful um so I think kind of I want to take into this question of the why now question sort of like the how large language models are facilitating unstructured in this technology what's what's now making it easier I think um what large language models have done is really increase the demand side of the equation the right hand side where um there's a there's a couple attributes of them that that are really important one of them is that um from a generation and I'm not telling anyone anything they don't already know but like they're just a lot easier to use for generative tasks um right than um than old you know kind of the older um class of models and so it's just the time to value and the cost of value is a lot lower and the second is that they're a lot less fragile a lot less brittle to to the data that you're putting in them into the problem if you're using like in context learning or whatever and so what that has what that means for us is that our Persona has shifted a little bit so we were in a world where you needed to create these incredibly accurate pre-processing pipelines because if there's any noise or shift in the document template or anything it would throw off a lot of the um the downstream the structured data that may be fed into a knowledge graph or something Downstream now um there's since they're a bit more tolerant and there's a lot more that you can do with them um folks are saying Hey I want to use everything that we're producing like every file type everywhere we're creating um we're recreating you know natural language data that's relevant to our organization I want to I want to use it in conjunction with a language model right and so on our end um it's gone from lots of super precise pre-processing pipelines to hey how do I take an S3 bucket or an Azure block full of like you know just everything that you can imagine send it through unstructured and then dump it and weviate and now I can chat my data or interact with it that way right and so that's um shifted some of our like the Paradigm that we're approaching a lot of our engineering work and what we're prioritizing gotcha got you uh so I let me get your thoughts on I was I remember like with Lang chain and llama index when they came out they had like a data connectors Hub and I remember things like S3 Google Drive notion uh you know like or just like the PDFs so what was your reaction to that kind of thing is that that is that kind of the like a lot of the unstructured part is like connecting to these data sources as well as the Transformations oh yeah I mean Jerry and Harrison have done just awesome work there and um and unlocking a lot of this uh the potential over here um we're looking to kind of build on their shoulders for Enterprise production grade deployments um and it's just a slightly different sort of take on it that's feeding into orange engineering requirements and so what that means in Practical terms is that like we're I'll talk about our connectors and what how worth thinking about connectors versus data loaders right and the popularity there and then also what we're doing on the transform side that's not there and really this is a story about how do you enable all the data like the thousands of data scientists out there right now that are prototyping for them to go from prototype to production and I know Harrison's been spending a ton of time talking about productionization of agents and these other things we're we're in this narrow strike we're staying disciplined in this narrow strike zone of ingestion and pre-processing so on the connector side some some things that differentiate our abstractions from um from those data loaders are that um the data letters are kind of grabbing everything and moving them over we're able to um ascribe canonical um numbers to them until we can measure net new and so not grab everything and so we're not like you're not shifting a lot of duplicate data in um and then also thinking about scheduling and these will be these will be supported by us so if they break in the middle of the night unstructured will fix it kind of like a 5tran type approach but they're built for the ground up from the ground up with llms in mind and so um some other things like they real we're building them in such a way that they lend themselves to parallelization across like CPUs and so um and so that's like a whole discussion in and of itself on the connector side but once you connect and you grab that data that net new data from G drive or SharePoint or Azure blob or whatever um we're doing all that transform work that you know the partition clean stage right and then handing off like our goal is our is for our users to be able to take raw data grab it transform it and then hand off wherever it might go might be a vector database like leviate without having to worry about any additional data engineering it's ready to go and so we've gone from raw to ml ready um with an instruction and so um so I think that there's a huge innovation in I wanted to talk about this kind of like PDFs to weavia I think this is one of the most exciting topics is that you know we've seen this at hackathons and stuff as people they want to chat with their PDFs because it makes a lot of sense so could we talk about the Innovations in unstructured like how would you process a PDF with construction yeah we're we're trying we're doing a ton of internal r d work right now um in this area that um expands a few different approaches and we're what we're imagining in our uh for our kind of Target Persona or our folks that are pro like our organizations that are producing huge volumes of data today so we're thinking a lot about compute efficiency like some of the large organizations that we're working with today um create and process more than like a quarter million files a day and so you don't want to burn up all your compute Budget on pre-processing right and llms are even though you can use llms for some of these things they're about a hundred times more expensive than the approaches that we're taking and so we're like okay how do you render the highest quality data with the least amount of compute so that you can get to a chat your data type approach and so one of them um we're taking like yellow X and we're you know fine-tuning that on our internal label data we got like tens of thousands of pieces of label data that we've been curating over the last several months um and so that's you know yolax's creating the bounding boxes you know choose your OCR vendor act whatever um and then couple it with our staging bricks and now voila you're ready to rock and roll that's one approach another approach is like a donut based approach so uh ocrls Vision trends like swim um Vision Transformer where we've been doing a lot of work there some performance you know benefits and trade-offs between that and an OCR approach and then also implications for compute and and so that's like another one you'll be seeing those apis coming out soon our general pipeline which is um realized more on like you know more traditional CV and NLP approaches and that's available today and then we're also um pre-training Our Own Foundation model from the ground up um that's our own Vision Transformer where we've curated all the pre-training data and we've curated all the fine-tuning data and with the with the idea being hey let's train a model that you can throw almost anything at it and it's seen it and it can you know um transform it and render it with a high degree of performance that's in process too we actually are just starting we're just wrapping up pre-training and transitioning to fine-tuning on that and so the goal is by mid-summer to have a wide range of different kind of like arrows in our quiver that our users can can use to um to get over that hump and to get to that clean Json amazing a couple things I want to take apart for firstly I think just your knowledge of the the state of the art and OCR and I think swin Transformers can you maybe just kind of explain that more because I you know I I know I don't know that much about it I'm sure our listeners are curious about it sure I think I mean at a high level here a lot of the approaches to date have been okay um infer bounding boxes around um around document elements and there's some models that that work pretty well on that we we've been doing a lot of really focused work on on I mean folks the stuff that'll just put a lot of people to bed uh to sleep but um how do you accurately draw a bounty box around an image caption and differentiate that from a list from a header or a footer or any of these things so that you have accurate metadata tags so you can strip all that stuff out and focus like a laser on exactly what you want Downstream so you can curate your data um and so that's you know Vision you know um a computer vision model coupled with OCR to do the extraction and then you know you can do some last mile tuning before you roll on the um Vision Transformer based approach um it's it's really interesting so it's you know it's it's tiling up a um a particular image and um and it's it's using an OCR list approach with so it's a vision encoder and then a text decoder in order to do the jump from an Imogen to Json out and um and you know we were pretty optimistic about that approach both from a compute standpoint and a performance but depending on the needs of the user if the user is going to need like coordinates for traceability like where a particular extraction came from and which page and where on a document um you know those all those specifics um May necessitate different like a different approach and so our goal is to have like a nice wide menu of approaches that kind of balance features with with compute efficiency it's so interesting to learn about all this I mean I well I oh well I want to come back into the plans of the foundation model I'm very curious like how you're seeing training a custom Foundation modeling exactly what that'll look like for unstructured but I have to quickly I'm curious about like with the um so so with visual document layout and then I'm I feel like web scraping is a huge application of this where I'm like you know we also just record a podcast with the Kappa guys who took all the documentation blog posts of weviate and put it into one of these retrieval augmented systems and I mean it's really remarkable and yeah so so like you could you just point me to a web like if I just had weevier.io just this website how would you then go about getting all the data out of it like so like what we're doing is um instead of trying to parse the HTML like really effectively and because it can be like there's nothing there's some solutions for it that work pretty well but they're not like fantastic so what we're doing is instead we're um we're reassembling the web pages and then Imaging those pages and then um and then feeding that those essentially like screenshots of the web pages through the model right in order to do that um and we think that that approach and there's been some recent like really great Twitter threads and some some papers on this the compute efficiency of these computer vision models is coming down and so you can do that at scale and the performance of those extractions um is going to be a lot better than what you can do in just the pure HTML parsing yeah that that is incredibly exciting I mean I think yeah wow it's really like you're really painting a compelling vision for the future of multimodal I think about just like you know I like I like to read a lot of scientific papers and I think about how much information is captured visually in the diagrams but then the next question and this is something that come that I've heard so many times with the vector search is like so with tables you can also kind of like extract tables and you know charts and stuff so is so yeah I mean I don't know if it's the question is maybe pretty self-ob obvious I think you do the same kind of thing as the OCR models and all that so yeah so I think it would be a good transition to come back to that Foundation model um so training Your Own Foundation model to do this kind of I want to say document layout parsing is that kind of the idea yeah yeah raw to Json um for files that contain natural language and so feed in anything almost any file um and what the approach that we're taking is convert it to with this particular model of other options but with this particular model convert everything to an image Vision encoder text decoder and you get Json out and it has um instead of just one big brick of text which is kind of like what you have with the document loaders now right what we're doing is we're saying taking Snippets out and saying okay this is the title and having a title metadata tag and this is a body type body text Chunk so and so forth and so you can be much more thoughtful about one what you're wanting to store and then two what you're feeding into prompts right Downstream and so again everything at the end of the day is going to come down to speed and cost of compute and you can chew up a lot if you're doing this in production for an Enterprise um without without having done that work on the on the on the front end and so the the goal then with this with this larger model then is take huge volumes of heterogeneous data and then you can curate it very thoughtfully um before you decide to go spend a bunch of money on all of it within the LM and like Jerry over it well I'm going to get a great piece in medium just um just last week looking at at Claude at 100 000 token window it's a dollar a prompt almost every every time you query the model I mean if you're doing this across an organization right like that's going to add up in a hurry and so what could you do to maybe avoid having it look across a hundred thousand tokens and do you want to right yeah and that brings me to one of the topics I'm most excited to talk to you about which is this kind of like uh extracting structured data from unstructured data and to as a quick background like with weaviate Vector search in addition to doing Vector search or hybrid search we also have wear filters so if you want to say like you know where web page equals webaio or like where price equals less than or like price is less than 100 like these kind of symbolic filters they integrate with the hsw vector index to facilitate using this kind of structured data in addition to searching through the text chunks or bricks I think that I like that abstraction a lot think about as as bricks but so I wanted to get your thoughts on this one paper and I'll give it tldr quickly but just in case you know people for everyone out there so this paper is called evaporate code plus is the name of the algorithm and so what you do is say you're looking at Wikipedia pages of MBA players first you look at a subset so like Kevin Durant Jason Tatum LeBron James in it and so from you look at this whole thing with expensive large language model take out the symbolic attributes like college height years in the NBA let's say and then you say and then the large language model writes python programs to extract those feature those attributes and then you generalize the python programs to doing the rest of the Wikipedia Pages uh so hopefully that was a good explanation of the technique and if if it's not unclear I'd be happy to clarify but if you think about that kind of like uh bootstrapped like here are the attributes and then here's a way to extract the attributes from unstructured data yeah I think um once you get it into that claimed Json that's prepped and ready to go like we think there's a lot of complexity in connecting to everywhere that an Enterprise or an organization stores a lot of this data and then hauls along tele file formats and so we've been less focused to date on trying to adapt like gpt4 or a Bard or something like that in order to do all of that um because over time we may pivot if the compute efficiency improves and like the performance improves of that to say okay let's let's build an ingestion and pre-processing platform for the ground up to do that get it into that base format and then use llms on top of that um on on top of that uh that data um it's just going to be a lot faster and a lot cheaper and it's going to be more performant for the most part right now and I think you're seeing some of these challenges with like agents um right now and moving them into production like incredibly promising incredibly exciting nobody's using them in production really yet right and it's going to be a while and so like we welcome open source Community continuing to like to to work on that and like as they progress like we make we may pivot on our strategy but we're most bullish on putting the llms against the clean Json um and then asking because you're just asking a lot less of the model which means that that the structured data that comes out the other end is going to be a lot a lot higher quality gotcha I think I'm starting to understand about it so so say I have like uh you know like parquet files or CSV files or um you know maybe I have like a graph database like I have some data in neo4j and I come to unstructured and I say hey could you transform this into like I need a weevia database could you turn this into could you like read the documentation and learn about however you transform data from one file to another is that kind of the general thinking and so could you could you shed more light on what exactly that looks like because I'm sure you have a lot more and knowledge about that thing yeah on the other side we've done a lot of work on staging bricks on like um on like Json or markdown Transformations and so once we get it into a particular base format um or dfv um adapting it to what's needed and this is really like last year we were doing a lot of work around like label studio and snorkel and label box and like the long tail of labeling you know vendors and then also some of the NLP vendors and everyone had like a slightly different Json schema requirement and so we were doing a lot of work on these staging bricks which would take a base one and then adapt it to the the schema that's required for that particular Downstream application and um we're probably gonna need to do continue doing more more of this but again like our our kind of you know our strike zone is okay I have a file that contains natural language and I need to get that into a consistent format so when I can curate it for you know to create a knowledge graph or something like that right and so um that's that's really kind of the sand trap for data scientists today and where I don't think we claim that we've solved it by any means but we're we're you know we're running hard against that that particular problem set it really I mean I think like so so if I could understand the problem a little better so I know things like you know say I have some kind of special character in my unstructured text that when I try to decode it it's like you know ASCII can't decode this character is it so mostly thinking about problems like that like where you're it yeah the the and those are those are bound and so we have all those cleaning bricks around those those types of problems right that are just focused on that that we're trying to actually embed those cleaning bricks into these models and so these models are are doing the partitioning the extraction the cleaning um all in one Gap and then you can stage it however you can make those choices that you want with our cleaning I mean our staging bricks Downstream but that's um that's where like you know data scientists that are dealing with this natural language data that are needing high quality um you know high quality pre-processing that's where they're getting bogged down today on just writing custom python custom regexes including them and then if you change the document template a little bit everything breaks and they got to start over again right and so um it's that sort of that that step right there that um we think we're probably delivering the most value so is it correct like I'm thinking about maybe like with um like uh how with like say like no SQL systems you have like this evolving schema you can transform your schema as your data kind of evolves and you're adding new properties attributes and then it's kind of like the syncing up of that at massive scale it could be one other part of it what we've done is we've come up with like our own ontology just mainly because the one hat that doesn't exist yet um elements so instead of like um of columns and rows it's like okay like we we featured a lot of thinking that's like over the winter in the spring we're like okay like we don't we need a bunch of categories but we don't need too many categories um of like you know what what are common across receipts and Powerpoints and memos right and like yeah in the natural language domain where like you know some of them I mentioned already but like lists and headers and Footers and advertisement and captions all of these things and so what you end up with are tokens fans with a metadata tag to what type of document out like Universal document element that corresponds to and so you could say oh I want to like exclude all of the headers and Footers and captions from what I send to wevia because I don't want those getting sucked up into what I'm prompting against for example right with an llm um or you know I want everything or I just want to isolate some things and so you can that's that allows like this rapid curation rather than having to guess and check you know writing regexes and Python scripts and then changes then the Smith categorizing it um you can do that effortlessly with our toolkit oh so yeah that quick list of the Powerpoints receipts that helped me help to click for me a little more just like wow like how much how much information is visually organized I have to yeah like I'm so biased in my own like experience of thinking just about like you know archive papers and like blog posts is the thing but I mean can you tell me more about just like what you're seeing with all the information out there that's like a visual document layout like you I don't know I mean PowerPoints receipts are you I mean the the world that we like the the customers we're talking to are primarily large organizations and just think about like okay how many data loaders have all the all the developers that are supporting llama index and um and link chain and others come up with right and a lot of those are kind of like easier data set and then you have like you have Salesforce and you have HubSpot um on the other hand you have slack and you have notion and then you have stuff that's out there on the internet and then you have who knows what in G drive or in SharePoint and then you have historical things that are that are stored in an S3 bucket right that nobody's ever done anything with you have recordings of podcasts and things like that right you can run through your texts oh but the bottom line all of this like there's this whole world of private information that organisms are generating every day that's specific to their particular mission right and the pro like the exciting thing here is like how do we like for them it's how do we take what's out there and publicly available what we're generating internally somewhere like weeviate and then utilize it in conjunction with an llm hire to enhance productivity or to make better decisions and um maybe we write memos a particular way maybe we make presentations a certain way maybe there's this historical knowledge about how we do things and I could query you know I have this vision of being able to query it how do we take all that valuable natural language data that's everywhere and then get it into eviate for example right and and utilize and touch with llm like that's what's so exciting for these organizations right now but that's there there's some huge bottlenecks along that that Journey for them to to navigate amazing I mean I think about like how like within our company how information is kind of like visually organized and like notion and Confluence and you know maybe like even like a jira board you could visually look at it and then parse it out from that and even just like as I was doing did that you know let's list out some things I started thinking about like resumes that are visually organized and parsing that out yeah it's all just incredibly interesting so let me ask you about this kind of chunking and determining Atomic units for text so we generally with text embedding models try to get it to around like 512 tokens and um so like with you know Lang chain has this recursive character splitter thing as a part of their PDF ingestion or like as part of their data ingestion Library where basically what it does is you know it like if if you have like it has a few elements to it so firstly like if you have 1500 tokens It'll like junk it up the first 512 an x512 and the remaining thing send that all out to the database or whatever or it'll be like if you're trying to parse like a python file It'll like particularly have delimiters for like new lines or like def for the function like these kind of little things um so do you think there's a lot of innovation in chunking yeah I think like what we're at least for on RN I think Harrison's on the right track with um with the recursive text splitter I think where where we're going with that is um chunking into increasingly smaller smaller units that have the metadata tags associated with it and so you're not like let's use a web page for example let's uh let's say you scrape a New York Times um along New York Times article right and what will happen right now is like that'll get chunked into you know 512 token spans or something like that right but within that you'll have advertisements you'll have links to other articles you'll have image captions you'll have you know all sorts of stuff right maybe a text box about the author or about a related article and all of that mm-hmm in an ideal world like if you're if you're just doing chat your data it probably doesn't matter as much if you're wanting to continuously update knowledge graphs or like Salesforce or something like that using llms matters a lot more and so what you want right is to say okay here are these here's this chunk of like maybe body text has said that's it's split up by 512 tokens right but then I know that this is an advertisement and I know this is a text box and talking about something else and I know that um you know these other elements aren't necessarily related to the topic of this article right here that's corresponds to this headline and so you can just do a little bit of work on the front and like throw in structured um so that you're not asking nearly as much of the llm and that when you're ready to turn the corner from chat your data to continuously update knowledge graphs or automate um there's going to be a lot less noise going into that data itself yeah it's super interesting and I really I do want to like so you mentioned knowledge graphs and this is probably like the the most use of structure is to you know connect each chunk to how it's like related to another chunk in this kind of thing um well earlier I had mentioned this kind of idea of just adding symbolic filters to your chunks and then using leviate to Surf through these symbolic filters and then like it reminds me of say like the text to SQL kind of research where you could have an llm generate a query that adds symbolic structure to the semantic search query but broadly what I want to ask you about is what is your sentiment on knowledge graphs do you think knowledge graphs are you know like like if you're betting on knowledge graphs as being more or less important as you know in the next three to five years I think like what we see in the in the last six months has been the fact that like chat your data has captured everyone's attention and I think that that's going to continue and it's incredibly exciting what I also think is that for a lot of business processes and organization processes um you need a high degree of precision in order to hand things off to automation and that is in the realm of of knowledge graphs and so if it's classification to like you know in a or topic modeling on the one hand to entity extraction to relation extraction among entities um and then and then more right those are still like there's still a ton of manual human work being done in those areas that like the previous generation transform-based models were good at but really expensive to set up and really brittle and that like you didn't see widespread adoption of NLP across like Fortune 50 Fortune 500 organizations in large part because there's a huge economic problem around it the economics are changing in a really positive direction but um but with it's still not a solved problem and so I think like we have a renewed opportunity um as as an NLP Community to go and attack that and to drive that value but that value has not been delivered yet uh by and large right it's been it's been prototyped and it's been described and there's been lots of cool Twitter demos but like there's still thousands and thousands of humans doing things that that model can do well and so now we have a chance to go back and re-attack that and so I think you're gonna have like there's just you know the generative tasks and um and others where chat your data or prompt your data is better but there's still a lot that like structure like the creation and maintenance of knowledge graphs are necessary if you're going to hand it off from humans to uh to models I think um yeah so with with the knowledge graphs I I'm just really curious like um if we could unpack a little more of when you need to use a Knowledge Graph I don't mean to be like because I'm just kind of skeptical I honestly I'm like to me I think the best thing about a knowledge graph is that if we link together the how chunks are related to each other maybe we can use some kind of graph neural network to kind of have embeddings flow through the graph and maybe get a more contextual sense of the embeddings that's more so how I like a Knowledge Graph can you tell me just like about when you query like because you have the the relational tuples like Conor and then you know all the things about me says an example can you tell me more about like how you query knowledge graphs and how that's useful in business yeah I think um look there's events and there's entities um right and the entities can be organizations they can be skus they can be um like uh supplier relationships there's all sorts of things right and then there's all sorts of um structured data associated with that relationship to um to meaning like what are key events that happened um what are the transactions um lots of things that are described in natural language that aren't that not necessarily be in um in a structured format already that you need to go attach to that record um right if you're actually going to hand things off and so um let me give you an example we were talking with um an investment advisory firm um several months back and they advise like endowments and others on investments and then and they themselves are receiving every month hundreds of limited part LP reports uh McKinsey reports um like some like Consulting reports like all sorts of of data um some kind of in a gray area if it's private or not some really private and they have a whole team of people that are just reading them and then just updating them into a CRM so that they have like a hundred percent accurate information on any particular business entity at any given time to inform investment decisions yes you can put an llm on top of that and ask questions but are you we're going to trust it with a 100 million dollar decision probably probably not right and so if you're gonna feed in all of those PowerPoint decks the McKinsey reports those LP memos all that sort of stuff and you want to create a hundred percent accurate or 99.99 accurate records about certain Securities organizations um You're Gonna Want like very high quality Knowledge Graph associated with it even though you could chat your data how the llms demonstrate that level of performance of the previous generation of Transformer based models um like it's going to be a challenge right and so that's there's that's probably the more common NLP use case today than Goldman Sachs wanting to chat all of their data across all of their divisions yeah amazing I mean that example really like lit a fire under my ass of the thinking about I'm Goldman Sachs and I now the like the hallucination thing of you give it the thing that hallucinates so how are you thinking I mean that's a great example of a hallucination case where you just absolutely cannot hallucinate um maybe we could talk about this topic quickly about the Innovations and hallucinations like it seems like there are some things you like as the language models get better and better it's becoming less of a problem that maybe I could just um well I guess it's gonna do things like with the knowledge graph thing it's like is like do I need these Tuple like it's entity relation entity this is how I should have my data structured in order to hand it to the Goldman Sachs LM or can I just have it kind of be Ascent like a natural sentence that comes out of a retrieval engine to hand to the large language model yeah I think I mean you deal with a lot of like type 1 anti-2 problems here of like hey did I um is is what's being generated anchored on something real uh or on something you know that that bears a significant resemblance or another thing is like um is it missing something in the generative side right is there um an unknown unknown right and so um like both of those are are critical to actually saw like putting this into production and so I think like by and large hallucination has is a lot less of a problem than it was than we thought it was maybe six months ago from like an open source developer Community perspective from a production perspective um this is an area ripe for Innovation um because we might be able to make sure that like what is generated is anchored on something real but I haven't seen a whole lot yet around making sure that you have a totality of information of of the right information being rendered for you and like look you can look at like the you know like Google's llm versus opener eyes LM and some of the horse racing just in the past few days between them on on what they're returning from prompts from very similar prompts and it's still early days right and so that's not to say that these aren't incredible but like you know there's I think there's still a Chasm between pure like Jasper AI generative applications and replacing humans that are doing um knowledge generation work right today yeah amazing and yeah so I think it was a really great coverage of all these topics I think what you've hit on with the um particularly kind of the visual document layout is what originally drew me to instruction I think you have hit such an important part of this kind of flow of these retrieval augmented LM Vector database all the world that we're in and and yeah it was so interesting learning about all how you think about all these things like the data connector uh like you know flowing data from say like parquet into the Json as you describe all that is so interesting um before we wrap up maybe uh do you have any like exciting upcoming announcements or any uh tips and viewers for where to catch up with you yeah I'd say um you know we're in our community slack um welcome others to hop on in we have our our Engineers are in there full time answering questions the ones who are building that are answering questions correct and so we have an amazing team um keep your um stay tuned for um some new models to be dropping over the next eight eight weeks we're going to have a good steady drip of those and just welcome feedback we're trying to build through real problems through real real users and um and Leverage The Power of this open source community and so um grateful for any contributions or feedback anyone anyone provides but Connor thanks for having me on thank you so much for your time Brian and yeah not to not to keep rolling out on the Azure but I as I did my list and that that new when you mentioned the suite of the new extraction models we are definitely going to be on top of that at weba trying to you know get help people get their data into using these tools from unstructured and yeah thanks again thanks ", "type": "Video", "name": "unstructured_with_brian_raymond__weaviate_podcast_48", "path": "", "link": "https://www.youtube.com/watch?v=b84Q2cJ6po8", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}