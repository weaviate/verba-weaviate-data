{"text": " \nforeign[Music]air we are super excited to uh have allof you here and it's something that Ihaven't done in the first and the secondedition and I kind of corner pointed itout so a big big thank you for takingthe time to listen to us either live orlater or especially if you do it oncelive and then re-watch it because thisis such an amazing show big big thankyou for uh for doing that uh and thankyou up front for asking any questions uhand and all of that because this is thethe beauty of um this episode of will bethere that this is a live show so I amgoing to encourage you to like pleaseplease ask questions uh and don'tnecessarily wait until the end of thesessionif you can ask questions at any time andI can do this thing like I'm just nowlike hiding and showing Banners at anytime I could actually pop up yourquestion and say like hey Connor thatwas like a really cool question for youor Zen I think I have something for youand that definitely I I really reallyencourage that uh for from all of you uhand then at the end we probably will doa bit of a q a as well so if you knowyou didn't feel very encouraged duringthe session we'll probably do that andthen that's also something that Conorpointed out like somebody actuallytweeted at us uh saying like hey whydon't you do a q a like we want to do qa that's why we are all live hereum a little warning I don't know howthis is going to go but right now me JPand Zen we are in Prague we came herefor a conference called devrocon so it'sbasically a conference for a developerAdvocates like us uh which is the goodside the bad side is that our internetcan cut out at any point so if suddenlyI go likethis that means that I'm not being rudeit's just like my internet went wrong orI'm just playing jokes on you so for nowthat's it this is not happening sothat's enough about the intro and and Ihave like uh we have like quite a fewthings to show some of them are inpreview not everything is super livejust yet um but we we have a whole bunchof really cool things for you to observeand watchum so this is really uh cool and thenthat's a little message from Connor uhso we should be definitely uh doing thatand that's how I can also show you aquestion so without any further Ado uh Ithink we have JP scheduled to uh talk tous aboutdata importer JP take it awayawesome thanks question yes um sohopefully my internet stays stable thiswhole time but so far so good I'm justgoing to keep my fingers crossed duringthe demoum this POC some of you might have seenbefore this really came about whilewe're on our location in Italy and inbetween all the team bonding and havinglovely food and whateverum we got to work on some cool projectsand one of the things that um themotivation for this for me was thatbecause I'm new and I'm learning a lotin terms of what's we've yet and how doI you know uh just let all the wonderfulthings you can do with itum so for example trying differentmodules and what have you I wanted tocome up with a way of quickly importingsome data because I'm repeating thatprocess over and over and over again andmaybe trying different data sets rightso the motivation behind this was how doI automate that process it's a littlebit easier and repeatable and at thesame time you know automated sort of tolike press the same or come up with thesame schema over and over again andwhatnotum so I ended up building a little uhproof of concept app based on streamlitum Sebastian if you wouldn't mindsharing my screen I'll try andabsolutely so much thanks you're verykindum so this is the um I just hacked thisreadme file together so it doesn'tactually look like that in real life butthat's the repo locationum that's where it lives it lives undermy username in GitHubbut yeah it's a fairly simpleum streamlit app and it's not really alot of lines of code so if I have a lookat uhit's um it looks like a lot of lines ofcode but it's just still under likecouple hundred or okay under about 250lines of code so it's not very much interms of uh the volume and a lot of itis just kind of formatting andum kind of making it presentable morethan the actual logic of what it doesand to run this hooks to run thisumright and you basically need to have acouple of packages openum sorry again installed in your virtualenvironment and really the only uhlibraries you need a webiate clientstreamlit and pandas obviously you needpython it's a python app so you're goingto need thatum so once you have these librariesinstalled in your poetry virtualenvironment or um kinda like I've gothere whatever all you would need to dois obviously need to clone the repo andyou need to get lfs enabled becausethere's a data file that I've saved inusing gitmet.fs as well but you justsimply run uh streamlitI'm going to screw this up probably butlet's hope notum dot piandwhat it does is to openthe app and at this point I will have tostop sharing this screen and switch overto my other screen so I think I had tostop the screen and then present oopssorrythe excitement of liveum hey that's part of the game that'spart of the game you knowdrop outfantastic so that's what the app lookslike it's uh I tried to make it sort ofas clean as possible and um uh it doesreally have all the Hallmarks of a POCapp because I added this really sillyjoke that doesn't even make a lot ofsense about Wizardsum and in order to connect to a weeviateinstance I've got one running in thebackground[Music]umlocalhost 80.you just type in the address here in theURL and it'll connect and you'll see thesubsequent options underneath so you canhave a look at things like a schema andthere's nothing in there you'll get thislovely test message and I can see hereand then there's a couple of optionshere right so you can go to demo datasets and I've got a placeholder dataI've got a couple of place or data setson the right hand side these are thingsthat you know I think would beinteresting to upload and I'll work onthat probably later but I've got thisCorpus of wine reviews from kaggle andwhat you can do is set the number ofobjects input so the default is set as500 but I can just make it you know 50.um and just click import right andthat'll connect to the instance it doeshave a predefined schema and upload thedata so if we scroll back up and goupdate database stats and expand thatblog window below you'll can see theschema and some example objectsso that's uh designed to streamline thatprocess and if I have different modulesenabled then what have youum that you know I can try and see whatmight or might not work and then startquerying it to see how those differentvectors and different Vector space andvectorizers might change what happensthe other thing I've tried to hacktogether here is the ability to importyour own dataum uh I haven't got any data here sothat's on me but the idea here is thatsorry I reset this repo and then when Ire-download I realized I deleted mylocal data but you should be able toselect your local Json and CSV files andit'll Auto generate a schema based onthat it'll kind of give a best guess asto if it's uhum if it looks like a bunch of integersfrom pandas it'll infer that and give itan integeralleviate file data type it would be anobject data type and whatnot and youshould be able to click just a fewbuttons and import data that way as wellum and also um JP yeah sorry tointerrupt How brave do you feel becauseI could send you like a dummy data fileuh to see if that works you're the trialyeah we could do let's let's how aboutwe Circle back to it and thenum I can try that while because I Iguess the thing is on my phone I'll haveto be downloading data and I don't knowhow Keen people are to be like watchingme download that off slack and then putit somewhere and then copy it but yeahlet's Circle back in it's a it's a twokilobyte file okay no excuse eightkilobytes apologies okay let's do it I'mdropping onslackum it's a date like a handmade data setI've created because I'm playing with itso it'd be cool to see what happensum do I get fired if it doesn't go livelike if it doesn't go well is this likethatum not on live like we'll do it afterthe call I like how non-committal youwere about this by the way you had tolike think about it for a long timewell you know like so here's the funnything he is just like to behind thatwall so like it's not like I have to gofire him I can go and Shout at himstraight upthat would be a first time right nope nodidn't work yeah so what I will say thisis that your handwritten Json file isnot the way I expected it so that'sthat's on you is um my take on thiswhole thingum so CSD csv's work better because Ifound that when I was pausing json'ssome of them were like nested by defaultum when I paused it and yeah it was abit tricky but csv's work work muchbetter on that oh okay okay sounds goodbut um the skeleton is kind of there andI'm planning on working on it when I getsome time uh to to say add a dummy imagedata set and what have you and and theidea is that you know now you've got uhyou know we're able to populate the theinstance here just a you know a coupleof seconds and if I'm making changes tothe instance and want to reset it andthen upload data I can just do that youknow reproducible consistent Manner andif you have your own data set you canstart from this and edit it and then youcan have your own interface and thisworks with one of the WSS WCS instanceas well so I won't go the same processagain because it's just going to be meclicking the same buttons but if youreplace this URL with I will do thisthoughum I think it's called import tests andmy networkit'll connect to that right then if I goupdate database stats now you can seeit's gone back to an empty instance andI can do the same thing in upload dataso yeah um that's what it is uh pleasefeel free to check it out and if you youknow find it useful I'd love to hearabout like how you would use it and ifit's actually addressing any kind oflike you know problems or annoyances youhave in your Dev environmentcool thanks JP and I think the intentionof it as well is like it's not like thisexact tool will become a productionready specifically application but it'salso I think something that we areexperimenting with and the kind ofthings we should have in the future oncethe real data importer arrives uh it'snot like this is this isn't real this isstill real but like you know somethingthat we can we go like oh yeah uh wewill definitely put our name behind ityeah I mean like it is uh yeah for sureit's a real app but it's not too farfrom just the idea of the ephemeral ideaof an appum so it's it's it kind of does the jobbut for sureum yeah it's I'm not sure it's going tobe production ready in its current statebutum hopefully it sort of gets theconversation going and gives you someideas about what might be useful downthe lineyeah absolutely and I think that thatfeedback for example on the ux of it orthe kind of features or maybe the kindof files that you may want to importlike JP was just saying oh I've testedon CSV but like Json you know differentthings may happenum I know like I think we could belooking at like the Json line files oror something so that kind of feedback issuper useful as well so even if you justplay with it and then try it out andthen say Oh we work well for my scenariothat was so and so then that helps usbuild that like uh the big thing lateryeahyeah I think so and the only thing youwould replace in this instance withyours would be the parser right for thefile and then once that's done it'llread the file correctly so yeah yeahjust like Vivid it can be modularizedright so if you then suddenly have likea very specific file format right youmight be able to just like add that uhimporter right they not important butlike the file parser right yeah if youlook in the codeum the file Plaza is modular so if itsees a CSV it calls a particularfunction if it sees a Json it calls adifferent function and you just replacejust a few lines of codeum I got a little bit lazy with the CSVand it loads it as a data frame whereaswith Json I try and get it to user tryto get it to user generator so itdoesn't obviously you know Json filestend to be quite large if you're readinga corpusum but yeah so you can adapt that toyour data set size if it's small enoughjust if you use a data frame it'll saveyou a lot of time because pandas are infor a particular data data type based onthe kind of data you haveum so did it up to like do as much workin terms of identifying data typesyourself based on some samples of thedatasounds goodso and and I know you we had like alittle request slash announcement slashuh thingy you know I can show the banneryou know if you in caseokay sorry yeah um I was just assumepeople would have had enough of me after10 minutes so uh so if you did get thevb8 newsletterum that went out last week I believe youmight have heard about this and the uhwe've been making some updates to ourdocumentationum the authentication part in particularand shout out to Dirk who's been helpingme a lot a lot with this and he's uh inparticular working on our clientsand the authentication capabilities thatthey will support natively so it's beendoing a lot of really cool work on thatI've been kind of learning a lot abouthow oidc Worksum one of the things we would like tobetter understand is your as webiateusers your needs and your desires as towhat are your authentication needs whatis your preferred things likeauthentication provider what is yourprefer what are your preferred clientsin terms of languages is it python wethink it's mostly python but would loveto hear what the breakdown looks likeum and and you know if you are usingoidc as your authentication provider ormechanism what uh YDC what they callflows which are different kind ofworkflows or mechanisms by which theyprovide the tokens right so we've got asurvey it went out along with thenewsletter but um I'll just drop a linein the comments I believe with the linkto survey so if this is relevant to yourinterests of course uh you knowum if you're not don't worry about thistoo much but if you would love to hearfrom you um please feel free to aresponsive survey or just even catch upwith us on our slack Channelokay I think that's perfect thanks soyeah so that's I'll drop that up hereabout now YouTube in a couple of secondswhen I get to it yeah I thinksorry JPum all right this is excellent this isexcellent so uh next up we have Zen uhZen what are you going to talk to usabout hello can you all hear me yeah Ican okay awesomeum so today I wanted to talk about areally cool data set that we've been uhworking on and this is a data set frommeta uh their AI team formerly Facebookit's called sphere so I'll post a linkfor this in the chat here for peoplethat are interested and this is aum this is the announcement that they uhthat they posted when they came out withthis data setand and you should be seeing that in thecomments uh soon here but the ideabehind fear is that it acts like aknowledge base so that you can trainlarge language models uh on using thatknowledge base so where does it comefrom simply stated basically engineersat meta scrape the web they scrapedabout 130 million uh news articles andthey chopped up those 130 million newsarticles into 100 word uh sentences orSnippets essentiallyand then they uh release that opensource that as a as a data set so intotal there's about approximately 900million sentences or statements thatconsist of the sphere data set and thereally cool thing about the sphere dataset is that it's one of the very fewlarge-scale data sets that have not onlythe sentence so something likeMcDonald's is an american-based uh fastfood chain it will have that sentencebut it also has the vectorizedrepresentation for that sentence soyeah 900 million very large scale soSebastian if you share my screen here Ican show you exactly what one data pointlooks like in this uh in this data setyeah so there you can see that everyevery data point has an ID just a uniqueidentifier it has the URL the thearticle from which they got thatsentence it has the title of thatarticle and then it has the raw uhnatural language what what are those 100words that consist of that data pointand then it has the vectorizedrepresentation for that uh for that uhfor that text right so hey Zen um smallInterruption can you like zoom in justlike one or two steps just so thateveryone can see and then pleasecontinueohis that betteryesyeah sookay I can zoom in even further if it'suh here let me let me I think should befine yeah Okay cool so let menice so this is just one you have 900million of these guys this is just oneuh one application right so this is theactual 100 word uh 100 word entry andthen you have the vectorizedrepresentation for this which is uhwhich are all these uh all these uhfloating Point numbers here right butthe main idea behind this the reason whythis data set is special is that you canconduct hybrid search on this data setso let's dive into what that means youcan use conventional uh word matchingsearch so you could uh you could saywhere in my database do I have the wordsHigh Commission of India in there and itwould literally do like a regularexpression type word matching and itwould give you all the uh returned uhobjects but you can combine that alongwith semantic search where now you cansayshow me all the articles that have thesewords in the in the article itself orthey have these words in the name of thearticle but also they they are theclosest in in terms of the vector spaceuh to to a particular article right soyou can combine both Vector search aswell as uh as well as your word matchingandso this is a very large data set whereyou have both the vectorizedrepresentations and the text fieldrepresentations of uh of yourum of 900 million statements essentiallyand so the reason why this is why we'relooking at this is because the amount ofapplications for this uh large code baseare are Limitless essentially you canthink about applications from fake newsuh all the way to question answeringum to hybrid search I mentioned one ofthe main applications of this is TheBenchmark algorithms for hybrid searchright and so what we did and we have ablog post out about this that came outyesterday so if you're more interestedin this you can read more about it andI'll post that in the in the YouTubecomments as well but essentially what wedid iswe wanted to make this data set veryaccessible meta AI open sourced it butone of the um the problems that it it'svery hard to get it set up on your owncomputer I tried to get it set up on mysystem and it was quite difficult sowhat we did is we took the sphere dataset and we chunked it up into 100 000data points a million data points thatessentially different size chunks andthen we released it so that you can goto you can go to the blog post and youcan get different chunks of this dataset and then you can play around with itand so in our blog post we talk abouthow you can take different sizes ofthese uh of this data set you can bringit into a python environment or a sparkinstance and then you can populate uhweaviate you can import this data setinto rewind and then you can do allsorts of interesting uh search orfunctionality that weva has on thesphere data set and so if you'reinterested in that let me let me postthelet me post the link here actuallyI'm curious if that's going to workbecause uh it seems like YouTube doesn'tallow sharing links so maybe what wehave to do is uh update the descriptionof the video after uh we're done andthen possibly tweet those as wellum yeahyeah we were just a bit too optimistichey it's part of the learning processyeahbut yeah that's that's uh essentiallywhat we did here is we took uh we wetook this data set that is reallyinteresting to work with we chunked itup so that it's easily accessible and uheasy to import into python or spark forthe uh wider developer community andthen we we had some fun with it webasically imported this data set into uhintoum into Eva and then we started doinghybrid search on this so all of that isdetailed if you want to follow along allthat is detailed in the blog so you canyou can read through it as wellperfect this is excitingum I know you have another topic thatyou want to touch on but uh I would liketo maybe Circle back to you uh becauseuh you also just mentioned uh hybridsearch which is sort of like a like acue for Erica because I know Erica youare looking into that yesum do you wanna like let me maybe I canshare your screen and then uh you cangive us a bit of insight of what this isabout yeah be greatall right there we areall right so what hybrid search does isit combines Vector search withtraditional keyword search in one queryum so yeah I'm sharing my full screenum so here this is the GitHub issue forhybrid search and it is going to bereleased in the 117 which is planned forDecember 20th I believe uh this monthum but I will say I can share the linkin the YouTube comments I think you saidit doesn't work but we can handle thatat another timeum but if you would like to vote onadding filters to bm25f and hybridsearch you can click on this Emoji hereand that's useful feedback for us to useand prioritize all right so back to theconsoleum I have this query on how to fish anAlaskan pollock and what this is thealpha if it is set to zero at the momentit is done search and if it is set toone then it is sparse um so it's reallyum one I mean the whole point of showingthis is that it's very intuitive andvery easy to use hybrid search so if Ijust run this and if I have Alpha set to0.5 this is hybrid search so I was usinga mix of both so you can run this andthe first result is The Ultimate Guideto Alaskan pollock fishing which isexactly what we're looking for right soit's just extremely easy to run thishybrid search and if we are curious onhow it is scoring it um we can use thisadditional filter and then type inexplain scoreand this will be cleaned up just alittle bit but I just want to like showwhat it looks like right now um so wecan see that bm25 has contributed thisamount and whereas a vector it's missingin this little part but it's supposed tobe Vector is scoring at this amount ofThe Ultimate Guide to Alaskan publicfishingum so yeah I just wanted to show a quickpreview of hybrid search and how easy itis to use in the VP console and I willshare a link to the GitHub issuesin a fewthank you this is exciting that and Ireally love how like the kind of thingsthat we're introducing on the Fly andthen the kind of things that Vivid cando and is really exciting and I actuallyreally love the fact that you were ableto like do that preview of of what'sgoing on because this is also a goodmoment for anyone to give us feedbackright so if you look at this query andyou're like well this looks actuallyrather complicated which I don't thinkit is actually personally it's prettystraightforward hybrid query Alpha butany ideas any suggestions like thatthere's still time for us to maybe acton it and then so feel free to give usfeedback to to let us know or just saylike hey double thumbs up this isamazing we love it right like so uh yeahabsolutely so uh thank you for sharingthis Erica because uh this is so coolthis is this is mind-blowing and thatcomes from a person that didn't know thedifference between Spurs and densevectors uh six months ago so now I'mlike so excitedbecause you're alwaysnice excellent excellentum yeah uh let me see who have we gotnext uh soumso Connor you you're doing like a anoverview of like what's new inuh in Ai and research so how about we dothat right uhtell us what's new in in AI in Novemberright like what papers have you beenreading uh what what really you gotexcited about because this is like yeahpretty exciting stuff as well awesomeyeah well I think uh firstly theobviously the the chat gbt is the big uhis the big thing so but kind of I wantto start with this new thing called Langchain so uh Lang chain is about how weuse prompting and kind of this glue inthe middle of how we use large languagemodels so as a quick example uh recentlyyou know I was writing this blog post onref to VEC and it had some errors in itJP Sebastian Erica Zane they helped meclean up this articleand so I was thinking I wonder how welluh gbt can edit text and I've beenfascinated with this idea of editingtext for a while now because I think itmakes a lot of sense is sort of how youwould use a language model to generatetext so the idea of Lang chain is tohave these sequential prompts so firstyou say correct the grammar and spellingthe sentence and then you say break upany long sentences into two or moreshorter cleaner sentences and so I'vepurposely kind of uh deleted some of theuh like made the text incorrect a littlebit like say ref the back is shortdeleted the They yeah should I be sharing your screenby the way uh oh sorry I didn't knowapologiesokay is it showing yes it is now yes abit small but we can see your screenokay great uh so I'll zoom in quicklyand explain this um so Lane chain isthis new library as I just said it'slike starting from the beginning but soit's like a way of uh chaining togetherprompts so you have sequentialgeneration of language models as well asother things that we'll get into next sothis is a quick example you give it theproblem correct the grammar spelling thesentence Pat paste in the paragraph thenit'll take that output as input to thisinput and then break up any longsentences into two or more shortercleaner uh sentences and when it runs Iguess it's running slowly but anyway soit'll iteratively edit the sentenceso I think and then you can kind ofmonitor your billing with the open Aiand I think generally is just anincredibly interesting kind of thinguh so I wanted to start off by showingthat quick example of langchang with thesequential prompting and that and thatkind of idea of text editing to sort ofset the stage for this idea of havingiterative prompting and what that evenmeans but sort of the thing that's moreinteresting especially in connection toalleviate is this problem ofhallucination this is like the coreproblem of these large language modelsso I saw this on LinkedIn somebody saiduh chat gbt is a search engine forget itthe question is does Lori Mark Kananplay for the Chicago Bulls and chatursays yes he plays with Bulls when reallyhe's been traded to the Utah Jazzso but what you can do is if you promptit by saying here's a history of Lorimarkan's NBA basketball career then youcopy and paste Wikipedia data then chatGPT will read the thing and tell you theright answer so the idea of levia wouldbe to retrieve the context that's neededto ground Chachi petite and factuallyrelevant informationso what we're looking at with Lang chainis how to really integrate this tool useI mean there's of course the generalidea of just you use the query toretrieve and then you just concatenatethe retrieved context with the query andthen just generate from there and thatmight work pretty well with just ageneral prompt like please ground youryour answer in this factual informationbut there's also this exciting idea of alanguage model tool use so we view ithas different kinds of queries so sayyou have all sorts of different objectsand you're about to search through anobject and maybe the language modelwants to say how many documents do Ihave or how many uh I don't knowparagraphs like imagine these dataschemas with weave yet where you canhave multiple classes you might want torun an aggregate query or you might wantto run different kinds of hybrid searchor vector search with wear filters allthese different kind of filters that youcould use and you can describe aninterface that explains the tool to thelanguage model and the language modelwill kind of know how to use it and thiswhole idea of self-ass Chain of ThoughtpromptingI think oh this is just incrediblyexciting so let's get a little more intochat gbt and why it's different fromgbt3so the the what they say in the paperinstruct gbt the language modelingobjective predicting the next token on aweb page from the Internet is differentfrom the objective follow the user'sinstructions helpfully and safely sothus this is this term called alignmentthat's what that's like kind of thebuzzword that describes his research isaligning the language model with likewhat we wanted to do we don't just wantit to predict what is most likely wewanted to do useful thingsso the way that they ohsorry I deleted this slide I guess ohall right anyway so what they do andI'll just describe this quickly is theyhire a team of 40 expert uh 40annotators and so what these annotatorsdo is first they take the prompt thatgpt3 received on the commercial API andthey write their own answers for it sothat's used as a supervised learningBaseline so you take the original gbt3and you fine tune it to copy the answersthat the humans had written then whatyou do is the the fine tune gbt3 isgoing to generate continuations and thehumans are going to score are going tocompare two of them at a time to train areward model and then that reward modelis used to continue training the open AIthe uh chat gbt with proximal policyoptimization so what proximal policyoptimization is is uh the the thelanguage model makes a sequence ofdecisionsand eventually it receives a reward sothe supervision is that reward for eachof the decisions and it updates asparameters like thatso I was a little curious about you knowhow much data did they collect I thinkthat's kind of the key question is likewhat's the sample efficiency of thisreinforcement learning from Humanfeedback like how expensive is it totrain these annotators and get this databut they don't give you in the paper theexact number of annotations they justgive you the number of number of thatthey have I think it's safe to say thattimes 40 is the upper bound maybe I youknow assuming that each of theannotators annotated them onceso that's kind of the key idea and yousee this difference in gbt3 compared toinstruct gbt on the question of what isthe purpose of the list C in the codebelowso kind of pivoting topics I want totalk a bit about this idea of the chatgbt that's been trained on internet textcompared to say some large languagemodel that's been trained on likespecific massive text so Galactica wasthis model for meta AI that came outtrained on over 48 million papers andthey actually took it down because thishallucination problem they you know itwas people come to it for scientificfacts and then it's when it does theincorrect generation it you know peoplewere kind of irritated with thatso this is kind of an example you knowit's trained on this kind of things likeSmiles strings code latex expressionsand it's just a different kind ofpre-training Corpus but I think the keyquestion more so is Galactica versuschat gbt do we think these largelanguage models need to be trained onspecific data or do they just betray arethey just train on any kind of data andthen we can just retrieve the specificdata and then just append it in thecontext and the language model is thegeneral Reasoner rather than some kindof specific knowledge holder and I thinkwe're leaning more towards the secondParadigm where you have the chat gbt canreason across any data and thensomething like weave retrieves thespecific data to the specific problemyou're working on and tools like Langchain maybe glue in the middle part ofit and we'll see how that all developsand of course weaviate is agnostic toexactly which part of the thing you plugin you could you know plug in your ownlanguage model or you could use open AIsimilarly with the retrieval modelsso now let's talk about model inferenceand I almost talked myself out ofleaving this joke in here so hopefullyit goes well so model inference is superexpensive so you've seen tweets from SamAltman saying that they're spendingabout 10 cents per every time someoneinterfaces with chat gbt and so I thinkthis is a really interesting thing toconsider as I've been playing with theLang chain I've been spending about likeone to three cents per generation but Ithink it's definitely something thatwill change the way you use these thingsif you have to be like every time Igenerate it it's going to cost me like10 cents to probably change the way thatyou use chat GPTso there was a lot of stuff on modelinference that came out I'm just goingto focus on one quick nugget of it uhyou know there's papers from Google onhow they're running 500 billionparameter models and really detailing itbut uh this this kind of so one of theseresearches is from neuromagic which is acompany working on sparsity forinference acceleration that I'm reallyinterested in and the headline of thisthey're you know they're using secondorder pruning this Hessian matrix thingis super interesting but the takeaway isallowing us for the first time toexecute a 175 billion parameter modelinside a single GPU so you knowobviously if you could run this on asingle GPU that is a breakthrough in howexpensive these models are to runso I spent last week in New Orleans atnurips and this talk was the one thatstood out to me more than anything elseinteraction-centric AI so we've seenthis kind of this terming ofmodel-centric data Centric uh where youknow I didn't even realize data Centrichad run its course I thought it wasstill the new thing but now there'sinteraction Centric so model Centric AIis about you know these metrics and yousay they he shows this example where youknow you're trying to generate imagesand you're solely measuring the qualitybased on you know how realistic theseimages are with metrics like Inceptionscore on these ground truth image datasets then data Centric AI came along andI think startups like snorkel AI waswere pushing this term really heavilywhere you know it's it's about let'sclean up this data sort of similar tothe idea of Galactica versus chat gbtwhere it's like instead of just languagemodeling something like the sphereCorpus let's language model likescientific papers and smile strings andstuff like that so data Centric I AI wasall about you know curate that the dataand organize the data really well andnow interaction Centric AI is I thinkquite a different Paradigm it's aboutimproving the user experience making itlike usable and making sure humans canuse the AI for what they are doing so uhDr Kim explained this by showing thisexample compared to someone who'sinteracting with the generative model toproduce an image you know you do thiskind of prompt thing and what are thetrade-offs of this user interface youknow it's sort of intuitive as you youknow prompt the dolly tothen there are problems with it liketrial and error you know lack ofspecific feedback and these kind ofthings is like The Guiding line for howwe're building these systems so with oursearch systems are we just going to tellyou the ndcg on you know the beerbenchmarks or are we going to tell youyou know maybe plugging it in with thatgbt and measuring how long it takeshumans to complete tasks and I thinkit's moving more so in like a humansubject testing kind of Directionin addition to these kind of automatedmetricsand then it you know it's the idea ofaligning models with user intent thealignment that we discussed from openaiwas obviously this reinforcing learningfrom Human feedback but now I want totransition into another paper that cameout on aligning models with user intentin our search systems which is Taskaware retrieval with instructions sothis they're setting the problemparticularly where you're using a searchsystem and you're going to explicitlydescribe your intent as well as yourqueryso they give the example of you searchimplementing batch normalization inPython now you could be looking forother questions you could be lookingdirectly for the answer or you might belooking for the python implementation sothe it's it's sort of like balancing allthese intents in one in the querycompared to where if you like explicitlytell it I want to find python code Iwant to see who else has asked questionslike this or just give me like the youknow like the Stagger overflow answerthat kind of thingso they gather all these informationretrieval data sets and they writeinstructions so natural questions hasthe instruction retrieve a Wikipediabeing the data domain paragraph theunits compared to say like sentencedialogue response that answers thisquestion which is the intentso for all these data sets like you knownatural questions arguana SCI fact theywrite this long list of explanations ofthe intent behind these tasks that theyBenchmark and you see how they have sothis is these are the instructions usedin training and then say these are theinstructions used in testing so you havetrain tests not only data splitting butalso instruction floodingso I'll go quickly they you know theytrain the models particularly whereyou're encoding the instruction as wella couple different architectures fordoing that whether it's just like a buyencoder style of doing it or whether youhave the cross encoder style of doing itand then how you kind of sample thenegatives and the negatives could beeither you know a hard example where uhyou know it's like umuh it's like a different kind ofargument or it could be like an inverseof the instructions so the instructionis particularly being perturbed so thenegative is that this thing didn'tfollow the instructionso here are some of the results and I Ireally think the most interesting thingis what we talked about in the firstmovie theater show which is promtogether and comparing it with this kindof approachso prom together is the idea they usethe large language model to generatetraining data for specific retrievalmodels for specific intents and at firstI like that idea a lot but now it soundssort of maybeoverly complex compared to if this ideaworks I think this is a bit of a simplerway to do itand then this is the ablation of theimpact of instructionso then kind of another paper that cameout is about how we measure informationretrieval benchmarks and I really likethis is about the cost for one millionqueries you see say bm25 is really cheapuh and I actually the problem with thisis I'm not super familiar with all thesetechniques uh so dessert ance I thinkthese are like vector retrieval methodsand I'm still kind of taking apart thedifferences in The Colbertimplementations like how the plateengine speeds it up and displayed uhdisplayed vectors I think is like alanguage modeling uh head on top of thetokens but but I think what's moreimportant is just this kind of focus onnot only how well it can retrieve thingsbut how much does it cost you to do thatso then kind of another idea inefficiency is this paper called Citadelwhich is a way to improve the efficiencyon Colbertso here are the results are showing thatyou know Citadel achieves really highyou know performance as well as low GPUlatencyso the idea of Colbert is in addition tojust kind of so the original Vectorsearch idea is you pool along all thetoken representations to get therepresentation of the question therepresentation of the documents and thenbang you search through the passages thepassage vectors in Colbert what you dois it's a re-ranking thing where firstyou would search through this and thenyou have the maximum similarity of theinner product with the token vectorsattending to all the other token vectorsso uh you know it's probably gonna belike 512 token vectors so you have avector Source through that so now whatthey're thinking about is like uh wespeed probably doesn't need to attend toall these other tokens how can we havesome kind of sparse Activation sothey're using the same idea as splayedwhich is this kind of like add a masslanguage modeling head onto the tokenrepresentation to get some kind ofsparse Vector because Mouse languagemodeling is going to predict what tokencould replace it in the length of thevocabulary so it kind of produces theselike keyword representation similar tobm25 so they're using that to have thesparse routing and make Cobra moreefficientuh so then I'm super excited about thisidea because I think it's veryinteresting to just be learning moreabout hsw and the you know the vectorindex structures so the key idea in Ooddisk a n is that when query data isdrawn from a different distribution soimagine that we build up our Vectorindex of image embeddings and thequeries are text embeddings so it'sactually disk a n face inverted file andhsw they're going to lose a lot of theirperformance Advantage when this happensso the authors proposed someimprovements where in robust vamana youknow when you have the graphoptimization step where you're makingsure that with the greedy searchheuristic you can reach all the uheverything in the base set you're goingto add a small set of the Ood queries tothis kind of graph connectivityoptimization that helps a lotI don't quite yet understand how they'vemodified product quantization butthey're adding pivots to it optimizingwith gradient descent and then they alsohave this parallel order thing which isright now when you're doing the graphtraversal you pretty much you Traversethe hnsw graph and then you add the outneighbors into the memory and this isusually done via Random Access which hasa lot of large cache Miss rates so theyhave this graph reordering to kind ofput the out neighbors together in thecache so you retrieve it efficiently andthis results in plus 40 latency plus 15recall benchmarked on these data setslike searching through image indexeswith text and I think this is extremelyinteresting for us to be mindful of aswe're you know building up multimodalsearch indexes or so you could also havejust like you you have a text index ofnutrition facts and then the queries arelike all about cancers something likethat I know it's kind of a dark thing togo to but it's so it's it is adistribution shift in the queriescompared to the documents and it wouldcause this kind of performancedegradation in the index so to summarizeI'm super interested in chat gbt andlarge language models Lang chain puttingthe glue between weviate retrieving thecontext and chat gbt you know reasoningacross the context and I think it's acomplete paradigm shift for how we'rethinking about how you use custom dataand deep Learning Systems I think thisinteraction-centric AI thing is a is ahuge step in how we're thinking aboutevaluating these systems I'm superinterested in these estimates of exactlyhow much these different retrievalstrategies costs and then tricks likeCitadel toyou know make it more efficient and thenI think this Ood disk a n was superInsight insightful about you know Vimanahnsw product quantization and the ideasI think that are core to alleviate aswell as the user experience for searchso thank you so much for watching uhplease any comments I'd love to haveanswer and please check out we've yet tolearn more about the web Vector searchengine[Music]thank you Connor this was a veryinsightful and very interesting asalwaysum and uh yeah if anyone has anyquestions about any of these parts thatConnor described just now and and papersthat he covered or any other things likefeel free to drop us a questionum and we talked about I I onlymentioned something that Zen has anotherthing he wanted to share but I thinkit's also a pretty good excuse becausethen there's a question for you uhcoming from Arium basically I was like yeah for asphere data set does the vectorrepresent the embedding only for the rawtext or Does it include the title Etcwhat do we know about it then yeahthat's a good questionumI actually don't know the answer to thatbecause I would assume that it includesboth the raw text and the title becausethe model that they're using tovectorize these uh vectorize this rawtext as a sentence to Vector model it'sactually the DPR model andit has the ability to you canconcatenate multiple passages togetherand it would convert it into a like a700 approximately 700 dimensional Vectorbut I don't know in the actual trainingof this in the creation of spherewhether they concatenated the title withthe URL with the description that's agood point Connor do you do you have anyidea around the generation did they dothat oruh yeah so uh the the vectors came froma dense passage retrieval model frommeta that is not open source but we wereable to we uh grabbed a dense passageretrieval model from hugging face andconfirmed like we ablated a few modelsand this one is like pretty close so itprobably was like fine-tuned from thatcheckpoint and didn't change therepresentation space too much orsomething like thatbut do we know if they um so did theyonly train it on the text the raw textor did they also concatenate the titlewith the raw text in the generation ofthese uh 700 dimensional vectorsoh that's a good questionum sorry I'm not sure I know that inweave yet we we have the vectorizedclass name thing umyeah that's a good question I'm not sureif it's common to vectorize titles withtheyeah that's a good point because I readthe the paper on the sphere data setlike the the web is your oyster thatpaper and I also read the DPR paper andit doesn't mention whether or not theyconcatenate the title with the raw textsothat's still an open question that's agood point I don't know I youintuitively would think that they woulddo that but you know uh not exactly sureI also had a question about one of thethings that you were talking aboutConnorum so I recently learned about the dataCentric approach and there was actuallya challenge around data Centric and nowthey're they're moving on to interactionCenterumhow do you validate or likehow do you know if a model trained inthis interaction-centric way is valid isit more around a b testing or likefor for good oldum model Centric I could just I have allthese validation metrics and it's wellstudied and the theory is all known Ihave no idea what interaction Centric isyeah I I agree with everything you saidI'm still trying to piece it together aswell because you think that if it wasthe human demonstrations you could tryto have that in a model-centric waywhere you still have the metrics kind ofbut I guess it's about thinking abouthow fast can JP generate the image ofyou know the if he's trying to generatesome particular image how fast does ittake him I think compared to just thequality of the imageokayyeah it seems like every every day whereI was only getting used to the dataCentric approach and now we have anotherCentric approach and this field movesfastyeah I I was like I don't know why yeahsame exact thought behindyeah we just need to wait for the cornerCentric modelsand approachesexcellentum so then I know you have one more bitto cover uh and uh go onyeah did we want to talk a little bitabout the uh or not not yetyeah so the one billion oh yeah yeahyeah yeahI'm sorryyeah I I am recovering from bit of acold or something that that's my excuseum yes so yeah very good point um so Iwas so excited about it that like mymind went completely blankum but intro the really interestingthing about this um the experiment thatwe went with sphere uh and then that'ssomething that we've been also going uhas a true as an exercise uh we are weyet we are trying to uh push Vivi to goget towith one billion objects uh that will goin into vv8 uh so this is actually areally big deal and how big of a deal isit is um uh literally I was talking toattia and who's our CTO and he's like ayear ago uh we're like pushing one twomillion objects into VV that was a biguse case right and then so 12 months agoone two millions and we just literallygo to like the next scale of like goingto a billion and and literally the thethe the process of how when we'reimporting the the big uh there's the960m uh file that file from sphere thatwe're importing it and this was actuallya conversation with I think that wasDerek one of our engineers and uh andTienI was like hey Derek what's going onhow's the import going and then therewas like yeah we are at 700 million uheverything looks fine like like it justkeeps going like like super fast uh andthen like it hit 900 and it was like yepdidn't even we didn't even break thesweat you know that was like so cool uhwhere the only reason why we didn't hit1 billion is because we ran out of dataum but like resources wise and all ofthat it was uh it was super uh it wasreally great to see uh and and then likewe had and this is the thing and it'snot even a joke like we are alreadyinternally talking about like trillionobjects right uh because 12 months agoif I told at the end I was like hey canwe do a billion you probably have aheart attack and then today we were liketrillion I was like yeah sure like uhwhat's the next magnitude once we haveto achieved this uh this is absolutelyuh crazy that we can achieve those kindof things uh without even being nervousabout it uh and then this is just amatter of uh use cases and I think asthen as you were preparing for the blogpost uh Bob like our CEO he actually runsome live queries for us that werelooking to include in the blog post andI think you have so yeah we were able toeven query that data set while the wholething was importing and then at the endlike the response rate was uh stillpretty decent I'm not going to throw anynumbers because I don't know them fromtop of my head but it was actually quitegood right like it was like wow that'sthat's impressive that's impressive andI I think Zen you can tell us a bit interms of like the structure of how thewhole thing was structured in terms ofthat importumyeah yeah for sure so uh basically whatwe did is uh you have a way that if youwant to get your feet wet I wouldrecommend going just with the directpython import because that's about what65 lines of code and you can get uh Iwouldn't recommend putting all 900million I don't think that's physicallypossible but we did it just uh just forfun so there's proof in the in the blogas well but I would recommend taking the100 000 data points of sphereimporting into vva and start queryingthat and just one of the examples thatwe showed uh in the blog post was youcould importsphere and then you can ask it you cando hybrid search on it so if you'reinterested in the example we used waswhat is uh good food to get in Italy andthen also do word matching at the sametime to make sure that you're getting itfrom credible sources and yeah I'm notgoing to release any uh any stats aroundhow quick or how slow that was but Ithink if there is interest from thecommunity we're also planning uh to havea more specific uh post on how the howthe road to 1 billion looked like rightas we were importing all of thoseobjectsum so we're talking about releasingthose numbers as well and if there isinterest then we'll definitely do thatas wellyeah I mean that's kind of the beauty ofbeing an open source company like a lotof the things we do is in the open rightlike even like our benchmarks that areon our website Etc uh like we we reallyproud about like what we are achievingandum you know always and this is alwayswill be the case when we look back amonth uh you know to the past we werelike well we were you know not where weare today we're like so much better somuch faster it's just like uh veryexciting that like uh the sky is thelimit right like uh I have a feelingthat like five years from now we justran out of data in the world and we andwe will be like still didn't break asweatthat's that's kind of the Hope right ingeneral yeah I was about to say is therea data set that we could try the onetrillion on I don't think that existsum there is there is a bob already uhyou know uh showed like a couple of datasets that had like a one and a halfbillionum and so we will be trying those outand we'll be sharing like uh our journeyas we go and and everything I mean in away in our quest to one trillion thatthis is going to be the new Questprobably now uh we have to find thatdata set is like uh to do thatum so yeah it would be interesting to uhto see what we can achieve with that andI guess the other thing is not just likea sheer volume but also uh let's have adata set that makes sense because if youhave just a trillion data points thatyou query but you don't get anymeaningful information that that doesn'thelp much right so I think that's whythat sphere data set was so uh is reallygreat because we could ask meaningfulquestions and get meaningful answers andthen whilst working at a huge volume uhso yeah that's kind of a trick and maybethat could be a question also to ouraudience right like where whether youwatch it live and and respond now or incomments later like if you re-watch itlater if you think of like some good usecases uh data sets that you think weshould uh give it a go uh trying to seeuh if you know if Sebastian starts getyou know breaking sweat because this isa difficult task or not uh we shoulddefinitely do that let us know what youthink uh that would be really cool toseeand yes thank you Zen thank you forreminding me about the point becausethat's like this is the onlyannouncement I have in this Vivid airand I forgot about it like how good isthat or how about we were it was allplanned we were saving the best for lastall right well that is is that why youare going to talk about audio to backsince you this is the last topic this isnot this is not really a talk it's justa bit of a teaser I'm I'm working on auh a uh with the engineering teama module that will allow you to uh passin audio data and then we're using uhthe uh whisper model from open AI thatwas recently open sourced and what wewant to do is right now you have theability to vectorize images and storethe vectors and the objects into youhave the ability to do that with textData uh soon hopefully you'll have theability to do that with audio data andthe applications that you can think ofuh are similar to like the Shazam appfor example if you take a clip of uh ofa song what are the 10 closest songs tothat song Or if you hum a like just atthe conference today I was talking toum talking to some of the speakers andthey were mentioning that an applicationof this could be if you hum a song whatare some of the closest songs to that tothat hum right but uh I think that wouldbe a really cool application uh when itdoes come out so this is not really anannouncement or anything we're justwe're working on it this is a bit of ateaserapplications for it yeah yeah but Ithink it's not so much like anannouncement by itself but but also thisis something that is somewhat a work inprogress like we we kind of like have iton the back burner and work it towardsit so if any of you finds that topicinteresting reach out to us like uhright right in here or on Twitter or onthe on the community slack like uh youwant to get involved like uh that wouldbe great whether it's involved throughdirect contribution or you go like Ijust want to play with it and see if Ican do like this exercise what are thisuh recognize what I want because younotice this I immediately was thinkingthere's this special test where if youhave a song in your head and all youhave to do is tap it to the Rhythm andthen get and make the other person uhguess what the song is and in your headyou always feel like you're tappingexactly and anyone should be able toguess it no it's almost impossible soyou'll be amazing if the audio to backcould recognize that right you even Idon't know you could try happy birthdayand then none of us would know it eventhough everybody knows like the rhythmof happy birthday happy birthday so thatcould be a cool thing but yeah yeah youwere gonna say something go on oh yeahI'm so excited for the music record themusic search I think like we'll connectit with the generative models that willcome up with like billions ofpotential Melody lyric combinations andthen you can take a snippet and be likeanything like this right and it'll justbe crazy I'm just excited to see once wedo have that out what what the communitydoes with it right what types ofapplication you can think of because nowwe're getting to a point where if youtake any modality of unstructured datawe have the ability to dosemantically understand what you'retalking about whether it's the podcastin in a uh audio file or whether it's uhimages like that's the really cool thingto me yeah I mean you could like I wasthinking like you could uh use it in afactory where like certain machines justuh before they break or just be whensomething's about to happen maybe theyrelease a specific kind of noise orsomething so what if we could train likea model on like you know here's ourhealthy machine Sounds here is like whensomething's happening or blah blah blahand then you could uh very easily kindof like then go uh you know havesomething that constantly uh keepschecking and testing and then one pointhey yeah there's something wrong withthis machineum so that'll be interesting and as Iwas saying I remember I read about it uhat one point there was this experimentwhere like toddlers how do you call likea kids that are like up to a month oldor something other toddlers right yesinfants infants yes yes that's the worduh so there's a thing withinfants that often you don't know whatthey mean when they cry but apparentlysomebody did the research where uhspecific types of Christ uh kind of meanI'm hungry I'm in pain or I'm justannoyed or or something and then unlessyou like fine-tuned to to it uh youcan't really hear it or understand sothe whole thing was that that there'sthis people that they train the dog torecognize these things and there werethis different kind of rewards dependingon the kind of cry the dog was behavingin a different way giving signals onbehalf of the baby uh and then they werelike oh right okay time to change thenappy or like oh the baby's hungry andthey're actually getting it right somaybe you could replace the dog withyour audio to vac model then that wouldbe coolyeah I was talking to Bob and uh Marcoand Chris from Facts yesterday aboutlike the interfaces for chat GBC and sokind of the transition from the storyyou're just telling Sebastian is I'mcurious about like these brain computerinterfaces like this whole idea of likeyou can uh you know have Bowens bark thenatural language or babies cry tonatural language is like I think we'llbe able to read directly from our brainto send a gbt and it'll like suggest youthings in textyeah yeah I we could do that go aheadI read a paper recently where a groupout ofum Stanford who's working on bci's braincomputer interfaces they were able toreconstruct uh the imagery from aperson's uh EEG so brain activity theycould actually tell what you werethinking or what you were imagining inyour mind just by looking at the brainactivity and the results were fairlygood if you looked at the image theywere shown and if you looked at theReconstruction from the brain activityit was pretty amazing that you could getthat just from brain activity like itwas amazingyeah the future is going to be weirdyeah that's scary technology manit's gonna be great it's gonna be greatum all right so let me just double checkin case we have questions uh Shivanisays hiso like let's all say hi Shivani thanksfor watchingjoining us and uh saying hi uh who elsewants to say hi he doesn't have to be aquestion or tell us where uh where youare coming from or should we bring anengineer or our CTO or somebody elselike that it doesn't necessarily have toalways be uh you know people from thedeveloper advocacy team uh so let usknow uh but we are all open to questionsuh and if there's a request that youwant to see Bowen again and I hope oneof you will say because Erica is tiredof doing it just because I ask and thenplease please ask to uh bring Owen backuh should we have a special camera forBowen I don't know like I I argue weshould but I think I needed some backingfrom the communityuh and and I'm about to create a fakeYouTube account just to do thatshe sleeps all day sorryto just watch her sleepI'll watch that yeah it's better thanwhat yeah she's very cute yeahso while we waiting bow and watch yeahthat's a very good idea then yeah wecould definitely do that we could dosome some sort of ml training thatreacts is like yeah Bowen is hungryBowen is you know sleepy you know all ofthose yeah we should definitely do thatso whilst waiting for another questionI'm going to do something go on I'mgoing you go I I can Circle back to theoh no no you need to be at your computerfor this Sebastianum can you share my screen for meumwell well listen I was listening totalks I did a couple of things so umSebastian that data you said I can nowimport itwas nicesexy and everything there we go so whenyou load the Jsonum it's loaded pause all the data itloads it onto the tableand the cool thing about this is thatyou can choose the data type for eachcolumn basicallyum so you got the name link content andjust the background this is uh list ofmeerkats or something is that what'sgoing on Sebastianum I'm not necessarily to review butit's basically like a paragraph fromWikipedia in different languages atleast the first three about meerkats butthen you have like all four and then youhave like other articles and some workyou know what's going on uh testingthings with different languages and andstuff so uh but uh I will talk we willpublish a blog post about it on Mondayuh that is a coordinated effort withsome of our partners but it's somethingreally exciting I can't tell you what itis just yet but I'm I swear it is goingto be mind-blowing it's it's amazingwhat a teaserum that's an amazing kind of intro sobasically what it does is um it's parsall that data and you can see thecolumns here with as in the schemaBuilder section and then what we can dois to choose the class name topless I'mjust going to call thatummeerkats let's say and we'll add a classand now if we check the data we've gotnow two classes in our data set we'vegot the wine review data from before andthe meerkats dataand all you would have to do then isclick on add dataand once that's all done that takes allof half a secondwe've got that importedum since Sebastian has hang on just twoseconds I've just dropped somethingwhathello where's your laptophey we're real we're next to each otherthis is not worth a bitumI'm going back but now I want to see youcome to my roomuh you're in a different Hotel mansome like TV production magic right hereum at one point we're talking aboutwhether we should do a bit where I'mbasically getting broken into in theapartment in the in the hotel but thenum didn't want to induce any anyunnecessary Panic hey Sebastian can Iask you are you there nowyes fantastic so can I share my uh vscard again you can'tyeahwell let's see whether I actually can umokay cool there we goum so if we can show that on screen souh all I've changed in my code base isI've added this or these two lines ofcode right so that's all I've done to beable to like be able to uh not pause theJson before because this was my Jsonparser to be honest I'm not even sure ifthis was working before I think it wouldhave beenum but just because the structure of theJson was differentum this is the only code that I'vechanged and now it is able to load themeerkat Json that Sebastian sent me soit's quite modular um I've got aseparate function and there's aequivalent function somewhere for theCSV and just for the magic[Music]and what happens isum uh basically at one point it fires acallback when you press the button andthis is the function that it calls anduh you can upload the data line by lineand you can specify things like themaximum number of objects just so thatyou don't accidentally spend you knowlike an hour uploading objects andvectorizing them and when you just wantto when all you want to do is test yourtest your setup so yeahum there you go are you problem solvingdone and hopefully I get to keep my jobwe'll see though okayyou're fine you save I know where youleave you know so what next door yournext door rightum no this is perfect and I'm glad thatyou were able to update that because Ithink that's the the value of that indata importer uh and and I do hope thatlike next year we'll have somethingwhere you anyone will be able to likesay like hey use all my data uh droplike this uh 900 million sphere data setyou know objects and then go like importconfigure everything you know like itshould be a lot more automated I thinklike the journey that we have today thatyou can write like let's say if you'reusing a python client or any of ourclient I think the import process is notthat difficult but this was one of myfirst impressions when I joined thecompany I was like it's not thatdifficult but it could be even easierright so I was super happy uh when youstarted kind of working on like hey it'sjust like a proof of concept is the ideaand I think that's where we all want tohead towards to right like make workingwith weave or make work work withvectors and then your data as easy aspossible right it should be a no-brainerit already is quite easy but I thinkthere's still some improvements that wecan makeum and often uh we can only improvethings that we know of so obviously theImporter was kind of obvious to me to JPand few of us but what if there'ssomething else that we completelymissing out on and it could be a lowhanging fruit even right like somethingyou could do uh quite quickly so pleaselet us knowum so I'm a bit of a spontaneous personand what I'm going to do like I'mactually pretty happy that like Ari andShivani like say hi ask the question andeverything uh so uh what I'm going to dois just say like I'm going to send you at-shirt like to each of you like one ofthose green wavy t-shirts uh so probablywhat we need is uh if you're on like thecommunity slackum like Ping me and then becauseobviously you don't want to share youraddress in here but yeah thing me withyour details and I'll be very happy tosend you a nice green wavy t-shirtum why not like you see this is what youdo you say hi and uh you get a t-shirtbow and say hi said hi as well so I needto look for a bow and size t-shirt aswell right but one you're excited youwant some T-shirts yeah yeahI'll send some dog or snackles as wellso that'll be fineum all right I think uh We've coveredeverything unless I really really forgotwe had another thing but uh thank youall for uh for listening thank you forsending questions saying Hi and um thankyou for being part of the community andthen because that helps us do what we doand makes us enjoy it even more sothat's for me thank you for uh forlistening and uh thank youthank you thanks a lot Owen are youwaving as well saying thank you no thankyoubye bye everybody[Laughter]nice", "type": "Video", "name": "Weaviate Air \u2013 Episode #3", "path": "", "link": "https://www.youtube.com/watch?v=EcXRX70t-Ts", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}