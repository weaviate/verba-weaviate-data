{"text": "Thank you so much for watching the 22nd Weaviate Podcast with Yaoshiang Ho! Yaoshiang is a Co-Founder of Masterful AI, ... \nhey everyone thank you so much forchecking out the wva podcast today isgoing to be another super excitingepisode we have yao shang ho frommasterful ai uh masterful ai from myfirst impression of it looks like thissuper interesting model training anddeployment computer vision software andi'm super excited to just dive into thedetails and get right into it so uh yashand thank thank you so much for comingon the podcast and can you tell us aboutmasterful ai yeah absolutely glad to behere thanks for having me so you askedabout you know what we are but let mejust let's set the stage a little bit soyou kind of understand the problem soobviously we know kind of deep learningbecame a big thing in the mid 2010s alexnet kind of groundbreaking performancegoogle acquires the company behindalexnet gets jeffrey hinton as part ofthat dealand a lot of use cases have been kind ofbeen cracked with it including then deeplearning also applying to nlp not justcv um so then kind of looking at thetooling a little bit um like back in theday you had to access gpu throughshaders thank god at least there's likecuda now and then tensorflow and pytorchbuilt on top of it and cloud providersoffering hardware really easily but butall these platforms are still reallyreally hard to use like you have to be adeep learning expert and as you know ittakes years to become a deep learningexpert so you know if i would describethe problem today it's sort of likethere's no equivalent of a mysqldatabase or nodejs much less a stripe orshopify there just aren't these likehigh-level services and platforms tosolve all your problems yeah there'stensorflow but it's like it's basicallylike having to program all of thoselibraries i just mentioned in c plusplus yourself just to get an applicationgoing so so let me answer your questionyou know really directlymasterful uh is a platform to buildcomputer vision models and itsimultaneously has a very simpleinterface and internally it has the mostadvanced algorithms so kind of think oflike when you go to aws and fire up aterabyte of s3 storage it's so simplefor you like three four clicks but youknow behind the scenes there's all thosemachinery going on there's all this youknow hardware being procured you knowconnections being made umthe advanced algorithms inside are whatmakes it like a really ready to deliverproduction-ready models and i wouldcontrast that with some other automlplatforms um that are out there incomputer vision so it's got a metallearning engine it's not anoff-the-shelf one it's one that we builtjust for our uh engineand what that does is it takes away alot of the hyper parameter tuning thatthe developer has to do and that's whatdelivers on that promise that you passin the data and you get them all it'snot you passing the data run 500experiments track them all and just sitand act like a human grid searchalgorithm for you know like five weekswhich is kind of how you do it today umit also delivers the best regularizationpolicy um regularization means umuh basically means getting the mostinformation or training the mostaccurate model from your labeledtraining data and then on top of that ithas a semi supervised learning enginewhich means your model will get evenmore accurate data and get even moreaccurate from the unlabeled data thatyou pass in so that means images with nolabelsthat technology is available today ithink you saw our technical report soyou saw some of the algorithms thatwe've kind of adapted including likenoisy student uh barlow's twins umand and some clr um so that's that'swhat the platform is today and and youknow the problem is trying to solve forfor developersso so with our webv8 audience i thinkthe sim clr and barlow twinsimplementations are just super relevantto what we're doing because that's wherewe get these embedding models thatproduce vector representations thatoptimize for semantic similarity but umbreaking down the things you said uh youknow we have and then also thethe meta learner the hyper parametersweeping and reminiscent of things likeweights and biases and determined ai andall that kind of uh user interface foryour model training stuff but so i'dlove to kind of break these things downand go into them one by one and i knowthe web listeners probably want to gointo barlow twin sim clear first but canwe actually start with noisy studentbecause i think you have some reallyinteresting perspectives on the noisystudent implementation in masterful yeahso notice student is kind ofconceptually drawn from a well-knownidea of knowledge distillation which islike one teaching one model generatessoft labels from unlabeled data and thenthere's a noise function and then astudent model gets trained as well andthat's kind of like the basic theory andi think where to contextualize it wherenoisy student isarguablyadvanced and different from uh some clrand bar less to sort of unsupervisedtraining techniques is umnoisy student i think was one of thefirst to prove that he worked at veryhigh cardinality dataum whereas some of the other techniqueswere more optimized for lowercardinality data sets so for example alot of times like if you look at theoriginal sim clr paper they'll use likeone percent of the image net labelsand so you're using a much smallercardinality like in the on the order of10 000labeled examples whereas noisy studentwas kind of trying to prove that theycould do it at very large cardinalitiesso they did full imagenet 1.3 millionimages and then used google'sproprietary jft 300 data set with 300million unlabeled images to then furthershow that you can actually improve ononimagenetsupervised trainingwithout necessarily adding additionalmodel architecture size although thathelps as wellbut you know i think one of the things iwant to point out since your audienceprobably understands some unsuperviseduh pre-training techniques like cmcr alittle bit better there is a very strongsimilarity between these and it's notoften thought about but in both casesyou are training two models kind of atthe same time you are noising the inputsbetween those two pairs of models andyou are trying to get them to match upwith each other so although noisystudent often is thought of as a teacherand a student ultimately you are tryingto get two models to match just likebarlow's just like sim clr and then theloss function in noisy studentis obviously kind of you knowa weighted cross entropy whereas in simclr it's contrastive and you knowbarlows has their exotic loss which kindof um you know adds its own kind of uhvariance covariance and umsorry i forget the thirdpart of barlow's last function butultimately you are trying to get twotwo parallel paths to take the sameimage noise themand then see if they can ultimatelymatch so they're not as different as youmight think is when you kind of lay themout side by sideyes yeah i love that perspective of thesimilarity between teacher studentknowledge distillation and then siamesecopies of the same network as we'retrying to learnrepresentations and then maybe just aquick i do think most listeners probablyare aware of this general paradigm ofcontrastive learning but the idea isthat you have a siamese copy of thenetwork say this is your burp model andthis is also that same burp model andthey run it and then they get the vectorembedding vector embedding and they'llbe uh you you want them to alignpositive pairs and make it dissimilar tonegative pairs so that's kind of thebackground behind that idea so just tosort of lay the foundation a little bitin case we just dove right into it alittle bitsorry about thatthat was my fault with the way that iset that up but but let me um so thething so noisy student for one this ideaof high cardinality such an interestinginsight i'd i'd never made thatconnection before that that i i guess iwasn't aware of the history of it thatthey were trying to show that knowledgedistillation can work for saying athousand label set icould you take me a little bit throughthat before we move onyeah um less about the number of labelsabout the more about the number ofexamples so you know as you know incomputer vision the classicdata set 1.3 million uh examples inimagenet um so again just to repeat likethe sim clr often they'll take onepercent of the labels and treat theother 99 of the images as your unlabeledset and so the point there is they sayhey off of just you know 10 000 labeledimages we can get really really goodaccuracybut we have to use you know 1.2 millionunlabeled or draw from that pool of 1.2million unlabeled images whereas withnoisy student they said we're going touse all you know 1.3 million labels fromimagenet and then we're going to take300 million moreimages to use as the unlabeleddataset and prove that we can stillimprove the accuracy of the model umthere's a talkthat aguy named godarus gave and i'll um sendyou the link to it so you can append itin the show notes where he kind ofcontextualizes just kind of where thesekind of ideas were kind of fitting inwith each other and why they kind ofwere solving slightly different problemsi remember when i was reading simclr i'mlike whywhy does some clr use this really tinydata set and why does nobody just didn'tuse the full one you know and there is alittle bit of a history and they weretrying to solve slightly different youknow slightly different nuances of theof the problem of sslhmm wow yeahthank you i think that's a reallyinteresting angle to it that i hadn'tthought about kind of thedirection i was i was thinking we weregoing to go in is talking about the thenoising used in noisy student and how wehave the augmentation and as i was goingthrough the masterful documentation isaw this thing this phrase augmentationclustering i read thati need to know what that is yeah yeahwell let me tell you just give you avery brief history here so i mean umwithin uh with a noisy student there'sthree types of noise that happen there'sjust classic data augmentationwhich by itself is extremely powerfuland sufficient to make it work and theyadd two other forms of regularizationthat you know i'm sure everyone's awareof one is dropout which is classic andmaybe one another one that maybe is notas used as much as a stochastic depth umbut the point is umthat to get these techniques to work youhave to have the right regularizationtechnique you cannot just throw in i'lljust do a couple image blurs and a zoomand a darkening and i'm sure that'llwork like these techniques are veryfragile so in a sense when you talkabout noisy student training people getexcited more by the student because it'skind of architecture based but that youcould argue the noise is at least asimportant um to the idea of noisystudent training and what we found um inthe lineage of let's just take dataaugmentation is that um there's a verystrong sense of overfitting to imagenetand imagenet images all have asimilarity that says that they're alltaken by human camera by humans holdinga camerawhereas a lot of other applicationsdon't have these attributes geospatialin manufacturing sometimes the camerasare very fixed sometimes they're veryprecise sometimes you don't a satellitedoesn't go up and down or i guess somedo but a lot of satellites stay in youknow geostationary orbit or at least atthe same altitude like they have thesame resolution no matter if you've gota camera pointing at a manufacturingline like that is it that that camera isnot going to move around so that's someof the explanation i think of why a lotof the classicideas of how to uh build a goodregularization policyin my opinion are just kind of overoverfit to imagenet so kind of thelineage that kind of we study and wementioned this is kind of the autoaugment and then randog and auto augmentis a very very deep search the problemwith the depth of that search is it's 15000 runs in their paper full trainingruns to figure out the correct policy sothat's obviously you know justintractable for any like normaldeveloper and then randog which i thinkis still considered state of the artbecause i think some clr used it umtried to reduce that search space byclustering they didn't use this term butif you kind of think about it that'swhat they did and the search spacereduces down to about on the order of100searching the number of uh transformsand the magnitude of those transformsbut the insight that we had is that umthe way that they clustered thosetransform magnitudes was just on a scaleof 0 through 30 and they would basicallytest all the transforms at level 1 allat level 2 all at level 3 etc and thenall at level 30. so you know they didn'tspecify the search algorithmspecifically but their point was in thepaper um the search space is smallenough to simply grid search sopresumably that's the algorithm they use100x is still way too long for anypractical person i mean if you'rerenting you knowaws machine for 24 bucks an hour likeyou know good luck telling your boss i'mgoing to need 100x more time you knowlike instead of 2 000 i'm going to need200 000 like you know forget that thatjust that's not going to happen so whatour insight was and it's kind of basedon uma very very simplified version ofuh an idea from gans which is forinception distance is the distanceof something really can give youinsights into how much you're perturbingthe image so maybe darkening orbrightening an image by one clickhelp affects it very little on thedarkness transform but maybe on therotation transform it's extremelydestructiveand so that means that instead of justsayingi'm going to search click one ofdarkness and click one of rotation whati really should be doing is searchingclick one of rotation and click 50of darkness because they're moreequivalent when you look at the distancethat the image generates so how do wecalculate that distance i mean you canhave you know your choice right you cando very exotic distances you can do l2of the feature maps uh another thing youcould do is just you know um calculatethe you know difference in the scalarlosses when you rotate an image inforward prop uh versus darken so that'show we kind of we run forward prop totry to understand how much all thesetransforms affect the umaffect the model perturb the model fromsome idea of a distance from theunperturbedimages the non-noised images once youhave that then you can very quicklycluster likefive big clusters and then grid searchfive clusters or three clusters insteadof grid searching um 100 clusters sothat's a little bit of a nutshell and umi apologize we haven't quite publisheduh that algorithm in detail but uhthat's the kind of heart of it isinstead of clustering just based on someheuristic of you know magnitude weactually impute these magnitudes basedon how much it perturbs um a model's uhperformance on specific datawow that is super that is so interestingand i maybe i do want to kind of justget a couple of terms so just yesbecause you're so knowledgeable and ireally just want to dive in but i alsowant to sort of lay the groundgroundwork for people listening um soagain a noisy student the teacher labelsthe data for the student network tolearn from we're going to augment thatdata for the student to learn from so wehave data augmentations like rotationblurringcropping flipping increasing making itmore blue than usual like there's amassive set of image data augmentationsthat are label preserving so then camealong the algorithm rand augment uh randaugment has two hyper parameters n and mn is uh how many to apply in sequenceand then m is the magnitude to becauseeach one has an associated magnitudewith say rotation you're either rotatingat 60 degrees negative 60 degrees or say30 degrees right so so the theclustering yao shang is describing is uhn and mthey do they define a cluster in thisspace of the massive configuration ofthe n and the m that you could have uhso then for shea inception distance wasthis thing used in gans that's like umyou you gans are a generative modelbefore diffusion models kind of stolethe show where it's like real fake lossand you say how do i say that thesegenerated images are uh quality imagesso what you do is you have the vectorspace of the generated imagesand then you have thevector distance of these generatedimages here to your original images andso that's that kind of foundationso and so now i think where we are isis we're looking at the vector distancebetween augmented images and theoriginal images to define thatof augmented images and and so this issuper relevant for uh so noisy studentof course so you're saying whichaugmentations should we apply to theimage for the teacher to label for thestudent but also for our generalframework of training contrastivelearning models that produce like thevector embedding models that go intowev8 iswe have to have this algorithm where westart with an imageor any data point if we're well let'swe'll keep it to images because that'sthat's like most of the label preservingdata augmentation research hasbeen done on so we start with an imageand then we're going to transform itinto the positive image and then alignthose vector embeddings that's thetraining objectiveokay so i hope that was like a decentlittle like kind of uh primer to it sonow let's dive a little more into thisidea of okay so we have so we'reaugmenting images and then we havesemantic clusters that come from theaugmentationsso is that i mean do do augmentationsreally like throw off the semanticrepresentations so much i know thatthere's like an issue with robustnesswith these models where say umyou take your test set and if you rotateall the images suddenly you might gofrom like 90 percent to like 60 percentyeah so yeah in your experiments theseaugmentations they create like verydifferent clustersuhi'm not sure how to answer that i meanthe purpose of these clusters was tocluster all of our transformsby the distance or by how much itperturbed the model soi think what's interesting ismaybe i'll answer it this wayif you look at a lot of research whenthey're classifying imagenet they'rejust going to use a human prior and do amirror for example but if they'reclassifying uh mnist or svhn they'relike oh i shouldn't do that becausenumbers shouldn't be horizontallymirrored um andindeed what we find is if you you knowif you if you if you rotate the num newnumeral by 180 degrees or you uh mirrorit you know models are just going to getreally confused by themuh and so you you see that reflected inthe amount of distance that gets comethat comes out when you measure you knowthe intermediate future maps or thefinal value so that immediately tellsyou that's a that's a fairly destructivetransformations and you know it usuallygets excluded very quickly by the searchalgorithmso i'm not sure if that's exactly whatyou're asking but yes we we do find thatthis approach kind of automaticallyfigures out umthese ideas i think anotherthing that maybe happens in geospatial alittle bit is because theobjects are often very small i mean theorder of you know single-digitdouble-digit pixels as opposed toimagenetumthis is my hypothesis i haven't done thestudy to prove this butif you took if you rotated the image ofme by five degrees you know you'd say ohthat's still me no problembut when you take something that's likethree pixels by eight pixels and yourotate it you're starting to getaliasing issues right the old computergraphics idea you know you're losingresolutionyou're adding blur to it and so i youknow what we've also found is a lot oftimes in geospatial imagesthese mild rotations are end up beingvery destructivein a sense the more mild the worse it isbecause the more aliasing you induce a45 degree rotation is much better from athan a two degree rotation i'm not gonnai'm not gonna prove that i don't have apaper to justify it we haven't done allthe studies but i'm just kind of givingyou a flavor of some of the things thatwe've kind of observed as we try toexplain why our algorithm has discoveredyou know certain made certain decisionsabout the right policy for you know inthis case data augmentation and appliesto regularization as a whole of courseit's kind of reminding me of this ideaof like um so out of distributiondetectionandso using the vector representations ofimages to determinethat uh we'veso yeah so we're up in a satellite andwe've got uh i think you look at likecrops or something i'm not an expert onthis kind of thing but and it's rotatedsuch that you really have it's you'vecorrupted the label it's no longer labelpreserving so from the vectorrepresentations of the images before andafter augmentation does that give usenough signal todo out of distribution detectionuh would i i don't know if maybe ishould have said out of distribution ishould have said just like labelcorruptingyeah this idea that you'velost the information and you can tellthat purely from the vector space andthus you have this heuristic to saylet's actually not have the teacherlabel that for the student and noisystudent is that how it comes or or whenwe're training simclair models let's notmake these positives because thataugmentation is no longer a positiveyeah we we we we don't umwe don't use this technique to takethings out of distribution and adjustthe label as much as we just treat it asa non-label preserving not sufficientlylabeled preserving transform and wedon't use it typically in umuh you know these regularizationpolicies you kind of have one policy andso you run it and the goal is that onepolicy will sufficiently perturb thedata so it's not identical but it'sstill within distributions it still youknowfeels like natural data to the model umif you and i eyeball it usually it lookslike you know relatively normal um butagain it's it's that's just trying toexplain what's kind of happening insideum but what's happening inside is is umuh it's it's easy to define kind ofmathematically but it's hard for me touh to truly tell you you know whywhy it's working these are just sort ofyou know my interpretations of whyi've seen these policies come out verydifferently for for very different typesof uh data sets i think the broaderpoint is that you have to be adaptive tothe data set and it kind of goes back tothe point i was saying that a lot ofresearch is overfed to imagenet andimagenet you know people think it's hardthere are a lot of things that make itreally easy actually it's a balanceddata set for one thing there's very highentropy in this data set umit's it's it's it's it's all handheld uhcameras taking it so there's all theseattributes about it that i thinkresearch is probably kind of like overfit toand you know when you try to apply anyof these techniques out to productiondata sets in the real world you just seehow it just it just doesn't work nearlyas as well as it does on you knowimagenet or cypher10 or something likethatcould we maybe stay i kind of want toexplore one more idea in the space ofusing the vector representations and howthey change and i'd like to kind ofdive into this idea of active learningand i think it's maybe relevant to themasterful model training suite and yeahlearning is basically this idea wherelet's forget randomly sampling the nextbatch let's try to intentionally findthe next batch of data that would be thebest one to train onand i've seen papers like focusing onthe biggest losers where you say youknow take your batch of 64. this one hada huge loss and so maybe you do like avector search with something like we v8touh go get more like that because that'slike clearly like a weak spot in ourdata space right have any thoughts onthat kind of idea yeah absolutely soi'll um you know it's on our productbacklog toum let me step back a little bit becausea lot of our unlabeled data comes fromvideo feedsum the problem some of our customershave is well video if you treat it as asequence of frames of images you got alot of ityou get to millions and billions offrames really quickly so the questionbecomes i can't train on all of that somy naive algorithm my null case wouldjust be randomly sample it or uniformlysample it i can't sample all of it butlike can't we do a little bit better andso indeed we have done some researchinternally on using exactly that activelearning ideas to say well what can ipick off from these you know millionsand millions of frames you know can ifind like the one percent that's goingto help my model learn the most and ithink one of the first algorithms like alot of us go towards is great we'll justpick the one the model's wrong on that'sgot to be the one you know to fixum and sometimes that's right butsometimes that doesn't help becausesometimes what ends up happening is youforce your model to get really good atthese really rare examplesand at the expense it forgets all thesimple examples so like you may havesolved that one percent really hardproblem but the 99 simple ones you knowyou just degraded that performance by 50because your model was all this energyso it's like it's not as easy as youknow as i think we we we think when wefirst you know start this problem and umuhso um some of our internal research inthis area is um probably less orientedtowards the vectors although we haveideas on on that as well um but a littlemore kind of going back towards the ganidea the generative adversarial networkis to say you know can i train umcan i can iagain typically is thought of asgenerating synthetic images and thetraining is that there's a generatorthat generates the image and thediscriminator that tries to decide ifthat's a real image or a fake image andthey fight each other in a so-calledmini max competition so you train oneand you train the other sort of um theanalogy people give is acounterfeiter and a policeman so youtrain the counterfeiter by having thepoliceman tell you you know is this acounterfeit or not and then you freezethe counterfeiter and you have thepoliceman now try to train itself to getbetter by telling it the reality ofwhether this is a counterfeit or not sothese two kind of compete with eachother to get better and sowithout giving too much away we have asimilar idea that the generator is uhyou know not really generating syntheticimages but just in this case has fullknowledge of whether it is uma label data labeled image or anunlabeled image and the discriminatortries to figure that out and if we canget to the point where umwe can trick the discriminator intothinking that uh an unlabeled data imageis labeled then maybe it's gonna thinkmaybe that gives us an algorithm to pickuh the most useful unlabeled images theones that don't look like labeled imagesin other words the ones that are notgoing tothe ones that are actually gonna uhsorry let me start over uh if we cantrain a discriminator using this ganthinking but not a true can again in theclassic formulation then we may be ableto use that discriminator to pick outthe most valuable unlabeled images thatcan teach the model something withoutbeing so hard that it's going to destroyuhwhat the models learn but not being soeasy that you're just wasting your timeyou know back propping on really simpleimages so that's sort of some of some ofthe heart of how we've been kind ofthinking about it um so apologies itdidn't go into the vector space but youknow our thinking there is a little moreinspired by you know sort of thediscriminator of againyeah i think umi had something i was going to ask youand then i and then i think i gained abetter understanding of of this idea andumso so purely from an unlabeled image thediscriminator would be able to tell ifit's been labeled because you also havea classifier that's been trained on alabeled setlike how does itconnect the informationyeah um the uhthe discriminator could have informationabout uhwould have information about the vectorsthat come out of atrained model um either the finalprediction or the intermittent featuremap so those would be the vectors or theembeddings and based on that its job isto predict you know is this data pointlabeled or notand uh you kind of you kind of use thatto figure out these are the ones thatare too easy so don't send it to themthese are the ones that are too hard andhere's ones that are right on the bubblewhere there's still some learning valuethat's not going to trigger you knowforgetting of some kindsoyeah again a little more inspired by thekind of gan discriminator concept um uhthan necessarily um sort of vectordistances although we have other ideason you know using vector distances inother um you know active learning typeideas where uh similar problem you knowjust can i label can i pick can i bemore intelligentlypick the data to spend my money to labelas opposed to just randomly sampling youknow data points i'm seeing frominference or or using human judgment toyou know find you know visually gothrough the data and look for clustersis there some way to try to automatethatyeah the way that i originally describedactive learning is i remember a papercalled mind your outliers that is asetting visual question answering datasets and that exact idea of which onewas the biggest loss and they find thatlike you know you cannot it's impossibleto answer and that's why it is soso yeah being a little more clever aboutthat uh so so semi-supervised learningand as i went through the masterfuldocumentation that you know jumped outto me islooking like one of the big focuses ofmasterful is on the summers obviously wetalked about uh noisy student where inaddition to having the image that youit's not just like you have the labeldata set and you take the image applynoise to it teacher labels that studentlearns it it's also like you haveunlabeled data as a part of this kind ofthing and and that's the idea ofsemi-supervised learning you have labeldata big unlabeled data setso i and i'm sorry if these names haveyou heard of the ada match the kind ofuh that kind of approach wherei think the key thing is they also theyhave a loss function at the label layerand then also in an intermediaterepresentation of theteacher got it got it kind ofso i guess kind of the question is uhlike what what are the uh what's thelandscape of semi-supervised learninglook like what are kind of some of thecutting edge ideas yeah yeah umso you're describing you know some ofthem uh which is umkind of stepping back um when you takethe pair of models and you ask them tokind of match upyou get to pickwhatever kind of layer to try to matchup and i'm not as i'm not obviously on ato match but sounds like you're tryingto get intermediate layers to match upas well as the final layer you can arguethat you know in noisy student you'regetting the final final layer to matchup which is a post postsoftmax um so itlooks like labels but the fact that thislabel is almost not as important as thefact that they just had to pick somefeature map um some outputof of of a convolution andand reluand in a lot of other models likebarlows then they stack on a whole bunchof other layers which they end upthrowing away and then you just use anintermediate uh layer soalmost think of like you know you gotresin at 50 and then you force it to doand just throw away the final layer soyou have just resonant 45 and you justfind that the intermediate layer is moregeneralizable in some way whereas thefinal layers get too task specific umtowards thepretextual task that you're usingpretextual means you've created a taskfor these modelsthat ison the surface not actually solving yourproblem but through solving thatpretextual problem you end up solvingthe problem you actually care aboutwhich is to learn better representationswhich will get you a more kind ofaccurate modelumuh so i think um one of the mostinteresting things is i think when youlook atsemi-supervised learning umuhalthough as i mentioned noisy studentand then the unsupervised pre-trainingtechniques like barlows and simclr lookvery different they actually have a lotof similarities ultimatelybut you know sending that similarityaside and looking just at the idea ofpre-trainingthis isn't the first paragraph of thismclr paper they talked about umcontrastive as one basic idea but theother one being generativeand so generative you know they actuallycite you know good fellows gand paper umso could you train againuh and then use the representation in aganto act as a technique for building anunsupervised feature extractorand the conclusion broadly was no thoserepresentations waste too much energytrying to get pixel perfectimages and so they aren't generally verygood but you know kind of the latestlatest right now is uh you knowtransformers and uh featurereconstruction kind of taking over whereyou just uh you know take an image andthen youliterally add white noise to the pixelsof an image and you ask again to kind ofreconstruct that and you know thatapparently has turned out to be a veryinteresting and powerful technique forthe model to learn representations uhinside and become a very good featureextractor and you know i think it goesback to um you know some older researchthat did not beat state of the art butit is i think very well known at thispointone wasto colorize images so if you have ablack if you have a color image turn itto black and white and then train yourmodel it's not exactly an auto encoderit's kind of an auto encoder at thatpoint to colorize the image and then umgodars i think his first name is spirosdon't want to butcher it i think around2017 or 2018 he did a classic paper itwas just sort of a proof of concept justrotating the image in 90 degrees 180degrees and 270 degrees and then yourpretextual task in that case was topredict the amount of rotation of theimage and you know they just found thatoh you these these these modelsum did learn an interestingrepresentation inside it may not havebeen better than supervised training onimagenet but it did learn something andthat was just really cool at the time sothen the next question was when does itget better than supervised and that'swhere you know some clr kind of uh wassuch a breakthrough in that you know forfor one percent of labels it actuallydoes train betterthan than a supervised technique umso sorry you asked a more directquestion and i wondered a little bit butit's such an exciting area and yeahit just always frustrated me that youneed labels because images haveinformation so it's not a feature thatyou need a label that's a bug i mean youknow there's there's nothing that holdsthere should be nothing that holds amodel back from learning from imagesyou know without labels uh so this thiswhole field is just you know superexcitingyeah andyou're so knowledgeable about it that itinspires so many different like ideasthat listen to you and there's so manylike questions i there's so manydirections like we could go in butso the question i'd want to follow onwith isso this idea where um it can uhas i kind of want to pivot into saycycle gans and the image to imagetranslation thing yeahto kind of build up this idea the ideathat you classify the rotationangle so you're rotating say that's a 45degree rotated airplane yeah thecyclogan image to image translation ideawould be like you rotate the airplaneand then it's like unrotated rotate yeahyeahso so i've been thinking about this ideaabout like could this help withdistribution shift where uhthe distribution shift happens kind ofand it can sort of map it back to theoriginal thing yeah absolutelydoes that idea make sense yeah i meancycle gans were um i mean the cycle partwas just a consistency to try to provethat you know if you can go from horseto zebra back to horse very effectivelythen the cycle was kind of completed andso it was a way to kind of enforce thegoal but the real task was not the cyclepart but um you know a technique calledyou know image to image uh translationwhich says like you know if i have aumyou know a horse and a pose can i thengenerate a zebra in that pose and can ithen generate a dog in that poseand um so i i'd suggest people look atyou know the series of papers thatxunhuang i believe study the cornellunder belongi um generated and his firstidea was neural style transfer which weall know just just by looking at thelayers but then adding adding the sameideas but with adversarial losses so theconcept from gans then allowed thesemore like really even more powerfultechniques umwe've researched that area quite a bitparticularly for domain adaptation asyou describedwe looked at one scenario where um youknow there are two major satelliteproviders and although visually you lookat the images they look the same um ifyou train a classifier just on one itwon't do very well on the other it'll belike two-thirds of the performance soyou know could you use some of thesetechniques to to do an unsupervisedimage to image translation umand um what we found wasand it may go back to that firstparagraph of the sim clr paper which iswhen youwell maybe it doesn't i'm not sure aboutthat but umit seems like a lot of energy is putintovisually appealing translations but itkind of doesn't really fool the modelthat you're trying to train so we werenot able to successfully perform domainadaptation uh using these techniques andi thinkif i had to hypothesize i think at leastin part againimage that images are often very largeobjects very well framed and in thegeospatial applications that we wereapplying you know you've got muchsmaller objects and so it was much amuch more difficult challenge if youhave a picture of a horse that's takingup 75 of the pixels and you translatethat into a dog i'm suspecting that'llprobably be useful um and morebelievable um but uh this is probably ithink the more umi'll go out on a limb i don't think thisis backed up by research but i think myobservation of just a generativeadversarial id of adversarially trainedmodels in general is that they seem towork better for these very large wellcropped images as opposed to lots offine grained images so in theapplications we've tried it you know wewere not able to umto to to get success i do think thereare some startups in the space of youknow combining um3d generated graphics but then usinggans for more textures which presumablymaybe that's where a little bit strongerso you can imagine having a 3d gameenginegenerate somestreet level imagery and then use againto kind of texturize it into rainy ornight time scenes maybe that adds morediversity to the datai can imagine something like thatworking um haven't done the researchmyselfyeah i've always been excited about thisidea that we can just use kind of dataaugmentation and maybe even like the waythat cyclogan enables a more flexibleinterface to cover a bigger distributionour data andit seems to me like deep learning canalways fit to the training set almost soit's like if you just blow up thetraining set with these priors aboutsemanticpreservation sort of that you can justdo anything sort of with it it's alwaysbeen the guy and think about it i wantedto kind of dive uh reference let me justpause you right there and just mentionthat you know what it seems like theseuh adversarially trainedapproaches umthe distribution actually gets narrowernot more diverse so i mean if you justif you just fire up style again and youlook at the faces that come out of ityou're like oh my god these images lookfantastic they look so realistic youknowbut then like um if you just you knoweven the paper they admit themselves youknowpart of the reason they look so good isthey're sampling kind of the very narrowparts of the distribution when you pushthat latentthe thethe embedding push it to really wide itjust gets really kind of like broken sothere's actually a shortage of diversitynot too much diversity but not enoughwithin and that's why it looks so goodit's maybe maybe and again i'm going togo out on limb i don't know if this istrue but maybe that's whythey look so good as they get reallygood at you know delivering 100 000faces you know really effectively butthey can't really represent the full youknow diversity of you know six billionyou know different faces umi think umthe the the github for uh that paperthat she wants a co-author onfoon it few shot unsupervised imageimage translations it's called it has animage of a dog rotating and all theseother breeds of dogs rotating the sameway you're like wow they've reallytransferred that over but look closelyall the noses are the same sizethat's just a little trick you juststare at it along enough you startnoticing these little tricks like oh didit really do a full translation allspecies breeds of dogs or is it justbelievable to me and you that these 10dogs look similar but it actuallydoesn't really represent anything closeto the full diversity of dog breeds muchless dog poses much less dog poses indifferent environments so maybe that'sone thing i'll just go on a limb and sayhow about i call that future workhypothesis rather than conclusion ortrendyeah well that inspires me thinkingabout like um i recently dove into apaper called augmax where they flip itto its friendly adversarial training andthey also do this uh like tsne spaceclustering where they show you that uhcertain generative techniques theyexactly as you describe they just makeit super dense on the yeahyeahyeah nice i hadn't read about that butobviously you know i'm familiar with tsneeze and i love actually you know ourlogo is actually inspired by a t-snelike when we were designing the logo itshould be blobby like a t-sne so that'sexactly why our logo is what it isoh yeah i see some cool art art whereit's like umsorry i'm gesturing like a t-shirt ihaven't seen this on a t-shirt yet buti've seen like uh the the data set usedto train hugging faces bloom model i'veseen that put into an embedding spaceand it's like colored by languages thesekind of ts yeah yeah yeah yeah a thetensorflow embedding has an open sourcetool you can you know just just clustermnist and all the dots will fly aroundyou can use other clustering techniquesi think umap and uh but yeah the ideathat you know these these things aremeant to be blobby this is the realworld this is not a tabular world likeif we're a tabular company we'd have agrid right like a good tabular worldwhere everything's neat and organizedbut no this is messy this is deeplearning this is this is the real worldyeah and umwe had so i guess uh it's like umso we certainly could continue thisconversation for a while i think there'squite a few ideas in this whole thingbut i i do want to kind of step out alittle bit for the sake of keeping ourpodcast a little dense and and maybekind of asking about like how you seethe we vva vector search technologyplaying with masterful and it kind ofcoming back into like the tools and thesoftware yeah absolutely absolutely so imean just stepping back to you know ourgoal is to provide more high level toolsfor developers so they can access allthe power of deep learning without youknow spending two years becoming youknow deep learning experts and readingall the papers you've read right likelikewhat what person building a back-end appwas gonna read all the papers onrelational theory and relational algebraand relational calculus like you don'thave to do that anymore right you justyou learn enough sql to do what you needto do and you got all this power that'ssitting behind the surface that youdon't need to know about anymore i meanthat is the vision we're trying to getto here um so we currently supportclassification object detection andsemantic segmentation and obviouslythere's a whole bunch more computervision tasks that we want to support umand there's something unique about uhsimilarity search you know whetheryou're applying it for computer visionor for uh text which is umin classification and and the othertasks i mentioned once you have atrained modelyou're ready to go to inferenceput it up in you know tensorflow servingor you know onyx runtime put it on akubernetes cluster or you can use youknow easier packages like you knowgoogle vertex uh endpoints you're readyto inferent you're ready to runinference you knowintegrate with your appin similarity search you are not doneonce the model is trained because nowyou've generated all these embeddingsand you've got you know a million ofthemwell now we've got to search over all ofthem so to deliver a complete product toour users obviously we need to partnerum with a company like like we v8because you've got to put thoseembeddings into the database to thenallow you know users to do that reverselookup you know indexing down throughthose embeddings and then rediscoverlike which faces is is this or whatbuilding or what room is thisembedding represent um so umour our initial i think uhthought on working together just as aproof of concept would just be to use aclassifier that's that's trained usingyou know our techniques of supervisedandsemi-supervised techniques taking anintermediate and betting and just seeingis that sort of a good enough mvp uhembedding just uh just to kind of provethe point that you know we can build anend-to-end product that involvestraining andthe vector search database umand so once we can prove that then um uhyou know it gives us you know uh uh moreconfidence that when we do kind of uh doa full on hardcore similarity search uhsupport that we'll already have all theintegration set up so we can deliver acomplete solution for our customers soagain they don't have to understandhierarchical small navigable worlds theydon't have to you know understand uhrandog they don't have to understand uhsim clr they can just say hey i gotthese two tools that'll solve that forme put in the training dataout comes the train model run theembedding put the embeddings in yourdatabase and now i've got a way to lookup any image by running inference getthe embedding look it up in the databaseand i can figure out what is what's thenearest you know image or person orthing that this image represents so uhso i'm excited to uh to pursue that andyou know we've already started some ofour uhsome of our uhproof of concept work on thisyeah that's super interesting i i love imean it perfectly said that thecombination between the model trainingthe end-to-end and then having thedatabase where you stored therepresentations that have been learnedand then the way that the these hmswstructures you don't want to do a dotproduct with a million imagesso the way they play together is sointeresting and i think that's also likekind of the interpretability of it likewhen you have a object detectioninference you can be like it looks likethis i've seen this kind of thing beforesort of do you like that kind ofinterpretability play of it and sorryactually let mei think this is a good chance to kind ofwe talk about retrieve then readpipelines all the time in naturallanguage processing because in naturallanguage processing it's so intuitiveyou have data sets like squad where yousay what is the atomic number of oxygenso naturally if you retrieve the contextof wikipedia going oxygen you know theatomic number eight and then you justcan classify it there so with nlp itmakes so much sense but can i ask youabout this retrieve then read uh forcomputer vision tasks likeclassification detection andsegmentation to just start thereyeah well i would i would i would uhframe that as i you knowcurrently would think of similaritysearch as a separate computer visiontask to support um uhin addition to our you know binaryclassification multi-labelclassification single label captioningobject detection semantic segmentationum other computer vision tasks we wantto support over time would be you knowkey point detection instancesegmentation uh pose estimation uhobject tracking um but what's a littlebit unique about uh this is uh you knowwe've built in um some unsupervisedpre-training techniques which um youknow i think as you know are evensuperior methods to generate embeddingsum but it's also superior to a userbecause they don't have to label theirdata to get the similarity searchnecessarily um the labels will help it'syet another data pointso um i'm not sure if i've answered yourquestion buti would just frame it as anothercomputer vision task for us to supportand then like i said as a proof ofconcept we thought could we just take aclassifier train using traditionaltechniques and use that embedding uh asa similarity just as a proof of conceptto to get our products to work togetherwith with the support of tasks that wehave today and like i said that alreadyuh seems to be a positiveyeah super cool and yeah i think with umlike self-supervised similarity learningis so sophisticated in computer visionwith again simclear and barlow twins thedata augmentation interface is so strongthat you get this search thing worksextremely well without labels and ithink that's just incredibly interestingi can ask like broadly um what drew yourinterest to computer vision if you kindof tell the story ofyeah yeah umit goes back a long way uhso it goes back to just being a gamer inhigh school and uh you know you somepeople like me get excited about youknow can i render my own computergraphics at some pointum and so there's that you knowi went and started my computer scienceprogram in an undergrad at berkeley youknow i i put down computer graphicsthat's a specialty for it and you knowonce you render something then you wantto like know what you rendered and iremember like hacking z buffers just totry to figure out you know basically youwould call it segmentation today if youcould access the you know the z bufferthat you knowfrustums and all that stuff and umi had a professor john forsythe whowrotean image classifier for horsesand that basically ended my interest incomputer graphics and computer vision inthe 90s because it was just soheuristics based it was just all theselike hand crafted ideas if we just throwthis and this and this and wait allthese little like hand-crafted rulesthen maybe we can figure out what ahorse is and i said that is just so notelegant um and i ended up going todatabases ironically enough having achat with it with a guy from a databasecompany because databases are appliedalgorithms you know b-plus trees arejust you know wide red black treesit was just such an elegant space andyou know indexesno no question my favorite part and soit was a really real really enjoyablefor me to to learn about uhhnsws and just seeing you know an indexapplied to a graph instead of you know avector or a heap it's uh it's been it'sbeen nice anyways umso i was out of computer vision you knowfrom the 90s until about five years agouh so my co-founder sam wookie our ctohe stayed in computer vision he'sfounded a startup he ended up selling tothe nfl to track you know playermovements on the field pre-deep learningand when deep learning came out i didn'tknow anything about it uh but he wasfollowing all this stuff he's like oh mygoddeep learning can correct all thecomputer vision problems that like ithought were uncrackable at the time sohe left microsoft and joined google justso he could kind of join the mecca ofdeep learning and and kind of you knowlearn learn the learn the secrets uh youknowso umuh you know sam sam umuhsamsaid to meall the stuff is workingtake the andrewing coursera course on mlto get up to speedtake the enduring deep learningspecialization and i hadn't programmedin 15 years and i did and first of all irealized i enjoyed programming butsecond againor still and then secondly i was justamazed by the power of deep learninglike holy cow from all those heuristicsfrom that john forsythe and i'm i'm notdenigrating his papers just that was astate of the art at the time but it justwasn't for me but there's just anelegance toto just having machines train themselvesuh and so so i was hooked so that's sortof my return my first and second uh loveaffair with uh with with deep learningand computer visionuh that's the magic of deep learningthe whole input outputit's amazing it's absolutely amazingit's amazingyeah i had a sim the uh story of uh samin the nfl thing i i the first computervision project i dove into isi was i had full basketball games and iwas trying to extract like made basketsand i think once like kind of comingback to your point about imagenet andbeing object centric and you know i igot started studying like c far 10 ithinkand yeah once you get into like a realproblem you really appreciate thesethings like uh class imbalance or uhjust the challenge of labeling data andwhy these semi supervised like once youget a real project you'll reallyappreciate the semi-supervised learningthing just because of the cost oflabeling the data andall that cool stuff uh so one otherquestion i really want to ask you aboutis is your experience at masterful aican you tell me about the inception ofmasterful and all that kind of stuffabsolutely well i mean the beginning ofthis story is just exactly where my laststory you know trailed off which is youknow sam got me super excited about deeplearning um he was at the mecca you knowat google but you know he startedtelling me some of the um the ugly truthabout deep learning um which is hugemodels huge compute budgets you know uhhuge labeling budgetsand you know sam instinctively was justlike this is cool but like how do wemake this work foryou know just just you know maybe peoplecompanies with two deep learningengineers or maybe just a general youknow software engineer like a lot of ourfriends who you know don't want to spendtwo years learning all this stuff andthat was the basic instinct that he hadand umand he thought the technology was thereuh to achieve that um which is you knowall the stuff in our platform and thenon the flip side umthe technology may have been there andit may have been his instinct but iwanted to see if there was really amarket demand for it so umi linked up with another friend of mineuh who i respect a lot tom reichardwho's now my third co-founder umhe uh had been a venture capitalist uhor was then still a venture capitalisthad been one for about 10 years so ireally trusted his insight in kind ofwhat the market wasand um in particular he like he wasn'tone of those guys investing in socialyou know media networks he was he wasyou know investing in deep tech so heinvested in a lot of ai companies and hesaid yeah i've seen lots of startupstrying to commercialize deep learningand it's incredibly hard to get themodels going so kind of between my youknow inspiration and the space from samand his kind of idea that the technologywas ready to go and then tom saying yeahthe demand is there we said well if bothof those things are there then you knowit's the right time for us to kind ofbuild a startup to to kind of bridgethat gap and so that's the birth of masstools so i've got two just you knowabsolutely amazing co-founders and tomand sam and uh and we barked on this uhthis journeycan i ask about without sort of sayinglike could you describe the competitionin the space could i ask about sort ofthe the category that you think thatmasterful falls into in some adjacentcategories yeah absolutely umthe most narrow and closest adjacent tous would be umthe automl providers and umyou know we live up to the same promiseas automl feed and data and get back atrain model where we've been a littlereluctant to brand ourselves there is umwe philosophically are areare not in the same place as as most ofthe existing auto mal providers umthey are very neural architectureoriented whereas our hypothesisfundamentally is more data centricwe think it's a lot less interesting totry to fine-tune the perfect model sizeand it's a lot more interesting tounderstand you know how to get the mostout of those architectures so in our inour command line interface for exampleyou can you can select the rightarchitecture we think you should pickthe architecture based on your inferencelatency you know throughput you knowcost requirements which you can predictjust from you know the architecture soif you're on an edgy device you may needa certain device if you if you havemaximum accuracy then pick efficient atb7 you're going to get like 99.9 of allthe improvements that you can fromarchitecture just by doing thatthe interesting thing is how do youmanipulate the data throughregularization how do you add unlabeleddataso so i would say that they are broadlyour competition we have a very differentphilosophy um and not to plug my productbut you know we put out the blog reportsshowing how we beat like like googlevertex automl on a whole series of datasets um because i think we take a moreholistic approachand a more data centric approach um tothe problem um kind of expanding thering a little bitum thinking about less as just sort ofdata in and model out think of this asdollars in and more accuracy out youmight say that we also kind of kind ofmaybe you might imagine we kind of bumpup up against like active learning alittle bit you know again this idea ofyou know can i be more efficient with mylabeling spend but the good news is wedon't really compete in that space asmuch as these are complementarytechniques um people can easily haveboth umso um that kind of contextualizes kindof where we fit um in the space and kindof what what other platforms somebodymight be you knowcomparing us to depending on you knowwhat profile they're inawesome well yeah shane thank you somuch for this i'm so grateful for youtaking your time to do this and i'm solucky to have like such a informationheavy podcast in our web podcastcollections is really amazing so thankyou so muchi've super enjoyed it i didn't realizewe're gonna go like this technical butuh but i'm glad we did because it's umit's just so inspirational you know andsometimes it's hard toit's hard to describe to you know myparents like why this stuff is just soexciting but um i mean the breakthroughsare justit's umit's it's a perfect moment where there'sa ton of commercial opportunities sothere's a lot of money to fund thisresearch and there truly is like justgroundbreaking stuff coming out i meanit's it's deep learning is going to beway more important than the internet uhyou know in my in my humble opinion umbecause it touches real world things ittouches you know where people really areout there you know driving andthinking you knowlooking know agriculture andmanufacturing it's just um i'm superexcited about a spaceyeah awesome", "type": "Video", "name": "Yaoshiang Ho on Masterful AI - Weaviate Podcast #22", "path": "", "link": "https://www.youtube.com/watch?v=dHhQjlrLu9k", "timestamp": "", "reader": "JSON", "meta": {}, "chunks": []}