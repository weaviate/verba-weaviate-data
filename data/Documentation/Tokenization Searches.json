{"text": "\nYou saw how [tokenization affects filters](./300_filters.mdx). They impact keyword searches in a similar, but not identical, way. In this section, we'll see how different tokenization methods impact search results.\n\nA hybrid search combines results from a keyword search and a vector search. Accordingly, tokenization impacts the keyword search part of a hybrid search, while the vector search part is not impacted by tokenization.\n\nWe will not separately discuss hybrid searches in this course. However, the impact on keyword searches discussed here will apply to the keyword search part of a hybrid search.\n\n##  Impact on keyword searches\n\n###  How tokenization impacts keyword searches\n\nWe will use a similar method as in the previous section, with a difference being that we will now perform a keyword search instead of a filter.\n\nA keyword search ranks results using the [BM25f algorithm](https://en.wikipedia.org/wiki/Okapi_BM25). As a result, the impact of tokenization on keyword searches is twofold.\n\nFirstly, tokenization will determine whether a result is included in the search results at all. If none of the tokens in the search query match any tokens in the object, the object will not be included in the search results.\n\nSecondly, tokenization will impact the ranking of the search results. The BM25f algorithm takes into account the number of matching tokens, and the tokenization method will determine which tokens are considered matching.\n\n###  Search setup\n\nEach keyword query will look something like this.\n\nWe'll set up a reusable function to perform keyword searches, and display the top results along with their scores.\n\n\n\n###  Examples\n\n#### \"**Clark:** \"vs \"**clark**\" - messy text\n\nKeyword searches are similarly impacted by tokenization as filters. However, there are subtle differences.\n\nTake a look at this example, where we search for various combinations of substrings from the TV show title `\"Lois & Clark: The New Adventures of Superman\"`.\n\nThe table shows whether the query matched the title, and the score:\n\n|               | `word` | `lowercase` | `whitespace` | `field` |\n|---------------|--------|-------------|--------------|---------|\n| `\"clark\"`       | 0.613     | \u274c           | \u274c          | \u274c     |\n| `\"Clark\"`       | 0.613     | \u274c           | \u274c          | \u274c     |\n| `\"clark:\" `     | 0.613     | 0.48         | \u274c          | \u274c     |\n| `\"Clark:\" `     | 0.613     | 0.48         | 0.48        | \u274c     |\n| `\"lois clark\"`  | 1.226     | 0.48         | \u274c          | \u274c     |\n| `\"clark lois\"`  | 1.226     | 0.48         | \u274c          | \u274c     |\n\n\n  Python query & output\n\n\n\n\n\n\n\nHere, the same results are returned as in the filter example. However, note differences in the scores.\n\nFor example, a search for `\"lois clark\"` returns a higher score than a search for `\"clark\"`. This is because the BM25f algorithm considers the number of matching tokens. So, it would be beneficial to include as many matching tokens as possible in the search query.\n\nAnother difference is that a keyword search will return objects that match any of the tokens in the query. This is different from a filter, which is sensitive to the filtering operator. Depending on the desired result, you could use an `\"Equal\"` operator, `\"ContainsAny\"`, or `\"ContainsAll\"`, for example.\n\nThe next section will demonstrate this, as well as how stop words are treated.\n\n#### \"**A mouse**\" vs \"**mouse**\" - stop words\n\nHere, we search for variants of the phrase \"computer mouse\", where some queries include additional words.\n\nNow, take a look at the results.\n\n**Matches for `\"computer mouse\"`**\n\n|                              | `word`    | `lowercase` | `whitespace` | `field` |\n|------------------------------|-----------|-------------|--------------|---------|\n| `\"computer mouse\"`           | 0.889     | 0.819       | 1.01         | 0.982   |\n| `\"Computer Mouse\"`           | 0.889     | 0.819       | \u274c            | \u274c      |\n| `\"a computer mouse\"`         | 0.764     | 0.764       | 0.849        | \u274c      |\n| `\"computer mouse pad\" `      | 0.764     | 0.764       | 0.849        | \u274c      |\n\n**Matches for `\"a computer mouse\"`**\n\n|                              | `word`    | `lowercase` | `whitespace` | `field` |\n|------------------------------|-----------|-------------|--------------|---------|\n| `\"computer mouse\"`           | 0.889     | 0.819       | 1.01         | \u274c      |\n| `\"Computer Mouse\"`           | 0.889     | 0.819       | \u274c           | \u274c      |\n| `\"a computer mouse\"`         | 0.764     | 1.552       | 1.712        | 0.982   |\n| `\"computer mouse pad\" `      | 0.764     | 0.688       | 0.849        | \u274c      |\n\n\n  Python query & output\n\n\n\n\n\n\n\nThe results here are similar to the filter example, but more nuanced and quite interesting!\n\nUnder `word` tokenization, the search for `computer mouse` produces identical results to the search for `a computer mouse`. This is because the stop word `a` is not considered in the search.\n\nBut note that the scores are different for returned objects where the only differences are stopwords, such as `\"computer mouse\"` and `\"a computer mouse\"`. This is because the BM25f algorithm does [index stopwords](../../../weaviate/config-refs/schema/index.md#invertedindexconfig--stopwords-stopword-lists), and they do impact the score.\n\nAs a user, you should keep this in mind, and you can configure the stop words in the collection definition to suit your desired behavior.\n\nAnother interesting note is that the `lowercase` and `whitespace` tokenization methods do not remove stop words in the query.\n\nThis behavior allows users who want to include stop words in their search queries to do so.\n\n#### \"**variable_name**\" vs \"**variable name**\" - symbols\n\nThe table below shows keyword search results using the string `\"variable_name\"` and the resulting scores.\n\n|                              | `word`    | `lowercase` | `whitespace` | `field` |\n|------------------------------|-----------|-------------|--------------|---------|\n| `\"variable_name\"`            | 0.716     | 0.97        | 1.27         | 0.982   |\n| `\"Variable_Name:\" `          | 0.716     | 0.97        | \u274c            | \u274c      |\n| `\"Variable Name:\" `          | 0.716     | \u274c           | \u274c            | \u274c      |\n| `\"a_variable_name\"`          | 0.615     | \u274c           | \u274c           | \u274c      |\n| `\"the_variable_name\"`        | 0.615     | \u274c           | \u274c           | \u274c      |\n| `\"variable_new_name\" `       | 0.615     | \u274c           | \u274c           | \u274c      |\n\n\n  Python query & output\n\n\n\n\n\n\n\nThese results are once again similar to the filter example. If your data contains symbols that are important to your search, you should consider using a tokenization method that preserves symbols, such as `lowercase` or `whitespace`.\n\n###  Discussions\n\nThat's it for keyword searches and tokenization. Similarly to filters, the choice of tokenization method is a big part of your overall search strategy.\n\nOur generally advice for tokenization in keyword searching is similar to [our advice for filtering](./300_filters.mdx#discussions). Start with `word`, and consider others such as `lowercase` or `whitespace` if symbols, or cases encode important information in your data.\n\nUsing `field` tokenization may be too strict for keyword searches, as it will not match any\nobjects that do not contain the exact string in the exact order.\n\nLastly, keep in mind that keyword searches produce ranked results. Therefore, tokenization will not only affect the results set but also their ranking within the set.\n\nWith these considerations in mind, you can configure your tokenization strategy to best suit your data and your users' needs.\n\n\n", "type": "Documentation", "name": "Tokenization Searches", "path": "developers/academy/py/tokenization/400_searches.mdx", "link": "https://weaviate.io/developers/academy/py/tokenization/searches", "timestamp": "2024-05-08 10:48:23", "reader": "JSON", "meta": {}, "chunks": []}