{"text": "\n# ANN Benchmark\n\nThis vector database benchmark is designed to measure and illustrate Weaviate's Approximate Nearest Neighbor (ANN) performance for a range of real-life use cases.\n\nThis is not a comparative benchmark that runs Weaviate against competing vector database solutions. \nIf you'd like to discuss trade-offs with other solutions, please [contact sales](https://weaviate.io/pricing#contact-sales).\n\nTo make the most of this vector database benchmark, you can look at it from different perspectives:\n\n- **The overall performance** \u2013 Review the [benchmark results](#benchmark-results) to draw conclusions about what to expect from Weaviate in a production setting.\n- **Expectation for your use case** \u2013 Find the dataset closest to your production use case, and estimate Weaviate's expected performance for your use case.\n- **Fine Tuning** \u2013 If you don't get the results you expect. Find the optimal combinations of the configuration parameters (`efConstruction`, `maxConnections` and `ef`) to achieve the best results for your production configuration. (See [HNSW Configuration Tips](https://weaviate.io/developers/weaviate/config-refs/schema/vector-index#hnsw-configuration-tips))\n\n\n\n## Measured Metrics\n\nFor each benchmark test, we set these HNSW parameters:\n- **`efConstruction`** - Controls the search quality at build time.\n- **`maxConnections`**\t - The number of outgoing edges a node can have in the HNSW graph.\n- **`ef`** - Controls the search quality at query time.\n\nFor good starting point values and performance tuning advice, see [HNSW Configuration Tips](https://weaviate.io/developers/weaviate/config-refs/schema/vector-index#hnsw-configuration-tips).\n\n\n\nFor each set of parameters, we've run 10,000 requests, and we measured the following metrics:\n\n- The **Recall@1**, **Recall@10**, **Recall@100** - by comparing Weaviate's results to the ground truths specified in each dataset.\n- **Multi-threaded Queries per Second (QPS)** - The overall throughput you can\n  achieve with each configuration.\n- **Individual Request Latency (mean)** - The mean latency over all 10,000 requests.\n- **P99 Latency** - 99% of all requests (9,900 out of 10,000) have a latency that is lower than or equal to this number \u2013 this shows how fast\n- **Import time** - Since varying build parameters has an effect on import time, the import time is also included.\n\nBy request, we mean:\nAn unfiltered vector search across the entire dataset for the given test. All\nlatency and throughput results represent the end-to-end time that your\nusers would also experience. In particular, these means:\n\n* Each request time includes the network overhead for sending the results over the\n  wire. In the test setup, the client and server machines were located in the\n  same VPC.\n* Each request includes retrieving all the matched objects from disk. This is\n  a significant difference from `ann-benchmarks`, where the embedded libraries\n  only return the matched IDs.\n\nThis benchmark is [open source](https://github.com/weaviate/weaviate-benchmarking), so you can reproduce the results yourself.\n\n## Benchmark Results\n\n\nThis section contains datasets modeled after the [ANN Benchmarks](https://github.com/erikbern/ann-benchmarks). Pick a dataset that is closest to your production workload:\n\n\n| **Dataset** | **Number of Objects** | **Vector Dimensions** | **[Distance metric](https://weaviate.io/blog/distance-metrics-in-vector-search)** | **Use case** |\n| --- | --- | --- | --- | --- |\n| [SIFT1M](http://corpus-texmex.irisa.fr/) | 1 M | 128 | Euclidean | This dataset reflects a common use case with a small number of objects. |\n| [Glove-25](https://nlp.stanford.edu/projects/glove/) | 1.28 M | 25 | Cosine | Because of the smaller vectors, Weaviate can achieve the highest throughput on this dataset. |\n| [Deep Image 96](https://sites.skoltech.ru/compvision/noimi/) | 10 M | 96 | Cosine | This dataset gives a good indication of expected speed and throughput when datasets grow. It is about 10 times larger than SIFT1M, but the throughput is only slightly lower. |\n| [GIST 960](http://corpus-texmex.irisa.fr/) | 1 M | 960 | Euclidean | This dataset highlights the cost of high-dimensional vector comparisons. It has the lowest throughput of the sample datasets. Use this one if you run high-dimensional loads. |\n\n#### Benchmark Datasets \nThese are the results for each dataset:\n\n\n\n\n#### QPS vs Recall for SIFT1M\n\n![SIFT1M Benchmark results](./img/benchmark_sift_128.png)\n\n\n\n\n\n#### Recommended configuration for SIFT1M\n\n\n| `efConstruction` | `maxConnections` | `ef` | **Recall@10** | **QPS (Limit 10)** | **Mean Latency (Limit 10**) | **p99 Latency (Limit 10)** |\n| ----- | ----- | ----- | ----- | ----- | ----- | ----- | \n| 128 | 32 | 64 | 98.83% | 8905 | 3.31ms | 4.49ms |\n\n\n\n\n\n#### QPS vs Recall for Glove-25 \n\n![Glove25 Benchmark results](./img/benchmark_glove_25.png)\n\n\n\n\n\n#### Recommended configuration for Glove-25\n\n\n| `efConstruction` | `maxConnections` | `ef` | **Recall@10** | **QPS (Limit 10)** | **Mean Latency (Limit 10**) | **p99 Latency (Limit 10)** |\n| ----- | ----- | ----- | ----- | ----- | ----- | ----- | \n| 64 | 16 | 64 | 95.56% | 15003 | 1.93ms | 2.94ms |\n\n\n\n\n#### QPS vs Recall for Deep Image 96\n\n![Deep Image 96 Benchmark results](./img/benchmark_deepimage_96.png)\n\n\n\n\n\n#### Recommended configuration for Deep Image 96\n\n\n| `efConstruction` | `maxConnections` | `ef` | **Recall@10** | **QPS (Limit 10)** | **Mean Latency (Limit 10**) | **p99 Latency (Limit 10)** |\n| ----- | ----- | ----- | ----- | ----- | ----- | ----- | \n| 128 | 32 | 64 | 96.43% | 6112 | 4.7ms | 15.87ms |\n\n\n\n\n#### QPS vs Recall for GIST 960 \n\n![GIST 960 Benchmark results](./img/benchmark_gist_960.png)\n\n\n\n\n\n\n#### Recommended configuration for GIST 960 \n\n\n| `efConstruction` | `maxConnections` | `ef` | **Recall@10** | **QPS (Limit 10)** | **Mean Latency (Limit 10**) | **p99 Latency (Limit 10)** |\n| ----- | ----- | ----- | ----- | ----- | ----- | ----- | \n| 512 | 32 | 128 | 94.14% | 1935 | 15.05ms | 19.86ms |\n\n\n\n\n\n\n\n\n## Benchmark Setup\n\n### Scripts\n\nThis benchmark is [open source](https://github.com/weaviate/weaviate-benchmarking), so you can reproduce the results yourself.\n\n### Hardware\n\n![Setup with Weaviate and benchmark machine](/img/docs/weaviate_benchmark_setup.png)\n\nThis benchmark test uses two GCP instances within the same VPC:\n\n* **Benchmark** \u2013 a `c2-standard-30` instance with 30 vCPU cores and 120 GB memory \u2013 to host Weaviate.\n* **Script** \u2013 a smaller instance with 8 vCPU \u2013 to run benchmarking scripts.\n\n\n* It is large enough to show that Weaviate is a highly concurrent [vector search engine](https://weaviate.io/blog/what-is-a-vector-database).\n* It scales well while running thousands of searches across multiple threads.\n* It is small enough to represent a typical production case without inducing\n  high costs.\n\nBased on your throughput requirements, it is very likely that you will run Weaviate\non a considerably smaller or larger machine in production.\n\nWe have outlined in the [Benchmark FAQs](#what-happens-if-i-run-with-fewer-or-more-cpu-cores-than-on-the-example-test-machine)\n what you should expect when altering the configuration or\nsetup parameters.\n\n### Experiment Setup\n\nWe modeled our dataset selection after \n[ann-benchmarks](https://github.com/erikbern/ann-benchmarks). The same test\nqueries are used to test speed, throughput, and recall. The provided ground\ntruths are used to calculate the recall.\n\nWe use Weaviate's Python client to import data. \nWe use Go to measure the concurrent (multi-threaded) queries.\n Each language has its own performance characteristics. \n You may get different results if you use a different language to send your queries. \n\nFor maximum throughput, we recommend using the [Go](/developers/weaviate/client-libraries/go.md) or\n[Java](/developers/weaviate/client-libraries/java.md) client libraries.\n\nThe complete import and test scripts are available [here](https://github.com/weaviate/weaviate-benchmarking).\n\n## Benchmark FAQ\n\n### How can I get the most performance for my use case?\nIf your use case is similar to one of the benchmark tests, use the recommended HNSW parameter configurations to start tuning.\n\nFor more instructions on how to tune your configuration for best performance, see [HNSW Configuration Tips](https://weaviate.io/developers/weaviate/config-refs/schema/vector-index#hnsw-configuration-tips).\n\n### What is the difference between latency and throughput?\n\nThe latency refers to the time it takes to complete a single request. This\nis typically measured by taking a mean or percentile distribution of all\nrequests. For example, a mean latency of 5ms means that a single request takes, on average, 5ms to complete. This does not say anything about how many queries\ncan be answered in a given timeframe.\n\nIf Weaviate were single-threaded, the throughput per second would roughly equal\nto 1s divided by mean latency. For example, with a mean latency of 5ms, this\nwould mean that 200 requests can be answered in a second.\n\nHowever, in reality, you often don't have a single user sending one query after\nanother. Instead, you have multiple users sending queries. This makes the\nquerying side concurrent. Similarly, Weaviate can handle concurrent incoming\nrequests. We can identify how many concurrent requests can be served by measuring\nthe throughput.\n\nWe can take our single-thread calculation from before and multiply it with the\nnumber of server CPU cores. This will give us a rough estimate of what the\nserver can handle concurrently. However, it would be best never to trust this\ncalculation alone and continuously measure the actual throughput. This is because\nsuch scaling may not always be linear. For example, there may be synchronization\nmechanisms used to make concurrent access safe, such as locks. Not only do\nthese mechanisms have a cost themselves, but if implemented incorrectly, they\ncan also lead to congestion, which would further decrease the concurrent\nthroughput. As a result, you cannot perform a single-threaded benchmark and\nextrapolate what the numbers would be like in a multi-threaded setting.\n\nAll throughput numbers (\"QPS\") outlined in this benchmark are actual\nmulti-threaded measurements on a 30-core machine, not estimations.\n\n### What is a p99 latency?\n\nThe mean latency gives you an average value of all requests measured. This is a\ngood indication of how long a user will have to wait on average for\ntheir request to be completed. Based on this mean value, you cannot make any\npromises to your users about wait times. 90 out of 100 users might see a\nconsiderably better time, but the remaining 10 might see a significantly worse\ntime.\n\nPercentile-based latencies are used to give a more precise indication. A\n99th-percentile latency - or \"p99 latency\" for short - indicates the slowest\nrequest that 99% of requests experience. In other words, 99% of your users will\nexperience a time equal to or better than the stated value. This is a much\nbetter guarantee than a mean value.\n\nIn production settings, requirements - as stated in SLAs - are often a\ncombination of throughput and a percentile latency. For example, the statement\n\"3000 QPS at p95 latency of 20ms\" conveys the following meaning.\n\n- 3000 requests need to be successfully completed per second\n- 95% of users must see a latency of 20ms or lower.\n- There is no assumption about the remaining 5% of users, implicitly tolerating\n  that they will experience higher latencies than 20ms.\n\nThe higher the percentile (e.g. p99 over p95) the \"safer\" the quoted\nlatency becomes. We have thus decided to use p99-latencies instead of\np95-latencies in our measurements.\n\n### What happens if I run with fewer or more CPU cores than on the example test machine?\n\nThe benchmark outlines a QPS per core measurement. This can help you make a\nrough estimation of how the throughput would vary on smaller or larger\nmachines. If you do not need the stated throughput, you can run with fewer CPU\ncores. If you need more throughput, you can run with more CPU cores.\n\nPlease note that there is a point of diminishing returns with adding more CPUs because of synchronization mechanisms, disk, and memory bottlenecks. Beyond that point, you can scale horizontally instead of vertically. Horizontal scaling with replication will be [available in Weaviate soon](/developers/weaviate/roadmap/index.md).\n\n### What are `ef`, `efConstruction`, and `maxConnections`?\n\nThese parameters refer to the [HNSW build and query\nparameters](/developers/weaviate/config-refs/schema/vector-index.md#how-to-configure-hnsw).\nThey represent a trade-off between recall, latency & throughput, index size, and\nmemory consumption. This trade-off is highlighted in the benchmark results.\n\n### I can't match the same latencies/throughput in my own setup. How can I debug this?\n\nIf you are encountering other numbers in your own dataset, here are a couple of\nhints to look at:\n\n* What CPU architecture are you using? The benchmarks above were run on a GCP\n  `c2` CPU type, which is based on `amd64` architecture. Weaviate also supports\n  `arm64` architecture, but not all optimizations are present. If your machine\n  shows maximum CPU usage but you cannot achieve the same throughput, consider\n  switching the CPU type to the one used in this benchmark.\n\n* Are you using an actual dataset or random vectors? HNSW is known to perform\n  considerably worse with random vectors than with real-world datasets. This is due\n  to the distribution of points in real-world datasets compared to randomly\n  generated vectors. If you cannot achieve the performance (or recall)\n  outlined above with random vectors, switch to an actual dataset.\n\n* Are your disks fast enough? While the ANN search itself is CPU-bound, the objects\n  must be read from disk after the search has been completed. Weaviate\n  uses memory-mapped files to speed this process up. However, if not enough\n  memory is present or the operating system has allocated the cached pages\n  elsewhere, a physical disk read needs to occur. If your disk is slow,\n  it could then be that your benchmark is bottlenecked by those disks.\n\n* Are you using more than 2 million vectors? If yes, make sure to set the\n  [vector cache large enough](/developers/weaviate/concepts/resources.md#vector-cache)\n  for maximum performance.\n\n### Where can I find the scripts to run this benchmark myself?\n\nThe [repository is located here](https://github.com/weaviate/weaviate-benchmarking).\n\n\n", "type": "Documentation", "name": "Benchmarks Ann", "path": "developers/weaviate/benchmarks/ann.md", "link": "https://weaviate.io/developers/weaviate/benchmarks/ann", "timestamp": "2024-05-08 10:49:31", "reader": "JSON", "meta": {}, "chunks": []}