{"text": "\nWith Weaviate, you can perform semantic searches to find similar items based on their meaning. This is done by comparing the vector embeddings of the items in the database.\n\nAs we are using a multimodal model, we can search for objects based on their similarity to any of the supported modalities. Meaning that we can search for movies based on their similarity to a text or an image.\n\n##  Image query\n\n###  Code\n\nThis example finds entries in \"MovieMM\" based on their similarity to [this image of the International Space Station](https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/International_Space_Station_after_undocking_of_STS-132.jpg/440px-International_Space_Station_after_undocking_of_STS-132.jpg), and prints out the title and release year of the top 5 matches.\n\n\n  Query image\n\n![International Space Station](https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/International_Space_Station_after_undocking_of_STS-132.jpg/440px-International_Space_Station_after_undocking_of_STS-132.jpg)\n\n\n\n\n\n###  Explain the code\n\nThe results are based on similarity of the vector embeddings between the query and the database object. In this case, the vectorizer module generates an embedding of the input image.\n\nThe `limit` parameter here sets the maximum number of results to return.\n\nThe `return_metadata` parameter takes an instance of the `MetadataQuery` class to set metadata to return in the search results. The current query returns the vector distance to the query.\n\nNote that the results are very similar to the tone of the query image, as the top results are all space-themed movies.\n\n\n  Example results\n\nPosters for the top 5 matches:\n\n\n\n\n\n\nWeaviate output:\n\n```text\nInterstellar 2014 157336\nDistance to query: 0.354\n\nGravity 2013 49047\nDistance to query: 0.384\n\nArrival 2016 329865\nDistance to query: 0.386\n\nArmageddon 1998 95\nDistance to query: 0.400\n\nGodzilla 1998 929\nDistance to query: 0.441\n```\n\n\n\n###  Response object\n\nThe returned object is an instance of a custom class. Its `objects` attribute is a list of search results, each object being an instance of another custom class.\n\nEach returned object will:\n- Include all properties and its UUID by default except those with blob data types.\n    - Since the `poster` property is a blob, it is not included by default.\n    - To include the `poster` property, you must specify it and the other properties to fetch in the `return_properties` parameter.\n- Not include any other information (e.g. references, metadata, vectors.) by default.\n\n\n##  Text search\n\n###  Code\n\nThis example finds entries in \"MovieMM\" based on their similarity to the query \"red\", and prints out the title and release year of the top 5 matches.\n\n\n\n###  Explain the code\n\nThe results are based on similarity of the vector embeddings between the query and the database object. In this case, the vectorizer module generates an embedding of the input text.\n\nThe remaining parameters are the same as in the previous example.\n\nNote that the results actually include movies with red color themes in its poster. This is because the CLIP vectorizer encodes the color information of the image in the vectors.\n\n\n  Example results\n\nPosters for the top 5 matches:\n\n\n\n\n\n\nWeaviate output:\n\n```text\nDeadpool 2 2018 383498\nDistance to query: 0.670\n\nBloodshot 2020 338762\nDistance to query: 0.677\n\nDeadpool 2016 293660\nDistance to query: 0.678\n\n300 2007 1271\nDistance to query: 0.682\n\nThe Hunt for Red October 1990 1669\nDistance to query: 0.683\n```\n\n\n\n###  Response object\n\nThe returned object is in the same format as in the previous example.\n\n\n\n", "type": "Documentation", "name": "Mm_searches Multimodal", "path": "developers/academy/py/starter_multimodal_data/103_mm_searches/10_multimodal.mdx", "link": "https://weaviate.io/developers/academy/py/starter_multimodal_data/mm_searches/multimodal", "timestamp": "2024-05-08 10:48:06", "reader": "JSON", "meta": {}, "chunks": []}