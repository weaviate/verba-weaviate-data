{"text": "\n\n -->\n\nimport PreviewUnit from '../../_snippets/preview.mdx'\n\n\n\n## &nbsp;&nbsp;Overview\n\nIn the preceding section, we imported multiple chapters of a book into Weaviate using different chunking techniques. They were:\n\n- Fixed-length chunks (and 20% overlap)\n    - With 25 words per chunk, and\n    - With 100 words per chunk\n- Variable-length chunks, using paragraph markers, and\n- Mixed-strategy chunks, using paragraph markers and a minimum chunk length of 25 words.\n\nNow, we will use Weaviate to search through the book and evaluate the impact of the chunking techniques.\n\nSince the data comes from the first two chapters of a book about Git, let's search for various git-related concepts and see how the different chunking strategies perform.\n\n\n## &nbsp;&nbsp;Search / recall\n\nFirst of all, we'll retrieve information from our Weaviate instance using various search terms. We'll use a semantic search (`nearText`) to aim to retrieve the most relevant chunks.\n\n### &nbsp;&nbsp;Search syntax\n\nThe search is carried out as follows, looping through each chunking strategy by filtering our dataset. We'll obtain a couple of top results for each search term.\n\n\n\n\n\n\n\nUsing these search terms:\n- `\"history of git\"`\n- `\"how to add the url of a remote repository\"`\n\n### &nbsp;&nbsp;Results & discussions\n\nWe get the following results:\n\n#### Example 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe query in this example is a broad one on the `history of git`. The result is that here, the longer chunks seem to perform better.\n\nInspecting the result, we see that while the 25-word chunks may be semantically similar to the query `history of git`, they do not contain enough contextual information to enhance the readers' understanding of the topic.\n\nOn the other hand, the paragraph chunks retrieved - especially those with a minimum length of 25 words - contain a good amount of holistic information that will teach the reader about the history of git.\n\n#### Example 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe query in this example was a more specific one, for example one that might be run by a user looking to identify how to add the url of a remote repository.\n\nIn contrast to the first scenario, the 25-word chunks are more useful here. Because the question was very specific, Weaviate was able to identify the chunk containing the most suitable passage - how to add a remote repository (`git remote add  `).\n\nWhile the other result sets also contain some of this information, it may be worth considering how the result may be used and displayed. The longer the result, the more cognitive effort it may take the user to identify the relevant information.\n\n\n## &nbsp;&nbsp;Retrieval augmented generation (RAG)\n\nNext, let's take a look at the impact of chunking on RAG.\n\nWe discussed the relationship between chunk size and RAG earlier. Using shorter chunks will allow you to include information from a wider range of source objects than longer chunks, but each object will not include as much contextual information. On the other hand, using longer chunks means each chunk will include more contextual information, but you will be limited to fewer source objects.\n\nLet's try a few RAG examples to see how this manifests itself.\n\n### &nbsp;&nbsp;Query syntax\n\nThe query syntax is shown below. The syntax is largely the same as above, except for two aspects.\n\nOne is that to account for varying chunk sizes, we will retrieve more chunks where the chunk size is smaller.\n\nThe other is that the query has been modified to perform RAG, rather than a simple retrieval. The query asks the target LLM to summarize the results into point form.\n\n\n\n\n\n\n\n### &nbsp;&nbsp;Results & discussions\n\n#### Example 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe findings here are similar to the semantic search results. The longer chunks contain more information, and are more useful for a broad topic like the history of git.\n\n#### Example 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe results of the generative search here for `available git remote commands` are perhaps even more illustrative than before.\n\nHere, the shortest chunks were able to retrieve the highest number of `git remote` commands from the book. This is because we were able to retrieve more chunks from various locations throughout the corpus (book).\n\nContract this result to the one where longer chunks are used. Here, using longer chunks, we were only able to retrieve one `git remote` command, because we retrieved fewer chunks than before.\n\n#### Discussions\n\nYou see here the trade-off between using shorter and longer chunks.\n\nUsing shorter chunks allows you to retrieve more information from more objects, but each object will contain less contextual information. On the other hand, using longer chunks allows you to retrieve less information from fewer objects, but each object will contain more contextual information.\n\nEven when using LLMs with very large context windows, this is something to keep in mind. Longer input texts means higher fees for the API use, or inference time. In other words, there are costs associated with using longer chunks.\n\nOften, this is *the* trade-off that you will need to consider when deciding on the chunking strategy for a RAG use-case.\n\n&nbsp;&nbsp;Review\n\n\n\nAny quiz questions\n\n### &nbsp;&nbsp;Review exercise\n\nTry out ...\n\n### &nbsp;&nbsp;Key takeaways\n\nAdd summary\n\n\n\n\n", "type": "Documentation", "name": "chunking-example_search", "path": "developers/academy/standalone/chunking/40_example_search.mdx", "link": "https://weaviate.io/developers/academy/standalone/chunking/example_search", "timestamp": "2023-11-13 10:39:01", "reader": "JSON", "meta": {}, "chunks": []}