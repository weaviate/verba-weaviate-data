{"text": "\n\n## Overview\n\nThe `text2vec-transformers` module enables Weaviate to obtain vectors locally from text using a transformers-based model.\n\n`text2vec-transformers` encapsulates models in Docker containers, which allows independent scaling on GPU-enabled hardware while keeping Weaviate on CPU-only hardware, as Weaviate is CPU-optimized.\n\nKey notes:\n\n- This module is not available on Weaviate Cloud Services (WCS).\n- Enabling this module will enable the [`nearText` search operator](/developers/weaviate/api/graphql/search-operators.md#neartext).\n- This module is only compatible with models encapsulated in a Docker container.\n- [Pre-built images](#use-a-pre-built-image) are available with popular models.\n- You can also use other models, such as:\n    - By [building an image](#build-a-model) for any publicly available model from the [Hugging Face model hub](https://huggingface.co/models).\n    - By [building an image](#use-a-private-or-local-model) for any model compatible with Hugging Face's `AutoModel` and `AutoTokenizer`.\n\n\nTransformer model inference speeds are usually about ten times faster with GPUs. If you have a GPU, use one of the GPU enabled models.\n\nIf you use `text2vec-transformers` without GPU acceleration, imports or `nearText` queries may become bottlenecks. The ONNX-enabled images can use [ONNX Runtime](https://onnxruntime.ai/) for faster inference processing on CPUs. Look for the `-onnx` suffix in the image name.\n\nAlternatively, consider one of the following options:\n\n- an API-based module such as [`text2vec-cohere`](./text2vec-cohere.md) or [`text2vec-openai`](./text2vec-openai.md)\n- a local inference container such as [`text2vec-contextionary`](./text2vec-contextionary.md) or [`text2vec-gpt4all`](./text2vec-gpt4all.md)\n\n\n## Weaviate instance configuration\n\nThis module is not available on Weaviate Cloud Services.\n\n### Docker Compose file\n\nTo use `text2vec-transformers`, you must enable it in your Docker Compose file (e.g. `docker-compose.yml`).\n\nWhile you can do so manually, we recommend using the [Weaviate configuration tool](/developers/weaviate/installation/docker-compose.md#configurator) to generate the `Docker Compose` file.\n\n#### Parameters\n\nWeaviate:\n\n- `ENABLE_MODULES` (Required): The modules to enable. Include `text2vec-transformers` to enable the module.\n- `DEFAULT_VECTORIZER_MODULE` (Optional): The default vectorizer module. You can set this to `text2vec-transformers` to make it the default for all collections.\n- `TRANSFORMERS_INFERENCE_API` (Required): The URL of the default inference container.\n- `USE_SENTENCE_TRANSFORMERS_VECTORIZER` (Optional): (EXPERIMENTAL) Use the `sentence-transformer` vectorizer instead of the default vectorizer (from the `transformers` library). Applies to custom images only.\n\nInference container:\n\nAs of Weaviate `v1.24.2`, you can use multiple inference containers with `text2vec-transformers`. This allows you to use different models for different collections by [setting the `inferenceUrl` in the collection configuration](#collection-configuration).\n\n- `image` (Required): The image name of the inference container.\n- `ENABLE_CUDA` (Optional): Set to `1` to enable GPU usage. Default is `0` (CPU only).\n\n#### Example\n\nThis configuration enables `text2vec-transformers`, sets it as the default vectorizer, and sets the parameters for the Transformers Docker container, including setting it to use `sentence-transformers-multi-qa-MiniLM-L6-cos-v1` image and to disable CUDA acceleration.\n\n```yaml\nversion: '3.4'\nservices:\n  weaviate:\n    image: cr.weaviate.io/semitechnologies/weaviate:||site.weaviate_version||\n    restart: on-failure:0\n    ports:\n     - 8080:8080\n     - 50051:50051\n    environment:\n      QUERY_DEFAULTS_LIMIT: 20\n      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\n      PERSISTENCE_DATA_PATH: \"./data\"\n      # highlight-start\n      ENABLE_MODULES: text2vec-transformers\n      DEFAULT_VECTORIZER_MODULE: text2vec-transformers\n      TRANSFORMERS_INFERENCE_API: http://t2v-transformers:8080\n      # highlight-end\n      CLUSTER_HOSTNAME: 'node1'\n# highlight-start\n  t2v-transformers:  # Set the name of the inference container\n    image: cr.weaviate.io/semitechnologies/transformers-inference:sentence-transformers-multi-qa-MiniLM-L6-cos-v1\n    environment:\n      ENABLE_CUDA: 0 # set to 1 to enable\n  # Set additional inference containers here if desired\n# highlight-end\n...\n```\n\nMake sure to enable CUDA if you have a compatible GPU available (`ENABLE_CUDA=1`) to take advantage of GPU acceleration.\n\n\n### Alternative: Run a separate container\n\nAs an alternative, you can run the inference container independently from Weaviate. To do so, you can:\n\n- Enable `text2vec-transformers` in your Docker Compose file,\n- Omit `t2v-transformers` parameters,\n- Run the inference container separately, e.g. using Docker, and\n- Use `TRANSFORMERS_INFERENCE_API` or [`inferenceUrl`](#collection-level) to set the URL of the inference container.\n\nFor example, choose [any of our pre-built transformers models](#option-1-pre-built-images) and spin it up - for example using:\n\n```shell\ndocker run -itp \"8000:8080\" semitechnologies/transformers-inference:sentence-transformers-multi-qa-MiniLM-L6-cos-v1\n```\n\nThen, for example if Weaviate is running outside of Docker, set `TRANSFORMERS_INFERENCE_API=\"http://localhost:8000\"`. Alternatively if Weaviate is part of the same Docker network, e.g. because they are part of the same `docker-compose.yml` file, you can use Docker networking/DNS, such as `TRANSFORMERS_INFERENCE_API=http://t2v-transformers:8080`.\n\n## Collection configuration\n\nYou can configure how the module will behave in each collection through the [Weaviate schema](/developers/weaviate/manage-data/collections.mdx).\n\n### Vectorization settings\n\nYou can set vectorizer behavior using the `moduleConfig` section under each collection and property:\n\n#### Collection-level\n\n- `vectorizer` - what module to use to vectorize the data.\n- `vectorizeClassName` \u2013 whether to vectorize the collection name. Default: `true`.\n- `poolingStrategy` \u2013 the pooling strategy to use. Default: `masked_mean`. Allowed values: `masked_mean` or `cls`. ([Read more on this topic.](https://arxiv.org/abs/1908.10084))\n- `inferenceUrl` \u2013 the URL of the inference container, for when using [multiple inference containers](#weaviate-instance-configuration) (e.g. `http://service-name:8080`). Default: `http://t2v-transformers:8080`.\n\n- `queryInferenceUrl` & `passageInferenceUrl` \u2013 the URL of the inference container for query and passage respectively, for when using [multiple inference containers](#weaviate-instance-configuration) with a [`DPR` type model](https://huggingface.co/docs/transformers/en/model_doc/dpr) (e.g. `http://service-name:8080`).\n\nYou can only set one of `inferenceUrl` or (`queryInferenceUrl` and `passageInferenceUrl`). If you are running a DPR model, set `queryInferenceUrl` and `passageInferenceUrl` to use different inference containers for queries and passages when using inference containers with a DPR type model.\n\n#### Property-level\n\n- `skip` \u2013 whether to skip vectorizing the property altogether. Default: `false`\n- `vectorizePropertyName` \u2013 whether to vectorize the property name. Default: `false`\n\n#### Example\n\n```json\n{\n  \"classes\": [\n    {\n      \"class\": \"Document\",\n      \"description\": \"A collection called document\",\n      // highlight-start\n      \"vectorizer\": \"text2vec-transformers\",\n      \"moduleConfig\": {\n        \"text2vec-transformers\": {\n          \"vectorizeClassName\": false,\n          \"inferenceUrl\": \"http://t2v-transformers:8080\",  // Optional. Set to use a different inference container when using multiple inference containers.\n          // Note: You can only set one of `inferenceUrl` or (`queryInferenceUrl` and `passageInferenceUrl`).\n          // Set 'inferenceUrl' to use a different inference container when using multiple inference containers with most (i.e. non-DPR type) models.\n          // Set 'queryInferenceUrl' and 'passageInferenceUrl' to use different inference containers for queries and passages when using multiple inference containers with a DPR type model.\n          // \"queryInferenceUrl\": \"http://t2v-transformers-query:8080\",  // Optional. Set to use a different inference container for queries when using multiple inference containers with a DPR type model.\n          // \"passageInferenceUrl\": \"http://t2v-transformers-passage:8080\"  // Optional. Set to use a different inference container for passages when using multiple inference containers with a DPR type model.\n        }\n      },\n      // highlight-end\n      \"properties\": [\n        {\n          \"name\": \"content\",\n          \"dataType\": [\n            \"text\"\n          ],\n          \"description\": \"Content that will be vectorized\",\n          // highlight-start\n          \"moduleConfig\": {\n            \"text2vec-transformers\": {\n              \"skip\": false,\n              \"vectorizePropertyName\": false\n            }\n          }\n          // highlight-end\n        }\n      ],\n    }\n  ]\n}\n```\n\n## Select a model\n\nTo select a model, please point `text2vec-transformers` to the appropriate Docker container.\n\nYou can use one of our pre-built Docker images, or build your own (with just a few lines of code).\n\nThis allows you to use any suitable model from the [Hugging Face model hub](https://huggingface.co/models) or your own custom model.\n\n### Use a pre-built image\n\nWe have built images from publicly available models that in our opinion are well suited for semantic search. You can use any of the following:\n\n\n  List of pre-built images\n\n|Model Name|Image Name|\n|---|---|\n|`distilbert-base-uncased` ([Info](https://huggingface.co/distilbert-base-uncased))|`semitechnologies/transformers-inference:distilbert-base-uncased`|\n|`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` ([Info](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2))|`semitechnologies/transformers-inference:sentence-transformers-paraphrase-multilingual-MiniLM-L12-v2`|\n|`sentence-transformers/multi-qa-MiniLM-L6-cos-v1` ([Info](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1))|`semitechnologies/transformers-inference:sentence-transformers-multi-qa-MiniLM-L6-cos-v1`|\n|`sentence-transformers/multi-qa-mpnet-base-cos-v1` ([Info](https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-cos-v1))|`semitechnologies/transformers-inference:sentence-transformers-multi-qa-mpnet-base-cos-v1`|\n|`sentence-transformers/all-mpnet-base-v2` ([Info](https://huggingface.co/sentence-transformers/all-mpnet-base-v2))|`semitechnologies/transformers-inference:sentence-transformers-all-mpnet-base-v2`|\n|`sentence-transformers/all-MiniLM-L12-v2` ([Info](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2))|`semitechnologies/transformers-inference:sentence-transformers-all-MiniLM-L12-v2`|\n|`sentence-transformers/paraphrase-multilingual-mpnet-base-v2` ([Info](https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2))|`semitechnologies/transformers-inference:sentence-transformers-paraphrase-multilingual-mpnet-base-v2`|\n|`sentence-transformers/all-MiniLM-L6-v2` ([Info](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2))|`semitechnologies/transformers-inference:sentence-transformers-all-MiniLM-L6-v2`|\n|`sentence-transformers/multi-qa-distilbert-cos-v1` ([Info](https://huggingface.co/sentence-transformers/multi-qa-distilbert-cos-v1))|`semitechnologies/transformers-inference:sentence-transformers-multi-qa-distilbert-cos-v1`|\n|`sentence-transformers/gtr-t5-base` ([Info](https://huggingface.co/sentence-transformers/gtr-t5-base))|`semitechnologies/transformers-inference:sentence-transformers-gtr-t5-base`|\n|`sentence-transformers/gtr-t5-large` ([Info](https://huggingface.co/sentence-transformers/gtr-t5-large))|`semitechnologies/transformers-inference:sentence-transformers-gtr-t5-large`|\n|`google/flan-t5-base` ([Info](https://huggingface.co/google/flan-t5-base))|`semitechnologies/transformers-inference:google-flan-t5-base`|\n|`google/flan-t5-large` ([Info](https://huggingface.co/google/flan-t5-large))|`semitechnologies/transformers-inference:google-flan-t5-large`|\n|`BAAI/bge-small-en-v1.5` ([Info](https://huggingface.co/BAAI/bge-small-en-v1.5))|`semitechnologies/transformers-inference:baai-bge-small-en-v1.5`|\n|`BAAI/bge-base-en-v1.5` ([Info](https://huggingface.co/BAAI/bge-base-en-v1.5))|`semitechnologies/transformers-inference:baai-bge-base-en-v1.5`|\n|DPR Models|\n|`facebook/dpr-ctx_encoder-single-nq-base` ([Info](https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base))|`semitechnologies/transformers-inference:facebook-dpr-ctx_encoder-single-nq-base`|\n|`facebook/dpr-question_encoder-single-nq-base` ([Info](https://huggingface.co/facebook/dpr-question_encoder-single-nq-base))|`semitechnologies/transformers-inference:facebook-dpr-question_encoder-single-nq-base`|\n|`vblagoje/dpr-ctx_encoder-single-lfqa-wiki` ([Info](https://huggingface.co/vblagoje/dpr-ctx_encoder-single-lfqa-wiki))|`semitechnologies/transformers-inference:vblagoje-dpr-ctx_encoder-single-lfqa-wiki`|\n|`vblagoje/dpr-question_encoder-single-lfqa-wiki` ([Info](https://huggingface.co/vblagoje/dpr-question_encoder-single-lfqa-wiki))|`semitechnologies/transformers-inference:vblagoje-dpr-question_encoder-single-lfqa-wiki`|\n|Bar-Ilan University NLP Lab Models|\n|`biu-nlp/abstract-sim-sentence` ([Info](https://huggingface.co/biu-nlp/abstract-sim-sentence))|`semitechnologies/transformers-inference:biu-nlp-abstract-sim-sentence`|\n|`biu-nlp/abstract-sim-query` ([Info](https://huggingface.co/biu-nlp/abstract-sim-query))|`semitechnologies/transformers-inference:biu-nlp-abstract-sim-query`|\n|Snowflake models|\n|`Snowflake/snowflake-arctic-embed-xs` ([Info](https://huggingface.co/Snowflake/snowflake-arctic-embed-xs))|`semitechnologies/transformers-inference:snowflake-snowflake-arctic-embed-xs`|\n|`Snowflake/snowflake-arctic-embed-s` ([Info](https://huggingface.co/Snowflake/snowflake-arctic-embed-s))|`semitechnologies/transformers-inference:snowflake-snowflake-arctic-embed-s`|\n|`Snowflake/snowflake-arctic-embed-m` ([Info](https://huggingface.co/Snowflake/snowflake-arctic-embed-m))|`semitechnologies/transformers-inference:snowflake-snowflake-arctic-embed-m`|\n|`Snowflake/snowflake-arctic-embed-l` ([Info](https://huggingface.co/Snowflake/snowflake-arctic-embed-l))|`semitechnologies/transformers-inference:snowflake-snowflake-arctic-embed-l`|\n\n\n\n#### ONNX-enabled images (CPU only)\n\nWe also provide ONNX-enabled images for some models. These images use ONNX Runtime for faster inference on CPUs. They are quantized for ARM64 and AMD64 (AVX2) hardware.\n\nLook for the `-onnx` suffix in the image name.\n\n\n  List of pre-built images\n\n|Model Name|Image Name|\n|---|---|\n|`sentence-transformers/all-MiniLM-L6-v2` ([Info](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2))|`semitechnologies/transformers-inference:sentence-transformers-all-MiniLM-L6-v2-onnx`|\n|`BAAI/bge-small-en-v1.5` ([Info](https://huggingface.co/BAAI/bge-small-en-v1.5))|`semitechnologies/transformers-inference:baai-bge-small-en-v1.5-onnx`|\n|`BAAI/bge-base-en-v1.5` ([Info](https://huggingface.co/BAAI/bge-base-en-v1.5))|`semitechnologies/transformers-inference:baai-bge-base-en-v1.5-onnx`|\n|`BAAI/bge-m3` ([Info](https://huggingface.co/BAAI/bge-m3))|`semitechnologies/transformers-inference:baai-bge-m3-onnx`|\n|`Snowflake/snowflake-arctic-embed-xs` ([Info](https://huggingface.co/Snowflake/snowflake-arctic-embed-xs))|`semitechnologies/transformers-inference:snowflake-snowflake-arctic-embed-xs-onnx`|\n|`Snowflake/snowflake-arctic-embed-s` ([Info](https://huggingface.co/Snowflake/snowflake-arctic-embed-s))|`semitechnologies/transformers-inference:snowflake-snowflake-arctic-embed-s-onnx`|\n|`Snowflake/snowflake-arctic-embed-m` ([Info](https://huggingface.co/Snowflake/snowflake-arctic-embed-m))|`semitechnologies/transformers-inference:snowflake-snowflake-arctic-embed-m-onnx`|\n|`Snowflake/snowflake-arctic-embed-l` ([Info](https://huggingface.co/Snowflake/snowflake-arctic-embed-l))|`semitechnologies/transformers-inference:snowflake-snowflake-arctic-embed-l-onnx`|\n\n\n\n#### Is your preferred model missing?\n\nIf your preferred model is missing, please [open an issue](https://github.com/weaviate/weaviate/issues) to ask us to include it. Alternatively, follow the [steps below to build](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers#build-a-model) a custom image.\n\n#### How to set the version\n\nYou can explicitly set the version through a suffix.\n- Use `-1.0.0` to pin to a specific version. E.g. `semitechnologies/transformers-inference:distilbert-base-uncased-1.0.0` will always use the version with git tag `1.0.0` of the `distilbert-base-uncased` repository.\n- You can explicitly set `-latest` to always use the latest version, however this is the default behavior.\n\n### Build a model\n\nTo use a **public** model from the [Hugging Face model hub](https://huggingface.co/models), create a short, two-line Dockerfile to build the image. This example creates a custom image for the [`distilroberta-base` model](https://huggingface.co/distilroberta-base).\n\n#### Step 1: Create a `Dockerfile`\n\nCreate a new `Dockerfile` called `distilroberta.Dockerfile`. Add the following lines to `distilroberta.Dockerfile`:\n\n```\nFROM semitechnologies/transformers-inference:custom\nRUN MODEL_NAME=distilroberta-base ./download.py\n```\n\n#### Step 2: Build and tag your Dockerfile.\n\nTag the Dockerfile as `distilroberta-inference`:\n\n```shell\ndocker build -f distilroberta.Dockerfile -t distilroberta-inference .\n```\n\n#### Step 3: Use the image\n\nPush the image to a Docker registry or reference it locally in your Weaviate `docker-compose.yml` using the Docker tag `distilroberta-inference`.\n\nNote: When using a custom image, you have the option of using the `USE_SENTENCE_TRANSFORMERS_VECTORIZER` environment variable to use the `sentence-transformer` vectorizer instead of the default vectorizer (from the `transformers` library).\n\n\n### Use a private or local model\n\nYou can build a Docker image which supports any model which is compatible with Hugging Face's `AutoModel` and `AutoTokenizer`.\n\nIn the following example, we are going to build a custom image for a non-public model which we have locally stored at `./my-model`.\n\nCreate a new `Dockerfile` (you do not need to clone this repository, any folder on your machine is fine), we will name it `my-model.Dockerfile`. Add the following lines to it:\n\n```\nFROM semitechnologies/transformers-inference:custom\nCOPY ./my-model /app/models/model\n```\n\nThe above will make sure that your model end ups in the image at `/app/models/model`. This path is important, so that the application can find the model.\n\nNow you just need to build and tag your Dockerfile, we will tag it as `my-model-inference`:\n\n```shell\ndocker build -f my-model.Dockerfile -t my-model-inference .\n```\n\nThat's it! You can now push your image to your favorite registry or reference it locally in your Weaviate `docker-compose.yml` using the Docker tag `my-model-inference`.\n\nTo debug and test if your inference container is working correctly, you can send queries to the vectorizer module's inference container directly, so you can see exactly what vectors it would produce for which input.\n\nTo do so \u2013 you need to expose the inference container in your Docker Compose file \u2013 add something like this:\n\n```yaml\nports:\n  - \"9090:8080\"\n```\n\nto your `text2vec-transformers`.\n\nThen you can send REST requests to it directly, e.g.:\n\n```shell\ncurl localhost:9090/vectors -H 'Content-Type: application/json' -d '{\"text\": \"foo bar\"}'\n```\nand it will print the created vector directly.\n\n## Usage\n\n### Example\n\n\n\n### Chunking\n\nThe `text2vec-transformers` module can automatically chunk text based on the model's maximum token length before it is passed to the model. It will then return the pooled vectors.\n\nSee [HuggingFaceVectorizer.vectorizer()](https://github.com/weaviate/t2v-transformers-models/blob/main/vectorizer.py) for the exact implementation.\n\n\n## Model licenses\n\nThe `text2vec-transformers` module is compatible with various models. Each of the models has its own license. For detailed information, please review the license for the model you are using in the [Hugging Face Model Hub](https://huggingface.co/models).\n\nIt is your responsibility to evaluate whether the terms of its license(s), if any, are appropriate for your intended use.\n\n## Release notes\n\nFor details see, [t2v-transformers-model release notes](https://github.com/weaviate/t2v-transformers-models/releases/).\n\n\n", "type": "Documentation", "name": "Retriever-vectorizer-modules Text2vec-transformers", "path": "developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers.md", "link": "https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers", "timestamp": "2024-05-08 10:50:37", "reader": "JSON", "meta": {}, "chunks": []}