{"text": "\nWeaviate offers a variety of tokenization options to choose from. These options allow you to configure how keyword searches and filters are performed in Weaviate for each property.\n\nThe main options are:\n\n- `word`: alphanumeric, lowercased tokens\n- `lowercase`: lowercased tokens\n- `whitespace`: whitespace-separated, case-sensitive tokens\n- `field`: the entire value of the property is treated as a single token\n\nLet's explore each of these options in more detail, including how they work and when you might want to use them.\n\n##  Tokenization methods\n\n###  `word`\n\nThe `word` tokenization method splits the text by any non-alphanumeric characters, and then lowercases each token.\n\nHere are some examples of how the `word` tokenization method works:\n\n| Text | Tokens |\n| ---- | ------ |\n| `\"Why, hello there!\"` | `[\"why\", \"hello\", \"there\"]` |\n| `\"Lois & Clark: The New Adventures of Superman\"` | `[\"lois\", \"clark\", \"the\", \"new\", \"adventures\", \"of\", \"superman\"]` |\n| `\"variable_name\"` | `[\"variable\", \"name\"]` |\n| `\"Email: john.doe@example.com\"` | `[\"email\", \"john\", \"doe\", \"example\", \"com\"]` |\n\n#### When to use `word` tokenization\n\nThe `word` tokenization is the default tokenization method in Weaviate.\n\nGenerally, if you are searching or filtering \"typical\" text data, `word` tokenization is a good starting point.\n\nBut if symbols (such as `&`, `@` or `_`) are important to your data and search, or distinguishing between different cases is important, you may want to consider using a different tokenization method such as `lowercase` or `whitespace`.\n\n###  `lowercase`\n\nThe `lowercase` tokenization method splits the text by whitespace, and then lowercases each token.\n\nHere are some examples of how the `lowercase` tokenization method works:\n\n| Text | Tokens |\n| ---- | ------ |\n| `\"Why, hello there!\"` | `[\"why,\", \"hello\", \"there!\"]` |\n| `\"Lois & Clark: The New Adventures of Superman\"` | `[\"lois\", \"&\", \"clark:\", \"the\", \"new\", \"adventures\", \"of\", \"superman\"]` |\n| `\"variable_name\"` | `[\"variable_name\"]` |\n| `\"Email: john.doe@example.com\"` | `[\"email:\", \"john.doe@example.com\"]` |\n\n#### When to use `lowercase` tokenization\n\nThe `lowercase` tokenization can be thought of as `word`, but including symbols. A key use case for `lowercase` is when symbols such as `&`, `@` or `_` are significant for your data.\n\nThis might include cases where your database contains code snippets, email addresses, or any other symbolic notations with meaning.\n\nAs an example, consider filtering for objects containing `\"database_address\"`:\n\n| Text | Tokenization | Matched by `\"database_address\"` |\n| ---- | ------------ | ------- |\n| `\"database_address\"` | `word` | \u2705 |\n| `\"database_address\"` | `lowercase` | \u2705 |\n| `\"database_company_address\"` | `word` | \u2705 |\n| `\"database_company_address\"` | `lowercase` | \u274c |\n\nNote how the filtering behavior changes. A careful choice of tokenization method can ensure that the search results meet your and the users' expectations.\n\n###  `whitespace`\n\nThe `whitespace` tokenization method splits the text by whitespace.\n\nHere are some examples of how the `whitespace` tokenization method works:\n\n| Text | Tokens |\n| ---- | ------ |\n| `\"Why, hello there!\"` | `[\"Why,\", \"hello\", \"there!\"]` |\n| `\"Lois & Clark: The New Adventures of Superman\"` | `[\"Lois\", \"&\", \"Clark:\", \"The\", \"New\", \"Adventures\", \"of\", \"Superman\"]` |\n| `\"variable_name\"` | `[\"variable_name\"]` |\n| `\"Email: john.doe@example.com\"` | `[\"Email:\", \"john.doe@example.com\"]` |\n\n#### When to use `whitespace` tokenization\n\nThe `whitespace` tokenization method adds case-sensitivity to `lowercase`. This is useful when your data distinguishes between cases, such as for names of entities or acronyms.\n\nA risk of using `whitespace` tokenization is that it can be too strict. For example, a search for `\"superman\"` will not match `\"Superman\"`, as the tokens are case-sensitive.\n\nBut this could be managed on a case-by-case basis. It would be possible to construct queries that are case-insensitive, such as by having the query create two versions of the search term: one in lowercase and one in uppercase.\n\nOn the other hand, it will not be possible to construct case-sensitive queries using `word` or `lowercase` tokenization.\n\n###  `field`\n\nThe `field` tokenization method simply treats the entire value of the property as a single token.\n\nHere are some examples of how the `field` tokenization method works:\n\n| Text | Tokens |\n| ---- | ------ |\n| `\"Why, hello there!\"` | `[\"Why, hello there!\"]` |\n| `\"Lois & Clark: The New Adventures of Superman\"` | `[\"Lois & Clark: The New Adventures of Superman\"]` |\n| `\"variable_name\"` | `[\"variable_name\"]` |\n| `\"Email: john.doe@example.com\"` | `[\"Email: john.doe@example.com\"]` |\n\n#### When to use `field` tokenization\n\nThe `field` tokenization is useful when exact matches of strings in the exact order are important. Typically, this is useful for properties that contain unique identifiers, such as email addresses, URLs, or other unique strings.\n\nGenerally, `field` tokenization should be used judiciously due to its strictness.\n\nFor keyword searches, `field` tokenization has limited use. A keyword search for `\"computer mouse\"` will not match `\"mouse for a computer\"`, nor will it match `\"computer mouse pad\"` or even `\"a computer mouse\"`.\n\n##  Stop words\n\nWeaviate supports [stop words](https://en.wikipedia.org/wiki/Stop_word). Stop words are common words which are often filtered out from search queries because they occur frequently and do not carry much meaning.\n\nBy default, Weaviate uses a [list of English stop words](https://github.com/weaviate/weaviate/blob/main/adapters/repos/db/inverted/stopwords/presets.go). You can [configure your own list of stop words](../../../weaviate/config-refs/schema/index.md#invertedindexconfig--stopwords-stopword-lists) in the schema definition.\n\nThis means that after tokenization, any stop words in the text behave as if they were not present. For example, a filter for `\"a computer mouse\"` will behave identically to a filter for `\"computer mouse\"`.\n\n##  Language-specific tokenization\n\nThe above tokenization methods work well for English, or other languages that use spaces to separate words.\n\nHowever, not all languages rely on spaces to define natural semantic boundaries. For languages like Japanese or Chinese, where words are not separated by spaces, you may need to use a different tokenization method.\n\nStarting with `v1.24` Weaviate provides `gse` and `trigram` tokenization methods for this reason.\n\n`gse` implements the \"Jieba\" algorithm, which is a popular Chinese text segmentation algorithm. `trigram` splits text into all possible trigrams, which can be useful for languages like Japanese.\n\n\n\n", "type": "Documentation", "name": "Tokenization Options", "path": "developers/academy/py/tokenization/200_options.mdx", "link": "https://weaviate.io/developers/academy/py/tokenization/options", "timestamp": "2024-05-08 10:48:21", "reader": "JSON", "meta": {}, "chunks": []}