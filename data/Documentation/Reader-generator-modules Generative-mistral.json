{"text": "\n\n## In short\n\n\n* The Generative Mistral (`generative-mistral`) module performs retrieval augmented generation, or RAG, based on the data stored in your Weaviate instance.\n* The module can generate a response for each object returned from Weaviate, or a combined response for a group of objects.\n* The module enables generative search operations on the Weaviate instance.\n* The default model is `open-mistral-7b`.\n* The module requires an [API key for Mistral inference endpoints](https://docs.mistral.ai/) to perform the generation task.\n\n## Introduction\n\n`generative-mistral` performs retrieval augmented generation, or RAG, based on the data stored in your Weaviate instance.\n\nThe module works in two steps:\n1. (Weaviate) Run a search query in Weaviate to find relevant objects.\n2. (Mistral Inference API) Use a Large Language Model to generate a response based on the results (from the previous step) and the provided prompt or task.\n\nYou can use the Generative Mistral module with any other upstream modules. For example, you could use `text2vec-cohere`, `text2vec-huggingface` or `text2vec-openai` to vectorize and query your data, but then rely on the `generative-mistral` module to generate a response.\n\nThe generative module can perform RAG for:\n* each returned object - `singlePrompt`\n* the group of all results together \u2013 `groupedTask`\n\nYou need to input both a query and a prompt (for individual responses) or a task (for all responses).\n\n\n## Weaviate instance configuration\n\nIf you use Weaviate Cloud Services (WCS), this module is already enabled and pre-configured. You cannot edit the configuration in WCS.\n\n### Docker Compose file\n\nTo use `generative-mistral`, you must enable it in your Docker Compose file (`docker-compose.yml`). You can do so manually, or create one using the [Weaviate configuration tool](/developers/weaviate/installation/docker-compose.md#configurator).\n\n#### Parameters\n\n- `ENABLE_MODULES` (Required): The modules to enable. Include `generative-mistral` to enable the module.\n- `MISTRAL_APIKEY` Your Mistral API key. You can also provide the key at query time.\n\n#### Example\n\nThis configuration enables `generative-mistral` and sets the Mistral authentication credentials.\n\n```yaml\n---\nversion: '3.4'\nservices:\n  weaviate:\n    command:\n    - --host\n    - 0.0.0.0\n    - --port\n    - '8080'\n    - --scheme\n    - http\n    image: cr.weaviate.io/semitechnologies/weaviate:||site.weaviate_version||\n    ports:\n    - 8080:8080\n    - 50051:50051\n    volumes:\n    - weaviate_data:/var/lib/weaviate\n    restart: on-failure:0\n    environment:\n      QUERY_DEFAULTS_LIMIT: 25\n      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\n      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'\n      # highlight-start\n      MISTRAL_APIKEY: escecret-foobar  # Optional. Can be set at query time.\n      ENABLE_MODULES: 'text2vec-cohere,generative-mistral'  # Can include any modules\n      # highlight-end\n      CLUSTER_HOSTNAME: 'node1'\nvolumes:\n  weaviate_data:\n...\n```\n\n\n## Class configuration\n\nYou can configure how the module will behave in each class through the [Weaviate schema](/developers/weaviate/manage-data/collections.mdx).\n\n### API settings\n\n#### Parameters\n\n| Parameter | Required | Default | Purpose |\n| :- | :- | :- | :- |\n| `model` | No | `\"open-mistral-7b\"` | The model to use.\n| `temperature` | No | `0` | Control of LLM stochasticity. |\n| `maxTokens` | No | `2048` | Maximum number of tokens to generate. |\n\n### Supported models\n\nYou can use any of the following models with `generative-mistral`:\n\n* `open-mistral-7b` (aka `mistral-tiny-2312`)\n* `open-mixtral-8x7b` (aka `mistral-small-2312`)\n* `mistral-tiny`\n* `mistral-small`\n* `mistral-small-latest` (aka `mistral-small-2402`)\n* `mistral-medium`\n* `mistral-medium-latest` (aka `mistral-medium-2312`)\n* `mistral-large`\n* `mistral-large-latest` (aka `mistral-large-2402`)\n\n#### Example\n\nThe following example configures the `Document` class to use the `generative-mistral` module with the `Document` class, with the `mistral-medium-latest` model.\n\n```json\n{\n  \"classes\": [\n    {\n      \"class\": \"Document\",\n      \"description\": \"A class called document\",\n      ...,\n      \"moduleConfig\": {\n        // highlight-start\n        \"generative-mistral\": {\n          \"model\": \"mistral-medium-latest\",\n        },\n        // highlight-end\n      }\n    }\n  ]\n}\n```\n\n\n  New to Weaviate Schemas?\n\nIf you are new to Weaviate, check out the [Weaviate schema tutorial](/developers/weaviate/starter-guides/schema.md).\n\n\n\n\n## How to use\n\n### Query-time parameters\n\nYou can supply parameters at query time by adding them to the HTTP header.\n\n| HTTP Header | Value | Purpose | Note |\n| :- | :- | :- | :- |\n| `\"X-Mistral-Api-Key\"` | Your Mistral API key. | Authentication | [Learn more](https://docs.mistral.ai/platform/overview/)|\n\n### Queries\n\nThis module enables generative search queries.\n\n`generate` takes the following arguments:\n\n| Field | Data Type | Required | Example | Description |\n|- |- |- |- |- |\n| `singleResult {prompt}`  | string | no | `Summarize the following in a tweet: {summary}`  | Generates a response for each individual search result. You need to include at least one result field in the prompt, between braces. |\n| `groupedResult {task}`  | string | no | `Explain why these results are similar to each other`  | Generates a single response for all search results |\n\n#### Example of properties in the prompt\n\nWhen piping the results to the prompt, at least one field returned by the query must be added to the prompt. If you don't add any fields, Weaviate will throw an error.\n\nFor example, assume your schema looks like this:\n\n```graphql\n{\n  Article {\n    title\n    summary\n  }\n}\n```\n\nYou can add both `title` and `summary` to the prompt by enclosing them in curly brackets:\n\n```graphql\n{\n  Get {\n    Article {\n      title\n      summary\n      _additional {\n        generate(\n          singleResult: {\n            prompt: \"\"\"\n            Summarize the following in a tweet:\n\n            {title} - {summary}\n            \"\"\"\n          }\n        ) {\n          singleResult\n          error\n        }\n      }\n    }\n  }\n}\n```\n\n#### Example - single result\n\nHere is an example of a query where:\n* we get a podcast clip (with limit 1)\n* then we ask the generator module to summarize the content into one sentence.\n  * the query asks for the `speaker` and `content` fields, which are then included in the `prompt` argument of the `generate` operator.\n\n\n\n#### Example - grouped result\n\nHere is an example of a query where:\n* we run a vector search (with `nearText`) to find podcast clips semantically similar to `\"What is ref2vec?\"`\n* then we ask the generator module to answer the question: `\"What is ref2vec?\"` based on the search results.\n\n\n\n#### Further examples\n\nFor further usage examples, please see the [how-to search: generative](../../search/generative.md) page.\n\n\n\n\n", "type": "Documentation", "name": "Reader-generator-modules Generative-mistral", "path": "developers/weaviate/modules/reader-generator-modules/generative-mistral.md", "link": "https://weaviate.io/developers/weaviate/modules/reader-generator-modules/generative-mistral", "timestamp": "2024-05-08 10:50:21", "reader": "JSON", "meta": {}, "chunks": []}