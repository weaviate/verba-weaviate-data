{"text": "\n\n## In short\n\n* The OpenAI Question and Answer (Q&A) module is a Weaviate module for answer extraction from data through the OpenAI completions endpoint or the Azure OpenAI equivalent.\n* The module depends on a text vectorization module that should be running with Weaviate.\n* The module adds an `ask {}` operator to the GraphQL `Get {}` queries\n* The module returns a max. of 1 answer in the GraphQL `_additional {}` field.\n* The answer with the highest `certainty` (confidence level) will be returned.\n* Added in Weaviate `v1.16.6`\n\n\n\n\n## Introduction\n\nThe Question and Answer (Q&A) OpenAI module is a Weaviate module for answer extraction from data. It uses an OpenAI completions endpoint to try and extract an answer from the most relevant docs.\n\nThis module can be used in GraphQL `Get{...}` queries, as a search operator. The `qna-openai` module tries to find an answer in the data objects of the specified class. If an answer is found within the given `certainty` range, it will be returned in the GraphQL `_additional { answer { ... } }` field. There will be a maximum of 1 answer returned, if this is above the optionally set `certainty`. The answer with the highest `certainty` (confidence level) will be returned.\n\n## Inference API key\n\n`qna-openai` requires an API key from OpenAI or Azure OpenAI.\n\nYou only need to provide one of the two keys, depending on which service (OpenAI or Azure OpenAI) you are using.\n\n## Organization name\n\n\nFor requests that require the OpenAI organization name, you can provide it at query time by adding it to the HTTP header:\n- `\"X-OpenAI-Organization\": \"YOUR-OPENAI-ORGANIZATION\"` for OpenAI\n\n### Providing the key to Weaviate\n\nYou can provide your API key in two ways:\n\n1. During the **configuration** of your Docker instance, by adding `OPENAI_APIKEY` or `AZURE_APIKEY` as appropriate under `environment` to your `Docker Compose` file, like this:\n\n  ```yaml\n  environment:\n    OPENAI_APIKEY: 'your-key-goes-here'  # For use with OpenAI. Setting this parameter is optional; you can also provide the key at runtime.\n    AZURE_APIKEY: 'your-key-goes-here'  # For use with Azure OpenAI. Setting this parameter is optional; you can also provide the key at runtime.\n    ...\n  ```\n\n2. At **run-time** (recommended), by providing `\"X-OpenAI-Api-Key\"` or `\"X-Azure-Api-Key\"` through the request header. You can provide it using the Weaviate client, like this:\n\n\n\n\n## Module configuration\n\nThis module is enabled and pre-configured on Weaviate Cloud Services.\n\n### Docker Compose file (Weaviate open source only)\n\nYou can enable the OpenAI Q&A module in your Docker Compose file (e.g. `docker-compose.yml`). Add the `qna-openai` module (alongside any other module you may need) to the `ENABLE_MODULES` property, like this:\n\n```\nENABLE_MODULES: 'text2vec-openai,qna-openai'\n```\n\nHere is a full example of a Docker configuration, which uses the `qna-openai` module in combination with `text2vec-openai`:\n\n```yaml\n---\nversion: '3.4'\nservices:\n  weaviate:\n    command:\n      - --host\n      - 0.0.0.0\n      - --port\n      - '8080'\n      - --scheme\n      - http\n    image:\n      semitechnologies/weaviate:||site.weaviate_version||\n    ports:\n      - 8080:8080\n    restart: on-failure:0\n    environment:\n      QUERY_DEFAULTS_LIMIT: 25\n      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\n      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'\n      DEFAULT_VECTORIZER_MODULE: 'text2vec-openai'\n      ENABLE_MODULES: 'text2vec-openai,qna-openai'\n      OPENAI_APIKEY: sk-foobar  # For use with OpenAI. Setting this parameter is optional; you can also provide the key at runtime.\n      OPENAI_ORGANIZATION: your-orgname  # For use with OpenAI. Setting this parameter is optional; you can also provide the key at runtime.\n      AZURE_APIKEY: sk-foobar  # For use with Azure OpenAI. Setting this parameter is optional; you can also provide the key at runtime.\n      CLUSTER_HOSTNAME: 'node1'\n```\n\n## Schema configuration\n\nYou can define settings for this module in the schema.\n\n### OpenAI vs Azure OpenAI\n\n- **OpenAI** users can optionally set the `model` parameter.\n- **Azure OpenAI** users must set the parameters `resourceName` and `deploymentId`.\n\n### Model parameters\n\nYou can also configure additional parameters for the model through the parameters shown below.\n\n### Example schema\n\nFor example, the following schema configuration will set Weaviate to use the `qna-openai` model with the `Document` class.\n\nThe following schema configuration uses the `text-davinci-002` model.\n\n```json\n{\n  \"classes\": [\n    {\n      \"class\": \"Document\",\n      \"description\": \"A class called document\",\n      \"vectorizer\": \"text2vec-openai\",\n      \"moduleConfig\": {\n        \"qna-openai\": {\n          \"model\": \"text-davinci-002\", // For OpenAI\n          \"resourceName\": \"\",  // For Azure OpenAI\n          \"deploymentId\": \"\",  // For Azure OpenAI\n          \"maxTokens\": 16, // Applicable to both OpenAI and Azure OpenAI\n          \"temperature\": 0.0,  // Applicable to both OpenAI and Azure OpenAI\n          \"topP\": 1,  // Applicable to both OpenAI and Azure OpenAI\n          \"frequencyPenalty\": 0.0,  // Applicable to both OpenAI and Azure OpenAI\n          \"presencePenalty\": 0.0  // Applicable to both OpenAI and Azure OpenAI\n        }\n      },\n      \"properties\": [\n        {\n          \"dataType\": [\n            \"text\"\n          ],\n          \"description\": \"Content that will be vectorized\",\n          \"name\": \"content\"\n        }\n      ]\n    }\n  ]\n}\n```\n\nFor information on how to use the individual parameters you can check here\n\n## How to use\n\nThis module adds a search operator to GraphQL `Get{...}` queries: `ask{}`. This operator takes the following arguments:\n\n| Field | Data Type | Required | Example value | Description |\n|- |- |- |- |- |\n| `question`  | string | yes | `\"What is the name of the Dutch king?\"` | The question to be answered. |\n| `properties`  | list of strings | no | `[\"summary\"]` | The properties of the queries Class which contains text. If no properties are set, all are considered. |\n\nNotes:\n\n* The GraphQL `Explore { }` function does support the `ask` searcher, but the result is only a beacon to the object containing the answer. It is thus not any different from performing a nearText semantic search with the question. No extraction is happening.\n* You cannot use the `'ask'` operator along with a `'neaXXX'` operator!\n\n### Example query\n\n\n\n\n### GraphQL response\n\nThe answer is contained in a new GraphQL `_additional` property called `answer`. It contains the following fields:\n\n* `hasAnswer` (`boolean`): could an answer be found?\n* `result` (nullable `string`): An answer if one could be found. `null` if `hasAnswer==false`\n* `property` (nullable `string`): The property which contains the answer. `null` if `hasAnswer==false`\n* `startPosition` (`int`): The character offset where the answer starts. `0` if `hasAnswer==false`\n* `endPosition` (`int`): The character offset where the answer ends `0` if `hasAnswer==false`\n\nNote: `startPosition`, `endPosition` and `property` in the response are not guaranteed to be present. They are calculated by a case-insensitive string matching function against the input text. If the transformer model formats the output differently (e.g. by introducing spaces between tokens which were not present in the original input), the calculation of the position and determining the property fails.\n\n### Example response\n\n```json\n{\n  \"data\": {\n    \"Get\": {\n      \"Document\": [\n        {\n          \"_additional\": {\n            \"answer\": {\n              \"hasAnswer\": true,\n              \"result\": \" Stanley Kubrick is an American filmmaker who is best known for his films, including \\\"A Clockwork Orange,\\\" \\\"Eyes Wide Shut,\\\" and \\\"The Shining.\\\"\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n## How it works (under the hood)\n\nUnder the hood, the model uses a two-step approach. First it performs a semantic search to find the documents (e.g. a Sentence, Paragraph, Article, etc.) most likely to contain the answer. In a second step, Weaviate creates the required prompt as an input to an external call made to the OpenAI Completions endpoint. Weaviate uses the most relevant documents to establish a prompt for which OpenAI extracts the answer. There are three possible outcomes:\n\n1. No answer was found because the question can not be answered,\n2. An answer was found, but did not meet the user-specified minimum certainty, so it was discarded (typically the case when the document is on topic, but does not contain an actual answer to the question), and\n3. An answer was found that matches the desired certainty. It is returned to the user.\n\nThe module performs a semantic search under the hood, so a `text2vec-...` module is required. It does not need to be of the same type as the `qna-...` module. For example, you can use a `text2vec-contextionary` module to perform the semantic search, and a `qna-openai` module to extract the answer.\n\n## Additional information\n\n### Available models\n\nOpenAI has multiple models available for the extraction of answers from a given context.\n\n* For document embeddings you can choose one of the following models:\n  * ada\n  * babbage\n  * curie\n  * davinci\n\nThese models can be configured\n\n\n\n\n\n", "type": "Documentation", "name": "reader-generator-modules-qna-openai", "path": "developers/weaviate/modules/reader-generator-modules/qna-openai.md", "link": "https://weaviate.io/developers/weaviate/modules/reader-generator-modules/qna-openai", "timestamp": "2023-11-13 10:41:02", "reader": "JSON", "meta": {}, "chunks": []}