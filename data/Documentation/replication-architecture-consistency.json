{"text": "\n\n- API References | GraphQL | Get | Consistency Levels\n- API References | REST | Objects\n\nData consistency is a property of a database that refers to whether data in different nodes do or do not match. In Weaviate, availability is generally preferred over strong consistency. This doesn't mean that we don't pay attention to consistency at all. Schema and data consistency are as important as possible. As captured by the CAP Theorem, consistency and availability are a trade-off. In Weaviate, data consistency is tunable, so it's up to you how you make the trade-off between A and C.\n\nSchema consistency is not tunable, but set to a strong consistency protocol.\n\nThe strength of consistency can be determined by applying the following conditions:\n* If r + w > n, then the system is strongly consistent.\n    * r is the consistency level of read operations\n    * w is the consistency level of write operations\n    * n is the replication factor (number of replicas)\n* If r + w \n\n### Tunable write consistency\n\nAdding or changing data objects are **write** operations.\n\nWrite operations are tunable starting with Weaviate v1.18, to `ONE`, `QUORUM` (default) or `ALL`. In v1.17, write operations are always set to `ALL` (highest consistency).\n\nThe main reason for introducing configurable write consistency in v1.18 is because that is also when automatic repairs are introduced. A write will always be written to n (replication factor) nodes, regardless of the chosen consistency level. The coordinator node however waits for acknowledgements from `ONE`, `QUORUM` or `ALL` nodes before it returns. To guarantee that a write is applied everywhere without the availability of repairs on read requests, write consistency is set to `ALL` for now. Possible settings in v1.18+ are:\n* **ONE** - a write must receive an acknowledgement from at least one replica node. This is the fastest (most available), but least consistent option.\n* **QUORUM** - a write must receive an acknowledgement from at least `QUORUM` replica nodes. `QUORUM` is calculated as _n / 2 + 1_, where _n_ is the number of replicas (replication factor). For example, using a replication factor of 6, the quorum is 4, which means the cluster can tolerate 2 replicas down.\n* **ALL** - a write must receive an acknowledgement from all replica nodes. This is the most consistent, but 'slowest' (least available) option.\n\n\n*Figure below: a replicated Weaviate setup with write consistency of ONE. There are 8 nodes in total out of which 3 replicas.*\n\n\n\n*Figure below: a replicated Weaviate setup with Write Consistency of `QUORUM` (n/2+1). There are 8 nodes in total, out of which 3 replicas.*\n\n\n\n\n*Figure below: a replicated Weaviate setup with Write Consistency of `ALL`. There are 8 nodes in total, out of which 3 replicas.*\n\n\n\n\n### Tunable read consistency\n\nRead operations are GET requests to data objects in Weaviate. Like write, read consistency is tunable, to `ONE`, `QUORUM` (default) or `ALL`.\n\nPrior to `v1.18`, read consistency was tunable only for requests that obtained an object by id, and all other read requests had a consistency of `ALL`.\n\nThe following consistency levels are applicable to most read operations:\n\n- Starting with `v1.18`, consistency levels are applicable to REST endpoint operations.\n- Starting with `v1.19`, consistency levels are applicable to GraphQL `Get` requests.\n\n* **ONE** - a read response must be returned by at least one replica. This is the fastest (most available), but least consistent option.\n* **QUORUM** - a response must be returned by `QUORUM` amount of replica nodes. `QUORUM` is calculated as _n / 2 + 1_, where _n_ is the number of replicas (replication factor). For example, using a replication factor of 6, the quorum is 4, which means the cluster can tolerate 2 replicas down.\n* **ALL** - a read response must be returned by all replicas. The read operation will fail if at least one replica fails to respond. This is the most consistent, but 'slowest' (least available) option.\n\nExamples:\n* **ONE**\n  In a single datacenter with a replication factor of 3 and a read consistency level of ONE, the coordinator node will wait for a response from one replica node.\n\n  \n\n* **QUORUM**\n  In a single datacenter with a replication factor of 3 and a read consistency level of `QUORUM`, the coordinator node will wait for n / 2 + 1 = 3 / 2 + 1 = 2 replicas nodes to return a response.\n\n  \n\n\n* **ALL**\n  In a single datacenter with a replication factor of 3 and a read consistency level of `ALL`, the coordinator node will wait for all 3 replicas nodes to return a response.\n\n  \n\n\n### Tunable consistency strategies\n\nDepending on the desired tradeoff between consistency and speed, below are three common consistency level pairings for write / read operations. These are _minimum_ requirements that guarantee eventually consistent data:\n* `QUORUM` / `QUORUM` => balanced write and read latency\n* `ONE` / `ALL` => fast write and slow read (optimized for write)\n* `ALL` / `ONE` => slow write and fast read (optimized for read)\n\n## Repairs\n\nRepairs can be executed by Weaviate in case of a discovered inconsistency. A scenario where a repair could be necessary is the following: The user writes with a consistency level of `ONE`. The node dies before it can contact some of the other nodes. The node comes back up with the latest data. Some other nodes may now be out of sync and need to be repaired.\n\nRepairs happen in the background, for example when a read operation is done (\"repair-on-read\"), using a \"last write wins\" policy for conflict resolution.\n\nWhen the replication coordinator node receives different versions of objects for a read request from the nodes in the replica set, that means that at least one node has old (stale) objects. The repair-on-read feature means that the coordinator node will update the affected node(s) with the latest version of the object(s). If a node was lacking an object entirely (e.g. because a create request was only handled by a subset of the nodes due to a network partition), the object will be replicated on that node.\n\nConsider a scenario in which a request to delete objects was only handled by a subset of nodes in the replica set. On the next read that involves such a deleted object, the replication coordinator may determine that some nodes are missing that object - i.e. it doesn\u2019t exist on all replicas. `v1.18` introduces changes that enable the replication coordinator to determine the reason why an object was not found (i.e. it was deleted, or it never existed), along with the object itself. Thus, the coordinator can determine if the object:\n* never existed in the first place (so it should be propagated to the other nodes), or\n* was deleted from some replicas but still exists on others. In this latter case, the coordinator returns an error because it doesn\u2019t know if the object has been created again after it was deleted, which would lead to propagating the deletion to cause data loss.\n\nAn object that never existed will be propagated to the other nodes only if the object was queried with a _high enough_ consistency level, vs. the write consistency that was used to write the object:\n* if write was `QUORUM`, the read consistency level can be >= `QUORUM`\n* if the write was `ONE`, the object must be read with `ALL` to guarantee repair. This is because if only `ONE` node received the write request, then a `QUORUM` read request might only hit nodes that don't have the object, while an `ALL` request will reach that node as well.\n\n\n\n\n", "type": "Documentation", "name": "replication-architecture-consistency", "path": "developers/weaviate/concepts/replication-architecture/consistency.md", "link": "https://weaviate.io/developers/weaviate/concepts/replication-architecture/consistency", "timestamp": "2023-11-02 10:52:46", "reader": "JSON", "meta": {}, "chunks": []}