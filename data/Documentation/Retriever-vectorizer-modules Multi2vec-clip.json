{"text": "\n\n## Overview\n\nThe `multi2vec-clip` module enables Weaviate to obtain vectors locally from text or images using a CLIP model.\n\n`multi2vec-clip` encapsulates the model in a Docker container, which allows independent scaling on GPU-enabled hardware while keeping Weaviate on CPU-only hardware, as Weaviate is CPU-optimized.\n\nKey notes:\n\n- This module is not available on Weaviate Cloud Services (WCS).\n- Enabling this module will enable the [`nearText` and `nearImage` search operators](#additional-search-operators).\n- Model encapsulated in a Docker container.\n- This module is not compatible with Auto-schema. You must define your collections manually as [shown below](#collection-configuration).\n\n## Weaviate instance configuration\n\nThis module is not available on Weaviate Cloud Services.\n\n### Docker Compose file\n\nTo use `multi2vec-clip`, you must enable it in your Docker Compose file (e.g. `docker-compose.yml`).\n\nWhile you can do so manually, we recommend using the [Weaviate configuration tool](/developers/weaviate/installation/docker-compose.md#configurator) to generate the `Docker Compose` file.\n\n#### Parameters\n\nWeaviate:\n\n- `ENABLE_MODULES` (Required): The modules to enable. Include `multi2vec-clip` to enable the module.\n- `DEFAULT_VECTORIZER_MODULE` (Optional): The default vectorizer module. You can set this to `multi2vec-clip` to make it the default for all collections.\n- `CLIP_INFERENCE_API` (Required): The URL of the default inference container.\n\nInference container:\n\nAs of Weaviate `v1.24.2`, you can use multiple inference containers with `multi2vec-clip`. This allows you to use different models for different collections by [setting the `inferenceUrl` in the collection configuration](#collection-configuration).\n\n- `image` (Required): The image name of the inference container.\n- `ENABLE_CUDA` (Optional): Set to `1` to enable GPU usage. Default is `0` (CPU only).\n\n#### Example\n\nThis configuration enables `multi2vec-clip`, sets it as the default vectorizer, and sets the parameters for the Docker container, including setting it to use `multi2vec-clip:sentence-transformers-clip-ViT-B-32-multilingual-v1` image and to disable CUDA acceleration.\n\n```yaml\nversion: '3.4'\nservices:\n  weaviate:\n    image: cr.weaviate.io/semitechnologies/weaviate:||site.weaviate_version||\n    restart: on-failure:0\n    ports:\n     - 8080:8080\n     - 50051:50051\n    environment:\n      QUERY_DEFAULTS_LIMIT: 20\n      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\n      PERSISTENCE_DATA_PATH: \"./data\"\n      # highlight-start\n      ENABLE_MODULES: multi2vec-clip\n      DEFAULT_VECTORIZER_MODULE: multi2vec-clip\n      CLIP_INFERENCE_API: http://multi2vec-clip:8080\n      # highlight-end\n      CLUSTER_HOSTNAME: 'node1'\n# highlight-start\n  multi2vec-clip:  # Set the name of the inference container\n    image: cr.weaviate.io/semitechnologies/multi2vec-clip:sentence-transformers-clip-ViT-B-32-multilingual-v1\n    environment:\n      ENABLE_CUDA: 0 # set to 1 to enable\n  # Set additional inference containers here if desired\n# highlight-end\n...\n```\n\nThis module will benefit greatly from GPU usage. Make sure to enable CUDA if you have a compatible GPU available (`ENABLE_CUDA=1`).\n\n\n### Alternative: Run a separate container\n\nAs an alternative, you can run the inference container independently from Weaviate. To do so, you can:\n\n- Enable `multi2vec-clip` in your Docker Compose file,\n- Omit `multi2vec-clip` parameters,\n- Run the inference container separately, e.g. using Docker, and\n- Use `CLIP_INFERENCE_API` or [`inferenceUrl`](#collection-level) to set the URL of the inference container.\n\nThen, for example if Weaviate is running outside of Docker, set `CLIP_INFERENCE_API=\"http://localhost:8000\"`. Alternatively if Weaviate is part of the same Docker network, e.g. because they are part of the same `docker-compose.yml` file, you can use Docker networking/DNS, such as `CLIP_INFERENCE_API=http://multi2vec-clip:8080`.\n\n\n## Collection configuration\n\nYou can configure how the module will behave in each collection through the [Weaviate schema](/developers/weaviate/manage-data/collections.mdx).\n\n### Vectorization settings\n\nYou can set vectorizer behavior using the `moduleConfig` section under each collection and property:\n\n#### Collection-level\n\n- `vectorizer` - what module to use to vectorize the data.\n- `vectorizeClassName` \u2013 whether to vectorize the collection name. Default: `true`.\n- `Fields` - property names to map for different modalities (under `moduleConfig.multi2vec-clip`).\n    - i.e. one or more of [`textFields`, `imageFields`]\n- `weights` - optional parameter to weigh the different modalities in producing the final vector.\n- `inferenceUrl` \u2013 the URL of the inference container, for when using [multiple inference containers](#weaviate-instance-configuration) (e.g. `http://service-name:8080`). Default: `http://multi2vec-clip:8080`.\n\n#### Property-level\n\n- `skip` \u2013 whether to skip vectorizing the property altogether. Default: `false`\n- `vectorizePropertyName` \u2013 whether to vectorize the property name. Default: `false`\n- `dataType` - the data type of the property. For use in the appropriate `Fields`, must be set to `text` or `blob` as appropriate.\n\n#### Example\n\nThe following example collection definition sets the `multi2vec-clip` module as the `vectorizer` for the collection `ClipExample`. It also sets:\n\n- `name` property as a `text` datatype and as the text field,\n- `image` property as a `blob` datatype and as the image field,\n\n```json\n{\n  \"classes\": [\n    {\n      \"class\": \"ClipExample\",\n      \"description\": \"An example collection for multi2vec-clip\",\n      // highlight-start\n      \"vectorizer\": \"multi2vec-clip\",\n      \"moduleConfig\": {\n        \"multi2vec-clip\": {\n          \"textFields\": [\"name\"],\n          \"imageFields\": [\"image\"],\n          \"inferenceUrl\": \"http://multi2vec-clip:8080\"  // Optional. Set to use a different inference container when using multiple inference containers.\n        }\n      },\n      \"properties\": [\n        {\n          \"dataType\": [\"text\"],\n          \"name\": \"name\"\n        },\n        {\n          \"dataType\": [\"blob\"],\n          \"name\": \"image\"\n        }\n      ],\n      // highlight-end\n    }\n  ]\n}\n```\n\n#### Example with weights\n\nThe following example adds weights for various properties, with the `textFields` at 0.4, and the `imageFields`, `audioFields`, and `videoFields` at 0.2 each.\n\n```json\n{\n  \"classes\": [\n    {\n      \"class\": \"ClipExample\",\n      \"moduleConfig\": {\n        \"multi2vec-clip\": {\n          ...\n          // highlight-start\n          \"weights\": {\n            \"textFields\": [0.7],\n            \"imageFields\": [0.3],\n          }\n          // highlight-end\n        }\n      }\n    }\n  ]\n}\n```\n\n\n\n### Adding `blob` data objects\n\nAny `blob` property type data must be base64 encoded. To obtain the base64-encoded value of an image for example, you can use the helper methods in the Weaviate clients or run the following command:\n\n```bash\ncat my_image.png | base64\n```\n\n## Additional search operators\n\nThe `multi2vec-clip` vectorizer module will enable the following `nearText` and `nearImage` search operators.\n\nThese operators can be used to perform cross-modal search and retrieval.\n\nThis means that when using the `multi2vec-clip` module any query using one modality (e.g. text) will include results in all available modalities, as all objects will be encoded into a single vector space.\n\n## Usage example\n\n### NearText\n\n\n\n### NearImage\n\n\n\n## Model selection\n\nTo select a model, please point `multi2vec-clip` to the appropriate Docker container.\n\nYou can use our pre-built Docker image as shown above, or build your own (with just a few lines of code).\n\nThis allows you to use any suitable model from the [Hugging Face model hub](https://huggingface.co/models) or your own custom model.\n\n### Using a public Hugging Face model\n\nYou can build a Docker image to use any **public SBERT CLIP** model from the [Hugging Face model hub](https://huggingface.co/models) with a two-line Dockerfile. In the following example, we are going to build a custom image for the [`clip-ViT-B-32` model](https://huggingface.co/sentence-transformers/clip-ViT-B-32).\n\n\n#### Step 1: Create a `Dockerfile`\nCreate a new `Dockerfile`. We will name it `clip.Dockerfile`. Add the following lines to it:\n\n```\nFROM semitechnologies/multi2vec-clip:custom\nRUN CLIP_MODEL_NAME=clip-ViT-B-32 TEXT_MODEL_NAME=clip-ViT-B-32 ./download.py\n```\n\n#### Step 2: Build and tag your Dockerfile.\n\nWe will tag our Dockerfile as `clip-inference`:\n\n```shell\ndocker build -f clip.Dockerfile -t clip-inference .\n```\n\n#### Step 3: Use the image\n\nYou can now push your image to your favorite registry or reference it locally in your Weaviate `docker-compose.yml` using the docker tag `clip-inference`.\n\n### Using a private or local model\n\nYou can build a Docker image which supports any model which is compatible with Hugging Face's `SentenceTransformers` and `ClIPModel`.\n\nTo ensure that text embeddings will output compatible vectors to image embeddings, you should only use models that have been specifically trained for use with CLIP models.\n\nIn the following example, we are going to build a custom image for a non-public model which we have locally stored at `./my-clip-model` and `./my-text-model`.\n\nBoth models were trained to produce embeddings which are compatible with one another.\n\nCreate a new `Dockerfile` (you do not need to clone this repository, any folder on your machine is fine), we will name it `my-models.Dockerfile`. Add the following lines to it:\n\n```\nFROM semitechnologies/transformers-inference:custom\nCOPY ./my-text-model /app/models/text\nCOPY ./my-clip-model /app/models/clip\n```\n\nThe above will make sure that your model ends up in the image at `/app/models/clip` and `/app/models/text` respectively.. This path is important, so that the application can find the model.\n\nNow you just need to build and tag your Dockerfile, we will tag it as `my-models-inference`:\n\n```shell\ndocker build -f my-models.Dockerfile -t my-models-inference .\n```\n\nThat's it! You can now push your image to your favorite registry or reference it locally in your Weaviate `docker-compose.yml` using the Docker tag `my-models-inference`.\n\nTo debug if your inference container is working correctly, you can send queries to the vectorizer module's inference container directly, so you can see exactly what vectors it would produce for which input.\n\nTo do so \u2013 you need to expose the inference container in your Docker Compose file \u2013 add something like this:\n\n```yaml\nports:\n  - \"9090:8080\"\n```\n\nto your `multi2vec-clip`.\n\nThen you can send REST requests to it directly, e.g.:\n\n```shell\nlocalhost:9090/vectorize -d '{\"texts\": [\"foo bar\"], \"images\":[]}'\n```\n\nand it will print the created vector(s) directly.\n\n## Model license(s)\n\nThe `multi2vec-clip` module uses the [`clip-ViT-B-32` model](https://huggingface.co/sentence-transformers/clip-ViT-B-32) from the [Hugging Face model hub](https://huggingface.co/models). Please see the [model page](https://huggingface.co/sentence-transformers/clip-ViT-B-32) for the license information.\n\nIt is your responsibility to evaluate whether the terms of its license(s), if any, are appropriate for your intended use.\n\n\n\n", "type": "Documentation", "name": "Retriever-vectorizer-modules Multi2vec-clip", "path": "developers/weaviate/modules/retriever-vectorizer-modules/multi2vec-clip.md", "link": "https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/multi2vec-clip", "timestamp": "2024-05-08 10:50:28", "reader": "JSON", "meta": {}, "chunks": []}