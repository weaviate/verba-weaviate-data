{"text": "\n## Overview\n\nThis tutorial will show you how to import a large dataset (25k articles from Wikipedia) that already includes vectors (embeddings generated by OpenAI). We will,\n* download and unzip a CSV file that contains the Wikipedia articles\n* create a Weaviate instance\n* create a schema\n* parse the file and batch import the records, with Python and JavaScript code\n* make sure the data was imported correctly\n* run a few queries to demonstrate semantic search capabilities\n\n\n## Prerequisites\n\n\n\n\nBefore you start this tutorial, make sure to have:\n\n- An OpenAI API key. Even though we already have vector embeddings generated by OpenAI, we'll need an OpenAI key to vectorize search queries, and to recalculate vector embeddings for updated object contents.\n- Your preferred Weaviate client library installed.\n\n\n  \n    See how to delete data from previous tutorials (or previous runs of this tutorial).\n  \n\nimport CautionSchemaDeleteClass from '/_includes/schema-delete-class.mdx'\n\n\n\n\n\n\n## Download the dataset\n\nWe will use this Simple English Wikipedia dataset hosted by OpenAI (~700MB zipped, 1.7GB CSV file) that includes vector embeddings. These are the columns of interest, where `content_vector` is a vector embedding with 1536 elements (dimensions), generated using OpenAI's `text-embedding-ada-002` model:\n\n| id | url | title | text | content_vector |\n|----|-----|-------|------|----------------|\n| 1 | https://simple.wikipedia.org/wiki/April | April | \"April is the fourth month of the year...\" | [-0.011034, -0.013401, ..., -0.009095] |\n\nIf you haven't already, make sure to download the dataset and unzip the file. You should end up with `vector_database_wikipedia_articles_embedded.csv` in your working directory. The records are mostly (but not strictly) sorted by title.\n\n\n  Download Wikipedia dataset ZIP\n\n\n\n## Create a Weaviate instance\n\nWe can create a Weaviate instance locally using the embedded option on Linux (transparent and fastest), Docker on any OS (fastest import and search), or in the cloud using the Weaviate Cloud Services (easiest setup, but importing may be slower due to the network speed). Each option is explained on its Installation page.\n\nIf using the Docker option, make sure to select \"With Modules\" (instead of standalone), and the `text2vec-openai` module when using the Docker configurator, at the \"Vectorizer & Retriever Text Module\" step. At the \"OpenAI Requires an API Key\" step, you can choose to \"provide the key with each request\", as we'll do so in the next section.\n\n\n## Connect to the instance and OpenAI\n\nTo pave the way for using OpenAI later when querying, let's make sure we provide the OpenAI API key to the client.\n\nimport ProvideOpenAIAPIKey from '/_includes/provide-openai-api-key-headers.mdx'\n\n\n\n\n## Create the schema\n\nThe schema defines the data structure for objects in a given Weaviate class. We'll create a schema for a Wikipedia `Article` class mapping the CSV columns, and using the text2vec-openai vectorizer. The schema will have two properties:\n* `title` - article title, not vectorized\n* `content` - article content, corresponding to the `text` column from the CSV\n\nAs of Weaviate 1.18, the `text2vec-openai` vectorizer uses by default the same model as the OpenAI dataset, `text-embedding-ada-002`. To make sure the tutorial will work the same way if this default changes (i.e. if OpenAI releases an even better-performing model and Weaviate switches to it as the default), we'll configure the schema vectorizer explicitly to use the same model:\n\n```json\n{\n  \"moduleConfig\": {\n    \"text2vec-openai\": {\n      \"model\": \"ada\",\n      \"modelVersion\": \"002\",\n      \"type\": \"text\"\n    }\n  }\n}\n```\n\nAnother detail to be careful about is how exactly we store the `content_vector` embedding. Weaviate vectorizes entire objects (not properties), and it includes by default the class name in the string serialization of the object it will vectorize. Since OpenAI has provided embeddings only for the `text` (content) field, we need to make sure Weaviate vectorizes an `Article` object the same way. That means we need to disable including the class name in the vectorization, so we must set `vectorizeClassName: false` in the `text2vec-openai` section of the `moduleConfig`. Together, these schema settings will look like this:\n\n\n\n\nTo quickly check that the schema was created correctly, you can navigate to `/v1/schema`. For example in the Docker installation scenario, go to `http://localhost:8080/v1/schema` or run,\n\n```bash\ncurl -s http://localhost:8080/v1/schema | jq\n```\n\nThe `jq` command used after `curl` is a handy JSON preprocessor. When simply piping some text through it, `jq` returns the text pretty-printed and syntax-highlighted.\n\n\n## Import the articles\n\nWe're now ready to import the articles. For maximum performance, we'll load the articles into Weaviate via batch import.\n\n\n\n\n\n### Checking the import went correctly\n\nTwo quick sanity checks that the import went as expected:\n\n1. Get the number of articles\n2. Get 5 articles\n\nGo to the Weaviate GraphQL console, connect to your Weaviate endpoint (e.g. `http://localhost:8080` or `https://some-endpoint.weaviate.network`), then run the GraphQL query below:\n\n```graphql\nquery {\n  Aggregate { Article { meta { count } } }\n\n  Get {\n    Article(limit: 5) {\n      title\n      url\n    }\n  }\n}\n```\n\nYou should see the `Aggregate.Article.meta.count` field equal to the number of articles you've imported (e.g. 25,000), as well as five random articles with their `title` and `url` fields.\n\n\n## Queries\n\nNow that we have the articles imported, let's run some queries!\n\n### nearText\n\nThe `nearText` filter lets us search for objects close (in vector space) to the vector representation of one or more concepts. For example, the vector for the query \"modern art in Europe\" would be close to the vector for the article Documenta, which describes\n> \"one of the most important exhibitions of modern art in the world... [taking] place in Kassel, Germany\".\n\n\n\n\n### hybrid\n\nWhile `nearText` uses dense vectors to find objects similar in meaning to the search query, it does not perform very well on keyword searches. For example, a `nearText` search for \"jackfruit\" in this Simple English Wikipedia dataset, will find \"cherry tomato\" as the top result. For these (and indeed, most) situation, we can obtain better search results by using the `hybrid` filter, which combines dense vector search with keyword search:\n\n\n\n\n\n## Recap\n\nIn this tutorial, we've learned\n* how to efficiently import large datasets using Weaviate batching and CSV lazy loading with `pandas` / `csv-parser`\n* how to import existing vectors (\"Bring Your Own Vectors\")\n* how to quickly check that all records were imported\n* how to use `nearText` and `hybrid` searches\n\n\n## Suggested reading\n\n- Tutorial: Schemas in detail\n- Tutorial: Queries in detail\n- Tutorial: Introduction to modules\n\n\n\n\n\n", "type": "Documentation", "name": "tutorials-wikipedia", "path": "developers/weaviate/tutorials/wikipedia.md", "link": "https://weaviate.io/developers/weaviate/tutorials/wikipedia", "timestamp": "2024-02-08 20:23:53", "reader": "JSON", "meta": {}, "chunks": []}