{"text": "\nNow that you've learned about different tokenization methods, let's put them into practice. In this section, you'll see how tokenization impacts filters.\n\n##  Preparation\n\nFor this section, we'll work with an actual Weaviate instance to see how different tokenization methods impact filtering results.\n\nWe are going to use a very small, custom dataset for demonstration purposes.\n\n\n\nTo follow along, you can use the following Python code to add this data to your Weaviate instance.\n\n\n  Steps to create a collection\n\nWe will create a simple object collection, with each object containing multiple properties. Each properties will contain the same text, but with different tokenization methods applied.\n\n\n\nNote that we do not add object vectors in this case, as we are only interested in the impact of tokenization on filters (and keyword searches).\n\n\n\n\n  Steps to add objects\n\nNow, we add objects to the collection, repeating text objects as properties.\n\n\n\n\n\n\n##  Impact on filters\n\nNow that we have added a set of objects to Weaviate, let's see how different tokenization methods impact filtered retrieval.\n\nEach filtered query will look something like this, wherein we filter the objects for a set of query strings.\n\nWe'll set up a reusable function to filter objects based on a set of query strings. Remember that a filter is binary: it either matches or it doesn't.\n\nThe function will return a list of matched objects, and print the matched objects for us to see.\n\n\n\n###  \"**Clark:** \"vs \"**clark**\" - messy text\n\nTypical text is often messy, with punctuations, mixed cases and other irregularities. Take a look at this example, where we filter for various combinations of substrings from the TV show title `\"Lois & Clark: The New Adventures of Superman\"`.\n\nThe table shows whether the query matched the title:\n\n|               | `word` | `lowercase` | `whitespace` | `field` |\n|---------------|--------|-------------|--------------|---------|\n| `\"clark\"`       | \u2705     | \u274c         | \u274c          | \u274c     |\n| `\"Clark\"`       | \u2705     | \u274c         | \u274c          | \u274c     |\n| `\"clark:\" `     | \u2705     | \u2705         | \u274c          | \u274c     |\n| `\"Clark:\" `     | \u2705     | \u2705         | \u2705          | \u274c     |\n| `\"lois clark\"`  | \u2705     | \u274c         | \u274c          | \u274c     |\n| `\"clark lois\"`  | \u2705     | \u274c         | \u274c          | \u274c     |\n\n\n  Python query & output\n\n\n\n\n\n\n\nNote how the `word` tokenization was the only that consistently returned the matching title, unless the colon (`:`) was included in the query. This is because the `word` tokenization method treats the colon as a separator.\n\nUsers may not be expected to include any punctuation in their queries, nor the exact capitalization. As a result, for a typical text filter usage, the `word` tokenization method is a good starting point.\n\n###  \"**A mouse**\" vs \"**mouse**\" - stop words\n\nHere, we filter for variants of the phrase \"computer mouse\", where some queries include additional words.\n\nNow, take a look at the results.\n\n**Matches for `\"computer mouse\"`**\n\n|                              | `word`    | `lowercase` | `whitespace` | `field` |\n|------------------------------|-----------|-------------|--------------|---------|\n| `\"computer mouse\"`           | \u2705        | \u2705           | \u2705           | \u2705     |\n| `\"a computer mouse\"`         | \u2705        | \u2705           | \u2705           | \u274c     |\n| `\"the computer mouse:\" `     | \u2705        | \u2705           | \u2705           | \u274c     |\n| `\"blue computer mouse\" `     | \u274c        | \u274c           | \u274c           | \u274c     |\n\n**Matches for `\"a computer mouse\"`**\n\n|                              | `word`    | `lowercase` | `whitespace` | `field` |\n|------------------------------|-----------|-------------|--------------|---------|\n| `\"computer mouse\"`           | \u2705        | \u2705           | \u2705           | \u274c     |\n| `\"a computer mouse\"`         | \u2705        | \u2705           | \u2705           | \u2705     |\n| `\"the computer mouse:\" `     | \u2705        | \u2705           | \u2705           | \u274c     |\n| `\"blue computer mouse\" `     | \u274c        | \u274c           | \u274c           | \u274c     |\n\n\n  Python query & output\n\n\n\n\n\n\n\nThe results indicate that adding the word \"a\" or \"the\" to the query does not impact the filter results for all methods except `field`. This is because at every tokenization method, the word \"a\" or \"the\" is considered a stop word and is ignored.\n\nWith the `field` method, the difference is that stop word tokens like \"a\" or \"the\" are never produced. An input \"a computer mouse\" is tokenized to `[\"a computer mouse\"]`, containing one token.\n\nAdding another word, such as \"blue\", that is not a stop word, causes the query to not match any objects.\n\n###  \"**variable_name**\" vs \"**variable name**\" - symbols\n\nThe `word` tokenization is a good default. However, it may not always be the best choice. Take a look at this example where we filter for different variants of `\"variable_name\"`, to see if they match the object with the exact string (`\"variable_name\"`).\n\n|                              | `word`    | `lowercase` | `whitespace` | `field` |\n|------------------------------|-----------|-------------|--------------|---------|\n| `\"variable_name\"`            | \u2705        | \u2705           | \u2705           | \u2705     |\n| `\"Variable_Name:\" `          | \u2705        | \u2705           | \u274c           | \u274c     |\n| `\"Variable Name:\" `          | \u2705        | \u274c           | \u274c           | \u274c     |\n| `\"a_variable_name\"`          | \u2705        | \u274c           | \u274c           | \u274c     |\n| `\"the_variable_name\"`        | \u2705        | \u274c           | \u274c           | \u274c     |\n| `\"variable_new_name\" `       | \u2705        | \u274c           | \u274c           | \u274c     |\n\n\n  Python query & output\n\n\n\n\n\n\n\nWhat is the desired behavior here? Should a filter for `\"variable name\"` match the object with the property `\"variable_name\"`?\n\nWhat about a filter for `\"variable_new_name\"`? If the goal is to look through, say, a code base, the user might not expect a filter for `\"variable_new_name\"` to match `\"variable_name\"`.\n\nIn cases such as these, where symbols are important to your data, you should consider using a tokenization method that preserves symbols, such as `lowercase` or `whitespace`.\n\n##  Discussions\n\nWe've discussed how different tokenization methods impact filters.\n\nFor most filtering use, the `word` tokenization method is a good starting point. It is case-insensitive, and treats most symbols as separators.\n\nHowever, if symbols are important to your data, or if you need to distinguish between different cases, you may want to consider using a different tokenization method.\n\nAnd what about `field` tokenization? This method is most useful when you have text that should be treated as a single token. This is useful for properties like email addresses, URLs, or identifiers.\n\nA typical filtering strategy with a `field` tokenization method might involve exact matches, or partial matches with wildcards. Do note, however, that wildcard-based filters can be computationally expensive (slow) - so use them judiciously.\n\nNext, we'll discuss how tokenization impacts keyword searches.\n\n\n", "type": "Documentation", "name": "Tokenization Filters", "path": "developers/academy/py/tokenization/300_filters.mdx", "link": "https://weaviate.io/developers/academy/py/tokenization/filters", "timestamp": "2024-05-08 10:48:22", "reader": "JSON", "meta": {}, "chunks": []}