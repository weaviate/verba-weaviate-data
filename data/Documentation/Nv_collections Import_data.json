{"text": "\n##  Code\n\nThis example imports the movie data into our collection.\n\nAmazingly, the exact same code that we used for single vector configuration [in the multimodal course](../../starter_multimodal_data/102_mm_collections/20_create_collection.mdx) can be used here. This is because the named vector configuration is set up in the collection definition, and Weaviate handles the rest.\n\n\n\nThe code:\n- Loads the source text and image data\n- Gets the collection\n- Enters a context manager with a batcher (`batch`) object\n- Loops through the data and:\n    - Finds corresponding image to the text\n    - Converts the image to base64\n    - Adds objects to the batcher\n- Prints out any import errors\n\nWe won't repeat the explanation of the code here, as it is the same as in the multimodal course. If you would like a refresher, please review the [multimodal course](../../starter_multimodal_data/102_mm_collections/20_create_collection.mdx).\n\n##  Where do the vectors come from?\n\nWhen the batcher sends the queue to Weaviate, the objects are added to the collection. In our case, the movie collection.\n\nIn this case, recall that we have three named vectors for each object - `title`, `overview` and `poster_title`. The vectors are generated by the vectorizers that we set up in the collection definition.\n\n- The `title` vector is generated by the `text2vec-openai` vectorizer\n- The `overview` vector is generated by the `text2vec-openai` vectorizer\n- The `poster_title` vector is generated by the `multi2vec-clip` vectorizer\n\nNext, we will explore how these named vectors provide flexibility in searching for our data.\n\n\n", "type": "Documentation", "name": "Nv_collections Import_data", "path": "developers/academy/py/named_vectors/102_nv_collections/30_import_data.mdx", "link": "https://weaviate.io/developers/academy/py/named_vectors/nv_collections/import_data", "timestamp": "2024-05-08 10:47:38", "reader": "JSON", "meta": {}, "chunks": []}