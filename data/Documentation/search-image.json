{"text": "\n\n\n\n## Overview\n\nThis page covers additional, unique aspects related to similarity searches using an image as an input.\n\nIf you wish to search for images using a vector or another object, please refer to the How-to: similarity search page.\n\n- How-to: Similarity search\n\nImage-based search is currently not available in WCS, as the required modules are not available.\n\n### Target object types\n\nTo search using an image as an input, you must use the `img2vec-neural` or the `multi2vec-clip` vectorizer module. More specifically:\n- To find similar images, you can use `img2vec-neural` or `multi2vec-clip`\n- To find related text and image objects (i.e. for multi-modal search), you must use `multi2vec-clip`\n\n## Requirements\n\nTo search using an input image, you must:\n* Configure Weaviate with an image vectorizer module (`img2vec-neural` or `multi2vec-clip`), and\n* Configure the target class to use the image vectorizer module\n\n\n  How do I configure Weaviate with an image vectorizer module?\n\nYou must enable the desired vectorizer module and specify the inference API address in the relevant Docker Compose file (e.g. `docker-compose.yml`). You can generate this file using the Weaviate configuration tool.\n\nAn example `img2vec-neural` configuration is shown below:\n\n```yaml\nservices:\n  weaviate:\n    environment:\n      IMAGE_INFERENCE_API: \"http://i2v-neural:8080\"\n      DEFAULT_VECTORIZER_MODULE: 'img2vec-neural'\n      ENABLE_MODULES: 'img2vec-neural'\n  i2v-neural:\n    image: semitechnologies/img2vec-pytorch:resnet50\n```\n\nAnd an example `multi2vec-clip` configuration is shown below:\n\n```yaml\nservices:\n  weaviate:\n    environment:\n      CLIP_INFERENCE_API: 'http://multi2vec-clip:8080'\n      DEFAULT_VECTORIZER_MODULE: 'multi2vec-clip'\n      ENABLE_MODULES: 'multi2vec-clip'\n  multi2vec-clip:\n    image: semitechnologies/multi2vec-clip:sentence-transformers-clip-ViT-B-32-multilingual-v1\n    environment:\n      ENABLE_CUDA: '0'\n```\n\n\n\n\n  How do I configure the target class with the image vectorizer module?\n\nYou must configure the target class to:\n- Ensure that the target class is configured to use the image vectorizer module, such as by explicitly setting it as the vectorizer for the class. And\n- Specify in the `imageFields` property the blob field(s) that will store the images.\n\nFor using `img2vec-neural`, an example class definition may look as follows:\n\n```json\n{\n  \"classes\": [\n    {\n      \"class\": \"ImageExample\",\n      \"moduleConfig\": {\n        \"img2vec-neural\": {\n          \"imageFields\": [\n            \"image\"\n          ]\n        }\n      },\n      \"properties\": [\n        {\n          \"dataType\": [\n            \"blob\"\n          ],\n          \"description\": \"Grayscale image\",\n          \"name\": \"image\"\n        }\n      ],\n      \"vectorizer\": \"img2vec-neural\"\n    }\n  ]\n}\n```\n\nFor using `multi2vec-clip`, an example class definition may look as follows:\n\n```json\n{\n  \"classes\": [\n    {\n      \"class\": \"ClipExample\",\n      \"moduleConfig\": {\n        \"multi2vec-clip\": {\n          \"imageFields\": [\n            \"image\"\n          ]\n        }\n      },\n      \"properties\": [\n        {\n          \"dataType\": [\n            \"blob\"\n          ],\n          \"name\": \"image\"\n        }\n      ],\n      \"vectorizer\": \"multi2vec-clip\"\n    }\n  ]\n}\n```\n\nNote that for the multi2vec-clip vectorizer module, there are additional settings available such as how to balance text and image-derived vectors.\n\n\n\nSee the relevant module page for:\n- img2vec-neural\n- multi2vec-clip\n- multi2vec-bind\n\n## base64 nearImage search\n\nYou can find similar images by performing a `nearImage` search for the based64-encoded representation of the image.\n\nYou can obtain this representation (a long string) as below:\n\n\n  \n    \n  \n\n  \n    \n  \n\n  \n    \n  \n\n  \n\n  ```shell\n  base64 -i Corgi.jpg\n  ```\n\n  \n\n\nThen, you can search for similar images as follows:\n\n\n  \n    \n  \n\n  \n    \n  \n\n  \n    \n  \n\n\n\n\n  Example response\n\n  \n\n\n\n\n## Specify image by filename\n\nIf your target image is stored in a file, you can use the Python client to search for the image by its filename.\n\n\n  \n    \n  \n\n  \n    \n  \n\n  \n\n  > Not available yet. Vote for the feature request. DYI code below.\n\n  \n\n  \n\n\n\n  Example response\n\n  \n\n\n\n## Distance threshold\n\nYou can set a threshold for similarity search by setting a maximum `distance`. The distance indicates how dissimilar two images are.\nThe syntax is the same as for the other `nearXXX` operators.\n\n\n  \n    \n  \n\n  \n    \n  \n\n  \n\n  > Not available yet. Vote for the feature request. DYI code below.\n\n  \n\n  \n\n\n\n\n\n", "type": "Documentation", "name": "search-image", "path": "developers/weaviate/search/image.md", "link": "https://weaviate.io/developers/weaviate/search/image", "timestamp": "2023-11-13 10:41:33", "reader": "JSON", "meta": {}, "chunks": []}