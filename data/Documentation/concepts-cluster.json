{"text": "\n\n## Introduction\nWeaviate can be scaled horizontally by being run on a set of multiple nodes in a cluster. This section lays out various ways in which Weaviate can be scaled, as well as factors to consider while scaling, and Weaviate's architecture in relation to horizontal scaling.\n\n## Motivation to scale Weaviate\nGenerally there are (at least) three distinct motivations to scale out horizontally which all will lead to different setups.\n\n### Motivation 1: Maximum Dataset Size\nDue to the memory footprint of an HNSW graph it may be desirable to spread a dataset across multiple servers (\"nodes\"). In such a setup, a single class index is composed of multiple shards and shards are spread across servers.\n\nWeaviate does the required orchestration at import and query time fully automatically. The only adjustment required is to specify the desired shard count. See Sharding vs Replication below for trade-offs involved when running multiple shards.\n\n**Solution: Sharding across multiple nodes in a cluster**\n\nThe ability to shard across a cluster was added in Weaviate `v1.8.0`.\n\n### Motivation 2: Higher Query Throughput\nWhen you receive more queries than a single Weaviate node can handle, it is desirable to add more Weaviate nodes which can help in responding to your users' queries.\n\nInstead of sharding across multiple nodes, you can replicate (the same data) across multiple nodes. This process also happens fully automatically and you only need to specify the desired replication factor. Sharding and replication can also be combined.\n\n**Solution: Replicate your classes across multiple nodes in a cluster**\n\n### Motivation 3: High Availability\n\nWhen serving critical loads with Weaviate, it may be desirable to be able to keep serving queries even if a node fails completely. Such a failure could be either due to a software or OS-level crash or even a hardware issue. Other than unexpected crashes, a highly available setup can also tolerate zero-downtime updates and other maintenance tasks.\n\nTo run a highly available setup, classes must be replicated among multiple nodes.\n\n**Solution: Replicate your classes across multiple nodes in a cluster**\n\n## Sharding vs Replication\nThe motivation sections above outline when it is desirable to shard your classes across multiple nodes and when it is desirable to replicate your classes - or both. This section highlights the implications of a sharded and/or replicated setup.\n\nAll of the scenarios below assume that - as sharding or replication is increased - the cluster size is adapted accordingly. If the number of shards or the replication factor is lower than the number of nodes in the cluster, the advantages no longer apply.*\n\n### Advantages when increasing sharding\n* Run larger datasets\n* Speed up imports\n\n### Disadvantages when increasing sharding\n* Query throughput does not improve when adding more sharded nodes\n\n### Advantages when increasing replication\n* System becomes highly available\n* Increased replication leads to near-linearly increased query throughput\n\n### Disadvantages when increasing replication\n* Import speed does not improve when adding more replicated nodes\n\n## Sharding Keys (\"Partitioning Keys\")\nWeaviate uses specific characteristics of an object to decide which shard it belongs to. As of `v1.8.0`, a sharding key is always the object's UUID. The sharding algorithm is a 64bit Murmur-3 hash. Other properties and other algorithms for sharding may be added in the future.\n\n## Resharding\n\nWeaviate uses a virtual shard system to assign objects to shards. This makes it more efficient to re-shard, as minimal data movement occurs when re-sharding. However, due to the HNSW index, resharding is still a very costly process and should be used rarely. The cost of resharding is roughly that of an initial import with regards to the amount of data that needs to be moved.\n\nExample - assume the following scenario: A class is comprised of 4 shards and taking 60 minutes to import all data. When changing the sharding count to 5, each shard will roughly transfer 20% of its data to the new shard. This is equivalent to an import of 20% of the dataset, so the expected time for this process would be ~12 minutes.\n\nThe groundwork to be able to re-shard has been laid by using Weaviate's Virtual shard system. Re-sharding however is not yet implemented and not currently prioritized. See Weaviate's Architectural Roadmap.\n\n## Node Discovery\n\nNodes in a cluster use a gossip-like protocol through Hashicorp's Memberlist to communicate node state and failure scenarios.\n\nWeaviate - especially when running as a cluster - is optimized to run on Kubernetes. The Weaviate Helm chart makes use of a `StatefulSet` and a headless `Service` that automatically configures node discovery. All you have to do is specify the desired node count.\n\n## Node affinity of shards and/or replication shards\n\nWeaviate tries to select the node with the most available disk space.\n\nThis only applies when creating a new class, rather than when adding more data to an existing single class.\n\n\n  Pre-v1.18.1 behavior\n\nIn versions `v1.8.0`-`v1.18.0`, users could not specify the node-affinity of a specific shard or replication shard.\n\nShards were assigned to 'live' nodes in a round-robin fashion starting with a random node.\n\n\n\n## Consistency and current limitations\n\n* Changes to the schema are strongly consistent across nodes, whereas changes to data aim to be eventually consistent.\n* As of `v1.8.0`, the process of broadcasting schema changes across the cluster uses a form of two-phase transaction that as of now cannot tolerate node failures during the lifetime of the transaction.\n* As of `v1.8.0`, replication is currently under development. (See Roadmap).\n* As of `v1.8.0`, dynamically scaling a cluster is not fully supported yet. New nodes can be added to an existing cluster, however it does not affect the ownership of shards. Existing nodes can not yet be removed if data is present, as shards are not yet being moved to other nodes prior to a removal of a node. (See Roadmap).\n\n\n\n\n\n", "type": "Documentation", "name": "concepts-cluster", "path": "developers/weaviate/concepts/cluster.md", "link": "https://weaviate.io/developers/weaviate/concepts/cluster", "timestamp": "2023-11-13 10:40:25", "reader": "JSON", "meta": {}, "chunks": []}