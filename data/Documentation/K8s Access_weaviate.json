{"text": "\nWe have now spun up a Weaviate instance in our Kubernetes cluster. So what's next? In this section, we will look at how to access the Weaviate service, and how to configure it to suit your needs.\n\n##  Access Weaviate\n\nAlthough our Weaviate service is happily running, it is not yet accessible from the outside world. This is because we have not exposed the service to the outside world. Let's do that now.\n\n###  Expose the services\n\nRun the following command:\n\n```bash\nminikube tunnel\n```\n\nYou will recall that we configured the `weaviate` service as a `LoadBalancer` type in our Helm chart. So, when we run `minikube tunnel`, it will expose the service to the outside world - or at least, to our local machine.\n\nYou will see a message like:\n\n```bash\n\u2705  Tunnel successfully started\n\n\ud83d\udccc  NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ...\n\n\u2757  The service/ingress weaviate requires privileged ports to be exposed: [80]\n\ud83d\udd11  sudo permission will be asked for it.\n\ud83c\udfc3  Starting tunnel for service weaviate.\n\ud83c\udfc3  Starting tunnel for service weaviate-grpc.\n```\n\nAt this point you will be asked for your password. Enter it and the tunnel will be established. Note that closing the terminal or stopping the process will close the tunnel, making the services inaccessible again.\n\n[`minikube tunnel`](https://minikube.sigs.k8s.io/docs/handbook/accessing/#using-minikube-tunnel) creates a route between your local machine and the Minikube cluster. This allows services within your Minikube cluster that are exposed as LoadBalancer to be accessible on your local machine for development.\n\nWe suggest you only run the tunnel command when you need to access the service from your local machine. When you are done, you can stop the tunnel by pressing `Ctrl+C`.\n\n###  Confirm access\n\nNow, if you run:\n\n```bash\nkubectl get svc weaviate -n weaviate\n```\n\nYou will see the external IP address of the service. For example:\n\n```bash\nNAME       TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nweaviate   LoadBalancer   10.110.44.231   127.0.0.1     80:31230/TCP   61m\n```\n\nNavigate to `http://:80/v1` in your browser (typically `http://127.0.0.1:80/v1`). You should see the Weaviate REST root endpoint, with links to the various endpoints available in Weaviate.\n\nNow, you might also recall that we've opened up the gRPC service in our Kubernetes configuration. This service is available on port 50051. You can confirm this by running:\n\n```bash\nkubectl get svc weaviate-grpc -n weaviate\n```\n\nWhich will show:\n\n```bash\nNAME            TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE\nweaviate-grpc   LoadBalancer   10.109.237.21   127.0.0.1     50051:32150/TCP   90m\n```\n\n\n  \n    Another way to confirm access to the gRPC service\n  \n\nIf you have netcat installed, you can also try:\n\n```bash\nnc -zv 127.0.0.1 50051\n```\n\nWhich will show:\n\n```bash\nConnection to 127.0.0.1 port 50051 [tcp/*] succeeded!\n```\n\nNote that not all systems have `nc` installed by default. It's okay if you don't have it - the `kubectl get svc` command output is sufficient to confirm access to the gRPC service.\n\n\n\n\n##  Configure Weaviate\n\nOne of the best things about Kubernetes is that you can easily configure your services. Weaviate is no exception. You can configure Weaviate by updating the `values.yaml` file in the `weaviate` directory.\n\nFor example, you can enable additional modules such as `text2vec-openai` and `generative-openai` modules by setting them to `true`:\n\n```yaml\n  text2vec-openai:\n\n    enabled: true  # \u2b05\ufe0f Set to true\n\n  # ... other settings not shown ...\n\n  generative-openai:\n\n    enabled: true  # \u2b05\ufe0f Set to true\n```\n\nOr we can set resource limits for the Weaviate pods. Let's set them to utilize 30-50% of a CPU, and 150-300Mi of memory:\n\nThe `values.yaml` file contains multiple instances of `requests` and `limits` for different services, such as for local vectorization models. Make sure to set the `requests` and `limits` for the scale replicas of Weaviate towards the top of the file with no indentation.\n\n```yaml\n# Scale replicas of Weaviate. ...\nrequests:\n  cpu: '300m'\n  memory: '150Mi'\nlimits:\n  cpu: '500m'\n  memory: '300Mi'\n```\n\nTo apply these changes, save the `values.yaml` file and run:\n\n```bash\nhelm upgrade --install \\\n  \"weaviate\" \\\n  weaviate/weaviate \\\n  --namespace \"weaviate\" \\\n  --values ./values.yaml\n```\n\nYou will note that this is the same command we used to deploy Weaviate. This command will simply update the Weaviate deployment with the new configuration.\n\nThere are a whole host of other configurations you can set in the `values.yaml` file, such as modifying authentication, authorization, backups, monitoring, resource allocation and so on. Please refer to the in-line documentation in the `values.yaml` file, and the Weaviate documentation for more information.\n\nBefore we go, however, let's take a look at expanding our Weaviate deployment to include more nodes. This can help us to scale our Weaviate deployment to handle more traffic or growth, or to provide redundancy in case of node failure.\n\nWe'll take a look at both in the next section.\n\n\n", "type": "Documentation", "name": "K8s Access_weaviate", "path": "developers/academy/deployment/k8s/50_access_weaviate.mdx", "link": "https://weaviate.io/developers/academy/deployment/k8s/access_weaviate", "timestamp": "2024-05-08 10:47:31", "reader": "JSON", "meta": {}, "chunks": []}