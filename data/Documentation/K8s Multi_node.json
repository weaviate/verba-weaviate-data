{"text": "\nRecall that we have deployed Weaviate on a single node in our Kubernetes cluster with Minikube. Now, let's scale it up to a multi-node setup.\n\nTo do this, we will need a Kubernetes cluster with multiple nodes. Then, we can configure Weaviate to make use of these nodes.\n\n##  Add nodes to your Weaviate cluster\n\nWe'll stop the current Weaviate deployment and then deploy a new one with [multiple nodes with Minikube](https://minikube.sigs.k8s.io/docs/tutorials/multi_node/).\n\nKeep in mind that this runs multiple containers on the same host device for learning. In a production environment, you would typically have a managed Kubernetes cluster with multiple, isolated, physical or virtual nodes.\n\n###  Stop the current Weaviate deployment\n\nFirst, stop the tunnel by pressing `Ctrl+C` at the terminal where you ran `minikube tunnel`.\n\nThen, stop Minikube:\n\n```bash\nminikube stop\n```\n\nSince the minikube cluster exists, we will have to delete it before we can start a new one with multiple nodes:\n\nYou can also add nodes to an existing Minikube cluster. But we'll delete the current one and start a new one for simplicity.\n\n```bash\nminikube delete\n```\n\n###  Start a multi-node Kubernetes cluster\n\nTo start a multi-node Kubernetes cluster with Minikube, run:\n\n```bash\nminikube start --nodes \n```\n\nReplace `` with the number of nodes you want in your cluster. In our case, let's spin up a 6-node cluster:\n\n```bash\nminikube start --nodes 6\n```\n\nOnce that's finished, you can check the status of your nodes by running:\n\n```bash\nkubectl get nodes\n```\n\nAnd you should see output like:\n\n```bash\nNAME           STATUS   ROLES           AGE   VERSION\nminikube       Ready    control-plane   78s   v1.28.3\nminikube-m02   Ready              60s   v1.28.3\nminikube-m03   Ready              50s   v1.28.3\nminikube-m04   Ready              39s   v1.28.3\nminikube-m05   Ready              29s   v1.28.3\nminikube-m06   Ready              18s   v1.28.3\n```\n\nNow let's update the Weaviate deployment to use these nodes. To do this, we'll update the `replicas` field in the `values.yaml` file to match the number of nodes in the cluster. Along with our reduced resource requests and limits, this section should look as follows:\n\n```yaml\nreplicas: 6\nupdateStrategy:\n  type: RollingUpdate\nresources: {}\nrequests:\n  cpu: '300m'\n  memory: '150Mi'\nlimits:\n  cpu: '500m'\n  memory: '300Mi'\n```\n\nIf we restart weaviate by running:\n\n```bash\nhelm upgrade --install \\\n  \"weaviate\" \\\n  weaviate/weaviate \\\n  --namespace \"weaviate\" \\\n  --values ./values.yaml\n```\n\nThen, you should see Weaviate pods come up on each of the nodes in your cluster. You can check this by running:\n\n```bash\nkubectl get pods -n weaviate\n```\n\nAnd you should see output like:\n\n```bash\nNAME         READY   STATUS            RESTARTS   AGE\nweaviate-0   1/1     Running           0          31s\nweaviate-1   0/1     PodInitializing   0          9s\n```\n\nAs pods are created one by one.\n\nEventually, you will see something like:\n\n```bash\nNAME         READY   STATUS    RESTARTS   AGE\nweaviate-0   1/1     Running   0          113s\nweaviate-1   1/1     Running   0          91s\nweaviate-2   1/1     Running   0          79s\nweaviate-3   1/1     Running   0          57s\nweaviate-4   1/1     Running   0          35s\nweaviate-5   1/1     Running   0          13s\n```\n\nOpen a new terminal and run `minikube tunnel` to expose the Weaviate service to your local machine as before.\n\n##  Utilizing a multi-node setup\n\nThere are two main ways to utilize a multi-node setup, namely **replication** and **horizontal scaling**. Let's take a look at each of these.\n\n###  Replication\n\nReplication is the process of creating multiple copies of the same data across multiple nodes.\n\nThis is useful for ensuring data availability and fault tolerance. In a multi-node setup, you can replicate your Weaviate data across multiple nodes to ensure that if one node fails, the data is still available on other nodes. A replicated setup can also help distribute the load across multiple nodes to improve performance, and allow zero-downtime upgrades and maintenance.\n\nWeaviate can handle replication by setting a replication factor in the database collection definition. This tells Weaviate how many copies of the data to keep across the nodes in the cluster.\n\nThe code examples show how to configure replication. Keep in mind that the specified port may be different for you (e.g. `80`) than what is shown in the code snippet.\n\n\n\n###  Database sharding\n\nOn the other hand, sharding can be used to horizontally scale Weaviate. Sharding is the process of splitting a database into smaller parts, called shards, and distributing these shards across multiple nodes.\n\nSo a database that holds 2 million records do not need to store all 2 million records on a single node. Instead, it can split the records into smaller parts and store them on different nodes. This allows the database to scale horizontally by adding more nodes to the cluster.\n\n###  Sharding with replication\n\nYou can use both sharding and replication together to horizontally scale and ensure fault tolerance in your Weaviate setup.\n\nIn our example with 6 nodes, we could have a replication factor of 3 and shard the data across 2 nodes each. This way, we have 3 copies of the data spread across 6 nodes, ensuring fault tolerance and high availability.\n\nFor a production setup, this is a common approach to ensure that your Weaviate setup can handle high loads and remain available even if a node fails. Additionally, you can add more nodes to the cluster as needed to scale your Weaviate setup horizontally, ensuring that it can handle more data and more requests.\n\n##  Clean-up\n\nThat's it for this guide. You've successfully deployed Weaviate on a multi-node Kubernetes cluster and learned about replication and sharding.\n\nIf you would like, you can access Weaviate as described before, and work with it. We include [further resources on the next page](./90_next_steps.mdx) for you to explore.\n\nWhen you are finished, you can stop the tunnel by pressing `Ctrl+C` in the terminal where you ran `minikube tunnel`, Then, stop Minikube and delete the cluster:\n\n```bash\nminikube stop\nminikube delete\n```\n\n\n", "type": "Documentation", "name": "K8s Multi_node", "path": "developers/academy/deployment/k8s/70_multi_node.mdx", "link": "https://weaviate.io/developers/academy/deployment/k8s/multi_node", "timestamp": "2024-05-08 10:47:31", "reader": "JSON", "meta": {}, "chunks": []}