{"text": "\n\n## Overview\n\n- `text2vec-gpt` is only available from Weaviate version `v1.21`\n- Currently, `text2vec-gpt4all` is only available for `amd64/x86_64` architecture devices.\n    - This is as the `gpt4all` library currently does not support ARM devices, such as Apple M-series.\n\nThe `text2vec-gpt4all` module enables Weaviate to obtain vectors using the gpt4all library.\n\nKey notes:\n\n- This module is not available on Weaviate Cloud Services (WCS).\n- This module is optimized for CPU using the `ggml` library, allowing for fast inference even without a GPU.\n- Enabling this module will enable the `nearText` search operator.\n- By default, **input text longer than 256 tokens is mean-pooled with an overlapping context window up to the number of tokens in your input.**\n- Currently, the only available model is `all-MiniLM-L6-v2`.\n\n## Weaviate instance configuration\n\nThis module is not available on Weaviate Cloud Services.\n\n### Docker Compose file\n\nTo use `text2vec-gpt4all`, you must enable it in your Docker Compose file (`docker-compose.yml`). You can do so manually, or create one using the Weaviate configuration tool.\n\n#### Parameters\n\n- `ENABLE_MODULES` (Required): The modules to enable. Include `text2vec-gpt4all` to enable the module.\n- `DEFAULT_VECTORIZER_MODULE` (Optional): The default vectorizer module. You can set this to `text2vec-gpt4all` to make it the default for all classes.\n\n#### Example\n\nThis configuration enables `text2vec-gpt4all`, sets it as the default vectorizer, and sets the API keys.\n\n```yaml\n---\nversion: '3.4'\nservices:\n  weaviate:\n    command:\n    - --host\n    - 0.0.0.0\n    - --port\n    - '8080'\n    - --scheme\n    - http\n    image: semitechnologies/weaviate:||site.weaviate_version||\n    ports:\n    - 8080:8080\n    restart: on-failure:0\n    environment:\n      QUERY_DEFAULTS_LIMIT: 25\n      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\n      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'\n      DEFAULT_VECTORIZER_MODULE: 'text2vec-gpt4all'\n      # highlight-start\n      ENABLE_MODULES: 'text2vec-gpt4all'\n      GPT4ALL_INFERENCE_API: 'http://text2vec-gpt4all:8080'\n      # highlight-end\n      CLUSTER_HOSTNAME: 'node1'\n# highlight-start\n  text2vec-gpt4all:\n    image: semitechnologies/gpt4all-inference:all-MiniLM-L6-v2\n# highlight-end\n...\n```\n\n## Class configuration\n\nYou can configure how the module will behave in each class through the Weaviate schema.\n\n### Example\n\nThe following example configures the `Article` class by setting the vectorizer to `text2vec-gpt4all`:\n\n```json\n{\n  \"classes\": [\n    {\n      \"class\": \"Article\",\n      \"description\": \"A class called article\",\n      // highlight-start\n      \"vectorizer\": \"text2vec-gpt4all\"\n      // highlight-end\n    }\n  ]\n}\n```\n\n### Vectorization settings\n\nYou can set vectorizer behavior using the `moduleConfig` section under each class and property:\n\n#### Class-level\n\n- `vectorizer` - what module to use to vectorize the data.\n- `vectorizeClassName` \u2013 whether to vectorize the class name. Default: `true`.\n\n#### Property-level\n\n- `skip` \u2013 whether to skip vectorizing the property altogether. Default: `false`\n- `vectorizePropertyName` \u2013 whether to vectorize the property name. Default: `false`\n\n#### Example\n\n```json\n{\n  \"classes\": [\n    {\n      \"class\": \"Article\",\n      \"description\": \"A class called article\",\n      \"vectorizer\": \"text2vec-gpt4all\",\n      \"moduleConfig\": {\n        \"text2vec-gpt4all\": {\n          // highlight-start\n          \"vectorizeClassName\": false\n          // highlight-end\n        }\n      },\n      \"properties\": [\n        {\n          \"name\": \"content\",\n          \"dataType\": [\"text\"],\n          \"description\": \"Content that will be vectorized\",\n          // highlight-start\n          \"moduleConfig\": {\n            \"text2vec-gpt4all\": {\n              \"skip\": false,\n              \"vectorizePropertyName\": false\n            }\n          }\n          // highlight-end\n        }\n      ]\n    }\n  ]\n}\n```\n\n## Additional information\n\n### Available models\n\nCurrently, the only available model is `all-MiniLM-L6-v2`.\n\n### CPU optimized inference\n\nThe `text2vec-gpt4all` module is optimized for CPU inference and should be noticeably faster then `text2vec-transformers` in CPU-only (i.e. no CUDA acceleration) usage. You can read more about expected inference times here.\n\n### Usage advice - chunking text with `gpt4all`\n\n`text2vec-gpt4all` will truncate input text longer than `256` tokens (word pieces).\n\nAccordingly, this model is not suitable for use cases where larger chunks are required. In these cases, we recommend using other models that support longer input lengths, such as by selecting one from the  `text2vec-transformers` module or `text2vec-openai`.\n\n## Usage example\n\nThis is an example of a `nearText` query with `text2vec-gpt4all`.\n\n\n\n\n## Model license(s)\n\nThe `text2vec-gpt4all` module uses the `gpt4all` library, which in turn uses the `all-MiniLM-L6-v2` model. Please refer to the respective documentation for more information on their respective licenses.\n\nIt is your responsibility to evaluate whether the terms of its license(s), if any, are appropriate for your intended use.\n\n\n\n\n", "type": "Documentation", "name": "retriever-vectorizer-modules-text2vec-gpt4all", "path": "developers/weaviate/modules/retriever-vectorizer-modules/text2vec-gpt4all.md", "link": "https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-gpt4all", "timestamp": "2023-11-13 10:41:11", "reader": "JSON", "meta": {}, "chunks": []}